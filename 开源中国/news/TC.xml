<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - news - 繁體中文（台灣）</title>
    <link>https://www.oschina.net/news/project</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news" rel="self" type="application/rss+xml"/>
    <description>已對該 RSS 進行格式化操作：中英字符之間插入空格、使用直角引號、標點符號修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-tw</language>
    <lastBuildDate>Wed, 13 Aug 2025 02:43:22 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>阿里通義升級 Qwen Chat 的 Deep Research （深入研究）功能</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;阿里通義 Qwen 團隊宣佈對 Qwen Chat 的 Deep Research 能力進行了升級。此次升級旨在提供更智能、更具洞察力的研究報告。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1200" src="https://static.oschina.net/uploads/space/2025/0813/102724_zT9i_2720166.png" width="900" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;新版本通過更深入的搜索來獲取更豐富的研究發現，並提高了信息準確性以減少幻覺現象。技術上，新版本支持模塊化工具和並行執行。&lt;/p&gt; 
&lt;p&gt;此外，一個重要的新增功能是多模態輸入支持，允許用戶上傳文件和圖像進行研究。用戶可通過官方鏈接體驗此項新功能。&lt;/p&gt; 
&lt;p&gt;https://chat.qwen.ai/?inputFeature=deep_research&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365895</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365895</guid>
      <pubDate>Wed, 13 Aug 2025 02:31:40 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>騰訊混元發佈 52B 參數多模態理解模型 Large-Vision</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;騰訊混元團隊近日&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FgjZygQA9mRm-fYLWa1YqMA" target="_blank"&gt;發佈&lt;/a&gt;了全新的多模態理解模型——混元 Large-Vision，該模型採用騰訊混元擅長的 MoE（專家混合）架構，激活參數達到 52B 規模，在性能與效率之間實現了良好平衡。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="314" src="https://oscimg.oschina.net/oscnet/up-c4400b13dcd221f1bca8f7c1cfe0d3ae958.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="221" src="https://oscimg.oschina.net/oscnet/up-e4ccecb8e928aa25ac259f743c1c80510d1.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;混元 Large-Vision 的核心亮點在於其強大的多模態輸入支持能力。該模型不僅支持任意分辨率的圖像處理，還能處理視頻和 3D 空間輸入，為用戶提供了全方位的視覺理解體驗。這一技術突破意味着用戶可以直接輸入各種格式和尺寸的視覺內容，無需進行復雜的預處理操作。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;MoE 架構通過動態激活部分專家網絡來處理不同類型的輸入，既保證了模型的強大性能，又避免了全參數激活帶來的計算資源浪費。52B 的激活參數規模在當前多模態模型中處於先進水平，能夠處理複雜的視覺理解任務。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;該模型還重點提升了多語言場景理解能力，這對於全球化應用具有重要意義。在處理包含多種語言文字的圖像或視頻時，混元 Large-Vision 能夠準確識別和理解不同語言環境下的視覺內容，為跨語言的多模態應用提供了技術基礎。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;混元 Large-Vision 支持任意分辨率圖像輸入的特性尤其值得關注。傳統的視覺模型往往需要將輸入圖像調整到固定尺寸，這可能導致信息丟失或畫質下降。而混元 Large-Vision 能夠直接處理原始分辨率的圖像，保持了視覺信息的完整性，這對於需要精細視覺分析的應用場景具有重要價值。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;3D 空間輸入支持則進一步擴展了模型的應用範圍，為虛擬現實、增強現實、3D 建模等領域的 AI 應用提供了強有力的技術支撐。結合視頻處理能力，該模型有望在智能監控、視頻分析、內容創作等多個行業發揮重要作用。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365893</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365893</guid>
      <pubDate>Wed, 13 Aug 2025 02:27:40 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>崑崙萬維開源 Skywork UniPic 2.0</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"&gt;崑崙萬維&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FCN8NaVnxsqxFe6WF_eo2iA" target="_blank"&gt;宣佈&lt;/a&gt;正式開源 Skywork UniPic 2.0 模型——面向統一多模態建模的高效訓練和推理框架，圍繞生成和編輯模塊輕量化、連接多模態理解模型進行聯合訓練，構建了理解、生圖、編輯一體化的核心能力，旨在實現「高效、高質、統一」的多模態生成模型。&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"&gt;目前，Skywork UniPic 2.0 及其系列模型已全面開源，涵蓋模型權重、推理代碼、強化策略等。&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"&gt;Skywork UniPic 2.0 由三個核心模塊組成：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;生圖編輯（下圖中）：&lt;/strong&gt;基於 SD3.5-Medium 架構將原本只支持文本輸入的模型改進成也接受文本圖像同時輸入，然後通過高質量圖像生成和編輯數據的訓練將原本生圖能力擴展成生圖、編輯雙能力。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;統一模型能力（下圖左側與中間）：&lt;/strong&gt;通過凍結生圖編輯模塊，多模態模型（Qwen2.5-VL-7B），Pre-Train 連接器來構建出理解生成編輯一體化能力，再通過連接器和生圖編輯模塊一起聯合微調，實現最終的一體化理解、生圖、編輯模型。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;生圖編輯後訓練（下圖右）：&lt;/strong&gt;為提升生圖編輯整體性能，設計了基於 Flow-GRPO 的漸進式雙任務強化策略，實現了生成與編輯任務在不互相干擾下的協同優化，在預訓練的基礎上進一步提升了模型性能。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img height="203" src="https://oscimg.oschina.net/oscnet/up-67f45853462faa0c1bce2a535fd0a260701.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"&gt;Skywork UniPic 2.0 的核心優勢包括有：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;生成模塊輕量高效，性能拉滿&amp;nbsp;&lt;/strong&gt;生成模塊基於 2B 參數的 SD3.5-Medium 架構訓練，生圖和編輯指標超越生成模塊具有 7B 參數的 bagel，4B 參數的 OmniGen2，12B 參數的 UniWorld-V1 和 Flux-kontext 模型。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;引入強化學習，效果顯著&amp;nbsp;&lt;/strong&gt;基於 Flow-GRPO 首創漸進式雙任務強化策略，有效提升模型對複雜指令的理解能力與圖像生成和編輯的一致性，兩大任務協同優化、互不幹擾。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;一體化靈活切換，拓展能力強&amp;nbsp;&lt;/strong&gt;將生圖編輯的 Kontext 模型與多模態模型端到端整合，微調輕量連接器，即可快速構建統一理解-生成-編輯模型，並且生圖和編輯的性能進一步提升。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;更多詳情可&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FCN8NaVnxsqxFe6WF_eo2iA" target="_blank"&gt;查看官方公告&lt;/a&gt;。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365887</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365887</guid>
      <pubDate>Wed, 13 Aug 2025 02:07:40 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Fedora 43 獲準支持 Hare 編程語言，默認啓用硬鏈接</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Fedora 工程與指導委員會 (FESCo) 本週批准了即將發佈的 Fedora Linux 43 版本的多項新增功能。其中包括獲準發佈 Hare 軟件包，Hare 是一種新的系統編程語言，旨在簡化、穩定和健壯。&lt;/p&gt; 
&lt;p&gt;Hare 本身仍在開發中，但 FESCo 現已批准將 Hare 工具鏈打包併發布到 Fedora 43 的倉庫中。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0812/191447_Wl7H_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;FESCo 還批准在 Fedora 43 中發佈即將發佈的 PHP 8.4 版本，這並不令人意外。FESCo 還批准棄用 YASM，轉而使用 NASM。YASM 彙編器目前無人維護，而 NASM 的情況也好多了。&lt;/p&gt; 
&lt;p&gt;作為英特爾 oneAPI 線程構建版本 (TBB) 的最新更新，Threaded Building Blocks 2022.2 也已獲批准發佈。FESCo 本週還批准了默認對 Fedora RPM 軟件包中，相同的 /usr 文件進行硬鏈接的提案。&lt;/p&gt; 
&lt;p&gt;有關 Fedora 43 版本中這些新批准更改的更多詳細信息，&lt;u&gt;&lt;em&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flists.fedoraproject.org%2Farchives%2Flist%2Fdevel%40lists.fedoraproject.org%2Fthread%2FMVPWNTBSZUUJINZX6PZQGTYE2BA7NFKL%2F" target="_blank"&gt;請通過此 FESCo 郵件列表帖子獲取&lt;/a&gt;&lt;/em&gt;&lt;/u&gt;，該版本將於今年晚些時候發佈。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365798</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365798</guid>
      <pubDate>Mon, 11 Aug 2025 11:15:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>One Million Screenshots：收集了超過 100 萬張網站截圖的網站</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;「One Million Screenshots」 是一個專門收集網站截圖的網站，聲稱截圖了超過 100 萬個熱門 Web 主頁。此外還提供了搜索相似網站的功能，以及查看網站截圖的歷史變化。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1856" src="https://static.oschina.net/uploads/space/2025/0812/185333_PgpB_2720166.png" width="3360" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;https://onemillionscreenshots.com/&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;沒想到 OSCHINA 也榮幸出鏡了：&lt;em&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fonemillionscreenshots.com%2Foschina.net%2Fscreenshot" target="_blank"&gt;https://onemillionscreenshots.com/oschina.net/screenshot&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="6306" src="https://static.oschina.net/uploads/space/2025/0812/185758_Ydik_2720166.png" width="1604" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;下面是關於該網站的常見問題：&lt;/p&gt; 
&lt;p&gt;&lt;img height="2386" src="https://static.oschina.net/uploads/space/2025/0812/185221_4Hkz_2720166.png" width="1514" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365794</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365794</guid>
      <pubDate>Mon, 11 Aug 2025 11:00:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>人工智能正在降低知識的價值，大學應該重新考慮所教授的內容？</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;生成式人工智能，尤其是大型語言模型（LLM）的興起，正以前所未有的速度改變知識獲取的格局。奧克蘭大學商學院教授帕特里克·多德在《對話》(The Conversation) 上撰文指出，隨着 AI 以低成本、高效率的方式提供知識，大學作為傳統知識來源的價值正在受到挑戰。他認為，大學必須重新審視其核心功能，以適應這個由 AI 驅動的新時代。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;多德教授分析，大學長期以來奉行「知識稀缺」的原則，通過提供獨家課程和學位證書來證明學生獲取知識的能力。然而，AI 技術的進步已大大降低了獲取專業知識的門檻，LLM 不僅能檢索事實，還能進行解釋、翻譯和總結，使得曾經「稀缺」的知識價值大打折扣。這種變化已經在勞動力市場顯現，自 ChatGPT 問世以來，英國入門級職位空缺減少了約三分之一，美國部分州甚至取消了公共部門職位的學位要求。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;然而，多德強調，並非所有知識都同等貶值。雖然基礎知識的價值下降，但&lt;strong&gt;隱性知識&lt;/strong&gt;，如團隊協作、倫理判斷、創造力以及解決複雜問題的能力，仍是 AI 無法取代的稀缺資源。他指出，未來教育的重點應從傳授信息轉向培養這些關鍵的人類技能。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;為應對這一挑戰，多德教授為大學提出了四項轉型建議：&lt;/span&gt;&lt;/p&gt; 
&lt;ol style="margin-left:0; margin-right:0"&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;評估轉型&lt;/strong&gt;：將課堂評估重點從單純的知識記憶轉向&lt;strong&gt;判斷和綜合能力&lt;/strong&gt;的考察。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;體驗式學習&lt;/strong&gt;：投入資源開發導師指導項目、模擬現實場景，並利用 AI 作為工具進行倫理決策研究。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;技能微證書&lt;/strong&gt;：創建針對協作、自主學習和倫理判斷等關鍵能力的微證書。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;深化產學研合作&lt;/strong&gt;：大學提供專業知識，企業提供真實案例，學生則專注於驗證和完善想法，共同培養適應未來市場的複合型人才。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;多德總結道，如果大學想要在未來立於不敗之地，就必須從一個單純的&lt;strong&gt;信息來源&lt;/strong&gt;轉變為一個&lt;strong&gt;判斷力中心&lt;/strong&gt;，教會學生如何與 AI 協同思考，而非與之競爭。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365792</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365792</guid>
      <pubDate>Mon, 11 Aug 2025 10:40:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Syncthing 2.0.0 正式發佈，連續文件同步工具</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;Syncthing&amp;nbsp;是一個免費開源的工具，它能在你的各個網絡計算機間同步文件 / 文件夾，它的同步數據是直接從一個系統中直接傳輸到另一個系統的，並且它是安全且私密的。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#1f2328"&gt;Syncthing 全新 2.0 系列的首發版本已正式推出，一些更新亮點如下：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;數據庫後端從 LevelDB 切換到 SQLite。首次啓動時需要遷移，對於大型系統來説，遷移過程可能會比較耗時。新數據庫更易於理解和維護，且希望其穩定性更高。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;日誌格式已更改為使用結構化日誌條目（一條消息加上多個鍵值對）。此外，現在可以按包控制日誌級別，並在 INFO 和 ERROR 之間新增了 WARNING 日誌級別（此前該級別被稱為 WARNING...）。INFO 級別的日誌內容更加詳細，會顯示 Syncthing 執行的同步操作。新增命令行參數&lt;code&gt;--log-level&lt;/code&gt;可設置所有包的默認日誌級別，&lt;code&gt;STTRACE&lt;/code&gt;環境變量和 GUI&amp;nbsp;也已更新以支持按包設置日誌級別。-&lt;code&gt;--verbose&lt;/code&gt;和 &lt;code&gt;--logflags&lt;/code&gt;命令行選項已被移除，若指定將被忽略。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;已刪除的項目不再永久保存在數據庫中，而是在六個月後被清楚。如果你的用例要求刪除操作在六個月以上後生效，建議將&lt;code&gt;--db-delete-retention-interval&lt;/code&gt;命令行選項或相應的環境變量設置為零，或選擇更長的時間間隔。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;現代化的命令行選項解析。舊的 single-dash long 選項不再支持，例如，&lt;code&gt;-home&lt;/code&gt;必須改為&lt;code&gt;--home&lt;/code&gt;。部分選項已重命名，其他選項則變為子命令。所有服務選項現在也可作為環境變量接受。詳情可參閲&amp;nbsp;&lt;code&gt;syncthing --help&lt;/code&gt;和&lt;code&gt;syncthing serve --help&lt;/code&gt;。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;不再支持對 shifted data 的滾動 hash 檢測，因為這實際上毫無幫助。相反，沒有它，掃描和同步會更快、更高效。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;首次啓動時不再創建「default folder」。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;v2 設備之間現在默認使用多個連接。新的默認值是使用三個連接：一個用於索引元數據，兩個用於數據交換。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;遺憾的是，由於與 SQLite 交叉編譯相關的複雜性，以下平台目前無法在 syncthing.net 和 GitHub 上下載預構建的二進制文件：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;dragonfly/amd64&lt;/li&gt; 
   &lt;li&gt;illumos/amd64 and solaris/amd64&lt;/li&gt; 
   &lt;li&gt;linux/ppc64&lt;/li&gt; 
   &lt;li&gt;netbsd/*&lt;/li&gt; 
   &lt;li&gt;openbsd/386 and openbsd/arm&lt;/li&gt; 
   &lt;li&gt;windows/arm&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;涉及已刪除文件的 conflict 解決處理方式已更改。現在，刪除操作可以作為 conflict 解決的最終結果，從而導致已刪除文件被移動到 conflict copy。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="text-align:start"&gt;&lt;span style="color:#1f2328"&gt;本次更新還提供以下版本：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;APT repository:&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fapt.syncthing.net%2F" target="_blank"&gt;https://apt.syncthing.net/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Docker image:&amp;nbsp;&lt;code&gt;docker.io/syncthing/syncthing:2.0.0&lt;/code&gt;&amp;nbsp;or&amp;nbsp;&lt;code&gt;ghcr.io/syncthing/syncthing:2.0.0&lt;/code&gt;(&lt;code&gt;{docker,ghcr}.io/syncthing/syncthing:2&lt;/code&gt;&amp;nbsp;to follow just the major version)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;更多詳情可查看：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fsyncthing%2Fsyncthing%2Freleases%2Ftag%2Fv2.0.0" target="_blank"&gt;https://github.com/syncthing/syncthing/releases/tag/v2.0.0&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365789/syncthing-2-0-0-released</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365789/syncthing-2-0-0-released</guid>
      <pubDate>Mon, 11 Aug 2025 10:10:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>開源編程字體「Hack」創始人 Christopher Simpkins 去世</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Christopher Eric Simpkins 是知名開源編程字體「Hack」創始人，他於 2025 年 6 月 20 日在新罕布什爾州漢諾威突然&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftypo.social%2F%40Hilary%2F114845913381245488" target="_blank"&gt;去世&lt;/a&gt;，享年 51 歲。&lt;/p&gt; 
&lt;p&gt;&lt;img height="904" src="https://static.oschina.net/uploads/space/2025/0812/175131_lS6n_2720166.png" width="1150" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Christopher Simpkins 訃告頁面&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.legacy.com%2Fus%2Fobituaries%2Fvnews%2Fname%2Fchristopher-simpkins-obituary%3Fid%3D58856786" target="_blank"&gt;顯示&lt;/a&gt;，他&lt;span&gt;在佐治亞理工學院取得計算機博士學位後先在美軍服役，退役又完成醫學訓練成為一名器官移植外科醫生&lt;/span&gt;。他醫術精湛、待人溫和，被譽為「溫柔的巨人」，拯救了許多生命並屢獲教學獎。&lt;/p&gt; 
&lt;p&gt;後來他轉向科技領域，&lt;span&gt;加入 Google Fonts 團隊任&lt;/span&gt;高級用戶體驗項目經理&lt;span&gt;，&lt;/span&gt;專注字體開發&lt;span&gt;，發起 Codeface 項目為開發者整理並推薦高可讀性的編程字體，持續推動開源字體生態。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0812/175331_L0CQ_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;2015 年，&lt;/span&gt;Christopher Simpkins 創造了&lt;span&gt;開源 Hack 字體，這款基於 DejaVu Sans Mono 重新調校的等寬字體迅速成為程序員最喜愛的編輯器字體之一。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;谷歌近期發佈的開源字體&lt;/span&gt;&lt;a href="https://www.oschina.net/news/363609/googlesans-code" target="_blank"&gt;&amp;nbsp;Google Sans Code &lt;/a&gt;正是由&amp;nbsp;Christopher Simpkins 負責主導。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1710" src="https://static.oschina.net/uploads/space/2025/0812/180516_SXlX_2720166.png" width="1686" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365788</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365788</guid>
      <pubDate>Mon, 11 Aug 2025 10:07:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>英偉達推出 Cosmos 與 Nemotron 模型，推動物理 AI 與智能體發展</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblogs.nvidia.cn%2Fblog%2Fnvidia-opens-portals-to-world-of-robotics-with-new-omniverse-libraries-cosmos-physical-ai-models-and-ai-computing-infrastructure%2F" target="_blank"&gt;據英偉達官方消息&lt;/a&gt;，英偉達在技術領域再推新進展。其推出的 NVIDIA Cosmos 平台，整合前沿生成式世界基礎模型（WFM）、先進分詞器、護欄以及高效數據處理和管理工作流，旨在加速物理 AI 開發。該平台的世界基礎模型經 2000 萬小時真實世界數據訓練，能預測和生成虛擬環境未來狀態，助力開發者構建新一代機器人和自動駕駛汽車。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0812/174129_nIuV_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;同時，英偉達宣佈推出 Nemotron 模型系列。Llama Nemotron 基於熱門開源模型 Llama 構建，經剪枝和訓練，在指令遵循等方面表現出色，能為 AI 智能體開發提供優化基礎模組。Cosmos Nemotron 視覺語言模型（VLM）則可助力開發者構建智能體，使其能分析圖像和視頻並做出響應。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-9ddc7846bd0a958a9b0a9772dcf6c6a4e47.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;目前，已有眾多物理 AI 領域的領先者，如機器人公司，以及自動駕駛汽車開發商等開始與 Cosmos 協作，加速模型開發進程。開發者可在 NVIDIA API 目錄預覽相關模型，並從 NGC 目錄和 Hugging Face 下載模型系列與微調框架。&lt;/p&gt; 
&lt;p&gt;https://docs.nvidia.com/cosmos/&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365780</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365780</guid>
      <pubDate>Mon, 11 Aug 2025 09:41:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Altman：計劃在未來 5 個月內將算力集羣擴容一倍</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;OpenAI CEO 薩姆・奧爾特曼（SamAltman）在社交平台發文上表示，鑑於 GPT-5 帶來的需求激增，該公司計劃在未來幾個月的算力優先分配如下：&lt;/span&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;首先確保當前付費版 ChatGPT 用戶的總可用量比 GPT-5 推出前更多。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;其次優先滿足 API 需求，直至達到當前分配的產能和已對客戶做出的承諾。（粗略估計，以現有產能可在當前基礎上再支持約 30% 的新 API 增長。）&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;提升 ChatGPT 免費版的服務質量。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;再優先滿足新的 API 需求。計劃在未來 5 個月內將算力集羣擴容一倍，因此這一情況有望改善。&lt;/span&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="450" src="https://oscimg.oschina.net/oscnet/up-546ae50cc300f2af8894d1b266b905551e4.png" width="300" referrerpolicy="no-referrer"&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365777</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365777</guid>
      <pubDate>Mon, 11 Aug 2025 09:39:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>OpenAI 發佈面向 GPT-5 的 Prompt 指南</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;OpenAI 官方寫的 GPT-5 prompt 指南來了，看看官方是怎麼讓 GPT-5 表現更好的。該指南融匯貫通後，還可用於其他 AI 大模型。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0812/172857_F753_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;1、 明確角色和目標 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;開頭就讓 AI 模型知道它是誰、要做什麼，比如：&lt;/p&gt; 
&lt;p&gt;你是資深前端工程師，請幫我在現有 React 項目裏實現...&lt;/p&gt; 
&lt;p&gt;2、 設定工作方式 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;用分步指令，讓模型按既定節奏走，而非一次性輸出：&lt;/p&gt; 
&lt;p&gt;- 先分析需求和不確定點&lt;br&gt; - 再給執行計劃 &amp;nbsp; &amp;nbsp;&lt;br&gt; - 按計劃分步完成&lt;br&gt; - 每步結束時總結進度&lt;/p&gt; 
&lt;p&gt;3、 控制主動性 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;想要它多動腦，就加：在不確定時自行推斷並執行，完成後再告知用戶。 &amp;nbsp;&lt;br&gt; 想讓它少跑偏，就加：僅按已知信息執行，不額外探索。&lt;/p&gt; 
&lt;p&gt;4、 給出完成標準 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;告訴模型何時算任務完成，比如： &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;當所有代碼改動已在/app/theme 目錄生效，並通過現有測試時，結束任務。&lt;/p&gt; 
&lt;p&gt;5、 嵌入風格與規範 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;在提示裏放工程或寫作規範，讓它自動匹配你的需求： &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;變量用駝峯命名，CSS 類名用 BEM 規範，註釋保持英文簡短描述。&lt;/p&gt; 
&lt;p&gt;6、 善用示例 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;給它 1-2 個高質量示例，讓它照着學，比空口説效果好得多。&lt;/p&gt; 
&lt;p&gt;7、 善用「工具前言」&lt;/p&gt; 
&lt;p&gt;工具前言可以寫：先複述目標，再列計劃，執行時簡短説明當前步驟，最後單獨總結成果。&lt;/p&gt; 
&lt;p&gt;8、 清除歧義 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;檢查提示裏是否有前後矛盾或模糊指令，否則 GPT-5 會花大量精力試圖自圓其説，反而降低效率。&lt;/p&gt; 
&lt;p&gt;記住一個公式：角色+目標+步驟+完成標準+風格+示例，如此 GPT-5 才會既有創造力又不跑偏。&lt;/p&gt; 
&lt;p&gt;這本指南還涵蓋了 API 參數具體怎麼調，感興趣的開發者可以看看。&lt;/p&gt; 
&lt;p&gt;地址：cookbook.openai.com/examples/gpt-5/gpt-5_prompting_guide&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365773/openai-gpt-5-prompting-guide</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365773/openai-gpt-5-prompting-guide</guid>
      <pubDate>Mon, 11 Aug 2025 09:29:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>字節跳動推出視頻字幕無痕擦除方案，基於 DiT 大模型打造</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;字節跳動技術團隊&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FKsl_lF8KNwM0vRtsjzWaBA" target="_blank"&gt;宣佈&lt;/a&gt;推出一項創新技術，基於 DiT 大模型與字體級分割的視頻字幕無痕擦除方案，旨在助力短劇等視頻內容的全球化傳播。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在全球化內容製作中，原始視頻的中文字幕對於海外觀眾而言不僅是無效信息，還嚴重影響觀看體驗。傳統的字幕添加或馬賽克、GAN（生成對抗網絡）等字幕擦除方案，往往導致畫面雜亂、模糊或幀間閃爍，無法徹底解決這一問題。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;火山引擎視頻點播推出的這一方案，通過兩大核心技術突破和強大的工程能力，重新定義了字幕擦除標準，實現了全片真實自然的「無痕擦除」，並支持多字幕框、指定時間段的精準擦除。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="262" src="https://oscimg.oschina.net/oscnet/up-e6a7ee75485165b360e216e57f4f3f2e85f.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;該方案的核心在於兩個技術突破：一是 DiT 視頻字幕擦除模型，二是字體級分割模型。DiT 模型通過強魯棒性預訓練基底、擺脫輔助先驗依賴、兩階段訓練策略提升魯棒性與修復精細度，實現了像素級無痕修復。字體級分割模型則通過精準定位目標區域，實現了從「粗放擦除」到「像素級修復」的轉變，有效避免了傳統塊填充導致的背景模糊或紋理重複問題。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="143" src="https://oscimg.oschina.net/oscnet/up-dc217440e603a0e87eec97675dbaaf606fc.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;火山引擎多媒體實驗室聯合工程團隊構建了兼顧精度與效率的技術體系，經過超萬集視頻數據集驗證，擦除任務成功率達到 100%。創新的視頻分鏡技術結合服務器集羣分佈式計算，顯著提升了視頻處理效率。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;此外，該方案還支持多語言內容流轉，突破了中英文限制，支持多個小語種字幕擦除，為全球內容流轉提供了雙向通道。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365771</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365771</guid>
      <pubDate>Mon, 11 Aug 2025 09:23:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Spring AI 1.0.1 發佈</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Spring AI 1.0.1 現已&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fspring.io%2Fblog%2F2025%2F08%2F08%2Fspring-ai-1" target="_blank"&gt;發佈&lt;/a&gt;，此版本包括&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fspring-projects%2Fspring-ai%2Freleases%2Ftag%2Fv1.0.1" target="_blank"&gt;150 多項變化，&lt;/a&gt;重點關注穩定性、增強功能和文檔改進。&lt;/p&gt; 
&lt;h4&gt;&lt;strong&gt;展望未來：Spring AI 1.1 及未來&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;1.0.1 版本專注於穩定性和錯誤修復，而 Spring AI 團隊正在為 1.1 版本開發新功能。&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fclaude.ai%2Fpublic%2Fartifacts%2Fe211dc9e-249d-425d-abd6-9425b8a2bf16" target="_blank"&gt;2025 年路線圖&lt;/a&gt;提供了關鍵日期，並展示團隊基於全新 Spring Boot 4 基礎的 Spring AI 2.0 的規劃重點。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Spring AI 1.1 的當前重點領域&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;1.1 版本將專注於一系列高影響力的增強功能和有針對性的基礎工作，並明確關注在代碼凍結之前能夠切實完成的工作。&lt;/p&gt; 
&lt;hr&gt; 
&lt;h4&gt;&lt;strong&gt;1. Model Context Protocol (MCP)&amp;nbsp;支持&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;與最新的 MCP Java SDK 版本深度集成，使 Spring AI 與最新的協議和傳輸功能保持一致：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;多協議版本協商&lt;/strong&gt;（2024-11-05 和 2025-03-26）。&lt;/li&gt; 
 &lt;li&gt;通過新的傳輸定製器&lt;strong&gt;實現 OAuth2 安全的 MCP 服務器連接。&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;可流式傳輸的 HTTP 和 WebMVC/HttpServlet 服務器傳輸，用於反應式和 servlet 部署。&lt;/li&gt; 
 &lt;li&gt;使用 JSON Schema 強制執行的&lt;strong&gt;結構化輸出驗證。&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;分頁、保持活動 ping、URI 模板支持&lt;/strong&gt;更豐富的資源交互。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;改進的錯誤處理、日誌記錄和初始化流程&lt;/strong&gt;。&lt;/li&gt; 
 &lt;li&gt;遷移到&amp;nbsp;&lt;strong style="color:#363636"&gt;builder-based APIs&lt;/strong&gt;&lt;span style="background-color:#ffffff; color:#000000"&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;for tools and transport providers&lt;/span&gt;。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;h4&gt;&lt;strong&gt;2. Core Responses API Enhancements&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;擴展 Responses API 以縮小功能差距、改善 provider parity 並引入最新的 SDK 功能：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;及時緩存&lt;/strong&gt;以減少延遲和成本。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;「Thinking」模型支持&lt;/strong&gt;增強推理能力。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;消息批處理&lt;/strong&gt;以實現更高的吞吐量。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;跨提供商的本機 JSON 模式&lt;/strong&gt;和更強大的結構化輸出處理。&lt;/li&gt; 
 &lt;li&gt;在保持統一 API 的同時，為&lt;strong&gt;提供商特定的擴展&lt;/strong&gt;提供 Hook。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Google Vertex AI SDK 更新&lt;/strong&gt;– 升級到最新 SDK 以： 
  &lt;ul&gt; 
   &lt;li&gt;解鎖新發布的 endpoints（包括非聊天 API）。&lt;/li&gt; 
   &lt;li&gt;確保與增強的 Responses API 功能兼容。&lt;/li&gt; 
   &lt;li&gt;帶來安全修復和長期支持。&lt;/li&gt; 
   &lt;li&gt;刷新並擴展 Vertex AI 集成測試。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;h4&gt;&lt;strong&gt;3. Chat Memory&amp;nbsp;改進&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;改進 Spring AI 在生產環境中的內存管理：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;內存壓縮&lt;/strong&gt;來管理 token 預算。&lt;/li&gt; 
 &lt;li&gt;可配置長期對話的保留策略。&lt;/li&gt; 
 &lt;li&gt;改進了自定義內存存儲的集成點。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;h4&gt;&lt;strong&gt;4. 可觀察性和多客戶端配置&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;簡化的可觀察性設置&lt;/strong&gt;，包括更容易與 Langfuse 等工具集成。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;多客戶端配置改進&lt;/strong&gt;，簡化了在同一應用程序中與多個提供商的工作流程。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;h4&gt;&lt;strong&gt;5. Net new areas&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;這些大多是全新的實現，如果時間緊迫，可能超出 1.1 版本範圍，但早期準備工作可能已開始：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Azure OpenAI&lt;/strong&gt;&amp;nbsp;– 新的 SDK 支持。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;向量存儲改進&lt;/strong&gt;，包括混合搜索。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reranking&amp;nbsp;–&amp;nbsp;&lt;/strong&gt;對&amp;nbsp;re-ranker&amp;nbsp;模型提供一流的支持。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enterprise guardrails&amp;nbsp;–&amp;nbsp;&lt;/strong&gt;安全性和合規性功能。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;h4&gt;&lt;strong&gt;6. 可能進入孵化階段的項目&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;MemGPT-style chat memory&amp;nbsp;實現&lt;/strong&gt;。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AgentClient&amp;nbsp;&lt;/strong&gt;用於通過 Spring AI 運行自主 CLI 代理（例如 Claude Code）。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;p&gt;公告表示，項目團隊將繼續調整優先事項，以努力實現&amp;nbsp;9 月 23 日 1.1 版的 code freeze，同時平衡近期交付成果與 Spring AI 2.0 的戰略基礎。&lt;/p&gt; 
&lt;p&gt;更多詳情可&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fspring.io%2Fblog%2F2025%2F08%2F08%2Fspring-ai-1" target="_blank"&gt;查看官方公告&lt;/a&gt;。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365766/spring-ai-1-0-1-released</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365766/spring-ai-1-0-1-released</guid>
      <pubDate>Mon, 11 Aug 2025 09:09:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>谷歌向發現 Chrome 高危漏洞的安全研究員獎勵 25 萬美元</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span&gt;谷歌近日依據漏洞獎勵計劃（VRP）向一名安全研究員&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fissues.chromium.org%2Fissues%2F412578726" target="_blank"&gt;發放 25 萬美元（約合 179.8 萬元人民幣）獎金&lt;/a&gt;，獎勵其發現 Chrome 瀏覽器高危漏洞。 &lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0812/170559_YQ7o_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;該研究員於 4 月 23 日報告了一個「沙盒逃逸」漏洞，編號為 CVE-2025-4609，存在於 Chrome 內核的 IPCZ 通信系統中。攻擊者可通過誘導用戶訪問惡意網站，利用該漏洞突破瀏覽器沙箱限制，實現遠程代碼執行。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;儘管研究員最初將其標記為「中等危害」，但谷歌評估其嚴重性為 S0/S1 級，並列為 P1 優先級修復。 谷歌已於 5 月發佈的 Chrome 更新中修復該漏洞，並在 8 月 12 日公開披露細節。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;根據谷歌「漏洞獵人（&lt;/span&gt;Google Bug Hunters&lt;span&gt;）」計劃，提交包含 RCE 演示的高質量非沙盒進程逃逸或內存損壞漏洞報告，可獲 2.5 萬至 25 萬美元獎勵，此次為頂格獎勵。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365764</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365764</guid>
      <pubDate>Mon, 11 Aug 2025 09:06:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Actual - 個人理財工具</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                        
                                                                                    &lt;p&gt;&lt;span style="background-color:#ffffff; color:#1f2328"&gt;Actual 是一款本地優先的個人理財工具。它 100% 免費開源，使用 NodeJS 編寫，並具備同步功能，方便用戶在不同設備之間輕鬆遷移更改。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img height="271" src="https://static.oschina.net/uploads/space/2025/0806/154746_TajJ_4252687.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

                                                                    &lt;/div&gt;
                                                                </description>
      <link>https://www.oschina.net/p/actual</link>
      <guid isPermaLink="false">https://www.oschina.net/p/actual</guid>
      <pubDate>Mon, 11 Aug 2025 08:53:00 GMT</pubDate>
    </item>
    <item>
      <title>360 智腦推出 Light-IF 系列模型</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;360 智腦團隊&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FnwyQDZxYGFFA5pTmkxn3JQ" target="_blank"&gt;宣佈&lt;/a&gt;推出全新的 Light-IF 系列模型，這一創新框架旨在顯著提升大型語言模型（LLM）在複雜指令遵循方面的能力。隨着人工智能技術的不斷進步，儘管 LLM 在數學、編程等領域已經展現出了卓越的推理能力，但在遵循複雜指令方面仍存在不足。為瞭解決這一問題，360 智腦團隊提出了以預覽-自檢式推理和信息熵控制為核心的 Light-IF 框架。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;Light-IF 框架通過五個關鍵環節來提升模型性能:難度感知指令生成、Zero-RL 強化學習、推理模式提取與過濾、熵保持監督冷啓動、熵自適應正則強化學習。這一框架的提出，旨在破解當前推理模型中存在的「懶惰推理」現象，即模型在思考階段僅複述指令而不主動檢查約束是否被滿足，導致指令執行不準確的問題。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;img height="316" src="https://oscimg.oschina.net/oscnet/up-30ae24d430fc7fd7a393ecaf7c48fbadefc.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;在實驗中，Light-IF 系列模型在 SuperCLUE、IFEval、CFBench 及 IFBench 四個中文和跨語言指令遵循基準上均取得了顯著提升。特別是 32B 版本的 Light-IF-32B，其在 SuperClue 得分達到了 0.575，比下一個最佳模型高出 13.9 個百分點。此外，參數規模僅為 1.7B 的 Light-IF-1.7B 在 SuperClue 和 IFEval 上的表現甚至超過了 Qwen3-235B-A22B 等體量更大的模型。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;360 智腦團隊表示，Light-IF 系列模型的推出，不僅為開源社區提供了一套可復現的完整路線和配套的開源代碼，而且全系模型將陸續開放，供社區使用、對比與復現。同時，訓練中使用的冷啓動數據集也將同步開放。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;此外，360 與 SuperCLUE 聯合推出的中文精確指令遵循測評基準 SuperCLUE-CPIFOpen 也將在 Github 上開放，便於研究者評測模型的中文精確指令遵循能力。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365748</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365748</guid>
      <pubDate>Mon, 11 Aug 2025 08:31:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>MiniMax 發佈全球首個可交易 Agent Remix Marketplace</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;MiniMax 稀宇科技宣佈推出全球首個 Agent Remix Marketplace，並啓動了一項獎金高達 15 萬美金的全球挑戰賽。這一創新平台旨在將個人的想法轉化為商業價值，讓每個人都能成為「個體 GDP 創造者」。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;Agent Remix Marketplace 是一個允許用戶一鍵提效的工具，用戶可以通過點擊「Remix」對已發佈的成熟作品進行再創作，無需從零開始，從而將效率提升 10 倍。此外，用戶還可以通過發佈自己的 Agent 作品至 Gallery 並允許他人 Remix，每次作品被 Remix 都能獲得 100 積分的收益。這不僅是一個創作和分享的平台，也是一個漲粉和建立個人品牌的利器。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="285" src="https://oscimg.oschina.net/oscnet/up-d064cf16f7166c2be8c20d9ed0ee1bc940e.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;MiniMax 強調，這一平台是「Agent 全民經濟」的顛覆性突破，具有四大獨家優勢。用戶可以輕鬆地 Remix 各種模板，如香氛蠟燭電商模板，快速開啓自己的電商創業。此外，用戶還可以定製任何行業或主題的 Daily Newsletter，甚至將 Netflix 和 Bilibili、LinkedIn 和 Tinder 等不同平台的功能進行 Remix，創造出全新的用戶體驗。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在技術層面，MiniMax 在研發過程中注重 Agent 的可靠性，包括上下文壓縮總結、API 信息脫敏引擎以及多 Agent 任務路由等技術，以確保用戶數據的安全和隱私。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;全球挑戰賽面向所有人開放，鼓勵參與者用自己的想法挑戰 15 萬美金的獎池。挑戰賽分為原創和 Remix 雙賽道，無論是原創作品還是基於已發佈作品的二創，都有機會獲獎。參與者無需代碼能力，即可參與這一全球智能普惠的活動。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;體驗地址：&lt;/span&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fminimax-agent-hackathon.space.minimax.io%2F" target="_blank"&gt;https://minimax-agent-hackathon.space.minimax.io/&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365744</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365744</guid>
      <pubDate>Mon, 11 Aug 2025 08:20:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>華為發佈 AI 推理創新技術 UCM：可實現高吞吐、低時延推理體驗，計劃 9 月開源</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbaijiahao.baidu.com%2Fs%3Fid%3D1840229710674780713%26wfr%3Dspider%26for%3Dpc" target="_blank"&gt;根據報道&lt;/a&gt;，華為正式發佈了 AI 推理創新技術 UCM（推理記憶數據管理器）。&lt;/p&gt; 
&lt;p&gt;華為推出的 UCM（推理記憶數據管理器）是一款以 KV Cache 為中心的推理加速套件，融合多類型緩存加速算法工具，通過分級管理推理過程中產生的 KV Cache 記憶數據，擴大推理上下文窗口，實現高吞吐、低時延的推理體驗，，降低每 Token 推理成本。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0812/160552_0ocB_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;此外，華為計劃於 2025 年 9 月正式開源 UCM，屆時將在魔擎社區首發，後續逐步貢獻給業界主流推理引擎社區，並共享給業內所有 Share Everything (共享架構) 存儲廠商和生態夥伴。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365742</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365742</guid>
      <pubDate>Mon, 11 Aug 2025 08:06:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Claude 新增聊天記錄記憶功能</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Anthropic 為其 Claude 聊天機器人推出備受期待的&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Fclaudeai%2Fstatus%2F1954982275453686216" target="_blank"&gt;「記憶」功能&lt;/a&gt;，用戶可讓機器人檢索並參考過往對話內容。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0812/155847_bdCE_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;該功能支持網頁、桌面及移動端，能區分不同項目和工作區。用戶只需在 「個人資料」 的 「設置」 中開啓 「搜索和查看聊天記錄」，即可使用。&lt;/p&gt; 
&lt;p&gt;目前，Claude 的 Max、Team 和 Enterprise 訂閲層級已率先上線，其他套餐將在近期開放。與 ChatGPT 的持續記憶不同，Claude 的記憶功能為被動觸發模式，僅在用戶明確要求時才檢索過往對話，且不會構建用戶畫像。&lt;/p&gt; 
&lt;p&gt;作為 AI 領域的頭部企業，Anthropic 與 OpenAI 競爭激烈，雙方在語音模式、上下文窗口、訂閲服務等方面不斷角力。此次記憶功能的推出，旨在提升用戶黏性和使用時長。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365741</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365741</guid>
      <pubDate>Mon, 11 Aug 2025 07:59:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>微軟為 Excel 加入 AI 公式講解，內聯解釋直達單元格</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span&gt;微軟&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftechcommunity.microsoft.com%2Fblog%2Fexcelblog%2Fexplain-formulas-with-copilot%25E2%2580%2594now-on-the-grid%2F4424028" target="_blank"&gt;宣佈&lt;/a&gt;，其電子表格工具 Excel 迎來一項重要更新：由 Copilot 驅動的&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;strong&gt;「解釋此公式」（Explain Formula）&lt;/strong&gt;&lt;span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;功能正式上線，旨在幫助用戶快速理解複雜公式，顯著提升數據處理效率。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;該功能的&lt;span&gt;最大&lt;/span&gt;亮點在於操作簡便。用戶無需單獨打開聊天面板，只需點擊包含有效公式的單元格，並在旁邊的 Copilot 圖標中選擇「解釋此公式」，即可在單元格內直接獲得內聯解釋。這些解釋基於當前工作表的上下文生成，比傳統網絡搜索更精準、更貼合實際工作場景。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="225" src="https://oscimg.oschina.net/oscnet/up-68c25bd98edf5ade9b15bb52dea74ff751f.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;微軟表示，Copilot 能夠分解並逐步講解各種複雜程度的公式，幫助用戶快速掌握其邏輯。默認情況下，解釋會以內聯形式顯示;若 Copilot 聊天面板已開啓，內容將優先在面板中呈現。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;目前，該功能正分階段向 Windows 版和網頁版 Excel 用戶推送。微軟鼓勵用戶在每次使用後通過點贊或點踩反饋，協助優化 AI 解釋效果。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365740</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365740</guid>
      <pubDate>Mon, 11 Aug 2025 07:54:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
  </channel>
</rss>
