<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>開源中國-最新資訊</title>
        <link>https://www.oschina.net/news/project</link>
        <atom:link href="http://8.134.148.166:30044/oschina/news" rel="self" type="application/rss+xml"></atom:link>
        <description>開源中國-最新資訊 - Powered by RSSHub</description>
        <generator>RSSHub</generator>
        <webMaster>contact@rsshub.app (RSSHub)</webMaster>
        <language>en</language>
        <lastBuildDate>Thu, 02 Jan 2025 12:37:56 GMT</lastBuildDate>
        <ttl>5</ttl>
        <item>
            <title>開源日報 | Qwen-VL 大模型全面降價；華為輪值董事長孟晚舟新年致辭；「技術債務就像是倖存者的戰鬥傷痕」；國產 AI 舞台站滿了「90 後天才」</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;歡迎閲讀 OSCHINA 編輯部出品的開源日報，每天更新一期。&lt;/p&gt; 
&lt;h3&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;strong&gt;# 2024.12.31&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;h2&gt;&lt;span style=&quot;color:#27ae60&quot;&gt;今日要聞&lt;/span&gt;&lt;/h2&gt; 
&lt;h3&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.pingwest.com%2Fw%2F301378&quot; target=&quot;_blank&quot;&gt;IBM 計劃收購 HashiCorp，遭英國反壟斷監管機構審查&lt;/a&gt;&lt;/h3&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;p&gt;據 TechCrunch 報道，英國反壟斷監督機構競爭與市場管理局（CMA）已開始調查 IBM 計劃收購雲軟件廠商 HashiCorp 是否會影響競爭。&lt;/p&gt; 
  &lt;p&gt;CMA 週一表示，它將在 1 月 16 日前邀請有關各方就這一併購發表評論。該監管機構暫定 2 月 25 日為最後期限，以決定是批准該交易還是將其提交進一步審查。&lt;/p&gt; 
  &lt;p&gt;IBM 於今年 4 月宣佈同意以約 64 億美元的價格收購 HashiCorp。如果收購繼續進行，將擴大 IBM 在雲計算和人工智能領域的推進力度，並讓該公司獲得 HashiCorp 約 4400 家客戶的名冊。&lt;/p&gt; 
  &lt;p&gt;CMA 於 8 月通知 HashiCorp 將對合並進行審查。美國聯邦貿易委員會也在調查這一交易。&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;h3&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.pingwest.com%2Fw%2F301401&quot; target=&quot;_blank&quot;&gt;阿里雲再度降價：Qwen-VL 大模型全面降價&lt;/a&gt;&lt;/h3&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;p&gt;阿里雲今天宣佈，Qwen-VL 大模型全面降價。這是阿里雲本年度的第三輪降價。&lt;/p&gt; 
  &lt;p&gt;Qwen-VL-Plus 模型價格直降 81%，輸入價格僅為 0.0015 元/千 tokens，創下全網最低價格；而更高性能的 Qwen-VL-Max 降價至 0.003 元/千 tokens，降幅達到 85%。根據新的定價，1 元錢可以最多處理大約 600 張 720P 圖片，或者 1700 張 480P 圖片。&lt;/p&gt; 
  &lt;p&gt;Qwen-VL 系列大模型是阿里雲推出的多模態大模型，已成為開源社區最受歡迎的模型之一，具備強大的視覺推理能力。該模型不僅能夠識別不同分辨率和長寬比的圖片，還能理解 20 分鐘以上的長視頻，並具備自主操作手機和機器人等智能體的視覺理解能力。&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;h3&gt;&lt;span style=&quot;color:#245bdb&quot;&gt;&lt;a href=&quot;https://www.oschina.net/news/327292&quot;&gt;智譜深度推理模型 GLM-Zero 預覽版上線&lt;/a&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;div&gt; 
   &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;智譜宣佈發佈本年度最後一個模型 GLM-Zero 的初代版本 GLM-Zero-Preview，這是智譜首個基於擴展強化學習技術訓練的推理模型。&lt;/p&gt; 
   &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;根據介紹，GLM-Zero-Preview 是 GLM 家族中專注於增強 AI 推理能力的模型，擅長處理數理邏輯、代碼和需要深度推理的複雜問題。同基座模型相比，GLM-Zero-Preview 在不顯著降低通用任務能力的情況下，在專家任務能力方面的表現大幅提升，其在 AIME 2024、MATH500 和 LiveCodeBench 評測中，效果與 OpenAI o1-preview 相當。&lt;/p&gt; 
   &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;模型表現如下：&lt;/span&gt;&lt;/p&gt; 
   &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;img height=&quot;247&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-4dd02998897ee2c465a04b26d597ad65ae3.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;h3&gt;&lt;span style=&quot;color:#d83931&quot;&gt;&lt;a href=&quot;https://www.oschina.net/news/327323&quot;&gt;Altman 公佈 OpenAI 2025 年將發佈的技術產品&lt;/a&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span&gt;OpenAI 首席執行官薩姆・奧特曼（Sam Altman）發帖公佈了該公司 2025 年即將發佈的技術產品，分別是：&lt;/span&gt;&lt;/p&gt; 
  &lt;ul style=&quot;list-style-type:disc; margin-left:0; margin-right:0&quot;&gt; 
   &lt;li&gt;&lt;span&gt;AGI（通用人工智能）&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span&gt;Agents（智能體）&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span&gt;更好的 GPT-4o 升級版&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span&gt;更好的記憶存儲&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span&gt;更長的上下文窗口&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span&gt;「Grow up mode」（成人模式）&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span&gt;深度研究特色功能&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span&gt;更好的 Sora 以及更好的個性化定製&lt;/span&gt;&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-8b88947df326c888e90ad352129eac9d6b6.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;h3&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.pingwest.com%2Fw%2F301379&quot; target=&quot;_blank&quot;&gt;華為輪值董事長孟晚舟新年致辭：2024 年是原生鴻蒙關鍵一年，一年走過其它操作系統十多年發展之路&lt;/a&gt;&lt;/h3&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;p&gt;據華為官網顯示，華為輪值董事長孟晚舟今日發佈新年致辭，對客戶、生態夥伴、產業鏈夥伴、員工和家屬等表達了感謝。&lt;/p&gt; 
  &lt;p&gt;她在致辭中提到，在萬物智聯的賽道上，2024 年是原生鴻蒙的關鍵一年，鴻蒙生態建設千帆起航。鴻蒙千帆計劃得到了眾多行業夥伴的積極響應，短短一年時間，我們就走過其它操作系統十多年的發展之路，創造了「鴻蒙速度」。&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;hr&gt; 
&lt;h2&gt;&lt;span style=&quot;color:#27ae60&quot;&gt;今日觀察&lt;/span&gt;&lt;/h2&gt; 
&lt;div&gt; 
 &lt;h3&gt;&lt;span style=&quot;color:#ffffff&quot;&gt;&lt;span style=&quot;background-color:#ff9900&quot;&gt;社交觀察&lt;/span&gt;&lt;/span&gt;&lt;/h3&gt; 
 &lt;div&gt; 
  &lt;h4&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fweibo.com%2F3146485692%2FP6Fdbs7pg&quot; target=&quot;_blank&quot;&gt;「全球互聯網上中文內容比例很低」是一個誤讀&lt;/a&gt;&lt;/h4&gt; 
  &lt;p&gt;有人用圖一來説明全球互聯網上中文內容比例很低，只佔 1.4%，實際上這是一個誤讀。我以前説過一次，這個數據統計方法並不是計算文字量或者網頁數量，而是計算使用某種語言的網站數量。&lt;/p&gt; 
  &lt;p&gt;舉個例子，微博網站在這個統計中，只能將樣本數字+1，別管微博上邊有多少中文內容，在這個統計方法中，微博跟萬年沒人看的某些個人站沒有區別，都只算一個網站。同樣是 W3Techs 提供的數據，圖二就很能解釋這個問題，只是中文網站數量少，並不是中文內容少。&lt;/p&gt; 
  &lt;div&gt; 
   &lt;div&gt;
    &lt;img alt=&quot;&quot; height=&quot;766&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-482daac5671878f6a85c04be877532dd134.png&quot; width=&quot;400&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    &lt;img alt=&quot;&quot; height=&quot;463&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-6f886a14ad542db607c9f12aece90f15540.png&quot; width=&quot;400&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
   &lt;/div&gt; 
   &lt;div&gt;
    &amp;nbsp;
   &lt;/div&gt; 
  &lt;/div&gt; 
  &lt;div&gt; 
   &lt;div style=&quot;text-align:right&quot;&gt;
    &lt;span style=&quot;color:#000000&quot;&gt;- 微博&amp;nbsp;&lt;strong&gt;BugOS 技術組&lt;/strong&gt;&lt;/span&gt;
   &lt;/div&gt; 
  &lt;/div&gt; 
  &lt;div&gt; 
   &lt;h4&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fweibo.com%2F2194035935%2FP6DrHa8Ob&quot; target=&quot;_blank&quot;&gt;一個大模型需要多大 GPU 內存才能跑起來的計算公式&lt;/a&gt;&lt;/h4&gt; 
   &lt;p&gt;一個大模型需要多大 GPU 內存才能跑起來的計算公式： M = &amp;nbsp;( (P * 4B) / (32 / Q) &amp;nbsp;) * 1.2&lt;/p&gt; 
   &lt;p&gt;M: 所需的 GPU 顯存，單位是 GB。&lt;br&gt; P: 模型的參數數量。例如，7B 模型有 70 億個參數。&lt;br&gt; 4B: 每個參數佔用的字節數，這裏假設每個參數佔用 4 個字節（通常指 FP32 或 Float32 格式）。&lt;br&gt; 32: 4 個字節等於 32 位。&lt;br&gt; Q: 加載模型時使用的位數。例如，16 位 (FP16/BF16)，8 位 (INT8) 或 4 位 (INT4)。這通常稱為量化。&lt;br&gt; 1.2: 表示額外開銷的係數，通常為 20%。這考慮了除了模型權重之外還需要加載到 GPU 顯存中的其他數據，例如優化器狀態、梯度等。&lt;/p&gt; 
   &lt;p&gt;如使用 FP16 量化加載 Llama 70B 模型，計算過程就是&lt;br&gt; M = &amp;nbsp;( (70,000,000,000 * 4) / (32 / 16) &amp;nbsp;)* 1.2 = 168 GB&lt;/p&gt; 
   &lt;p style=&quot;text-align:right&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;- 微博&lt;strong&gt; 蟻工廠&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
   &lt;h4&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fweibo.com%2F2169039837%2FP7e1GcOVX&quot; target=&quot;_blank&quot;&gt;大模型導航資源&lt;/a&gt;&lt;/h4&gt; 
   &lt;p&gt;分享個大模型導航資源，裏面收集了幾乎全部的模型，具有里程碑意義的論文，排行榜，測試集，訓練框架，部署，應用，書籍等&lt;/p&gt; 
   &lt;p&gt;github.com/Hannibal046/Awesome-LLM&lt;/p&gt; 
   &lt;p style=&quot;text-align:right&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;- 微博&lt;strong&gt; karminski-牙醫&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
   &lt;h4&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fweibo.com%2F1706699904%2FP6mWLnFlv&quot; target=&quot;_blank&quot;&gt;英偉達雖然欠下來了大量的「技術債務」，但在他看來「技術債務就像是倖存者的戰鬥傷痕。」&lt;/a&gt;&lt;/h4&gt; 
   &lt;div&gt; 
    &lt;div&gt; 
     &lt;div&gt; 
      &lt;p&gt;關於先做個垃圾出來，讀《英偉達之芯》又看到了一個好例子：&lt;/p&gt; 
      &lt;p&gt;3dfx 破產之後，一個加入英偉達的員工被英偉達的代碼庫震驚到了，「簡直就像是癌症」「代碼寫得一塌糊塗，開發工具鏈也是一團亂麻，最重要的是，他們對此毫不在意」「他們一心只想着下一塊芯片流片，其他什麼都不顧。」&lt;/p&gt; 
      &lt;p&gt;而之前 3dfx 的工作方式則是追求完美，他在那裏寫出的程序優雅，開發的系統條理清晰、註釋詳盡，但結果卻是一敗塗地。&lt;/p&gt; 
      &lt;p&gt;他給的總結相當精闢，英偉達雖然欠下來了大量的「技術債務」，但在他看來「技術債務就像是倖存者的戰鬥傷痕。」&lt;/p&gt; 
     &lt;/div&gt; 
    &lt;/div&gt; 
   &lt;/div&gt; 
   &lt;p style=&quot;text-align:right&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;- 微博&amp;nbsp;&lt;strong&gt;i 陸三金&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
 &lt;h3&gt;&lt;span style=&quot;color:#ffffff&quot;&gt;&lt;strong&gt;&lt;span style=&quot;background-color:#ff9900&quot;&gt;媒體觀察&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
 &lt;h4&gt;&lt;strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbaijiahao.baidu.com%2Fs%3Fid%3D1819939622972077660%26wfr%3Dspider%26for%3Dpc&quot; target=&quot;_blank&quot;&gt;AI 發展：訓練數據即將遭遇瓶頸&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt; 
 &lt;div&gt; 
  &lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:#222222&quot;&gt;訓練數據即將遭遇的瓶頸已悄然浮現。有研究機構預測，到 2028 年左右，用於訓練 AI 模型的數據集典型規模將達到公共在線文本總估計量的規模。換句話説，AI 可能會在大約 4 年內耗盡訓練數據。與此同時，數據所有者（如報紙出版商）開始打擊對其內容的濫用行為，進一步收緊了訪問權限，這將引發「數據共享」規模上的危機。為此，開發人員必須尋找變通之道。&lt;/span&gt;&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;p style=&quot;text-align:right&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;-&lt;/span&gt;&lt;strong&gt;&lt;span style=&quot;color:#000000&quot;&gt;&amp;nbsp;科技日報&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;h4&gt;&lt;strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fwww.ce.cn%2Fcysc%2Ftech%2Fgd2012%2F202412%2F30%2Ft20241230_39251115.shtml&quot; target=&quot;_blank&quot;&gt;全面擁抱人工智能——訪 360 集團創始人周鴻禕&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:#000000&quot;&gt;我國人工智能大模型具有廣闊發展前景，但要在全球大模型產業競爭中贏得主動，一是要充分發揮我國制度優勢，與國外通用大模型展開競爭；二是充分用好我國工業種類齊全、場景眾多的優勢，將大模型和各種應用場景結合，推動一場新型工業革命，這是實現發展「彎道超車」的關鍵。&lt;/span&gt;&lt;/p&gt; 
 &lt;p style=&quot;margin-left:0px; margin-right:0px; text-align:right&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;-&lt;/span&gt;&lt;strong&gt;&lt;span style=&quot;color:#000000&quot;&gt;&amp;nbsp;經濟日報&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;div&gt; 
  &lt;h4&gt;&lt;strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbaijiahao.baidu.com%2Fs%3Fid%3D1819927511172343210%26wfr%3Dspider%26for%3Dpc&quot; target=&quot;_blank&quot;&gt;國產 AI 舞台，站滿了「90 後天才」&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt; 
  &lt;div&gt; 
   &lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:#222222&quot;&gt;從資本到產業對人才的大手筆搶先押注現狀來看，有關 AI 的比拼，無疑不止算力，而更在於人才。&lt;/span&gt;&lt;/p&gt; 
   &lt;p style=&quot;text-align:right&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;-&lt;strong&gt;&amp;nbsp;科創板日報&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
 &lt;h4&gt;&lt;strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbaijiahao.baidu.com%2Fs%3Fid%3D1819926514777138655%26wfr%3Dspider%26for%3Dpc&quot; target=&quot;_blank&quot;&gt;「國產英偉達」們，扎堆上市&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;因此，GPU 企業想要快速發展，必然離不開資本的助力，衝擊上市仍是「國產英偉達」們獲取資金彈藥的重要途徑。&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;而在等待資本市場的大門開啓之前，它們也需要直面生存的考驗。張建中曾直言，「摩爾線程目標為至少先存活 10 年」。在這場「國產替代」光榮而艱辛的征途中，中國算力企業的競逐才剛剛開始。&lt;/span&gt;&lt;/p&gt; 
 &lt;p style=&quot;margin-left:0px; margin-right:0px; text-align:right&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;- &lt;/span&gt;&lt;strong&gt;&lt;span style=&quot;background-color:#ffffff; color:rgba(0, 0, 0, 0.9)&quot;&gt;財經天下 WEEKLY&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;h4&gt;&lt;strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbaijiahao.baidu.com%2Fs%3Fid%3D1819915034550649526%26wfr%3Dspider%26for%3Dpc&quot; target=&quot;_blank&quot;&gt;冷眼與嘲諷之後，谷歌的 AI 大模型翻盤之路&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt; 
 &lt;div&gt; 
  &lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:#262626&quot;&gt;谷歌正在逐漸奪回大模型競賽的行業關注度和開發者認同，反壟斷大錘還尚未真正落下，谷歌獲得了一個難得的發展窗口來在新的技術革新潮流中暫時站穩腳跟，為下一個人工智能時代真正到來前做好準備。&lt;/span&gt;&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;p style=&quot;text-align:right&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;- &lt;/span&gt;&lt;span style=&quot;color:#252525&quot;&gt;&lt;strong&gt;錦緞研究院&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
 &lt;h4&gt;&lt;strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftech.ifeng.com%2Fc%2F8fjJTSFA8ou&quot; target=&quot;_blank&quot;&gt;AI「爆改」快遞行業的第二年&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt; 
 &lt;div&gt; 
  &lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:#222222&quot;&gt;從簡單的寄件、查件入手，到面向快遞小哥打造「知識庫」、再到幫助完成業務信息的彙總整理，甚至到供應鏈的智慧控制，大模型在快遞行業的能力正在被逐步釋放。選擇私有化部署模型、自研大模型的快遞公司們都相信一點：大模型是值得的長期投資，它在快遞行業的應用上限仍然有一個廣闊空間等待發掘。&lt;/span&gt;&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;p style=&quot;margin-left:0px; margin-right:0px; text-align:right&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;-&lt;strong&gt;&amp;nbsp;光錐智能&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
 &lt;hr&gt; 
 &lt;h2&gt;&lt;span style=&quot;color:#27ae60&quot;&gt;&lt;strong&gt;今日推薦&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt; 
 &lt;h3&gt;&lt;span style=&quot;color:#ffffff&quot;&gt;&lt;strong&gt;&lt;span style=&quot;background-color:#ff9900&quot;&gt;開源項目&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
 &lt;h4&gt;&lt;strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fliriliri%2Faya&quot; target=&quot;_blank&quot;&gt;liriliri/aya&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;333&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-3ebd9d1bb174c2b67c92d4e694388364cd8.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fliriliri%2Faya&quot; target=&quot;_blank&quot;&gt;https://github.com/liriliri/aya&lt;/a&gt;&lt;/u&gt;&lt;/em&gt;&lt;/p&gt; 
 &lt;p style=&quot;color:#1f2328; text-align:start&quot;&gt;&lt;span style=&quot;background-color:#ffffff; color:#333333&quot;&gt;AYA 是一款內置 ADB 並基於其功能編寫用戶界面的桌面應用。相比於原始的 ADB 命令行輸入，AYA 安裝傻瓜，功能齊全，全圖形化界面，一鍵操作，極大地提高用戶效率。&lt;/span&gt;&lt;/p&gt; 
 &lt;h3&gt;&lt;span style=&quot;color:#ffffff&quot;&gt;&lt;strong&gt;&lt;span style=&quot;background-color:#ff9900&quot;&gt;每日一博&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
 &lt;h4&gt;&lt;strong&gt;&lt;a href=&quot;https://my.oschina.net/u/4939618/blog/16883119&quot; target=&quot;_blank&quot;&gt;網頁多模態建模思考&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span style=&quot;background-color:#ffffff; color:#333333&quot;&gt;本文從網頁理解業務出發，從多模態信息融合，預訓練任務構建角度，探討通用網頁建模方案。首先，指出網頁的特殊性，即從不同觀察視角下，網頁存在富文本、樹形結構，和圖層堆疊三種形態。在此基礎上，對比了多種多模態融合思路的優缺點，給出一種較好的方案。進一步，提出多粒度、多維度的網頁預訓練方案；最後，探索了大模型時代，利用現有多模態模型，低成本的適配到網頁的一種可行思路。&lt;/span&gt;&lt;/p&gt; 
 &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;img alt=&quot;圖片&quot; height=&quot;173&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-42f7e135775e8f8238f20f5e042a576e488.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
 &lt;hr&gt; 
 &lt;div&gt; 
  &lt;h2&gt;&lt;span style=&quot;color:#27ae60&quot;&gt;&lt;strong&gt;開源之聲&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt; 
 &lt;/div&gt; 
 &lt;h3&gt;&lt;span style=&quot;color:#ffffff&quot;&gt;&lt;strong&gt;&lt;span style=&quot;background-color:#ff9900&quot;&gt;用戶觀點&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
 &lt;h4&gt;&lt;strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FjS984AtnzvXfNwjPVFakZg&quot; target=&quot;_blank&quot;&gt;最強開源終端模擬器 Ghostty 正式發佈 1.0：原生 UI 體驗、採用 Zig 編寫、速度飛快、支持 Mac 和 Linux、支持 GPU 加速&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt; 
 &lt;blockquote&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 1：什麼玩意？不支持 windows？我今晚就去提 issue，炮轟作者&lt;/span&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 2：不至於，README&amp;nbsp;裏有寫是有計劃支持&amp;nbsp;Windows&amp;nbsp;的。終端模擬器不支持&amp;nbsp;Windows&amp;nbsp;是非常常見的情況&lt;/span&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 3：zig&amp;nbsp;比 rust 吹實在&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 4：只要 C&amp;nbsp;ABⅠ在行業上佔大頭，zig 就永遠實在。zig 直接調用 C 真的很爽！&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 5：完全可以理解，等下就去試試。Who&amp;nbsp;care&amp;nbsp;Windows?&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 6：和 Rust 寫的 Warp 比如何？Zig 應用越來越多，好事。&lt;/span&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 7：Warp&amp;nbsp;性能不太行，輸出多了卡，&amp;nbsp;不知道後續的版本會不會優化&lt;/span&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 8：目前在用 wezterm，感覺真正的 killer&amp;nbsp;feature 是 multiplexing，tmux 快捷鍵記不住。目前看 ghost 沒有 multiplexing，也沒有 tmux&amp;nbsp;integration，期待。&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 9：好吧，我還是用 WinTerm 吧&lt;/span&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 10：你不覺得這玩意反應要慢半拍麼，而且偽開源不讓人放心。&lt;/span&gt; 
      &lt;ul&gt; 
       &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 11：我看不懂源代碼，所以不存在放心與否～&lt;/span&gt;&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 12：不知道跟 wezterm 比起來怎麼樣&lt;/span&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 13：用上了，之前用 wezterm，個人感覺比 wezterm 更簡潔高效。兩個都很好。&lt;/span&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 14：可以替換掉&amp;nbsp;iTerm2 了&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 15：我用 powershell7.5&lt;/span&gt;&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/blockquote&gt; 
 &lt;h4&gt;&lt;strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FLAiX2TcypVQbdG8s9Y2OMA&quot; target=&quot;_blank&quot;&gt;中國 AI 的進步之快，讓美國人開始懷疑現實了&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt; 
 &lt;blockquote&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 1：飄了&lt;/span&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 2：哪來這麼多反思哥反思姐？作為機器學習領域的從業者，國內 ai 領域實際上就是在突飛猛進的發展，海外各類先進模型和理論也至少一半是大陸出海華人的貢獻。現在大環境不好，但不是國內從業者夜以繼日的努力是網民一句話所能掩蓋的！&lt;/span&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 3：當然 DeepSeek 不太一樣的是，它不太缺卡，2021 年就囤了 1 萬張英偉達 A100，那會兒 ChatGPT 還沒影呢，和 Meta 為了元宇宙囤卡卻陰差陽錯的趕上 AI 浪潮很像，DeepSeek 買那麼多卡，是為了做量化交易⋯⋯&lt;/span&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 4：陰差陽錯，就好比你買了一把鍋鏟，本來打算是用來炒菜的，後來發現打老公也挺好使！&lt;/span&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 5：中美 ai 發展的確差不多，期待落地&lt;/span&gt;&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/blockquote&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;em&gt;&lt;strong&gt;---END---&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2024/0912/150800_DfGR_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/327414</link>
            <guid isPermaLink="false">https://www.oschina.net/news/327414</guid>
            <pubDate>Tue, 31 Dec 2024 11:26:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>開源日報 | Top 15 中國互聯網公司首次全部盈利；百度網頁版新增「AI 搜」；DeepSeek V3 架構圖；AI 公司爬蟲無視 robots.txt 協議；2024 年度數據庫回顧</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;歡迎閲讀 OSCHINA 編輯部出品的開源日報，每天更新一期。&lt;/p&gt; 
&lt;h3&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;strong&gt;# 2025.1.2&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;h2&gt;&lt;span style=&quot;color:#27ae60&quot;&gt;今日要聞&lt;/span&gt;&lt;/h2&gt; 
&lt;h3&gt;&lt;span style=&quot;color:#245bdb&quot;&gt;&lt;a href=&quot;https://www.oschina.net/news/327607/xorg-server-2024-gitstats&quot;&gt;X.Org Server 的代碼提交次數創 10 年新高&lt;/a&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.phoronix.com%2Fmisc%2Fxserver-eoy-2024%2Findex.html&quot; target=&quot;_blank&quot;&gt;根據 X.Org Server 的&amp;nbsp;&lt;/a&gt;&lt;span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.phoronix.com%2Fmisc%2Fxserver-eoy-2024%2Findex.html&quot; target=&quot;_blank&quot;&gt;Git 提交記錄&lt;/a&gt;，在剛剛過去的 2024 年，&lt;/span&gt;X.Org Server 的代碼&lt;span&gt;提交次數達到了 2014 年以來的最高峯。雖然提交次數比前幾年多了不少，但這並不意味着&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;X.Org Server&amp;nbsp;&lt;span&gt;的復興，因為 Wayland 仍在 Linux 桌面上佔據主導地位。&lt;/span&gt;&lt;/p&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-d93cec2031e1ac95173fc30bde92b9e8801.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-a555233d997eb149ec68424bd0390abacf2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;據統計，X.Org Server 去年有 708 次提交... 比起 2018 年的 535 次提交（每年 200~300 次）要多得多。但即使是在 2010 年代中期，Wayland 還在積極開發的時候，每年也只有 400~500 次提交... 在 2024 年之前，提交次數最多的是 2014 年，當時有 952 次提交。&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;h3&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FVaWJNoUcZG0KhaMsCC564Q&quot; target=&quot;_blank&quot;&gt;Top 15 中國互聯網公司首次全部盈利&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;據晚點 LatePost 報道，隨着 B 站在 2024 年第三季度實現盈虧平衡，Top 15 中國互聯網公司首次全部盈利。而經營效率本就高的幾個大公司，多家盈利破了紀錄。&lt;/p&gt; 
&lt;p&gt;從該媒體整理的排名來看，2024 年騰訊、字節、阿里、拼多多、美團、SHEIN、網易、京東、攜程、百度、快手、貝殼、滴滴、小紅書、B 站成為 Top 15 中國互聯網公司。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-acddf66870f714eb2f765eb51478780f423.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;span style=&quot;color:#245bdb&quot;&gt;&lt;a href=&quot;https://www.oschina.net/news/327593&quot;&gt;百度網頁版新增「AI 搜」功能，基於文心大模型打造&lt;/a&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;百度近日在百度搜索 Web 端首頁上線了百度「AI 搜」入口，「AI 搜」基於原百度搜索 AI 夥伴改版升級而來，在此前的基礎上做功能升級。&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0102/100753_fgNl_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;據介紹，百度「AI 搜」是基於百度文心大模型打造的桌面端 AI 搜索引擎，目前內容側已經打通百度搜索引擎、百度健康、百度律臨、百度文庫、百度教育等內容生態，可確保搜索結果可靠、權威。&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;目前百度「AI 搜」主要提供包括話題探索、問題解決、決策輔助、知識答疑、主題研究、學習創作等功能，覆蓋文生圖、文生文、邏輯推理、多輪對話、智能摘要、AI 修圖等 AI 技術。&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;此外，百度「AI 搜」也提供了文心智能體入口，在對話框中可通過 @方式與不同智能體進行交互，方便用戶使用和創建智能體。&lt;/p&gt; 
&lt;h3&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2F36kr.com%2Fnewsflashes%2F3105267059887878&quot; target=&quot;_blank&quot;&gt;阿里雲與零一萬物達成戰略合作，成立「產業大模型聯合實驗室」&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;近期，阿里雲和大模型頭部企業零一萬物達成模型平台業務的戰略合作，雙方將成立「產業大模型聯合實驗室」，聯手加速大模型從技術到應用的落地，進一步擴大產業大模型的生態整合。據瞭解， 雙方深度戰略合作的產業大模型聯合實驗室包含技術、業務、人才等板塊，雙方將結合兩個團隊的大模型研發實力，形成從下一代基座模型技術探索到產業落地大模型服務的組合拳，全面通過阿里雲百鍊大模型平台的模型服務層面向市場。&lt;/p&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;h3&gt;&lt;span style=&quot;color:#245bdb&quot;&gt;&lt;a href=&quot;https://www.oschina.net/news/327681/swoole-server-v6-released&quot;&gt;Swoole v6 正式發佈，增加 16 項新功能&lt;/a&gt;&lt;/span&gt;&lt;/h3&gt; 
  &lt;div&gt; 
   &lt;div&gt; 
    &lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:#191b1f&quot;&gt;隨着&lt;/span&gt;&lt;span style=&quot;background-color:#ffffff; color:#191b1f&quot;&gt;&amp;nbsp;&lt;/span&gt;&lt;code&gt;2024&lt;/code&gt;&lt;span style=&quot;background-color:#ffffff; color:#191b1f&quot;&gt;&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;background-color:#ffffff; color:#191b1f&quot;&gt;年的結束，各位&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;code&gt;PHP&lt;/code&gt;&lt;span style=&quot;background-color:#ffffff; color:#191b1f&quot;&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;開發者們所期待的&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;code&gt;Swoole v6&lt;/code&gt;&lt;span style=&quot;background-color:#ffffff; color:#191b1f&quot;&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;正式發佈了。作為我們技術進步的結晶，這一版本不僅整合了過去一年間社區的反饋與需求，還展現了開發團隊的創新與努力。它標誌着&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;code&gt;Swoole&lt;/code&gt;&lt;span style=&quot;background-color:#ffffff; color:#191b1f&quot;&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;在性能優化和功能拓展上的重大突破，為&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;code&gt;PHP&lt;/code&gt;&lt;span style=&quot;background-color:#ffffff; color:#191b1f&quot;&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;開發者提供了更加強大的工具，助力我們在新的一年裏開啓更多無窮的可能。藉此機會，讓我們共同期待&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;code&gt;Swoole v6&lt;/code&gt;&lt;span style=&quot;background-color:#ffffff; color:#191b1f&quot;&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;能夠在實際應用中展現出超乎尋常的能力，推動我們的項目走向更加成功的未來！&lt;/span&gt;&lt;/p&gt; 
   &lt;/div&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;hr&gt; 
&lt;h2&gt;&lt;span style=&quot;color:#27ae60&quot;&gt;今日觀察&lt;/span&gt;&lt;/h2&gt; 
&lt;div&gt; 
 &lt;h3&gt;&lt;span style=&quot;color:#ffffff&quot;&gt;&lt;span style=&quot;background-color:#ff9900&quot;&gt;社交觀察&lt;/span&gt;&lt;/span&gt;&lt;/h3&gt; 
 &lt;div&gt; 
  &lt;h4&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fweibo.com%2F1402400261%2FP7zX8Cpd8&quot; target=&quot;_blank&quot;&gt;AI 時代工程師的核心能力&lt;/a&gt;&lt;/h4&gt; 
  &lt;div&gt; 
   &lt;div&gt; 
    &lt;p&gt;以往我們總認為工程師的核心就是技術實力，但 AI 編程工具的出現正在重塑這個認知。&lt;/p&gt; 
    &lt;p&gt;如果説 Devin、Cursor 和 Windsurf 這樣的 AI 助手是「精通各種編程語言的員工」，那麼工程師更像是一位「技術主管」。&lt;/p&gt; 
    &lt;p&gt;這個轉變意味着什麼？工程師的價值正在向上遷移：&lt;br&gt; - 首先是溝通能力：需要準確地向 AI 描述需求，審核和整合 AI 的輸出&lt;br&gt; - 其次是管理能力：協調多個 AI 工具，把控項目方向和質量&lt;br&gt; - 最後才是技術能力：這成為了確保 AI 輸出質量的基礎保障&lt;/p&gt; 
    &lt;p&gt;説實話，這種變化其實是件好事。它讓工程師從繁瑣的代碼實現中解放出來，轉而專注於更具創造性和戰略性的工作。就像一位優秀的指揮家，重要的不是他能演奏所有樂器，而是懂得如何讓整個樂團奏出最動聽的樂章。&lt;/p&gt; 
    &lt;p&gt;這不是技術能力的貶值，而是能力結構的升級。掌握這三種能力的平衡，才是未來工程師的制勝之道。&lt;/p&gt; 
   &lt;/div&gt; 
  &lt;/div&gt; 
  &lt;div&gt; 
   &lt;div style=&quot;text-align:right&quot;&gt;
    &lt;span style=&quot;color:#000000&quot;&gt;- 微博&amp;nbsp;&lt;strong&gt;愛可可-愛生活&lt;/strong&gt;&lt;/span&gt;
   &lt;/div&gt; 
  &lt;/div&gt; 
  &lt;div&gt; 
   &lt;h4&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fweibo.com%2Fttarticle%2Fp%2Fshow%3Fid%3D2309405118295841046578&quot; target=&quot;_blank&quot;&gt;現在的搜索引擎，快被 AI 垃圾淹成賽博糞坑了&lt;/a&gt;&lt;/h4&gt; 
   &lt;p&gt;昨天新年，白天我想找一張蛇的動漫風格參考圖，來做個新年賀卡。然而就是這麼簡單的一次搜索，讓我真的覺得，現在的互聯網，越來越不對勁了。&lt;/p&gt; 
   &lt;p&gt;我就在搜索引擎搜了一下關鍵詞「蛇，動漫」。第一頁的結果是這樣，那一瞬間，我只覺得寒意從電腦屏幕爬上脊背。&lt;/p&gt; 
   &lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-02ede1cdc5d4e8a418919a6bd5b17552bc5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
   &lt;p style=&quot;text-align:right&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;- 微博&lt;strong&gt; 數字生命卡茲克&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
   &lt;h4&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fweibo.com%2F6045441276%2FP7G62c7XS&quot; target=&quot;_blank&quot;&gt;AI 公司的爬蟲無視 robots.txt 協議&lt;/a&gt;&lt;/h4&gt; 
   &lt;p&gt;去中心化社交網絡項目 Diaspora 的開發者近日透露，其論壇、維基和項目網站的流量中，有 70% 來自 AI 公司的網絡爬蟲。開發者表示，在過去 60 天內，Diaspora 的網絡資產共接收到 1130 萬次請求，平均每秒 2.19 次請求。其中，OpenAI 的 gptbot 爬蟲佔據了 24.6% 的流量，亞馬遜的爬蟲佔 17.1%，Anthropic 和 Meta 的爬蟲分別佔 4.3% 和 2.2%。相比之下，Google 和 Bing 等傳統搜索引擎的爬蟲僅佔 0.14% 的流量。&lt;/p&gt; 
   &lt;p&gt;開發者對此表示強烈不滿，指出這些 AI 公司的爬蟲無視 robots.txt 協議，試圖索引每一個頁面的細微變化，並且頻繁切換 IP 地址，繞過 User Agent 字符串的封鎖。開發者認為，這種行為無異於對互聯網的分佈式拒絕服務（DDoS）攻擊，給服務器帶來了不必要的負擔，並可能影響正常用戶的訪問體驗。&lt;/p&gt; 
   &lt;p style=&quot;text-align:right&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;- 微博&lt;strong&gt; blackorbird&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
   &lt;h4&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fweibo.com%2F2194035935%2FP7FFhCkw5&quot; target=&quot;_blank&quot;&gt;DeepSeek V3 架構圖&lt;/a&gt;&lt;/h4&gt; 
   &lt;div&gt; 
    &lt;div&gt; 
     &lt;div&gt; 
      &lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-afe81d65a30a76bc4c87a0ecb2facf9f8f5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
     &lt;/div&gt; 
    &lt;/div&gt; 
   &lt;/div&gt; 
   &lt;p style=&quot;text-align:right&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;- 微博&amp;nbsp;&lt;strong&gt;蟻工廠&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
   &lt;p&gt;&amp;nbsp;&lt;/p&gt; 
   &lt;h4&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FpJcEo_8xBpCmeNVRSaNLZg&quot; target=&quot;_blank&quot;&gt;Andy Pavlo: 2024 年度數據庫回顧&lt;/a&gt;&lt;/h4&gt; 
   &lt;div&gt; 
    &lt;div&gt; 
     &lt;div&gt; 
      &lt;p&gt;就像突然有人一記「腦瓜沖天炮」般直擊，我又來了！為大家奉上我每年的數據庫大亂鬥總結。沒錯，以前我是在 OtterTune 的博客上寫這些東西，然而公司已經 Game Over（願它安息）。現在我就跑回自己的教授個人博客來搞事。&lt;/p&gt; 
      &lt;p&gt;過去這一年裏發生了不少事，從 10 位數的收購案、廠商到處撒野亂改許可證、再到某位超級有錢的數據庫界八旬老漢為了追求新女神、砸錢拉攏大學橄欖球明星等傳奇故事，好不熱鬧。我答應過我第一任老婆，今年要寫得更專業點。而且聽説有些大學把我每年的總結當作數據庫課的必讀材料。所以今年我得好好斟酌。但話説回來，想想我之前兩年的文風，也就那樣吧。反正咱先試試，看能不能穩住。&lt;/p&gt; 
     &lt;/div&gt; 
    &lt;/div&gt; 
   &lt;/div&gt; 
   &lt;p style=&quot;text-align:right&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;- 微信&amp;nbsp;&lt;strong&gt;非法加馮&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
 &lt;h3&gt;&lt;span style=&quot;color:#ffffff&quot;&gt;&lt;strong&gt;&lt;span style=&quot;background-color:#ff9900&quot;&gt;媒體觀察&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
 &lt;h4&gt;&lt;strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.stcn.com%2Farticle%2Fdetail%2F1476193.html&quot; target=&quot;_blank&quot;&gt;大模型頻繁降價，行業洗牌將加速？&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt; 
 &lt;div&gt; 
  &lt;p&gt;&lt;span style=&quot;color:#141414&quot;&gt;值得關注的是，大模型降價對整個 AI 產業影響深遠。在業內人士看來，大模型持續降價進一步加速行業洗牌，成本優勢低、融資能力弱、規模小的企業或將被淘汰出局。同時，大模型降價的本質是讓利給企業和開發者，更低的成本價格可以真正滿足企業複雜業務場景需求，充分驗證大模型的應用價值，推動企業以更低成本加速業務創新，實現 AI 普惠。&lt;/span&gt;&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;p style=&quot;text-align:right&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;-&lt;/span&gt;&lt;strong&gt;&lt;span style=&quot;color:#000000&quot;&gt;&amp;nbsp;證券日報&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;h4&gt;&lt;strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.163.com%2Fdy%2Farticle%2FJKTM7AHL053179F1.html&quot; target=&quot;_blank&quot;&gt;OpenAI 12 集「發佈會」背後：對中國產業 AI 落地的五大啓示&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#404040&quot;&gt;AI 技術將進一步融入企業的日常工作流程，成為提高生產力和效率的標配。隨着技術的成熟和成本的降低，企業將更加依賴 AI 來優化決策、提升服務質量、增強客戶體驗。AI 技術的無縫集成將使得企業能夠更加靈活地應對市場變化，快速響應客戶需求。&lt;/span&gt;&lt;/p&gt; 
 &lt;p style=&quot;margin-left:0px; margin-right:0px; text-align:right&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;-&lt;/span&gt;&lt;strong&gt;&lt;span style=&quot;color:#000000&quot;&gt;&amp;nbsp;產業家&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;div&gt; 
  &lt;h4&gt;&lt;strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fyovwa6Qckao8JXaaA6ABrQ&quot; target=&quot;_blank&quot;&gt;被 AI 分掉精力的數學天才陶哲軒，論文被拒了&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt; 
  &lt;div&gt; 
   &lt;p&gt;&lt;span&gt;近日，&lt;/span&gt;&lt;span&gt;菲爾茨獎&lt;/span&gt;&lt;span&gt;獲得者、華裔數學家陶哲軒在個人社交平台上講述了其最新論文被投稿期刊拒絕的過程和感受，隨後引發了種種熱議：「審稿人的選擇完全是胡扯。」「如果最優秀、最聰明的數學家之一都能讓一篇論文被拒絕，那麼這幾乎可能發生在任何人身上。」「至少這説明某些系統運行得還算正常（不是僅憑名字就發表任何東西）......」&lt;/span&gt;&lt;/p&gt; 
   &lt;p style=&quot;text-align:right&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;-&lt;strong&gt;&amp;nbsp;AI 前線&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
 &lt;h4&gt;&lt;strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbaijiahao.baidu.com%2Fs%3Fid%3D1820117690620699987%26wfr%3Dspider%26for%3Dpc&quot; target=&quot;_blank&quot;&gt;離開英偉達，融資超 3 億美元，他闖入一條地獄級賽道&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;徐馳很希望看到產業中的人都能賺到錢，但他又擔憂如果僅僅是做一個更便宜的 Meta Ray-Ban 恐怕很難賺到錢。他不希望看到大家都因為熱點突然衝進來，最終折騰一圈發現並不好賺錢，又啪一下走掉了，轉身進入另一個行業。&lt;/span&gt;&lt;/p&gt; 
 &lt;p style=&quot;margin-left:0px; margin-right:0px; text-align:right&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;- &lt;/span&gt;&lt;strong&gt;&lt;span style=&quot;background-color:#ffffff; color:rgba(0, 0, 0, 0.9)&quot;&gt;中國企業家雜誌&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;h4&gt;&lt;strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2F36kr.com%2Fp%2F3103452421983745&quot; target=&quot;_blank&quot;&gt;「中國液晶之父」挑戰半導體&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt; 
 &lt;div&gt; 
  &lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:#262626&quot;&gt;中國半導體領域的綜合性企業北京奕斯偉科技集團 (奕斯偉集團、ESWIN) 力爭推動晶圓部門進行首次公開募股 (IPO)。奕斯偉由被稱為「中國液晶面板之父」的王東昇擔任經營首腦。在中美對立導致芯片進口變得困難的情況下，將加快在國內量產作為半導體基板的晶圓。&lt;/span&gt;&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;p style=&quot;text-align:right&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;- &lt;/span&gt;&lt;span style=&quot;color:#252525&quot;&gt;&lt;strong&gt;日經中文網&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
 &lt;h4&gt;&lt;strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FiEMn1KxTWN3o9YsihhiyKw&quot; target=&quot;_blank&quot;&gt;大模型「六小虎」走向分化&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt; 
 &lt;div&gt; 
  &lt;p&gt;當前大模型應用缺乏新計算設備（即手機、電腦之外的新設備）帶來的新場景，在手機和電腦上的應用場景受限，尚未出現令人眼前一亮的原生或殺手級應用，比如新一代類似微信、拼多多、美團、字節跳動級別的應用。「目前的 AI 應用多是對現有業務的 AI 增強，但未達最理想狀態。」&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;p style=&quot;margin-left:0px; margin-right:0px; text-align:right&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;-&lt;strong&gt;&amp;nbsp;創投日報&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
 &lt;hr&gt; 
 &lt;h2&gt;&lt;span style=&quot;color:#27ae60&quot;&gt;&lt;strong&gt;今日推薦&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt; 
 &lt;h3&gt;&lt;span style=&quot;color:#ffffff&quot;&gt;&lt;strong&gt;&lt;span style=&quot;background-color:#ff9900&quot;&gt;開源項目&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
 &lt;h4&gt;&lt;strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fghostty-org%2Fghostty&quot; target=&quot;_blank&quot;&gt;ghostty-org/ghostty&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;img height=&quot;297&quot; src=&quot;https://static.oschina.net/uploads/space/2024/1230/152151_7zXa_2720166.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fghostty-org%2Fghostty&quot; target=&quot;_blank&quot;&gt;https://github.com/ghostty-org/ghostty&lt;/a&gt;&lt;/u&gt;&lt;/em&gt;&lt;/p&gt; 
 &lt;p style=&quot;color:#1f2328; text-align:start&quot;&gt;&lt;span style=&quot;background-color:#ffffff; color:#333333&quot;&gt;Ghostty 是採用 Zig 語言編寫的開源跨平台終端模擬器，支持 GPU 加速，在 Linux 和 macOS 上都使用了各自平台的 GUI 構建，macOS 是基於 SwiftUI，而 Linux 是基於 GTK —— 暫未支持 Windows。&lt;/span&gt;&lt;/p&gt; 
 &lt;h3&gt;&lt;span style=&quot;color:#ffffff&quot;&gt;&lt;strong&gt;&lt;span style=&quot;background-color:#ff9900&quot;&gt;每日一博&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
 &lt;h4&gt;&lt;strong&gt;&lt;a href=&quot;https://my.oschina.net/u/5783135/blog/16884662&quot; target=&quot;_blank&quot;&gt;盤點這些年搭建器在用戶體驗優化的實踐&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span style=&quot;background-color:#ffffff; color:#333333&quot;&gt;得物 App 中嵌入了大量的前端 Web 頁面用以承接各種靈活多變的業務場景和玩法，但因為眾所周知的原因，Web 應用的用戶體驗是很難與原生應用相比的。然而，隨着搭建器功能的不斷完善，支持的業務場景和組件也越來越多，越來越多的團隊和部門優選使用搭建器搭建會場頁面投放於得物 App 當中，這對搭建器的整體用戶體驗提出了更高的要求。&lt;/span&gt;&lt;/p&gt; 
 &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;img alt=&quot;抖動率.jpg&quot; height=&quot;192&quot; src=&quot;https://oscimg.oschina.net/oscnet//a3dd7d286788bd2832fad8e2865de5bb.jpg&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
 &lt;hr&gt; 
 &lt;div&gt; 
  &lt;h2&gt;&lt;span style=&quot;color:#27ae60&quot;&gt;&lt;strong&gt;開源之聲&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt; 
 &lt;/div&gt; 
 &lt;h3&gt;&lt;span style=&quot;color:#ffffff&quot;&gt;&lt;strong&gt;&lt;span style=&quot;background-color:#ff9900&quot;&gt;用戶觀點&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
 &lt;h4&gt;&lt;strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FWm7gPEplvZ2UNPlXjD3b_Q&quot; target=&quot;_blank&quot;&gt;2024 年系統編程語言調查報告：Rust 穩居榜首、Zig 緊隨其後&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt; 
 &lt;blockquote&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 1：累了，希望明天世界毀滅&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 2：如果整個開發團隊只有我一個人，我毫不猶豫選 rust，但是如果未來會有更多人蔘與到項目中，我毫不猶豫選 java。&lt;/span&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 3：我想不到什麼樣的項目會讓團隊面臨 rust 還是 Java 的選擇&lt;/span&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 4：給 Elixir 一個面子？&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 5：期待 Rust 越來越好!&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 6：rust 居榜首 zig 緊跟其後&amp;nbsp;説明大眾對 rust 不滿意&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 7：go&amp;nbsp;不香嗎&lt;/span&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 8：系統編程語言，不能有 gc&lt;/span&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 9：c3 呢？&lt;/span&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 10：你要這麼説，那還有很多沒列出來的，比如 v，nim 等&lt;/span&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 11：產品多樣化，是最可怕的，明明可以迭代，非要自立門戶哈哈，商業化需求，擾亂視聽，製造門檻&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 12：kotlin&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 13：不要反抗，接受 rust 神教的統治吧，所有人都要向偉大的蟹神膜拜&lt;/span&gt;&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/blockquote&gt; 
 &lt;h4&gt;&lt;strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FPwTw3OlKl4NBdoUWVR8lZQ&quot; target=&quot;_blank&quot;&gt;C 語言就是神&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt; 
 &lt;blockquote&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 1：為什麼程序員總是糾結什麼語言牛逼，什麼技術最新，而不是現在什麼產品需要什麼技術來開發，什麼語言市場需求大……陷入這種無意義的虛無的攀比有什麼用？&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 2：東坡肉是豬，扣肉是豬，咕嚕肉是豬，然而你們這幫廚子仍然不考慮學習養豬&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 3：jdk&amp;nbsp;is&amp;nbsp;cpp&lt;/span&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 4：我至今搞不懂為什麼這麼多人用&amp;nbsp;java，唯一的理由是懶&lt;/span&gt; 
      &lt;ul&gt; 
       &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 5：我至今搞不懂為什麼這麼多人黑 java，唯一的理由是菜&lt;/span&gt;&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 6：頂級程序員在改變世界，高級程序員在改變生活，低級程序員在討論哪個語言天下第一&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 7：Rust:&amp;nbsp;yes,&amp;nbsp;but&amp;nbsp;your&amp;nbsp;destiny&amp;nbsp;is&amp;nbsp;to&amp;nbsp;be&amp;nbsp;rewritten&amp;nbsp;by&amp;nbsp;me.&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 8：不知有木有人説，「C 語言是神，Rust 語言是大神」？感覺似乎全世界都在爭先恐後地把 Rust 當新神捧...&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 9：系統語言之王&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 10：一幫寫 crud 的碼農覺得全世界都是 crud。&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 11：go&amp;nbsp;is&amp;nbsp;go&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 12：幾十年的沉澱，不是説替代就能替代的。&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 13：c&amp;nbsp;is&amp;nbsp;彙編語言&lt;/span&gt;&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/blockquote&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;em&gt;&lt;strong&gt;---END---&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2024/0912/150800_DfGR_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/327696</link>
            <guid isPermaLink="false">https://www.oschina.net/news/327696</guid>
            <pubDate>Tue, 31 Dec 2024 10:41:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>RWKV 生態內容徵集大賽（2025 年）</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;很高興告訴大家：RWKV 社區推出&quot;&lt;strong&gt;RWKV 生態內容徵集大賽&lt;/strong&gt; &quot;，此活動在 &lt;strong&gt;2025 年全年內&lt;/strong&gt;公開徵集 RWKV 相關的內容，包括：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;與 RWKV 相關的論文&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;講解 RWKV 的教程，例如文章、視頻、動畫&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;基於 RWKV 的應用&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;我們會根據&lt;strong&gt;內容的質量、新穎度、與 RWKV 的相關度&lt;/strong&gt;，發放生態獎勵：&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;th&gt;獎項&lt;/th&gt; 
   &lt;th&gt;獎金&lt;/th&gt; 
   &lt;th&gt;參考論文&lt;/th&gt; 
   &lt;th&gt;參考教程&lt;/th&gt; 
   &lt;th&gt;參考應用&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;鉑獎&lt;/td&gt; 
   &lt;td&gt;6888 元&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2406.06973&quot; target=&quot;_blank&quot;&gt;RWKV-CLIP&lt;/a&gt;，&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2406.19369&quot; target=&quot;_blank&quot;&gt;RWKV-SAM&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fpoloclub.github.io%2Ftransformer-explainer%2F&quot; target=&quot;_blank&quot;&gt;鉑金教程參考&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FOpenMOSE%2FRWKV-LM-RLHF&quot; target=&quot;_blank&quot;&gt;RWKV-LM-RLHF&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;金獎&lt;/td&gt; 
   &lt;td&gt;4888 元&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2412.19535&quot; target=&quot;_blank&quot;&gt;StyleRWKV&lt;/a&gt;，&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2412.10856&quot; target=&quot;_blank&quot;&gt;RWKV-edge&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2FProfTomYeh%2Fstatus%2F1839706195508208089&quot; target=&quot;_blank&quot;&gt;金獎教程參考&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FOpenMOSE%2FRWKV-Infer&quot; target=&quot;_blank&quot;&gt;RWKV-Infer&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;銀獎&lt;/td&gt; 
   &lt;td&gt;2888 元&lt;/td&gt; 
   &lt;td&gt;---&lt;/td&gt; 
   &lt;td&gt;---&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fshengxia%2FRWKV_Role_Playing&quot; target=&quot;_blank&quot;&gt;RWKV_Role_Playing&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;銅獎&lt;/td&gt; 
   &lt;td&gt;1888 元&lt;/td&gt; 
   &lt;td&gt;---&lt;/td&gt; 
   &lt;td&gt;---&lt;/td&gt; 
   &lt;td&gt;---&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;鐵獎&lt;/td&gt; 
   &lt;td&gt;888 元&lt;/td&gt; 
   &lt;td&gt;---&lt;/td&gt; 
   &lt;td&gt;---&lt;/td&gt; 
   &lt;td&gt;---&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;除了獲獎作品，其他所有&lt;strong&gt;符合條件的投稿&lt;/strong&gt; 均可獲得 &lt;strong&gt;RWKV 周邊&lt;/strong&gt;一套，包括 RWKV T 恤、帆布袋、徽章、冰箱貼各 1 個。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-77625c3ead2e14d462793268eba2d3cb448.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;投稿規則&lt;/h2&gt; 
&lt;h3&gt;RWKV 論文&lt;/h3&gt; 
&lt;p&gt;我們徵集任何與 RWKV 相關的論文，&lt;strong&gt;無論是誰寫的，無論是否中會，只要內容新穎。&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;RWKV 相關的論文&lt;strong&gt;不限發佈平台&lt;/strong&gt;，支持所有可公開查閲的論文平台。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;RWKV 文章&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;RWKV 相關的文章以&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fzhuanlan.zhihu.com%2Fwrite&quot; target=&quot;_blank&quot;&gt;知乎-文章&lt;/a&gt;為默認發佈平台，允許多平台分發。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;在知乎發佈文章時，請添加 &lt;code&gt;rwkv&lt;/code&gt; 話題：&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-e980b9c31863b214fac301b41edaa9fcd3a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;發佈後，應當可以在&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.zhihu.com%2Ftopic%2F27422569%2Fnewest&quot; target=&quot;_blank&quot;&gt;知乎- RWKV 話題&lt;/a&gt;中查看您的投稿。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;投稿的 RWKV 文章需要滿足以下要求：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;文章正文不少於 300 字，每個章節或操作步驟需要有合理的配圖&lt;/li&gt; 
 &lt;li&gt;文章應有合理的結構，示例結構：準備微調數據 -&amp;gt; 微調的調參等配置過程 -&amp;gt; 遇到的問題和解決方案 -&amp;gt; 微調效果&lt;/li&gt; 
 &lt;li&gt;文章語句清晰易懂，內容新穎&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;RWKV 視頻和動畫&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;RWKV 相關的視頻和動畫以 &lt;strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.bilibili.com%2F&quot; target=&quot;_blank&quot;&gt;bilibili&lt;/a&gt;&lt;/strong&gt; 為默認投稿平台。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;在 Bilibili 發佈 RWKV 視頻或動畫時，需要帶上 &lt;code&gt;RWKV&lt;/code&gt; 標籤。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-e5881d6a993bc1f88f77a388cf6aa70284c.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;投稿的 RWKV 視頻和動畫需要滿足以下條件：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;視頻時長不低於 1 分鐘&lt;/li&gt; 
 &lt;li&gt;視頻畫質不低於 720P&lt;/li&gt; 
 &lt;li&gt;視頻音頻無限制，可以是 AI 或真人配音，如無音頻則需要配字幕&lt;/li&gt; 
 &lt;li&gt;內容新穎&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;RWKV 應用&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;RWKV 應用需要開源發佈，以 &lt;strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2F&quot; target=&quot;_blank&quot;&gt;GitHub&lt;/a&gt;&lt;/strong&gt; 為默認發佈平台。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;在 GitHub 開源發佈 RWKV 應用時，需要在 GitHub 倉庫的設置中加上 &lt;code&gt;rwkv&lt;/code&gt; 標籤。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-1cfbe9e8859dff7e3a4dafc4cffd2ec2bda.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;倉庫添加話題後，應當可以在 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Ftopics%2Frwkv%3Fo%3Ddesc%26s%3Dupdated&quot; target=&quot;_blank&quot;&gt;GitHub-rwkv 話題最新項目&lt;/a&gt;中查看您的 RWKV 應用。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;投稿的 RWKV 應用需要滿足以下條件：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;應用需具有清晰的核心功能，並能夠在真實場景中運行，內容新穎&lt;/li&gt; 
 &lt;li&gt;代碼具備可讀性，包含必要的註釋&lt;/li&gt; 
 &lt;li&gt;README 等文檔中包含&lt;strong&gt;依賴版本&lt;/strong&gt; 和&lt;strong&gt;操作步驟&lt;/strong&gt;等用戶指南&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;活動規則&lt;/h2&gt; 
&lt;h3&gt;活動時間&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;用戶投稿時間：2025 年全年（2025.01.01 ~ 2025.12.31）&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;投稿反饋通道&lt;/h3&gt; 
&lt;p&gt;投稿後，請加入 &lt;strong&gt;RWKV 社區活動&lt;/strong&gt; QQ 羣：858016738 ，聯繫管理員登記投稿。&lt;/p&gt; 
&lt;p&gt;任何關於本活動的疑問，也可以在羣內討論。&lt;/p&gt; 
&lt;h3&gt;評審規則&lt;/h3&gt; 
&lt;p&gt;由彭博等 RWKV 社區核心成員、大模型專家組成評審團，對參賽作品進行評審。&lt;/p&gt; 
&lt;p&gt;評審結果會&lt;strong&gt;在 2025 年每個自然月的下旬公佈&lt;/strong&gt;，作品獎勵會在次月發放。&lt;/p&gt; 
&lt;h3&gt;投稿內容範圍&lt;/h3&gt; 
&lt;p&gt;所有投稿內容需要&lt;strong&gt;與 RWKV 架構或模型相關&lt;/strong&gt;，其中：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;論文&lt;/strong&gt;：基於 RWKV 架構或其變體，在語言、多模態、序列、強化學習等等領域的論文，也包括可解釋性、理論分析、量化壓縮、下游任務等等&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;教程（文章、視頻、動畫等）&lt;/strong&gt;：RWKV 架構解析、代碼解讀、微調案例等教程&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;應用&lt;/strong&gt;：基於 RWKV 架構或模型的應用，例如訓練和推理框架，也包括角色扮演、助手、寫作、遊戲等等具體應用&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;獎品發放方式&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;現金獎勵的幣種為人民幣，以匯款或轉賬方式發出，獎金為含稅金額&lt;/li&gt; 
 &lt;li&gt;RWKV 周邊獎勵為實物，以快遞方式發出&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;p&gt;&lt;strong&gt;本活動最終解釋權歸元始智能所有。&lt;/strong&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/327403</link>
            <guid isPermaLink="false">https://www.oschina.net/news/327403</guid>
            <pubDate>Tue, 31 Dec 2024 09:21:00 GMT</pubDate>
            <author>來源: 投稿</author>
        </item>
        <item>
            <title>盤點這些年搭建器在用戶體驗優化的實踐</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                    
                                                                                                                                                    &lt;h1&gt;一、背景&lt;/h1&gt; 
&lt;p&gt;得物 App 中嵌入了大量的前端 Web 頁面用以承接各種靈活多變的業務場景和玩法，但因為眾所周知的原因，Web 應用的用戶體驗是很難與原生應用相比的。然而，隨着搭建器功能的不斷完善，支持的業務場景和組件也越來越多，越來越多的團隊和部門優選使用搭建器搭建會場頁面投放於得物 App 當中，這對搭建器的整體用戶體驗提出了更高的要求。&lt;/p&gt; 
&lt;p&gt;從我開始接觸搭建器後，看到了很多搭建器項目為了用戶體驗優化所做的一些努力與優秀的解決方案，這些方案在各自的應用場景當中發揮了極其重要的作用。因此，抽時間以前端開發人員的視角梳理了現有的一些優秀方案，一則作為知識沉澱留檔，方便之後查閲，二則也可以給後來者一些參考與借鑑。&lt;/p&gt; 
&lt;h1&gt;二、用戶體驗指標&lt;/h1&gt; 
&lt;p&gt;談到用戶體驗，肯定首先要做的就是梳理衡量/驗收指標以及當前瓶頸，這樣才能做到有的放矢，針對高優的體驗瓶頸進行針對性的優化，以最小的成本換取最大的收穫。&lt;/p&gt; 
&lt;h2&gt;體驗指標統計&lt;/h2&gt; 
&lt;p&gt;説到體驗指標，或許每個公司都有不同的定義與口徑，但無論如何變化，始終離不開以下幾點核心要素：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;用戶可以看見有意義內容的時間（FMP）&lt;/li&gt; 
 &lt;li&gt;核心信息展示時間（LCP）&lt;/li&gt; 
 &lt;li&gt;頁面的抖動頻率與幅度（CLS）&lt;/li&gt; 
 &lt;li&gt;用戶交互流暢度（TTI）&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;結合上述核心要素，在得物中落地時被轉化為以下指標：&lt;/p&gt; 
&lt;h3&gt;秒開率&lt;/h3&gt; 
&lt;p&gt;秒開率是衡量 H5 打開速度的重要指標。在業界，普遍會使用 FMP（全稱 &quot;First Meaningful Paint&quot;，翻譯為&quot;首次有效繪製&quot;）表示頁面的&quot;主要內容&quot;開始出現在屏幕上的時間點, 秒開率基本等同於 FMP。得物的秒開率計算方式為：count_if( webview 啓動時間 + FMP 時間 &amp;lt; 1000) / count(*)&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;業界方案&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;秒開率的統計與上報，繞不開 FMP 指標的計算與統計，我們參考了業界的一些現有方案，並結合業務特點設計更貼合我們業務的 FMP 計算公式。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;第一篇《前端監控實踐------FMP 的智能獲取算法》&lt;/li&gt; 
 &lt;li&gt;第二篇《定位性能指標 FMP》&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;兩個方案大致相同都是基於權重計算出關鍵 dom。通過 mutationobserver 來監聽變化，記錄對應時間；然後在渲染結束後篩選出比較重要的 dom， 再用這些 dom 拿到對應的耗時。&lt;/p&gt; 
&lt;p&gt;他們區別如下：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;第一篇篩選出一批 dom 算平均值，第二篇篩選出權重最大的值。&lt;/li&gt; 
 &lt;li&gt;dom 類型的權重也有細微區別&lt;/li&gt; 
 &lt;li&gt;具體類型資源的計算方式有細微區別&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;dom 類型：svg、canvas、img、video、object、embed&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;const IGNORE_TAG_SET = [&quot;SCRIPT&quot;, &quot;STYLE&quot;, &quot;META&quot;, &quot;HEAD&quot;, &quot;LINK&quot;];
// 如果一個頁面內有一個容器，容器內有多張圖片，圖片的重要應該高於容器, 這個值不宜設置過小，
// 否則會出現大多數場景 body 就是權重最大的元素，而不是裏面的圖片元素。
// 至少應該在 3*3，2*5 這樣的佈局中權重最大的元素為其中最後顯示的圖片，
// 由於存在空隙、文本等其他元素，大致為 40
const TAG_WEIGHT_MAP = {
  SVG: 60,
  IMG: 60,
  CANVAS: 60,
  
  OBJECT: 120,
  EMBED: 120,
  VIDEO: 120
};
// 普通節點權重：1
// 權重計算公示：width*height*weight，有背景圖片的 div 等同於 img 標籤
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;我們的方案&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;我們的方案大致和上文中提到的一致，部分細節做了一些適配和優化。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;文章中提到的計算資源的的方法有兩種: 資源的計算方式：performance timing api dom 變動的計算方式：diff + responseEnd&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;文章中提到的 dom 變動的計算方式有些問題，兩個相加的方式會造成誤差比較大。因此我們選擇使用 performance.mark 來計算，不過隱藏的問題是這個只是資源加載的時間，沒有包含渲染的時間，數值會偏小。&lt;/p&gt; 
&lt;p&gt;由於 cat-design（內部組件 UI 庫）對圖片有 CDN 裁剪優化，我們需要把圖片處理成去掉這些參數後的形式，以免資源名稱不一致。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;監聽 dom 的停止條件，過期時間：超過 10s dom 變化的時間間隔：超過 1s&lt;/li&gt; 
 &lt;li&gt;選取權重排名前三的元素，計算其中 fmp 的最大值，如果出現異常，使用 fcp 兜底。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//6e3a28ad1522cf54f8ce30953a4d5852.jpg&quot; alt=&quot;我們的方案.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;抖動率&lt;/h3&gt; 
&lt;p&gt;抖動率是衡量一個頁面是否穩定的核心指標，如果打開頁面後，頁面上的模塊一直頻繁變換，用戶體驗無疑極差的。因此，我們也得關注頁面的 CLS 指標，防止大範圍頻繁抖動。後續也會對項目中針對頁面抖動的優化做詳細的介紹。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//a3dd7d286788bd2832fad8e2865de5bb.jpg&quot; alt=&quot;抖動率.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;用戶滿意度調查&lt;/h2&gt; 
&lt;p&gt;由於用戶設備所處的環境千奇百怪，可能是設備兼容性問題，也可能是網絡問題，純粹通過數據的統計，總是可能出現一些疏漏，並且缺乏對用戶實際體驗的真實反饋。為了補足這一部分可能缺失的數據，我們在一些用戶訪問頻繁的核心頻道頁面，如：天天領券、瘋狂週末、隨心省，等頁面設置了用戶體驗調查問卷，讓有反饋需求的用戶可以在這邊反饋他們所遇到的體驗問題：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//2491fbebedbf26459905a0b25030ef78.jpg&quot; alt=&quot;用戶滿意度調查.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 通過用戶反饋的一些高頻體驗問題，我們會針對性地進行排查可能導致問題的原因。&lt;/p&gt; 
&lt;p&gt;例如：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;卡頓：可能因為頁面 js 主線程存在耗時長任務導致頁面操作卡頓。&lt;/li&gt; 
 &lt;li&gt;閃退：可能因為頁面邏輯出現死循環或未正常退出的遞歸，導致系統爆棧，內存佔滿，部分設備在這種情況下會直接殺死有問題的進程（webview 實例）以確保其他程序的正常運行。&lt;/li&gt; 
 &lt;li&gt;白屏：可能因為網絡鏈路不通或延遲導致無法正常下載 html 文檔，又或者是核心渲染邏輯因為一些前置 js 的邏輯報錯或資源獲取失敗而沒有正常執行。當然，我們發現，很多時候，用戶反饋的白屏，其實並不是真正的白屏，而是展示了頁面骨架，此時有可能是進行 CSR（客戶端渲染）時數據接口請求異常或邏輯處理異常。&lt;/li&gt; 
 &lt;li&gt;抖動：可能因為 AB 實驗、風控攔截、邏輯隱藏/展示、人羣定投常見下出現人羣躍遷等原因，導致頁面骨架跟實際用戶展示的不一致，骨架缺少某些組件，但用戶展示的時候需要展示，反之亦然。這樣就會導致頁面因組件數量的變化而發生劇烈的抖動，影響用戶的體驗。&lt;/li&gt; 
 &lt;li&gt;手機發熱：可能因為死循環、密集計算等佔用 CPU 資源過高，導致 CPU 發熱嚴重&lt;/li&gt; 
 &lt;li&gt;圖片出不來：可能因為訪問圖片資源的目標 CDN 節點故障，導致訪問異常，也可能因為用戶網絡環境不佳，圖片加載過慢，也可能是因為頁面資源並行下載量過高，導致圖片資源加載延遲等等。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;當然，很多時候，出現這些問題，不一定是代碼實現有問題，有可能確實是用戶的設備老舊，渲染性能和運行內存較低或者是用戶所處的網絡環境不佳（如在電梯中）導致的一些體驗問題。因此用戶的這些體驗調查，僅作為體驗指標統計的補充，我們的優化依然還是主要圍繞着體驗指標數據進行，再輔以用戶反饋高頻問題的排查以達到最真實的用戶體驗優化效果。&lt;/p&gt; 
&lt;h1&gt;三、體驗優化&lt;/h1&gt; 
&lt;p&gt;確定了體驗指標和優化的方向之後，我們再來具體的看一下應該如何針對這些指標進行針對性的優化。&lt;/p&gt; 
&lt;h2&gt;靜態資源優化&lt;/h2&gt; 
&lt;p&gt;在絕大部分性能體驗優化中，靜態資源的優化都是首當其衝的，因為這個優化的效果往往是最為直接的，並且優化起來也是比較容易的，沒有太多的彎彎繞繞，只需要想辦法「降體增速」即可。&lt;/p&gt; 
&lt;h3&gt;文檔類資源&lt;/h3&gt; 
&lt;p&gt;文檔類資源指的是 html、js、css 等文件，這類的文件通常生成之後都是固定的，我們通常可以利用以下方式進行優化：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;【降體】文件體積壓縮&lt;/li&gt; 
 &lt;li&gt;【降體|增速】資源公私分離 （通常公共的文件因業務需求變化的概率較小，沒變化時可以直接訪問瀏覽器緩存中的資源，而私有業務資源則因業務需求變化改變的概率較大，因此將文件進行公私分離有利於更細粒度的利用瀏覽器緩存）&lt;/li&gt; 
 &lt;li&gt;【降體】gzip 壓縮&lt;/li&gt; 
 &lt;li&gt;【增速】瀏覽器的緩存策略&lt;/li&gt; 
 &lt;li&gt;【增速】CDN 加速&lt;/li&gt; 
 &lt;li&gt;【增速】離線訪問（App Cache、PWA 等）&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;除了上述通用優化策略外，我們通常還需要對 html 文件進行進一步的優化，原因主要是：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Html 文件是應用的入口，html 中有足夠多的有效信息能夠降低用戶訪問白屏的時間，優化用戶體驗&lt;/li&gt; 
 &lt;li&gt;現代前端應用大多是 SPA（單頁應用），html 中的有效信息極少&lt;/li&gt; 
 &lt;li&gt;很多頁面的數據需要服務端接口返回數據後才能確定如何展示&lt;/li&gt; 
 &lt;li&gt;有些頁面的數據針對不同人羣展示不同&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;因此，如果我們想要最大限度的利用上 html 文件，那麼就需要解決以下兩個問題：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;提升 html 當中有效信息的佔比&lt;/li&gt; 
 &lt;li&gt;提升 html 訪問返回速度&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;我們針對上述兩個問題逐個分析，逐個解決&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;提升有效信息佔比&lt;/strong&gt; 我們想要提升頁面中有效信息的佔比，可以利用上 SSR（服務端渲染）技術，在返回 html 信息前，現在 node.js 服務端訪問接口，把首屏需要展示的信息獲取回來進行首次預渲染，並獲取首屏展示所需要的 html 文本並塞會返回的 html 文檔當中，使用這種方式，就可以解決 SPA（單頁應用）html 內容有效信息過少的問題。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;當然，我們需要注意，儘可能只是獲取與首屏展示相關的信息，非首屏展示相關的不要再服務端渲染，不然會導致 html 體積增大從而影響資源響應速度。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;提升返回速度&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;使用 SSR 之後，html 的有效信息確實是得到提升了，但 CDN 加速對 SSR 並不友好，CDN 更適合用於緩存加速一些靜態資源，而針對 SSR 這種動態資源有點力不從心。但如果我們想要資源響應速度得到進一步的提升，CDN 又是不可或缺的一環。&lt;/p&gt; 
&lt;p&gt;因此，我們需要更近一步，從 SSR 變為 SSG，從服務端渲染到服務端生成，也就是説，我們在使用 SSR 拿到了首屏渲染的 html 字符串後，不再是直接返回給瀏覽器，而是將其導出成 html 文件，並上傳至 CDN，這樣就能夠充分利用 CDN 的加速能力加速首屏 html 的獲取了。&lt;/p&gt; 
&lt;p&gt;不過我們使用 SSG+CDN 雖然達到了提速的目的，但是有個場景的問題不容忽視：針對不同用戶、人羣有不同展示的個性化組件。由於 CDN 緩存是沒有狀態和身份的，因此，所有用戶訪問的內容都是一樣的，此時我們就沒辦法針對不同的用戶在首屏渲染時展示特異性的數據。&lt;/p&gt; 
&lt;p&gt;基於上述原因，我們決定對組件進行分類：&lt;/p&gt; 
&lt;p&gt;通用骨架屏：針對有實驗、目標人羣、邏輯動態顯示/隱藏的組件，在 SSG 階段時不再直接按照接口返回數據展示，而是展示一個通用的骨架屏，當到了用戶設備瀏覽器中進行客戶端渲染時（此時可以拿到用戶身份），再對骨架進行數據填充完成渲染。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//9d0eec7d760ab2f1d66ea18a49072fc4.jpg&quot; alt=&quot;骨架屏.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;SSR 首屏渲染：針對所有用戶全量展示的組件，我們直接在 SSG 階段就直接用服務端返回的數據渲染首屏頁面結構，由於該組件跟用戶身份無關，因此到了瀏覽器進行客戶端渲染時，服務端返回的數據只會有極其細微的數據差異，只需將部分數據替換即可完成展示。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//33a51a783f88462582fced8e50075f41.jpg&quot; alt=&quot;SSR 首屏渲染.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;圖片類資源&lt;/h3&gt; 
&lt;p&gt;我們上面的用戶滿意度調查當中，有一項是&quot;圖片不出來&quot;，而從收集上來的用戶反饋來説，圖片加載問題其實反饋還是挺頻繁的。再加上我們大部分的組件都需要通過圖片的方式為用戶提供更加豐富的表達，因此，對於圖片類資源的優化也是很有必要的。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//e7cfedf421cf4510e659b49a7f8368f2.jpg&quot; alt=&quot;圖片類資源.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;圖片類資源也屬於靜態資源，因此同樣可以使用上面文檔類資源使用的一些優化方案，如：CDN 加速、緩存策略、圖片壓縮等。除此之外，我們還需要針對圖片資源進行更細粒度的優化。&lt;/p&gt; 
&lt;p&gt;通常我們在開發時，為了確保圖片在高清屏不會模糊，我們下載下來的圖片一般都是多倍圖（搭建器這邊通常用的是 3 倍），但如果在一些非高清屏護着是屏幕分辨率較低的設備上，下載多倍圖無疑是畫蛇添足的，不僅沒能達到更好的展示效果，還可能出現鋸齒，同時使得資源下載時間變得更長，推遲了用戶看到圖片的時間。&lt;/p&gt; 
&lt;p&gt;我們期望的效果是：在瀏覽器請求圖片資源時，需要根據當前設備的分辨率、DPI 等屏幕信息，選擇最優的圖片尺寸和清晰度，從而減少在低端設備圖片下載的體積，提升下載速度，又能確保在高清設備當中能夠展示高清圖。&lt;/p&gt; 
&lt;p&gt;因此，在搭建器當中，我們封裝了一個自定義的 Image 組件，當傳入的圖片是符合預設域名要求時，我們將會給圖片鏈接上加上如下請求參數：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//4f0a27a31e3b0afa6a757fae564d37e2.jpg&quot; alt=&quot;搭建器.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;這個參數是 CDN 服務器為我們提供的將圖片轉換為 webp 格式的參數，當帶有這個參數的圖片請求到服務器後，服務器給我們返回的格式便是 webp。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//073292b793f766b13379cc27a276a716.jpg&quot; alt=&quot;CDN.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;或許有同學會説，webp 好像並不是所有設備都支持吧，那如果在不支持 webp 的設備，圖片不是就展示不了了？&lt;/p&gt; 
&lt;p&gt;確實，因此我們的 Image 組件經過多輪改造以確保圖片在不同設備中均能正常展示：&lt;/p&gt; 
&lt;p&gt;版本 1：&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;&amp;lt;picture&amp;gt;
  &amp;lt;source srcset=&quot;https://h5static.dewucdn.com/node-common/bbdb0b2c-8549-b2cf-ceb8-62b98de2c983-1125-984.jpg?x-oss-process=image/format,webp/resize,w_750&quot; type=&quot;image/webp&quot; /&amp;gt;
  &amp;lt;img src=&quot;https://h5static.dewucdn.com/node-common/bbdb0b2c-8549-b2cf-ceb8-62b98de2c983-1125-984.jpg&quot; alt=&quot;&quot; /&amp;gt;
&amp;lt;/picture&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;我們使用 picture 去加載圖片，如果支持 webp 的設備，就使用 webp，不支持的話，就還是用兜底的原圖。但這個方案在 IOS 設備上會同時加載 webp 和原圖，造成不必要的流量損耗和佔用瀏覽器並行下載數，後來被廢棄。&lt;/p&gt; 
&lt;p&gt;版本 2：&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;try {
  window._promiseimgWebpError = new Promise(function(resolve,reject){
      var img = new Image();
      img.src = &#39;data:image/webp;base64,UklGRkwAAABXRUJQVlA4WAoAAAAQAAAAAAAAAAAAQUxQSAIAAAAAL1ZQOCAkAAAAUAEAnQEqAQABAAFAJiUAToAoAAD+8iJYwmknR5t5G30DAAAA&#39;; // 替換為小 webp 的 base64
      img.onerror = function() {
        resolve(&#39;小 webp 圖片 error&#39;)
      };
  })
} catch (e) {
  console.error(e);
}

// ...

window._promiseimgWebpError.then(() =&amp;gt; {
  const { src, type, options } = this.props;
  this.setState({
    localStr: transformSrc(type, src, options,     , false),
  });
});
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;在這個版本中，我們嘗試在瀏覽器中加載一個很小的 webp 圖片，如果加載失敗，就説明當前設備不支持 webp 圖片，我們就會使用兜底的原始圖片。這種方式的檢測，就不會出現在 IOS 設備同時加載兩種格式圖片的情況，又可以確保在支持 webp 的設備展示 webp ，不支持的設備展示兜底圖。&lt;/p&gt; 
&lt;h2&gt;接口請求效率優化&lt;/h2&gt; 
&lt;p&gt;靜態資源優化後，會場頁面的整體體驗已經得到了極大的提升了，絕大部分情況下用戶訪問頁面時，能夠以最快的速度獲取到 html 文檔和圖片資源。&lt;/p&gt; 
&lt;p&gt;但是，還是有一些情況會導致首屏頁面加載體驗下滑，經過分析，這些體驗下滑的會場有以下特點：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;抖動頻繁：頁面存在眾多組件交付接口的請求，這些請求響應的時間不一，在接口尚未返回時，有些組件處於骨架狀態，返回後又隱藏了，如果多個組件都存在這種情況，就會導致頁面頻繁抖動&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//2ef84e7cf76f9bd07997adace1794afe.jpg&quot; alt=&quot;抖動頻繁.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;接口請求滯後：由於我們訪問一個會場時需要等待文檔下載、html 解析、main.js 執行、組件交付接口等流程，等待組件交付接口返回後，才能真正展示核心信息，這個延遲將近 2s 左右。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;上述兩個問題都出現在「組件交付接口」上：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;組件交付接口請求次數過多（通常與組件的數量是正相關的）&lt;/li&gt; 
 &lt;li&gt;組件交付接口請求時間滯後&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;因此，要解決這兩個問題，搭建器這邊提出了：「接口聚合」、「接口前置」的概念。&lt;/p&gt; 
&lt;h3&gt;接口聚合&lt;/h3&gt; 
&lt;p&gt;接口聚合主要是為瞭解決一個頁面中存在多個依賴組件交付接口的組件時，需要發起多次組件交付接口造成的抖動以及網絡資源的浪費問題。核心的實現思路就是：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//b1c0ec544a8aa865c4cad3981ae8db6a.jpg&quot; alt=&quot;接口聚合.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;接口前置&lt;/h3&gt; 
&lt;p&gt;就如上文所説，瀏覽器請求組件交付接口需要等待：文檔下載、html 解析、main.js 執行、組件交付接口等流程，出現了較長時間的滯後，如果我們可以把這個請求交付接口的階段提前，放到文檔下載之後，無疑是可以讓用戶能夠更快的看到核心內容的。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//a2b05ceb3e9ec0c033aa86112b358dd8.jpg&quot; alt=&quot;接口前置.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;接口預請求&lt;/h3&gt; 
&lt;p&gt;上面兩個接口優化，都是在 h5 層面上的優化，始終還是得經歷「webview 啓動 -&amp;gt; 下載 html」這樣的一個過程，如果 html 體積偏大，那麼這期間也是會產生一定的耗時的。為了在一些特定場景能夠跨越這一個看似無法逾越的天塹。h5 團隊聯合 native 團隊一起，設計了一套 「接口預請求」機制，期望將首屏數據請求進一步的提前，在 native 打開 webview 的同時就並行地發起請求。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//ece2a128c486c09ddd3b9ff74badf735.jpg&quot; alt=&quot;接口預請求.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;有了這樣的預請求機制，我們首屏頁面所依賴的接口數據返回的時間又可以縮短很多，讓我們這些頁面的首屏渲染體驗達到最佳。&lt;/p&gt; 
&lt;p&gt;上圖中提到了一個&quot;競速&quot;機制，即哪個返回比較快就用哪個，但後續數據驗證客戶端請求在 99% 的情況下是快於 h5 的請求的，並且接口競速在會場會有去重問題，因此目前最新的方案是使用的是等待超時走 h5 請求的兜底邏輯。&lt;/p&gt; 
&lt;h2&gt;頁面體驗優化&lt;/h2&gt; 
&lt;p&gt;上面我們分別從資源和接口層面嘗試優化了從用戶請求到實際展示內容的鏈路，讓用戶能夠儘早的看到核心內容。接下來我們再來看一下當頁面到達了瀏覽器進行 CSR（客戶端渲染）後的用戶體驗優化。&lt;/p&gt; 
&lt;h3&gt;SSR 佔位&lt;/h3&gt; 
&lt;p&gt;對於一些跟用戶無關，所有用戶都展示一樣的組件，我們在進行 SSG 生成 html 文件時，實際已經獲得了這些組件的核心數據了，那麼此時用戶一打開網頁，看到的實際上就是我們之前已經獲取好的這些數據展現的組件樣式。這樣一來，用戶一進入頁面，白屏的時間幾乎可以忽略，差不多一進來就可以看到一些內容。只需要等 CSR 的時候接口返回的數據去更新一下一些差異即可，對用戶來説前後的變化比較小，從感官上就像是一打開就看到了實際內容一樣。&lt;/p&gt; 
&lt;h3&gt;骨架屏填充&lt;/h3&gt; 
&lt;p&gt;如果某些組件的展示嚴重依賴於用戶身份的，像上面所説的， CDN 中無法識別用戶身份，此時我們只能展示一個通用的骨架，至少讓用戶知道有這麼一個模塊，並且防止 CSR 後展示了這個模塊後出現較嚴重的頁面抖動。等待 CSR 接口返回之後，我們再去替換這個骨架完成渲染。&lt;/p&gt; 
&lt;h3&gt;組件展示動畫&lt;/h3&gt; 
&lt;p&gt;上面説的 SSR 佔位和骨架屏填充還有一個比較嚴重的體驗問題需要解決：&lt;/p&gt; 
&lt;p&gt;由於在得物 App 中，很多組件都會設置 AB 實驗或者是某些組件只是針對特定人羣展示，如：新客。而在 CDN 中拿到的緩存頁面，實際上是區分不了人羣和用戶身份的，就會導致在 CDN 緩存中的頁面，不知道究竟是否應該展示這個組件，如果展示了，到了客戶端發現當前用戶不應該展示，就會像上述視頻一樣出現剛開始有個模塊，CSR 之後消失的情況。如果不展示，到了客戶端返現當前用戶應該展示時，又會導致憑空多出一個組件把下面的組件直接往下擠的抖動情況。&lt;/p&gt; 
&lt;p&gt;針對這種情況，我們針對這種根據用戶信息判斷是否要展示的組件，在服務端渲染時，都將組件的高度默認設置為 0，等到了客戶端渲染時，如果發現當前組件需要展示，那麼再將這個組件的高度設置為 auto ，而為了讓高度變化時不會突然變化，讓用戶看起來特別奇怪，我們為這個組件的高度變化設置了漸變過渡，讓其逐步展開。就這樣，一個原本看起來是極為生硬，體驗拉胯的頁面，經過改造之後，就變成了好像是精心設計好的動畫一樣，毫無違和感。&lt;/p&gt; 
&lt;h3&gt;流式渲染&lt;/h3&gt; 
&lt;p&gt;經過上面幾輪的優化之後，我們會場頁面的用戶體驗可以説又上了一個台階。當然，我們進行上述優化的過程中，也產生了一些副作用。我們先來看幾張圖：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//c665ada632d495aa0b8a3bd7497ae993.jpg&quot; alt=&quot;流式渲染.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt; &lt;em&gt;CSR 渲染流程&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//ce5e1a5034a29eb1a60cba99570765cb.jpeg&quot; alt=&quot;mm.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt; &lt;em&gt;SSR 渲染流程&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;我們可以看到，從我們將 CSR 渲染首屏換成 SSR 渲染首屏後，TTFB 變得比以前更長了，即在用戶訪問頁面到頁面文檔返回的時間變長了。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;TTFB TTFB 測量的是從用戶或客戶端發出 HTTP 請求到客戶端的瀏覽器接收到頁面的第一個字節的持續時間，由發送 HTTP 請求所花費的時間以及獲取頁面的第一個字節所花費的時間組成。TTFB 用於衡量 Web 服務器或其他網絡資源的響應能力。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;原因是因為我們在 SSR 渲染階段，需要獲取頁面全量組件的數據並將其渲染成 HTML，而每個組件的數據獲取都需要一定的耗時，從而導致我們最終獲取到 HTML 的時間拉長。當然，我們上面説的 SSG + CDN 的方案可以很大程度上緩解用戶可感知的等待時間，但每次 CDN 回源時依然還是需要走 SSR 的流程，TTFB 的變長終歸對用戶體驗有一些影響。&lt;/p&gt; 
&lt;p&gt;恰巧最近比較火的「流式渲染」就能夠解決上述痛點，因此，團隊也嘗試在流式渲染的方向上摸索前進，預計達到的效果：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//76a1dd2c4250f5745fd8af2948ca7d0e.jpg&quot; alt=&quot;TTFB.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;接入流式渲染的頁面，TTFB 將會得到很大的降低，用戶能夠感知的白屏時間也被最大限度的縮短，並且可以利用瀏覽器空閒時間，高效且並行的進行多組件異步加載，哪個組件先加載好久展示哪個，沒有加載好之前，依然可以展示骨架屏兜底展示，防止頁面抖動。&lt;/p&gt; 
&lt;h2&gt;組件異常處理&lt;/h2&gt; 
&lt;p&gt;目前搭建器組件有 100 多個，涉及到的業務領域包括但不限於營銷、交易、增長等多個業務域的 20 餘組件開發者，每個雙週迭代都會有大量的組件業務迭代需求。面對這如此密集的業務迭代以及涉及眾多業務域的影響範圍，倘若組件沒有進行較為完善的容錯機制，其中的某一個組件因為某個版本的改動而出現異常，就極有可能導致該頁面的其他組件也受到影響，最嚴重的可能導致整個頁面白屏。&lt;/p&gt; 
&lt;p&gt;本着「敬畏線上，謹慎編碼」的原則，需要一個比較完善的組件容錯機制和告警機制，一來確保即使某個組件出現嚴重 Bug 時不影響頁面其他組件的正常工作，二來我們可以第一時間感知組件出現的異常，及時排查，修復止損。&lt;/p&gt; 
&lt;h3&gt;組件異常隱藏機制&lt;/h3&gt; 
&lt;p&gt;在搭建器的組件渲染時，為每一個組件的渲染單獨包裹了一個錯誤邊界組件，這個組件將會捕獲當前組件的異常和錯誤，防止該錯誤繼續往上冒泡影響到頁面其他組件。這樣就可以將當前組件的錯誤影響範圍始終都限制在組件範圍內，而不會擴大影響其他組件。&lt;/p&gt; 
&lt;p&gt;而當我們捕獲到異常時，我們會直接隱藏這個組件，這樣就可以避免因出現異常而導致組件渲染混亂而影響用戶的使用。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//3b75ebf6d3b186790be57fba56e3c5e8.jpg&quot; alt=&quot;組件異常隱藏機制.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;組件異常上報機制&lt;/h3&gt; 
&lt;p&gt;在上面捕獲到異常之後，我們會將捕獲到的組件異常上報到監控平台並告警，這樣，一旦正式環境有某些組件因業務迭代改動導致異常時我們可以第一時間感知，並及時處理。&lt;/p&gt; 
&lt;h1&gt;四、體驗劣化管控&lt;/h1&gt; 
&lt;p&gt;至此對於搭建器的用戶體驗優化已經告一段落了。但我們還需要想辦法對後續的業務迭代的體驗劣化進行管控。就算你這一次體驗做得再好，經過幾輪業務迭代之後，可能體驗又大幅下滑了。&lt;/p&gt; 
&lt;p&gt;因此，我們期待通過一些手段來防止前端頁面的體驗劣化。&lt;/p&gt; 
&lt;p&gt;得益於現成的體驗卡口平台：體驗卡口平台&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//55382d588b6c36822f00d84b68708cab.jpg&quot; alt=&quot;體驗卡口.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;我們只需要基於這個平台進行一定的改造和功能新增，就可以對我們關注的體驗指標進行細粒度檢測，如：接口前置、圖片轉 webp、接口響應時間等等。後續我們還會不斷的豐富檢測能力，支持流式檢測、ssr 檢測等等，儘可能通過這個平台的檢測與管控，防止前端頁面體驗下滑。後續也可能做成強卡形式，如果高優體驗問題不解決，禁止上線，以此保障前端頁面的交付質量。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//5fcf631d99238f6def471ec67523e5c0.jpg&quot; alt=&quot;交付質量.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h1&gt;五、優化成果驗收&lt;/h1&gt; 
&lt;p&gt;經歷了上面這些體驗後，是否真的達到了我們的預期呢？我們是不是身處於自身描繪的理想環境當中，而真正的用戶體驗不增反降呢？這一切的一切，都需要用實際的數據説話。&lt;/p&gt; 
&lt;h2&gt;秒開率&lt;/h2&gt; 
&lt;p&gt;首先，從我們的核心體驗指標&quot;秒開率&quot;看一下：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//5f40a55b5f231430abd8b15ffae033b9.jpg&quot; alt=&quot;秒開率.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 從對秒開數據的統計來看，雖然每次版本迭代都有不同程度的上下波動，但整體趨勢上還是穩步提升的，由此也可以看出，我們在用戶體驗上的優化，至少在秒開率上是得到了正向的反饋。&lt;/p&gt; 
&lt;h2&gt;抖動率&lt;/h2&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//0d8f58b7a68ff7a652ae2d980b1a20d2.jpg&quot; alt=&quot;抖動率.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 從抖動率的指標來看，進行優化後項目的穩定率整體長期保持在 99.5% 左右，由此可看出對於頁面抖動相關的優化以及在開發時有意識地避免一些可能出現抖動的技術方案還是頗有成效的。&lt;/p&gt; 
&lt;h2&gt;用戶反饋&lt;/h2&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//83caf3609c32438cc3c611cb8143a75c.jpg&quot; alt=&quot;用戶反饋.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 而收集上來的用戶體驗報告來説，正向反饋還是佔了絕大多數的。由此可見，我們的優化成果，不僅僅是我們單方面的臆想，而是實實在在能讓用戶感受出來的體驗提升。當然，其中仍有一小部分問題反饋，我們也會持續跟進，在業務迭代之餘，逐步優化體驗，力求為用戶提供最佳的使用體驗。&lt;/p&gt; 
&lt;h1&gt;六、結語&lt;/h1&gt; 
&lt;p&gt;至此就算梳理完了當前搭建器及其關聯項目在用戶體驗優化上的一些實踐了。這些實踐大部分都是我加入團隊之前，團隊的其他同學就已經完成的。當然，我也參與了其中一部分功能的開發與優化。&lt;/p&gt; 
&lt;p&gt;總的來説，團隊對於用戶體驗的優化是孜孜不倦的，力求給用戶最好的體驗，促使用戶能夠順利在平台上&quot;得到好物&quot;。&lt;/p&gt; 
&lt;p&gt;文 / 星河&lt;/p&gt; 
&lt;p&gt;關注得物技術，每週、更新技術乾貨&lt;/p&gt; 
&lt;p&gt;要是覺得文章對你有幫助的話，歡迎評論轉發點贊～&lt;/p&gt; 
&lt;p&gt;未經得物技術許可嚴禁轉載，否則依法追究法律責任。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/u/5783135/blog/16884662</link>
            <guid isPermaLink="false">https://my.oschina.net/u/5783135/blog/16884662</guid>
            <pubDate>Tue, 31 Dec 2024 09:18:00 GMT</pubDate>
            <author>原創</author>
        </item>
        <item>
            <title>網頁多模態建模思考</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                    
                                                                                                                                                    &lt;span id=&quot;OSC_h1_1&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;導讀&lt;/h1&gt; 
&lt;p&gt;本文從網頁理解業務出發，從多模態信息融合，預訓練任務構建角度，探討通用網頁建模方案。首先，指出網頁的特殊性，即從不同觀察視角下，網頁存在富文本、樹形結構，和圖層堆疊三種形態。在此基礎上，對比了多種多模態融合思路的優缺點，給出一種較好的方案。進一步，提出多粒度、多維度的網頁預訓練方案；最後，探索了大模型時代，利用現有多模態模型，低成本的適配到網頁的一種可行思路。&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_2&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;01 綜述&lt;/h1&gt; 
&lt;p&gt;網頁本質上是一種超文本，一般由超文本標記語言來定義（例如 HTML）。HTML 是一種基礎技術，常與 CSS、JavaScript 一起被眾多網站用於設計網頁、網頁應用程序以及移動應用程序的用戶界面 。網頁瀏覽器內核通過解釋 HTML 文件，通過視覺引擎將其渲染成可視化網頁。&lt;/p&gt; 
&lt;p&gt;由於 HTML 的複雜性特點，使得網頁體現出多模態性，多粒度性，並且這些模態內部存在複雜的對應關係。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;多模態性&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;所謂多模態性，即從不同視角下，網頁體現出不同的形態。從信息載體角度看，它是文本、圖像、視頻等多媒體元素集合。從視覺層面看，它擁有圖層的概念，是各層圖像堆疊起來形成了一張完整的「圖片」。從底層代碼邏輯看，它是一種特殊的類 XML 語言，定義了一棵具有層次關係的樹（dom-tree）。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;多粒度性&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;所謂多粒度性，即網頁無論從哪種模態看，都是由粒度不等的元素組成的。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;以資訊類網頁舉例，從信息載體模態看，網頁由段落組成，段落又由句子組成，句子由 tokens 組成；&lt;/p&gt; 
 &lt;p&gt;從視覺層面，網頁由不同尺寸的圖層，依次堆疊而成；&lt;/p&gt; 
 &lt;p&gt;從底層代碼邏輯看，html 由不同高度以及大小的子樹構成。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;內在的對齊邏輯&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;多種模態的基本元素之間，存在多對多的對齊關係。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;對於 dom-tree 的結點，對應視覺層面的一個圖層，亦可對應着一個或者多個句子&lt;/p&gt; 
 &lt;p&gt;一個句子，可能對應着一個結點，也可能對應着多個結點&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;一個例子：多模態的網頁表示&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-9a68799619487d4d13e1e65530c502edaaa.png&quot; alt=&quot;圖片&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;△語義：句子{圖像、視頻等可文本化）的集合&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-1c057353f56b7591a592066fd456df918a0.png&quot; alt=&quot;圖片&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;△結構：dom 結點構成的樹&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-de73382e2ab59c30101daf9bbaa02660652.png&quot; alt=&quot;圖片&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;△視覺：圖層的疊加（輪廓圖）&lt;/p&gt; 
&lt;p&gt;由於網頁的多模態性，多粒度性，以及潛在對齊關係的特點，使得對網頁的建模，與對富文本的建模思路有着顯著的不同。如果將網頁作為富文本處理，會丟失大量的信息&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;舉一個簡單的例子，一個文本位於網頁的不同位置（譬如正文區域，推薦區域），它的重要性是完全不一樣的。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;而複雜的業務下游應用，例如網頁質量甄別，網頁結構化分塊等，僅依賴文本的語義信息是遠遠不夠的，需要綜合考慮多模態的信息，以及多模態間的對齊信息。&lt;/p&gt; 
&lt;p&gt;下面，結合業界研究和我們的探索，從多模態信息融合、預訓練方案方面展開。最後，探討 LLM 時代，網頁多模態模型的可能的探索方向。&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_3&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;02 多模態多粒度特徵融合&lt;/h1&gt; 
&lt;p&gt;如何將多粒度，多模態的特徵融合，是一個複雜的問題。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;1. 多粒度信息的表示&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;這個部分，業內解決方案較多。&lt;/p&gt; 
&lt;p&gt;對&lt;strong&gt;語義信息建模&lt;/strong&gt;，基於 hierarchical attention 的有較多方案。一種方式，是通過 bert 等編碼器，輸入 tokens，取 CLS 單元輸出作為句子的向量表示；再通過 transformer 結構，輸入句子向量，計算句子間 attention，得到句子以及篇章的稠密向量表示，用於下游任務。&lt;/p&gt; 
&lt;p&gt;對於&lt;strong&gt;結構建模&lt;/strong&gt;，以 html-dom 為基本單元。通過全連接層，融合 bounding box 座標、webkit 提取的 css style 信息等。&lt;/p&gt; 
&lt;p&gt;對於&lt;strong&gt;視覺建模&lt;/strong&gt;，可以基於 vit，以 patch 為基本單元。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2. 多對多對應關係下，多模態信息融合&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;這部分，相關研究主要有四種方案：頂層融合，底層融合、多模態統一建模以及多模態交叉 attention 的方案。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-800824f2678ef96daec39a396908620c116.png&quot; alt=&quot;圖片&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;底層融合即在輸入層將多模態信息對齊後拼接作為 transformer 層的輸入。這種方案對多任務預訓練方案設計（包括任務類型，多任務 loss 權重設計）要求較高，不利於多種模態的信息的平衡。&lt;/p&gt; 
&lt;p&gt;頂層融合即在頂層獲取一個結點或者語句對應的多模態向量，拼接後用於下游分類或迴歸任務。缺點在於，各模態獨立建模的時候，缺少了相關信息交互，不能充分利用多模態之間的對齊信息。&lt;/p&gt; 
&lt;p&gt;多模態統一建模，以 LayoutV2 為例，將文本、圖片信息通過不同的編碼器定長編碼後，輸入統一的 transformer 中。對於多模態綜合理解任務來説，很難在輸入層顯示的注入多模態的對齊信息；網頁結構的單元向量與語義、視覺（網頁輪廓圖）的單元向量亦很難通過淺層的編碼器投影到相同的語義空間內。&lt;/p&gt; 
&lt;p&gt;通過對比，認為在網頁建模場景下，多模態交叉 attention 是一個較好的方案。具體來説，各模態分域表示，域之間相互獨立；通過多模態交叉 attention 層完成多模態間信息交互。DocFormer 提出的 multi-modal attention，Beit3 提出的 MultiWay Transformer 等本質上均為這種思路。&lt;/p&gt; 
&lt;p&gt;在多對多對齊關係下（即一個模態的基本單元，可能對應另外一個模態多個基本單元）。可使用聚合函數（例如 average-pooling，lstm 等），將對應模態上一層 transformer-encode 輸出的對應單元序列，通過聚合操作後變換為定長向量，輸入到多模態 attention 層計算。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-42f7e135775e8f8238f20f5e042a576e488.png&quot; alt=&quot;圖片&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-d458c22d4438625ff4b8a868228cd254a5b.png&quot; alt=&quot;圖片&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_4&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;03 預訓練任務設計&lt;/h1&gt; 
&lt;p&gt;如何設計針對多個模態，不同粒度的任務，以及如何低成本的構建偽標籤，是訓練網頁基座模型的關鍵。&lt;/p&gt; 
&lt;p&gt;分析業務中下游任務的特點，在預訓練階段設計瞭如下 4 個類別的的預訓練任務。 &lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-4623962da781deb0d521bd0b838298fb8b2.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;對於細粒度的語義預訓練場景，借鑑 token 粒度 MLM 的思路，mask 完整的句子，並且通過 decoder 重建句子。&lt;/p&gt; 
&lt;p&gt;對於粗粒度的篇章預訓練場景，結合搜索點擊日誌，mask 掉 title 後，通過 decoder 去噪重建 title 以及生成用戶點擊的 query。&lt;/p&gt; 
&lt;p&gt;對於細粒度的 html-dom 粒度預訓練場景，通過 html_tag mask/重建，結點亂序重排進行訓練。&lt;/p&gt; 
&lt;p&gt;對於篇章粒度的結構任務，通過 GPT 等生成頁面類型的偽標籤，作為監督信號訓練。&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_5&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;04 展望：LLM 時代的網頁基座模型探索方向&lt;/h1&gt; 
&lt;p&gt;現階段，多模態大模型發展到新的階段，已經可以將圖片（視頻）、文本通過統一的 decoder 模型處理&amp;nbsp;。如何有效利用已有大模型的能力，低成本適配到網頁，是當前研究的熱點和難點。&lt;/p&gt; 
&lt;p&gt;一個樸素的思想，是將整個 html 源碼輸入給大模型，做進一步 postpretrain 使得模型適配網頁。但是，由於網頁源碼的平均長度非常大（根據我們對百度網頁庫的統計，平均源碼長度在 160k），如果再將節點的樣式以 style 標籤形式注入，源碼長度預計會翻數十倍。面向海量網頁計算極難落地。再者，針對網頁場景下做若干輪 post-pretrain 成本亦很高。&lt;/p&gt; 
&lt;p&gt;一個可行思路是，通過 adaptor 網絡，將網頁 html-dom 的結構、位置以及視覺信息變換到已有多模態大模型的空間中，壓縮成若干定長向量表示。&lt;/p&gt; 
&lt;p&gt;通過 adaptor 網絡與 LLM 聯合訓練，調低 LLM 的學習率（儘量不擾動已有 LLM 的參數，保留 LLM 的泛化性）；通過特殊標籤，注入 adaptor 產出的 tokens 向量，讓 LLM 解釋隱式向量代表的含義，訓練 adaptor 網絡。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;例如，構建 prompt：&lt;/p&gt; 
 &lt;p&gt;以下是一個網頁的 dom 結點表示&amp;lt;STRUCT&amp;gt;adaptor_tokens&amp;lt;/STRUCT&amp;gt;，輸出 css 描述文本：style=&quot;xxxxx&quot;&lt;/p&gt; 
 &lt;p&gt;訓練 adator 網絡，使得產出的 tokens 向量能夠與 LLM 的語義空間打平&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;———— END————&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;推薦閲讀&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247603657%26idx%3D1%26sn%3D6ba08a7cf4a124c94c4cd51786217499%26scene%3D21%23wechat_redirect&quot; target=&quot;_blank&quot;&gt;百度垂搜一站式研發平台演進實踐&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247603628%26idx%3D1%26sn%3Df75ddec65ee183dc0c3d48b7e1fdb1ea%26scene%3D21%23wechat_redirect&quot; target=&quot;_blank&quot;&gt;初探圖譜 Embedding 用於異常檢測（一）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247603616%26idx%3D1%26sn%3D6f18533697c0a083f9c58373ac6b1a85%26scene%3D21%23wechat_redirect&quot; target=&quot;_blank&quot;&gt;AIAPI - 轉向 AI 原生檢索&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247603585%26idx%3D1%26sn%3D1ea31a1565c49bc466ddb99b6d9e63d9%26scene%3D21%23wechat_redirect&quot; target=&quot;_blank&quot;&gt;學校新來了一位 AI 作文老師：能看、會評、還教改寫&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247603457%26idx%3D1%26sn%3Db3a0dcf00cb7a38bf62729c279619824%26scene%3D21%23wechat_redirect&quot; target=&quot;_blank&quot;&gt;搞定十萬卡集羣！貧窮限制了我的想象力…&lt;/a&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/u/4939618/blog/16883119</link>
            <guid isPermaLink="false">https://my.oschina.net/u/4939618/blog/16883119</guid>
            <pubDate>Tue, 31 Dec 2024 08:37:00 GMT</pubDate>
            <author>原創</author>
        </item>
        <item>
            <title>OpenAI 未能在 2025 年之前提供其承諾的 opt-out 工具</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;早在 5 月份，OpenAI 就表示正在開發一款工具，讓創作者可以指定他們希望自己的作品如何納入或排除在其 AI 訓練數據中。但 7 個月過去了，這項功能仍未面世。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;OpenAI 當時表示，該工具名為「Media Manager」，可以「識別受版權保護的文本、圖像、音頻和視頻」，以反映創作者「跨多個來源」的偏好。該旨在幫助該公司規避一些抨擊，並避免 OpenAI 免受與知識產權相關的法律挑戰。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;但知情人士告訴 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftechcrunch.com%2F2025%2F01%2F01%2Fopenai-failed-to-deliver-the-opt-out-tool-it-promised-by-2025%2F&quot; target=&quot;_blank&quot;&gt;TechCrunch&lt;/a&gt;，該工具在內部很少被視為 important launch。一位前 OpenAI 員工表示，「我不認為這是一個優先事項。説實話，我不記得有人在開發它」。一位與該公司協調工作的非僱員也告訴 TechCrunch，他們過去曾與 OpenAI 討論過這款工具，但最近沒有任何進展。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;而 OpenAI 法律團隊中一位曾擔任媒體經理的 Fred von Lohmann 則於 10 月轉任兼職顧問。OpenAI 公關部通過電子郵件向 TechCrunch 證實了 Von Lohmann 的調動。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;目前，OpenAI 尚未公佈 Media Manager 的進展情況，該公司公司也錯過了自我設定的「2025 年之前」推出該工具的最後期限。（需要明確的是，「by 2025」可以理解為包括 2025 年，但 TechCrunch 將 OpenAI 的措辭解讀為到 2025 年 1 月 1 日。）&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;339&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-4a108e4f582c99d0d1ba3f125401690907e.png&quot; width=&quot;700&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;事實上，OpenAI 為創作者提供了幾種「opt out」其 AI 訓練的臨時方式。去年 9 月，該公司推出了一個提交表單，允許藝術家標記自己的作品，以便從未來的訓練集中刪除。而且 OpenAI 長期以來一直允許網站管理員阻止其網絡爬蟲程序在其域中抓取數據。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;但創作者批評這些方法雜亂無章，不夠充分。對於書面作品、視頻或錄音，沒有具體的退出機制。而對於圖像的退出表格則要求提交每張要刪除的圖像的副本以及説明，過程相當繁瑣。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Media Manager 則被宣傳為 OpenAI 退出解決方案的徹底改進和擴展。OpenAI 在 5 月份的公告中表示，Media Manager 將使用「尖端機器學習研究」，使創作者和內容所有者能夠「tell [OpenAI] what they own」。OpenAI 聲稱在開發該工具時正在與監管機構合作，並表示希望 Media Manager 能夠「為整個 AI 行業樹立標準」。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;但從那以後，OpenAI 再也沒有公開提及過 Media Manager。一位發言人告訴 TechCrunch，截至 8 月份該工具「仍在開發中」，但沒有回應 12 月中旬的後續評論請求。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;OpenAI 尚未透露 Media Manager 何時推出，甚至沒有透露其將具備哪些功能。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;假設 Media Manager 確實將在某個時候出現，但專家們並不相信它能減輕創作者的擔憂，或者在解決圍繞 AI 和知識產權使用的法律問題方面發揮很大作用。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Stubbs Alderton &amp;amp; Markiles 的知識產權律師 Adrian Cyhan 指出，Media Manager 是一項雄心勃勃的事業。即使是像 YouTube 和 TikTok 這樣的大型平台也難以，大規模實現內容識別。OpenAI 真的能做得更好嗎？&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;「確保遵守法律要求的創作者保護和潛在的補償要求帶來了挑戰，尤其是考慮到國家和地方司法管轄區內法律環境的快速發展和潛在差異。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;非營利組織 Fairly Trained 的創始人 Ed Newton-Rex 則認為，Media Manager 會不公平地將控制 AI 訓練的負擔轉嫁給創作者；如果不使用這項技術，創作者可能會默許他們的作品被使用。「大多數創作者甚至都不會聽説過它，更不用説使用它了。但它仍然會被用來保護創作作品免受創作者意願的大規模利用。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;MBHB 人工智能實踐小組聯合主席 Mike Borella 指出，opt-out 系統並不總是考慮到對作品可能進行的轉換，例如對圖像進行 downsampled。Pryor Cashman 的知識產權和媒體律師 Joshua Weigensberg 補充説，它們也可能無法解決第三方平台託管創作者內容副本這一常見情況。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;「創作者和版權所有者無法控制，甚至通常不知道他們的作品在互聯網上出現在哪裏。即使創作者告訴每一個 AI 平台他們選擇退出訓練，這些公司仍可能會繼續使用第三方網站和服務上提供的作品副本進行訓練。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;至少從法理學的角度來看，Media Manager 對 OpenAI 來説可能並不是特別有利。Dorsey &amp;amp; Whitney 專門從事版權法的合夥人 Evan Everist 表示，雖然 OpenAI 可以使用該工具向法官證明其正在減輕對受知識產權保護的內容的訓練，但如果發現公司侵權，Media Manager 可能不會保護該公司免受損害。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;「版權所有者沒有義務在侵權行為發生之前預先告知他人不要侵犯其作品。版權法的基本原則仍然適用——即未經許可不得盜用和複製他人的作品。此功能可能更多地與公關有關，並將 OpenAI 定位為內容的道德用戶。」&lt;/span&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;&lt;span&gt;更多獨家技術見解與熱門話題討論，盡在&lt;a href=&quot;https://www.oschina.net/app&quot;&gt;【開源中國 APP】&lt;/a&gt;，與數百萬開發者一起，隨時隨地探索技術無限可能。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/327671/openai-failed-deliver-opt-out-tool-by-2025</link>
            <guid isPermaLink="false">https://www.oschina.net/news/327671/openai-failed-deliver-opt-out-tool-by-2025</guid>
            <pubDate>Tue, 31 Dec 2024 08:07:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>「AI 為伍，重啓征程」2024 OSC 源創會年終盛典在珠海圓滿落幕</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                    
                                                                                                                                                    &lt;div&gt;
  12 月 28 日， 
 &lt;strong&gt;「&lt;/strong&gt; 
 &lt;strong&gt;AI&lt;/strong&gt; 
 &lt;strong&gt; 為伍，重啓征程」2024 OSC 源創會年終盛典&lt;/strong&gt;在珠海嘉遠世紀酒店圓滿落下帷幕。本次活動由開源中國、Gitee 主辦，華為聯合主辦，珠海市香洲區科技和工業信息化局、廣東省科學院珠海產業技術研究院、珠海市軟件行業協會、珠海市科技發展促進會、澳門亞太 IT 協會提供支持。 
&lt;/div&gt; 
&lt;div&gt;
  &amp;nbsp; 
&lt;/div&gt; 
&lt;div&gt;
  本次活動秉承「自由、開放、分享」的宗旨，自開啓報名後就受到了全國各地開發者和 IT 企業的關注，吸引到行業內的頂尖專家、技術領袖和一線開發者積極報名， 
 &lt;strong&gt;現場觀眾達 400&lt;/strong&gt; 
 &lt;strong&gt;餘人，會場座無虛席，參會人數再創新高。&lt;/strong&gt; 
&lt;/div&gt; 
&lt;div&gt;
  &amp;nbsp; 
&lt;/div&gt; 
&lt;div&gt;
  2024 年，源創會走過不同城市，舉辦了 8 場城市沙龍，1 場年終盛典，匯聚上千位開發者、近 70 位優秀講師。與此同時，開源中國和 Gitee AI 社區生態的發展也離不開業界專家與合作伙伴的支持。為了感謝各位合作伙伴的支持與貢獻，本次大會組委會特別頒發 
 &lt;strong&gt;「源創會 2024 年度技術領航者」&lt;/strong&gt; 
 &lt;strong&gt;、&lt;/strong&gt; 
 &lt;strong&gt;「開源中國 2024 年度突出貢獻專家」&lt;/strong&gt; 、 
 &lt;strong&gt;「 Gitee AI 年度最佳合作伙伴」&lt;/strong&gt;三大獎項。 
&lt;/div&gt; 
&lt;div&gt;
  &amp;nbsp; 
&lt;/div&gt; 
&lt;div&gt;
  大會現場巧妙設置了一系列精彩紛呈、趣味盎然的活動，如「可樂滾滾樂」、「展台互動集章」、「尋找神祕人」等小遊戲，讓參會者門在繁忙的學習交流之餘，也能盡情享受活動帶來的歡樂時光。 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;img height=&quot;757&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-60c6f827a4c838c5c35d9126e2b007ab682.png&quot; width=&quot;1140&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;img height=&quot;582&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-ea03f4327186e36e640b473448e4f40f4fe.png&quot; width=&quot;1132&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span id=&quot;OSC_h1_1&quot;&gt;&lt;/span&gt; 
 &lt;h1&gt;&lt;span style=&quot;color:#27ae60&quot;&gt;大會精彩內容集錦&lt;/span&gt;&lt;/h1&gt; 
 &lt;span id=&quot;OSC_h2_2&quot;&gt;&lt;/span&gt; 
 &lt;h2&gt;「AI 為伍，開源同行」主論壇&lt;/h2&gt; 
 &lt;blockquote&gt; 
  &lt;div&gt;
    聚焦開源與大模型技術的融合與發展 
  &lt;/div&gt; 
 &lt;/blockquote&gt; 
 &lt;div&gt; 
  &lt;img height=&quot;625&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-f6288527b46f51a02af66a1db7f29a28fd7.png&quot; width=&quot;942&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
 &lt;/div&gt; 
 &lt;div&gt;
   上午，在「AI 為伍，開源同行」的主論壇現場，華為資深開源工程師李佳偉發表了題為《主流開源軟件原生支持昇騰：大模型訓練與推理的輕鬆之選》的精彩演講，詳細闡述了華為昇騰在對主流開源軟件，諸如 vLLM 、ONNXRuntime 、ollama 、llama.cpp 等進行原生支持方面所取得的顯著進展以及當前的實際狀況，旨在為廣大開發者搭建起更為便捷、高效的大模型訓練與推理平台，助力其在 AI 領域的探索與創新之路更加順暢無阻。 
 &lt;/div&gt; 
 &lt;div&gt;
   李佳偉指出，在當今時代的科技浪潮中，AI 軟件領域正呈現出爆發式增長的強勁態勢，不斷突破傳統邊界，實現着顛覆性的成長與跨越，同時，代碼規模朝着更加精簡高效的方向發展，已成為不可逆轉的趨勢。面對智能計算領域開源軟件如雨後春筍般蓬勃湧現的局面，華為昇騰秉持着開放、包容的態度，誠摯歡迎各路賢才精英踴躍加入，共同挖掘技術潛力，拓展創新邊界，攜手推動 AI 技術邁向新的高峯，為全球科技產業的發展貢獻力量，共繪智能未來的宏偉藍圖。 
 &lt;/div&gt; 
 &lt;div&gt;
   &amp;nbsp; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;img height=&quot;615&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-9bc4705e402f42be1a8d343736695b058c6.png&quot; width=&quot;928&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
 &lt;/div&gt; 
 &lt;div&gt;
   紅帽大中華區首席架構師張家駒帶來題為《大模型技術創新與合作——在人工智能領域擁抱開源價值觀》的分享。步入 AI 時代，開源概念亦需順勢革新，秉持 100% 開源價值觀成為必然要求，這意味着不僅代碼要開源、權重需開放，訓練數據以及訓練方法等方面同樣要實現開源共享。基於這樣的理念，紅帽精心發起了 InstructLab 項目。InstructLab 志在打造一個開放包容的社區平台，讓每一個人都能擁有平等參與大模型開發的契機，使 AI 真正化作普惠大眾的技術力量。 
 &lt;/div&gt; 
 &lt;div&gt;
   &amp;nbsp; 
 &lt;/div&gt; 
 &lt;div&gt; 
  &lt;img height=&quot;603&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-012129eb7b9d5c29e6c8ccb962d26c18c78.png&quot; width=&quot;928&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
 &lt;/div&gt; 
 &lt;div&gt;
   在《大模型在研發安全的應用實踐》的分享中，騰訊代碼安全負責人張棟強調，代碼安全已成為大企業推進安全左移的核心點。傳統代碼安全方案在效率與能力上存在明顯瓶頸，騰訊混元大模型通過其卓越的語義理解與泛化能力，在存量場景中突破傳統能力上限，有效提升高危風險檢出的準確率（質）、檢出數（量）和修復效率。更重要的是在增量場景中，大模型為邏輯類漏洞和自動審計提供了落地的可能，使傳統技術較難解決的複雜問題得以推進，實現從「提質提量」到「擴邊增效」，推動代碼安全實現質的突破與應用領域的拓展。 
 &lt;/div&gt; 
 &lt;div&gt;
   &amp;nbsp; 
 &lt;/div&gt; 
 &lt;div&gt; 
  &lt;img height=&quot;616&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-a015e8fd1330ce4668420c5dfd1b510175c.png&quot; width=&quot;927&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
 &lt;/div&gt; 
 &lt;div&gt;
   螞蟻集團高級算法專家餘航則是分享了 CodeFuse 基座模型。CodeFuse 源於螞蟻自身的開發場景及代碼庫沉澱，基於海量高質量代碼數據和代碼領域特色詞表，以及多任務微調技術 MFT ，已從單環節智能化演進到企業級端到端的研發智能體探索，並開源了多個自研和微調的代碼大模型，總下載量近 200 萬。 
 &lt;/div&gt; 
 &lt;div&gt;
   餘航詳細介紹了，CodeFuse 旗下極具特色的倉庫級代碼圖大模型 CGM，在行業權威的 SWE-Bench Lite 榜單上表現卓越，成功解決了 41.67% 的問題，在競爭激烈的 SWE-Bench Lite 開源榜單中脫穎而出，榮登榜首之位。這一成績的取得，不僅彰顯了 CodeFuse 模型的高超性能與精準能力，更為整個代碼大模型領域樹立了新的標杆，為後續的研究與應用提供了極具價值的參考與借鑑，有望引領行業朝着更加高效、智能的方向發展。 
 &lt;/div&gt; 
 &lt;div&gt;
   &amp;nbsp; 
 &lt;/div&gt; 
 &lt;div&gt; 
  &lt;img height=&quot;610&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-199292b7ce5c5f0cd5d7393c7a66ffc5dc5.png&quot; width=&quot;919&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
 &lt;/div&gt; 
 &lt;div&gt;
   Vivo 高級系統架構專家徐海波在題為《 vivo 藍河操作系統的 AI 技術探索與前沿實踐》的分享。他介紹，BlueOS 藍河操作系統是 vivo 自研面向通用人工智能時代的智能操作系統，具備更智慧的 AI 交互、更流暢的性能、更安全的內核及框架等特點。 
 &lt;/div&gt; 
 &lt;div&gt; 
  &lt;span id=&quot;OSC_h2_3&quot;&gt;&lt;/span&gt; 
  &lt;h2&gt;「GenAI 開發關鍵技術」主論壇&lt;/h2&gt; 
  &lt;blockquote&gt; 
   &lt;div&gt;
     聚焦 GenAI 開發中的關鍵技術 
   &lt;/div&gt; 
  &lt;/blockquote&gt; 
  &lt;div&gt; 
   &lt;img height=&quot;604&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-08cb237f3c5cb587ad110e4b2e7f94810ee.png&quot; width=&quot;897&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
  &lt;/div&gt; 
  &lt;div&gt;
    在下午「GenAI 開發關鍵技術」主論壇中，英飛流創始人兼 CEO 張穎峯發表題為《新一代企業級多模態 RAG 引擎》的演講。張穎峯表示，隨着 LLM 多模態能力的增強，RAG 也需要步入多模態時代，它並不限於對日常圖片，音視頻的檢索增強，還應該涵蓋當下佔據大部分的非結構化文檔，發掘出這些數據的商業價值。 
  &lt;/div&gt; 
  &lt;div&gt;
    &amp;nbsp; 
  &lt;/div&gt; 
  &lt;div&gt; 
   &lt;img height=&quot;618&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-e6a80737e9be486a7d34acaf407bf736f4c.png&quot; width=&quot;913&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
  &lt;/div&gt; 
  &lt;div&gt;
    Gitee 私有云產品總監林靖靖發表《數據智能跟蹤體系的構建》分享，深入闡述了 Gitee DevOps 如何打破信息孤島，形成研發管理全域智能的產品組合，結合企業過程資產庫和研發過程資產信息庫，基於 AI 大模型 multiagents 和 RAG 技術，實現企業組織研發過程智能化、體系化，加速體系成熟，構築智能化軟件工廠。 
  &lt;/div&gt; 
  &lt;div&gt;
    &amp;nbsp; 
  &lt;/div&gt; 
  &lt;div&gt; 
   &lt;img height=&quot;604&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-4cec210f483ef6a15b932c90d0eb7aeed78.png&quot; width=&quot;915&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
  &lt;/div&gt; 
  &lt;div&gt;
    文心快碼 Baidu Comate 架構師徐曉強發佈題為《文心快碼在代碼生成場景下的知識豐富探索與實踐》的演講。為了提供給開發者更加準確的生成結果，文心快碼這兩年不斷豐富上下文的探索，在代碼續寫場景下做「準確度」和「速度」的平衡。也探索了基於 Agent 的代碼改寫能力。隨着模型能力的提升，文心快碼已經能夠在更多場景和更模糊的指令下完成更困難的任務。 
  &lt;/div&gt; 
  &lt;div&gt;
    &amp;nbsp; 
  &lt;/div&gt; 
  &lt;div&gt; 
   &lt;img height=&quot;607&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-10b5d6472c3fa89cd02d73699ad65380c7e.png&quot; width=&quot;922&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
  &lt;/div&gt; 
  &lt;div&gt;
    IDEA 基礎軟件中心高級工程師費浩祥發佈題為《MoonBit 和 AI 的協同設計》的演講。會上，費浩祥為大家介紹了 MoonBit 是如何在編程語言和工具鏈的上針對 AI 代碼生成進行協同設計，並介紹這些設計是如何改善模型的性能，從而幫助 MoonBit 用戶完成常見的編碼任務。 
  &lt;/div&gt; 
  &lt;div&gt;
    &amp;nbsp; 
  &lt;/div&gt; 
  &lt;div&gt; 
   &lt;img height=&quot;615&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-7ad18a4d35ede9554bee7a8bde8c201b7e7.png&quot; width=&quot;915&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
  &lt;/div&gt; 
  &lt;div&gt;
    華為開源生態專家楊滔發表《大模型時代的昇騰 AI 》主題分享。楊滔指出，人工智能時代，昇騰基礎軟硬件平台提供從底層算力、算子、框架、套件等層面對人工智能從模型開放到應用的全流程支持。 
  &lt;/div&gt; 
  &lt;div&gt;
    在人工智能框架方面，昇思 MindSpore 持續創新，通過易用性提示，對大模型訓推的支持，擁抱 AI 時代的創新，降低用戶開發和應用成本。 
  &lt;/div&gt; 
  &lt;div&gt;
    AI 應用使能套件作為昇騰生態領域的關鍵窗口，專注於賦予開發者圍繞模型的全方位的能力，涵蓋模型訓練與推理一體的高效流程，有力地降低了昇騰硬件開發的技術門檻。 
  &lt;/div&gt; 
  &lt;div&gt;
    &amp;nbsp; 
  &lt;/div&gt; 
  &lt;div&gt; 
   &lt;img height=&quot;616&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-9323e5b39bf9903dfaf010be001731182d1.png&quot; width=&quot;915&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
  &lt;/div&gt; 
  &lt;div&gt;
    矩陣起源研發 VP 趙晨陽在題為《如何利用多模態模型構建適用於 LLM 搜索的數據》的分享中表示，智能體表現好壞依賴於數據，也進一步應證了高質量「知識」對於 LMM 的重要性。隨後，趙晨陽進一步闡述在多模態數據融合階段，更是需要創新性的算法和模型架構，來打破不同模態之間的語義鴻溝，實現數據的有機整合和協同表達。 
  &lt;/div&gt; 
  &lt;div&gt;
    &amp;nbsp; 
  &lt;/div&gt; 
  &lt;div&gt; 
   &lt;img height=&quot;618&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-2158984da12de417f86ea183d0f0092ba11.png&quot; width=&quot;927&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
  &lt;/div&gt; 
  &lt;div&gt;
    Java 開發者應該如何構建 Agent？會上，Spring AI Alibaba 項目負責人劉軍則向大家介紹了基於百鍊模型服務的 AI 應用開發框架「 Spring AI Alibaba 」及其開發框架的架構與基本使用。Spring AI Alibaba 開源項目基於 Spring AI 構建，是阿里雲通義系列模型及服務在 Java AI 應用開發領域的最佳實踐，提供高層次的 AI API 抽象與雲原生基礎設施集成方案，可以幫助開發者快速構建 AI 應用。 
  &lt;/div&gt; 
  &lt;div&gt;
    &amp;nbsp; 
  &lt;/div&gt; 
  &lt;div&gt; 
   &lt;img height=&quot;616&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-e129d7547d9c85e6c0c4a4b7b8d5b4968d3.png&quot; width=&quot;921&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
  &lt;/div&gt; 
  &lt;div&gt;
    Alluxio 首席架構師傅正佳帶來題為《構建大模型時代的高性能 AI 數據底座》的分享。傅正佳介紹，Alluxio 是一個位於數據存儲和計算框架之間，提供數據抽象、統一訪問、分佈式緩存加速、數據親和性調度等功能的開源數據編排平台。Alluxio 通過幫助企業構建大模型時代的高性能 AI 數據底座以應對 I/O 挑戰，提升 AI 算力的效率與性能，被廣泛應用於模型訓練與推理、自動駕駛、AI 製藥、金融量化以及視頻渲染等場景。 
  &lt;/div&gt; 
  &lt;div&gt; 
   &lt;span id=&quot;OSC_h2_4&quot;&gt;&lt;/span&gt; 
   &lt;h2&gt;「昇騰 AI 大模型與應用開發」分論壇&lt;/h2&gt; 
   &lt;blockquote&gt; 
    &lt;div&gt;
      聚焦昇騰 AI 大模型與應用開發 
    &lt;/div&gt; 
   &lt;/blockquote&gt; 
   &lt;div&gt; 
    &lt;img height=&quot;649&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-09f43971f063549241591e17cf938f1dd4a.png&quot; width=&quot;919&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
   &lt;/div&gt; 
   &lt;div&gt;
     在下午的「昇騰 AI 大模型與應用開發」分論壇上，華為昇思生態總監王神迪博士帶來題為《昇思 MindSpore AI 框架使能大模型原生創新》的分享。昇思 MindSpore 作為大模型時代 AI 框架的新選擇，作為中國乃至世界的框架「新勢力」，引領技術創新，加速全面智能化時代到。目前，社區下載量 1000 萬+，社區核心貢獻者 3.5 萬，認證企業數超 1500+ 家。 
   &lt;/div&gt; 
   &lt;div&gt;
     &amp;nbsp; 
   &lt;/div&gt; 
   &lt;div&gt; 
    &lt;img height=&quot;687&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-8d5d0f602b6d8542509a415f14e4183c21c.png&quot; width=&quot;916&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
   &lt;/div&gt; 
   &lt;div&gt;
     華為主任工程師張俊怡發表了題為《昇騰大模型 MindSpeed 訓練加速庫系列介紹》的演講。張俊怡圍繞 MindSpeed 向大家介紹訓練加速庫系列，深入闡釋了其核心技術架構與獨特優勢。MindSpeed 訓練加速庫旨在應對當前人工智能領域對高效、快速訓練日益增長的需求，通過優化算法、改進內存管理以及充分利用硬件並行計算能力等手段，顯著提升了模型訓練的速度與效率。 
   &lt;/div&gt; 
   &lt;div&gt;
     &amp;nbsp; 
   &lt;/div&gt; 
   &lt;div&gt; 
    &lt;img height=&quot;814&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-c0d7b41b6e29813fd062bffb97d3e009cf8.png&quot; width=&quot;1084&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
   &lt;/div&gt; 
   &lt;div&gt;
     華為昇騰生態套件項目架構師潘邵武帶來題為《昇騰生態開發套件，模型訓推新體驗》的分享。為提升昇騰平台的模型開發效率，加速開發者 AI 應用創新，華為計算產品線牽頭開發了 AI 應用使能套件，已適配 LLaMa-Factory 、Stable Diffusion WebUI 等開源生態套件，覆蓋了微調訓練、推理部署、模型評測等模型開發應用全流程。會上，潘邵武圍繞昇騰生態，向大家展示了 AI 應用使能套件生態全景，以及 OpenI 啓智社區所開展的各類活動，希望與廣大開發者共建昇騰 AI 生態。 
   &lt;/div&gt; 
   &lt;div&gt;
     &amp;nbsp; 
   &lt;/div&gt; 
   &lt;div&gt; 
    &lt;img height=&quot;627&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-4b9ffa343341260036dfd60c7ce1a2a362e.png&quot; width=&quot;921&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
   &lt;/div&gt; 
   &lt;div&gt;
     迅龍軟件系統開發工程師徐洋帆為大家帶來題為《香橙派：開源+ AI ，探索無限可能》的分享。徐洋帆介紹，香橙派與華為昇騰目前聯合研發的高算力人工智能產品，包括 OrangePi Alpro、OrangePi Al Studio 等，具有強大的計算能力和高效的運算速度，能滿足市場上各行各業及個人開發者對 AI 推理應用開發的需求，能讓企業以更低的門檻嘗試 A，推動企業的智能化升級。 
   &lt;/div&gt; 
   &lt;div&gt;
     &amp;nbsp; 
   &lt;/div&gt; 
   &lt;div&gt; 
    &lt;img height=&quot;654&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-aca8e06a98a5ebc243f4a2bb5fba04fd894.png&quot; width=&quot;922&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
   &lt;/div&gt; 
   &lt;div&gt;
     魔樂社區負責人、天翼雲專家李寶龍為大家帶來題為《與魔樂一起，繁榮國產 AI 生態》的分享。魔樂社區（Modelers）是全新的人工智能社區，擁有包容的工具鏈體系，已託管和展示昇思、DeepSpeed、AI 應用使能套件等框架或平台。他還表示，魔樂社區堅持走開源、公益的路線，免費、長期支撐應用創新。值得一提的是，魔樂社區對用戶制定了成長激勵計劃，鼓勵用戶在不同領域深入學習和實踐，從而實現個人和專業上的成長與發展。 
   &lt;/div&gt; 
   &lt;div&gt; 
    &lt;img height=&quot;625&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-935398b1541ccdf5c2d97900708b27e5cc9.png&quot; width=&quot;912&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
   &lt;/div&gt; 
   &lt;div&gt;
     開源中國 Gitee AI 負責人彭博則為大家分享《 Gitee AI 如何在國產算力上構建 Serverless API 及其應用場景》。彭博指出，模型引擎和應用引擎已經暴露出一些問題，如模型引擎體驗失敗率高，應用引擎要編寫跟 GPU 推理相關的代碼門檻高等等。因此 Gitee AI 推出 Serverless API，直接調用 API，無須關心底層的 GPU 推理代碼；同時兼容 OpenAl 接口，門檻低；體驗穩定，部署簡單；按次付費，價格實惠。 
   &lt;/div&gt; 
   &lt;div&gt;
     &amp;nbsp; 
   &lt;/div&gt; 
   &lt;div&gt; 
    &lt;img height=&quot;685&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-e2bd0c60f0082318248c3626e8197027977.png&quot; width=&quot;913&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
   &lt;/div&gt; 
   &lt;div&gt;
     落到具體，情感機器（北京）科技有限公司 AI 生態負責人陳少宏則是為大家帶來題為《 SwanLab+openMind 打造國產 AI 開發者工具鏈》的分享。他介紹，情感機器（北京）科技有限公司是一家專注於人工智能和機器學習底層工具研發的高科技企業。旗下 SwanLab 是一款專為 AI 訓練設計的過程記錄工具，幫助開發者發掘出最具潛力的 AI 模型，將與 AI 應用使能套件共同打造全球領先的人工智能研發工具鏈。 
   &lt;/div&gt; 
   &lt;div&gt;
     &amp;nbsp; 
   &lt;/div&gt; 
   &lt;div&gt;
     在本次 
    &lt;strong&gt;2024 &lt;/strong&gt; 
    &lt;strong&gt;OSC&lt;/strong&gt; 
    &lt;strong&gt; 源創會年終盛典&lt;/strong&gt;的推進過程中，我們心懷無盡感激，向一路同行的贊助商、支持單位、合作伙伴們致以最誠摯的鳴謝。 
   &lt;/div&gt; 
   &lt;ul&gt; 
    &lt;li&gt; 
     &lt;div&gt; 
      &lt;strong&gt;贊助商及支持單位&lt;/strong&gt; 
     &lt;/div&gt; &lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;div&gt; 
    &lt;img height=&quot;526&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-76342f33f173a817318980d182400389d1e.png&quot; width=&quot;904&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
   &lt;/div&gt; 
   &lt;ul&gt; 
    &lt;li&gt; 
     &lt;div&gt; 
      &lt;strong&gt;合作伙伴&lt;/strong&gt; 
     &lt;/div&gt; &lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;div&gt; 
    &lt;img height=&quot;358&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-e25b48eb839592ed279e28923d77b51b260.png&quot; width=&quot;925&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
   &lt;/div&gt; 
   &lt;div&gt;
     &amp;nbsp; 
   &lt;/div&gt; 
   &lt;div&gt; 
    &lt;em&gt;&lt;strong&gt;我們明年源創會再見&lt;/strong&gt;&lt;/em&gt; 
   &lt;/div&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/oschinaofficial/blog/16928110</link>
            <guid isPermaLink="false">https://my.oschina.net/oschinaofficial/blog/16928110</guid>
            <pubDate>Tue, 31 Dec 2024 06:29:00 GMT</pubDate>
            <author>原創</author>
        </item>
        <item>
            <title>smart-chatRoom —— 分佈式簡易聊天系統</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                                                                        
                                                                                    &lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:#40485b&quot;&gt;基於 Redis/RocketMQ + SpringBoot + Vue +Websocket 實現分佈式、跨服務器節點共享的分佈式簡易聊天系統，通過該核心實例實現了跨服務器節點無法共享 WebSocket 會話 Session，無法跨節點查詢用戶會話信息的痛點； 藉助 Redis 實現了跨通道 WebSocket 通信，按通道進行會話交流的亮點; 藉助 RocketMQ 實現高併發場景下的會話消費堆積、會話丟失問題。&lt;/span&gt;&lt;/p&gt;

&lt;p style=&quot;color:#252b3a; margin-left:0; margin-right:0; text-align:start&quot;&gt;主要亮點如下：&lt;/p&gt;

&lt;ul style=&quot;margin-left:0; margin-right:11px&quot;&gt;
&lt;li style=&quot;list-style-type:disc&quot;&gt;支持基於 Redis 多通道的訂閲發送，按通道實現 Session 共享&lt;/li&gt;
&lt;li style=&quot;list-style-type:disc&quot;&gt;支持基於 Redis 多通道的消息會話隔離，保證消息交流的安全性&lt;/li&gt;
&lt;li style=&quot;list-style-type:disc&quot;&gt;支持基於 Redis 多通道消息監控、消息存儲，實現離線消息的暫存&lt;/li&gt;
&lt;li style=&quot;list-style-type:disc&quot;&gt;支持基於 RocketMQ 中間件的消息發佈、訂閲，按消息主題區分消息會話&lt;/li&gt;
&lt;li style=&quot;list-style-type:disc&quot;&gt;支持基於 RocketMQ 中間件的消息分類處理，實現按標籤共享 Session、消費 Session&lt;/li&gt;
&lt;li style=&quot;list-style-type:disc&quot;&gt;支持基於 RocketMQ 中間件的消息離線發送，用戶上線按照上次消費位點接收離線消息&lt;/li&gt;
&lt;li style=&quot;list-style-type:disc&quot;&gt;支持基於 RocketMQ 中間件的消息監聽，處理重複消息，實現消息發佈的高效率&lt;/li&gt;
&lt;li style=&quot;list-style-type:disc&quot;&gt;支持單聊、羣聊、廣播、按指定通道交流&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

                                                                    &lt;/div&gt;
                                                                </description>
            <link>https://www.oschina.net/p/smart-chatroom</link>
            <guid isPermaLink="false">https://www.oschina.net/p/smart-chatroom</guid>
            <pubDate>Tue, 31 Dec 2024 06:01:00 GMT</pubDate>
        </item>
        <item>
            <title>百度 25 週年李彥宏發全員信：AI 應用將在 2025 年井噴</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;2025 年 1 月 1 日是百度成立 25 週年，百度創始人李彥宏晚間發出全員信表示，「25 年來，我們始終走在技術的最前沿，始終相信技術創新才是百度的核心競爭力。」&lt;/p&gt; 
&lt;p&gt;李彥宏在信中表明瞭對 2025 年的期待，「雖然超級應用尚未出現，但 AI 的實際滲透率已經不低，並且將在 2025 年繼續井噴式增長。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;附全員信原文：&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;各位百度同學，今天是百度成立 25 週年的日子。25 年前的今天，七個懷揣着技術改變世界的夢想的年輕人在中關村北大資源賓館的兩間小屋子裏開始了一段創業旅程。中國互聯網的歷史從此發生了改變。今天，超過一半的中國人每月都要使用百度獲取信息，找到所求，「百度」這兩個原本毫無意義的漢字成了一個家喻戶曉的名字。&lt;/p&gt; 
 &lt;p&gt;25 年來，我們不忘初心，風雨兼程，先後經歷了 PC 互聯網時代，移動互聯網時代，現在已經基本進入了人工智能時代。我們從簡單的網頁搜索功能開始，逐步發展出了貼吧、知道、百科、地圖、文庫、網盤等明星產品，我們依託強大的用戶基礎，歷時十餘年的時間，逐步打造出了人工智能時代從芯片、框架到模型、應用等四層全棧技術，為中國互聯網和世界 AI 領域培養了一批又一批的科學技術人員、開發者和創業者。&lt;/p&gt; 
 &lt;p&gt;25 年來，我們始終走在技術的最前沿，始終相信技術創新才是百度的核心競爭力，我們多年來一直把超過收入 20% 的資金投入到研發上，並且不遺餘力地嘗試把最前沿的技術產品化，讓更多的人從中受益，因為我們相信只有規模化的應用才能讓技術發揮它的價值，甚至近年來在人工智能方面的實踐表明，重大的技術突破，顛覆式的創新往往是規模化應用的結果，而不是原因。沒有萬卡集羣就不會有大模型的智能湧現，就不會有這次生成式 AI 的浪潮；沒有數以億計的運營公里數，無人駕駛就不可能比有人駕駛安全十倍；沒有大量的 AI 原生應用的推動，國產 AI 芯片就不可能真正成熟！&lt;/p&gt; 
 &lt;p&gt;當然，走在技術的最前沿也意味着我們要冒更大的風險，要承受高於同行的失敗概率，要耐得住寂寞，要忍受別人的不理解甚至白眼，要不斷試錯，要知道哪一天方向走錯了需要迅速調整方向，重新出發，甚至要對自己的能力邊界有清醒的認知，並且不斷總結經驗教訓，以利再戰！&lt;/p&gt; 
 &lt;p&gt;剛剛過去的 2024 年也是過去 25 年的一個縮影，充滿了機遇和挑戰，時而令人興奮，時而令人沮喪，有些工作一直到最後一天才知道成或不成。如同過去一樣，這一年我們堅定地在 AI 技術上探索創新，我們在全球首創了基於圖片的檢索增強技術 iRAG，大大降低了圖片生成的幻覺問題；我們致力於讓不會寫程序的素人具備程序員的能力，為此我們發佈了秒噠，這與全球主流的代碼輔助生成形成鮮明的對比；我們也在大模型應用領域獨樹一幟，為 4000 萬文庫的付費用戶提供無與倫比的內容創作和思想碰撞能力！&lt;/p&gt; 
 &lt;p&gt;對於 2025 年，我們充滿期待。我們意識到今天的人工智能領域，競爭比任何時候都更加激烈，技術迭代的速度比以往任何時候都更快，我們面臨的挑戰也是前所未有的。但是我們堅信，大模型賦能的 AI 原生應用正在各行各業各種場景迅速普及，雖然超級應用尚未出現，AI 的實際滲透率已經不低，並且將在 2025 年繼續井噴式增長。我們也期待，我們在 2023、2024 種下的種子能夠在 2025 生根發芽，開花結果，並且不斷獲得市場的驗證和認可。&lt;/p&gt; 
 &lt;p&gt;感謝每一位同學一直以來對百度使命的忠誠陪伴，全情投入，願百度在未來的日子裏，繼續乘風破浪，勇往直前，創造更加輝煌的明天！&lt;/p&gt; 
 &lt;p&gt;Robin&lt;/p&gt; 
 &lt;hr&gt; 
 &lt;p&gt;&lt;span&gt;更多獨家技術見解與熱門話題討論，盡在&lt;a href=&quot;https://www.oschina.net/app&quot;&gt;【開源中國 APP】&lt;/a&gt;，與數百萬開發者一起，隨時隨地探索技術無限可能。&lt;/span&gt;&lt;/p&gt; 
&lt;/blockquote&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/327634</link>
            <guid isPermaLink="false">https://www.oschina.net/news/327634</guid>
            <pubDate>Tue, 31 Dec 2024 05:59:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>歡迎 PaliGemma 2 – 來自 Google 的新視覺語言模型</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                    
                                                                                                                                                    &lt;p&gt;我們很高興迎來 Google 全新的視覺語言模型 &lt;strong&gt;PaliGemma 2&lt;/strong&gt;，這是 PaliGemma 的一個新版本。與其前代產品一樣，PaliGemma 2 使用強大的 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fcollections%2Fgoogle%2Fsiglip-659d5e62f0ae1a57ae0e83ba&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;SigLIP&lt;/strong&gt;&lt;/a&gt; 進行視覺處理，但在文本解碼部分升級到了最新的 &lt;strong&gt;Gemma 2&lt;/strong&gt;。&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_1&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;模型規模和輸入分辨率&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;PaliGemma 2 提供了新的預訓練模型，參數規模包括 &lt;strong&gt;3B&lt;/strong&gt; 、 &lt;strong&gt;10B&lt;/strong&gt; 和 &lt;strong&gt;28B&lt;/strong&gt;。所有模型均支持以下多種輸入分辨率:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;224x224&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;448x448&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;896x896&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;這種多樣化的組合為不同的使用場景提供了極大的靈活性，使實踐者能夠根據質量和效率需求之間的平衡進行選擇。與之相比，上一代 PaliGemma 僅提供 &lt;strong&gt;3B&lt;/strong&gt; 版本。&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_2&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;預訓練和微調能力&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;這些預訓練模型被設計為更容易適配下游任務。首個 PaliGemma 模型因其廣泛適配性被社區用於多種任務。本次迭代引入了更高質量的預訓練模型和更多選擇，進一步增強了靈活性。&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_3&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;DOCQI 數據集示例&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;Google 此次發佈了一些基於 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fdatasets%2Fgoogle%2Fdocci&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;DOCCI&lt;/strong&gt;&lt;/a&gt; 數據集的微調模型，展現了長篇、細緻和富有表現力的圖像描述能力。這些微調模型提供 &lt;strong&gt;3B&lt;/strong&gt; 和 &lt;strong&gt;10B&lt;/strong&gt; 兩個版本，支持輸入分辨率 &lt;strong&gt;448x448&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;此次發佈包含了所有開放的模型倉庫、Transformers 框架的集成、微調腳本，以及我們基於 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fdatasets%2FHuggingFaceM4%2FVQAv2&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;VQAv2 數據集&lt;/strong&gt;&lt;/a&gt; 微調的視覺問答模型演示。這些資源為用戶提供了全面的工具支持，助力探索和開發更多創新應用。&lt;/p&gt; 
&lt;hr&gt; 
&lt;span id=&quot;OSC_h2_4&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;資源鏈接&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;本次發佈包括開源模型庫、transformers 集成、微調腳本以及視覺問答演示。以下是相關資源鏈接:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fcollections%2Fgoogle%2Fpaligemma-2-release-67500e1e1dbfdd4dee27ba48&quot; target=&quot;_blank&quot;&gt;發佈合集&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fmerveenoyan%2Fsmol-vision%2Fblob%2Fmain%2FFine_tune_PaliGemma.ipynb&quot; target=&quot;_blank&quot;&gt;微調腳本&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fspaces%2Fmerve%2Fpaligemma2-vqav2&quot; target=&quot;_blank&quot;&gt;微調模型演示 Demo&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fpapers%2F2412.03555&quot; target=&quot;_blank&quot;&gt;技術報告&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;span id=&quot;OSC_h1_5&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;&lt;span&gt;PaliGemma 2 介紹&lt;/span&gt;&lt;/h1&gt; 
&lt;p&gt;PaliGemma 2 是 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fblog%2Fpaligemma&quot; target=&quot;_blank&quot;&gt;PaliGemma 視覺語言模型&lt;/a&gt; 的一個新迭代，由 Google 於五月發佈。&lt;/p&gt; 
&lt;p&gt;PaliGemma 2 將強大的 SigLIP 圖像編碼器與 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fblog%2Fgemma2&quot; target=&quot;_blank&quot;&gt;Gemma 2&lt;/a&gt; 語言模型連接起來。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;PaliGemma2 Architecture&quot; src=&quot;https://eastwind-cdn.dongs.xyz/image/202412180054322.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;PaliGemma2 Architecture&lt;/p&gt; 
&lt;p&gt;新的模型基於 &lt;strong&gt;Gemma 2&lt;/strong&gt; 的 &lt;strong&gt;2B&lt;/strong&gt; 、&lt;strong&gt;9B&lt;/strong&gt; 和 &lt;strong&gt;27B&lt;/strong&gt; 語言模型，分別對應 &lt;strong&gt;3B&lt;/strong&gt; 、&lt;strong&gt;10B&lt;/strong&gt; 和 &lt;strong&gt;28B&lt;/strong&gt; 的 PaliGemma 2 變體。這些模型的名稱考慮了緊湊圖像編碼器的附加參數。正如上文所述，這些模型支持三種不同的分辨率，為下游任務的微調提供了很大的靈活性。&lt;/p&gt; 
&lt;p&gt;PaliGemma 2 根據 &lt;strong&gt;Gemma 許可證&lt;/strong&gt; 分發，該許可證允許重新分發、商業使用、微調以及創建模型衍生品。&lt;/p&gt; 
&lt;p&gt;此版本包含以下基於 &lt;strong&gt;bfloat16&lt;/strong&gt; 精度的檢查點:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;9 個預訓練模型&lt;/strong&gt;: 3B、10B 和 28B，分辨率支持&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;224x224&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;448x448&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;896x896&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;2 個在 DOCCI 數據集上的微調模型&lt;/strong&gt;: 基於 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fdatasets%2Fgoogle%2Fdocci&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;DOCCI&lt;/strong&gt;&lt;/a&gt; 數據集 (圖像-文本配對)，支持 &lt;strong&gt;3B&lt;/strong&gt; 和 &lt;strong&gt;10B&lt;/strong&gt; 的 PaliGemma 2 變體，輸入分辨率為 &lt;strong&gt;448x448&lt;/strong&gt;。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id=&quot;OSC_h2_6&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;模型能力&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;如同之前的 PaliGemma 發佈一樣，預訓練 (pt) 模型在下游任務的微調中表現出色。&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_7&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;預訓練數據集&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;pt 模型在以下數據混合集上進行了預訓練。這些多樣化的預訓練數據集使模型能夠在相似領域的下游任務中使用更少的示例進行微調。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;WebLI&lt;/strong&gt;: 一個基於公共網絡構建的大規模多語言圖像 - 文本數據集。WebLI 數據集的多樣化分割使模型具備了多方面的能力，如視覺語義理解、物體定位、視覺文本理解和多語言能力。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;CC3M-35L&lt;/strong&gt;: 從網頁上精心挑選的英語圖像 - 替代文本數據集 (&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Faclanthology.org%2FP18-1238%2F&quot; target=&quot;_blank&quot;&gt;Sharma et al., 2018&lt;/a&gt;)。數據集的標籤通過 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcloud.google.com%2Ftranslate&quot; target=&quot;_blank&quot;&gt;Google Cloud Translation API&lt;/a&gt; 翻譯成了 34 種額外的語言。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Visual Question Generation with Question Answering Validation (VQ2A)&lt;/strong&gt;: 一個改進的問題回答數據集。該數據集也被翻譯成了相同的 34 種語言，使用了 Google Cloud Translation API。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;OpenImages&lt;/strong&gt;: 檢測和物體感知的問答數據集 (Piergiovanni et al., 2022)，通過手動規則生成，基於 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fstorage.googleapis.com%2Fopenimages%2Fweb%2Ffactsfigures_v7.html&quot; target=&quot;_blank&quot;&gt;OpenImages 數據集&lt;/a&gt;。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;WIT&lt;/strong&gt;: 從 Wikipedia 收集的圖像和文本數據集 (Srinivasan et al., 2021)。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;span id=&quot;OSC_h2_8&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;微調模型與基準測試&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;PaliGemma 2 團隊在多種視覺語言理解任務上對 PT 模型進行了內部微調，並提供了這些微調模型的基準測試結果。詳細信息可以在 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fgoogle%2Fpaligemma2-28b-pt-896%23paligemma-2-results-by-model-resolution-and-size&quot; target=&quot;_blank&quot;&gt;模型卡&lt;/a&gt; 和 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fpapers%2F2412.03555&quot; target=&quot;_blank&quot;&gt;技術報告&lt;/a&gt; 中找到。&lt;/p&gt; 
&lt;p&gt;PaliGemma 2 基於 &lt;strong&gt;DOCQI 數據集&lt;/strong&gt; 微調，可以實現多種圖像描述任務，包括文本渲染、捕捉空間關係以及包含世界知識的描述。&lt;/p&gt; 
&lt;hr&gt; 
&lt;span id=&quot;OSC_h2_9&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;性能比較&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;以下表格展示了 DOCQI 微調模型與其他模型的性能對比 (數據來自 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fpapers%2F2412.03555&quot; target=&quot;_blank&quot;&gt;技術報告&lt;/a&gt; 中的 Table 6):&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;th&gt;模型&lt;/th&gt; 
   &lt;th&gt;參數量&lt;/th&gt; 
   &lt;th&gt;字符數 (#char)&lt;/th&gt; 
   &lt;th&gt;句子數 (#sent)&lt;/th&gt; 
   &lt;th&gt;NES ↓&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniGPT-4&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;484&lt;/td&gt; 
   &lt;td&gt;5.6&lt;/td&gt; 
   &lt;td&gt;52.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;mPLUG-Owl2&lt;/td&gt; 
   &lt;td&gt;8B&lt;/td&gt; 
   &lt;td&gt;459&lt;/td&gt; 
   &lt;td&gt;4.4&lt;/td&gt; 
   &lt;td&gt;48.4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;InstructBLIP&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;510&lt;/td&gt; 
   &lt;td&gt;4.0&lt;/td&gt; 
   &lt;td&gt;42.6&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LLAVA-1.5&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;395&lt;/td&gt; 
   &lt;td&gt;4.2&lt;/td&gt; 
   &lt;td&gt;40.6&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;VILA&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;871&lt;/td&gt; 
   &lt;td&gt;8.6&lt;/td&gt; 
   &lt;td&gt;28.6&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PaliGemma&lt;/td&gt; 
   &lt;td&gt;3B&lt;/td&gt; 
   &lt;td&gt;535&lt;/td&gt; 
   &lt;td&gt;8.9&lt;/td&gt; 
   &lt;td&gt;34.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PaLI-5B&lt;/td&gt; 
   &lt;td&gt;5B&lt;/td&gt; 
   &lt;td&gt;1065&lt;/td&gt; 
   &lt;td&gt;11.3&lt;/td&gt; 
   &lt;td&gt;32.9&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;PaliGemma 2&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;3B&lt;/td&gt; 
   &lt;td&gt;529&lt;/td&gt; 
   &lt;td&gt;7.7&lt;/td&gt; 
   &lt;td&gt;28.4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;PaliGemma 2&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;10B&lt;/td&gt; 
   &lt;td&gt;521&lt;/td&gt; 
   &lt;td&gt;7.5&lt;/td&gt; 
   &lt;td&gt;20.3&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr&gt; 
&lt;span id=&quot;OSC_h3_10&quot;&gt;&lt;/span&gt; 
&lt;h3&gt;&lt;span&gt;指標説明:&lt;/span&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;#char&lt;/strong&gt;: 生成的描述中平均字符數。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;#sent&lt;/strong&gt;: 平均句子數。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;NES&lt;/strong&gt;: 非蘊含句子數 (數值越低越好)，用於衡量事實不準確性。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;您可以在下面找到 DOCQI 檢查點的部分模型輸出，展示模型的多樣性和靈活性。&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Input Image&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Caption&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img alt=&quot;Image 1&quot; src=&quot;https://eastwind-cdn.dongs.xyz/image/202412180054547.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;折線圖展示了 ImageNet 模型在微調後的 Top-1 準確率表現。圖中有四條不同顏色的線條: 藍色、橙色、綠色和黑色。&lt;strong&gt;藍色線條是四條線中最低的一條&lt;/strong&gt; ，它代表了表現最差的模型結果。&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img alt=&quot;Image 2&quot; src=&quot;https://eastwind-cdn.dongs.xyz/image/202412180054606.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;一張白紙的特寫鏡頭，上面用黑色的文字打印着內容。紙張中間稍微彎曲，文字使用打字機字體呈現。紙張頂部寫着 &quot;&lt;strong&gt;Ashley Hotel West Coast&lt;/strong&gt;&quot;，其下是 &quot;&lt;strong&gt;WiFi Internet Service&lt;/strong&gt;&quot;。再下面是 &quot;&lt;strong&gt;Username: fqpp&lt;/strong&gt;&quot;，最後是 &quot;&lt;strong&gt;Password: aaeu&lt;/strong&gt;&quot;。&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img alt=&quot;Image 3&quot; src=&quot;https://eastwind-cdn.dongs.xyz/image/202412180055484.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;一幅描繪大衞·鮑伊「Ziggy Stardust」造型的壁畫被畫在一面白牆上。壁畫展示了三張並排的面孔，每張都有紅色的頭髮，眼睛上畫着藍色的閃電圖案。面孔的妝容包括藍色眼影、粉紅色腮紅和紅色嘴脣。中間的面孔上方有一個黑色的方形窗口，窗口內用白色文字寫着 &quot;&lt;strong&gt;JAM&lt;/strong&gt;&quot;，字體為藍色。畫面的一側停着一輛銀色汽車。&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img alt=&quot;Image 4&quot; src=&quot;https://eastwind-cdn.dongs.xyz/image/202412180055346.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;從上方俯瞰一張白色大理石台面，枱面上放着四個咖啡杯。左邊有兩個灰色的杯子，左下角有一個白色的杯子，右側則是另一個灰色的杯子。右上角放着一個帶木質底座的金屬水果籃，裏面裝滿了橙子。左邊還有一個裝有水的透明玻璃水壺，畫面中僅顯示了部分內容。&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img alt=&quot;Image 5&quot; src=&quot;https://eastwind-cdn.dongs.xyz/image/202412180055610.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;一張白色書本的特寫，上半部分是白色區域，底部有一條藍色條紋。白色部分印有黑色文字，內容為: &quot;&lt;strong&gt;Visual Concept Learning from User-tagged Web Video&lt;/strong&gt;&quot; 。黑色文字下方有一個白色框，框內包含五張小圖片。最左邊的圖片是一名站在草地中的人，右側緊接的是一張藍色海洋的圖片。&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;span id=&quot;OSC_h2_11&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;演示&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;為了演示效果，Hugging Face 團隊對 &lt;strong&gt;PaliGemma 2 3B&lt;/strong&gt; 模型進行了微調，輸入分辨率為 448x448，數據集使用的是 &lt;strong&gt;VQAv2&lt;/strong&gt; 的一小部分。我們採用了 &lt;strong&gt;LoRA 微調&lt;/strong&gt; 和 &lt;strong&gt;PEFT&lt;/strong&gt; 方法，具體細節將在微調部分進行講解。&lt;/p&gt; 
&lt;p&gt;下面的演示展示了最終結果。您可以自由查看 Space 中的代碼瞭解其工作原理，或者克隆代碼以適配您的自定義微調需求。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://eastwind-cdn.dongs.xyz/image/202412180057439.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_12&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;如何與 Transformers 一起使用&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;您可以使用 🤗 Transformers 庫對 &lt;strong&gt;PaliGemma 2&lt;/strong&gt; 模型進行推理，通過 &lt;strong&gt;PaliGemmaForConditionalGeneration&lt;/strong&gt; 和 &lt;strong&gt;AutoProcessor&lt;/strong&gt; APIs 實現操作。請確保您安裝的 Transformers 版本為 &lt;strong&gt;4.47 或更高&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip&amp;nbsp;install&amp;nbsp;transformers&amp;gt;=4.47
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;在安裝完成後，您可以按照以下示例運行推理。同樣重要的是，請確保遵循用於訓練模型的任務提示格式，以獲得最佳效果:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&lt;span&gt;from&lt;/span&gt;&amp;nbsp;transformers&amp;nbsp;&lt;span&gt;import&lt;/span&gt;&amp;nbsp;AutoProcessor,&amp;nbsp;PaliGemmaForConditionalGeneration
&lt;span&gt;from&lt;/span&gt;&amp;nbsp;PIL&amp;nbsp;&lt;span&gt;import&lt;/span&gt;&amp;nbsp;Image
&lt;span&gt;import&lt;/span&gt;&amp;nbsp;requests

model_id&amp;nbsp;=&amp;nbsp;&lt;span&gt;&quot;google/paligemma2-10b-ft-docci-448&quot;&lt;/span&gt;
model&amp;nbsp;=&amp;nbsp;PaliGemmaForConditionalGeneration.from_pretrained(model_id)
model&amp;nbsp;=&amp;nbsp;model.to(&lt;span&gt;&quot;cuda&quot;&lt;/span&gt;)
processor&amp;nbsp;=&amp;nbsp;AutoProcessor.from_pretrained(model_id)

prompt&amp;nbsp;=&amp;nbsp;&lt;span&gt;&quot;&amp;lt;image&amp;gt;caption&amp;nbsp;en&quot;&lt;/span&gt;
image_file&amp;nbsp;=&amp;nbsp;&lt;span&gt;&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/cats.png&quot;&lt;/span&gt;
raw_image&amp;nbsp;=&amp;nbsp;Image.open(requests.get(image_file,&amp;nbsp;stream=&lt;span&gt;True&lt;/span&gt;).raw).convert(&lt;span&gt;&quot;RGB&quot;&lt;/span&gt;)

inputs&amp;nbsp;=&amp;nbsp;processor(prompt,&amp;nbsp;raw_image,&amp;nbsp;return_tensors=&lt;span&gt;&quot;pt&quot;&lt;/span&gt;).to(&lt;span&gt;&quot;cuda&quot;&lt;/span&gt;)
output&amp;nbsp;=&amp;nbsp;model.generate(**inputs,&amp;nbsp;max_new_tokens=&lt;span&gt;200&lt;/span&gt;)

input_len&amp;nbsp;=&amp;nbsp;inputs[&lt;span&gt;&quot;input_ids&quot;&lt;/span&gt;].shape[&lt;span&gt;-1&lt;/span&gt;]
print(processor.decode(output[&lt;span&gt;0&lt;/span&gt;][input_len:],&amp;nbsp;skip_special_tokens=&lt;span&gt;True&lt;/span&gt;))
&lt;span&gt;#&amp;nbsp;A&amp;nbsp;medium&amp;nbsp;shot&amp;nbsp;of&amp;nbsp;two&amp;nbsp;cats&amp;nbsp;laying&amp;nbsp;on&amp;nbsp;a&amp;nbsp;pile&amp;nbsp;of&amp;nbsp;brown&amp;nbsp;fishing&amp;nbsp;nets.&amp;nbsp;The&amp;nbsp;cat&amp;nbsp;in&amp;nbsp;the&amp;nbsp;foreground&amp;nbsp;is&amp;nbsp;a&amp;nbsp;gray&amp;nbsp;tabby&amp;nbsp;cat&amp;nbsp;with&amp;nbsp;white&amp;nbsp;on&amp;nbsp;its&amp;nbsp;chest&amp;nbsp;and&amp;nbsp;paws.&amp;nbsp;The&amp;nbsp;cat&amp;nbsp;is&amp;nbsp;laying&amp;nbsp;on&amp;nbsp;its&amp;nbsp;side&amp;nbsp;with&amp;nbsp;its&amp;nbsp;head&amp;nbsp;facing&amp;nbsp;the&amp;nbsp;bottom&amp;nbsp;right&amp;nbsp;corner&amp;nbsp;of&amp;nbsp;the&amp;nbsp;image.&amp;nbsp;The&amp;nbsp;cat&amp;nbsp;in&amp;nbsp;the&amp;nbsp;background&amp;nbsp;is&amp;nbsp;laying&amp;nbsp;on&amp;nbsp;its&amp;nbsp;side&amp;nbsp;with&amp;nbsp;its&amp;nbsp;head&amp;nbsp;facing&amp;nbsp;the&amp;nbsp;top&amp;nbsp;left&amp;nbsp;corner&amp;nbsp;of&amp;nbsp;the&amp;nbsp;image.&amp;nbsp;The&amp;nbsp;cat&#39;s&amp;nbsp;body&amp;nbsp;is&amp;nbsp;curled&amp;nbsp;up,&amp;nbsp;its&amp;nbsp;head&amp;nbsp;is&amp;nbsp;slightly&amp;nbsp;turned&amp;nbsp;to&amp;nbsp;the&amp;nbsp;right,&amp;nbsp;and&amp;nbsp;its&amp;nbsp;front&amp;nbsp;paws&amp;nbsp;are&amp;nbsp;tucked&amp;nbsp;underneath&amp;nbsp;its&amp;nbsp;body.&amp;nbsp;There&amp;nbsp;is&amp;nbsp;a&amp;nbsp;teal&amp;nbsp;rope&amp;nbsp;hanging&amp;nbsp;from&amp;nbsp;the&amp;nbsp;fishing&amp;nbsp;net&amp;nbsp;in&amp;nbsp;the&amp;nbsp;top&amp;nbsp;right&amp;nbsp;corner&amp;nbsp;of&amp;nbsp;the&amp;nbsp;image.&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;您還可以使用 transformers 集成中的 &lt;strong&gt;&lt;code&gt;bitsandbytes&lt;/code&gt;&lt;/strong&gt; 來加載具有量化的模型。以下示例使用了 &lt;strong&gt;4-bit &lt;code&gt;nf4&lt;/code&gt;&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&lt;span&gt;from&lt;/span&gt;&amp;nbsp;transformers&amp;nbsp;&lt;span&gt;import&lt;/span&gt;&amp;nbsp;BitsAndBytesConfig

bnb_config&amp;nbsp;=&amp;nbsp;BitsAndBytesConfig(
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;load_in_4bit=&lt;span&gt;True&lt;/span&gt;,
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;bnb_4bit_quant_type=&lt;span&gt;&quot;nf4&quot;&lt;/span&gt;,
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;bnb_4bit_compute_dtype=torch.bfloat16
)
model&amp;nbsp;=&amp;nbsp;PaligemmaForConditionalGeneration.from_pretrained(
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;model_id,
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;quantization_config=bnb_config,
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;device_map={&lt;span&gt;&quot;&quot;&lt;/span&gt;:&lt;span&gt;0&lt;/span&gt;}
)

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;我們快速測試了量化對性能的影響，通過評估一個 3B 微調檢查點在 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fdatasets%2Flmms-lab%2Ftextvqa&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;textvqa&lt;/strong&gt;&lt;/a&gt; 數據集上的表現，使用 224x224 輸入圖像。這是我們在 5,000 個驗證集條目上獲得的結果:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;bfloat16&lt;/strong&gt;，無量化: &lt;strong&gt;60.04%&lt;/strong&gt; 準確率。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;8-bit&lt;/strong&gt;: **59.78%**。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;4-bit&lt;/strong&gt;，使用上面代碼片段中的配置: **58.72%**。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;這些結果非常鼓舞人心！當然，量化對於更大的檢查點更有意義，我們建議您始終在您所使用的領域和任務上測量結果。&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_13&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;微調&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;如果您之前已經微調過 &lt;strong&gt;PaliGemma&lt;/strong&gt;，那麼用於微調 &lt;strong&gt;PaliGemma 2&lt;/strong&gt; 的 API 是相同的，您可以直接使用現有代碼。我們提供了 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fmerveenoyan%2Fsmol-vision%2Fblob%2Fmain%2Fpaligemma.py&quot; target=&quot;_blank&quot;&gt;微調腳本&lt;/a&gt; 和一個 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fmerveenoyan%2Fsmol-vision%2Fblob%2Fmain%2FFine_tune_PaliGemma.ipynb&quot; target=&quot;_blank&quot;&gt;notebook&lt;/a&gt; 來幫助您微調模型，凍結模型部分參數，或應用內存高效的微調技術，如 &lt;strong&gt;LoRA&lt;/strong&gt; 或 &lt;strong&gt;QLoRA&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;我們使用 &lt;strong&gt;LoRA&lt;/strong&gt; 對 PaliGemma 2 模型在 VQAv2 驗證集的一半進行了微調，以供演示。這項任務使用了 &lt;strong&gt;3 塊 A100&lt;/strong&gt; 顯卡 (80GB VRAM)，耗時半小時。&lt;/p&gt; 
&lt;p&gt;您可以在 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fmerve%2Fpaligemma2-3b-vqav2&quot; target=&quot;_blank&quot;&gt;這裏&lt;/a&gt; 找到模型，此外 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fspaces%2Fmerve%2Fpaligemma2-vqav2&quot; target=&quot;_blank&quot;&gt;這個 Gradio 演示&lt;/a&gt; 展示了模型的效果。&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_14&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;&lt;span&gt;結論&lt;/span&gt;&lt;/h1&gt; 
&lt;p&gt;新發布的 &lt;strong&gt;PaliGemma 2&lt;/strong&gt; 比之前的版本更加令人興奮，具有不同的規模以滿足各種需求，並提供更強大的預訓練模型。我們期待看到社區能夠構建出什麼樣的成果！&lt;/p&gt; 
&lt;p&gt;我們感謝 Google 團隊發佈了這一令人驚歎且開放的模型系列。特別感謝 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2FMolbap&quot; target=&quot;_blank&quot;&gt;Pablo Montalvo&lt;/a&gt; 將模型集成到 Transformers 中，以及 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Flysandre&quot; target=&quot;_blank&quot;&gt;Lysandre&lt;/a&gt;、&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2FRaushanTurganbay&quot; target=&quot;_blank&quot;&gt;Raushan&lt;/a&gt;、&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2FArthurZ&quot; target=&quot;_blank&quot;&gt;Arthur&lt;/a&gt;、&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fydshieh&quot; target=&quot;_blank&quot;&gt;Yieh-Dar&lt;/a&gt; 和團隊其他成員的努力，他們迅速完成了模型的評審、測試和合並工作。&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_15&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;資源&lt;/span&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fcollections%2Fgoogle%2Fpaligemma-2-release-67500e1e1dbfdd4dee27ba48&quot; target=&quot;_blank&quot;&gt;發佈合集&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fblog%2Fpaligemma&quot; target=&quot;_blank&quot;&gt;PaliGemma 博客文章&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fmerveenoyan%2Fsmol-vision%2Fblob%2Fmain%2FFine_tune_PaliGemma.ipynb&quot; target=&quot;_blank&quot;&gt;微調腳本&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fmerve%2Fpaligemma2-3b-vqav2&quot; target=&quot;_blank&quot;&gt;在 VQAv2 上微調模型&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fspaces%2Fmerve%2Fpaligemma2-vqav2&quot; target=&quot;_blank&quot;&gt;微調模型演示&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fpapers%2F2412.03555&quot; target=&quot;_blank&quot;&gt;技術報告&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;英文原文: &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhf.co%2Fblog%2Fpaligemma2&quot; target=&quot;_blank&quot;&gt;https://hf.co/blog/paligemma2&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;原文作者: Merve Noyan, Andreas P. Steiner, Pedro Cuenca, Aritra Roy Gosthipaty&lt;/p&gt; 
 &lt;p&gt;譯者: xiaodouzi666&lt;/p&gt; 
&lt;/blockquote&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/HuggingFace/blog/17019541</link>
            <guid isPermaLink="false">https://my.oschina.net/HuggingFace/blog/17019541</guid>
            <pubDate>Tue, 31 Dec 2024 05:39:00 GMT</pubDate>
            <author>原創</author>
        </item>
        <item>
            <title>IBM 收購 HashiCorp 交易面臨英國反壟斷審查</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftechcrunch.com%2F2024%2F12%2F30%2Fuk-antitrust-watchdog-launches-review-of-ibms-hashicorp-takeover%2F&quot; target=&quot;_blank&quot;&gt;據 TechCrunch 報道&lt;/a&gt;，英國反壟斷監督機構競爭與市場管理局（CMA）已開始調查 IBM 計劃收購雲軟件廠商 HashiCorp 是否會影響競爭。&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-699fccb664555c8767ef7615467cd7deac2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;CMA 週一表示，它將在 1 月 16 日前邀請有關各方就這一併購發表評論。該監管機構暫定 2 月 25 日為最後期限，以決定是批准該交易還是將其提交進一步審查。&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;IBM 於今年 4 月宣佈同意以約 64 億美元的價格收購 HashiCorp。如果收購繼續進行，將擴大 IBM 在雲計算和人工智能領域的推進力度，並讓該公司獲得 HashiCorp 約 4400 家客戶的名冊。&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;CMA 於 8 月通知 HashiCorp 將對合並進行審查。美國聯邦貿易委員會也在調查這一交易。&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;相關閲讀：&lt;a href=&quot;https://www.oschina.net/news/289379/ibm-acquire-hashicorp-6-4-billion&quot; target=&quot;news&quot;&gt;IBM 以 64 億美元收購 HashiCorp&lt;/a&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;&lt;span&gt;更多獨家技術見解與熱門話題討論，盡在&lt;a href=&quot;https://www.oschina.net/app&quot;&gt;【開源中國 APP】&lt;/a&gt;，與數百萬開發者一起，隨時隨地探索技術無限可能。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/327615/ibms-acquire-hashicorp-takeover-faces-uk-antitrust-scrutiny</link>
            <guid isPermaLink="false">https://www.oschina.net/news/327615/ibms-acquire-hashicorp-takeover-faces-uk-antitrust-scrutiny</guid>
            <pubDate>Tue, 31 Dec 2024 03:59:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>一個大模型需要多大 GPU 內存才能跑起來的計算公式</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;一個大模型需要多大 GPU 內存才能跑起來的計算公式： M = &amp;nbsp;((P * 4B) / (32 / Q) &amp;nbsp;) * 1.2&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;M: 所需的 GPU 顯存，單位是 GB。&lt;/li&gt; 
 &lt;li&gt;P: 模型的參數數量。例如，7B 模型有 70 億個參數。&lt;/li&gt; 
 &lt;li&gt;4B: 每個參數佔用的字節數，這裏假設每個參數佔用 4 個字節（通常指 FP32 或 Float32 格式）。&lt;/li&gt; 
 &lt;li&gt;32: 4 個字節等於 32 位。&lt;/li&gt; 
 &lt;li&gt;Q: 加載模型時使用的位數。例如，16 位 (FP16/BF16)，8 位 (INT8) 或 4 位 (INT4)。這通常稱為量化。&lt;/li&gt; 
 &lt;li&gt;1.2: 表示額外開銷的係數，通常為 20%。這考慮了除了模型權重之外還需要加載到 GPU 顯存中的其他數據，例如優化器狀態、梯度等。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;如使用 FP16 量化加載 Llama 70B 模型，計算過程就是&lt;br&gt; &lt;strong&gt;M = &amp;nbsp;( (70,000,000,000 * 4) / (32 / 16) &amp;nbsp;)* 1.2 = 168 GB&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-0ecd94df1c281b114f9cbb19306dc7f358e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;——&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fweibo.com%2F2194035935%2FP6DrHa8Ob&quot; target=&quot;_blank&quot;&gt;蟻工廠&lt;/a&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;&lt;span&gt;更多獨家技術見解與熱門話題討論，盡在&lt;a href=&quot;https://www.oschina.net/app&quot;&gt;【開源中國 APP】&lt;/a&gt;，與數百萬開發者一起，隨時隨地探索技術無限可能。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/327612</link>
            <guid isPermaLink="false">https://www.oschina.net/news/327612</guid>
            <pubDate>Tue, 31 Dec 2024 03:53:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>2025 立個 Flag，先賺它一個小目標</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;div&gt; 
 &lt;div&gt;
  嘿，小夥伴們，你們 2025 的「小目標」是啥？買個溫馨的 house？牽手心中佳人？賺它一個億？炒掉 boss？還是來一場説走就走的旅行？還是寫出全球最火的開源項目？還是搞定那個讓人頭大的 bug？
  &lt;br&gt; &amp;nbsp;
 &lt;/div&gt; 
 &lt;div&gt;
  別光想，來立個 Flag，告訴全世界！ 現在參與，前 3 位最具創意、最能打動人心的 Flag，將贏得我們精心準備的定製揹包+T 恤套裝 1 套，助你成為開源界的時尚先鋒，互動數（點贊+評論）最多前 6 位將獲得定製揹包一個。
  &lt;br&gt; &amp;nbsp;
 &lt;/div&gt; 
 &lt;div&gt;
  快來
  &lt;a href=&quot;https://www.oschina.net/oscTweet/topic/%232025%E7%AB%8B%E4%B8%AAFlag&quot;&gt;【動彈】&lt;/a&gt;發佈你的 Flag，發佈在話題
  &lt;a href=&quot;https://www.oschina.net/oscTweet/topic/%232025%E7%AB%8B%E4%B8%AAFlag&quot;&gt;#2025 立個 Flag#&lt;/a&gt;下，讓全世界都看到你的決心和勇氣！ 記得，Flag 立得響，獎品拿得爽，説不定還能吸引一羣志同道合的夥伴，一起賺它一個小目標呢！
  &lt;br&gt; 
  &lt;br&gt; 別讓 Flag 只是説説而已，行動起來，讓 2025 因你而精彩！PC 或者掃碼下載 APP 參與。
 &lt;/div&gt; 
 &lt;div&gt;
  &lt;img alt=&quot;&quot; height=&quot;300&quot; src=&quot;https://static.oschina.net/uploads/space/2024/1231/114955_fkkn_7282530.png&quot; width=&quot;300&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
  &lt;br&gt; 
  &lt;strong&gt;活動時間：&lt;/strong&gt;2024 年 12 月 31 日~2025 年 1 月 15 日
 &lt;/div&gt; 
&lt;/div&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/327335</link>
            <guid isPermaLink="false">https://www.oschina.net/news/327335</guid>
            <pubDate>Tue, 31 Dec 2024 03:53:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>X.Org Server 的代碼提交次數創 10 年新高</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.phoronix.com%2Fmisc%2Fxserver-eoy-2024%2Findex.html&quot; target=&quot;_blank&quot;&gt;根據 X.Org Server 的&amp;nbsp;&lt;/a&gt;&lt;span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.phoronix.com%2Fmisc%2Fxserver-eoy-2024%2Findex.html&quot; target=&quot;_blank&quot;&gt;Git 提交記錄&lt;/a&gt;，在剛剛過去的 2024 年，&lt;/span&gt;X.Org Server 的代碼&lt;span&gt;提交次數達到了 2014 年以來的最高峯。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;雖然提交次數比前幾年多了不少，但這並不意味着 &lt;/span&gt;X.Org Server&amp;nbsp;&lt;span&gt;的復興，因為 Wayland 仍在 Linux 桌面上佔據主導地位。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-d93cec2031e1ac95173fc30bde92b9e8801.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;據統計，X.Org Server 去年有 708 次提交... 比起 2018 年的 535 次提交（每年 200~300 次）要多得多。但即使是在 2010 年代中期，Wayland 還在積極開發的時候，每年也只有 400~500 次提交... 在 2024 年之前，提交次數最多的是 2014 年，當時有 952 次提交。&lt;/p&gt; 
&lt;p&gt;按行數計算，2024 年 X.Org Server 新增了 11998 行代碼，刪除了 14680 行代碼。這比近幾年每年 X.Org Server 代碼庫中通常 5-6 千行代碼的變化要多。但仍遠低於 X.Org Server 積極開發的 2000 年代更高的代碼更新量。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-a555233d997eb149ec68424bd0390abacf2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;2024 年的 X.Org Server 之所以如此活躍主要有兩個原因。&lt;/p&gt; 
&lt;p&gt;首先，X.Org Server 中的 XWayland 代碼仍在繼續積極開發，用於支持新的 Wayland 協議和其他修復/添加內容... XWayland 是 X.Org Server 中繼續開發新功能的主要領域，也是唯一的領域。&lt;/p&gt; 
&lt;p&gt;另一個原因是開源開發者 Enrico Weigelt。&lt;/p&gt; 
&lt;p&gt;Enrico Weigelt 主要負責 X.Org Server 的修復和改進工作，圍繞 X.Org Server 的測試和更好的 CI BSD 覆蓋範圍。在紅帽或英特爾等主要廠商都沒有投資 X.Org Server 開發的情況下，Enrico 幾乎是單槍匹馬地完成了一些 X.Org Server 修復和其他小功能工作。&lt;/p&gt; 
&lt;p&gt;Enrico Weigelt 負責了今年 X.Org Server&amp;nbsp; 63% 的&amp;nbsp;&lt;span&gt;Git 提交... 其他活躍的開發者包括 Olivier Fourdan、Michel Dänzer、Alan Coopersmith、Peter Hutterer 和 Erik Kurzinger，他們大多隻關注 XWayland 的改進/修正。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;除此之外，去年 X.Org Server 的許多提交都是為了安全修復。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-b87e1a0baefae75b023d31ef6fe6bafe102.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0102/111918_vgsn_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;就代碼行數而言，X.Org Server 多年來基本處於停滯狀態，唯一的主要功能工作是圍繞 XWayland 進行的。&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.phoronix.com%2Fmisc%2Fxserver-eoy-2024%2Findex.html&quot; target=&quot;_blank&quot;&gt;點此查看 X.Org Server 的更多 GitStats 數據&lt;/a&gt;。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/327607/xorg-server-2024-gitstats</link>
            <guid isPermaLink="false">https://www.oschina.net/news/327607/xorg-server-2024-gitstats</guid>
            <pubDate>Tue, 31 Dec 2024 03:19:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>Star 超 9 k，AI 小白也能玩轉企業 LLM 應用開發</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                    
                                                                                                                                                    &lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:#333333&quot;&gt;現在大模型 workflow 產品有很多，比如 BISHENG 、MaxKB、Dify、FastGPT、RagFlow 等等。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:#333333&quot;&gt;其中，畢昇 BISHENG 是一個開源的&lt;/span&gt;&lt;span style=&quot;background-color:#ffffff; color:#40485b&quot;&gt;大模型應用開發平台，專門&lt;/span&gt;&lt;span style=&quot;background-color:#ffffff; color:#333333&quot;&gt;面向企業場景，具備高精度文檔解析 ETL4LLM 能力。自去年 8 月份開源以來， GitHub 上的 Star 數已經超過 9k 了。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;background-color:#ffffff; color:#333333&quot;&gt;對於 AI 小白來説，畢昇 BISHENG &lt;/span&gt;平台也很友好。&lt;/strong&gt;因為平台上有很多現成的模板，開發一個大模型應用簡直不要太簡單！編程、開發測試、合同審核、招投標、高級翻譯、會議紀要等等，只需要根據自己需求，調整參數就能用。全程都是可視化界面操作。&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;img height=&quot;788&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-f6995a9ece3a8a4f407abd152f1b17f921d.png&quot; width=&quot;1015&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;1 月 3 日，OSCHINA 開源中國將邀請 BISHENG 產品負責人魯力，做客直播欄目《開源項目老牌與新秀》第 3 期，分享其對 BISHENG—— 大模型 workflow 產品設計思考、技術方案。談一談它，與 Dify、Coze 有什麼差異。同時，還將分享他在企業大模型應用落地過程中踩過的那些坑。&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;strong&gt;本次直播中有一個重點環節，就是實操演示 —— 手把手教你怎麼用 BISHENG 搭建各種大模型應用，例如合同審核、報告生成等。想體驗的小夥伴可以點擊鏈接：&lt;/strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbisheng.dataelem.com%2F&quot; target=&quot;_blank&quot;&gt;https://bisheng.dataelem.com/&lt;/a&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;微信掃碼，趕緊預約直播吧~&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;img height=&quot;2388&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-d72821bf78b0b3f33f49b2d818702279e20.png&quot; width=&quot;600&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;strong&gt;另外，本次直播得到了諸多社區或組織的大力支持，在此特別表示感謝：&lt;/strong&gt;&lt;/p&gt; 
&lt;div style=&quot;text-align:left&quot;&gt; 
 &lt;div&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;strong&gt;Gitee&lt;/strong&gt;&lt;/p&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;Gitee（碼雲）是開源中國於 2013 年推出的基於 Git 的代碼託管平台、企業級研發效能平台，提供中國本土化的代碼託管服務。&lt;/p&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;目前，Gitee 已經有超過 1200 萬名開發者，累計託管超過 2800 萬個代碼倉庫，是中國境內規模最大的代碼託管平台。同時，旗下企業級 DevOps 研發效能管理平台 Gitee 企業版已服務超過 30 萬家企業。&lt;/p&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;網址：&lt;a href=&quot;https://gitee.com/&quot;&gt;https://gitee.com/&lt;/a&gt;&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;渠成開源社區&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;span&gt;&lt;span&gt;渠成開源社區由禪道項目管理軟件團隊發起，社區的經營主體為青島渠成開源計算機網絡技術研究中心，是非營利性社會服務活動的社會組織。 渠成開源社區主要面向一線開源軟件生產者、貢獻者、組織者、贊助商和用戶，以解決具體實際問題為宗旨，旨在打造以開源軟件為核心紐帶的開源生態系統，真正做到讓每一個優秀的開源軟件都能實現商業化。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;span&gt;&lt;span&gt;官網：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fwww.qucheng.cc&quot; target=&quot;_blank&quot;&gt;www.qucheng.cc&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;hr&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;「&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;開源項目老牌與新秀&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;」&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;是開源中國 OSCHINA 推出的一檔直播欄目，旨在為開源項目提供一個展示平台，每週五晚上開播&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;欄目邀請開源項目的作者、核心團隊成員或資深用戶作為嘉賓，通過路演式直播分享項目的亮點和經驗，有助於提高項目的知名度，吸引更多的用戶和開發者關注。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span&gt;&lt;span style=&quot;background-color:#ffffff&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;如果你手上也有好的開源項目，想要跟同行交流分享，歡迎聯繫我，欄目隨時開放～&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:center&quot;&gt;&lt;img height=&quot;537&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-aeeec97bc896bc61c715698bd6acc1f83b3.jpg&quot; width=&quot;400&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/u/3859945/blog/16971110</link>
            <guid isPermaLink="false">https://my.oschina.net/u/3859945/blog/16971110</guid>
            <pubDate>Tue, 31 Dec 2024 03:09:00 GMT</pubDate>
            <author>原創</author>
        </item>
        <item>
            <title>Altman 公佈 OpenAI 2025 年將發佈的技術產品</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span&gt;OpenAI 首席執行官薩姆·奧特曼（Sam Altman）發帖公佈了該公司 2025 年即將發佈的技術產品，分別是：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span&gt;AGI（通用人工智能）、Agents（智能體）、更好的 GPT-4o 升級版、更好的記憶存儲、更長的上下文窗口、「Grow up mode」（成人模式）、深度研究特色功能、更好的 Sora 以及更好的個性化定製。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img height=&quot;418&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-8b88947df326c888e90ad352129eac9d6b6.jpg&quot; width=&quot;300&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/327323</link>
            <guid isPermaLink="false">https://www.oschina.net/news/327323</guid>
            <pubDate>Tue, 31 Dec 2024 03:09:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>Star 超 9 k，AI 小白也能玩轉企業 LLM 應用開發</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                    
                                                                                                                                                    &lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:#333333&quot;&gt;現在大模型 workflow 產品有很多，比如 BISHENG 、MaxKB、Dify、FastGPT、RagFlow 等等。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:#333333&quot;&gt;其中，畢昇 BISHENG 是一個開源的&lt;/span&gt;&lt;span style=&quot;background-color:#ffffff; color:#40485b&quot;&gt;大模型應用開發平台，專門&lt;/span&gt;&lt;span style=&quot;background-color:#ffffff; color:#333333&quot;&gt;面向企業場景，具備高精度文檔解析 ETL4LLM 能力。自去年 8 月份開源以來， GitHub 上的 Star 數已經超過 9k 了。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;background-color:#ffffff; color:#333333&quot;&gt;對於 AI 小白來説，畢昇 BISHENG &lt;/span&gt;平台也很友好。&lt;/strong&gt;因為平台上有很多現成的模板，開發一個大模型應用簡直不要太簡單！編程、開發測試、合同審核、招投標、高級翻譯、會議紀要等等，只需要根據自己需求，調整參數就能用。全程都是可視化界面操作。&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;img height=&quot;788&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-f6995a9ece3a8a4f407abd152f1b17f921d.png&quot; width=&quot;1015&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;1 月 3 日，OSCHINA 開源中國將邀請 BISHENG 產品負責人魯力，做客直播欄目《開源項目老牌與新秀》第 3 期，分享其對 BISHENG—— 大模型 workflow 產品設計思考、技術方案。談一談它，與 Dify、Coze 有什麼差異。同時，還將分享他在企業大模型應用落地過程中踩過的那些坑。&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;strong&gt;本次直播中有一個重點環節，就是實操演示 —— 手把手教你怎麼用 BISHENG 搭建各種大模型應用，例如合同審核、報告生成等。想體驗的小夥伴可以點擊鏈接：&lt;/strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbisheng.dataelem.com%2F&quot; target=&quot;_blank&quot;&gt;https://bisheng.dataelem.com/&lt;/a&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;微信掃碼，趕緊預約直播吧~&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;img height=&quot;2388&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-d72821bf78b0b3f33f49b2d818702279e20.png&quot; width=&quot;600&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;strong&gt;另外，本次直播得到了諸多社區或組織的大力支持，在此特別表示感謝：&lt;/strong&gt;&lt;/p&gt; 
&lt;div style=&quot;text-align:left&quot;&gt; 
 &lt;div&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;strong&gt;Gitee&lt;/strong&gt;&lt;/p&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;Gitee（碼雲）是開源中國於 2013 年推出的基於 Git 的代碼託管平台、企業級研發效能平台，提供中國本土化的代碼託管服務。&lt;/p&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;目前，Gitee 已經有超過 1200 萬名開發者，累計託管超過 2800 萬個代碼倉庫，是中國境內規模最大的代碼託管平台。同時，旗下企業級 DevOps 研發效能管理平台 Gitee 企業版已服務超過 30 萬家企業。&lt;/p&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;網址：&lt;a href=&quot;https://gitee.com/&quot;&gt;https://gitee.com/&lt;/a&gt;&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;渠成開源社區&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;span&gt;&lt;span&gt;渠成開源社區由禪道項目管理軟件團隊發起，社區的經營主體為青島渠成開源計算機網絡技術研究中心，是非營利性社會服務活動的社會組織。 渠成開源社區主要面向一線開源軟件生產者、貢獻者、組織者、贊助商和用戶，以解決具體實際問題為宗旨，旨在打造以開源軟件為核心紐帶的開源生態系統，真正做到讓每一個優秀的開源軟件都能實現商業化。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;span&gt;&lt;span&gt;官網：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fwww.qucheng.cc&quot; target=&quot;_blank&quot;&gt;www.qucheng.cc&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;hr&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;「&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;開源項目老牌與新秀&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;」&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;是開源中國 OSCHINA 推出的一檔直播欄目，旨在為開源項目提供一個展示平台，每週五晚上開播&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;欄目邀請開源項目的作者、核心團隊成員或資深用戶作為嘉賓，通過路演式直播分享項目的亮點和經驗，有助於提高項目的知名度，吸引更多的用戶和開發者關注。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span&gt;&lt;span style=&quot;background-color:#ffffff&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;如果你手上也有好的開源項目，想要跟同行交流分享，歡迎聯繫我，欄目隨時開放～&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:center&quot;&gt;&lt;img height=&quot;537&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-aeeec97bc896bc61c715698bd6acc1f83b3.jpg&quot; width=&quot;400&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/u/3859945/blog/16971110</link>
            <guid isPermaLink="false">https://my.oschina.net/u/3859945/blog/16971110</guid>
            <pubDate>Tue, 31 Dec 2024 03:09:00 GMT</pubDate>
            <author>原創</author>
        </item>
        <item>
            <title>另闢蹊徑打造高精度的 VL 文字提取工具</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                    
                                                                                                                                                    &lt;span id=&quot;OSC_h2_1&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;一、應用場景：設備銘牌識別環境&lt;/h2&gt; 
&lt;p&gt;在工業和醫療等領域的設備管理中，設備銘牌的信息提取是一項常見但極具挑戰性的任務。這些銘牌通常包含關鍵信息如型號、序列號、製造日期等，對於資產管理、維護記錄和故障排除至關重要。然而，實際拍攝的照片往往存在以下難題：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;角度與透視問題&lt;/strong&gt;：由於拍攝角度各異，上傳的照片中設備銘牌可能出現嚴重的透視變形，導致文字傾斜或扭曲。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;光線與反射干擾&lt;/strong&gt;：現場光線條件複雜，強光反射或陰影遮擋使得銘牌上的文字難以清晰辨認。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;背景雜亂&lt;/strong&gt;：周圍環境複雜，背景中的其他物體可能幹擾文本區域的識別。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;這些問題直接使用視覺語言（VL）模型進行識別時，會導致極低的準確率和可靠性。為瞭解決這些問題，最近和 Gitee AI 團隊進行了深度友好的溝通，最終得到了一套完整的解決方案，通過 UVDoc 圖像校正工具預處理圖片，再利用 QwenVL 進行信息識別，並最終使用大型語言模型（LLM）實現結構化數據提取，顯著提升了銘牌文字的提取效果。&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;202&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-f0fe8c7dbf79cea800ebd5fd45eacd503ea.jpg&quot; width=&quot;250&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&amp;nbsp;&amp;nbsp;&lt;img alt=&quot;&quot; height=&quot;202&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-c0a5b16fd9f82586a0a5e5b09bb17c64735.jpg&quot; width=&quot;141&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&amp;nbsp;&lt;img alt=&quot;&quot; height=&quot;202&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-4f047384ddb6dfa3a43fd16be34037953c4.jpg&quot; width=&quot;269&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&amp;nbsp;&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_2&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;二、技術方法&lt;/h2&gt; 
&lt;span id=&quot;OSC_h3_3&quot;&gt;&lt;/span&gt; 
&lt;h3&gt;1. UVDoc 圖像校正工具：提升輸入質量&lt;/h3&gt; 
&lt;p&gt;針對上述難題，我們首先採用 GiteeAI 團隊最新發布的&lt;a href=&quot;https://ai.gitee.com/serverless-api?model=UVDoc&quot; rel=&quot;nofollow&quot;&gt;UVDoc 圖像校正工具&lt;/a&gt;對原始照片進行預處理。該工具利用先進的計算機視覺算法，自動檢測並糾正圖像中的透視變形，恢復銘牌的真實形狀。同時，它還可以調整圖像的亮度和對比度，減少光線和反射帶來的幹擾。經過 UVDoc 校正後的圖像不僅提高了文本的可讀性，還為後續的文字識別提供了更佳的基礎。&lt;br&gt; 地址：&lt;a href=&quot;https://ai.gitee.com/serverless-api?model=UVDoc&quot; rel=&quot;nofollow&quot;&gt;https://ai.gitee.com/serverless-api?model=UVDoc&lt;/a&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h3_4&quot;&gt;&lt;/span&gt; 
&lt;h3&gt;2. QwenVL：強大的信息識別引擎&lt;/h3&gt; 
&lt;p&gt;完成圖像預處理後，接下來是關鍵的信息識別階段。我們選擇了 QwenVL 作為核心識別引擎，其融合了最新的視覺語言模型技術，能夠在複雜背景條件下精準定位並識別出文本內容。QwenVL 不僅可以處理常規印刷體文字，還能應對手寫體以及多種語言混合的情況，極大地拓寬了應用範圍。此外，QwenVL 還支持多模態輸入，可以同時解析圖像中的其他非文本元素，如圖標、表格等，為用戶提供更加全面的信息提取服務。&lt;br&gt; 地址：&lt;a href=&quot;https://ai.gitee.com/serverless-api?model=Qwen2-VL-72B&quot; rel=&quot;nofollow&quot;&gt;https://ai.gitee.com/serverless-api?model=Qwen2-VL-72B&lt;/a&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h3_5&quot;&gt;&lt;/span&gt; 
&lt;h3&gt;3. LLM 結構化數據提取：智能化處理結果&lt;/h3&gt; 
&lt;p&gt;最後一步是將 QwenVL 輸出的結果進一步轉化為結構化的數據格式。這一步驟依賴於 Qwen2.5-72B-Instruct，它具備強大的自然語言理解能力，可以從非結構化的文本中抽取出有價值的結構化信息。例如，在設備銘牌識別場景中，Qwen2.5-72B-Instruct 可以自動識別並分類不同的字段，如型號、序列號、製造日期等；同時生成易於檢索和分析的結構化數據，極大地方便了後續的數據管理和應用。&lt;br&gt; 地址：&lt;a href=&quot;https://ai.gitee.com/serverless-api?model=Qwen2.5-72B-Instruct&quot; rel=&quot;nofollow&quot;&gt;https://ai.gitee.com/serverless-api?model=Qwen2.5-72B-Instruct&lt;/a&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_6&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;三、結果展示&lt;/h2&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;img height=&quot;957&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-e1ac616d8505ee8718c66b5aa216a66688f.png&quot; width=&quot;1370&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;為了驗證我們的方案的有效性，我們進行了實驗，共同處理了 30 張具有不同角度和光線條件的設備銘牌照片。實驗分為兩組：一組直接使用 QwenVL 進行識別（直接 VL 組），另一組先使用 UVDoc 工具預處理後再使用 QwenVL 識別（聯合處理組）。以下是兩組的數據對比及更深入的統計分析：&lt;/p&gt; 
&lt;span id=&quot;OSC_h3_7&quot;&gt;&lt;/span&gt; 
&lt;h3&gt;（一）數據對比表&lt;/h3&gt; 
&lt;table&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;th&gt;識別情況&lt;/th&gt; 
   &lt;th&gt;直接 VL 組 (張)&lt;/th&gt; 
   &lt;th&gt;聯合處理組 (張)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;正確識別&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;28&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;部分識別&lt;/td&gt; 
   &lt;td&gt;12&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;完全不能識別&lt;/td&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;總計&lt;/td&gt; 
   &lt;td&gt;30&lt;/td&gt; 
   &lt;td&gt;30&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;span id=&quot;OSC_h3_8&quot;&gt;&lt;/span&gt; 
&lt;h3&gt;（二）進一步統計分析&lt;/h3&gt; 
&lt;p&gt;最近我在做科研項目，所以簡單按照科研項目的分析邏輯做了一下進一步的數據分析，相關內容就截圖了。&lt;/p&gt; 
&lt;span id=&quot;OSC_h4_9&quot;&gt;&lt;/span&gt; 
&lt;h4&gt;1. 準確率提升&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;正確識別率&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;直接 VL 組：8/30 = 26.7%&lt;/li&gt; 
   &lt;li&gt;聯合處理組：28/30 = 93.3%&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;部分識別率&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;直接 VL 組：12/30 = 40%&lt;/li&gt; 
   &lt;li&gt;聯合處理組：2/30 = 6.7%&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;完全不能識別率&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;直接 VL 組：10/30 = 33.3%&lt;/li&gt; 
   &lt;li&gt;聯合處理組：0/30= 0%&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id=&quot;OSC_h4_10&quot;&gt;&lt;/span&gt; 
&lt;h4&gt;2. 平均準確度&lt;/h4&gt; 
&lt;p&gt;平均準確度定義為每個樣本被正確識別的比例。計算方法如下：&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-196c8ee868b55cd221c9cb66160298a7693.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;直接 VL 組平均準確度：8/30 = 26.7%&lt;/li&gt; 
 &lt;li&gt;聯合處理組平均準確度：28/30 = 93.3%&lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id=&quot;OSC_h4_11&quot;&gt;&lt;/span&gt; 
&lt;h4&gt;3. Kappa 係數（Cohen&#39;s Kappa）&lt;/h4&gt; 
&lt;p&gt;Kappa 係數用於衡量分類系統的可靠性，考慮了偶然一致性。其公式為：&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-f2688c169d186ec03283ab51948f08da1ec.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Kappa 係數表明聯合處理組的一致性遠高於直接 VL 組，説明前者在實際應用中更為可靠。&lt;/p&gt; 
&lt;span id=&quot;OSC_h3_12&quot;&gt;&lt;/span&gt; 
&lt;h3&gt;四、結論&lt;/h3&gt; 
&lt;p&gt;從以上數據分析可以看出，聯合處理組的表現顯著優於直接 VL 組：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;準確性大幅提升&lt;/strong&gt;：聯合處理組的正確識別率從 26.7% 提高到了 93.3%，幾乎達到了完全正確識別。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;部分識別減少&lt;/strong&gt;：聯合處理組部分識別的比例從 40% 降低到 6.7%，表明大多數情況下都能實現完全正確的識別。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;無法識別消除&lt;/strong&gt;：聯合處理組實現了零失敗，所有照片均能至少部分識別，而直接 VL 組有 10 張照片完全不能識別。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;可靠性更高&lt;/strong&gt;：Kappa 係數顯示聯合處理組的一致性遠高於直接 VL 組，證明瞭其在實際應用中的優越性能。&lt;br&gt; &amp;nbsp;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;img height=&quot;514&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-e116320452808f0eb70d24e1a6e9b74d503.png&quot; width=&quot;1781&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;綜上所述，在高精度的圖文識別場景中，通過 UVDoc 圖像校正工具預處理圖片、QwenVL 進行信息識別以及 LLM 進行結構化數據提取，成功解決了設備銘牌識別中的難題，構建了一個高效且精確的文字提取系統。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/bojinzhu/blog/17003148</link>
            <guid isPermaLink="false">https://my.oschina.net/bojinzhu/blog/17003148</guid>
            <pubDate>Tue, 31 Dec 2024 02:36:00 GMT</pubDate>
            <author>原創</author>
        </item>
        <item>
            <title>百度網頁版新增「AI 搜」功能，基於文心大模型打造</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;百度近日在百度搜索 Web 端首頁上線了百度「AI 搜」入口，「AI 搜」基於原百度搜索 AI 夥伴改版升級而來，在此前的基礎上做功能升級。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0102/100753_fgNl_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;據介紹，百度「AI 搜」是基於百度文心大模型打造的桌面端 AI 搜索引擎，目前內容側已經打通百度搜索引擎、百度健康、百度律臨、百度文庫、百度教育等內容生態，可確保搜索結果可靠、權威。&lt;/p&gt; 
&lt;p&gt;目前百度「AI 搜」主要提供包括話題探索、問題解決、決策輔助、知識答疑、主題研究、學習創作等功能，覆蓋文生圖、文生文、邏輯推理、多輪對話、智能摘要、AI 修圖等 AI 技術。&lt;/p&gt; 
&lt;p&gt;此外，百度「AI 搜」也提供了文心智能體入口，在對話框中可通過@方式與不同智能體進行交互，方便用戶使用和創建智能體。&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;&lt;span&gt;更多獨家技術見解與熱門話題討論，盡在&lt;a href=&quot;https://www.oschina.net/app&quot;&gt;【開源中國 APP】&lt;/a&gt;，與數百萬開發者一起，隨時隨地探索技術無限可能。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/327593</link>
            <guid isPermaLink="false">https://www.oschina.net/news/327593</guid>
            <pubDate>Tue, 31 Dec 2024 02:09:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>你好，2025！</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:#3e3e3e&quot;&gt;飛致雲恭祝大家新年快樂！&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;1920&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-cc8718206c51fbfc57a88fe25d62b1bbcee.jpg&quot; width=&quot;1081&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/327509</link>
            <guid isPermaLink="false">https://www.oschina.net/news/327509</guid>
            <pubDate>Mon, 30 Dec 2024 06:24:00 GMT</pubDate>
            <author>來源: 投稿</author>
        </item>
    </channel>
</rss>