<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - news - 繁體中文（台灣）</title>
    <link>https://www.oschina.net/news/project</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news" rel="self" type="application/rss+xml"/>
    <description>已對該 RSS 進行格式化操作：中英字符之間插入空格、使用直角引號、標點符號修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-tw</language>
    <lastBuildDate>Tue, 09 Sep 2025 02:42:49 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>字節 Seedream 4.0 圖像創作模型正式發佈</title>
      <description/>
      <link>https://www.oschina.net/news/371058</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371058</guid>
      <pubDate>Tue, 09 Sep 2025 02:38:32 GMT</pubDate>
    </item>
    <item>
      <title>知名 Android 第三方桌面 Nova Launcher 將停止維護</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;知名 Android 第三方桌面&lt;span&gt;啓動器 Nova Launcher 創始人和原始開發者 Kevin Barry 宣佈，他已經離開收購 Nova Launcher 的分析公司 Branch，並不再參與該項目。&lt;/span&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;img height="1658" src="https://static.oschina.net/uploads/space/2025/0909/103239_Hnwx_2720166.png" width="1502" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;https://teslacoilapps.com/nova/solong.html&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;據悉，Nova Launcher 由 Kevin Barry 帶隊開發，於 2022 年被 Branch 收購。當時，Branch 承諾不會將 Nova Launcher 變為訂閲式付費、帶有廣告的普通 Android 桌面啓動器。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-d68adf6667956ed5f537c0b8b93ebbb0b93.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;據 Kevin Barry 透露，其在過去幾個月不斷為 Nova Launcher 的開源進行付出。其表示，雖然 Branch 曾在收購 Nova Launcher 時承諾，其若離職，Nova Launcher 最終則會開源，但 Barry 現被要求停止開發 Nova Launcher 和終止進行開源工作。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371057</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371057</guid>
      <pubDate>Tue, 09 Sep 2025 02:35:32 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>阿里通義發佈語音識別模型 Qwen3-ASR-Flash</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;通義千問系列最新的語音識別模型 Qwen3-ASR-Flash 已正式發佈，它基於 Qwen3 基座模型，經海量多模態數據以及千萬⼩時規模的 ASR（自動語音識別）數據訓練構建而成。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0909/101857_EGZg_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Qwen3-ASR-Flash 實現了⾼精度⾼魯棒性的語⾳識別性能，⽀持 11 種語⾔和多種⼝⾳。與眾不同的是，Qwen3-ASR-Flash⽀持⽤戶以任意格式提供⽂本上下⽂，從⽽獲得定製化的 ASR 結果，同時還⽀持歌聲識別。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0909/101903_kNR1_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img height="664" src="https://static.oschina.net/uploads/space/2025/0909/101933_MOCR_2720166.png" width="1280" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="1395" src="https://static.oschina.net/uploads/space/2025/0909/101944_O6J2_2720166.png" width="1280" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Qwen3-ASR-Flash 單模型支持多種語言、方言和口音的精準轉錄：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;中文：包括普通話以及四川話、閩南語、吳語、粵語等主要方言。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;英語：支持英式、美式及多種其他地區口音。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;其他支持語言：法語、德語、俄語、意大利語、西班牙語、葡萄牙語、日語、韓語和阿拉伯語。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Qwen3-ASR-Flash 的核心特性：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;領先的識別準確率：Qwen3-ASR-Flash 在多箇中英文，多語種 benchmark 測試中表現最優。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;驚豔的歌聲識別能力：支持歌唱識別,包括清唱與帶 bgm 的整歌識別，實測錯誤率低於 8%。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;定製化識別：用戶可以以任意格式 (如詞彙表、段落或完整文檔) 提供背景文本，模型能智能利用該上下文識別並匹配命名實體和其他關鍵術語，輸出定製化的識別結果。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;語種識別與非人聲拒識：模型能精確分辨語音的語種，自動過濾非語音片段，包括靜音和背景噪聲。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;魯棒性：面對長難句、句中語言切換和重複詞語等困難文本模式，以及在複雜的聲學環境中，模型仍能保持高準確率。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;體驗方式：&lt;/p&gt; 
&lt;p&gt;ModelScope&lt;strong&gt;：&lt;/strong&gt;https://modelscope.cn/studios/Qwen/Qwen3-ASR-Demo&lt;/p&gt; 
&lt;p&gt;HuggingFace:&amp;nbsp;https://huggingface.co/spaces/Qwen/Qwen3-ASR-Demo&lt;/p&gt; 
&lt;p&gt;阿里雲百鍊 API&lt;strong&gt;：&lt;/strong&gt;https://bailian.console.aliyun.com/?tab=doc#/doc/?type=model&amp;amp;url=2979031&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371054</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371054</guid>
      <pubDate>Tue, 09 Sep 2025 02:21:32 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>deepin 截圖錄屏智能存儲上線，AI 大招在路上</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span&gt;deepin 截圖錄屏作為大家日常使用頻率 Top 級應用，之前收到了很多用戶的真誠反饋和寶貴建議，感謝大家的積極參與和建言獻策。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span&gt;目前，&lt;/span&gt;&lt;strong&gt;隨着 deepin 25.0.7 版本的更新，deepin 截圖錄屏新功能也上線啦！&lt;/strong&gt;&lt;span&gt;本文將為大家詳細介紹本次更新的具體內容，並透露一下 deepin 截圖錄屏在後續的產品功能規劃，一起來看看吧。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;新增功能：智能區分存儲方式&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span&gt;在之前的反饋中，很多用戶提出了對&lt;/span&gt;&lt;strong&gt;截圖存儲方式&lt;/strong&gt;&lt;span&gt;進行優化的需求，希望能夠將「保存至本地」和「保存至剪貼板」功能區分開來。本次版本便實現了此功能，如下所示：&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;strong&gt;點擊「√」（或雙擊截圖區域、按回車等），&lt;/strong&gt;&lt;span&gt;截圖自動複製到剪貼板，方便直接粘貼使用；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;div&gt; 
   &lt;p style="text-align:left"&gt;&lt;img align="left" src="https://oscimg.oschina.net/oscnet//0f8adb194cc09a47ebccc062f818f2b1.jpg" width="865" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
 &lt;p&gt;&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;span&gt;點擊「保存」，則將圖片存儲至本地，方便後續查找與管理。&lt;/span&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
  &lt;p style="text-align:left"&gt;&lt;img align="left" src="https://oscimg.oschina.net/oscnet//f26dfdac799761b692f5066427950a1b.jpg" width="865" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;p style="text-align:left"&gt;&lt;span&gt;但如果你不想每次存圖都選路徑，別擔心——舊版本中「固定文件夾存儲」的便捷方式我們也做了保留！&lt;/span&gt;&lt;/p&gt; 
  &lt;p style="text-align:left"&gt;&lt;span&gt;更貼心的是，此次版本在「設置」中新增了「保存方式」選項：&lt;/span&gt;&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt;&lt;span&gt;勾選「每次詢問」：每次保存時均可自主選擇文件夾；&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span&gt;勾選「指定位置」並選擇具體位置：截圖將自動存入預設文件夾，不再詢問，省心省力。&lt;/span&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
  &lt;p style="text-align:left"&gt;&lt;img align="left" src="https://oscimg.oschina.net/oscnet//6b16bcccdde6f02ed4b186f5ebe954fb.jpg" width="648" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
  &lt;p style="text-align:left"&gt;&lt;span&gt;注意：設置內的「保存方式」僅對「保存到本地」場景生效，不會影響複製剪貼板的相關操作。&lt;/span&gt;&lt;/p&gt; 
  &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
  &lt;p style="text-align:left"&gt;&lt;span&gt;此外，deepin 截圖錄屏還支持「智能打碼、OCR 文字提取、貼圖置頂、滾動長截圖」等實用功能。更多關於 deepin 截圖錄屏的詳細使用介紹可參考：&lt;/span&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FAzxuEFQAJT-SZaG5BsDPww" target="_blank"&gt;&lt;span&gt;《霸榜用戶最愛，deepin 截圖錄屏為何穩坐 Top 1 寶座？》&lt;/span&gt;&lt;/a&gt;&lt;/p&gt; 
  &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
  &lt;p style="text-align:center"&gt;&lt;strong&gt;&lt;strong&gt;搶先看：後續產品規劃&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;p style="text-align:left"&gt;&lt;strong&gt;&lt;strong&gt;OCR 體驗優化與文字識別模型升級&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
  &lt;p style="text-align:left"&gt;&lt;span&gt;我們關注到用戶反饋的 OCR 使用體驗問題，特別是在文字識別時需要先保存圖片的環節。針對這一問題，我們已制定優化方案，後續會將 OCR 識別邏輯改為"複製到剪貼板"，&lt;/span&gt;&lt;strong&gt;&lt;strong&gt;實現無需保存本地圖片即可識別文字&lt;/strong&gt;&lt;/strong&gt;&lt;span&gt;，讓操作更加流暢。&lt;/span&gt;&lt;/p&gt; 
  &lt;p style="text-align:left"&gt;&lt;span&gt;同時，我們也將對 OCR 文字識別模型進行升級，大幅提升識別準確率，為大家帶來更好的使用體驗。&lt;/span&gt;&lt;/p&gt; 
  &lt;p style="text-align:left"&gt;&lt;span&gt;注：目前最新版截圖錄屏中，如果您希望避免每次 OCR 識別時彈窗詢問保存位置，可以進入"設置"-"保存方式"，選擇"指定位置"來提升使用體驗。&lt;/span&gt;&lt;/p&gt; 
  &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
  &lt;p style="text-align:left"&gt;&lt;strong&gt;&lt;strong&gt;截圖錄屏集成 AI 能力&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;p style="text-align:left"&gt;&lt;span&gt;UOS AI 將接入視覺處理大模型，支持識別圖片、基於視覺推理，後續截圖錄屏也將擁有識別圖片的能力。&lt;/span&gt;&lt;/p&gt; 
  &lt;p style="text-align:left"&gt;&lt;span&gt;你可以對屏幕上的任意內容進行截圖，並直接向 AI 提問！具體功能還在緊張設計中，更多驚喜，敬請期待～&lt;/span&gt;&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371053</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371053</guid>
      <pubDate>Tue, 09 Sep 2025 02:15:37 GMT</pubDate>
      <author>來源: 資訊</author>
    </item>
    <item>
      <title>Databricks 融資 10 億美元，估值超 1000 億美元</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;Databricks &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.databricks.com%2Fcompany%2Fnewsroom%2Fpress-releases%2Fdatabricks-surpasses-4b-revenue-run-rate-exceeding-1b-ai-revenue" target="_blank"&gt;宣佈&lt;/a&gt;即將完成 10 億美元的 K 輪融資，對應估值超過 1000 億美元。此輪融資由 Andreessen Horowitz、Insight Partners、MGX、Thrive Capital 和 WCM Investment Management 共同領投。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;Databricks 表示，將利用這筆新資金加速其 AI 戰略——擴展 Agent Bricks，推出全新 Lakebase 產品線，並推動全球增長。以及支持 Databricks 未來的 AI 收購，並深化 AI 研究。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="245" src="https://oscimg.oschina.net/oscnet/up-ba2a1094a2ce8345cb7359294aa377ea3d0.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在公告中，Databricks 還透露了部分財務狀況，披露其第二季度的年收入運行率超過 40 億美元，同比增長 50%，並在過去 12 個月中實現了正自由現金流。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;該公司還表示，其人工智能產品的年營收運行率近期已超過 10 億美元，淨留存率超過 140%，目前有超過 650 家客戶使用 Databricks 的產品，年收入超過 100 萬美元。目前，共有超過 2 萬家企業和組織在使用其軟件。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;Databricks 聯合創始人兼首席執行官 Ali Ghodsi 在公告中表示：「我們的團隊正在構建企業未來幾十年將依賴的數據和 AI 基礎設施，從而取得這些成果。有了這筆新資金，我們將能夠加快 Agent Bricks 的發展步伐，幫助各行各業的客戶將其數據轉化為生產級 AI 代理，並在創建新的 Lakebase 類別、為 AI 代理重塑數據庫的過程中獲得更大的發展動力。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;Databricks 還指出，在前兩個季度中，該公司已與微軟、谷歌雲、Anthropic、SAP 和 Palantir 建立或擴大了合作伙伴關係。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371049</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371049</guid>
      <pubDate>Tue, 09 Sep 2025 02:08:32 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>🔥🔥造物分享：流浪地球 550W（MOSS）小智 AI 生態中樞</title>
      <description/>
      <link>https://www.oschina.net/ai-creation/details/2186</link>
      <guid isPermaLink="false">https://www.oschina.net/ai-creation/details/2186</guid>
      <pubDate>Tue, 09 Sep 2025 01:49:32 GMT</pubDate>
    </item>
    <item>
      <title>李彥宏頒發「百度最高獎」：心流團隊獲 100 萬美元獎勵</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;今日，百度創始人李彥宏在內部活動上為技術團隊頒發「百度最高獎」，獲獎團隊得到 100 萬美元獎勵，合人民幣超 700 萬元。「百度最高獎」已歷經 15 屆，語音識別、深度學習平台、大模型等大量 AI 技術均曾獲獎，獎金總金額將近 4 億元人民幣。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-322339010570111171fc427256170f32b22.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;據瞭解，「百度最高獎」於 2010 年 7 月設立，鼓勵「小團隊做出大事業」，是百度公司最高級別的獎項，給予每個獲獎團隊 100 萬美元獎勵。獎項評選需滿足三項條件：項目意義重大；成果遠超預期；團隊足夠小，必須是小於等於 10 人。&lt;/p&gt; 
&lt;p&gt;本次百度最高獎的獲獎團隊為「心流」團隊。據介紹，「心流」團隊率先實現了端到端的多模態內容理解與序列生成技術。李彥宏在頒獎時表示，到今天，模型發展已經非常接近臨界點，很快就會有各種有價值的應用被創造出來，「我們生活在一個非常令人興奮、非常令人期待的環境當中」。&lt;/p&gt; 
&lt;p&gt;李彥宏稱，百度搜索已有近 70% 結果含有 AI 生成內容，且通過「百看」帶來富媒體形式，是全球所有的搜索引擎當中改造最激進的，這也代表搜索引擎的未來。&lt;/p&gt; 
&lt;p&gt;同時，百度慧博星數字人已達到「以假亂真」的地步，「很多人看不出是數字人還是真人」；百度蘿蔔快跑已覆蓋全球 16 座城市，代表着最新一代的無人駕駛技術。&lt;/p&gt; 
&lt;p&gt;頒獎典禮現場，李彥宏在談及 AI 發展時指出，「AI 大模型發展到今天，其實已接近了臨界點，很快就會有各種各樣非常有價值的應用能夠創造出來，我們正生活在一個非常令人興奮、非常令人期待的市場環境當中。」&lt;/p&gt; 
&lt;p&gt;「我們所從事的每一項工作都代表着未來，我也希望大家和我一起去期待，去迎接、去奮鬥出一個創新在 C 位的社會。」李彥宏表示。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370987</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370987</guid>
      <pubDate>Sat, 06 Sep 2025 11:28:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>「AI 教父」辛頓竟然被前女友竟用 ChatGPT 提分手</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;近日，被譽為「AI 教父」 的 Geoffrey Hinton 在接受採訪時&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.ft.com%2Fcontent%2F31feb335-4945-475e-baaa-3b880d9cf8ce" target="_blank"&gt;透露&lt;/a&gt;，他的前女友曾用 ChatGPT 給他發送分手信息。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1776" src="https://static.oschina.net/uploads/space/2025/0908/192243_YVS9_2720166.jpg" width="1280" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Hinton 作為人工智能領域的先驅，其在 1980 年代的工作為機器學習和人工神經網絡奠定了基礎，去年還獲得了諾貝爾物理學獎。&lt;/p&gt; 
&lt;p&gt;這位 AI 領域的權威人士卻未能預料到自己會被 AI 工具所「傷害」，他的前女友用 ChatGPT 告訴他他有多糟糕，讓他非常驚訝。「她用聊天機器人説出我的缺點，再傳給我。」不過辛頓自認沒有聊天機器人説的那麼糟，所以也沒有太難過。&lt;/p&gt; 
&lt;p&gt;事實上，讓像 ChatGPT 這樣的聊天機器人撰寫分手短信等似乎並不是什麼新鮮事，畢竟越來越多的人就一系列問題向 AI 諮詢。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370985</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370985</guid>
      <pubDate>Sat, 06 Sep 2025 11:23:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Agent Client Protocol —— 代碼編輯器與 Agent 的通信協議</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                            &lt;p&gt;Agent Client Protocol (ACP) 是用於連接代碼編輯器和 Agent 的協議，對代碼編輯器（用於查看和編輯源代碼的交互式程序）與編碼 Agent（使用生成式 AI 自主修改代碼的程序）之間的通信進行了標準化。&lt;/p&gt;

&lt;p&gt;這一協議讓開發者可以在編輯器中自由接入任意第三方智能體（Agent），無需依賴官方內置工具。其理念類似於語言服務器協議（LSP），通過解耦編輯器與 Agent 的交互方式，提供更靈活的擴展能力。&lt;/p&gt;

&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-2f621ad18024ec580d997b820ea9139346e.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

&lt;p&gt;ACP 協議已經以 Apache 開源許可證發佈，任何開發者都可基於它集成自己的 AI Agent。&lt;/p&gt;

                                                                    &lt;/div&gt;
                                                                </description>
      <link>https://www.oschina.net/p/agent-client-protocol</link>
      <guid isPermaLink="false">https://www.oschina.net/p/agent-client-protocol</guid>
      <pubDate>Sat, 06 Sep 2025 11:18:00 GMT</pubDate>
    </item>
    <item>
      <title>商湯日日新為 Claude API 用戶提供「搬家」大禮包</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;9 月 5 日，Anthropic 宣佈將禁止中資控股超過 50% 的公司使用 Claude 服務，並限制企業通過海外雲服務、第三方平台等方式間接使用。&lt;/p&gt; 
&lt;p&gt;即日起，商湯日日新大模型 SenseNova 將為 Claude 用戶提供「搬家」服務，幫助客戶繼續享受高質量的模型能力和服務。&lt;/p&gt; 
&lt;p&gt;相關模型詳情可訪問 platform.sensenova.cn 註冊。&lt;/p&gt; 
&lt;p&gt;商湯將為從 Claude 遷移到「日日新」的新用戶贈送 5000 萬 Tokens 體驗包；同時為用戶提供專屬搬家顧問，提供遷移系列培訓，讓新用戶入駐新家舒適順利。相關模型詳情可訪問 platform.sensenova.cn 註冊。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0908/185753_qsE0_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;商湯還提供最新交互模型——日日新 SenseNova V6.5 Omni API 的免費接入測試。用戶也可在應用商店下載「商量 APP」免費體驗！&lt;/p&gt; 
&lt;p&gt;另外，針對用戶對高質量的編程和 Agent 工具的需求，商湯小浣熊還將提供 300,000 元會員權益，所有用戶均可掃描文末二維碼領取。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370978</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370978</guid>
      <pubDate>Sat, 06 Sep 2025 10:58:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>英偉達推出通用深度研究（UDR）系統</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;英偉達&lt;span&gt;最新&lt;/span&gt;發佈另外一個通用深度研究（UDR）系統，目前仍處於原型階段。該系統不僅可以與任何大語言模型 (LLM) 兼容，更為用戶提供了高度定製的深度研究策略，徹底改變了以往研究智能體的工作方式。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;根據英偉達的&lt;span&gt;最新&lt;/span&gt;論文，UDR 系統的核心優勢在於其極強的靈活性。過去，深度研究智能體往往依賴硬編碼的方式，用戶只能使用固定的工具和策略進行研究，無法進行個性化調整。而 UDR 系統的推出，意味着用戶可以隨心所欲地創建、編輯和優化自己的研究策略，甚至無需進行額外的模型訓練。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="351" src="https://oscimg.oschina.net/oscnet/up-7461c3da8a2ceb3b17c20d0c5f84c7d2fba.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;UDR 系統配備了一個用戶友好的界面，方便用戶輸入研究提示，隨時更新進度並查看最終報告。與傳統的對話式 LLM 不同，UDR 能夠在研究過程中持續向用戶反饋進展，極大提升了研究效率。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;值得一提的是，UDR 系統在設計上將研究邏輯與語言模型解耦，使開發者能夠靈活選擇&lt;span&gt;最先&lt;/span&gt;進的 AI 模型，並將其與量身定製的研究方案結合使用。這種創新的組合方式，讓用戶能夠創造出更強大、更具適應性的深度研究工具。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;儘管 UDR 系統具有諸多優點，但仍存在一些需要改進的地方。系統的準確性依賴於底層 AI 模型生成代碼的質量，同時用戶設計的研究策略必須合理可行，否則可能導致生成的報告質量低下。此外，當前版本在執行過程中缺乏用戶幹預的能力，所有決策都需在研究開始前預設，限制了靈活性。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;研究人員已提出了進一步的改進方案，包括提供可修改的策略庫和更靈活的用戶控制功能。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370976</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370976</guid>
      <pubDate>Sat, 06 Sep 2025 10:39:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>rainfrog - 命令行數據庫工具</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                            &lt;p&gt;&lt;span style="background-color:#ffffff; color:#1f2328"&gt;Rainfrog 的目標是提供一個輕量級的、基於終端的數據庫交互工具。該項目目前處於測試階段。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;特性：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;通過類似 vim 的鍵綁定和鼠標控制實現高效導航&lt;/li&gt;
&lt;li&gt;具有關鍵字高亮顯示、會話歷史記錄和收藏夾的查詢編輯器&lt;/li&gt;
&lt;li&gt;快速複製數據、過濾表以及在模式之間切換&lt;/li&gt;
&lt;li&gt;查看錶元數據和屬性的快捷方式&lt;/li&gt;
&lt;li&gt;跨平台（macOS、linux、windows、android 通過 termux）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img alt="" height="278" src="https://static.oschina.net/uploads/space/2025/0905/115915_L0YY_4252687.gif" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

                                                                    &lt;/div&gt;
                                                                </description>
      <link>https://www.oschina.net/p/rainfrog</link>
      <guid isPermaLink="false">https://www.oschina.net/p/rainfrog</guid>
      <pubDate>Sat, 06 Sep 2025 10:11:00 GMT</pubDate>
    </item>
    <item>
      <title>騰訊混元翻譯模型 Hunyuan-MT-7B 登頂開源熱榜</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;騰訊混元&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F19W9SEUxq7nuYQvVJz8faA" target="_blank"&gt;宣佈&lt;/a&gt;，混元翻譯模型 Hunyuan-MT-7B 登頂 Hugging Face 模型趨勢榜第一位。官方表示，該模型和混元世界模型家族最新成員 HunyunWorld-Voyager 一起，拿下前三中的兩席。&lt;/p&gt; 
&lt;p&gt;Hunyuan-MT-7B 於 9 月 1 日開源，其總參數量僅 7B，支持 33 個語種、5 種民漢語言/方言互譯，是一個能力全面的輕量級翻譯模型。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1506" src="https://static.oschina.net/uploads/space/2025/0908/175938_Dkn8_2720166.png" width="1216" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在 8 月底結束的國際計算語言學協會（ACL）WMT2025 比賽中，Hunyuan-MT-7B（參賽名稱：Shy-hunyuan-MT）拿下了全部 31 個語種比賽中的 30 個第 1 名，處於絕對領先地位。&lt;/p&gt; 
&lt;p&gt;這 31 個語種除了中文、英語、日語等常見語種，也包含捷克語、馬拉地語、愛沙尼亞語、冰島語等小語種。&lt;/p&gt; 
&lt;p&gt;騰訊混元表示，在業界常用的翻譯能力測評數據集 Flores200 上，騰訊混元 Hunyuan-MT-7B 模型也有卓越的效果表現，明顯領先於同尺寸模型，與超大尺寸模型效果對比也不遜色。&lt;/p&gt; 
&lt;p&gt;針對翻譯場景，騰訊混元提出了一個完整的翻譯模型訓練範式，覆蓋從預訓練、到 CPT 再到監督調參、翻譯強化和集成強化全鏈條，使得模型的翻譯效果達到業界最優。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370969</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370969</guid>
      <pubDate>Sat, 06 Sep 2025 10:01:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>上海發佈 AI 廣告扶持政策：最高 500 萬補貼大模型</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span&gt;上海市近日發佈了《上海市支持人工智能賦能廣告業創新發展的若干措施》，旨在通過一系列具體的扶持政策，推動人工智能技術在廣告行業的深度應用和發展。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;strong&gt;核心扶持措施概覽&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;新政策的核心在於「AI+數字廣告」生產要素的強化支持，具體措施包括:&lt;/p&gt; 
&lt;ul style="margin-left:0; margin-right:0"&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;strong&gt;大模型私有化部署補貼:&lt;/strong&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;對於採用第三方大模型進行私有化部署，並將其應用於廣告垂類領域的數字廣告企業，上海市將提供&lt;span&gt;最高&lt;/span&gt;可達覈定合同額&lt;strong&gt;50%&lt;/strong&gt;，&lt;span&gt;最高&lt;/span&gt;&lt;strong&gt;500 萬元&lt;/strong&gt;的補貼。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;strong&gt;語料研發與應用補貼:&lt;/strong&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;鼓勵企業購買非關聯方的語料進行廣告垂類應用和「智能體」等研發。對於此類投入，企業可獲得&lt;span&gt;最高&lt;/span&gt;覈定合同額&lt;strong&gt;30%&lt;/strong&gt;，&lt;span&gt;最高&lt;/span&gt;&lt;strong&gt;500 萬元&lt;/strong&gt;的補貼。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;strong&gt;算力租用支持:&lt;/strong&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;此外，有條件的區政府還將對租用算力的數字廣告企業提供支持，按實際投入的&lt;strong&gt;30%&lt;strong&gt;比例，給予單個主體年度&lt;span&gt;最高&lt;/span&gt;&lt;/strong&gt;2000 萬元&lt;/strong&gt;的支持。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;這一系列政策的出台，不僅體現了上海市搶佔「AI+廣告」產業制高點的決心，也旨在通過真金白銀的投入，降低企業在技術研發和部署上的成本，激發市場的創新活力。通過支持大模型私有化部署、語料研發和算力投入，上海正着力打造一個集技術、數據和算力於一體的完整 AI 廣告生態系統。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;這些措施預計將吸引更多 AI 技術公司和傳統廣告企業在上海落地和發展，加速人工智能在廣告創意、內容生成、精準投放等環節的深度融合，從而推動整個廣告行業的數字化和智能化轉型。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370959</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370959</guid>
      <pubDate>Sat, 06 Sep 2025 09:25:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>HuggingFace 開源 FinePDFs 與 FineVision 數據集</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Hugging Face 開源了兩個大規模數據集 FinePDFs 和 FineVision，前者是目前最大的公開 PDF 語料庫，後者則專為視覺-語言模型訓練設計，旨在顯著提升開源模型的能力。&lt;/p&gt; 
&lt;p&gt;https://huggingface.co/datasets/HuggingFaceFW/finepdfs&lt;br&gt; https://huggingface.co/datasets/HuggingFaceM4/FineVision&lt;/p&gt; 
&lt;p&gt;FinePDFs 是目前最大的公開 PDF 語料庫，完全由 PDF 文件構建，包含約 3 萬億 tokens，覆蓋 4.75 億份文檔、1733 種語言，數據量 3.65TB。&lt;/p&gt; 
&lt;p&gt;語料來自 105 個 CommonCrawl 快照（2013 夏—2025 年 2 月），經 datatrove 庫去重、過濾與 PII 匿名化，採用 ODC-By 1.0 許可證。文檔平均長度接近 HTML 數據集的兩倍，長於 10 萬，字符的樣本顯著，可用於提升開源 LLM 的長上下文能力。&lt;/p&gt; 
&lt;p&gt;數據集已按語言-腳本對劃分，978 種語言超 100 萬 tokens，66 種語言超 10 億 tokens。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-7cbae8687f50206187cf62b7ba1d65da7be.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;FineVision 面向視覺-語言模型訓練，整合 200 餘個來源，含 1730 萬張圖像、2430 萬樣本、8890 萬輪對話、95 億回答 tokens，支持 GUI 導航、指向、計數等新能力。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-945829421e543e2f159fb676f6f537bbadb.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;官方稱在 10 項基準上帶來 20% 以上提升，可顯著增強開源 VLM 性能。數據已轉為 Parquet，總量約 4.48 TB，支持流式加載。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370951</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370951</guid>
      <pubDate>Sat, 06 Sep 2025 09:15:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>宇樹科技衝刺 IPO 將影響機器人產業格局</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;近日，國內機器人領域頭部企業宇樹科技宣佈，預計在 2025 年 10 月份至 12 月份期間向證券交易所提交 IPO 申請文件。這一消息在科技界和資本市場引發了廣泛關注。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;作為人形機器人商業化落地的標杆企業，宇樹科技衝刺 IPO，有望成為影響機器人產業格局的關鍵節點。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;首先，宇樹科技衝刺 IPO，有望向市場證明其技術商業化的可行性。公司 2024 年營收突破 10 億元，且連續 4 年實現盈利，其中，2024 年四足機器人貢獻了 65% 的收入，驗證了消費級場景的變現能力。若成功上市，通過完整披露研發數據、客戶結構及成本模型，宇樹科技將進一步證明其技術護城河並非只是「實驗室成果」。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;機器人企業不僅要注重技術研發，還要重視商業化落地。通過拓展應用場景，開發滿足市場需求的產品，實現技術的商業價值轉化，才能獲得穩定的收入，增強資本吸引力。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;其次，宇樹科技衝刺 IPO，將成為機器人產業鏈價值重估的催化劑，持續推動上游精密製造、中游系統集成、下游場景運營的全鏈條資本化，形成「技術—資本—產業」正循環，從而進一步優化產業鏈。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;目前，宇樹科技已經實現電機、減速器、控制器等核心部件全棧自研，國產化率超 90%。業內預計，宇樹科技或將募資重點投向高扭矩密度電機、輕量化材料等領域，以打破機器人規模化應用的成本瓶頸。上市後，宇樹科技勢必會通過融資擴大產能，相關供應鏈企業有望迎來訂單放量的黃金機遇，上下游協同的良性生態有望加速形成。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;最後，宇樹科技選擇 IPO，本質上是資本效率與技術週期的精準匹配。2025 年 6 月份，宇樹科技完成 C 輪融資，投後估值已達 120 億元。該輪融資由中國移動旗下基金、騰訊、錦秋基金、阿里巴巴、螞蟻集團和吉利資本共同領投。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;宇樹科技衝刺 IPO，是機器人產業加速資本化的縮影。相信在資本市場與機器人產業的「雙向奔赴」中，中國機器人企業將大幅提升競爭力。（證券日報）&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370948</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370948</guid>
      <pubDate>Sat, 06 Sep 2025 09:05:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>未來可能有高達 50% 的入門級工作將被 AI 取代</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;隨着人工智能（AI）的迅速發展，許多公司正在經歷前所未有的變革。曾經的職場成功故事，如 Hewlett Packard Enterprise 的首席執行官安東尼・內裏 (Antonio Neri) 從客服代理晉升為 CEO，正在逐漸被 AI 的興起所取代。分析師預測，未來可能有高達 50% 的入門級工作將被 AI 取代，這意味着許多剛剛步入職場的大學畢業生將面臨前所未有的挑戰。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在一項針對公共科技公司和成長中的風險投資企業的研究中，數據顯示，從 2019 年到 2024 年，具有不到一年工作經驗的求職者的就業機會下降了 50%。這一趨勢影響到了銷售、市場營銷、工程、招聘、運營、設計、財務和法律等各個核心職能。這種變化不僅影響了求職者，也讓企業面臨重新培養人才的壓力。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;儘管如此，行業專家指出，這種失去入門級崗位的情況可能促使組織內部的人才培養模式發生改變。隨着公司的結構變得更加扁平化，入門級崗位可能會轉變為更高要求的技能角色，要求求職者在進入職場前具備更多的能力。雖然對於即將畢業的學生來説，這意味着他們需要自行掌握這些技能，但也可能成為他們在競爭激烈的求職市場中脫穎而出的優勢。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;各大高校也在積極調整課程，旨在為學生提供與 AI 相關的技能培訓。雖然技術進步可能在短期內對就業率產生影響，但歷史上技術革新在長期內並未導致大規模的失業。專家認為，當前大學畢業生面臨的挑戰，可能在未來幾年內影響他們的職業發展。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;然而，儘管有許多未知數，許多經濟學家認為 AI 對勞動市場的長期影響仍然具有高度的不確定性，企業和社會將需要時間來適應這一變化。隨着技術的不斷進步和 AI 的普及，職場的未來可能會迎來全新的模式，而不僅僅是對現有職場階梯的替代。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370944</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370944</guid>
      <pubDate>Sat, 06 Sep 2025 08:47:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>英偉達收購 AI 編程初創公司 Solver</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.theinformation.com%2Fbriefings%2Fnvidia-acquires-coding-startup-solver" target="_blank"&gt;據 The Information 報道&lt;/a&gt;，英偉達最近完成了對 AI 編程公司 Solver 的收購，進一步強化其在 AI 全棧生態的佈局。&lt;/p&gt; 
&lt;p&gt;Solver 成立於 2022 年，前身為 Laredo Labs，專注於 AI Coding Agent，其產品能通過自然語言指令管理完整代碼庫（如生成、測試、修復代碼），而非僅代碼補全。公司曾獲 800 萬美元融資，創始團隊包括前 Siri 和三星 Viv Labs 核心成員。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0908/161953_FkOn_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Solver 的技術突破在於學習「超過一億個軟件項目的開發歷史」，理解代碼演進邏輯，可執行復雜任務（如重構模塊、修復系統性漏洞）。其 API 支持多語言（Python、JavaScript 等），並與主流開發工具無縫集成。&lt;/p&gt; 
&lt;p&gt;此次收購是英偉達 2024-2025 年系列收購的關鍵一環，旨在構建「硬件+軟件+雲服務」全棧生態。Solver 將整合至英偉達開發者工具鏈（如 CUDA、NVIDIA AI Enterprise），降低 AI 應用開發門檻，反向驅動 GPU 需求增長。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370938/nvidia-acquires-coding-startup-solver</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370938/nvidia-acquires-coding-startup-solver</guid>
      <pubDate>Sat, 06 Sep 2025 08:20:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>OpenAI 重組 ChatGPT 「模型行為團隊」</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;OpenAI 內部郵件確認，原「模型行為團隊」（Model Behavior）整體併入「後訓練團隊」（Post Training），直接向該團隊負責人 Max Schwarzer 彙報。此舉旨在把 AI 個性、安全與用戶體驗研究更深地嵌入核心模型開發流程，為 GPT-5 後續版本提供更快的迭代支持。&lt;/p&gt; 
&lt;p&gt;該團隊原有 14 人，長期負責減少諂媚、平衡政治偏見、定義聊天語氣等「人格化」工作。與此同時，模型行為團隊創始負責人 Joanne Jang 宣佈轉崗，啓動新項目 OAI Labs，探索超越傳統聊天窗口的人機協作界面。Jang 稱，新實驗室將「讓 AI 成為思考、創作、遊戲、學習和連接的工具」。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0908/160802_ml0W_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0908/160900_9Cth_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;業內分析指出，此次重組反映出 OpenAI 對「模型性格」商業化影響的重視：用戶反饋 GPT-5「過於冷淡」或「過度迎合」後，公司已臨時開放舊模型訪問權限，並加速個性調優。同期發表的 OpenAI 研究論文也警告，行業慣用的「應試型」評估可能鼓勵模型幻覺，未來需在評分機制中引入「不確定性誠實」指標。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370934</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370934</guid>
      <pubDate>Sat, 06 Sep 2025 08:10:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Tilde AI 發佈開源 TildeOpen LLM</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;Latvian 語言技術公司 Tilde 發佈了 TildeOpen LLM，這是一個開源的基礎大語言模型（LLM），旨在支持歐洲語言，特別是那些較少被代表的國家和地區語言。這一舉措標誌着歐盟在語言公平和數字主權方面邁出了重要的一步。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="290" src="https://oscimg.oschina.net/oscnet/up-a3afc0c462ebfde5158ba6a9fda49510c9d.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;TildeOpen LLM 是一個擁有 300 億參數的稠密解碼器模型，採用了 CC-BY-4.0 的寬鬆許可證，能夠支持從拉脫維亞語、立陶宛語到烏克蘭語、土耳其語等多種語言。該模型的訓練是在歐洲的&lt;span&gt;超級&lt;/span&gt;計算機 LUMI（芬蘭）和 JUPITER 上進行的，使用了歐盟委員會的大型人工智能大獎挑戰賽所提供的 200 萬 GPU 小時的計算資源。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在技術細節方面，TildeOpen LLM 通過受 EleutherAI 啓發的 GPT-NeoX 腳本進行訓練，共進行了 45 萬次更新，使用了約 2 萬億個令牌。其訓練過程包含三階段採樣：首先在語言間均勻分佈，其次是對高數據量語言的自然分佈進行增強，最後再進行均勻的掃查以確保平衡。模型的超參數包括 60 層、嵌入維度 6144、48 個注意力頭、8192-token 的上下文窗口，以及使用 SwiGLU 激活、RoPE 位置編碼和 RMSNorm 層規範化。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在語言公平和數據主權方面，傳統的主流模型往往側重於英語和其他主要語言，導致在處理波羅的海、斯拉夫及其他較小的歐洲語言時表現不佳，常常出現語法錯誤和奇怪的措辭。而 TildeOpen 通過引入 「公平的標記器」，使得不同語言的文本以相似方式進行表示，從而減少標記數量，提高較少代表語言的推理效率。此外，組織可以選擇在本地數據中心或符合歐盟要求的安全雲中自我託管，確保遵循 GDPR 及其他數據保護法規，從而解決了與美國或亞洲託管模型相關的主權問題。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;TildeOpen 作為基礎模型，預計會推出更多專門化版本，例如經過指令調優的翻譯模型，這將進一步增強其功能。拉脫維亞通過 Tilde 的努力，期望在全球科技領域佔據一席之地，同時致力於保護語言多樣性。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370933</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370933</guid>
      <pubDate>Sat, 06 Sep 2025 08:08:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
  </channel>
</rss>
