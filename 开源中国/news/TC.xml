<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - news - 繁體中文（台灣）</title>
    <link>https://www.oschina.net/news/project</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news" rel="self" type="application/rss+xml"/>
    <description>已對該 RSS 進行格式化操作：中英字符之間插入空格、使用直角引號、標點符號修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-tw</language>
    <lastBuildDate>Thu, 03 Jul 2025 12:48:53 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>全球 AI 人才榜單首次曝光，華人撐起半邊天</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;7 月 3 日，2025 全球數字經濟大會上，一份重磅榜單面向全球首次揭曉。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;該榜單基於近十年近 10 萬篇文獻深度分析，列出了全球人工智能領域的 Top100 人才&lt;/strong&gt;。其中，華人依舊拿下了主力席位。&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FsDvBk1WCxUrJqMOu2Q9p6g" target="_blank"&gt;鳳凰網科技從中提煉了出了一份較為矚目的人員名單&lt;/a&gt;，這些人現多數就職於國內外企業，仍在人工智能前沿探索領域活躍，包括：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;何愷明&lt;/strong&gt;，曾任職於 AI 殿堂美國麻省理工學院（MIT），作為 ResNet 之父，是深度學習革命的關鍵推手，其提出的殘差學習（Residual Learning）概念，一舉攻破了困擾神經網絡多年的「梯度消失」難題，讓深達千層的網絡訓練成為可能。其論文引用數以駭人的數十萬次傲視羣雄（公開數據約 40 萬 +），被譽為「CV 界的諾獎級工作」。6 月 26 日，何愷明剛剛官宣入職 GoogleDeepMind，擔任傑出科學家，同時還保留了 MIT 終身副教授身份。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/194046_NzCP_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;張祥雨&lt;/strong&gt;，曾是曠視研究院的頂樑柱，現已入職階躍星辰任首席科學家； 2016 年，以 ResNet（作為核心貢獻者之一）問鼎 CVPR 最佳論文，震動世界！隨後在 ImageNet、COCO 等視覺「奧林匹克」賽場多次屠榜。其與團隊開創的 ResNet、ShuffleNet 系列影響頗深，Google Scholar 引用逾 40,000 次，是無數手機、攝像頭、自動駕駛系統的「核心引擎」。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/194104_my7v_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;任少卿&lt;/strong&gt;，蔚來汽車自動駕駛的靈魂人物，計算機視覺與自動駕駛融合領域的頂尖專家，曾發表多篇極具影響力的 CV 頂會論文。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/194115_XBIO_2720166.png" referrerpolicy="no-referrer"&gt;&lt;img alt="" src="https://img.ithome.com/newsuploadfiles/2025/7/9ccdcd47-cf38-495f-a15c-11c7758e41da.png?x-bce-process=image/format,f_avif" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;田奇&lt;/strong&gt;，華為在人工智能領域的重要人物，也是華為計算產品線（昇騰 AI 處理器等）和 MindSpore 框架背後的核心操盤手。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/194127_1K5b_2720166.png" referrerpolicy="no-referrer"&gt;&lt;img alt="" src="https://img.ithome.com/newsuploadfiles/2025/7/06d18e54-babe-4198-a0f2-f0ed8e8fc7d2.png?x-bce-process=image/format,f_avif" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;王雲鶴&lt;/strong&gt;，華為諾亞方舟實驗室的研究員。專注於 AI 基礎模型、神經網絡架構搜索（NAS）、模型輕量化等前沿方向，是華為 AI「頂天」（前沿）又「立地」（落地）戰略的重要實踐者。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/194139_3XP0_2720166.png" referrerpolicy="no-referrer"&gt;&lt;img alt="" src="https://img.ithome.com/newsuploadfiles/2025/7/3e5805f0-1b78-44bb-ba08-0ba2b5e6c899.png?x-bce-process=image/format,f_avif" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;謝凌曦&lt;/strong&gt;，華為天才少年，在計算機視覺，特別是視覺大模型、自監督學習、對抗魯棒性等領域有系列開創性工作。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/194149_Jxf5_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://img.ithome.com/newsuploadfiles/2025/7/85b5a95b-03b3-4936-96f3-fecdbf8a954a.png?x-bce-process=image/format,f_avif" referrerpolicy="no-referrer"&gt;&lt;strong&gt;王曉剛&lt;/strong&gt;，商湯科技聯合創始人、核心技術奠基人之一。在商湯，他主導了核心視覺算法框架的構建。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/194158_juQG_2720166.png" referrerpolicy="no-referrer"&gt;&lt;img alt="" src="https://img.ithome.com/newsuploadfiles/2025/7/b2340c80-9b01-4125-9c86-035e2353db25.png?x-bce-process=image/format,f_avif" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;石建萍&lt;/strong&gt;，商湯科技自動駕駛研發團隊的領軍女將。帶領團隊在智能駕駛視覺感知、多傳感器融合、高精定位等方面取得突破性進展。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/194208_Tj3r_2720166.png" referrerpolicy="no-referrer"&gt;&lt;img alt="" src="https://img.ithome.com/newsuploadfiles/2025/7/b0c91622-96b2-4f33-be72-40b2b4802389.png?x-bce-process=image/format,f_avif" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;閆俊傑&lt;/strong&gt;，MiniMax 創始人，國內最早一批成立的大模型公司核心人物。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/194219_yNU3_2720166.png" referrerpolicy="no-referrer"&gt;&lt;img alt="" src="https://img.ithome.com/newsuploadfiles/2025/7/6b21b373-3805-435f-8f02-df120995bb9a.png?x-bce-process=image/format,f_avif" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;曹越&lt;/strong&gt;，前微軟傑出工程師，現 Sand.AI 創始人兼 CEO。專注打造下一代 AI 智能體（AI Agent）平台。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/194228_wqU6_2720166.png" referrerpolicy="no-referrer"&gt;&lt;img alt="" src="https://img.ithome.com/newsuploadfiles/2025/7/f1b580d1-bd41-490b-9080-e4bdc8effc1e.png?x-bce-process=image/format,f_avif" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;陶大程&lt;/strong&gt;，新加坡南洋理工大學（NTU）協理副校長、澳大利亞桂冠教授，視覺大牛，IEEE / AAAS / ACM 三料 Fellow，曾任京東探索研究院院長（2023 年卸任）。研究橫跨計算機視覺、機器學習、統計學習、可信 AI（魯棒性、可解釋性、公平性）等核心領域。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/194240_wWsg_2720166.png" referrerpolicy="no-referrer"&gt;&lt;img alt="" src="https://img.ithome.com/newsuploadfiles/2025/7/cb439f78-abc0-4ebb-a823-9121a7ada5a5.png?x-bce-process=image/format,f_avif" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;劉子緯&lt;/strong&gt;，新加坡南洋理工大學（NTU）新鋭實力派教授，學術明星。在視覺-語言理解（VLP）、多模態大模型、信息檢索方向成果斐然。其工作多次發表於 CVPR, ICCV, NeurIPS 等頂會，並常居排行榜前列，是推動多模態預訓練模型發展與應用的重要力量。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/194307_cOI1_2720166.png" referrerpolicy="no-referrer"&gt;&lt;img alt="" src="https://img.ithome.com/newsuploadfiles/2025/7/e10b6eaa-b533-4846-ac3d-1688c5e7ca50.png?x-bce-process=image/format,f_avif" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;賈佳亞&lt;/strong&gt;，香港中文大學計算機科學與工程系教授，思謀科技創始人。著名計算機視覺學者，專注於底層視覺重建、圖像增強、圖像分割、醫學圖像分析等方向。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/194322_DDlL_2720166.png" referrerpolicy="no-referrer"&gt;&lt;img alt="" src="https://img.ithome.com/newsuploadfiles/2025/7/f17121d8-c2e7-4ff0-9203-e1743fc8674f.png?x-bce-process=image/format,f_avif" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;楊明玄&lt;/strong&gt;，美國加州大學默塞德分校（UC Merced）計算機科學教授,Google DeepMind 研究員。深耕機器學習和數據挖掘領域，特別是在圖神經網絡（GNN）、推薦系統、社交網絡分析等方面有突出建樹。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/194332_aIW0_2720166.png" referrerpolicy="no-referrer"&gt;&lt;img alt="" src="https://img.ithome.com/newsuploadfiles/2025/7/5c0ee9b8-ae73-4c39-b4ad-9ba80f95187f.png?x-bce-process=image/format,f_avif" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;劉威&lt;/strong&gt;，前騰訊混元大模型技術負責人之一，騰訊傑出科學家，2025 年初離職創業。2024 年領導開源混元文生圖模型、3D 生成模型「Hunyuan3D-1.0」，推動騰訊內部 700 + 業務接入 AI 能力（如微信輸入法、騰訊會議）。發表頂會論文 100 + 篇，總引用 3600 + 次，獲 CVPR 青年研究者獎、SIGIR 最佳論文榮譽獎。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/194344_6WyQ_2720166.png" referrerpolicy="no-referrer"&gt;&lt;img alt="" src="https://img.ithome.com/newsuploadfiles/2025/7/44dd9265-ca1d-4db3-8e44-70ee47565a3a.png?x-bce-process=image/format,f_avif" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;顏水成&lt;/strong&gt;，新加坡國立大學終身教授，培養 50 + 名博士，建立亞太最大計算機視覺實驗室，歷任 360 首席科學家、依圖 CTO、Sea 集團 AI Lab 主任；2023 年加入崑崙萬維任 2050 全球研究院院長，2024 年底卸任。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/194359_j5g6_2720166.png" referrerpolicy="no-referrer"&gt;&lt;img alt="" src="https://img.ithome.com/newsuploadfiles/2025/7/496764e9-1b16-47dd-a8e1-3349844dd95d.png?x-bce-process=image/format,f_avif" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;據東壁科技數據創始人、深圳大學特聘教授吳登生介紹，該報告基於東壁全球科技文獻數據平台（dbdata.com），對全球 AI 研究生態進行系統分析，數據集涵蓋近 20 萬名來自 175 個國家和地區、3847 個機構的學者，時間跨度為 2015 至 2024 年。&lt;/p&gt; 
&lt;p&gt;值得一提的是，何愷明、劉子緯、王曉剛、陶大程均師承中國人工智能先驅、商湯科技創始人湯曉鷗，張祥雨曾師從何愷明。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358641</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358641</guid>
      <pubDate>Thu, 03 Jul 2025 11:45:57 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Bytebase 3.8.0 - 顯著優化 schema 同步 / 回滾兼容性</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                                        &lt;h1&gt;🔔 重大變更&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;顯著優化多數據庫（MySQL/PostgreSQL/TiDB/SQL Server/Oracle）的 schema 同步/回滾兼容性，支持絕大多數常見數據庫對象。 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.bytebase.com%2Fchange-database%2Fsynchronize-schema%23supported-objects" target="_blank"&gt;文檔地址&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-21a7f23efb479d2c33408917a613e8bb91b.png" alt="file" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;下線工單訂閲功能。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;將 SQL 審核中心更名為變更計劃。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;🎄 改進&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;新增查詢結果行數限制功能。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-cad54dad1deaf4e15ad8f12933c09c1609c.png" alt="file" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;新增多域名配置支持。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-4380358be992a487045a8f39061a6ba967e.png" alt="file" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;簡化默認值輸入：不再區分表達式和值，統一通過文本輸入框填寫。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;在審批流程處展示全部審批人，且可懸停顯示細節。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-5c8191a2cf70d8536b5ec930d9fca7b36e2.png" alt="file" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h1&gt;🐞 Bug 修復&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;修復了 ClickHouse 查詢中的 JSON 數據類型顯示問題。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;在 Terraform SSL 配置中新增 &lt;code&gt;use_ssl&lt;/code&gt; 字段。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;📕 安裝及升級&lt;/h1&gt; 
&lt;p&gt;新安裝 &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.bytebase.com%2Fget-started%2Fself-host" target="_blank"&gt;https://docs.bytebase.com/get-started/self-host&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;升級 &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.bytebase.com%2Fget-started%2Fupgrade" target="_blank"&gt;https://docs.bytebase.com/get-started/upgrade&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;strong&gt;升級前請備份元數據庫，升級後無法回退版本。&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;💡 更多資訊，請關注 Bytebase 公號：Bytebase&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/6148470/blog/18683355</link>
      <guid isPermaLink="false">https://my.oschina.net/u/6148470/blog/18683355</guid>
      <pubDate>Sun, 11 May 2025 10:24:00 GMT</pubDate>
      <author>原創</author>
    </item>
    <item>
      <title>Perplexity 上線月費 200 美元的「Max」訂閲服務</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Perplexity 公司推出了&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.perplexity.ai%2Fhub%2Fblog%2Fintroducing-perplexity-max" target="_blank"&gt; Perplexity Max &lt;/a&gt;訂閲服務，月費為 200 美元（約合 1433 元人民幣），可以享受諸多權益。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-1abe17d787b691e146a089eccf4834806a9.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;購買 Perplexity Max 訂閲計劃的用戶，可以無限制訪問電子表格和報告生成工具 Labs，並支持提前體驗 Comet 瀏覽器在內的諸多新功能。&lt;/p&gt; 
&lt;p&gt;該公司表示，Perplexity Max 是為以下用戶設計的：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;需要無限訪問全面分析工具的專業人士&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;需要大量研究能力的內容創作者和作家&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;進行競爭情報和市場研究的商業策略師&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;從事複雜、多方面項目的學術研究人員&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Max 用戶還支持在 Perplexity 服務中，調用 OpenAI 的 o3-pro 和 Claude Opus 4 等先進 AI 模型。隨着 Max 的推出，Perplexity 成為最新一家推出超高端訂閲服務層的 AI 提供商。&lt;/p&gt; 
&lt;p&gt;OpenAI 是首家推出每月 200 美元的 ChatGPT Pro 訂閲服務的公司，但近幾個月來，Google、Anthropic 和 Cursor 也紛紛效仿。&lt;/p&gt; 
&lt;p&gt;Perplexity 目前提供多種訂閲計劃。除了每月 200 美元的 Max 計劃外，還提供每月 20 美元的消費者 Pro 計劃，以及每人每月 40 美元的企業 Pro 計劃。該公司表示，最終也將為企業客戶推出超高端的 Max 計劃。&lt;/p&gt; 
&lt;p&gt;Perplexity 在 2024 年主要依靠每月 20 美元的 Pro 計劃訂閲，實現了大約 3400 萬美元的收入，但據 The Information 看到的財務數據，公司仍燒掉了約 6500 萬美元現金。&lt;/p&gt; 
&lt;p&gt;據報道，Perplexity 的現金消耗主要來自對雲服務器的重金投入以及購買 OpenAI 和 Anthropic 的 AI 模型訪問權。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;相關文檔：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.perplexity.ai%2Fhelp-center%2Fen%2Farticles%2F11680686-perplexity-max" target="_blank"&gt;https://www.perplexity.ai/help-center/en/articles/11680686-perplexity-max&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358621/perplexity-max</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358621/perplexity-max</guid>
      <pubDate>Sun, 11 May 2025 09:14:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>X 平台（原 Twitter）上線 AI 筆記功能</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;X 平台（原 Twitter）&lt;u&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2FCommunityNotes%2Fstatus%2F1940132205486915917" target="_blank"&gt;宣佈&lt;/a&gt;&lt;/u&gt;正式啓動 AI Note Writer 的 API 試點計劃，允許全球開發者創建 AI 機器人來撰寫社區內容。未來，這些由 AI 撰寫的筆記如果被一定數量的用戶認為有幫助，就可以在平台上公開展示。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-b9141239b889f5cfe9e01d79314f39bdc80.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;X 表示，該計劃旨在加速社區內容的產出效率，同時通過社區反饋機制，不斷優化 AI 的準確性、公正性與實用性。AI 筆記將遵循與人工內容相同的審核準則，並會在介面中明確標註。&lt;/p&gt; 
&lt;p&gt;首批 AI Note Writer 將在本月內開放上線，且將逐步開放。&lt;/p&gt; 
&lt;p&gt;詳情查看文檔：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcommunitynotes.x.com%2Fguide%2Fen%2Fapi%2Foverview" target="_blank"&gt;https://communitynotes.x.com/guide/en/api/overview&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358606/ai-note-writer-api</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358606/ai-note-writer-api</guid>
      <pubDate>Sun, 11 May 2025 08:35:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>畢馬威：醫療大模型中國發布數量佔全球七成</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;畢馬威中國發布《首屆健康科技 50》報告指出，醫療大模型目前主要分為五類，包括大型語言模型 (LLM)、語言條件多智能體大型、多模態大模型、圖學習大模型、視覺語言大模型。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在全球範圍內，據不完全統計，在已發佈的醫療大模型中，按模型類別來看，大語言模型數量最多，佔比近 65%；按分佈海內外發表數量來看，中國發布數量佔比超 70%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="265" src="https://oscimg.oschina.net/oscnet/up-0f5af0008879ca0ee50480307bd0d62db20.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;此外報告指出，2024 年，中國醫療科技市場規模突破百億，達到 102.5 億元，同比增長 75.3%。2025-2027 年，中國醫療科技的增幅預計會有所放緩，但行業總市場規模仍呈現穩健增長趨勢。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;2023 年中國醫療機器人市場規模達到約 108 億元，近五年年均複合增長率高達 25.74%。中國智能醫療器械市場增長迅猛，預計 2025 年將達 242.3 億元，2026 年-2027 年總體有望保持較高速增長。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;更多詳情可&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fassets.kpmg.com%2Fcontent%2Fdam%2Fkpmg%2Fcn%2Fpdf%2Fzh%2F2025%2F07%2Fkpmg-china-healthcare-health-tech-50.pdf" target="_blank"&gt;查看完整報告&lt;/a&gt;。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358603</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358603</guid>
      <pubDate>Sun, 11 May 2025 08:21:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>IT 技術人員被解僱，怒改公司所有密碼，獲刑 7 個月</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;據報道，在英國西約克郡，一位被解僱 IT 技術人員因心懷怨恨，對僱主公司發動了一場數字攻擊，最終被判處 7 個月零 14 天的監禁。&lt;/p&gt; 
&lt;p&gt;根據警方的公告，2022 年 7 月，Mohammed Umar Taj 在被公司暫停工作後的數小時內，便開始實施惡意的 「數字暴行」。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-d7c99b29132ed474e2163c68143d842ddfa.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;他非法侵入公司系統，擅自更改登錄憑證，還破壞了公司的多因素身份驗證系統，致使公司日常運營受到嚴重幹擾，造成至少 20 萬美元的損失。&lt;/p&gt; 
&lt;p&gt;公司稱 Taj 的報復行為不僅給公司帶來了經濟損失，還損害了公司的聲譽，給公司帶來了聲譽損害和業務損失。&lt;/p&gt; 
&lt;p&gt;由於網絡攻擊的連鎖反應，不僅約克郡的員工工作受阻，英國、德國以及巴林的客戶也受到了不同程度的影響。&lt;/p&gt; 
&lt;p&gt;上週，31 歲的 Taj 在約克郡的利茲刑事法庭出庭受審，他在之前的聽證會上承認了 「未經授權對計算機進行操作並妨礙計算機訪問」 的罪名，最終被判處 7 個月零 14 天的監禁。&lt;/p&gt; 
&lt;p&gt;警方還在報告中提醒所有企業：「保護好你們的網絡，這不僅能防止數據丟失和代價高昂的網絡攻擊，還能維護與客戶和利益相關者的信任關係。」&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;以下是該事件的具體介紹：&lt;/p&gt; 
&lt;h3&gt;案件經過&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Taj 在被公司停職後，公司未及時撤銷其賬號權限，他便利用這一漏洞，非法侵入系統，篡改了公司的系統權限，把公司員工以及海外客戶都踢出系統之外，導致公司運營陷入癱瘓。&lt;/li&gt; 
 &lt;li&gt;他在實施攻擊後，還曾 「自豪地談論」 自己的攻擊過程，警方在後續調查中找到了他作案的操作記錄和通話內容。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;法院判決&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Taj 最終在利茲皇家法院承認自己未經授權幹擾公司計算機系統的正常運行，於 2025 年 6 月 26 日在利茲刑事法庭被判處七個月零 14 天的監禁。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;事件影響&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;直接經濟損失&lt;/strong&gt; ：該公司遭受了至少 20 萬英鎊的損失，約合人民幣近 200 萬元。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;業務中斷&lt;/strong&gt; ：系統的癱瘓直接導致公司業務無法正常開展，無論是內部的日常運作還是與海外客戶的合作都受到了嚴重影響，公司的聲譽也受到了損害。&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358595</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358595</guid>
      <pubDate>Sun, 11 May 2025 08:10:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Cloudflare 推出「按次抓取付費」計劃，發起「內容獨立日」倡議</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Cloudflare 宣佈推出一項名為 「Pay per crawl」 的新功能，並聯合多家內容平台發起 「內容獨立日」 倡議，旨在改變 AI 公司無償抓取網絡內容進行模型訓練的現狀。該計劃允許網站所有者向 AI 爬蟲收取內容訪問費用，為內容創作者提供除完全開放或完全封鎖之外的第三種選擇。Cloudflare 將擔任該計劃的記錄商家（Merchant of Record）。&lt;/p&gt; 
&lt;p&gt;該功能基於 HTTP 狀態碼 402 (Payment Required) 實現。當 AI 爬蟲請求受保護內容時，若未攜帶支付意圖，將收到 402 響應及定價信息。網站所有者可以為自己的域名設定一個統一的、按次請求的單價，並對不同的 AI 爬蟲設置三種策略：允許免費訪問、按價收費或完全阻止。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-5f8ad61f543b26ca4f0d391b8c340b1f6a3.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;技術上，該系統通過 「Web Bot Auth」 機制，使用 Ed25519 密鑰對和 HTTP 消息簽名來驗證爬蟲身份，防止欺騙。爬蟲可通過在請求頭中加入 crawler-max-price（願意支付的最高價格）或在收到 402 響應後加入 crawler-exact-price（同意支付的確切價格）來表明支付意圖。交易成功後，響應頭中會包含 crawler-charged 字段。&lt;/p&gt; 
&lt;p&gt;過去 30 年，谷歌和內容創作者之間形成了一種默契：谷歌用創作者的內容吸引用戶搜索，再把用戶送回原網站，讓創作者賺取廣告費或訂閲收入。但隨着 AI 工具興起，用戶越來越多地直接從 AI 獲得答案，原創內容的網站流量暴跌，創作者的收益嚴重受損。&lt;/p&gt; 
&lt;p&gt;為此，Cloudflare 聯合眾多內容平台，在 2025 年 7 月 1 日宣佈了 「內容獨立日」，明確要求 AI 公司不能再免費抓取內容，必須為內容創作者支付合理的報酬。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-b62a9cf03f1853620715f17a648ca213898.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Cloudflare 表示，此舉旨在為內容創作者提供對其數字資產的程序化控制，確保他們能從自己的工作中獲得補償，從而維持一個健康、多樣化的互聯網內容生態。未來，該系統有望演變為一個更復雜的代理（Agent）經濟市場，AI 代理可以根據預算，以編程方式協商併購買所需的數據訪問權限。&lt;/p&gt; 
&lt;p&gt;他們希望通過這樣的行動，讓創作者重新獲得應有的價值和尊重，同時推動 AI 和原創內容之間形成一種新的、公平的生態模式。&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;更多詳情：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.cloudflare.com%2Fcontrol-content-use-for-ai-training%2F"&gt;https://blog.cloudflare.com/control-content-use-for-ai-training/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.cloudflare.com%2Fzh-cn%2Fcontent-independence-day-no-ai-crawl-without-compensation%2F"&gt;https://blog.cloudflare.com/zh-cn/content-independence-day-no-ai-crawl-without-compensation/&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358592/content-independence-day-no-ai-crawl</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358592/content-independence-day-no-ai-crawl</guid>
      <pubDate>Sun, 11 May 2025 07:56:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>騰訊提醒開發者可將微信小程序遷移至 QQ 客戶端</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;騰訊 QQ 小程序開發者平台發文，提醒 QQ 客戶端將全面接入微信小程序，&lt;strong&gt;開發者可以將微信小程序遷移至 QQ 以取代原有的舊版 QQ 小程序&lt;/strong&gt;，開發者當前已上線的舊版 QQ 小程序仍可正常使用和更新，不過官方稱「強烈建議儘早遷移，以便獲得更完整的接口支持，同時享受 QQ + 微信的雙端流量紅利」。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-e508f98702091d79c43aa333caf718d83a9.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;遷移前，QQ 小程序引擎實際上對微信小程序的大多數接口進行了兼容，原本只需要在微信小程序原有代碼基礎上做一些簡單判斷（主要是登錄方面）就可以分別提交兩個平台。&lt;/p&gt; 
&lt;p&gt;而在遷移後，QQ 端運行的就是微信小程序，體驗比 QQ 小程序會好一些。開發者需要通過 QQ 提供的插件（qq-wxmini-plugin）區分運行環境、處理登錄邏輯。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;來源：https://m.ithome.com/html/864991.htm&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;相關閲讀：&lt;a href="https://www.oschina.net/news/346915" target="news"&gt;手機版 QQ 支持微信小程序&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358586</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358586</guid>
      <pubDate>Sun, 11 May 2025 07:38:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>多模態才是智能應用爆發的關鍵？</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                                        &lt;p&gt;此前，快手發佈 2025 年一季度財報時，一個數字引發關注：成立僅兩年的 AI 業務線「可靈 AI」單季度貢獻營收 1.5 億元，同比增長 320%。而可靈 AI 正是一個多模態應用的典型產品，涉及到語言、視頻、音頻等交互。&lt;/p&gt; 
&lt;p&gt;前不久，在 OSCHINA 和小度教育技術負責人丁小晶的&lt;a href="https://my.oschina.net/u/4489239/blog/18426743" rel="nofollow"&gt;對話&lt;/a&gt;中。丁小晶表示，多模態技術非常重要，甚至可以説，沒有多模態技術效果的快速提升，教育行業不可能如此迅猛發展。比如 AI 作業批改和 AI 講題答疑方向的應用，完全靠純文本大模型是無法滿足需求的，非常依賴對大模型的圖片理解能力。還比如超擬人 AI 老師，語音情感大模型就起來非常關鍵的作用。&lt;/p&gt; 
&lt;p&gt;百度最新發布的發佈文心快碼 Comate AI IDE 產品，其中也提到了多模態能力的增強，比如支持 Figma 設計稿一鍵轉換為高可用代碼，能實現圖層的精準還原。百度工程效能部前端研發經理楊經緯告訴開源中國，無論是從自然語言、圖片還是設計稿生成代碼，最終都是為了能更加接近人類工程的意圖，因為人類去描述自己想要實現的想法的方式與形態是多種多樣的，也就對應了研發過程中的多模態形式。&lt;/p&gt; 
&lt;p style="text-align:center"&gt;&lt;img height="210" src="https://oscimg.oschina.net/oscnet/up-db06f16dbd4e854566d762bff8c3dfe1e5f.png" width="800" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;人類從不會只用一種感官認知世界。人工智能也勢必不能僅有一種交互途徑。&lt;/p&gt; 
&lt;p&gt;我們聞到咖啡香氣的瞬間，腦海裏會立刻浮現深褐色液體與白瓷杯的畫面；聽到「貓」這個詞時，腦海中自動補全毛茸茸的觸感和呼嚕聲。這種多模態信息融合，正是人類智能的底層邏輯。而單一模態交換的 AI 模型的信息處理能力有限，例如文本生成模型難以理解圖像語義，無法根據文字生成圖像，視頻生成工具則無法同步解析聲音與畫面邏輯。這種時候，就需要多模態模型或是能力的配合。&lt;/p&gt; 
&lt;span id="OSC_h2_1"&gt;&lt;/span&gt; 
&lt;h2&gt;多模態，比文本慢一步&lt;/h2&gt; 
&lt;p&gt;智源研究院院長王仲遠不久前公開&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.msn.cn%2Fzh-cn%2F%25E6%258A%2580%25E6%259C%25AF%2F%25E6%258A%2580%25E6%259C%25AF%25E5%2585%25AC%25E5%258F%25B8%2F%25E8%2581%259A%25E7%2584%25A6%25E5%25A4%259A%25E6%25A8%25A1%25E6%2580%2581-chatgpt%25E6%2597%25B6%25E5%2588%25BB%25E6%259C%25AA%25E5%2588%25B0-2025%25E5%25A4%25A7%25E6%25A8%25A1%25E5%259E%258B-%25E5%258F%2598%25E6%2585%25A2-%25E4%25BA%2586%25E5%2590%2597%2Far-AA1GjaHk%3Focid%3DBingNewsSerp" rel="nofollow" target="_blank"&gt;指出&lt;/a&gt;，當前多模態大模型的學習路徑，尤其是多模態理解模型，通常是先將語言模型訓練到很強的程度，再學習其他模態信息。在這個過程中，模型的能力可能會出現下降。&lt;/p&gt; 
&lt;p&gt;比單一模態更難的是，多模態模型還需解決一個核心問題：如何將圖像、文本、音頻等異構數據在語義層面對齊並融合。&lt;/p&gt; 
&lt;p&gt;文本、圖像、聲音等模態的數據結構天然異構——文本是離散符號序列，圖像是連續像素矩陣，音頻是時間序列信號。比如要讓模型理解「貓」的文本描述與貓的圖片、叫聲之間的關聯，需構建跨模態的共享語義空間。&lt;/p&gt; 
&lt;p&gt;早期，有研究嘗試通過數據級拼接，將圖像像素和文本特徵直接拼接，實現跨模態融合，但由於圖像和文本的時空特性差異較大，導致特徵對齊困難，最終效果不佳。直到對比學習和注意力機制的出現，才實現跨模態語義映射。比如 OpenAI 2021 年推出的一種基於對比學習只的多模態預訓練模型 CLIP，它通過大規模的圖像和文本數據進行訓練，使得模型能夠理解圖像內容和相關文本之間的語義關係。CLIP 的核心貢獻在於它打破了傳統的固定類別標籤範式，通過對比學習的方式，將圖像和文本映射到同一個向量空間中，從而實現跨模態的檢索和分類。但是 CLIP 模型的訓練數據規模龐大，據 OpenAI 披露，其使用了約 4 億圖像-文本對進行訓練，訓練成本高達數千 GPU 日，遠超 GPT-3 等純文本模型。&lt;/p&gt; 
&lt;p style="text-align:center"&gt;&lt;img height="304" src="https://oscimg.oschina.net/oscnet/up-4ad6b286433edebde043654fd53af191e30.png" width="800" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="text-align:center"&gt;&lt;span style="color:#8f959e"&gt;&lt;em&gt;CLIP 模型方法概述 &lt;/em&gt;&lt;/span&gt;&lt;span style="color:#8f959e"&gt;&lt;u&gt;&lt;em&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2103.00020" rel="nofollow" target="_blank"&gt;Learning Transferable Visual Models From Natural Language Supervision&lt;/a&gt;&lt;/em&gt;&lt;/u&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;多模態融合需處理高維數據，如 4K 視頻的像素量是文本的百萬倍，傳統 Transformer 的二次方計算複雜度成為致命短板。對此，業界也有一些解決方式，比如此前 Mamba 架構通過狀態空間模型 SSM 將計算複雜度降至線性，2025 年擴展動態融合模塊&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F2985554863" rel="nofollow" target="_blank"&gt; FusionMamba&lt;/a&gt;，在其中實現多模態特徵高效交互，推理速度提升 3 倍。&lt;/p&gt; 
&lt;p&gt;不僅如此，相較於文本的資料庫和數據集，高質量多模態數據集也更加稀缺，收集難度更大。比如醫療影像、工業質檢的報告中的缺陷描述等，就需專家級別的標註人員。&lt;/p&gt; 
&lt;span id="OSC_h2_2"&gt;&lt;/span&gt; 
&lt;h2&gt;落地需求更多&lt;/h2&gt; 
&lt;p&gt;雖然技術上還有諸多難點，但是多模態能力正在逐步提升，並且帶來非常可觀的價值和效果。&lt;/p&gt; 
&lt;p&gt;比如，從圖片或者是 Figma 設計稿直接生成代碼可以幫助許多開發者或是產品經理完成一些開發工作。這項能力此前在一些低代碼或是輔助編程工具中也存在，但往往是通過 Figma DSL 進行設計稿解析，通過節點虛擬化技術實現像素級還原，其不足在於不一定適配當前項目，比如轉了一套 Vue 框架的代碼，就無法在 React 框架項目中使用。&lt;/p&gt; 
&lt;p&gt;楊經緯介紹，此次文心快碼 Comate AI IDE 的發佈以及相關功能更新後，通過大模型能力增強了 Figma to Code 和當前項目的融合度。首先在 IDE 裏進行操作，天然就可以理解用戶當前環境和本地優勢，而 IDE 內智能體 Zulu 的接入，會更深入到本地項目中瞭解當前的框架、能力、代碼風格等，再結合 Image to Code 的能力，可以實現較高的還原度，並且適配當前的項目。&lt;/p&gt; 
&lt;p&gt;而根據一些公開信息顯示，可靈 AI 的多模態技術，支持通過圖片、文字、聲音甚至手繪軌跡等輸入生成視頻。在上半年的 2.0 模型的迭代中，可靈 AI 也發佈了 AI 視頻生成的全新交互理念 Multi-modal Visual Language（MVL），讓用戶能夠結合圖像參考、視頻片段等多模態信息，將腦海中包含身份、外觀、風格、場景、動作、表情、運鏡在內的多維度複雜創意，直接高效地傳達給 AI。MVL 由 TXT（Pure Text，語義骨架）和 MMW（Multi-modal-document as a Word，多模態描述子）組成，能從視頻生成設定的基礎方向以及精細控制這兩個層面。此外，其技術也結合了類 Sora 的 DiT 結構和 Flow 擴散模型，提升在物理模擬和細節上的表現。&lt;/p&gt; 
&lt;p&gt;基於這些技術特徵。商業化層面，截至今年 6 月，可靈 AI 已為超過 1 萬家企業客戶提供 API 服務，覆蓋廣告營銷、影視動畫等領域，企業客戶續費率較高。&lt;/p&gt; 
&lt;p&gt;此外，一些傳統行業或場景也在結合多模態能力，實現與 AI 的加速融合。比如迪瑞醫療近期採用的多模態 AI 大模型算法技術為臨牀診斷帶來了重要的技術革新，結合多種檢測結果和患者的多維信息，如尿常規、血常規、生化和化學發光免疫，以及患者的個人背景、臨牀表現、現病史與既往病史等，進行全面分析。&lt;/p&gt; 
&lt;p&gt;這種跨學科的信息整合使得診斷提示更加精準，對於減少漏診、誤診的概率具有顯著的作用，並進一步提升了醫療診療的整體效率。大洋彼岸，斯坦福醫學院的科研團隊研發出了一種名為&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzkxMTM0OTQzNQ%3D%3D%26mid%3D2247486194%26idx%3D1%26sn%3D5ac605d67ca7019b3b2e524d65b0f88e%26chksm%3Dc0eed67e545679711993370e69032cc62e9d4fc0c6ff3e283c43854fd93355eae8a07b4fcb02%23rd" rel="nofollow" target="_blank"&gt; MUSK 的 AI 模型&lt;/a&gt;，將視覺數據，如病理圖像和文本數據的病歷和臨牀記錄相結合，為癌症治療帶來了新的可能。MUSK 模型不僅提高了預測癌症患者預後和治療反應的準確性，而且通過分析數千個數據點，更準確地確定了哪些療法對個體患者最有效。&lt;/p&gt; 
&lt;p style="text-align:center"&gt;&lt;img height="285" src="https://oscimg.oschina.net/oscnet/up-196e0ee8b1058ba8ee70698e626a846fe72.png" width="800" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="text-align:center"&gt;&lt;span style="background-color:#f2f3f5"&gt;&lt;em&gt;視覺問答測試，圖片來源於網絡&lt;/em&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;在金融領域。江蘇銀行通過本地化部署微調 DeepSeek-VL2 多模態模型、輕量 DeepSeek-R1 推理模型，分別運用於智能合同質檢和自動化估值對賬場景中，通過對海量金融數據的挖掘與分析，重塑金融服務模式，實現金融語義理解準確率與業務效率雙突破。具體而言，DeepSeek-VL2 多模態模型採用了最新的 Transformer 架構，結合多層次的特徵融合機制，有效提升了金融合同、賬單等複雜文本與圖像信息的理解能力。模型在智能合同質檢場景中表現出色，準確率較傳統方法提升了 15% 以上，顯著降低了人工審核成本。同時，輕量化的 DeepSeek-R1 推理模型則在自動化估值與對賬場景中展現出極佳的實時響應能力，推理速度提升了 30%，為金融業務流程的自動化提供了堅實支撐。&lt;/p&gt; 
&lt;span id="OSC_h2_3"&gt;&lt;/span&gt; 
&lt;h2&gt;新的基礎設施&lt;/h2&gt; 
&lt;p&gt;應用邊界在不斷拓寬的同時，多模態模型的能力也在成長。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;而隨着應用場景的深化，模型架構也在同步進化，從基礎感知邁向複雜推理成為必然趨勢。OpenAI 在 2025 年 4 月發佈了多模態模型 O3 和 O4-mini，實現了「用圖像思考」的突破性能力。這些模型不僅能夠識別圖像內容，還能將圖像信息整合進推理思維鏈，支持多步推理和因果分析，比如夠處理模糊、倒置或複雜的圖像輸入，並給出合理的推理結果。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;其背後的關鍵技術包括分層注意力機制，將圖像分解為局部細節、全局關係和時序邏輯三層結構，從而提升對圖像內容的理解能力；動態工具鏈調用，在推理過程中，模型可以自主選擇 Python 分析、知識圖譜檢索、圖像生成等工具輔助決策，以及安全約束模塊，通過對抗訓練減少模型的幻覺輸出。&lt;/p&gt; 
&lt;p&gt;就在本月，中國科學院自動化研究所等單位的科研人員&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fkw.beijing.gov.cn%2Fxwdt%2Fkcyx%2Fxwdtkjqy%2F202506%2Ft20250611_4111006.html" rel="nofollow" target="_blank"&gt;首次證實&lt;/a&gt;，多模態大語言模型在訓練過程中自己學會了「理解」事物，而且這種理解方式和人類非常像。&lt;/p&gt; 
&lt;p&gt;科研人員借鑑人腦認知的原理，設計了一個巧妙的實驗：讓大模型和人類玩「找不同」遊戲。實驗人員會給出三個物品概念（選自 1854 種常見物品），要求選出最不搭的那個。通過分析高達 470 萬次的判斷數據，科研人員繪製出了大模型的「思維導圖」——「概念地圖」。通過實驗證實多模態大模型具備類人「概念理解」能力。研究團隊設計「找不同」遊戲，基於 470 萬次判斷數據繪製大模型「概念地圖」，提煉 66 個理解維度（如物體功能、文化意義），發現其與人腦神經活動高度一致，證明多模態模型比純文本模型更接近人類思維模式。&lt;/p&gt; 
&lt;p&gt;據谷歌雲在 2024 年年底發佈的&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnews.qq.com%2Frain%2Fa%2F20241219A07AW200" rel="nofollow" target="_blank"&gt;《2025 年人工智能商業趨勢報告》&lt;/a&gt;，預測到 2025 年，多模態 AI 將成為企業採用 AI 的主要驅動力。這種技術通過整合圖像、視頻、音頻和文本等多種數據源，使 AI 能夠以前所未有的準確性從更廣泛的上下文源中學習，提供更精確、定製化的輸出，創造自然直觀的體驗。報告預計，全球多模態 AI 市場規模將在 2025 年達到 24 億美元，到 2037 年底達到 989 億美元。&lt;/p&gt; 
&lt;p&gt;2025 進度已經過半，我們也能看到市面上許多多模態技術和產品的進展，而這場變革的終極圖景，或許正是讓 AI 真正成為理解世界、服務人類的「多模態智能夥伴」。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/4489239/blog/18679654</link>
      <guid isPermaLink="false">https://my.oschina.net/u/4489239/blog/18679654</guid>
      <pubDate>Sun, 11 May 2025 07:31:00 GMT</pubDate>
      <author>原創</author>
    </item>
    <item>
      <title>Windows 11 記事本正式支持 Markdown</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;5 月底，預覽體驗計劃中的 Windows 11 記事本迎來史詩級更新：&lt;a href="https://www.oschina.net/news/353253/windows-notepad-markdown"&gt;支持 Markdown 格式&lt;/a&gt;。現在普通用戶也可以使用這個版本了，只需要在應用商店中更新記事本即可使用。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-af88d0da29ab2cf4246999e5b5025bf874c.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;目前支持：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;粗體&lt;/li&gt; 
 &lt;li&gt;斜體&lt;/li&gt; 
 &lt;li&gt;鏈接&lt;/li&gt; 
 &lt;li&gt;序號&lt;/li&gt; 
 &lt;li&gt;標題（H1～H5）&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;如下面的屏幕截圖所示，您可以點擊新的「H1」圖標，然後選擇您喜歡的標題：標題、副標題、章節，甚至是小節。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-387e4d297dcd5ac663e49554b55e2f51091.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;接下來，我們可以看到項目符號和數字列表按鈕，以及用於加粗或應用斜體的選項，但最吸引我注意的是超鏈接支持。現在，您可以使用 Ctrl + K 鍵盤快捷鍵（該快捷鍵在 Word 中也使用）插入帶有錨文本的鏈接，並在默認瀏覽器中打開。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-79d07dc4e8307d47c79e780de309830f227.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;此外，您可以點擊屏幕底部的「格式化視圖」按鈕切換到 Markdown 語法（原始）視圖。&lt;/p&gt; 
&lt;p&gt;與「格式」視圖不同，語法視圖允許您將井號轉換為標題、使用星號強調、用反引號包裹代碼等等。語法視圖類似於在後端使用 Markdown 編輯器，但它不會更改輸出。這取決於您在記事本中使用 Markdown 的方式。&lt;/p&gt; 
&lt;p&gt;在我們的測試中，Windows 最新版本觀察到記事本默認啓用 Markdown，但您有兩個選擇。您可以單擊清理格式按鈕，返回原始記事本體驗，而無需禁用 Markdown 支持。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-78b526954a85441998f9ce8f508e77c65f8.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;或者，您可以打開「設置」，向下滾動一點，找到一個名為「格式化」的新選項。關閉「格式化」後，記事本的經典體驗將恢復。您將不再看到格式化欄，Windows 也不會提示您使用它。&lt;/p&gt; 
&lt;p&gt;測試中，我們還注意到微軟在記事本中實現了非常輕量級的 Markdown 功能，它不會讓您的電腦運行速度變慢。&lt;/p&gt; 
&lt;p&gt;有些人可能會認為，給記事本添加太多功能違背了它作為純文本編輯器的初衷。這種觀點很有道理，但只要 Markdown 之類的功能是可選的，我並不介意。如果我需要它們，可以在「設置」中打開；如果不需要，也可以再次關閉。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;相關閲讀：&lt;a href="https://www.oschina.net/news/353253/windows-notepad-markdown" target="news"&gt;Windows 記事本支持 Markdown&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358574/windows-11-notepad-markdown</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358574/windows-11-notepad-markdown</guid>
      <pubDate>Sun, 11 May 2025 07:09:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>抖音內容技術團隊開源 ContentV：有限算力下高效訓練視頻生成模型的新路徑</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                                        &lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//adfb650ff16d8feb12d600710037b8a4.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;div class="ckeditor-html5-video" style="text-align:center"&gt; 
  &lt;video controls="controls" controlslist="nodownload" src="https://www.bilibili.com/video/BV1jC3azYEaS/?spm_id_from=333.1387.upload.video_card.click&amp;amp;vd_source=c09f0713b2507369924e94f4fec6c133"&gt;
    &amp;nbsp; 
  &lt;/video&gt; 
 &lt;/div&gt; 
 &lt;div class="ckeditor-html5-video" style="text-align:center"&gt; 
  &lt;video controls="controls" controlslist="nodownload" src="https://www.bilibili.com/video/BV1jC3azYEuW/?spm_id_from=333.1387.upload.video_card.click&amp;amp;vd_source=c09f0713b2507369924e94f4fec6c133"&gt;
    &amp;nbsp; 
  &lt;/video&gt; 
 &lt;/div&gt; 抖音內容技術團隊開源了 ContentV，一種面向視頻生成任務的高效訓練方案。該方案在多項技術優化的基礎上，使用 256 塊顯卡，在約 4 周內完成了一個 8B 參數模型的訓練。儘管資源有限，ContentV 在多個評估維度上取得了與現有主流方案相近的生成效果。該工作探索了在有限算力條件下訓練視頻生成模型的可行路徑。目前，推理代碼與模型權重已對外開放。 
&lt;/div&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;項目主頁&lt;/strong&gt;：https://contentv.github.io&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;技術報告&lt;/strong&gt;：https://arxiv.org/abs/2506.05343&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;代碼倉庫&lt;/strong&gt;：https://github.com/bytedance/ContentV&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;模型權重&lt;/strong&gt;：https://huggingface.co/ByteDance/ContentV-8B&lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="OSC_h2_1"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;核心亮點&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;極簡設計&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;CogVideoX、HunyuanVideo 和 Wan2.1 等一系列優秀的開源工作表明，視頻生成的關鍵並不在於架構上的特殊設計，而在於如何高效利用有限的數據資源，並有效對齊人類偏好。&lt;/p&gt; 
&lt;p&gt;為驗證 ContentV 方案的通用性，本次開源的版本在擴散模型部分採用了經典的文生圖模型 Stable Diffusion 3.5 Large。為了適配視頻模態，模型在結構上僅做了以下兩項必要調整：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;將原始圖像 VAE 替換為 Wan2.1 中使用的 3D-VAE；&lt;/li&gt; 
 &lt;li&gt;將 2D 位置編碼升級為 3D 版本。在具體編碼方式上，團隊對比了傳統的絕對位置編碼與主流的旋轉位置編碼。評估結果顯示，兩者在客觀指標和主觀感受上差異較小，因此保留了計算更高效的絕對位置編碼方案。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//a970cfacc9f335795a1cb051ba654811.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="text-align:center"&gt;ContentV 模型結構‌&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;多階段漸進訓練策略&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;上述的最小化結構改動，在解鎖了視頻生成能力的同時，也最大限度地保留了原模型的圖像生成能力。實驗證明，在新的 VAE 和位置編碼的適配階段，沿用 Flow Matching 的訓練方式，僅需 1000 步左右的微調，就能基本還原模型的圖片生成能力，大幅節省圖片預訓練階段的訓練成本。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//da7aab81f63561b7c0d8679eceba1022.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="text-align:center"&gt;VAE 適配過程‌&lt;/p&gt; 
&lt;p&gt;在視頻生成的預訓練階段，為加速收斂實現高效訓練，研究團隊設計了一套從「低清短片」到「高清長片」的多階段漸進式訓練流程，逐步引導模型學習時間維度與空間維度上的動態表徵，從而提升視頻的連續性、動態表現力和畫面細節。&lt;/p&gt; 
&lt;p&gt;此外，實驗證明，在推理階段引入非線性採樣步長機制（Flow Shift）能夠顯著提升視頻的整體生成質量。通過多組對比實驗，團隊最終確定了最優的採樣策略，進一步優化了生成效果。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;輕量級 RLHF 強化訓練&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//25bf5c0471707e1d93343f8649c7f46a.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//aaf7366e824c8a20dc80a43af6d4e872.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="text-align:center"&gt;RLHF 顯著提升畫面質感‌&lt;/p&gt; 
&lt;p&gt;在後訓練階段，除了使用高質量數據集進行微調外，通過 RLHF 或 DPO 等對齊人類偏好的監督訓練，也能顯著提升視頻生成質量。然而，這類方法通常依賴大量人工標註，用於訓練獎勵模型或直接監督擴散模型。同時，相較於圖像，視頻的序列長度顯著增加了 RLHF 和 DPO 的訓練資源需求。&lt;/p&gt; 
&lt;p&gt;為此，ContentV 研究團隊提出了一種輕量級的 RLHF 訓練方案，旨在不依賴人工標註的前提下，低成本提升視頻質量：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;利用開源的圖像獎勵模型對生成視頻的單幀進行監督。相較於視頻場景，目前圖像獎勵模型的訓練數據更易獲取，且在實際效果中表現更佳。實驗證明，由於 MM DiT 採用全局注意力機制，僅優化單幀即可帶動整體視頻質量的提升；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;將監督範圍限制在生成視頻的前 1 秒，相較於對完整視頻進行監督，可大幅減少訓練資源的消耗，同時獲得相近的質量提升效果。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;採用上述策略後，在無需人工標註的情況下，僅使用少量訓練資源，便可顯著提升畫面質量。RLHF 微調後，模型在視覺質量（VQ）指標上的表現大幅提升，評估勝率高達 89.38%。&lt;/p&gt; 
&lt;span id="OSC_h2_2"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;效果對比&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;在 VBench 這一主流視頻生成評測基準上，ContentV（8B）取得了 85.14 的綜合得分，表現優於多個現有的商業閉源模型，包括 Sora、Kling 1.6 和 Gen-3 等。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//2e5d2e9c6fb2a651ef95db56aa008420.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="text-align:center"&gt;VBench 榜單 (按照 Overall 分數降序排列)‌&lt;/p&gt; 
&lt;p&gt;為更貼近真實用戶偏好，研究團隊圍繞感知質量、指令跟隨、物理一致性和視覺效果四個維度開展了人類偏好評估。結果顯示，ContentV 在整體表現上與 CogVideoX-5B、HunyuanVideo-13B 和 Wan2.1-14B 等主流開源模型相比具有一定優勢。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//547c06ccbfa684f90c21d1432e0c6786.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="text-align:center"&gt;人類偏好評估指標‌&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/6210722/blog/18683305</link>
      <guid isPermaLink="false">https://my.oschina.net/u/6210722/blog/18683305</guid>
      <pubDate>Sun, 11 May 2025 06:53:00 GMT</pubDate>
      <author>原創</author>
    </item>
    <item>
      <title>Bilibili 開源動漫視頻生成模型 AniSora V3 版</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;Bilibili（B 站）宣佈其開源動漫視頻生成模型&lt;strong&gt;AniSora&lt;/strong&gt;迎來重大更新，正式發佈&lt;strong&gt;AniSora V3&lt;/strong&gt;。作為 Index-AniSora 項目的一部分，V3 版本在原有基礎上進一步優化了生成質量、動作流暢度和風格多樣性，為動漫、漫畫及 VTuber 內容創作者提供了更強大的工具。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="292" src="https://oscimg.oschina.net/oscnet/up-27ad0a0400878a1e24e9451fd20c7a87882.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;AniSora V3 基於 Bilibili 此前開源的 CogVideoX-5B 和 Wan2.1-14B 模型，結合&lt;strong&gt;強化學習與人類反饋（RLHF）&lt;/strong&gt;框架，顯著提升了生成視頻的視覺質量和動作一致性。其支持一鍵生成多種風格的動漫視頻鏡頭，包括番劇片段、國創動畫、漫畫視頻改編、VTuber 內容）等。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;核心升級包括：&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;ul style="margin-left:0; margin-right:0"&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;時空掩碼模塊（Spatiotemporal Mask Module）優化&lt;/strong&gt;：V3 版本增強了時空控制能力，支持更復雜的動畫任務，如精細的角色表情控制、動態鏡頭移動和局部圖像引導生成。例如，提示「五位女孩在鏡頭放大時起舞，左手上舉至頭頂再下放至膝蓋」能生成流暢的舞蹈動畫，鏡頭與角色動作同步自然。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;數據集擴展&lt;/strong&gt;：V3 繼續依託超過 1000 萬高質量動漫視頻片段（從 100 萬原始視頻中提取）進行訓練，新增數據清洗流水線，確保生成內容的風格一致性和細節豐富度。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;硬件優化&lt;/strong&gt;：V3 新增對華為 Ascend910B NPU 的原生支持，完全基於國產芯片訓練，推理速度提升約 20%，生成 4 秒高清視頻僅需 2-3 分鐘。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;多任務學習&lt;/strong&gt;：V3 強化了多任務處理能力，支持從單幀圖像生成視頻、關鍵幀插值到脣部同步等功能，特別適合漫畫改編和 VTuber 內容創作。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;在最新基準測試中，AniSora V3 在&lt;strong&gt;VBench&lt;/strong&gt;和雙盲主觀測試中，角色一致性和動作流暢度均達到業界頂尖水平（SOTA），尤其在複雜動作 (如違反物理規律的誇張動漫動作) 上表現突出。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;Bilibili 強調，AniSora 是「對動漫世界的開源禮物」，鼓勵社區協作優化模型。用戶需填寫申請表併發送至指定郵箱（如 yangsiqian@bilibili.com）以獲取 V2.0 權重和完整數據集訪問權限。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;V3 還引入了首個針對動漫視頻生成的&lt;strong&gt;RLHF 框架&lt;/strong&gt;，通過 AnimeReward 和 GAPO 等工具對模型進行微調，確保輸出更符合人類審美和動漫風格需求。社區開發者已開始基於 V3 開發定製化插件，例如增強特定動漫風格（如吉卜力風）的生成效果。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;AniSora V3 支持多種動漫風格，包括日本動漫、國產原創動畫、漫畫改編、VTuber 內容及惡搞動畫（鬼畜動畫），覆蓋 90% 的動漫視頻應用場景。 具體應用包括：&lt;/span&gt;&lt;/p&gt; 
&lt;ul style="margin-left:0; margin-right:0"&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;單圖轉視頻&lt;/strong&gt;：用戶上傳一張高質量動漫圖像，配合文本提示（如「角色在向前行駛的車中揮手，頭髮隨風擺動」），即可生成動態視頻，保持角色細節和風格一致。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;漫畫改編&lt;/strong&gt;：從漫畫幀生成帶脣部同步和動作的動畫，適合快速製作預告片或短篇動畫。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;VTuber 與遊戲&lt;/strong&gt;：支持實時生成角色動畫，助力獨立創作者和遊戲開發者快速測試角色動作。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;高分辨率輸出&lt;/strong&gt;：生成視頻支持高達 1080p，確保在社交媒體、流媒體平台上的專業呈現。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;AIbase 測試顯示，V3 在生成複雜場景（如多角色交互、動態背景）時，相比 V2 減少了約 15% 的偽影問題，生成時間縮短至平均 2.5 分鐘 (4 秒視頻)。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;相比 OpenAI 的 Sora 或 Kling 等通用視頻生成模型，AniSora V3 專注於動漫領域。 與字節跳動的 EX-4D 相比，AniSora V3 更專注於 2D/2.5D 動漫風格，而非 4D 多視角生成。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358565</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358565</guid>
      <pubDate>Sun, 11 May 2025 06:46:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>智譜 AI 開源通用視覺推理模型 GLM-4.1V-Thinking</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;智譜 AI 於 7 月 2 日發佈了 GLM-4.1V-Thinking 系列通用視覺推理模型，並宣佈獲得來自浦東創投集團和張江集團的 10 億元聯合戰略投資。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/143332_18Al_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;同時，公司推出了全新生態平台「Agent 應用空間」，並啓動「Agents 開拓者計劃」，投入數億資金扶持 AI Agents 創業團隊。&lt;/p&gt; 
&lt;p&gt;為慶祝模型發佈，智譜大模型開放平台為用戶提供新模型 Flash 版 1 億的「高併發版」Tokens，同時，該模型可通過 API 免費使用。&lt;/p&gt; 
&lt;p&gt;此次率先開源的是 GLM-4.1V-9B-Thinking，一個 9B 參數量的多模態模型，對應官方平台的 GLM-4.1V-Thinking-Flash。該模型旨在提升模型的深度思考與複雜推理能力。該模型在多項基準測試中表現卓越，其性能在 18 項任務上持平甚至超過了參數量為其 8 倍的 Qwen-2.5-VL-72B 和 GPT-4o 等主流視覺語言模型。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/143242_aYUB_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;模型具備強大的多模態能力，能夠解析長達 2 小時的視頻、進行數學與科學推理、看圖編寫網頁，並具備 GUI Agent 能力，可識別並操作手機、電腦等屏幕界面元素，完成用戶指令。例如，在解析足球比賽時，模型能理解球員位置和戰術特點。&lt;/p&gt; 
&lt;p&gt;GLM-4.1V-Thinking 模型架構由視覺編碼器、MLP 適配器和語言解碼器組成，其卓越性能得益於引入了「課程採樣強化學習」（Reinforcement Learning with Curriculum Sampling）策略，通過由易到難的訓練任務安排，高效提升了模型在 STEM 解題、智能體任務、文檔圖表理解等多個領域的推理能力。&lt;/p&gt; 
&lt;p&gt;目前，GLM-4.1V-9B-Thinking 模型已在 GitHub、魔搭社區及 Hugging Face 上開源。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;開源列表&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;文檔：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbigmodel.cn%2Fdev%2Fhowuse%2Fvisual-reasoning-model%2Fglm-4.1v-thinking" target="_blank"&gt;https://bigmodel.cn/dev/howuse/visual-reasoning-model/glm-4.1v-thinking&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Github：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FTHUDM%2FGLM-4.1V-Thinking" target="_blank"&gt;https://github.com/THUDM/GLM-4.1V-Thinking&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ModelScope：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmodelscope.cn%2Fcollections%2FGLM-41V-35d24b6def9f49" target="_blank"&gt;https://modelscope.cn/collections/GLM-41V-35d24b6def9f49&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Hugging Face：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fcollections%2FTHUDM%2Fglm-41v-thinking-6862bbfc44593a8601c2578d" target="_blank"&gt;https://huggingface.co/collections/THUDM/glm-41v-thinking-6862bbfc44593a8601c2578d&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;HuggingFace 體驗鏈接：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fspaces%2FTHUDM%2FGLM-4.1V-9B-Thinking-Demo" target="_blank"&gt;https://huggingface.co/spaces/THUDM/GLM-4.1V-9B-Thinking-Demo&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358562/glm-4-1-v-thinking</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358562/glm-4-1-v-thinking</guid>
      <pubDate>Sun, 11 May 2025 06:32:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>開源中國聯合發起「全球數字友好開源社區」，共建開放協同新生態</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;2025 年 7 月 2 日，2025 全球數字經濟大會在北京國家會議中心隆重開幕。本次大會經國務院批准，由北京市人民政府、國家互聯網信息辦公室、國家數據局、新華通訊社與聯合國開發計劃署共同主辦，聚焦「建設數字友好城市」主題，來自全球多國政府機構、國際組織、城市代表、科研院所和科技企業代表齊聚一堂，圍繞數字技術賦能城市發展的路徑與合作機制深入交流。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/120249_MSqc_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在大會首場主論壇「數字友好城市建對話」階段，&lt;strong&gt;北京市經開區工委副書記、管委會副主任王磊指出北京正在加快打造以「模力方舟國際開源社區」為代表的 AI 開放創新平台集羣&lt;/strong&gt;，匯聚全球 AI 開發者資源，支撐企業間協同與城市間互信，推動開源力量深度融入全球數字治理體系。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/120301_ICF0_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;隨後，主論壇迎來了重點環節之一——&lt;strong&gt;「全球數字友好開源社區」正式啓動&lt;/strong&gt;。該社區由開源中國、統信軟件、平凱星辰等十八家中外企業、聯盟和機構共同發起，旨在打造面向全球的數字化開放協同平台。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;開源中國作為發起單位之一，研發副總裁李彥成代表公司出席啓動儀式，並與各方共同見證社區成立&lt;/strong&gt;。開源社區已成為推動全球數字協作與技術創新的重要力量。從早期由開發者驅動的協作模式，到如今以城市、企業、場景多元聯動為特徵的深度共建，開源正在加速融入數字城市治理的底層邏輯。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/120314_bEZv_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;開源已成為推動全球數字協作與技術創新的重要力量。從早期由開發者社區推動的技術共享，到如今以城市、企業、場景多元聯動為特徵的深度協作，開源的發展路徑正不斷拓展邊界。在聯合國「數字促進可持續發展」倡議框架下，「全球數字友好開源社區」應運而生，標誌着開源協作正在進入以城市友好關係為紐帶、以產業聯合體為主體的新階段。&lt;/p&gt; 
&lt;p&gt;作為國家重點開源基礎設施平台之一，開源中國·Gitee 始終致力於建設安全、自主、可控的開源生態。&lt;strong&gt;此次參與社區聯合發起，是開源中國積極服務國家數字戰略、深度參與國際開源共建進程的重要舉措&lt;/strong&gt;。依託自身在開源治理、企業級協同平台、人工智能服務平台等方向的長期積累，開源中國將與生態夥伴一道，共同推動開源協作從「項目共建」走向「城市共建」，為打造全球數字友好生態注入持續動能。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/120331_bONF_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在數字技術高速演進、人工智能重塑產業格局的時代背景下，構建開放、包容、可持續的全球協作機制顯得尤為重要。開源中國將繼續秉持開放精神，深度參與社區建設，攜手全球夥伴共建共享，為推動構建人類數字命運共同體貢獻更多開源力量。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358536</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358536</guid>
      <pubDate>Sun, 11 May 2025 04:03:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>馬斯克旗下人工智能公司 xAI 完成 100 億美元融資</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;馬斯克旗下人工智能公司 xAI 近日&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.cnbc.com%2F2025%2F07%2F01%2Felon-musk-xai-raises-10-billion-in-debt-and-equity.html" target="_blank"&gt;完成了 100 億美元融資&lt;/a&gt;，其中包括 50 億美元債務融資和 50 億美元戰略股權投資。這筆資金將用於開發 AI 解決方案、建設數據中心，並推動其旗艦 AI 助手 Grok 的進一步發展。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/114408_7iP8_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;除已完成的融資外，xAI 仍在洽談約 200 億美元的股權融資。若成功，其估值可能飆升至 1200 億至 2000 億美元，成為全球最具價值的 AI 公司之一。&lt;/p&gt; 
&lt;p&gt;然而，知情人士透露，xAI 的運營成本極高——2025 年預計將消耗 130 億美元，相當於每月燒錢超 10 億美元。目前的大規模融資僅能勉強跟上其鉅額開支，未來仍需持續輸血以維持技術研發和市場擴張。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358529/elon-musk-xai-raises-10-billion-in-debt-and-equity</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358529/elon-musk-xai-raises-10-billion-in-debt-and-equity</guid>
      <pubDate>Sun, 11 May 2025 03:44:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>搜索數據建設系列之數據架構重構</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                                        &lt;span id="OSC_h1_1"&gt;&lt;/span&gt; 
&lt;h1&gt;導讀&lt;/h1&gt; 
&lt;p&gt;主要概述百度搜索業務數據建設的創新實踐，重點圍繞寬表模型設計、計算引擎優化和新一代業務服務交付模式（圖靈 3.0 開發模式）三大方向，解決了傳統數倉在搜索場景下面臨的諸多挑戰，實現了搜索數據建設的高效、穩定、低成本；為百度搜索業務敏捷迭代奠定夯實基礎。&lt;/p&gt; 
&lt;span id="OSC_h1_2"&gt;&lt;/span&gt; 
&lt;h1&gt;&lt;strong&gt;名詞解釋&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;TDS（Turing Data Studio）&lt;/strong&gt;&lt;/strong&gt;： 是基於圖靈（百度內部數據分析平台）的數據建設解決方案，提供，數據開發、數倉管理、監控運維、資源管理等一站式服務的數據開發平台。詳情參見：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247599508%26idx%3D1%26sn%3D19094522609ca58528295a8ccbb061bd%26chksm%3Dc03f75e8f748fcfe192c0c8817ceee53c0e1f57dcbafd16e22ee371889f4dbf9e0b813b700fa%26token%3D1515601853%26lang%3Dzh_CN%26scene%3D21%23wechat_redirect" target="_blank"&gt;百度 MEG 數據開發治理平台-TDS&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;TDA（Turing Data Analysis）&lt;/strong&gt;&lt;/strong&gt;：是一款可視化 BI 產品，旨在幫助用戶輕鬆上手及使用，進行拖拽式可視化數據分析及儀表盤建設。產品模塊包括儀表盤、數據集、可視化分析及智能分析模塊。詳情參見：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247584876%26idx%3D1%26sn%3D7bf459415ef8d51685d4e1c335b0603e%26chksm%3Dc03fbc10f7483506eb7206b02265f9010ddfa434359d7fa975a32d54c1b4886e734051ff9067%26scene%3D21%23wechat_redirect" target="_blank"&gt;百度一站式數據自助分析平台（TDA）建設&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;TDE（Turing Data Engine ）&lt;/strong&gt;&lt;/strong&gt;：是基於圖靈生態的計算引擎，包含 Spark 計算引擎和 ClickHouse。詳情參見：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247581388%26idx%3D1%26sn%3De71a4f3c4ca283ac6e8fe51d0a45a02b%26scene%3D21%23wechat_redirect" target="_blank"&gt;揭祕百度數倉融合計算引擎&lt;/a&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247601378%26idx%3D1%26sn%3D9234aeef05c3813cfb34d9a064261984%26chksm%3Dc03f7c9ef748f5886cc17147d8dfc8bd62ce062de822444ec592914d5ca6e0ab224cfc963e3e%26token%3D286675411%26lang%3Dzh_CN%26scene%3D21%23wechat_redirect" target="_blank"&gt;ClickHouse 在百度 MEG 數據中台的落地和優化&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;UPI（Udw-API）&lt;/strong&gt;&lt;/strong&gt;：百度內部編程訪問接口；以 Map/Reduce 計算框架為主，適用於計算邏輯複雜，以及多種數據源混合計算的例行離線數據挖掘業務&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;數倉融合計算引擎&lt;/strong&gt;&lt;/strong&gt;：是百度自主研發，基於 spark 自研的 adhoc 服務。提供數據查詢分析，具有簡單易用、超大規模支持、成本極低等特點，能實現 T 級數據秒級查詢，也適用於例行生產的 ETL 場景。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;函谷&lt;/strong&gt;&lt;/strong&gt;：圖靈核心模塊，作為圖靈查詢的 gateway，完成圖靈查詢的接收、分發、提交執行、查詢進展輪詢、結果獲取等一系列功能。&lt;/p&gt; 
&lt;span id="OSC_h1_3"&gt;&lt;/span&gt; 
&lt;h1&gt;01 背景與問題&lt;/h1&gt; 
&lt;span id="OSC_h2_4"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;1.1 背景&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;在當今互聯網產品發展日新月異、業務迭代迅猛的時代；跨業務分析的需求日益增長，這種變化既為企業創造了敏捷決策、精準運營的新機遇，也帶來數據割裂、價值釋放滯後等嚴峻挑戰。特別是大型互聯網企業，往往構建有複雜的多業務、多模塊、多線條體系，每日持續產出海量的數據信息。這些數據的服務對象正逐步從數據研發人員擴展至更為廣泛的產品相關人員，如何高效開展數據獲取工作，打破數據孤島現象，充分挖掘並釋放數據驅動業務的潛力，已成為業界廣泛關注和討論的焦點議題。針對該問題，業界傳統數倉常採用的是經典分層模型的數倉架構，從 ODS（Operational Data Store）&amp;gt;DWD（Data Warehouse Detail）&amp;gt;DWS（Data Warehouse Summary）&amp;gt;ADS（Application Data Store）逐層建模，但我們會發現，從傳統固化開發的角度來看，傳統經典數倉模型是比較有優勢的。然而，面對當下數據需求靈活多變的時代，其侷限性也日益凸顯。如下圖&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-939fa5fcdf0baaf3f9376861ee1c7269bb5.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h2_5"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;1.2 搜索場景下的困境與挑戰&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;搜索作為百度的核心支柱業務，涵蓋通用搜索、智能搜索、阿拉丁與垂類等多元化、多模態的搜索產品，具有&lt;strong&gt;&lt;strong&gt;快速迭代、模塊多元化且複雜&lt;/strong&gt;&lt;/strong&gt;的特性，搜索數據更是複雜多樣，整體數倉規模達到數百 PB 以上。&lt;/p&gt; 
&lt;p&gt;隨着搜索業務各個模塊之間的聯繫日益緊密，交叉分析的需求也在不斷增長。使用人員對數據獲取的便捷性提出了更高的要求，其中涵蓋了數據分析師、策略、業務產品經理、運營、評估等多類角色。他們的訴求期望能夠跨越複雜的數據架構壁壘，以更加&lt;strong&gt;&lt;strong&gt;高效、直觀、快速&lt;/strong&gt;&lt;/strong&gt;的方式獲取到所需數據。&lt;/p&gt; 
&lt;p&gt;而傳統的搜索數倉建設體系，無論從建模角度還是技術框架上，都與現階段用戶訴求背道而馳。&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;建模角度：多層的傳統分層建模。往往會出現（大表數據量大、查詢慢、存儲冗餘、口徑不統一）等問題，影響業務分析效率，從而達不到數據驅動業務的效果。數據開發側作為需求的被動承接方，根據業務側提出的數據需求進行數據開發與交付，存在需求交付週期長、人力成本高等問題。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;技術框架角度：搜索數倉過去大多是採用&lt;strong&gt;&lt;strong&gt;UPI&lt;/strong&gt;&lt;/strong&gt;框架（以 C++ MR 計算框架為主）進行 ETL 處理。由於該框架技術陳舊，往往會出現以下問題影響數倉整體時效、穩定。從而使業務部門感知需求支持遲緩、數據產出延遲及數據質量低等一系列問題。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;容易出現服務不穩定。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;處理能力薄弱：處理不了特殊字符，從而導致數據丟失或任務失敗等。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;只能通過物理機遠程執行的方式提交，有單節點風險。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;無法 Writer 將數據寫到 Parquet 文件，需要進行特定腳本 ETLServer 框架進行轉換。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;思考&lt;/strong&gt;&lt;/strong&gt;：如何更好的滿足用戶角色需求，進一步降低數據獲取的使用門檻？&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;破局&lt;/strong&gt;&lt;/strong&gt;：擁抱變化，以用戶訴求為核心出發點。 探索更適合用戶的 &lt;strong&gt;&lt;strong&gt;體系化建模&lt;/strong&gt;&lt;/strong&gt; 來進行實質、有效的數據管理。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;體系化建模：&lt;/strong&gt;以業務產品需求驅動模型設計，以模型設計驅動和約束開發實施，防止因模型設計與業務產品割裂、開發實施缺少約束帶來的無序、「煙囪式」開發。在機制上形成模型設計與開發實施的有效協同。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;切入點&lt;/strong&gt;&lt;/strong&gt;：以規範「基礎數據建設」，消除因「煙囪式」開發給業務帶來的困擾和技術上的浪費。&lt;/p&gt; 
&lt;p&gt;由此我們探索出一套新的建模體系：&lt;strong&gt;大寬表+數據集&lt;/strong&gt;：其核心點就是基於寬表，將之前的"需求-交付"解耦為以數據集為中心的建設，從而提升搜索內業務數據分析效率與分析深度，更好助力業務決策。以下將從寬表建模、計算引擎架構優化、新型業務交付模式等方向為大家介紹搜索數據團隊業務實踐。&lt;/p&gt; 
&lt;span id="OSC_h1_6"&gt;&lt;/span&gt; 
&lt;h1&gt;02 搜索建模思路與技術方案&lt;/h1&gt; 
&lt;span id="OSC_h2_7"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;2.1 建模模型&lt;/strong&gt;&lt;/h2&gt; 
&lt;span id="OSC_h3_8"&gt;&lt;/span&gt; 
&lt;h3&gt;2.1.1 思路&lt;/h3&gt; 
&lt;p&gt;基於搜索產品功能特性與差異化業務場景，我們對日誌數據進行主題化的分類。在每個主題域內，結合業務運營的具體環節特徵，構建具備高整合度的寬表模型。在模型構建過程中，保持 ODS（操作數據存儲）層與 DWD（明細數據層）的表結構粒度一致，確保數據的一致性與連貫性。所構建的寬表不僅完整涵蓋下游業務所需的全部字段，包括業務明細表中的基礎數據，還整合了各層級的維度屬性與計算指標。通過這種方式，形成一個全面、統一的數據底座，為上層業務的多維分析、指標監控及決策支持提供堅實的數據支撐，有效滿足多樣化的業務分析需求。&lt;/p&gt; 
&lt;span id="OSC_h4_9"&gt;&lt;/span&gt; 
&lt;h4&gt;2.1.1.1 舉例&lt;/h4&gt; 
&lt;p&gt;以展點主題為例，從歷史的模型表粒度和模型層級來分析：ODS 與 DWD、DWS 錶行為、檢索、點擊各個主題在同層模型或者跨模型之間都存在字段、口徑的冗餘，如下圖&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-a9f6f8e899639d2fe381e3fd22248b8c407.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h4_10"&gt;&lt;/span&gt; 
&lt;h4&gt;2.1.1.2 思路分析過程&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;核心思想過程：展點主題下明確粒度，豐富維度&amp;amp;指標，建設寬表模型。&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;將展點主題下各層之間的事實表複雜嵌套字段打平後與各個維度表、指標等進行 join 生成寬表，寬表的列最終分為公共屬性、展點行為屬性、業務屬性和指標屬性。&lt;/p&gt; 
&lt;p&gt;消除：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;數倉層間：字段存儲冗餘問題&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;數倉層內：口徑不一致問題&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-75e2f39732709336398f3ea0e2f605afc62.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-cc8563c18a8af68b1e8ce82a384dea8aa25.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-c837b8c7763edc5c4290cd758752771d0ce.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h3_11"&gt;&lt;/span&gt; 
&lt;h3&gt;2.1.2 建模核心思想&lt;/h3&gt; 
&lt;p&gt;基於思路分析過程，總結出一套核心建模理論，核心思想如下圖&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-59b8209db3232d786eb0920bd402783d568.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;構建搜索系統性數據建模：根據產品功能和業務不同，按照不同主題構建寬表。從而達到節約存儲、精簡表數量、口徑更清晰的目標。&lt;/p&gt; 
&lt;span id="OSC_h3_12"&gt;&lt;/span&gt; 
&lt;h3&gt;2.1.3 整體模型架構&lt;/h3&gt; 
&lt;p&gt;基於核心建模思想理論得到整體的模型架構，如下圖&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-65a11aadcf98271e6926e75e6f4de59f9cf.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;採用 Parquet 列式存儲，可支持寬表數百、千列，超多字段，再經過按列的高效壓縮（bucket sort 後，壓縮率更高），降低了數倉整體存儲空間，提高了 IO 效率，起到了降低上層應用延遲的效果。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;將各層之間的表複雜嵌套字段打平後與各個維度表、指標等進行 join 生成百列寬表，寬表的列最終分為公共屬性、業務維度屬性和指標屬性，便於業務分析，實現快速迭代。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="OSC_h2_13"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;2.2 計算引擎&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;為了保證數據生產穩定、準確性。我們對計算引擎的選擇做了升級，採用傳統 Spark 結合&lt;strong&gt;&lt;strong&gt;數倉融合計算引擎&lt;/strong&gt;&lt;/strong&gt;對搜索數倉 ETL 進行重構。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-a8f5a69d8cba77d36bc869cdaa8f603c4e6.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h3_14"&gt;&lt;/span&gt; 
&lt;h3&gt;2.2.1 從架構&amp;amp;處理流程上&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;C++ MR&lt;/strong&gt;&lt;/strong&gt; ：多進程，每個任務獨立運行，必須經過 Map-Shuffle-Reduce，然後中間結果寫磁盤。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;Spark&lt;/strong&gt;&lt;/strong&gt; ：多線程，任務在 Executor 內以線程執行。基於 DAG，可以在內存中緩存數據，減少 IO。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Spark 框架，相較於 MR 框架優勢在於&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;基於內存計算，處理速度快。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;支持多種計算模式，功能豐富，適合迭代處理數據。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;提供了高級的 API，開發效率高。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;基於平台提交，有效避免單節點計算風險。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;且在有 shuffle 情況下計算表現更好（MR 在 Shuffle 時默認進行排序，spark 對 shuffle 有優化，只有在部分場景才需要排序），在具體業務實踐中：同耗時的情況下，Spark 計算資源相較於 MR 節省 20% 左右。&lt;/p&gt; 
&lt;span id="OSC_h3_15"&gt;&lt;/span&gt; 
&lt;h3&gt;2.2.2 ETLServer 到數倉融合引擎轉變&lt;/h3&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-91ec2c9be50fe59678b383ecdee2fe2439a.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;各主題寬表模型的計算通過&lt;strong&gt;&lt;strong&gt;數倉融合計算引擎&lt;/strong&gt;&lt;/strong&gt;（通過 Spark Application Context 常駐方式做到資源有效複用；省去了啓動 Driver 的時間實現任務的快速啓動，來提升任務執行時間）可直接 Writer 將數據寫到 Parquet 文件，文件無需進行多次腳本 server 轉換。&lt;/p&gt; 
&lt;p&gt;在具體業務實踐中，各主題計算耗時由之前 40min 縮短至 10min（減少了 30min），實現數倉快速產出。&lt;/p&gt; 
&lt;span id="OSC_h2_16"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;2.3 新數據模型及架構下的挑戰與解決方案&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;任何數倉模型架構不會存在一個絕對完美的、涵蓋所有方面的解決方案。寬表設計僅是當前環境數倉模型的最優解，它依然面臨着諸多不容忽視的挑戰。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-d897150d0eb4ebe4cbfe141af1e656db0a7.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h3_17"&gt;&lt;/span&gt; 
&lt;h3&gt;2.3.1 挑戰 1 解決方案&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;strong&gt;列式存儲&amp;amp;讀取：&lt;/strong&gt;&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;寬表採用了 Parquet 列式存儲，以及 ZSTD 高效壓縮算法。結合數倉融合引擎，達到 Data Skipping（即讀的越少、計算越快）的效果，僅需讀取查詢涉及的分區及列，減少了磁盤 I/O 和內存傳輸的數據量來提升查詢效率，通過 Sql 分析服務發現熱點複雜字段，主動引導業務建設物化列，命中後查詢性能提升 80%。&lt;/p&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;strong&gt;複雜嵌套字段打平&lt;/strong&gt;&lt;/strong&gt;：&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;業務常用核心指標以及高頻字段口徑下沉寬表。雖然行數變多了，但是避免了 explode，get_json_object、array、map 等複雜字段獲取的耗時操作，查詢性能相較於之前提升了 2.1 倍。&lt;/p&gt; 
&lt;span id="OSC_h3_18"&gt;&lt;/span&gt; 
&lt;h3&gt;2.3.2 挑戰 2 解決方案&lt;/h3&gt; 
&lt;p&gt;搜索數據升級到了湖倉一體架構，藉助&lt;strong&gt;&lt;strong&gt;Iceberg Merge Into&lt;/strong&gt;&lt;/strong&gt;功能，實現高效回溯方式：對錶數據進行行級別的更新或刪除。相比 insert overwrite 操作更加高效，減少了不必要的數據移動和存儲浪費。&lt;/p&gt; 
&lt;p&gt;通過單一原子操作實現了複雜的數據整合需求。相比傳統的先刪除再插入的方式，&lt;strong&gt;&lt;strong&gt;Merge Into&lt;/strong&gt;&lt;/strong&gt;提供了更好的性能和一致性保證，其原理是通過重寫包含需要刪除和更新行數據所在的 date files。Merge Into 可以使用一個查詢結果數據來更新目標表的數據，其語法類似 join 關聯方式，根據指定的匹配條件對匹配的行數據進行相應的操作&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Merge Into&lt;/strong&gt;&lt;/strong&gt;基本語法&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-d2bd76ce88c3a899a2b4c7e9030565f332e.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;回溯原理流程如下圖&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-65989e3d43c224d06d336056c29f796c598.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;1. 關聯匹配&lt;/strong&gt;&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;目標表和源表根據指定 key 進行 join 操作。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;2. 條件判斷&lt;/strong&gt;&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;若 Key 匹配&lt;/strong&gt;&lt;/strong&gt;：根據源表操作類型，對目標表中的記錄執行相應的操作（更新或刪除）。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;若 Key 不匹配&lt;/strong&gt;&lt;/strong&gt;：執行 Insert 操作，將源表數據插入目標表。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;3. 原子性操作&lt;/strong&gt;&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;整個流程是事務性的，確保數據一致性。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;以下是特定回溯場景下 hive 與 iceberg 不同方式的回溯耗時對比，可以看的出來用 merge into 代替 insert overwrite 進行回溯，回溯更新效率整體可提高&lt;strong&gt;&lt;strong&gt;54% 左右。&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-83ae59dc2b2014d6f454f8ccb8112fdcaa4.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h3_19"&gt;&lt;/span&gt; 
&lt;h3&gt;2.3.3 挑戰 3 解決方案&lt;/h3&gt; 
&lt;span id="OSC_h4_20"&gt;&lt;/span&gt; 
&lt;h4&gt;2.3.3.1 重排序、高效壓縮&lt;/h4&gt; 
&lt;p&gt;開發 ATO 優化器 (通過任務依次執行重排序、壓縮等一系列 Rules，實現分區優化和數據重分佈)，高效率壓縮，解決存儲成本，存儲節約 20%。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-0748d5efaac1312246563e0bce0ba04e1c3.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;（1）壓縮編碼&lt;/p&gt; 
&lt;p&gt;數倉表字段元信息採集：通過任務對圖靈寬表表進行字段元信息採集，分析數據分佈情況，獲取重排序字段。&lt;/p&gt; 
&lt;p&gt;具體做法：通過 RLE、Delta 等縮碼方式來提升數據壓縮效率；數據重複度越高、連續性越好（有序）的場景，壓縮效率會越高，RLE、Delta 編碼原理如下。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-e0a7287f220ad886011cb4c0b407334ac22.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;（2） 壓縮格式&lt;/p&gt; 
&lt;p&gt;使用 ZSTD 壓縮格式和更大的壓縮 level，在不影響查詢性能的情況下，更大的壓縮 level 能進一步提高壓縮率，level=9 時在壓縮效率和耗時上最為平衡，讀寫耗時和壓縮率對比效果如下。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-9f7b43c93732b26daa2a8d7c38e2da18252.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;（3） Page Size&lt;/p&gt; 
&lt;p&gt;針對 Parquet 文件格式特性進行深入挖掘 ，對 Parquet page size 進行分頁擴容，將 Page Size 從 1MB 增大至 5MB，讓更多相似的數據集中到同一個數據頁中，充分利用編碼的壓縮特性，進一步減少各個數據頁之間存在的相似數據。在 ZSTD 的基礎上，能進一步提升壓縮效果，效果如下&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-e1a9ccfc36925c7c5a6ba0e90ecb3ae1a78.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h4_21"&gt;&lt;/span&gt; 
&lt;h4&gt;2.3.3.2 歷史裁剪，管理有效字段&lt;/h4&gt; 
&lt;p&gt;開發了一套半自動化的通用裁剪模式，通過採集日常任務代碼，sql parser 模塊解析出無用字段信息（尤其是大 json 大 map 類型擴展字段的無用字段）自動化實現了裁剪。減少了 &lt;strong&gt;&lt;strong&gt;50%&lt;/strong&gt;&lt;/strong&gt; 的回溯任務計算資源消耗，將人力投入從 5 人/天降低到 0.5 人/天。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-59334d514c3341fa8f02863a2b78ccdc7ab.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;字段頻率統計模塊&lt;/strong&gt;&lt;/strong&gt;：通過對函谷 SQL 數據庫和 TDS 平台 No SQL 任務的物理執行計劃進行解析，實現對寬表 SQL 任務和非 SQL 任務的字段訪問頻率的自動化統計。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;裁剪字段抽取模塊&lt;/strong&gt;&lt;/strong&gt;：基於字段訪問頻率，每月抽取冷溫字段，形成可視化的字段訪問頻率報表，生成裁剪 SQL。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;**冷溫字段告警模塊：**通過對比前一個月和本月冷溫字段列表，生成當月新增冷溫字段列表，然後向產品研發團隊和數據 RD 團隊發出告警，確認需要動態調整的裁剪字段；引入冷溫字段告警模塊，成功實現了裁剪字段的動態調整。最後，通過滾動裁剪模塊自動裁剪 395 天前的數據，進一步降低人力/資源的消耗。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;滾動裁剪模塊&lt;/strong&gt;&lt;/strong&gt;：自動化滾動裁剪，裁剪寬表中 395 天前的數據。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;基於業務實踐證明：&lt;strong&gt;&lt;strong&gt;寬表數倉模型&lt;/strong&gt;&lt;/strong&gt;與&lt;strong&gt;&lt;strong&gt;數倉融合計算引擎&lt;/strong&gt;&lt;/strong&gt;的結合，比傳統數倉模型，更適合，面向服務於快速迭代的驅動型業務，主要體現在&lt;/p&gt; 
&lt;p&gt;1. 查詢性能巨大提升帶來快速響應支持業務需求：&lt;/p&gt; 
&lt;p&gt;簡單查詢場景 ：Adhoc 查詢場景，耗時在數十秒級別，相比於普通 Spark 性能提升 5 倍。&lt;/p&gt; 
&lt;p&gt;複雜場景：業務常用複雜字段拆分打平，避免數組、map 等複雜字段耗時操作、查詢性能提升 2.1 倍。&lt;/p&gt; 
&lt;p&gt;2.口徑封裝下沉：封裝業務核心口徑，解決業務長期數據源多、口徑不一致帶來的數據準確性問題，省去不必要的溝通，使用更加簡便。&lt;/p&gt; 
&lt;p&gt;3.減少冗餘存儲：相較於經典傳統數倉同主題模型下存儲降低 30% 左右。&lt;/p&gt; 
&lt;span id="OSC_h1_22"&gt;&lt;/span&gt; 
&lt;h1&gt;03 基於建模與技術框架初步整合&amp;nbsp;&lt;strong&gt;&lt;strong&gt;探討圖靈 3.0 生態新一代業務服務交付模式&lt;/strong&gt;&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;隨着搜索數倉模型&amp;amp;計算引擎架構的重構和技術棧統一，搜索數倉定義逐步清晰化、數倉個數大幅度降低，整體趨向更加緊湊、高效以及收斂的態勢。在此基礎上，為了助力數據迭代效率和分析效率進一步提升，在業務線基礎數倉及應用層數據建設上，百度 MEG 內部開發了圖靈 3.0 生態系統（即，數倉合理建設，數據分析需求儘可能收斂到 TDA 平台，配套數據集建設完善），包括 Turing Data Engine(TDE) 計算引擎、Turing Data Studio(TDS) 數據開發治理平台和 Turing Data Analysis(TDA) 可視化 BI 產品。依託圖靈 3.0 生態，我們進而形成了一套新的開發範式—— 圖靈 3.0 新開發模式，用來提升搜索內業務數據分析效率與分析深度，如下圖（階段三）所示&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-d20b1a79b0fbe50ef85eb7c922eab6fc5bd.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h2_23"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;3.1 &lt;strong&gt;&lt;strong&gt;階段一到階段二&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;如之前所述：由於搜索數倉早期查詢性能不理想，為了提升業務分析效率建設了大量的業務表。從而導致數據冗餘、數據鏈路穩定性差、效率低、指標口徑不一致等一系列問題。搜索數據團隊通過數倉模型（將多層數據模型控制在 1-2 層）以及計算引擎架構升級重構、湖倉一體、高效壓縮、裁剪等一系列措施解決了這些問題。數據建設更加完善規範化，搜索數倉表的數量由過去的數百張減少至 20 張左右，時效性大幅提升，全數據鏈路全流程提速 4H+，數據穩定性及運維成本降低 30%。&lt;/p&gt; 
&lt;span id="OSC_h2_24"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;3.2 階段二到階段三&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;隨着圖靈 3.0 生態系統（包括 TDA、TDS、TDE）及搜索數倉模型的日益完善，內部提出了，以數據集為核心來構建數據應用層，將數據開發側與業務側的依賴關係從之前的"需求-交付"解耦為以數據集為中心的建設，實現數據集&amp;lt;-&amp;gt;可視化分析&amp;lt;-&amp;gt;儀表盤的數據分析閉環，解決業務常用維度、指標長週期的查詢分析需求 ——&amp;gt; 圖靈 3.0 新開發模式。&lt;/p&gt; 
&lt;p&gt;圖靈 3.0 新開發模式核心思想在於數據集的建設，我們將不再僅僅只是根據業務需求來定製開發數據報表，而是構建一個靈活、可擴展的數據集。使業務側能夠自主地根據需求從中提取、分析和可視化數據，以滿足不斷變化的業務需求。&lt;/p&gt; 
&lt;p&gt;那麼，在數據集建模實踐中，如何才能合理構建一個靈活、可擴展且高質量的數據集？是數據研發對數據集建模關鍵核心，也是最大的挑戰。&lt;/p&gt; 
&lt;span id="OSC_h3_25"&gt;&lt;/span&gt; 
&lt;h3&gt;3.2.1 數據集建模合理度挑戰&lt;/h3&gt; 
&lt;p&gt;1. 為了滿足業務需求的多樣性與廣泛性，並支持更多的維度和指標。我們往往會傾向於在單個數據集中不斷疊加新的維度和指標，這種做法雖然表面上看起來方便快捷，但實際上卻導致了數據集行數的急劇增加，進而對聚合查詢的性能造成了不利影響&lt;/p&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;為了確保查詢的高效性，同時兼顧更多維度與指標的業務需求。我們往往的做法，就是建立更多的數據集，以空間換時間去滿足查詢性能。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;顯然，這些做法之間存在着明顯的矛盾，具體如下圖。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-cd8fad4be3fd6dfa2887117d9f41ac20272.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h3_26"&gt;&lt;/span&gt; 
&lt;h3&gt;3.2.2 解決方案&lt;/h3&gt; 
&lt;p&gt;為了更好地找到平衡點，搜索數據團隊採取了以下解決措施：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;明確邊界&lt;/strong&gt;&lt;/strong&gt;：分主題建設對應數據集，單主題內，數據集儘量做到合併統一，以達到更高的集成度與一致性。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;明確粒度&lt;/strong&gt;&lt;/strong&gt;：從業務場景需求出發，單主題內數據集建設前明確數據集最小粒度 ，確保數據最小粒度既能滿足主題分析的精度要求，又避免因過度細化或粗放導致的分析效能損耗，為後續數據集的結構化構建與高效奠定基礎。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;深度性能優化&lt;/strong&gt;&lt;/strong&gt;：充分利用了 TDE-ClickHouse 強大基礎引擎，例如在處理高基數去重計數字段時，創新性地採用 NoMerge 技術來替代傳統的 COUNT(DISTINCT) 方法，降低了聚合層的計算負擔，實現了查詢性能 5 至 10 倍的提升，極大地優化了數據處理速度。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;span id="OSC_h2_27"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;3.3 新模式帶來的改變&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-11ffa0f8e33c9d7a46ccb8a729f3b0031ce.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;△ 圖靈 3.0 的數據開發新模式&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;強化主動能力，業務自助效率顯著提升&lt;/strong&gt;&lt;/strong&gt;：相較於以往被動式的一對一需求定製化開發模式，數據研發工作已從單純響應被動需求轉變為主動規劃構建數據集。圖靈 3.0 新開發模式下，實現數據集&amp;lt;-&amp;gt;可視化分析&amp;lt;-&amp;gt;儀表盤的數據分析閉環（滿足 90% 查詢；其餘 10% 長尾交給 Adhoc 查詢），業務人員對日常通用需求的分析工作轉移到數據集自助查詢與分析上（根據數據集自助創建可視化數據報表）。可視化分析佔比、&lt;strong&gt;&lt;strong&gt;業務自助率提高至 90%，數據研發日常需求量減少 80%。&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;非核心常用維度指標查詢性能顯著提升&lt;/strong&gt;&lt;/strong&gt;：非核心常用維度指標由以往業務提需，查表或單獨建設報表來獲取數據的方式，轉變為通過數據集自助下鑽、拖拉拽自由組合常用維度指標，實現可視化分析的方式。藉助 TDE-ClickHouse 強大基礎引擎能力：可視化分析效率大幅提升，&lt;strong&gt;&lt;strong&gt;從小時、分鐘級的數據分析效率，提升至秒級分析&lt;/strong&gt;&lt;/strong&gt;。單次查詢數據週期由&lt;strong&gt;&lt;strong&gt;1 周內，提升至 1 年內（&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;秒級完成查詢&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;）&lt;/strong&gt;&lt;/strong&gt;，真正做到即需即查即用。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;血緣管理規範化，運維效率顯著提升&lt;/strong&gt;&lt;/strong&gt;：數據血緣更加完整流程化，數倉-數據集，血緣在 TDS 完成閉環，數據集內字段血緣在 TDA 完成閉環，以數據集為紐帶串聯整個數據流全過程，&lt;strong&gt;&lt;strong&gt;數據鏈路運維效率提升 2-3 倍&lt;/strong&gt;&lt;/strong&gt;。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;目前，該模式已經廣泛應用於搜索各業務數據運營人員早報、週報等多種業務彙報場景。得益於該模式，搜索產品線下&lt;strong&gt;&lt;strong&gt;儀表盤周均查詢（PV）高達 1.7W 次&lt;/strong&gt;&lt;/strong&gt;左右，&lt;strong&gt;&lt;strong&gt;可視化分析周均 0.93W 次左右 ，每週&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;超過 400 多名用戶參與 TDA 搜索數據分析工作&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;。&lt;strong&gt;&lt;strong&gt;更重要的是，需求的交付週期實現了顯著縮短，由&lt;/strong&gt;&lt;/strong&gt;以往的單/雙週縮短至按天交付&lt;/strong&gt;&lt;/strong&gt;；甚至在某些情況下，業務人員能夠直接自助獲取所需數據。在處理重點項目時，該模式也能確保業務團隊在第一時間獲取到 P0 級別的關鍵數據。這種方式的轉變不僅能夠減輕數據開發團隊的工作負擔——人力成本由原先的&lt;strong&gt;&lt;strong&gt;3 人鋭減至 1 人&lt;/strong&gt;&lt;/strong&gt;，還能提高業務側的數據使用效率和自主性，使得團隊得以從繁瑣的「取數」與「跑數」任務中解放出來，將更多的精力投入到數倉模型的優化、技術框架的探索與治理等更具戰略價值的工作中去。&lt;/p&gt; 
&lt;span id="OSC_h1_28"&gt;&lt;/span&gt; 
&lt;h1&gt;04 總結與展望&lt;/h1&gt; 
&lt;p&gt;數據建模領域正經歷從「技術驅動」向「價值驅動」的深刻轉型，更加強調的是敏捷性、可解釋性和業務對齊。儘管當前的技術工具愈發強大，但成功的關鍵依舊在於跟業務的緊密協作與一個清晰明確的治理框架。&lt;/p&gt; 
&lt;p&gt;搜索業務，作為百度內部最核心且最為複雜的板塊，涵蓋了多個至關重要的產品線。近年來，搜索數據團隊始終致力於運用前沿技術來不斷優化和完善數倉體系的建設，以堅實的基礎數倉架構支撐起數據質量飛躍提升，從而高效賦能業務，帶來可量化、可感知的業務成效與用戶體驗升級。&lt;/p&gt; 
&lt;p&gt;展望未來，隨着 AI 代理和邊緣計算等技術的蓬勃發展，數據建模有望朝着自適應與嵌入式方向進一步進化。搜索數據側還將在以下關鍵方向與大家進行深入探討、交流和學習：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;通用數據流解決方案&lt;/strong&gt;&lt;/strong&gt;：構建事件規則引擎等通用數據流處理工具，簡化數據處理流程，提高數據處理效率與靈活性。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;日誌埋點技術（含無埋點）&lt;/strong&gt;&lt;/strong&gt;：探索高效的自動化埋點機制，提升數據採集的全面性與準確性，為業務洞察提供堅實的數據基礎。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;寬表模型框架抽象層&lt;/strong&gt;&lt;/strong&gt;：探索更為高效、靈活的模型統一抽象方法層。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;AI 大模型時代下的高效開發模式&lt;/strong&gt;&lt;/strong&gt;：探索如何通過利用大模型技術，來優化代碼質量、數據鏈路等，打造更加高效、可靠的數據開發與運維體系。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;我們期待之後再次與大家見面探討這些議題，共同推動數據領域的創新與發展。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/4939618/blog/18683272</link>
      <guid isPermaLink="false">https://my.oschina.net/u/4939618/blog/18683272</guid>
      <pubDate>Sun, 11 May 2025 03:38:00 GMT</pubDate>
      <author>原創</author>
    </item>
    <item>
      <title>AI 編程軟件 Cursor 開發商聘請兩位 Anthropic 前高管</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;AI 編程軟件 Cursor 開發商 Anysphere 聘請了兩位來自 Anthropic 的前高管，分別擔任首席架構師兼工程主管和產品負責人。&lt;/p&gt; 
&lt;p&gt;Boris Cherny，曾是 Claude Code 的開發負責人，將擔任 Anysphere 的首席架構師兼工程主管。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0703/113518_qNjw_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Boris Cherny 於 2024 年 9 月加入 Anthropic，入職還不到一年，此前在 Meta 任職首席軟件工程師、 Instagram 的服務器架構和開發基礎設施主管， 以及 Meta 的代碼質量主管，畢業於美國加州大學聖迭戈分校。&lt;/p&gt; 
&lt;p&gt;Cat Wu，曾是 Claude Code 的產品經理，將擔任產品負責人。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0703/113527_RWRC_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Cat Wu 全名 Catherine Wu，2024 年 8 月加入 Anthropic，擅長構建高可靠、可解釋、可控制的人工智能系統，本科畢業於普林斯頓大學，專業計算機科學，加入 Anthropic 之前有多段不同領域工作實習經歷，最長兩年，比如在谷歌實習任職軟件工程師，在 J.P. 摩根實習任職交易員，在 Alexandr Wang 公司 Scale AI 作為作為產品經理任職兩年。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358526</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358526</guid>
      <pubDate>Sun, 11 May 2025 03:37:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>富士康推出首款 AI 推理大模型 「FoxBrain」，商標申請已提交</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;近日，鴻海精密工業股份有限公司（也就是大家熟悉的富士康）在國家知識產權局商標局提交了 「FoxBrain」 商標註冊申請。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;這款 AI 推理大模型不僅是富士康的首次嘗試，更是台灣省首個該類型的 AI 模型。根據公開資料顯示，該商標的國際分類為科學儀器，目前正處於 「等待實質審查」 的狀態。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;img height="363" src="https://oscimg.oschina.net/oscnet/up-853c20df5c0da43f4e610f96af33d644262.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;「FoxBrain」 是鴻海研究院重磅推出的 AI 推理大模型，涵蓋數據分析、數學推理、代碼生成等多個功能。富士康聲稱，FoxBrain 的初始版本基於 Meta 的 Llama3.1 模型進行開發，使用了 120 塊英偉達 H100GPU 進行了為期一個月的訓練。這一模型特別針對繁體中文進行了優化，儘管其性能相較於其他模型，如 DeepSeek，可能稍顯不足。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;值得一提的是，富士康並非台灣省唯一在 AI 領域發力的公司。早前，聯發科也推出了 Llama-Breeze2 系列 AI 模型，這些模型雖然定位為 「輕量級」，但同樣主打繁體中文處理能力。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358520</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358520</guid>
      <pubDate>Sun, 11 May 2025 03:16:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Grok 4 將提供面向編程的「Code」版本</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2FAiBattle_%2Fstatus%2F1940139539525419512" target="_blank"&gt;博主「AiBattle」爆料稱&lt;/a&gt;，其在 xAI 控制枱的源代碼中發現了 2 個 Grok 4 模型的相關信息。&lt;/p&gt; 
&lt;p&gt;據悉，&lt;strong&gt;本次 Grok 4 將擁有標準版和麪向編程的 Code 版&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Grok 4：xAI 最出色、最新的旗艦模型，在自然語言、數學和推理方面表現優秀。&lt;/li&gt; 
 &lt;li&gt;Grok 4 Code：專為編程而生，能夠諮詢代碼相關問題，或者將 Grok 4 Code 嵌入代碼編輯器中。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-9bcf46eea400a93a91e4827ee44b5545da0.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-3a6e92d6992d4ffe25867b6b3a69cdcbc4f.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;另據爆料，Grok 4 的權限已經部分開通，可通過 API 訪問。目前，Grok 4 支持文本模態以及視覺、圖像生成等功能，其他功能也即將推出。&lt;/p&gt; 
&lt;p&gt;馬斯克近日也透露，Grok 4 計劃在今年 7 月 4 日之後發佈；並且新模型將嘗試從第一性原理出發進行推理，也就是將物理學的方法應用到思維過程中。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0630/185329_AUaz_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;相關閲讀：&lt;a href="https://www.oschina.net/news/358033" target="news"&gt;Grok 4 將於 7 月 4 日後發佈&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358516</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358516</guid>
      <pubDate>Sun, 11 May 2025 03:05:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>字節跳動開源 4D 視頻生成框架 EX-4D</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;&lt;span style="background-color:#ffffff"&gt;字節跳動旗下 PICO-MR 團隊正式開源了 EX-4D，一款突破性的 4D 視頻生成框架；能夠從單一視角（單目）視頻生成高質量、多視角的 4D 視頻序列 (3D 空間+時間維度)。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;傳統視頻生成技術在多視角生成方面面臨兩大挑戰:一是需要昂貴的多視角相機和數據集進行訓練;二是難以處理遮擋區域，導致生成的視頻在極端視角下出現物體穿幫或細節失真。EX-4D 通過創新的深度密閉網格（DW-Mesh）表示和輕量級適配架構，成功解決了這些問題。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;DW-Mesh 是 EX-4D 的核心技術，它通過構建全密閉網格結構，記錄場景中的可見和隱形面片，無需多視角監督即可統一處理複雜場景拓撲。結合預訓練深度預測模型，EX-4D 將單幀像素投影到 3D 空間，形成網格頂點，並根據幾何關係精準標記遮擋區域。這種方法確保了生成視頻在極端視角（如±90°）下仍能保持物理一致性和細節完整性。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;此外，EX-4D 引入了兩種模擬 mask 生成策略——渲染 mask 和跟蹤 mask，通過模擬視角移動和幀間一致性，破解了多視角訓練數據的稀缺難題。這些策略使 EX-4D 僅憑單目視頻即可「腦補」全視角數據，極大降低了數據採集成本。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;性能測試結果表明，EX-4D 在 FID（弗雷歇距離）、FVD(弗雷歇視頻距離) 和 VBench 等行業標準指標上全面超越現有開源方法。尤其在極端視角 (如接近 90°) 的生成任務中，EX-4D 的性能優勢尤為明顯，生成的視頻在物體細節和遮擋邏輯上表現更為真實。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;img height="251" src="https://oscimg.oschina.net/oscnet/up-dbd1b11be587afb0f59e10fdb2c7026588a.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0px; margin-right:0px"&gt;&lt;span style="color:#000000"&gt;在一項由 50 位志願者參與的主觀評估中，70.7% 的參與者認為 EX-4D 在極端視角下的物理一致性遠超其他開源方法。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;EX-4D 基於預訓練的 WAN-2.1 模型，結合 LoRA-based Adapter 架構，在保持計算效率的同時，融入了 DW-Mesh 的幾何先驗信息，確保生成視頻的幾何一致性和幀間連貫性。這種輕量級設計使得 EX-4D 在資源受限的環境下也能高效運行，適合廣泛的開發場景。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;span style="background-color:#ffffff"&gt;字節跳動 PICO-MR 團隊負責人表示，EX-4D 是團隊在 3D 重建與 4D 場景生成領域多年研究的結晶，未來將繼續優化模型性能，探索更廣泛的應用場景。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358512</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358512</guid>
      <pubDate>Sun, 11 May 2025 02:52:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
  </channel>
</rss>
