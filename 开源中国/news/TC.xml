<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - news - 繁體中文（台灣）</title>
    <link>https://www.oschina.net/news/project</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news" rel="self" type="application/rss+xml"/>
    <description>已對該 RSS 進行格式化操作：中英字符之間插入空格、使用直角引號、標點符號修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-tw</language>
    <lastBuildDate>Tue, 05 Aug 2025 02:43:08 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>疑似回應「全員裁員」傳言，硅基智能稱預計全年新增崗位數百個</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;近日，一家成立 8 年的 AI 獨角獸南京硅基智能科技有限公司（下稱「硅基智能」）捲入「全員裁員」傳言。脈脈平台顯示，當前「硅基智能 CEO:準備全員裁員，養不起你們」佔據熱榜第四。&lt;/p&gt; 
&lt;p&gt;8 月 3 日，硅基智能在微信公眾號發出&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fmq8xKJFLo6gCwvG_Vn_rEA" target="_blank"&gt;聲明&lt;/a&gt;，稱「目前擁有一支穩定的產研與銷售團隊，且持續在全球範圍內擴大招聘規模。2025 年，我們將重點佈局杭州、嘉興、香港、新加坡等地，預計全年新增崗位數百個，2026 年將達到新增數千人的擴張節奏。」&lt;/p&gt; 
&lt;p&gt;&lt;img height="293" src="https://oscimg.oschina.net/oscnet/up-e9194ccac4d60e4e961661160dfc6b4e061.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;幾天前，一則疑似硅基智能創始人司馬華鵬在公司工作羣發言的截圖在業內流傳，引發關注。&lt;/p&gt; 
&lt;p&gt;截圖內文文字顯示，司馬華鵬@所有人並表示：「各位，昨天我去看研發，只有徐超一個人在加班，公司今天已經做好了全員裁員的計劃，算法給港科大和清華做，工程化留幾個骨幹，其他的都自尋出路，硅基養不起這樣的團隊，請大家見諒。」&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;針對傳言及相關聲明，記者於 8 月 4 日向硅基智能求證。硅基智能相關人士表示，「我們暫時不對外發聲。團隊專心做好產品和業務，團隊穩定且在擴展。」&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;根據前述聲明，硅基智能稱，「2025 年上半年，公司超額完成銷售目標，盈利能力逐步增強；下半年剛剛開始一個月，我們已鎖定超 3 億元的 AIGC 訂單，並將加快推動「新質生產力賦能中心」在浙江等地落地。同時，新加坡、香港的出海團隊也已初具規模。」&lt;/p&gt; 
&lt;p&gt;據瞭解，硅基智能已於 2025 年 6 月完成來自浙江嘉興的新一輪數億元融資，同時也拿到各大銀行提供的數億元授信額度。目前其賬面現金可支持 120 個月以上的工資發放，同時擁有近億元規模的算力硬件資產。&lt;/p&gt; 
&lt;p&gt;硅基智能成立於 2017 年，提供企業級 AIGC 數字人解決方案。其總部位於南京，屬於專精特新小巨人企業、高新技術企業，當前已擁有發明授權專利一百多項，其中包括二十多項海外專利。股東包括騰訊、招銀國際、國新央企、海松資本、紅杉資本、浦信資本、奇虎 360 等，最新估值近 10 億美元。&lt;/p&gt; 
&lt;p&gt;硅基智能聯合創始人、高級副總裁孫凱彼時在 WAIC 某沙龍上談及公司戰略時透露，硅基智能正在從傳統的工具收費向」按結果付費」轉型。「我們的海外合作伙伴依靠硅基智能提供 AI 數字人技術接口，今年收入就已超過 1 億美元。真正優秀的 AI 員工，不僅要賺月薪，更應成為客戶的合夥人。」&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364227</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364227</guid>
      <pubDate>Tue, 05 Aug 2025 02:28:50 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>深度解析 RocketMQ 核心組件：ConsumeQueue 的設計與實現</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                                        &lt;h2&gt;導語&lt;/h2&gt; 
&lt;p&gt;在分佈式消息隊列 RocketMQ 中，ConsumeQueue（消費隊列） 是消息消費的核心組件之一。它作為 &amp;nbsp;CommitLog 的索引機制，幫助消費者快速定位並拉取消息。如果沒有 ConsumeQueue，消費者將無法高效地從海量消息中篩選出自己訂閲的數據。&lt;/p&gt; 
&lt;p&gt;本文將基於 RocketMQ 5.0 源碼，深入探討 ConsumeQueue 的設計原理與實現細節。&lt;/p&gt; 
&lt;h2&gt;為什麼需要 ConsumeQueue？&lt;/h2&gt; 
&lt;p&gt;在深入探討 ConsumeQueue 之前，我們有必要先了解 RocketMQ 的消息寫入和存儲方式。&lt;/p&gt; 
&lt;p&gt;CommitLog 是 RocketMQ 的消息存儲模塊，用戶生產的所有消息都持久化存儲在該模塊中，它具備兩個特點：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;使用的持久化存儲是一個文件隊列，文件保存於指定目錄下，每個文件的大小是固定的，通常是 1GB。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;只有一個文件可寫入，且僅支持追加寫，文件寫滿後自動切換至新的文件。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;RocketMQ 設計者出於寫入優先的考慮，沒有為不同 Topic 隊列的消息分配不同的存儲文件，而是將消息直接寫入 CommitLog，不同 Topic 的消息混合分佈在 CommitLog 的文件中。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/f7014d6f-45a9-4e8c-aca4-4eca355e5e5b.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;從上圖中可以看出，儘管消息的寫入非常高效，但是消費者需要按照其訂閲的 Topic 來從 CommitLog 中讀取該 Topic 的消息，顯而易見，RocketMQ 需要一種索引機制來快速讀取指定 Topic 隊列的消息，這正是 ConsumeQueue 要做的事情。&lt;/p&gt; 
&lt;h2&gt;ConsumeQueue 的設計原理&lt;/h2&gt; 
&lt;p&gt;ConsumeQueue 作為 RocketMQ 的消息索引樞紐，其設計核心在於高效映射邏輯隊列與物理存儲。我們通過下面的圖示來介紹 ConsumeQueue 的核心設計：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/36fce070-c2fd-4a3d-9449-0650f6c9946f.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;每個 Topic 隊列有其對應的唯一的 ConsumeQueue，當一條消息寫入到 CommitLog 後，RocketMQ 會構建該消息的索引，按異步方式將其寫入到對應 Topic 隊列的 ConsumeQueue 中。使用索引可以快速定位到消息在 CommitLog 文件的位置並讀取它。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;消息索引對象在 ConsumeQueue 中的位置被稱為 Offset，是個從 0 開始的序號數，maxOffset 即 ConsumeQueue 索引的最大 Offset，會隨着新消息的寫入遞增。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;基於這個設計，消費者通過與 ConsumeQueue 的 Offset 交互來實現消息的消費。最常見的場景就是，我們記錄消費組在 ConsumeQueue 上當前消費的 Offset，那麼消費者下線後再上線仍然可從上次消費的位置繼續消費。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;基於文件的傳統實現方案&lt;/h2&gt; 
&lt;h3&gt;數據存儲與格式&lt;/h3&gt; 
&lt;p&gt;與 CommitLog 類似，ConsumeQueue 使用文件隊列來持久化存儲消息索引。ConsumeQueue 使用的文件目錄所在路徑由其對應的 Topic 隊列確定，舉例説明，一個名為 ABC 的 Topic，其隊列 0 所在的文件目錄路徑是 /data/rocketmq_data/store/consumequeue/abc/0/。消息的索引對象是固定的 20 個字節大小，其內部格式定義見下圖。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/9c1e2702-a9f4-4d84-9bd8-3c5f1b5d7b18.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;為了方便描述，從這裏開始我們將索引對象叫作 CqUnit。ConsumeQueue 的文件隊列中每個文件的大小是固定的，默認配置可存儲 30 萬個 CqUnit，當文件寫滿後，會切換到新文件進行寫入。文件名稱的命名方式是有講究的，它以文件存儲的第一個 CqUnit 的 Offset 作為名稱，這樣做的好處是，按 Offset 查詢 CqUnit 時，可以根據文件名稱，快速定位到該 Offset 所在的文件，大幅減少對文件的讀取操作頻次。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/dc6e56f6-83c4-4e99-92f5-4c2702207472.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;構建過程&lt;/h3&gt; 
&lt;p&gt;當消息寫入到 CommitLog 後，該消息對於消費者是不可見的，只有在 ConsumeQueue 中增加這條消息的 CqUnit 後，消費者才能消費到這條消息，因此寫入消息時須立刻往 ConsuemQueue 寫入消息的 CqUnit。我們需要給每一條消息指定其在 ConsumeQueue 中的 Offset，QueueOffsetOperator 類維護了一個 Topic 隊列與其當前 &amp;nbsp;Offset 的表，當寫入一條新消息時，DefaultMessageStore 從 QueueOffsetOperator 中取出該 Topic 隊列的當前 Offset，將其寫入到消息體中，在消息成功寫入到 CommitLog 後，指示 QueueOffsetOperator 更新為當前 Offset + 1。為了防止其他寫入線程併發訪問 Topic 隊列的當前 Offset，在讀取和修改 Offset 期間，會使用一個 ReentrantLock 鎖定該 Topic 隊列。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/bd666fd0-892a-4923-8f29-f459737f4981.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;ReputMessageService 作為異步任務，會不停的讀取 CommitLog，當有新的消息寫入，它會立即讀取到該消息，然後根據消息體構建一個 DispatchRequest 對象，CommitLogDispatcherBuildConsumeQueue 處理 DispatchRequest 對象，最終將 CqUnit 寫入到 ConsumeQueue 的存儲中。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/89cd3d56-7582-4cbe-ae34-1478e76b2a7c.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;按 Offset 查找消息&lt;/h3&gt; 
&lt;p&gt;消費者通常是從某個 Offset 開始消費消息的，比如消費者下線後再次上線會從上次消費的 Offset 開始消費。DefaultMessageStore 的 GetMessage 方法實現從一個 Topic 隊列中拉取一批消息的功能，每次拉取要指定讀取的起始 Offset 以及該批次讀取的最大消息數量。下面截取了部分源碼展示實現的基本思路：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp;@Override
    public GetMessageResult getMessage(final String group, final String topic, final int queueId, final long offset,
        final int maxMsgNums, final int maxTotalMsgSize, final MessageFilter messageFilter) {
        long beginTime = this.getSystemClock().now();
        GetMessageStatus status = GetMessageStatus.NO_MESSAGE_IN_QUEUE;
        long nextBeginOffset = offset;
        long minOffset = 0;
        long maxOffset = 0;
        GetMessageResult getResult = new GetMessageResult();
        final long maxOffsetPy = this.commitLog.getMaxOffset();
        ConsumeQueueInterface consumeQueue = findConsumeQueue(topic, queueId);
        if (consumeQueue != null) {
            minOffset = consumeQueue.getMinOffsetInQueue();
            maxOffset = consumeQueue.getMaxOffsetInQueue();
            if (maxOffset == 0) {
            //             
            } else {
                long maxPullSize = Math.max(maxTotalMsgSize, 100);
                if (maxPullSize &amp;gt; MAX_PULL_MSG_SIZE) {
                    LOGGER.warn("The max pull size is too large maxPullSize={} topic={} queueId={}", maxPullSize, topic, queueId);
                    maxPullSize = MAX_PULL_MSG_SIZE;
                }
                status = GetMessageStatus.NO_MATCHED_MESSAGE;
                long maxPhyOffsetPulling = 0;
                int cqFileNum = 0;
                while (getResult.getBufferTotalSize() &amp;lt;= 0
                    &amp;amp;&amp;amp; nextBeginOffset &amp;lt; maxOffset
                    &amp;amp;&amp;amp; cqFileNum++ &amp;lt; this.messageStoreConfig.getTravelCqFileNumWhenGetMessage()) {
                    ReferredIterator&amp;lt;CqUnit&amp;gt; bufferConsumeQueue = null;
                    try {
                        bufferConsumeQueue = consumeQueue.iterateFrom(group, nextBeginOffset, maxMsgNums);
                        long nextPhyFileStartOffset = Long.MIN_VALUE;
                        long expiredTtlOffset = -1;
                        while (bufferConsumeQueue.hasNext() &amp;amp;&amp;amp; nextBeginOffset &amp;lt; maxOffset) {
                            CqUnit cqUnit = bufferConsumeQueue.next();
                            long offsetPy = cqUnit.getPos();
                            int sizePy = cqUnit.getSize();
                            SelectMappedBufferResult selectResult = this.commitLog.getMessage(offsetPy, sizePy);
                            getResult.addMessage(selectResult, cqUnit.getQueueOffset(), cqUnit.getBatchNum());
                            status = GetMessageStatus.FOUND;
                            nextPhyFileStartOffset = Long.MIN_VALUE;
                        }
                    } catch (RocksDBException e) {
                        ERROR_LOG.error("getMessage Failed. cid: {}, topic: {}, queueId: {}, offset: {}, minOffset: {}, maxOffset: {}, {}",
                            group, topic, queueId, offset, minOffset, maxOffset, e.getMessage());
                    } finally {
                        if (bufferConsumeQueue != null) {
                            bufferConsumeQueue.release();
                        }
                    }
                }
                long diff = maxOffsetPy - maxPhyOffsetPulling;
                long memory = (long) (StoreUtil.TOTAL_PHYSICAL_MEMORY_SIZE
                    * (this.messageStoreConfig.getAccessMessageInMemoryMaxRatio() / 100.0));
                getResult.setSuggestPullingFromSlave(diff &amp;gt; memory);
            }
        } else {
            status = GetMessageStatus.NO_MATCHED_LOGIC_QUEUE;
            nextBeginOffset = nextOffsetCorrection(offset, 0);
        }
        getResult.setStatus(status);
        getResult.setNextBeginOffset(nextBeginOffset);
        getResult.setMaxOffset(maxOffset);
        getResult.setMinOffset(minOffset);
        return getResult;
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;上述代碼片段的要點：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Topic 隊列的 ConsumeQueue 的 IterateFrom 方法依據 Offset 生成一個 Iterator 對象。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;在 Iterator 有效的情況，不斷從 Iterator 拉取 CqUnit 對象，即按 Offset 順序讀取 CqUnit。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;使用 CqUnit 對象中的 OffsetPy 和 SizePy 從 CommitLog 中讀取消息內容，返回給消費者。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;接下來，我們介紹 ConsumeQueue 的 IterateFrom 方法是如何讀取 CqUnit 的。從下面的源碼中可以看到，GetIndexBuffer 方法先從 MappedFileQueue 中找到 Offset 所在的 MappedFile，然後找到 Offset 在 MappedFile 中的位置，從該位置讀取文件剩餘的內容。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;public SelectMappedBufferResult getIndexBuffer(final long startIndex) {
        int mappedFileSize = this.mappedFileSize;
        long offset = startIndex * CQ_STORE_UNIT_SIZE;
        if (offset &amp;gt;= this.getMinLogicOffset()) {
            MappedFile mappedFile = this.mappedFileQueue.findMappedFileByOffset(offset);
            if (mappedFile != null) {
                return mappedFile.selectMappedBuffer((int) (offset % mappedFileSize));
            }
        }
        return null;
    }
    @Override
    public ReferredIterator&amp;lt;CqUnit&amp;gt; iterateFrom(long startOffset) {
        SelectMappedBufferResult sbr = getIndexBuffer(startOffset);
        if (sbr == null) {
            return null;
        }
        return new ConsumeQueueIterator(sbr);
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ConsumeQueueIterator 的 Next 方法和 hasNext 方法是對 getIndexBuffer 方法返回的 SelectMappedBufferResult 對象，即文件內容的 ByteBuffer，進行訪問。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp;private class ConsumeQueueIterator implements ReferredIterator&amp;lt;CqUnit&amp;gt; {
        private SelectMappedBufferResult sbr;
        private int relativePos = 0;
        public ConsumeQueueIterator(SelectMappedBufferResult sbr) {
            this.sbr = sbr;
            if (sbr != null &amp;amp;&amp;amp; sbr.getByteBuffer() != null) {
                relativePos = sbr.getByteBuffer().position();
            }
        }
        @Override
        public boolean hasNext() {
            if (sbr == null || sbr.getByteBuffer() == null) {
                return false;
            }
            return sbr.getByteBuffer().hasRemaining();
        }
        @Override
        public CqUnit next() {
            if (!hasNext()) {
                return null;
            }
            long queueOffset = (sbr.getStartOffset() + sbr.getByteBuffer().position() - relativePos) / CQ_STORE_UNIT_SIZE;
            CqUnit cqUnit = new CqUnit(queueOffset,
                sbr.getByteBuffer().getLong(),
                sbr.getByteBuffer().getInt(),
                sbr.getByteBuffer().getLong());
            return cqUnit;
        }
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;我們再講下 MappedFileQueue 的 FindMappedFileByOffset 方法，該方法從其維護的文件隊列中查找到 Offset 所在的文件。前面我們介紹過，ConsumeQueue 的文件隊列中的文件是按 Offset 命名的，MappedFile 的 GetFileFromOffset 就是文件的名稱，那麼只需要按照 Offset 除以文件的大小便可得文件在隊列中的位置。這裏要注意的是，這個位置必須要先減去 FirstMappedFile 的位置後才是有效的，因為 ConsumeQueue 會定期清除過期的文件，所以 ConsumeQueue 管理的 MappedFileQueue 的第一個文件對應的 Offset 未必是 0。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp;public MappedFile findMappedFileByOffset(final long offset, final boolean returnFirstOnNotFound) {
        try {
            MappedFile firstMappedFile = this.getFirstMappedFile();
            MappedFile lastMappedFile = this.getLastMappedFile();
            if (firstMappedFile != null &amp;amp;&amp;amp; lastMappedFile != null) {
                if (offset &amp;lt; firstMappedFile.getFileFromOffset() || offset &amp;gt;= lastMappedFile.getFileFromOffset() + this.mappedFileSize) {
                    LOG_ERROR.warn("Offset not matched. Request offset: {}, firstOffset: {}, lastOffset: {}, mappedFileSize: {}, mappedFiles count: {}",
                        offset,
                        firstMappedFile.getFileFromOffset(),
                        lastMappedFile.getFileFromOffset() + this.mappedFileSize,
                        this.mappedFileSize,
                        this.mappedFiles.size());
                } else {
                    int index = (int) ((offset / this.mappedFileSize) - (firstMappedFile.getFileFromOffset() / this.mappedFileSize));
                    MappedFile targetFile = null;
                    try {
                        targetFile = this.mappedFiles.get(index);
                    } catch (Exception ignored) {
                    }
                    if (targetFile != null &amp;amp;&amp;amp; offset &amp;gt;= targetFile.getFileFromOffset()
                        &amp;amp;&amp;amp; offset &amp;lt; targetFile.getFileFromOffset() + this.mappedFileSize) {
                        return targetFile;
                    }
                    for (MappedFile tmpMappedFile : this.mappedFiles) {
                        if (offset &amp;gt;= tmpMappedFile.getFileFromOffset()
                            &amp;amp;&amp;amp; offset &amp;lt; tmpMappedFile.getFileFromOffset() + this.mappedFileSize) {
                            return tmpMappedFile;
                        }
                    }
                }
                if (returnFirstOnNotFound) {
                    return firstMappedFile;
                }
            }
        } catch (Exception e) {
            log.error("findMappedFileByOffset Exception", e);
        }
        return null;
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;按時間戳查找消息&lt;/h3&gt; 
&lt;p&gt;除了從指定 Offset 消費消息這種方式，消費者還有回溯到某個時間點開始消費的需求，這要求 RocketMQ 支持查詢指定的 Timestamp 所在的 Offset，然後從這個 Offset 開始消費消息。&lt;/p&gt; 
&lt;p&gt;我們可以從 ConsumeQueue 的 GetOffsetInQueueByTime 方法直接瞭解按時間戳查找消息的具體實現。&lt;/p&gt; 
&lt;p&gt;消息是按時間先後寫入的，ConsumeQueue 文件隊列中的 CqUnit 也是按時間先後排列的，那麼每個 MappedFile 都對應一段時間區間內的 CqUnit。從下面代碼可以看出，我們可以先根據 Timestamp 找到其落在時間區間的 MappedFile，然後在該 MappedFile 裏查找最接近該 Timestamp 的 CqUnit。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@Override
    public long getOffsetInQueueByTime(final long timestamp, final BoundaryType boundaryType) {
        MappedFile mappedFile = this.mappedFileQueue.getConsumeQueueMappedFileByTime(timestamp,
            messageStore.getCommitLog(), boundaryType);
        return binarySearchInQueueByTime(mappedFile, timestamp, boundaryType);
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;GetConsumeQueueMappedFileByTime 的具體實現主要分為兩個部分：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;找到每個 MappedFile 的 StartTimestamp 和 StopTimestamp，即 MappedFile 裏第一個 CqUnit 對應消息的時間戳和最後一個 CqUnit 對應消息的時間戳，需要訪問兩次 CommitLog 來得到消息內容。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;使用 Timestamp 和每個 MappedFile 的 StartTimestamp 和 StopTimestamp 比較。當 Timestamp 落在某個 MappedFile 的 StartTimestamp 和 StopTimestamp 區間內時，那麼該 MappedFile 是下一步查找 CqUnit 的目標。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;接下來，要按照二分查找法在該 MappedFile 中找到最接近 Timestamp 的 CqUnit。根據二分查找的法則，每次查找需要比較中間位置的 CqUnit 引用消息的存儲時間和目標 Timestamp 以確定下一個查找區間，直至 CqUnit 滿足最接近目標 Timestamp 的條件。要注意的是，獲取 CqUnit 引用消息的存儲時間需從 CommitLog 中讀取消息。&lt;/p&gt; 
&lt;h2&gt;基於 RocksDB 的優化方案&lt;/h2&gt; 
&lt;p&gt;儘管基於文件的實現比較直觀，但是當 Topic 隊列達到一定數量後，會出現明顯的性能和可用性問題。Topic 隊列數量越多，代表着 ConsumeQueue 文件越多，產生的隨機讀寫也就越多，這會影響系統整體的 IO 性能，導致出現生產消費 TPS 不斷下降，延遲不斷增高的趨勢。在我們內部的測試環境和客戶的生產環境中，我們都發現使用的隊列數過多直接影響系統的可用性，而且我們無法通過不斷升級 Broker 節點配置來消除這種影響，因此我們騰訊雲 TDMQ RocketMQ 版在產品控制枱上會限制客戶可創建的 Topic 數量以確保消息服務的穩定性。&lt;/p&gt; 
&lt;p&gt;那麼有沒有辦法能夠解決上面的問題讓服務能夠承載更多的 Topic 呢？我們可以把 ConsumeQueue 提供的功能理解為使用 Topic 隊列的 Offset 來找到 CqUnit，那麼 Topic 隊列和 Offset 構成了 Key，CqUnit 是 Value，是一個典型的 KV 使用場景。在單機 KV 存儲的軟件裏，最著名的莫過於 RocksDB 了，它被廣泛使用於 Facebook，LinkedIn 等互聯網公司的業務中。從下面的設計圖看，RocksDB 基於 SSTable + MemTable 的實現能夠提供高效寫入和查找 KV 的能力，有興趣的讀者可以研究下 RocksDB 的具體實現 (&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Ffacebook%2Frocksdb%2Fwiki%2FRocksDB-Overview" target="_blank"&gt;https://github.com/facebook/rocksdb/wiki/RocksDB-Overview&lt;/a&gt;)，這裏不展開説明。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/097e131f-0b3f-41fa-9973-57cab02459f0.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;如果我們使用 RocksDB 讀寫 CqUnit，那麼 ConsumeQueue 文件數量不會隨着 Topic 隊列的數量線性增長，便不必擔心由此帶來的 IO 開銷。&lt;/p&gt; 
&lt;p&gt;下面我們來介紹如何使用 RocksDB 來實現 ConsumeQueue。&lt;/p&gt; 
&lt;h3&gt;數據存儲與格式&lt;/h3&gt; 
&lt;p&gt;在基於 RocksDB 的實現裏，RocketMQ 使用兩個 ColumnFamily 來管理不同類型的數據，這裏不熟悉 RocksDB 的讀者可以將 ColumnFamily 視作 MySQL 裏的 Table。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;第一個 ColumnFamiliy，簡稱為 DefaultColumnFamily，用於管理 CqUnit 數據。&lt;/p&gt; &lt;p&gt;Key 的內容格式定義參考下圖，其包含 Topic 名稱、QueueId 和 ConsumeQueue 的 Offset。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/831d29f2-7280-4583-8cf7-794df3d6d057.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Value 的內容格式，與前文中文件實現裏的索引對象定義類似，但是多了一個消息存儲時間的字段。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/cbba8329-dbd2-4dd0-9557-a8e455bcb831.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;第二個 ColumnFamily，簡稱為 OffsetColumnFamily，用於管理 Topic 隊列的 MaxOffset 和 MinOffset。&lt;/p&gt; &lt;p&gt;MaxOffset 是指 Topic 隊列最新一條消息在 ConsumeQueue 中的 Offset，隨着消息的新增而變化。MinOffset 是指 Topic 隊列最早一條消息在 ConsumeQueue 中的 Offset，當消息過期被刪除後發生變化。MaxOffset 和 MinOffset 確定消費者可讀取消息的範圍，在基於文件的實現裏，通過訪問 ConsumeQueue 文件隊列裏的隊尾和隊首文件得到這兩個數值。而在 RocksDB 的實現裏，我們單獨保存這兩個數值。&lt;/p&gt; &lt;p&gt;下圖是 Key 的格式定義，其包含 Topic 名稱、QueueId 以及用於標記是 MaxOffset 或 MinOffset 的字段。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/84dd4df8-a509-4f23-8467-cd67f7b5be20.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Value 保存 ConsumeQueue 的 Offset，以及該 Offset 對應消息在 CommitLog 的位置。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/0e4c53a0-12b1-44fb-a457-3694f870b0c9.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;構建過程&lt;/h3&gt; 
&lt;p&gt;ConsumeQueue 的 CqUnit 的構建過程與前文中基於文件的實現的過程一致，此處不再贅述，不同的是前文中 ReputMessageService 使用的 ConsumeQueueStore 被替換為 RocksDBConsumeQueueStore。在這個過程中，RocksDBConsumeQueueStore 主要完成兩件事：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;往 DefaultColumnFamily 寫入消息對應的 CqUnit。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;往 OffsetColumnFamily 更新消息對應 Topic 隊列的 maxOffset。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp;private boolean putMessagePosition0(List&amp;lt;DispatchRequest&amp;gt; requests) {
        if (!this.rocksDBStorage.hold()) {
            return false;
        }
        try (WriteBatch writeBatch = new WriteBatch(); WriteBatch lmqTopicMappingWriteBatch = new WriteBatch()) {
            final int size = requests.size();
            if (size == 0) {
                return true;
            }
            long maxPhyOffset = 0;
            for (int i = size - 1; i &amp;gt;= 0; i--) {
                final DispatchRequest request = requests.get(i);
                DispatchEntry entry = DispatchEntry.from(request);
                dispatch(entry, writeBatch, lmqTopicMappingWriteBatch);
                dispatchLMQ(request, writeBatch, lmqTopicMappingWriteBatch);
                final int msgSize = request.getMsgSize();
                final long phyOffset = request.getCommitLogOffset();
                if (phyOffset + msgSize &amp;gt;= maxPhyOffset) {
                    maxPhyOffset = phyOffset + msgSize;
                }
            }
            // put lmq topic Mapping to DB if there has mapping exist
            if (lmqTopicMappingWriteBatch.count() &amp;gt; 0) {
                // write max topicId and all the topicMapping as atomic write
                ConfigHelperV2.stampMaxTopicSeqId(lmqTopicMappingWriteBatch, this.topicSeqIdCounter.get());
                this.configStorage.write(lmqTopicMappingWriteBatch);
                this.configStorage.flushWAL();
            }
            this.rocksDBConsumeQueueOffsetTable.putMaxPhyAndCqOffset(tempTopicQueueMaxOffsetMap, writeBatch, maxPhyOffset);
            this.rocksDBStorage.batchPut(writeBatch);
            this.rocksDBConsumeQueueOffsetTable.putHeapMaxCqOffset(tempTopicQueueMaxOffsetMap);
            long storeTimeStamp = requests.get(size - 1).getStoreTimestamp();
            if (this.messageStore.getMessageStoreConfig().getBrokerRole() == BrokerRole.SLAVE
                || this.messageStore.getMessageStoreConfig().isEnableDLegerCommitLog()) {
                this.messageStore.getStoreCheckpoint().setPhysicMsgTimestamp(storeTimeStamp);
            }
            this.messageStore.getStoreCheckpoint().setLogicsMsgTimestamp(storeTimeStamp);
            notifyMessageArriveAndClear(requests);
            return true;
        } catch (Exception e) {
            ERROR_LOG.error("putMessagePosition0 failed.", e);
            return false;
        } finally {
            tempTopicQueueMaxOffsetMap.clear();
            consumeQueueByteBufferCacheIndex = 0;
            offsetBufferCacheIndex = 0;
            this.rocksDBStorage.release();
        }
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;按 offset 查找消息&lt;/h3&gt; 
&lt;p&gt;在前文中我們已介紹過按 Offset 查找消息的流程，RocksDB 的實現裏，DefaultMessageStore 的 GetMessage 方法中使用的 ConsumeQueue 被替換成了 RocksDBConsumeQueue。這裏我們只關注其 IterateFrom 方法的實現，以下是該方法的代碼片段。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;public ReferredIterator&amp;lt;CqUnit&amp;gt; iterateFrom(String group, long startIndex, int count) throws RocksDBException {
        long maxCqOffset = getMaxOffsetInQueue();
        if (startIndex &amp;lt; maxCqOffset) {
            int num = Math.min((int) (maxCqOffset - startIndex), count);
            if (MixAll.isLmq(topic) || PopAckConstants.isStartWithRevivePrefix(topic)) {
                return iterateUseMultiGet(startIndex, num);
            }
            if (num &amp;lt;= messageStore.getMessageStoreConfig().getUseScanThreshold()) {
                return iterateUseMultiGet(startIndex, num);
            }
            if (!messageStore.getMessageStoreConfig().isEnableScanIterator()) {
                return iterateUseMultiGet(startIndex, num);
            }
            final String scannerIterKey = group + "-" + Thread.currentThread().getId();
            ScanRocksDBConsumeQueueIterator scanRocksDBConsumeQueueIterator = scanIterators.get(scannerIterKey);
            if (scanRocksDBConsumeQueueIterator == null) {
                if (RocksDBConsumeQueue.this.messageStore.getMessageStoreConfig().isEnableRocksDBLog()) {
                    LOG.info("new ScanIterator Group-threadId{} Topic:{}, queueId:{},startIndex:{}, count:{}",
                        scannerIterKey, topic, queueId, startIndex, num);
                }
                ScanRocksDBConsumeQueueIterator newScanIterator = new ScanRocksDBConsumeQueueIterator(startIndex, num);
                scanRocksDBConsumeQueueIterator = scanIterators.putIfAbsent(scannerIterKey, newScanIterator);
                if (scanRocksDBConsumeQueueIterator == null) {
                    scanRocksDBConsumeQueueIterator = newScanIterator;
                } else {
                    newScanIterator.closeRocksIterator();
                }
                return scanRocksDBConsumeQueueIterator;
            }
            if (!scanRocksDBConsumeQueueIterator.isValid()) {
                scanRocksDBConsumeQueueIterator.closeRocksIterator();
                if (RocksDBConsumeQueue.this.messageStore.getMessageStoreConfig().isEnableRocksDBLog()) {
                    LOG.info("new ScanIterator not valid Group-threadId{} Topic:{}, queueId:{},startIndex:{}, count:{}",
                        scannerIterKey, topic, queueId, startIndex, count);
                }
                ScanRocksDBConsumeQueueIterator newScanIterator = new ScanRocksDBConsumeQueueIterator(startIndex, num);
                scanIterators.put(scannerIterKey, newScanIterator);
                return newScanIterator;
            } else {
                if (RocksDBConsumeQueue.this.messageStore.getMessageStoreConfig().isEnableRocksDBLog()) {
                    LOG.info("new ScanIterator valid then reuse Group-threadId{} Topic:{}, queueId:{},startIndex:{}, count:{}",
                        scannerIterKey, topic, queueId, startIndex, count);
                }
                scanRocksDBConsumeQueueIterator.reuse(startIndex, num);
                return scanRocksDBConsumeQueueIterator;
            }
        }
        return null;
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;在上面的代碼中，首先通過 GetMaxOffsetInQueue 方法獲取該 Topic 隊列 ConsumeQueue 的 MaxOffset，MaxOffset 結合 Count 參數共同指定 Iterator 掃描的 Offset 區間。&lt;/p&gt; 
&lt;p&gt;然後，我們可以看到 IterateFrom 方法中根據不同的條件判斷分支返回不同類型的 Iterator 類對象，RocksDBConsumeQueueIterator 和 ScanRocksDBConsumeQueueIterator。下面是 &amp;nbsp;IteratorUseMultiGet 方法中創建 RocksDBConsumeQueueIterator 對象的調用鏈中最核心的代碼， RangeQuery 方法根據 StartIndex 和 Num 構建了要查詢的 Key 列表，然後調用 RocksDB 的 MultiGet 方法查詢到 Key 列表對應的 Value 列表，RocksDBConsumeQueueIterator 使用該 Value 列表上提供迭代器的功能。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;nbsp; &amp;nbsp;public List&amp;lt;ByteBuffer&amp;gt; rangeQuery(final String topic, final int queueId, final long startIndex,
        final int num) throws RocksDBException {
        final byte[] topicBytes = topic.getBytes(StandardCharsets.UTF_8);
        final List&amp;lt;ColumnFamilyHandle&amp;gt; defaultCFHList = new ArrayList&amp;lt;&amp;gt;(num);
        final ByteBuffer[] resultList = new ByteBuffer[num];
        final List&amp;lt;Integer&amp;gt; kvIndexList = new ArrayList&amp;lt;&amp;gt;(num);
        final List&amp;lt;byte[]&amp;gt; kvKeyList = new ArrayList&amp;lt;&amp;gt;(num);
        for (int i = 0; i &amp;lt; num; i++) {
            ByteBuffer keyBB;
            // must have used topicMapping
            if (this.topicMappingTable != null) {
                Long topicId = topicMappingTable.get(topic);
                if (topicId == null) {
                    throw new RocksDBException("topic: " + topic + " topicMapping not existed error when rangeQuery");
                }
                keyBB = buildCQFixKeyByteBuffer(topicId, queueId, startIndex + i);
            } else {
                keyBB = buildCQKeyByteBuffer(topicBytes, queueId, startIndex + i);
            }
            kvIndexList.add(i);
            kvKeyList.add(keyBB.array());
            defaultCFHList.add(this.defaultCFH);
        }
        int keyNum = kvIndexList.size();
        if (keyNum &amp;gt; 0) {
            List&amp;lt;byte[]&amp;gt; kvValueList = this.rocksDBStorage.multiGet(defaultCFHList, kvKeyList);
            final int valueNum = kvValueList.size();
            if (keyNum != valueNum) {
                throw new RocksDBException("rocksdb bug, multiGet");
            }
            for (int i = 0; i &amp;lt; valueNum; i++) {
                byte[] value = kvValueList.get(i);
                if (value == null) {
                    continue;
                }
                ByteBuffer byteBuffer = ByteBuffer.wrap(value);
                resultList[kvIndexList.get(i)] = byteBuffer;
            }
        }
        final int resultSize = resultList.length;
        List&amp;lt;ByteBuffer&amp;gt; bbValueList = new ArrayList&amp;lt;&amp;gt;(resultSize);
        for (int i = 0; i &amp;lt; resultSize; i++) {
            ByteBuffer byteBuffer = resultList[i];
            if (byteBuffer == null) {
                break;
            }
            bbValueList.add(byteBuffer);
        }
        return bbValueList;
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ScanRocksDBConsumeQueueIterator 則是使用了 RocksDB 的 Iterator 特性（&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Ffacebook%2Frocksdb%2Fwiki%2FIterator%EF%BC%89%EF%BC%8C%E7%9B%B8%E6%AF%94" target="_blank"&gt;https://github.com/facebook/rocksdb/wiki/Iterator），相比&lt;/a&gt; MultiGet，其擁有更好的性能。&lt;/p&gt; 
&lt;p&gt;下面是 ScanQuery 的實現，代碼比較簡潔，指定 Iterator 的 BeginKey 和 UpperKey，再調用 RocksDB 的 API 返回 Iterator 對象。&lt;/p&gt; 
&lt;p&gt;BeginKey 是通過 Topic 隊列信息和 StartIndex 參數構造的 Key。UpperKey 的構造比較精妙，還記得在 DefaultColumnFamily 介紹裏 Key 的格式吧，Key 的倒數第二個部分是 CTRL_1，作為 CqUnit 的 Key 時是個常量，Unicode 值為 1。構造 UpperKey 時，CTRL_1 被替換為 CTRL_2， Uinicode 值為 2，這樣能保證 Iterator 掃描區間的上限不超過 Topic 隊列 Offset 的理論最大值。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;public RocksIterator scanQuery(final String topic, final int queueId, final long startIndex,
        ReadOptions scanReadOptions) throws RocksDBException {
        final ByteBuffer beginKeyBuf = getSeekKey(topic, queueId, startIndex);
        if (scanReadOptions.iterateUpperBound() == null) {
            ByteBuffer upperKeyForInitScanner = getUpperKeyForInitScanner(topic, queueId);
            byte[] buf = new byte[upperKeyForInitScanner.remaining()];
            upperKeyForInitScanner.slice().get(buf);
            scanReadOptions.setIterateUpperBound(new Slice(buf));
        }
        RocksIterator iterator = this.rocksDBStorage.scan(scanReadOptions);
        iterator.seek(beginKeyBuf.slice());
        return iterator;
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;按時間戳查找消息&lt;/h3&gt; 
&lt;p&gt;與基於文件的實現類似，使用 RocksDB 來按時間戳查找消息，首先也需要確定 Topic 隊列 ConsumeQueue 的 MinOffset 和 MaxOffset，然後使用二分查找法查找到最接近指定時間戳的 CqUnit。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp;@Override
    public long getOffsetInQueueByTime(String topic, int queueId, long timestamp,
        BoundaryType boundaryType) throws RocksDBException {
        final long minPhysicOffset = this.messageStore.getMinPhyOffset();
        long low = this.rocksDBConsumeQueueOffsetTable.getMinCqOffset(topic, queueId);
        Long high = this.rocksDBConsumeQueueOffsetTable.getMaxCqOffset(topic, queueId);
        if (high == null || high == -1) {
            return 0;
        }
        return this.rocksDBConsumeQueueTable.binarySearchInCQByTime(topic, queueId, high, low, timestamp,
            minPhysicOffset, boundaryType);
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;與基於文件的實現不同的是，由於 RocksDB 的 CqUnit 裏保存了消息存儲的時間，比較時間戳時不必再讀取 CommitLog 獲取消息的存儲時間，這樣提升了查找的時間效率。&lt;/p&gt; 
&lt;h2&gt;總結及展望&lt;/h2&gt; 
&lt;p&gt;本文和讀者分享了 ConsumeQueue 的設計與實現，着重介紹其在消息消費場景的應用。鑑於篇幅限制，仍有許多細節未涉及，比如 ConsumeQueue 的容錯恢復、過期清理機制等。近些年，RocketMQ 往 Serveless 化方向發展，在 5.0 的架構裏，已經將計算和存儲分離，Proxy 作為計算集羣，Broker 作為存儲集羣。從實際應用上來講，Broker 作為存儲角色，從計算的角色釋放出來之後，多出的性能和資源應該用於承載更多的 Topic，而基於文件的 ConsumeQueue 實現限制了 Broker 的上限，因此我們需要 RocksDB 的實現方案來解決這個問題。&lt;/p&gt; 
&lt;p&gt;目前，騰訊雲的 TDMQ RabbitMQ Serveless、MQTT 產品均基於 RocketMQ 5.0 的架構部署運行，Broker 集羣已採用 RocksDB 的方案支持百萬級的 Topic 隊列，滿足 RabbitMQ 和 MQTT 協議需要大量 Topic 支持的場景。在騰訊雲 RocketMQ 5.0 的產品上，我們開始逐漸在新版本中灰度開啓該方案，為客戶提供更好性能更穩定的消息隊列服務。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/4587289/blog/18499646</link>
      <guid isPermaLink="false">https://my.oschina.net/u/4587289/blog/18499646</guid>
      <pubDate>Tue, 05 Aug 2025 02:23:50 GMT</pubDate>
      <author>原創</author>
    </item>
    <item>
      <title>阿里通義發佈開源文生圖模型 Qwen-Image</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;阿里通義千問團隊開源了其首個圖像生成基礎模型 Qwen-Image。該模型是一個擁有 200 億參數的 MMDiT（多模態擴散 Transformer）模型，基於 Apache 2.0 許可證開源。&lt;/p&gt; 
&lt;p&gt;Qwen-Image 在複雜文本渲染和精確圖像編輯方面取得了顯著進展，尤其在中文文本渲染上表現卓越。&lt;/p&gt; 
&lt;p&gt;Qwen-Image 的主要特性包括：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;卓越的文本渲染能力:&amp;nbsp;Qwen-Image 在複雜文本渲染方面表現出色，支持多行佈局、段落級文本生成以及細粒度細節呈現。無論是英語還是中文，均能實現高保真輸出。&lt;/li&gt; 
 &lt;li&gt;一致性的圖像編輯能力:&amp;nbsp;通過增強的多任務訓練範式，Qwen-Image 在編輯過程中能出色地保持編輯的一致性。&lt;/li&gt; 
 &lt;li&gt;強大的跨基準性能表現:&amp;nbsp;在多個公開基準測試中的評估表明，Qwen-Image 在各類生成與編輯任務中均獲得 SOTA，是一個強大的圖像生成基礎模型。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;如需體驗 Qwen-Image，訪問 QwenChat（chat.qwen.ai) 並選擇「圖像生成」功能。同時該模型已在魔搭社區與 Hugging Face 開源。&lt;/p&gt; 
&lt;p&gt;ModelScope：https://modelscope.cn/models/Qwen/Qwen-Image&lt;br&gt; Hugging Face：https://huggingface.co/Qwen/Qwen-Image&lt;br&gt; GitHub：https://github.com/QwenLM/Qwen-Image&lt;br&gt; Technical report：https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/Qwen_Image.pdf&lt;br&gt; Demo:&amp;nbsp;https://modelscope.cn/aigc/imageGeneration?tab=advanced&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;示例展示&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;宮崎駿的動漫風格。平視角拍攝，陽光下的古街熱鬧非凡。一個穿着青衫、手裏拿着寫着「阿里雲」卡片的逍遙派弟子站在中間。旁邊兩個小孩驚訝的看着他。左邊有一家店鋪掛着「雲存儲」的牌子，裏面擺放着發光的服務器機箱，門口兩個侍衞守護者。右邊有兩家店鋪，其中一家掛着「雲計算」的牌子，一個穿着旗袍的美麗女子正看着裏面閃閃發光的電腦屏幕；另一家店鋪掛着「雲模型」的牌子，門口放着一個大酒缸，上面寫着「千問」，一位老闆娘正在往裏面倒發光的代碼溶液。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0805/101844_6QPa_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364222</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364222</guid>
      <pubDate>Tue, 05 Aug 2025 02:19:50 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>KiCad - 開源免費電子設計自動化（EDA）套件</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                        
                                                                                    &lt;p style="color:#000000; margin-left:auto; margin-right:auto; text-align:start"&gt;KiCad EDA 是一款開源的電子設計自動化（EDA）軟件，基於 GPLv3 開源協議，最初由法國人 Jean-Pierre Charras 於 1992 年推出，現由 KiCad 開源社區維護。&lt;/p&gt;

&lt;p style="color:#000000; margin-left:auto; margin-right:auto; text-align:start"&gt;KiCad 提供了一個完整的設計流程，從原理圖到 PCB 佈局，以及 3D 模型和 BOM 生成。KiCad 支持多種文件格式，可以與其他 EDA 軟件兼容，並且可以在多種操作系統上運行，包括 Windows，Linux 和 Mac OS X，軟件包含工程項目管理、原理圖設計、線路板繪製、符號庫設計、封裝庫設計、線路板 3D 顯示、Gerber 查看、線路板實用計算等工具。&lt;/p&gt;

&lt;p style="color:#000000; margin-left:auto; margin-right:auto; text-align:start"&gt;&lt;img alt="" height="1080" src="https://oscimg.oschina.net/oscnet/up-416b95962f155bd5c9c0b3172706d97017f.png" width="1920" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

&lt;p style="color:#000000; margin-left:auto; margin-right:auto; text-align:start"&gt;&lt;img alt="" height="1080" src="https://oscimg.oschina.net/oscnet/up-a671e278e4aae67095a053103e543bc62a3.png" width="1920" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

&lt;p style="color:#000000; margin-left:auto; margin-right:auto; text-align:start"&gt;&lt;img alt="" height="1080" src="https://oscimg.oschina.net/oscnet/up-6501cd15c8e91e73f511b9edeaeed9ac221.png" width="1920" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

&lt;p&gt;KiCad EDA 官網：&lt;a href="https://www.kicad.org/"&gt;https://www.kicad.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;KiCad EDA 開源中國&amp;nbsp;&lt;a href="https://gitee.com/kicad-eda"&gt;https://gitee.com/kicad-eda&lt;/a&gt;&lt;/p&gt;

                                                                    &lt;/div&gt;
                                                                </description>
      <link>https://www.oschina.net/p/kicad</link>
      <guid isPermaLink="false">https://www.oschina.net/p/kicad</guid>
      <pubDate>Tue, 05 Aug 2025 02:11:50 GMT</pubDate>
    </item>
    <item>
      <title>​Perplexity AI 被指控祕密抓取被禁止的網站內容</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p style="margin-left:0; margin-right:0"&gt;根據互聯網基礎設施提供商 Cloudflare 的&lt;span&gt;最新&lt;/span&gt;研究&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.cloudflare.com%2Fperplexity-is-using-stealth-undeclared-crawlers-to-evade-website-no-crawl-directives%2F" target="_blank"&gt;報告&lt;/a&gt;，人工智能初創公司 Perplexity 被指控在抓取網站內容時忽視了明確的阻止指令。Cloudflare 表示，他們觀察到 Perplexity 在嘗試抓取網頁時隱藏了自己的身份，以此規避網站的偏好設置。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&lt;img height="312" src="https://oscimg.oschina.net/oscnet/up-cc5a99ff2f39168f9451614222dc2d73118.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Perplexity 等人工智能產品通常依賴於從互聯網收集大量數據，而這些初創公司長期以來在未獲得許可的情況下抓取文本、圖像和視頻，以便支持其產品的正常運作。近年來，許多網站通過使用標準的 Robots.txt 文件來應對這一問題，該文件指示搜索引擎和 AI 公司哪些頁面可以被索引，哪些頁面不可以。然而，當前這些努力的成效並不顯著。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;根據 Cloudflare 的分析，Perplexity 似乎通過更改其機器人的 「用戶代理」 來繞過這些限制。「用戶代理」 是指用於識別網站訪問者的設備和版本類型的信號。Cloudflare 還提到，Perplexity 更改了其自治系統網絡（ASN），這是一個識別互聯網上大型網絡的數字標識。Cloudflare 在數萬個域名和數百萬個請求中觀察到了這一行為，憑藉機器學習和網絡信號的結合成功識別了這一爬蟲。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Perplexity 的發言人 Jesse Dwyer 對 Cloudflare 的指控表示反駁，並稱其博客文章為 「推銷」。他補充稱，文中截圖顯示並沒有訪問內容。他進一步聲稱，Cloudflare 所提到的爬蟲並非其所擁有的。Cloudflare 表示，他們最初注意到這些問題是由於客戶投訴 Perplexity 仍在抓取其網站內容，儘管這些網站已通過 Robots 文件阻止了該爬蟲的訪問。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Cloudflare 的分析表明，Perplexity 不僅使用了其聲明的用戶代理，還在其被阻止時利用一個模擬 Google Chrome 的通用瀏覽器。最終，Cloudflare 決定將 Perplexity 的爬蟲從其驗證列表中移除，並採取新的技術來阻止其活動。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;值得注意的是，Cloudflare 最近對人工智能爬蟲表示反對，並推出了一個市場，允許網站所有者向訪問其網站的 AI 爬蟲收費。Cloudflare 的首席執行官馬修・普林斯曾警告稱，人工智能正在破壞互聯網的商業模式，尤其是出版商的盈利模式。這並非 Perplexity&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;第一&lt;/span&gt;次面臨未經授權抓取的指控，早在去年，《連線》雜誌等媒體就曾指控 Perplexity 抄襲其內容。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364217</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364217</guid>
      <pubDate>Tue, 05 Aug 2025 01:59:50 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>十年磨一劍，初心終不負，寫在 AntFlow 1.0.0 發佈之際</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;blockquote&gt; 
 &lt;p&gt;十年前的那個深夜，我在十八線小城的出租屋裏寫下第一行流程執行代碼時，絕不會想到它會在今天成為一款被數以千計的人點亮 ⭐ 的開源工作流引擎。 —— AntFlow 作者 TylerZhou&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;這段塵封的往事，直到 1.0.0 發佈當天才第一次被公開。&lt;/h2&gt; 
&lt;p&gt;其實 &lt;strong&gt;AntFlow 的故事遠比 2024 年更早&lt;/strong&gt;。 早在 2020 &lt;strong&gt;年&lt;/strong&gt;，口罩還沒爆發的時候，我和另一位同樣癡迷工作流的朋友就曾在深夜的出租屋裏，守着兩台發燙的筆記本，用 Vue2 + ElementUI 擼出了一個可拖拽的流程設計器雛形。 那時候我們給它起了個土味名字——「EasyFlow」。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;✔ 能畫節點、能連箭頭&lt;/li&gt; 
 &lt;li&gt;✔ 支持會籤/或籤、打回修改&lt;/li&gt; 
 &lt;li&gt;✔ 甚至在 GitHub 上收穫了第一批 20 顆星星&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;可命運總愛開玩笑：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;2021 年春節前後，口罩突襲，開始了長達半年的居家辦公,一下子突然不適應,工作效率極低,很多工作任務很難及時完成,antflow 也基本處於停滯狀態。&lt;/li&gt; 
 &lt;li&gt;2022 年，艱難地度過 2021 年，一切開始步入正軌，然而三月的某天的早上，同事開始一個個被 HRBP 叫去談話，快到中午的時候終於到我了，不出意外，意外還是來了，我所在的項目組就地解散；&lt;/li&gt; 
 &lt;li&gt;後面的幾個月大家都知道，開始九九八十一天休行。雖然沒有作有大量時間來做事，然而此時實在沒有心情，整天惶恐、驚悸、走神。。。整天被焦慮包圍，加個吃不飽飯，整個人暴瘦了近 40 斤，根本沒法工作&lt;/li&gt; 
 &lt;li&gt;倉庫停滯，Issue 堆灰，Vue2 版本也就此 &lt;strong&gt;「爛尾」。。。&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;那一整年和後面很長一段時間裏，我們各自為生計奔波，偶爾在微信裏互相調侃：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;EasyFlow 要是真黃了，對得起那 20 個 star 嗎？&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;有一天天午後，我隨手點開 GitHub，本想隨便看一下，然而卻像被釘在座椅上——Issues 裏陸續跳出的不是機器人，而是真真切切的用戶： 有人貼出報錯截圖、有人用蹩腳的中文寫下「加油」，還有人把 AntFlow 偷偷用在了畢業設計裏，只為省下一筆商業授權費。。。&lt;/p&gt; 
&lt;p&gt;我一條一條往下翻，輕軌的報站聲、窗外的蟬鳴全都褪成背景。鼠標滾到最底，光標靜止，屏幕映出一張發呆的臉。&lt;/p&gt; 
&lt;p&gt;記憶突然倒灌—— 2020 年，我和朋友擠在，十幾，平米的合租房裏，對着泛黃的枱燈，把「EasyFlow」的節點拖來拽去，敲得鍵盤噼啪作響； 2021 年初的年夜飯，我們端着餃子隔着視頻互相喊話「等年後回上海了就上線！」； 無數個繁星點點的夜晚當很多人已經伴着月亮入睡的時候我們還在對着牆上釘的草圖指點江山；&lt;/p&gt; 
&lt;p&gt;千千萬萬個早上太陽剛剛穿過黃海開始照亮這座城市的時候，我們早早起來跟着晨練的隊伍做半小時的訓練，然後開始投入編碼，直接八點半出門上班的鬧鐘響起。&lt;/p&gt; 
&lt;p&gt;我的偶像科比曾經有這樣靈魂的發問，你見過四點半洛杉磯的太陽嗎？我可以無愧於心地説，雖然未曾見過洛杉磯四點半的太陽，但我見到過無數上海早晨的第一縷陽光。&lt;/p&gt; 
&lt;p&gt;如今這些畫面像潮水一樣撞在胸口，屏幕上的字漸漸模糊。 我不甘心。。。。 不甘心那，嘔心瀝血的付出最終連簡歷的一行腳註都上不了，不甘心那些深夜的鍵盤聲最終被城市噪音吞沒， 更不甘心讓相信過我們的用戶，在 Issue 裏空等一個「won’t fix」。&lt;/p&gt; 
&lt;p&gt;我抬手合上電腦，指關節發白。 窗外飛機轟鳴而過，像一聲長哨——&lt;/p&gt; 
&lt;p&gt;有付出，有努力，有歡聲，有淚水，這一切不應當成為過眼雲煙。我們還很年輕，我們不應當就這樣消沉下去！&lt;/p&gt; 
&lt;p&gt;後面我們開始接觸頻繁起來。朋友向我介紹，這一年多時間裏他一直在想着重啓項目，為此花了不少時間學習 vue3，其實自己私下已經做出 vue3 版本的雛形了。並且介紹 vue3 並不是 vue2 的簡單&lt;/p&gt; 
&lt;p&gt;框架升級，在動手之前結合之前用戶反饋，以及思考 vue2 版本設計不足之處做了大量重構。。。後面一段時間裏開始頻繁溝通交流，隔着屏幕展示自己的代碼或者演示文稿，這場景是那麼的相似。。。&lt;/p&gt; 
&lt;p&gt;只不過已經由當初面對面交談變成了屏幕前對話。直到有一天，我們約定在一個週末見一面當面聊一下，挑個黃道吉日重建倉庫&lt;/p&gt; 
&lt;h2&gt;這一次，不是重啓，是赴約。&lt;/h2&gt; 
&lt;p&gt;直到 &lt;strong&gt;2024 年春夏之交的一天&lt;/strong&gt;，我們約在上海一家咖啡館見面.老友相逢,雖然有有説不盡的家常要嘮,但是我們只是簡單的寒暄了幾句就開始步入正題,因為我們深知見面的時間非常寶貴,要聊的議題非常多. 兩杯拿鐵下肚，翻出當年舊代碼，屏幕上的 Vue2 設計器早已跑不起來，但邏輯依舊清晰。 朋友拍桌子：「重來！這次用 Vue3，徹底重寫!」&lt;/p&gt; 
&lt;p&gt;當天晚上，我們各自回家，拉羣、建倉庫、起分支——&lt;strong&gt;2024 年 5 月 18 日 0:17&lt;/strong&gt;，第一條 commit 悄咪咪地躺進了 Gitee。 5 月 18 日，我們把 &lt;strong&gt;AntFlow-Vue3&lt;/strong&gt; 扔到 Gitee，24 小時內衝上了當周熱榜； 8 月 18 日，後端跟着開源，Star 數一路從 50 飆到 300、500、直接今天接近一千，加上前段早已超過一千，我們做了一個千星項目!&lt;/p&gt; 
&lt;p&gt;整整 &lt;strong&gt;90 天&lt;/strong&gt;，白天搬磚、晚上寫碼，凌晨三點還在調虛擬節點的 JSON 結構。&lt;/p&gt; 
&lt;h2&gt;所以今天你看到的一切——&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;釘釘風的 UI&lt;/li&gt; 
 &lt;li&gt;三分鐘就能接好的接口&lt;/li&gt; 
 &lt;li&gt;會籤/或籤/加簽/減籤/退回/灰度/委託/轉辦/變更處理人的「中國式全家桶」&lt;/li&gt; 
 &lt;li&gt;支持 TiDb 無限水平擴展&lt;/li&gt; 
 &lt;li&gt;支持低代碼模式和 DIY 模式&lt;/li&gt; 
 &lt;li&gt;支持 SaaS 化部署&lt;/li&gt; 
 &lt;li&gt;支持多模式多租戶多數據源&lt;/li&gt; 
 &lt;li&gt;支持 Java 8-21 各種版本&lt;/li&gt; 
 &lt;li&gt;支持 Spring Boot 1.x -3.x&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;都是兩個被口罩耽誤的程序員，在 2024 年親手補上的 2020 年的遺憾&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;如果你也曾因為不可抗力擱置過夢想，希望 AntFlow 的故事能讓你相信： &lt;strong&gt;只要人還在,熱情還在，重啓永遠來得及。&lt;/strong&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;h2&gt;🍂 十年前的第一行代碼：從自研引擎到「火葬場」，我的心路歷程&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;2014 年 · 三個月外包交付&lt;/strong&gt;為了趕一個 OA 項目，我用一週時間搭了 6 張表、一千行代碼，跑通了「請假—審批—抄送」的小閉環。交付那天，客戶請我們吃燒烤，大家舉起啤酒杯向我致敬：「大佬！」結果三個月後——&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;領導祕書要代批 ✅&lt;/li&gt; 
   &lt;li&gt;審計科臨時加節點 ✅&lt;/li&gt; 
   &lt;li&gt;領導想「看得見但點不動」 ✅ 代碼越糊越厚，我卻在深夜的機房裏懷疑人生：原來 &lt;strong&gt;「寫時一時爽，維護火葬場」&lt;/strong&gt; 是真的。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;2016 年 · 第一次逃離&lt;/strong&gt; 我離職了，帶着「再也不碰工作流」的誓言去了大城市。 結果第二家公司：「聽説你做過 OA？來，把這塊接一下。」 我：……&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;h2&gt;🌱 歸零重來：大廠風暴中的涅槃&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;2021 年（雖然口罩還沒過去，但是那真是少有的黃金時光） · 大廠修羅場&lt;/strong&gt;公司上市、流量暴漲、老系統隨時爆炸。新大佬空降，「你感覺現在 OA 怎麼樣？」然而我沒有被他的氣場壓住，也沒有被他的問題嚇懵，我不避諱的談了當前系統的現狀及原因。隨後我把早已熬夜整理的 37 頁 &lt;strong&gt;《為什麼我們需要一個新的中國式工作流引擎》&lt;/strong&gt; 遞了上去：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Activiti/Flowable 太重，畫個圖都要專業程序員&lt;/li&gt; 
   &lt;li&gt;中國式加簽、會籤、或籤、打回修改全是「補丁式開發」&lt;/li&gt; 
   &lt;li&gt;流程一旦出問題，日誌像天書，排查全靠猜，大佬看完只問了一句：「你有 demo 嗎？」 我把在朋友公司落地的 &lt;strong&gt;AntFlow-β&lt;/strong&gt; 打開，三分鐘跑完一條「客服離職交接」流程。 大佬拍板：「幹！給你三個月，外加一個釘釘出來的產品。研發需要幾個人，你來定」&lt;/li&gt; 
   &lt;li&gt;兩個產品，三個專職前端，三個後端開發，三個外包，風風火火的開始了。空降大佬雷厲風行的做事風格和裝 B 的做人風格實在讓人敬佩，不得不由衷地叫一聲：大佬！&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;近 300 個流程，有的從 0 到 1，有的從 1 到 2&lt;/strong&gt; 三個月重構上線，半年遷移 30 條老流程，一年新增 200 條。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;像打開了潘多拉的盒子，壓抑的需求井噴式到來，最誇張的一天，業務方排隊找：「今天能再幫我上線一條嗎？」 我：「三分鐘講完需求，excel 表格留下，三十分鐘！」 那一年，我第一次覺得 &lt;strong&gt;「流程也能像搭積木一樣快樂」&lt;/strong&gt;。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;h2&gt;🌟 開源的底氣：把「能用」變成「好用」&lt;/h2&gt; 
&lt;table&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;th&gt;維度&lt;/th&gt; 
   &lt;th&gt;十年前自研&lt;/th&gt; 
   &lt;th&gt;傳統 Activiti&lt;/th&gt; 
   &lt;th&gt;AntFlow 1.0.0&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;畫流程圖&lt;/td&gt; 
   &lt;td&gt;手改 JSON&lt;/td&gt; 
   &lt;td&gt;Eclipse 插件&lt;/td&gt; 
   &lt;td&gt;釘釘式拖拽&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;加簽/或籤&lt;/td&gt; 
   &lt;td&gt;改表 + 發版&lt;/td&gt; 
   &lt;td&gt;寫監聽器&lt;/td&gt; 
   &lt;td&gt;UI 勾選&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;排查問題&lt;/td&gt; 
   &lt;td&gt;翻日誌&lt;/td&gt; 
   &lt;td&gt;翻更長的日誌&lt;/td&gt; 
   &lt;td&gt;調試台一鍵追蹤&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;新人上手&lt;/td&gt; 
   &lt;td&gt;看祖傳代碼&lt;/td&gt; 
   &lt;td&gt;看 800 頁手冊&lt;/td&gt; 
   &lt;td&gt;10 分鐘小抄&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;二次開發&lt;/td&gt; 
   &lt;td&gt;重寫引擎&lt;/td&gt; 
   &lt;td&gt;繼承 37 個接口&lt;/td&gt; 
   &lt;td&gt;實現 1 個接口&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;「把複雜留給自己，把簡單留給用戶」——這是 AntFlow 寫在 README 裏的第一句話，也是我們在 1.0.0 版本里兌現的最後一條承諾。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr&gt; 
&lt;h2&gt;🚀 1.0.0 正式版亮點速覽&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;**虛擬節點（VNode）**首創 &lt;strong&gt;流程流轉業務與引擎 API 100% 解耦&lt;/strong&gt;，零流程引擎知識也能上手。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;中國式特色全家桶&lt;/strong&gt;會籤/或籤、順序會籤、前後去重、打回修改、加批、委託、灰度發佈……一口氣説完不帶喘。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;三分鐘即可嵌入老系統&lt;/strong&gt;只需實現一個接口：&lt;code&gt;MyBusinessAdapter implements FlowBizAdapter&lt;/code&gt;，老系統瞬間擁有釘釘同款流程。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;全鏈路可觀測&lt;/strong&gt;流程圖 = JSON，審批路徑 = JSON，調試台 = JSON，出了問題直接「所見即所得」。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;生產級高可用&lt;/strong&gt; 
  &lt;ol&gt; 
   &lt;li&gt;已在 &lt;strong&gt;3 萬客服、日均百萬流程量&lt;/strong&gt; 的環境穩定運行 3 年，MySQL 5.7/8.0、Java 8-17 全版本通過。&lt;/li&gt; 
   &lt;li&gt;已經在有 20 餘萬員工的快速公司中生產投產使用,貫穿 spring boot 1.3x -spring boot 3.x(目前主流支持 springboot 2.x,3.x. 1.x 需要改造才能運行)&lt;/li&gt; 
   &lt;li&gt;已經有 20 多家企業登記使用,其中不缺乏中海油天津,中國工商銀行浙江分行等知名公司&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;h2&gt;🎉 致謝：每一顆 ⭐ 都是繼續前行的光&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;給第一個 Star 的陌生人&lt;/strong&gt;那天凌晨兩點，我看到郵箱提示 「someone starred your repo」，激動得把熟睡的老婆搖醒：「有人給我點星了！」&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;給提第一個 Issue 的同學&lt;/strong&gt;你的一句「節點支持多條件嗎？」讓我們連夜加了 200 行代碼，第二天就發版。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;給默默登記企業案例的你們&lt;/strong&gt; 1.0.0 發版之前我們已經上線了捐贈感謝名單,最近捐贈用戶越來越多,非常感謝大家對開源的支持。那密密麻麻的名單，讓「小螞蟻」第一次有了「螞蟻森林」的感覺。今天是梭梭樹，名天也許就是胡楊林。。。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;h2&gt;🔮 下一站： 2.0 已在路上&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;引入 amis 引擎，讓低代碼表單發揮到極致。&lt;/li&gt; 
 &lt;li&gt;流程 AI 智能助手（自然語言生成流程圖）&lt;/li&gt; 
 &lt;li&gt;完善 SaaS 多租戶版本體驗&lt;/li&gt; 
 &lt;li&gt;對接更多幹預能力,諸如變更處理人,變更未來節點處理人,當前/未來節點加減籤&lt;/li&gt; 
 &lt;li&gt;……&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;關於名字，我們特別想多説一句&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;AntFlow 的 「Ant」 與 Ant Design 的 「Ant」 只是字母上的巧合，兩者沒有任何血緣關係，也不存在蹭流量的意圖。&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Ant Design = 前端 UI 設計語言&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AntFlow = 工作流引擎 &amp;amp; 審批平台&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;我們之所以堅持叫 &lt;strong&gt;AntFlow&lt;/strong&gt;，靈感來自兩個極小的瞬間：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;在 vue2 版本研發期間,一次我們忙到了深夜，朋友把第一張流程圖跑通後，指着屏幕説：「你看，這些節點像不像螞蟻搬家？一小步一小步就把業務搬完了,乾脆項目就叫 AntFlow 吧",我們一拍即合,就這樣愉快的決定了,沒想到後來成了不少疑問的根源...&lt;/li&gt; 
 &lt;li&gt;我們希望這款引擎像螞蟻一樣——&lt;strong&gt;小、輕、卻能搬動比自己重幾十倍的業務需求&lt;/strong&gt;。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;因此，&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;沒有「Ant Design 的表弟」；&lt;/li&gt; 
 &lt;li&gt;沒有「阿里系」背景；&lt;/li&gt; 
 &lt;li&gt;更沒有在 npm 包、域名、logo 上做任何容易混淆的設計。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;如果未來有人因為名字產生誤會，我們會第一時間解釋並尊重 Ant Design 的品牌權益。 也歡迎社區隨時監督——&lt;strong&gt;清者自清，代碼會説話&lt;/strong&gt;。&lt;/p&gt; 
&lt;h2&gt;一個默默無聞的開發者的結語&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;「開源不是終點，而是新的起點。」 如果你也曾被工作流折磨到凌晨三點，歡迎來 GitHub/Gitee 給我們一顆 ⭐， 讓我們一起把「中國式審批」的快樂，帶給更多還在加班的程序員。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr&gt; 
&lt;p&gt;&lt;strong&gt;AntFlow 1.0.0 現已發佈&lt;/strong&gt;&lt;a href="https://gitee.com/tylerzhou/Antflow"&gt;Gitee 倉庫&lt;/a&gt; | &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fmrtylerzhou%2FAntFlow-activiti" target="_blank"&gt;GitHub 倉庫&lt;/a&gt; | &lt;a href="https://gitee.com/tylerzhou/Antflow/wikis"&gt;官方文檔&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;「願你我用代碼改變世界，而不是被流程淹沒。」 —— TylerZhou 於 2025.08.04&lt;/p&gt; 
&lt;/blockquote&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364172/antflow-activiti-1-0-0-released</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364172/antflow-activiti-1-0-0-released</guid>
      <pubDate>Sat, 02 Aug 2025 14:24:00 GMT</pubDate>
      <author>來源: 資訊</author>
    </item>
    <item>
      <title>中國開源 AI 社區 7 月高亮時刻回顧</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;Hugging Face&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2FAdinaYakup%2Fstatus%2F1951020254269939964" target="_blank"&gt;發佈&lt;/a&gt;了中國 AI 社區 7 月高亮時刻，回溯這一個月來令人眼花繚亂的開源浪潮。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1504" src="https://static.oschina.net/uploads/space/2025/0804/184858_39sJ_2720166.png" width="962" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;包括：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;✨ 另一個「DeepSeek 時刻」——Kimi K2&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;✨ Qwen 完全矩陣化- Instruct / Thinking / Coder 模型跨越 30B - 480B 參數規模&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;✨ 多模態浪潮：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;GLM-4.1V-Thinking: Image+Text &amp;gt; Text&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Intern-S1: Image+Text &amp;gt; Text&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Wan 2.2 - Text +Image &amp;gt; video&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Skywork-R1V3: Image+Text &amp;gt; Text&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Skywork-UniPic: Text &amp;gt; Image / Image &amp;gt; Text&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Tar-7B: Any-to-Any&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Ming-Lite-Omni-1.5: Any-to-Any&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Step3: Image+Text &amp;gt; Text&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;HunyuanWorld-1: Image &amp;gt; 3D&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ThinkSound: Video &amp;gt; Audio&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Neta-Lumina: Text &amp;gt; Image&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;✨ 輕量級、可部署的模型&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;SmallThinker runs on 1GB RAM&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;✨ Agentic 編程成為主流&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Qwen3-Coder: fully spec'd tool calling&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;GLM-4.5: browser agents, IDE assistant&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Qwen3 WebDev demo: text-to-frontend code&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;✨特定領域和實用的模型/工具/數據集&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Science one S1: Scientific model&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Agentar DeepFinance: Finance dataset&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ObjectClear: Interactive Vision Tool&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Qwen3 MT Demo: Machine Translation Tool&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;其中回顧的 7 月 31 個亮眼開源模型、1 個框架、1 個數據集，來自 16 家企業、高校或研究機構：&lt;/p&gt; 
&lt;p&gt;阿里（9 個）、月之暗面（2 個）、智譜（2 個）、階躍星辰（1 個）、字節跳動（2 個）、崑崙萬維（2 個）、智源研究院（1 個）、中國電信人工智能研究院（1 個）、螞蟻集團（4 個）、快手（1 個）、捏 Ta（1 個）、磐石（3 個）、上海交通大學（1 個）、騰訊（1 個）、上海人工智能實驗室（1 個）、復旦大學（1 個）。&lt;/p&gt; 
&lt;p&gt;1、阿里（9 個）：編程模型 Qwen3-Coder-30B-A3B-Instruct、Qwen3-Coder-480B-A35B-Instruct，深度思考模型 Qwen3-30B-A3B-Thinking-2507、Qwen3-235B-A22B-Thinking-2507，基礎模型 Qwen3-235B-A22B-Instruct-2507、Qwen3-30B-A3B-Instruct-2507，CoT 音頻模型 ThinkSound，統一視頻生成模型 Wan2.2-TI2V-5B，文生視頻 Wan2.2-T2V-A14B。&lt;br&gt; 2、月之暗面（2 個）：MoE 基礎模型 Kimi-K2-Base，與 Numina 團隊聯合研發的數學定理證明模型 Kimina-Prover-72B。&lt;br&gt; 3、智譜（2 個）：多模態大模型 GLM-4.1V-9B-Thinking，基礎模型 GLM-4.5。&lt;br&gt; 4、階躍星辰（1 個）：基礎模型 Step3。&lt;br&gt; 5、字節跳動（2 個）：智能體模型 Tar-7B，多語言翻譯模型 Seed-X-Instruct-7B。&lt;br&gt; 6、崑崙萬維（2 個）：多模態推理大模型 Skywork-R1V3-38B，多模態統一模型 Skywork-UniPic-1.5B。&lt;br&gt; 7、智源研究院（1 個）：文生配音視頻框架 MTVCraft。&lt;br&gt; 8、中國電信人工智能研究院（1 個）：AI-Flow-Ruyi-7B-Preview0704。&lt;br&gt; 9、螞蟻集團（4 個）：多模態推理模型 M2-Reasoning，多模態大模型 Ming-Lite-Omni-1.5，金融訓練數據集 Agentar-DeepFinance-100K，交互式深度推理模型 KAG-Thinker-en-ch-7b-instruct。&lt;br&gt; 10、快手（1 個）：自適應思考模型 KAT-V1-40B。&lt;br&gt; 11、捏 Ta（1 個）：動漫風格圖像生成模型 Neta-Lumina。&lt;br&gt; 12、磐石（3 個）：科學基礎大模型 S1-Base-671B、S1-Base-8B、S1-Base-32B。&lt;br&gt; 13、上海交通大學（1 個）：端側原生大模型 SmallThinker-4BA0.6B-Instruct。&lt;br&gt; 14、騰訊（1 個）：3D 世界生成模型 HunyuanWorld-1。&lt;br&gt; 15、上海人工智能實驗室（1 個）：科學多模態大模型 Intern-S1。&lt;br&gt; 16、復旦大學（1 個）：語音生成模型 MOSS-TTSD-v0.5。&lt;/p&gt; 
&lt;p&gt;更多內容查看：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fcollections%2Fzh-ai-community%2Fjuly-2025-open-works-from-the-chinese-community-686586f1a8840797e477ae5a" target="_blank"&gt;https://huggingface.co/collections/zh-ai-community/july-2025-open-works-from-the-chinese-community-686586f1a8840797e477ae5a&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364138</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364138</guid>
      <pubDate>Sat, 02 Aug 2025 10:50:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>應用多點開花，AI 大模型從「炫技」走向「實幹」</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;&lt;span style="color:#000000"&gt;京東宣佈旗下言犀大模型品牌全新升級為 JoyAI，並推出行業首個 100% 開源的企業級智能體 JoyAgent；由釘釘 AI 平台訓練的垂類婦科大模型通過主任醫師考試；網易靈動發佈行業首個工程機械具身智能模型「靈掘」……近期，國產大模型頻頻「上新」，並不斷刷新應用「進度條」。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;而在近日舉行的 2025 世界人工智能大會（WAIC）期間，AI 大模型也是格外引人關注。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在物流領域，倉內無人機、無人車等智能物流設施驚豔亮相；工業場景中，AR 眼鏡可以輔助產業工人精準質檢並推薦維修方案；零售體驗台前，系統可以自動個性化推薦商品、瞬間生成海量商品廣告素材……位於上海世博展覽館一號館的京東展區內，展示了全新升級的 JoyAI 大模型深度應用的諸多場景。與此同時，京東雲還正式開源 JoyAgent 智能體。作為行業首個 100% 開源的企業級智能體，JoyAgent 依託多智能體協同引擎實現高效協作，並融合大小模型優勢，打通 AI 落地「最後一公里」。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;今年以來，國內大模型迭代速度提升，加快賦能千行百業。其中，不少企業着力打造垂類大模型，推動大模型快速走向「實幹」。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;日前，壹生檢康（杭州）生命科技有限公司研發的「豆蔻婦科大模型」成功通過國家婦產科衞生高級職稱（正高）筆試考試。釘釘 CTO 朱鴻介紹，豆蔻婦科大模型是釘釘 AI 平台上誕生的第一個專業垂類大模型，雙方團隊只經過短短一個多月的協作，就將模型準確率提升到了 90.2%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;「通過正高職稱考試，意味着該模型已具備主任級醫師的專業判斷力。」壹生檢康創始人王強宇表示，大模型的核心價值在於，為女性用戶提供居家自診斷支持，實現「術前分流」與「院外健康管理」；針對無需就診的情況提供科普指導與生活建議；為醫療、醫美等行業機構提供專業支撐，同時可通過機構的數據訓練專科模型，提升醫療效率。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;「這一突破為 AI 在婦產科臨牀決策輔助、循證醫學研究、患者健康教育、醫學生學習考試等場景的深度應用開闢了新路徑。」浙江大學醫學院附屬婦產科醫院婦科周博士表示。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;據悉，隨着技術的不斷完善和推廣，豆蔻婦科大模型不僅有望在更多醫療場景中發揮重要作用，還將進一步優化醫療資源配置。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在工業領域，垂類模型也有不少突破性進展。WAIC 期間，網易旗下工程機械智能化品牌網易靈動推出全球首個專為露天礦山挖掘機裝車場景打造的具身智能模型——「靈掘」。在網易靈動展位，觀眾通過智能座艙可以實時體驗內蒙古礦山的無人挖掘機自動裝車功能。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在內蒙古霍林河北露天煤礦的嚴苛環境中，「靈掘」單機裝車效率已達人工的 80%，近 70% 的作業時間無需人為幹預，成功適配極寒、高粉塵等嚴苛環境與多型號礦卡。「這項技術讓 AI 成為礦山的‘鐵臂戰友’，裝車 3 精度和連續性遠超預期，為行業安全與效率提升開闢了新路徑。」內蒙古某露天煤礦代表在實測後評價道。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;據瞭解，網易靈動將首次開源「靈掘」數據集，並向全行業發起「2027 產業協同計劃」。該計劃將聯合徐工、三一、山河智能等主機廠及各露天煤礦企業，通過技術共享平台推動聯合研發、場景共創與標準制定。作為「靈掘」技術基石的端到端訓練框架——「機械智心」，已支撐「靈掘」在礦山場景的成功實踐，並快速向港口清艙、混凝土拌合站、地銷煤等十餘個場景遷移，未來將延伸至農業、智能製造等領域。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;數據顯示，中國目前已發佈 1509 個大模型，在全球已發佈的 3755 個大模型中數量位居首位。業內指出，AI 大模型正從「炫技」走向「實幹」，2025 年成為大模型應用全面落地的關鍵轉折點。這場由技術驅動、場景牽引的深度應用革命，正在重塑千行百業的生產力圖譜。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364136</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364136</guid>
      <pubDate>Sat, 02 Aug 2025 10:39:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Taro on HarmonyOS 技術架構深度解析</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                                        &lt;blockquote&gt; 
 &lt;p&gt;2025 年 6 月，在華為開發者大會 2025 開發者場景技術共建分論壇，本文作者進行了《京東 Taro 框架鴻蒙版本正式開源，助力鴻蒙版三方應用開發》專題演講。期間闡述了 Taro on HarmonyOS 的技術實現方案、核心優化策略，以及開源版本的主要特性。&lt;/p&gt; 
 &lt;p&gt;本文將詳細介紹 Taro on HarmonyOS 的技術架構、性能優化實踐和開源進展，分享我們在跨端開發中遇到的問題和解決思路。&lt;/p&gt; 
 &lt;p&gt;期待更多人可以參與開源共建，一起交流討論！&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr&gt; 
&lt;span id="OSC_h2_1"&gt;&lt;/span&gt; 
&lt;h2&gt;背景&lt;/h2&gt; 
&lt;p&gt;回顧 Taro 的發展歷程，從 2018 年 6 月開源至今，作為開放式的跨端跨框架解決方案在眾多熱心開源貢獻者的支持下，從初出茅廬逐步邁向成熟。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="taro_background" src="https://oscimg.oschina.net/oscnet//e99ecb394150ea01d8d0df9310c36e30.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;從最初僅支持面向編譯時的小程序端解決方案，到如今擁有支持多種前端框架和 UI 庫的強大能力；從單一的構建工具，到通過開放生態為開發者提供 &lt;code&gt;Webpack&lt;/code&gt;、&lt;code&gt;Vite&lt;/code&gt;、&lt;code&gt;ESBuild&lt;/code&gt; 等豐富的工具選擇，讓團隊能夠定製專屬的研發流程；從專注小程序開發，到覆蓋各大小程序平台以及 Web、iOS、Android、HarmonyOS 等移動端場景——Taro 的每一步成長都離不開社區的力量。&lt;/p&gt; 
&lt;p&gt;這些年來，我們在 GitHub 上收穫了 &lt;strong&gt;36,000+ star&lt;/strong&gt; 和&lt;strong&gt;近 5,000 fork&lt;/strong&gt;，更重要的是得到了眾多企業團隊和個人開發者貢獻的寶貴功能特性。在此，我們要向所有支持 Taro 發展的朋友們表示衷心的感謝！&lt;/p&gt; 
&lt;span id="OSC_h3_2"&gt;&lt;/span&gt; 
&lt;h3&gt;技術架構演進&lt;/h3&gt; 
&lt;p&gt;説到 &lt;code&gt;HarmonyOS&lt;/code&gt;，Taro 從 2022 年開始佈局鴻蒙適配，走過了一條持續演進的技術路徑。最初我們推出了 &lt;code&gt;JSUI&lt;/code&gt; 版本的端平台插件，為鴻蒙支持打下基礎；2023 年開源了 &lt;code&gt;ETS&lt;/code&gt; 版本的端平台插件，大幅提升了開發體驗和業務性能；而在最近釋出的 4.1 版本中，&lt;code&gt;C-API&lt;/code&gt; 版本的 Harmony 端平台插件也正式發佈了，這標誌着 Taro 鴻蒙支持能力的重要突破。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="taro_on_harmonyos" src="https://oscimg.oschina.net/oscnet//b251d41c504ba0e92bd0ce7994398d6f.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;目前，我們仍在持續優化 Harmony C-API 插件的性能表現。團隊正在推進多線程以及更多版本特性的內部驗證，期待在驗證完成後能夠將其開源，為開發者在鴻蒙端帶來更優秀的研發體驗。&lt;/p&gt; 
&lt;span id="OSC_h3_3"&gt;&lt;/span&gt; 
&lt;h3&gt;面向多端研發&lt;/h3&gt; 
&lt;p&gt;面向多端的複雜場景，從來都不是一件容易的事情。在傳統的多端開發中，開發者往往需要面對各端語法標準不統一、組件和 API 接口各異、開發環境複雜多樣等諸多挑戰。當業務邏輯需要調整時，開發者必須在多個平台上重複實現相同功能，代碼複用率極低，維護工作量成倍增長。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="taro_cross_platform" src="https://oscimg.oschina.net/oscnet//35b595f2278f17a13dd02df5716f8266.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;如圖所示，Taro 現已成功在 HarmonyOS 平台上實現了與 Web 端、小程序及其他平台一致的 UI 呈現效果。&lt;/p&gt; 
&lt;p&gt;基於 Taro 跨端研發標準推進業務實現，開發者只需編寫一套代碼，就能夠在多個平台上獲得統一的用戶體驗，最大限度地節省多端業務的研發成本，讓團隊能夠將更多精力投入到業務創新上。&lt;/p&gt; 
&lt;span id="OSC_h3_4"&gt;&lt;/span&gt; 
&lt;h3&gt;京東鴻蒙版&lt;/h3&gt; 
&lt;p&gt;&lt;img alt="taro_harmony_jd" src="https://oscimg.oschina.net/oscnet//473a9f8879d8cddefdbac83f93802a3a.gif" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;以京東鴻蒙版本為例，基於 Taro on HarmonyOS 解決方案，成功在研發效率與應用性能之間達成了理想平衡，其性能表現和穩定性均位居行業前列。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="taro_jd_detail" src="https://oscimg.oschina.net/oscnet//e92b9ee175f09086507c7a5526832104.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;對於商品詳情頁等高複雜度、高數據量的核心業務場景，該方案展現出強大的技術適配能力。僅是在單線程 C-API 架構的支持下，這些重載業務場景的運行性能已達到與原生應用相當的水準，充分驗證了跨端技術在複雜場景下的可行性。&lt;/p&gt; 
&lt;span id="OSC_h2_5"&gt;&lt;/span&gt; 
&lt;h2&gt;技術架構&lt;/h2&gt; 
&lt;p&gt;為了達成這一目標，我們需要在技術架構層面進行深度優化。&lt;/p&gt; 
&lt;p&gt;Taro 在各平台的適配邏輯保持高度一致性。開發者通過統一的 &lt;code&gt;DSL&lt;/code&gt;以及標準化的組件和 API 庫即可完成全部代碼開發，樣式規範完全遵循 &lt;code&gt;W3C&lt;/code&gt; 標準，使前端開發者能夠以極低的學習成本快速上手。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="taro_2025" src="https://oscimg.oschina.net/oscnet//ee72a5129bf8027a11211bcd29c13fa8.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在編譯層面，Taro 通過 CLI 工具和插件系統實現各端的差異化處理。各個端平台插件可以在編譯核心中選擇基於 &lt;code&gt;Webpack&lt;/code&gt;、&lt;code&gt;Vite&lt;/code&gt; 或 &lt;code&gt;Metro&lt;/code&gt; 為基礎的編譯流程，將開發者的源代碼高效轉換為各目標平台的可執行代碼。&lt;/p&gt; 
&lt;p&gt;在運行時中，通過集成語法適配器、&lt;code&gt;DOM&lt;/code&gt;、&lt;code&gt;BOM&lt;/code&gt; 模擬實現以及其他核心模塊，確保開發者項目能夠在 HarmonyOS 等各類平台上穩定運行，真正實現一碼多端的開發願景。&lt;/p&gt; 
&lt;span id="OSC_h3_6"&gt;&lt;/span&gt; 
&lt;h3&gt;渲染層適配&lt;/h3&gt; 
&lt;p&gt;儘管 Taro 在 HarmonyOS 平台的插件架構歷經多輪重大版本升級，但其核心架構設計依舊可從以下幾個維度來理解：&lt;/p&gt; 
&lt;p&gt;&lt;img alt="taro_harmonyos_rendering" src="https://oscimg.oschina.net/oscnet//73d229191f933afdedd47c60cc69d5cc.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;代碼轉換流程&lt;/strong&gt;：從開發者編寫的 &lt;code&gt;React&lt;/code&gt; 代碼出發，通過與 &lt;code&gt;React Reconciler&lt;/code&gt; 的深度集成，系統構建出完整的虛擬節點樹。隨後，運行時環境通過模擬的 &lt;code&gt;DOM&lt;/code&gt; 和 &lt;code&gt;BOM&lt;/code&gt; API，實現 &lt;code&gt;React&lt;/code&gt; 節點樹與 Taro 內部節點樹的精確映射。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;平台適配實現&lt;/strong&gt;：結合標準化的組件庫和 API 庫，系統將抽象的節點結構轉換為 HarmonyOS 平台的原生原子組件，最終構建出完整的 &lt;code&gt;ArkUI&lt;/code&gt; 渲染樹，並呈現在用戶界面上。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;擴展能力支持&lt;/strong&gt;：除了核心渲染流程外，運行時還集成了佈局計算、事件處理、動畫效果等關鍵模塊，並持續接入更多 HarmonyOS 平台特有能力，為開發者提供完整的跨平台開發體驗。&lt;/p&gt; 
&lt;span id="OSC_h3_7"&gt;&lt;/span&gt; 
&lt;h3&gt;架構方案迭代&lt;/h3&gt; 
&lt;p&gt;在技術架構層面，&lt;code&gt;ETS&lt;/code&gt; 方案與 &lt;code&gt;C-API&lt;/code&gt; 方案本質上都遵循着相同的設計理念。兩者均構建了一套完整的三層節點樹體系：應用層的 &lt;code&gt;React&lt;/code&gt; 節點樹首先轉換為中間層的 Taro 節點樹，隨後進一步映射到底層的 &lt;code&gt;ArkUI&lt;/code&gt; 節點樹，最終實現界面的完整渲染。&lt;/p&gt; 
&lt;p&gt;然而，儘管在宏觀架構上兩種方案展現出高度的相似性，我們仍然堅定地推進從 &lt;code&gt;ETS&lt;/code&gt; 向 &lt;code&gt;C-API&lt;/code&gt; 的技術轉型。這一決策的背後，是團隊對性能極致追求的不懈努力。在移動應用開發的激烈競爭中，每一毫秒的性能提升都可能成為用戶體驗的關鍵差異點，而 C-API 方案正是在這樣的背景下應運而生的技術選擇。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="taro_harmonyos_architecture" src="https://oscimg.oschina.net/oscnet//d56195a048781803635354a4f73e5b0d.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;C-API&lt;/code&gt; 方案帶來的性能提升是全方位的。在節點操作層面，我們徹底摒棄了傳統的聲明式遞歸構建模式，轉而採用更加靈活的實現方式，這為底層節點 API 的深度優化創造了前所未有的空間。同時，通過引入指令式節點操作機制，不同節點樹之間的數據交互效率得到了顯著改善，原本複雜的跨樹通信變得更加高效流暢。&lt;/p&gt; 
&lt;p&gt;更為重要的是，我們將樣式處理、佈局計算、事件管理等核心功能模塊全面下沉至 &lt;code&gt;C++&lt;/code&gt; 原生層。這一架構調整不僅大幅減少了跨語言調用的頻次和開銷，更從根本上提升了系統的執行效率。通過這些多維度的優化措施，整個應用的性能表現實現了質的飛躍。&lt;/p&gt; 
&lt;span id="OSC_h3_8"&gt;&lt;/span&gt; 
&lt;h3&gt;跨端研發標準&lt;/h3&gt; 
&lt;p&gt;在適配鴻蒙和其他各端能力的基礎上，Taro 正在構建一套完整的跨端研發標準體系。這套標準不僅能夠最大限度地節約不同端之間的適配成本，更重要的是能夠充分兼容現有的前端生態系統，讓團隊多年積累的組件庫、工具鏈和技術沉澱得以無縫複用。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="taro_cross_standard" src="https://oscimg.oschina.net/oscnet//2e9017e41ce210f8f74ed721a42b25bf.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;目前，以 &lt;code&gt;React&lt;/code&gt; 作為 UI 基礎庫，該標準已涵蓋了 &lt;code&gt;View&lt;/code&gt;、&lt;code&gt;Text&lt;/code&gt; 等 26 個常用組件和網絡請求、圖片等 88 個常用 API。在樣式規範方面，我們遵循 W3C 標準實現了包含 93 條常用規範的樣式子集。與此同時，我們正在持續努力擴充這套標準體系，不斷增加新的組件類型、API 接口和樣式規範，以滿足日益複雜的業務場景需求。&lt;/p&gt; 
&lt;p&gt;更為關鍵的是，這套不斷完善的標準體系具備良好的擴展性和兼容性，能夠與團隊現有的 UI 組件庫、業務組件以及各類前端工具庫形成有機整合。我們致力於通過標準的持續演進，確保開發團隊能夠在跨端開發中充分發揮既有技術資產的價值，避免重複建設帶來的資源浪費，同時為未來更多端側適配需求預留充足的擴展空間。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="taro_harmony_style" src="https://oscimg.oschina.net/oscnet//ba286db72609455a07cb982564d36146.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;為實現跨平台開發的一致性標準，我們設計了 &lt;code&gt;C++&lt;/code&gt; 底層樣式處理架構。該架構整合了包括 &lt;code&gt;Yoga&lt;/code&gt; 這類成熟佈局引擎，構建統一的佈局計算體系，保障各端樣式渲染的視覺一致性。通過將樣式計算邏輯完全遷移至 &lt;code&gt;C-API&lt;/code&gt; 底層，系統獲得了顯著的性能優化潛力——不僅消除了對主渲染線程和業務邏輯的性能幹擾，還通過 &lt;code&gt;C++&lt;/code&gt; 的高效執行特性實現了跨端樣式處理的統一化管理，從根本上提升了整體渲染效率。&lt;/p&gt; 
&lt;p&gt;針對鴻蒙端的特殊需求，我們在編譯階段引入了創新的預處理機制。通過在編譯流程中的 &lt;code&gt;Rust&lt;/code&gt; 插件集成 &lt;code&gt;lightingCSS&lt;/code&gt;，我們能夠將標準樣式預先轉換為鴻蒙平台可以識別的樣式，進一步節省運行時運算的負擔。這一機制不僅實現了 W3C 標準屬性到各端專用單位和屬性值的智能轉換，更為跨端樣式的統一管理奠定了堅實的底層基礎。&lt;/p&gt; 
&lt;p&gt;基於這套完善的 &lt;code&gt;C++&lt;/code&gt; 樣式處理體系，UI 庫和業務團隊能夠輕鬆應對各種複雜場景的適配需求。無論是摺疊屏的多形態展示、關懷模式的無障礙優化，還是暗黑模式的主題切換，都可以通過靈活的樣式選擇器機制實現精準控制。同時，動畫效果和過渡轉場也能夠通過高效的樣式更新和節點刷新機制，呈現出極為流暢的用戶體驗。&lt;/p&gt; 
&lt;span id="OSC_h2_9"&gt;&lt;/span&gt; 
&lt;h2&gt;方案特性&lt;/h2&gt; 
&lt;p&gt;基於此架構，Taro on HarmonyOS 方案積累了豐富的核心特性。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;研發效能層面&lt;/strong&gt;：通過兼容 React 生態體系和 W3C 樣式規範，開發者能夠充分利用前端成熟的工具鏈和生態資源，高效完成業務功能迭代與開發調試工作，完善鴻蒙端的開發體驗。同時，開發者編寫的樣式代碼可在鴻蒙、小程序和 Web 端無縫複用，實現真正的"一次編寫，多端運行"。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;生態擴展層面&lt;/strong&gt;：提供靈活的組件和 API 擴展機制，支持業務團隊根據實際需求定製運行時環境。更重要的是，通過跨端統一的原生混合調用方案，Taro C++ 模塊與 ArkTS 原生模塊可實現雙向互調，為團隊間協作提供了更多可能性，有效避免重複開發，提升整體研發效率。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="taro_harmony_tech" src="https://oscimg.oschina.net/oscnet//2f6fbb8105c9d720737b1c639d96f421.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h3_10"&gt;&lt;/span&gt; 
&lt;h3&gt;性能體驗&lt;/h3&gt; 
&lt;p&gt;在 C-API 方案中，我們圍繞卓越性能體驗實現了多項核心特性：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;運行時性能優化&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;將 DOM Tree、事件處理、樣式計算等高頻操作模塊完全下沉至 C++ 層，顯著提升運行時執行效率。通過底層優化，減少了 JavaScript 與原生層之間的頻繁通信開銷，避免了數據序列化/反序列化的性能損耗。同時，C++ 層的內存管理更加高效，能夠更好地控制對象生命週期，減少內存碎片，為複雜應用場景提供更穩定的性能表現。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;高階組件能力&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;img alt="taro_harmony_list" src="https://oscimg.oschina.net/oscnet//3edfae841c90d8ab8bb6b83a5fd4701b.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; &lt;p&gt;基於 HarmonyOS 原生的 List、WaterFlow 等組件特性，深度定製實現虛擬列表、瀑布流等高性能組件，充分發揮系統級優勢。&lt;/p&gt; &lt;p&gt;這些高階組件不僅繼承了系統組件的原生性能，還針對前端開發習慣進行了接口封裝，支持動態數據加載、智能緩存策略、滾動性能優化等特性。開發者可以像使用傳統前端組件一樣輕鬆實現大數據量的列表展示，無需關心底層的複雜優化邏輯。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;圖片處理模塊&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;構建專門服務於樣式渲染、背景繪製、Image 組件的圖片處理系統，實現更優秀的圖片加載性能和內存管理。該模塊集成了多級緩存機制，支持內存緩存、磁盤緩存和網絡緩存的智能調度，大幅減少重複加載時間。&lt;/p&gt; &lt;p&gt;&lt;img alt="jd_image" src="https://oscimg.oschina.net/oscnet//e1c0bf07e61ef7b88ac2337b49f4d197.gif" referrerpolicy="no-referrer"&gt;&lt;/p&gt; &lt;p&gt;同時提供了圖片壓縮、格式轉換、尺寸適配等功能，能夠根據設備性能和網絡狀況自動選擇最優的圖片處理策略，有效降低內存佔用和網絡帶寬消耗。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;文字與繪圖支持&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;通過 PixelMap 技術為文字組件提供豐富的字體屬性和渲染能力，同時為 Canvas 組件及相關 API 提供底層支持，覆蓋分享海報生成等複雜業務繪製場景。文字渲染支持多種字體格式、文字效果（陰影、描邊、漸變等）和排版佈局，滿足不同設計需求。&lt;/p&gt; &lt;p&gt;&lt;img alt="taro_harmony_shared" src="https://oscimg.oschina.net/oscnet//443eb38b5216494391fcaf0ddc35e649.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; &lt;p&gt;Canvas 繪圖能力則支持路徑繪製、圖形變換、濾鏡效果等高級功能，為數據可視化、遊戲開發、創意設計等場景提供強大的圖形處理能力。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;視頻播放能力&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;基於 AVPlayer 重構 Video 組件和相關 API 實現，在 C-API 層直接接入，減少調用鏈路，為業務提供更靈活的視頻適配方案。新的視頻播放架構支持多種視頻格式和編碼標準，提供了精確的播放控制、實時進度反饋、音視頻同步等核心功能。&lt;/p&gt; &lt;p&gt;&lt;img alt="taro_harmony_video" src="https://oscimg.oschina.net/oscnet//9f5a8a4b269af1f5b7354aaf25169c9e.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;span id="OSC_h2_11"&gt;&lt;/span&gt; 
&lt;h2&gt;總結展望&lt;/h2&gt; 
&lt;p&gt;Taro 在 HarmonyOS 平台的深度適配，旨在為全場景應用開發開闢新的技術路徑。通過構建完善的鴻蒙端能力體系，我們致力於為更廣泛的業務場景提供技術支撐，推動跨平台開發在鴻蒙生態中的創新應用。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="taro_harmony_jd_all" src="https://oscimg.oschina.net/oscnet//61ac5ecfa77950e92954c28304b10de7.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在實際應用中，Taro 成功支撐了京東鴻蒙 APP 的商業化落地。該應用的首頁、搜索推薦以及核心購物流程等關鍵業務模塊均基於 Taro 技術棧開發，在確保快速迭代交付的同時，實現了業界領先的性能表現和系統穩定性。應用上線後迅速在華為應用市場購物類別中登頂，充分驗證了技術方案的商業價值。&lt;/p&gt; 
&lt;span id="OSC_h3_12"&gt;&lt;/span&gt; 
&lt;h3&gt;生態建設與合作拓展&lt;/h3&gt; 
&lt;p&gt;基於成功實踐的示範效應，更多京東生態應用正在加速鴻蒙化進程，包括一號會員店、七鮮等重要業務線的鴻蒙版本已上架鴻蒙應用市場或者進入開發階段。同時，我們的技術方案也獲得了外部合作伙伴的認可，58 同城、樸樸超市等知名企業均選擇採用 Taro 相關的鴻蒙開發解決方案，共同構建更加繁榮的鴻蒙應用生態。&lt;/p&gt; 
&lt;span id="OSC_h3_13"&gt;&lt;/span&gt; 
&lt;h3&gt;未來展望&lt;/h3&gt; 
&lt;p&gt;我們將持續深化開源戰略，在內部版本充分驗證後，逐步向社區開放多線程等更多核心技術特性。同時不斷擴展跨端標準覆蓋範圍，讓更多組件和 API 實現跨平台一致性，為開發者提供更優質的開發體驗和更完善的調試工具鏈，也為動態化能力構建更堅實的技術基礎。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="taro_harmony_jd_devtools" src="https://oscimg.oschina.net/oscnet//2aaf8134cf5e8255e8d878eacb2805a8.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在性能優化方面，我們將持續推進更多核心模塊向 C++ 層遷移，包括 React 的 C++ 版本實現和高頻 API 運行時模塊優化，同時積極借鑑節點樹扁平化等社區驗證的優秀實踐。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="taro_harmony_jd_c_react" src="https://oscimg.oschina.net/oscnet//21bfd7a53be846ef5a0b4bc0437f6180.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;雖然 Taro on HarmonyOS 的 C-API 版本插件開源時間不長，但已經吸引了眾多開發者的積極參與。我們期待更多技術同仁能夠加入這個充滿活力的開源生態，共同推動 Taro on HarmonyOS 方案的不斷完善，在開源共建的道路上續寫跨端開發的新篇章。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/4090830/blog/18686949</link>
      <guid isPermaLink="false">https://my.oschina.net/u/4090830/blog/18686949</guid>
      <pubDate>Sat, 02 Aug 2025 10:20:00 GMT</pubDate>
      <author>原創</author>
    </item>
    <item>
      <title>我國連續 12 年保持全球最大工業機器人市場</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;&lt;span style="color:#000000"&gt;2024 年，我國工業機器人市場銷量達 30.2 萬套，連續 12 年保持全球最大工業機器人市場。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;中國電子學會理事長徐曉蘭介紹，自 2015 年首屆世界機器人大會在北京召開以來，我國機器人產業實現一系列科技創新突破。2024 年，我國機器人專利申請量佔全球機器人專利申請總量的 2/3。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;產業發展方面，我國是全球第一大機器人生產國，工業機器人產量由 2015 年的 3.3 萬套增長至 2024 年的 55.6 萬套，服務機器人產量為 1051.9 萬套，同比增長 34.3%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;北京、上海分別成立國家地方共建具身智能機器人創新中心、國家地方共建人形機器人創新中心，浙江、安徽、湖北、廣東、四川等地均成立省級機器人創新中心，集聚區域產業優勢力量，推動技術共享與聯合攻關。機器人整機企業充分發揮引領作用，帶動產業鏈上下游零部件企業配套發展，形成大中小協同、上下游聯動的良好生態。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;應用場景方面，工業機器人已應用於國民經濟 71 個行業大類、236 個行業中類，製造業機器人密度已躍升至全球第三位。服務機器人在家用服務、倉儲物流、商用服務、養老助殘、醫療康復等領域的滲透率顯著提升。國際數據公司數據顯示，2024 年，中國廠商在全球商用服務機器人市場中佔據主導地位，出貨量佔比高達 84.7%，規模優勢明顯。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;「人形機器人是人工智能與機器人深度融合的產物，是機器人的高階形態和具身智能的良好載體。」徐曉蘭表示，人形機器人有望在家政服務、生產製造、倉儲物流、邊防海防、教育醫療等場景發揮作用，拉動新消費、催生新產業、擴大新就業，推動新質生產力加快發展。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;2025 世界機器人大會將於 8 月 8 日至 12 日在北京經濟技術開發區北人亦創國際會展中心舉辦。大會期間，200 餘家國內外優秀機器人企業的 1500 餘件展品將亮相，企業數量較去年增長 25%。其中，首發新品 100 餘款，數量是去年的近 2 倍。（人民日報）&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364133</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364133</guid>
      <pubDate>Sat, 02 Aug 2025 10:09:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>蘋果組建新 AI 團隊「AKI」，打造類似 ChatGPT 的 AI 搜索工具</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;彭博社記者&amp;nbsp;Mark Gurman &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.bloomberg.com%2Fnews%2Fnewsletters%2F2025-08-03%2Fapple-s-chatgpt-rival-from-new-answers-team-iphone-17-spotted-in-the-wild-mdvmqs6g" target="_blank"&gt;報道稱&lt;/a&gt;，蘋果正開發一款類似 ChatGPT、能夠直接回答用戶廣泛問題的搜索引擎。&lt;/p&gt; 
&lt;p&gt;該項目由一個新成立的&lt;strong&gt;「Answers, Knowledge, and Information，簡稱 AKI」&lt;/strong&gt;（答案、知識與信息）內部團隊負責。領導該新項目的是高級總監 Robby Walker，他曾負責 Siri 的研發工作。雖然該項目仍處於早期階段，但該團隊正在構建所謂的 「答案引擎」—— 一個能夠抓取網頁以回應通用知識問題的系統。目前正在探索開發一款獨立應用，同時也在搭建新的後端基礎設施，旨在為未來版本的 Siri、Spotlight 和 Safari 提供搜索功能。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0804/175856_2Ke0_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;蘋果最近已在其招聘網站上為該團隊發佈了職位空缺，他們正在為該團隊招募具有搜索算法和引擎開發經驗的工程師。招聘信息稱：「我們的工作為蘋果一些最具標誌性的產品（包括 Siri、Spotlight、Safari、Messages、Lookup 等）提供直觀的信息體驗。加入我們，共同塑造全球與信息連接方式的未來！」&lt;/p&gt; 
&lt;p&gt;此舉標誌着蘋果在 AI 上的重大轉變，因為此前蘋果高管曾多次表示，無意開發自有聊天機器人，而是選擇集成第三方服務。&lt;/p&gt; 
&lt;p&gt;目前，Siri 在處理複雜問題時表現不佳，蘋果與谷歌之間價值約 200 億美元的默認搜索引擎協議正面臨美國司法部的嚴格審查，蘋果可能因此感到有必要開發自主引擎。不過與此同時，蘋果正面臨 AI 人才流失問題，負責開發大語言模型的團隊在一個月內已有四名關鍵研究員離職，轉投競爭對手 Meta。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364130</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364130</guid>
      <pubDate>Sat, 02 Aug 2025 10:01:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>AI 編程工具 Roo Code 支持通過對話歷史提供更智能的建議</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;AI 編程助手 Roo Code 發佈了&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.roocode.com%2Fupdate-notes%2Fv3.25.4" target="_blank"&gt;v3.25.4&lt;/a&gt;更新，支持基於最近 10 條消息作為上下文來增強其代碼建議，從而提供更智能、更少幻覺的響應。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0804/174428_JrSy_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;用戶可以完全控制 API 路由和歷史記錄的開關，以平衡上下文和隱私需求。&lt;/p&gt; 
&lt;hr&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;Roo Code 是一個 AI 驅動的開源自主編碼 Agent，它存在於您的編輯器中。&lt;/p&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0618/193229_CZTb_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;功能&lt;/p&gt; 
&lt;ul style="list-style-type:disc; margin-left:0; margin-right:0"&gt; 
 &lt;li&gt;用自然語言溝通&lt;/li&gt; 
 &lt;li&gt;直接在您的工作區讀寫文件&lt;/li&gt; 
 &lt;li&gt;運行終端命令&lt;/li&gt; 
 &lt;li&gt;自動化瀏覽器操作&lt;/li&gt; 
 &lt;li&gt;與任何 OpenAI 兼容或自定義的 API / 模型集成&lt;/li&gt; 
 &lt;li&gt;通過&lt;strong&gt;自定義模式&lt;/strong&gt;調整其 "個性" 和能力&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364125</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364125</guid>
      <pubDate>Sat, 02 Aug 2025 09:46:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>辯證看待 KubeSphere 閉源刪庫，前核心團隊成員的解讀</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;ul&gt; 
 &lt;li&gt;&lt;em&gt;文章來源：微信公眾號 Cloud Native Fun&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;作者：周鵬飛&lt;/em&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong style="color:#000000"&gt;2025 年 8 月 1 日，青雲科技在 KubeSphere 社區發佈消息，宣佈暫停 KubeSphere 的開源版本支持（&lt;/strong&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fkubesphere%2Fkubesphere%2Fissues%2F6550" target="_blank"&gt;&lt;u&gt;https://github.com/kubesphere/kubesphere/issues/6550&lt;/u&gt;&lt;/a&gt;）。這一消息如同投下一顆重磅炸彈，引發了全球開源社區的強烈反響。&lt;/span&gt;&lt;/p&gt; 
 &lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;作為前青雲科技的高級社區經理、KubeSphere 項目的前核心維護者，我曾參與這個項目從零到一的過程。在這篇文章中，我站在一個長期開源從業者的角度，嘗試用辯證的視角去還原事件背後更深層的邏輯，並回答一些幾個普遍關注的問題。&lt;/span&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;KubeSphere 自 2018 年 4 月份開始在 GItHub 寫下第一行代碼並開源，至今七年多的時間，曾被全球數以萬計的大小企業所使用，與眾多知名開源項目集成，被全球各大雲廠商認可和合作，不可否認的事實是，KubeSphere 是一個非常優秀的開源項目和雲原生產品，項目創始人 Ray 也是一個很有開源情懷的人。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;我曾在 2018-2022 年擔任青雲科技的高級社區經理，在全球不遺餘力地推廣 KubeSphere，&lt;/span&gt;&lt;span&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzA4MDcyOTc2Nw%3D%3D%26mid%3D2247485443%26idx%3D1%26sn%3D0d66089771b9ea44006666eeb4471af8%26scene%3D21%23wechat_redirect" target="_blank"&gt;從零到一構建了活躍多元化的開源社區&lt;/a&gt;&lt;/span&gt;&lt;span style="color:#000000"&gt;，創作了官網、用戶文檔、博客、視頻教程，在全國各大城市辦過幾十場活動，並將 KubeSphere 孵化的 3 個子項目捐給 CNCF 基金會。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="324" src="https://oscimg.oschina.net/oscnet/up-a83df931bfba4277ce01f9eb46797ef1d7d.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;2020-2022 年幾乎是 KubeSphere 在全球飛速發展的三年，我記得曾經幾乎每天都能看到新的用戶增長和使用反饋，以及來自全球不同公司的貢獻者參與。説實話，那曾是我最有工作成就感和成長飛快的時光，在開源社區認識了很多優秀的開發者，也給公司帶來過一些商業機會。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="330" src="https://oscimg.oschina.net/oscnet/up-49bfe588b4f1c2be1102d667fadac349a14.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;h2 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong style="color:#773098"&gt;一次不留餘地的急轉彎&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;令人遺憾的是，這次青雲的決定來了個 180 度急轉彎，不僅將前端代碼閉源，還刪除所有已發佈的文檔和安裝鏡像，徹底將 KubeSphere 推向了深淵，在全球開源社區引發了巨大的輿論和開源信任危機。巧合的是，消息發佈當天正是項目創始人 Ray 宣佈離職的時間點。我不知道這是否是有意為之，但屬實是不講「情面」的一個選擇，也引發了國內外很多開發者對「中國式開源」的發問和深層焦慮。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="319" src="https://oscimg.oschina.net/oscnet/up-1e8382254228e4b54d873e3631e7c2654cd.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;h2 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong style="color:#773098"&gt;牆倒眾人推，並不能讓中國開源更好&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;事實上，我剛開始看到消息時也有些情緒化，因為 「KubeSphere 閉源刪庫」後，互聯網輿論呈現了牆倒眾人推的趨勢，很多博主的文章以及吃瓜羣眾的評論，站在制高點一味地批判和指責商業公司「閉源」的行為，詆譭該開源項目的價值，甚至還有很多人直接全盤否定 「中國開源」 的努力和價值。這些帶有偏見的觀點，很容易誤導大眾的認知和情緒，並且不會讓這個行業變得更好。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;我也看到了很多曾經的用戶、貢獻者、前同事們表示唏噓和惋惜，感嘆自己曾經參與過的開源明星項目的隕落。我沒有選擇在第一時間發佈這篇文章，在閲讀了國內外很多博主的文章、討論和用戶在 GitHub 上的回覆後，帶着辯證的角度來分享自己的一些觀點。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;截止目前，我看到的可能有價值的一個討論是，有一些用戶在 GitHub 上希望通過投票和眾籌的方式重新組建純社區自發驅動的 KubeSphere 開源貢獻者團隊，Issue 下也有眾多用戶和貢獻者表示支持，這讓我感受到了些許欣慰，因為它展現了開源社區開發者的集體力量，但這個目標要實現的難度不亞於去經營一家公司。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="229" src="https://oscimg.oschina.net/oscnet/up-bab13eb637d429b475fd818bbd06bc869b5.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;h2 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong style="color:#773098"&gt;維護開源是商業公司應有的社會責任嗎？&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;這是一個值得每位開源參與者深思的問題。開源項目由商業公司主導，並不意味着它對社區有「義務」無限維護。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;顯然 「KubeSphere 閉源刪庫」 是一個唐突而又糟糕的決策，但我想説，&lt;strong style="color:#000000"&gt;選擇「閉源」或核心團隊撤離開源項目這件事情本身從來不是某一家商業公司的過錯，&lt;/strong&gt;任何一家商業公司主導的開源項目都存在這樣的「閉源」風險，這在開源界也已有多個案例。客觀來説，在當前的經濟下行的市場環境下，幾乎所有商業公司都在收縮投資或調整戰略，對於營收增長慢的項目和人力都有被優化的可能。&lt;strong style="color:#000000"&gt;維護開源項目並不是一家商業公司應有的社會責任或義務。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;但是，開源項目背後的主導公司如果希望撤資開源投入，&lt;strong style="color:#000000"&gt;需要遵循開源項目客觀存在的生命週期，&lt;/strong&gt;而不是把開源社區一刀切，用開源斷供來「綁架」用戶，轉到自家商業產品，這並不是一項精明的生意決策。實際上，&lt;strong&gt;如果青雲選擇將 KubeSphere 項目歸檔 （Archive）並選擇提前一段時間在社區發佈項目 Retire 的公告，同時致謝所有參與貢獻的開發者和用戶，那就不會出現今天的危機局面&lt;/strong&gt;。&lt;/span&gt;&lt;/p&gt; 
&lt;h2 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong style="color:#773098"&gt;失去開發者=失去企業服務信任&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;「得開發者得天下」 是一個很通俗的道理，很多大小企業選擇走開源的策略，也正是因為開源是一個能夠低成本地快速地獲取全球開發者用戶的機會和建立跨企業的社區合作模式，比傳統的閉源軟件開發和 Marketing 傳播更快，通過開源社區協作開發也更容易建立規模與行業標準。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;但很多商業公司錯誤地把開源作為低成本「獲客」的渠道，&lt;strong style="color:#000000"&gt;把開源作為一項「免費廣告」的福利，利用開發者對開源技術的關注來獲取銷售商機，這樣的做法是對開源社區最大的傷害，也是對商業公司口碑的破壞&lt;/strong&gt;。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;實際上，很多開源項目的開發者用戶是一些企業的運維開發負責人或內部決策影響者，他們雖然可能不能直接決定公司的軟件採購，但他們提供的觀點和意見會直接或間接地影響企業管理層的決策。很可惜，很多開源商業化公司的 CXO 們並不懂開發者的重要性，忽視了開源社區的規模影響力。&lt;/span&gt;&lt;/p&gt; 
&lt;h2 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong style="color:#773098"&gt;青雲做些什麼能補救？&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;答案是能。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;青雲作為一家上市公司，公司層面一定還是會關注自己在業內的口碑以及客戶的信任。這裏我不適合作為局外人指點江山，但提供思路作為參考，&lt;strong style="color:#000000"&gt;畢竟在社區裏還有很多用戶在關注 KubeSphere 後續是否會有好的轉機。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;讓我們來看一個正面的例子，CNCF 畢業項目 Flux 背後的核心維護公司 WeaveWorks 在去年 2 月份雖然宣佈了公司關閉停止運營，但 CEO 第一時間在聯繫一些大公司的用戶和貢獻者參與 Flux 項目的維護，並積極尋求 CNCF 的幫助，確保該項目在即使沒有了 WeaveWorks 公司的支持，還能繼續在社區維護和迭代。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="504" src="https://oscimg.oschina.net/oscnet/up-5c4d3b4efba258bd1ad215ef4c196d40c65.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;青雲是否會考慮做一個 「git revert」 的回滾操作，來響應社區目前呼聲最高的提議&lt;/strong&gt;：恢復閉源的倉庫、下線的文檔、歷史鏡像，並將開源項目交由給中立的社區自發去維護？青雲作為商業公司繼續去做自己的商業產品，或許還有重建信任的機會。如果公司層面擔心開源項目搶了自己的商業產品的蛋糕，那必然是商業產品與服務做得還不夠好。&lt;/span&gt;&lt;/p&gt; 
&lt;h2 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong style="color:#773098"&gt;對 KubeSphere 開源協議的疑問？&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;KubeSphere 有兩個子項目的開源協議值得探討，第一個是此次被閉源的前端 Console 項目開源協議是 AGPL-3.0，這個協議除了要求二次分發必須開源沒有其它問題，一些知名項目如 Grafana 也採用該協議。還記得 KubeSphere 曾多次被國內和海外的一些大公司換 Logo 後改個名字後就拿去賣錢，AGPL-3.0 的協議某種程度上也有一定的品牌保護作用。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;但值得注意的是，後端開源是在 Apache 2.0 基礎上加入了「附加條款」的開源協議，例如：&lt;/span&gt;&lt;/p&gt; 
&lt;ul style="margin-left:0; margin-right:0"&gt; 
 &lt;li&gt; &lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;禁止用於商業化 SaaS 服務&lt;/strong&gt;；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;禁止移除或修改 KubeSphere 品牌和 logo&lt;/strong&gt;；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;要求貢獻者接受維護者可以在未來&lt;strong&gt;改變授權方式&lt;/strong&gt;的條款；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;對 fork 或商用做出限制。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;雖然它的目的是出於保護青雲的商業產品利益和 KubeSphere 品牌，但這個修改後的協議是不符合 OSI（Open Source Initiative）定義的開源標準的，破壞了 Apache 2.0 的開放性和自由性。所以青雲回覆 KubeSphere 將繼續保持核心代碼開源的聲明，是需要推敲的。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;開源協議的選擇，以及&lt;strong style="color:#000000"&gt;所選開源協議的開放程度和商業友好程度，會從本質上影響和決定這個項目最終是否會有眾多企業級貢獻者來參與&lt;/strong&gt;。&lt;/span&gt;&lt;/p&gt; 
&lt;h2 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong style="color:#773098"&gt;關於「K8s 上游貢獻」的誤解&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;圈子裏有開源從業者提出了一個問題：&lt;strong style="color:#000000"&gt;KubeSphere 作為 K8s 發行版，秉着 「Upstream First」 的原則， KubeSphere 項目維護者在上游 K8s 社區的貢獻有多少？&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;實際上，研究這個問題對於分析這個事件的的意義並不大，因為 KubeSphere 不算嚴格意義上 K8s 發行版，因為它沒有改 K8s 一行代碼，沒有對 K8s 進行二次分發，它是一個構建於 Kubernetes 之上的平台型項目，用戶可以使用自己已有的 K8s 對接 KubeSphere，所以從產品定義層面這個説法不是非常準確；&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;其次，KubeSphere 團隊對 K8s 項目的貢獻多少，並不會直接影響到 KubeSphere 開源項目本身的可持續性發展，上游貢獻的指標僅對下游企業在 K8s 上游社區的話語權和影響力能產生影響，或對企業內部基於 K8s 做了擴展或二次修改的廠商，或是直接提供託管 K8s 雲服務的雲廠商，能體現技術實力。&lt;/span&gt;&lt;/p&gt; 
&lt;h2 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong style="color:#773098"&gt;可持續的開源項目，有哪些共同特徵？&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;em&gt;「一個人可以走得很快，但一羣人可以走得更遠。」&lt;/em&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;很多用戶評估開源項目是否值得在企業內部特別是生產環境採用，習慣先去看這個項目的 GitHub Star 數量來評估這個項目的流行度，從而判斷該項目的可持續性。實際上，這樣的方式太過於片面和業餘。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;我如果作為用戶，我認為最可靠的方式是去關注這些指標：&lt;/span&gt;&lt;/p&gt; 
&lt;ul style="list-style-type:disc; margin-left:0; margin-right:0"&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;有多少家公司參與了開源項目的貢獻與維護？貢獻比例分別是多少？（單一廠商控制的開源項目屬於高風險）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;開源社區治理和開發流程是否公開和透明？&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;有多少大的企業已經採用了該開源項目？（這通常在官網或 README 中能找到）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;該開源項目對商業化是否友好？業內是否已有多家商業公司集成和提供商業支持？&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style="color:#000000"&gt;…&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;我的觀點是，真正強大的開源項目，往往擁有更豐富的「社區股東」和更清晰的合作模型。&lt;strong style="color:#000000"&gt;一個開源項目的貢獻者和商業生態越多元化，擁抱它的企業越多，它的可持續性將會越強大。&lt;/strong&gt;如果只有單一廠商在維護，並且只有用戶生態，缺乏不同組織的貢獻者，並且用戶市場還侷限在國內，這樣的開源項目大概率是走不遠的。&lt;/span&gt;&lt;/p&gt; 
&lt;h2 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong style="color:#773098"&gt;我們從該事件能學到什麼？&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong style="color:#000000"&gt;對於希望構建開源商業化的公司&lt;/strong&gt;：開源的核心價值並不僅僅是代碼和技術，而是圍繞這些能力構建起來的社區生態與信任體系。如果把開源僅僅視為一種「獲客渠道」，通過免費開放源碼吸引用戶，再通過商用版本進行轉化，卻忽略了社區治理、合作機制、開發者關係和中立性建設，最終可能得到短期流量，但失去長期信任。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong style="color:#000000"&gt;對開源項目使用者的啓示：&lt;/strong&gt;&amp;nbsp;如何選型一個更有可持續性的項目是非常關鍵的，上面提到的一些指標可以作為參考。作為用戶在有餘力的情況下，可以思考自己作為用戶如何參與到開源社區中，畢竟 「眾人拾柴火焰高」。&lt;/span&gt;&lt;/p&gt; 
&lt;h2 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong style="color:#773098"&gt;寫在最後&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;我在西雅圖的週日晚午夜時分寫完這篇文章，雖然我已經離開了前司青雲三年多時間，但依然對曾經一起參與 KubeSphere 維護的前同事、社區貢獻者和用戶們合作的時光非常懷念，當時 KubeSphere 項目也確實在四海大地聚集了很有有熱情和信仰的開源愛好者。如今的局面，個人還是衷心希望能有一些好的轉機！&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364121</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364121</guid>
      <pubDate>Sat, 02 Aug 2025 09:36:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>谷歌變更 goo.gl 短鏈接服務「停用」計劃，會保留活躍鏈接</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;谷歌去年&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdevelopers.googleblog.com%2Fen%2Fgoogle-url-shortener-links-will-no-longer-be-available%2F" target="_blank"&gt;宣佈&lt;/a&gt;，它將於 2025 年 8 月 25 日關閉 Google URL Shortener 短鏈接服務（goo.gl/*），屆時所有 goo.gl 鏈接將會&lt;a href="https://www.oschina.net/news/362402"&gt;停止響應&lt;/a&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-3de2e977a212c4b76a27a10e76c003b1a75.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;距離關閉日期不到一個月時間，在依賴於 goo.gl 短鏈接的開發者、教育工作者和記者等表達擔憂之後，&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.google%2Ftechnology%2Fdevelopers%2Fgoogl-link-shortening-update%2F" target="_blank"&gt;谷歌改變了主意&lt;/a&gt;，採取了更溫和的立場：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0804/173324_wscd_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;它將只禁用自 2024 年底以來沒有任何活動的 goo.gl 鏈接，如果 goo.gl 鏈接在活躍使用或點擊，這些鏈接將能繼續使用&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-c970c162854ec8999ffbf88f6e05480f82b.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364120/google-googl-shutdown-reversal</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364120/google-googl-shutdown-reversal</guid>
      <pubDate>Sat, 02 Aug 2025 09:29:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>揭祕字節跳動內部流量調度與容災實踐</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                                        &lt;div&gt; 
 &lt;div&gt;
   資料來源： 
  &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.volcengine.com%2F" target="_blank"&gt;火山引擎-開發者社區&lt;/a&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; 摘要： 在字節跳動，平衡超大規模流量的穩定性、性能、容量與成本，是一系列產品共同面臨的挑戰，其中， Trafficroute GTM 起到了不可忽視的作用&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;Trafficroute GTM 承載了字節跳動億級流量、覆蓋了大規模場景，是一款基於 DNS 的流量路由服務，我們將通過兩期文章，揭祕字節跳動如何通過 Trafficroute GTM 巧妙應對以上挑戰，實現高效流量管理！&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;上期內容中，我們主要介紹了基於 TrafficRoute GTM 的 GEO-基礎路由模式進行自定義流量編排，感興趣的小夥伴可以點擊瞭解：《&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzkyNzY0OTE5Ng%3D%3D%26mid%3D2247487345%26idx%3D1%26sn%3D8240962635c24ae6f065a483be88d8eb%26scene%3D21%23wechat_redirect" target="_blank"&gt;揭祕字節跳動內部流量調度與容災實踐【上】&lt;/a&gt;》。&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;本文為下期，主要介紹基於 TrafficRoute GTM 的 Perf-智能路由模式落地全智能、可觀測、可微調的流量調度，主要內容包括：&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;1.TrafficRoute GTM 介紹&lt;br&gt; 2.TrafficRoute GTM 的 Perf-智能路由關鍵技術&lt;br&gt; 3.字節跳動智能流量調度內部實踐&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;面臨超大規模流量時，平衡好穩定性、性能、容量、成本，能確保用戶在訪問服務時獲得流暢、快速且可靠的體驗，這對於提高用戶滿意度和粘性至關重要。TrafficRoute GTM 為業務提供基於 DNS 的全球流量負載均衡、智能調度、自動容災服務，可以幫助業務提升連續性，實現資源優化，獲取更多競爭優勢。&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;strong&gt;1.火山引擎 Trafficroute GTM 簡介&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;火山引擎 Trafficroute GTM 是基於 DNS 的流量路由服務。它依託全球 1100+ 分佈式探測節點及 IDC 質量數據等，構建出強大的網絡質量感知能力，實現了對「端-邊-雲」全鏈路流量的質量感知，從而根據 APP 應用的實時訪問質量、節點負載和健康狀況作出動態流量調度。&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;此外，Trafficroute GTM 還提供靈活的調度策略，其中 GEO-基礎，路由功能豐富，包括負載均衡、會話粘性（內部使用中，暫未對外開放）和故障轉移等多種特性。而 Perf-智能路由則在基礎路由的基礎上，進一步提供性能優先，容量優先和負載反饋等智能調度能力，以滿足更高層次的調度需求。&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//27f3f1197212e22ab45afebdef1b5d94.jpg" width="4000" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; 一圖看懂 TrafficRoute GTM&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;在字節跳動內部業務中，諸多業務基於 TrafficRoute GTM 的 Perf-智能路由，藉助 GTM 的全球網絡質量地圖、APP 全鏈路可用性、APP 實時負載等感知能力落地了全智能、可觀測、可微調的流量調度。&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;strong&gt;2.Perf-智能路由，實現流量智能調度&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;TrafficRoute GTM 的 Perf-智能路由旨在為邊緣計算、IoT 物聯網、多雲混合等大規模分佈式場景提供智能化的流量調度方案。&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;用戶無需人工進行流量編排，只需在 GTM 中輸入目標節點地址，GTM 即刻呈現最優的流量調度策略；同時，GTM 會根據全球網絡質量，目標節點健康狀況等動態的更新流量調度規則，真正地實現自動、智能的流量調度。&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//51866a5a42c1752611bb4008749e8321.jpg" width="4000" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; 憑藉以下關鍵技術，Perf-智能路由實現了更智能、更動態的流量調度。&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;2.1 感知中心&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;GTM 感知中心通過分佈於全球 1100+ 的節點實時採集：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;全球網絡鏈路質量，反映網絡鏈路的連通性/時延/抖動等&lt;/li&gt; 
 &lt;li&gt;目標資源健康情況，反映業務的資源節點當前健康程度&lt;/li&gt; 
 &lt;li&gt;目標資源實時負載，反映業務的資源節點當前工作負荷&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;這些數據經過預處理、轉換、分析後作為策略中心的決策依據。&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//755a66782dbc63a888425e5bdb229491.jpg" width="4000" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; 以感知中心生成的中國大陸的網絡質量地圖為例：&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//46181b2fb4b81bfa8d7342a946b61e71.jpg" width="1005" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; 該有向圖表達了 6 個省份-運營商之間的網絡質量，節點代表省份-運營商，邊表示節點之間的連通性&amp;amp;時延&amp;amp;抖動。&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;在實際應用中，GTM 的策略中心亦可根據業務需求，對該有向圖施加【成本系數、ISP 親和、GEO 親和】等約束，這些約束最終會影響到流量調度。&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;2.2 策略中心&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;策略中心根據感知中心上報的事件，利用實例設定的策略算法進行路由計算，進而生成動態的調度拓撲。Perf-智能路由主要有 3 種模式，分別面向對性能、容量、成本、穩定性等有不同訴求的業務場景。&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//3a18a1d64b5cf0c4990422b915b0ddde.jpg" width="3412" referrerpolicy="no-referrer"&gt; 
 &lt;br&gt; 
 &lt;br&gt; 性能優先 | Perf 
&lt;/div&gt; 
&lt;div&gt;
  &amp;nbsp; 
&lt;/div&gt; 
&lt;div&gt;
  適用於量級可控、資源容量充沛、追求極致性能的業務 
&lt;/div&gt; 
&lt;ul&gt; 
 &lt;li&gt;核心原理：據全球網絡質量，動態的將各地區的客戶端調度至其訪問最快的資源節點&lt;/li&gt; 
 &lt;li&gt;核心特色：以數據 （ 網絡質量 ） 驅動調度而非經驗，將流量調度變得更加智能、實時、精確&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//8829efb01af6d09d6e03a63deb50b9cc.jpg" width="1482" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; &lt;br&gt; 容量優先 | Perf-Cap&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;適用於量級中等，資源分佈不均，要求在資源約束下實現最高性能的業務&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;核心原理：根據全球網絡質量，在容量限制的前提下，動態地將各地區客戶端調度至其訪問最快的資源節點&lt;/li&gt; 
 &lt;li&gt;核心特色：在 Perf 性能優先的基礎上，引入資源節點容量的約束，能夠更加智能的實現容量，性能的平衡&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//fc0b6086dc2a8b0704f5e7b93fbe5d64.jpg" width="936" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; 負載，反饋 | Perf-Feedback&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;適用於量級波動大，資源分佈廣且不均，追求容量&amp;amp;性能&amp;amp;成本的平衡，尤其適合邊緣下沉場景&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;核心原理：根據全局和節點負載，動態的將流量在可用節點中分配，同時兼顧性能最優和容量安全&lt;/li&gt; 
 &lt;li&gt;核心特色：以最合理的資源成本，穩定支撐量級&amp;amp;波動大的業務，實現容量/性能/成本/穩定性的平衡&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//4f0e9bd329249ed0ebe8ec044f0a7a95.jpg" width="4000" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; Perf-Feedback 內置兩種調度傾向：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;當全局平均負載較低時，GTM 傾向於性能，將客戶端流量調度至其訪問最快的資源節點&lt;/li&gt; 
 &lt;li&gt;當全局平均負載較高時，GTM 傾向於穩定，確保每個節點的水位不高於全局平均負載水位&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;如下圖所示，相比於 Perf-Cap，GTM 的調度輸入中引入了實時負載的數據。&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//fbb51d8fe9e5aeec563326aafa3b54a5.jpg" width="936" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;Perf 自定義路由&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;適用於用戶需要對 Perf 智能路由流量進行微調，以滿足特定場景的業務&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;核心原理：自定義路由規則的優先級高於 Perf 智能路由生成的路由規則優先級&lt;/li&gt; 
 &lt;li&gt;核心特色：在智能化的同時也為業務方提供更多的靈活性，滿足特定業務需求&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//0f2575e23ad2499c9ac0ab47b58bb6d6.jpg" width="2850" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; 2.3 流量可視化&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;Perf-智能調度智慧透明，配備全面工具集，助力業務深入分析流量動態，通過 Perf-智能調度，可以觀測到實時流量拓撲、客戶端請求趨勢、客戶端地區分佈等流量動態。&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//725ec82c907b637a27e73d5ab3598418.jpg" width="2722" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; 實時流量拓撲&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//9fc1792f281616eeef95de1b8fdb01d2.jpg" width="1864" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; 客戶端請求趨勢&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//3da45538936e9772728e8057243a3548.jpg" width="1872" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; 客戶端地區分佈&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;strong&gt;3.字節跳動智能流量調度內部實踐&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;在字節跳動，越來越多的業務正通過邊緣計算將服務去中心化，從而實現更優的用戶體驗和更低的基建成本。面對邊緣節點分佈廣泛、數量龐大、能力參差不齊的挑戰，TrafficRoute GTM 的 Perf 智能路由展現出天然優勢。通過 Perf -智能路由的三種調度模式，幫助字節跳動內部多個業務落地了邊緣下沉，在成本、性能和穩定性上取得較大收益。&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;3.1 RTC 實時音頻，訪問時延降低 10%+&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;字節跳動某款 APP 的 RTC 實時音頻服務，在全國三個城市部署了 9 個接入節點。通過採用 TrafficRoute GTM 的 Perf 性能優先模式，確保全國的企業用戶在不同工作場所均能體驗到極低延遲的音頻接入服務，保障了通信的高效與流暢。&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//b46e6cf10f0195428eb729c7d2c6b570.jpg" width="1048" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//717de0c554e9a316da09e95cae8b0522.jpg" width="3752" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; GTM 感知中心實時感知全國網絡質量，智能地為不同地區客戶端動態制定調度規則，確保用戶始終連接到最健康、速度最快的音頻接入點，以優化通信體驗。整個應用過程中，GTM 的 Perf 性能優先模式充分發揮了獨特功能，涵蓋了智能動態調度策略、顯著降低了接入成本以及顯著提升了應用性能，展現出其卓越的技術優勢。&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//22ae0e72f459a8d877bbd5c14643deb5.jpg" width="2442" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; 採用 TrafficRoute GTM 的 Perf 性能優先模式，相比較 GEO 基礎路由，最終業務實現瞭如下收益：&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//0e9a9657ca6bffdeb34296935aadd2ec.jpg" width="4000" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;成本收益：智能調度代替了人工維護，每月降低了 3 人天以上；&lt;/li&gt; 
 &lt;li&gt;性能收益：訪問時延 avg 降低 10%+，p95 降低 25%；請求成功率 avg 提升 0.05%；&lt;/li&gt; 
 &lt;li&gt;穩定性收益：業務實現了分鐘級全鏈路自動容災，最快做到 3 分鐘全國 95%+ 流量收斂。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; 3.2 千萬 QPS 業務，成本降低 35%，性能提升 20%&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;在邊緣計算浪潮的推動下，能否有效駕馭大規模邊緣算力，成為業務邊緣下沉成功的關鍵。TrafficRoute GTM 深度參與了一個超 1500 萬 QPS 的業務邊緣下沉項目，通過使用 Perf-Cap 容量優先模式，助力其在字節內部率先落地端-邊-雲一體化的架構，成為先行者。&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//e7deae9af5c6d09fccc0c1904c31f291.jpg" width="3132" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; 通過將中心 Region 數據面服務下沉至全國 30+ 省份、50+ 邊緣節點，來實現提升用戶訪問體驗 (邊緣節點距離終端客戶端更近) 和降低帶寬&amp;amp;算力成本 (邊緣資源成本約為中心的 20%~60%)。GTM 的 P erf-Cap 容量優先模式，根據業務的客戶端請求分佈、全國網絡質量地圖 ， 在滿足各邊緣節點容量約束的前提下，生成全局總時延最低的流量調度規則。&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//ad93894a8582bd02fd6e4d9c63309094.jpg" width="2596" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; GTM 上實際配置如下圖：&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//433b27fc81232ee1a9ec89b390e0f5ed.jpg" width="1532" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; 此時，用戶無需繁瑣的容量規劃、節點統籌、流量調度，只需在 console 上填入邊緣節點的元信息 (IP 地址+容量)，GTM 即刻生成&lt;em&gt;智能&lt;/em&gt;、&lt;em&gt;動態&lt;/em&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;em&gt;的調度&lt;/em&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;*， *&lt;em&gt;時刻保證最終客戶的體驗最優。&lt;/em&gt;&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;通過抖音客戶端 AB 數據分析，該業務邊緣下沉帶來的整體收益如下：&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//d60771c32ba3fa11f057436c4149292f.jpg" width="2074" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; 其中，邊緣下沉 x GTM Perf-Cap 模式，額外取得的收益如下：&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//c2dd755e546b20a5daee185333db42d1.jpg" width="2092" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; 3.3 302 服務，端上播放質量顯著提升&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;字節跳動 302 服務承擔了抖音、頭條、西瓜等 APP 點播&amp;amp;下載的重定向功能，其流量呈現明顯的波峯波谷特徵，日內 QPS 在 30-350 萬範圍波動。&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;為實現最優訪問性能和最低基建成本，要求 TrafficRoute GTM 將動態波動的流量在最小資源冗餘的火山引擎邊緣節點上合理調度，既要保證性能全局最優，又要保證全局水位健康。&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//4fcd16dcc51697e3731fb733c9e6a59c.jpg" width="4000" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; 採用 Perf-Feedback 負載反饋模式，302 服務實現瞭如下收益：&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//69cfbed876af215db19663285d846b05.jpg" width="4000" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;ul&gt; 
 &lt;li&gt;100+ 邊緣節點的負載更加可控，資源利用率更加合理，節點負載跑超率從 20% 降至 0%；&lt;/li&gt; 
 &lt;li&gt;TCP 建聯失敗率下降明顯：晚高峯 19%-&amp;gt;16.5% ，午高峯 18%-&amp;gt; 14%；&lt;/li&gt; 
 &lt;li&gt;客戶端 7 層負面指標均下降 ： 其中播放 error 錯誤率、播放 play_break 中斷率降幅超 50%。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; END&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;Trafficroute GTM 通過 Perf -智能路由的三種調度模式，幫助字節跳動內部 RTC 實時音頻業務、千萬 QPS 業務、302 服務實現了在成本、性能和穩定性上的收益，進一步助力字節跳動內部業務經受超大規模流量考驗，確保始終為用戶提供穩定服務。&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;最後，給大家預告番外篇，後續我們將聚焦更新的 GTM 調度功能，詳細闡述技術思路、關鍵技術和實踐經驗，感興趣的小夥伴記得持續關注~&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/6800876/blog/18684814</link>
      <guid isPermaLink="false">https://my.oschina.net/u/6800876/blog/18684814</guid>
      <pubDate>Sat, 02 Aug 2025 09:24:00 GMT</pubDate>
      <author>原創</author>
    </item>
    <item>
      <title>國產開源推理引擎「赤兔」發佈 v0.4 版本</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;國產開源推理引擎「赤兔」&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fthu-pacman%2Fchitu%2Freleases%2Ftag%2Fv0.4.0" target="_blank"&gt;發佈了 v0.4 版本&lt;/a&gt;，&lt;strong&gt;大幅提升了一體機推理部署場景的性能和穩定性，適配昇騰、英偉達、沐曦、海光，支持 DeepSeek、Qwen、GLM、Kimi 等模型&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;Chitu（赤兔）是由清華系&amp;nbsp;AI Infra 明星創企——清程極智聯合清華大學團隊發佈的開源項目。赤兔定位於「生產級大模型推理引擎」，充分考慮企業 AI 落地從小規模試驗到大規模部署的漸進式需求，專注於提供以下重要特性：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;多元算力適配：不僅支持 NVIDIA 最新旗艦到舊款的多系列產品，也為國產芯片提供優化支持。&lt;/li&gt; 
 &lt;li&gt;全場景可伸縮：從純 CPU 部署、單 GPU 部署到大規模集羣部署，赤兔引擎提供可擴展的解決方案。&lt;/li&gt; 
 &lt;li&gt;長期穩定運行：可應用於實際生產環境，穩定性足以承載併發業務流量。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;在今年三月首個版本發佈時，赤兔通過底層算子優化（如 GeMM、MoE 的指令級重構）和編譯技術創新，首次實現在無 FP8 硬件單元的算力芯片上原生運行 FP8 高精度模型，賦能眾多存量算力芯片推理 DeepSeek-R1 滿血版大模型。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364113/chitu-0-4</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364113/chitu-0-4</guid>
      <pubDate>Sat, 02 Aug 2025 09:21:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>B 站上線「AI 原聲翻譯功能」，將加入日語等語言</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p style="color:#2b2b2b; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;B 站今年 5 月下架國際版 App，與國內版合併為一個統一 App。為解決海內外內容互通問題，B 站現公佈一項自研的「AI 原聲翻譯功能」，號稱可以幫助海外用戶更好體驗遊戲、科技、二次元等主推內容。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#2b2b2b; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;據 B 站介紹，&lt;strong&gt;目前相應功能已向海外用戶開放，暫僅支持英語&lt;/strong&gt;，主要提供畫面和音頻兩大翻譯能力，在畫面方面支持自動擦除原中文字幕改為英文、自動翻譯彈幕、各類按鈕語言。在音頻方面號稱可以還原 UP 主的聲線、音色、氣口，而非傳統的機器音翻譯。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#2b2b2b; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;img height="259" src="https://oscimg.oschina.net/oscnet/up-a4c2bba74fb87025c787b7a83000654e41b.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2b2b2b; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;B 站表示，相應翻譯功能的技術難點&lt;strong&gt;在於遊戲、二次元等專有名詞梗的密集領域「如何實現原風格精準保留與語音時長完美對應」&lt;/strong&gt;。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#2b2b2b; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;為此，相應技術團隊基於大語言模型（LLM）構建翻譯引擎，採用對抗式強化學習（RL）訓練驅動模型；並引入 Deep Research 深度挖掘技術，專攻專有名詞與流行梗點的翻譯難點，確保最終譯文準確傳神。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#2b2b2b; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;後續，B 站還將視需求為「AI 原聲翻譯」功能新增日語等更多語言，持續擴展在海外市場的適配能力。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364111</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364111</guid>
      <pubDate>Sat, 02 Aug 2025 09:13:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>高德發佈全球首個地圖 AI 原生智能體</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;&lt;span style="color:#000000"&gt;高德地圖正式&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F4qsBRg16CnO1ayKe6sHiFA" target="_blank"&gt;宣佈&lt;/a&gt;其全面 AI 化，結合前沿的空間智能技術，推出了全球首個 AI 原生地圖應用 —— 高德地圖 2025。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;公告稱，高德地圖 2025 旨在打造具備深度時空理解和自主推理決策能力的一體化出行生活智能體，以及 AI 領航、AI 即刻、AI 探索、AR 打卡等創新場景工具，為用戶提煉一個更加符合習慣和喜好的個性化數字孿生世界，基於空間智能架構解決一切出行生活需求。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="228" src="https://oscimg.oschina.net/oscnet/up-f16397c20425bb4922c0dbd519e0b8bed5e.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;「空間智能是在三維空間和時間中感知、推理、和行動的能力，能夠讓地圖實現被動感知到主動預判的跨越。」高德地圖 CEO 郭寧表示，希望從高德地圖 2025 開始，推動 AI 從 「對話工具」蛻變為「行動夥伴」。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;即日起用戶升級高德地圖 APP 至最新版，搜索「空間智能」，即可體驗。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#3e3e3e; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;根據介紹，用戶可與高德主智能體「小高老師」的語音交流。該語音技術以自然語言交互為核心，通過全雙工語音技術實現流暢交流，支持用戶隨時打斷指令、動態調整規劃；內置的回聲消除算法與異常語義拒識模型，可智能去噪以精準聆聽用戶聲音；而情感計算模塊賦予對話溫度，不僅提供清晰的路線指引，更能在旅途中提供情緒陪伴。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#3e3e3e; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;語音感知之後，即進入思考決策環節。基於高德與通義深度共建的大模型簇，小高老師能夠進行基於空間智能的推理、計劃、反思和行動，並通過 MCP 協同調用出行服務、生活服務、空間服務等子智能體和工具鏈，整合內外部知識庫來制定最優方案。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364108</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364108</guid>
      <pubDate>Sat, 02 Aug 2025 09:01:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>「問小白」發佈第四代開源大模型 XBai o4，擅長複雜推理</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;「問小白」發佈了第四代開源大模型&lt;strong&gt;XBai o4&lt;/strong&gt;（其中「o」代表「open」），該模型在複雜推理能力方面表現出色，在 Medium 模式下已全面超越&lt;strong&gt;OpenAI-o3-mini&lt;/strong&gt;，並在部分基準測試中優於&lt;strong&gt;Anthropic Claude Opus&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0804/163409_xSmx_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;XBai o4 基於創新的&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FRxWjzZe5WWVGKKOk2JbfJQ" target="_blank"&gt;「反思型生成範式」&lt;strong&gt;（reflective generative form）&lt;/strong&gt;&lt;/a&gt;，融合了 Long-CoT 強化學習&lt;strong&gt;與&lt;/strong&gt;過程評分學習（Process Reward Learning），使單個模型同時具備深度推理和高質量推理鏈路篩選的能力。通過共享過程評分模型（PRMs）和策略模型的主幹網絡，XBai o4 顯著降低了 99% 的過程評分推理耗時。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-8840d76d6357daa77dda0a67b9e0727ff4c.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;該模型提供三種模式（low、medium、high），在多個基準測試（如 AIME24、AIME25、LiveCodeBench v5、C-EVAL 等）中均展現出強大性能，相關訓練和評估代碼已在 GitHub 開源。&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FMetaStone-AI%2FXBai-o4" target="_blank"&gt;https://github.com/MetaStone-AI/XBai-o4&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364101</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364101</guid>
      <pubDate>Sat, 02 Aug 2025 08:35:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>OpenAI CEO 首次公開 GPT-5 對話界面</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;OpenAI CEO Sam Altman 全網首次公開了 GPT-5 的對話界面，揭示這款尚未發佈的大模型在真實使用場景中的表現。&lt;/p&gt; 
&lt;p&gt;截圖顯示，Altman 向 GPT-5 提問：「什麼是最發人深省的 AI 題材電視劇」。GPT-5 隨即給出了一份詳細推薦清單，內容不僅涵蓋劇集名稱、播出平台和評分，還深入介紹了每部劇集的核心議題與哲學內核。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-2bf8d8ceeab9104926e943d4d197b5de237.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;有趣的是，網友的高贊評論卻是吐槽 ChatGPT 的標點使用習慣：「請永久刪除 ChatGPT 中的破折號。」&lt;/p&gt; 
&lt;p&gt;後續，Altman 還表示「很快進入 SaaS 的快時尚時代」，疑似暗示 GPT-5 不僅是一次模型升級，更可能重塑傳統 SaaS 產品的形態與節奏。&lt;/p&gt; 
&lt;p&gt;&lt;img height="290" src="https://static.oschina.net/uploads/space/2025/0804/162106_XXoY_2720166.png" width="1212" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0804/162136_6vpA_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364098</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364098</guid>
      <pubDate>Sat, 02 Aug 2025 08:21:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
  </channel>
</rss>
