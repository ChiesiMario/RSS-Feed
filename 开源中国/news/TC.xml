<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - news - 繁體中文（台灣）</title>
    <link>https://www.oschina.net/news/project</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news" rel="self" type="application/rss+xml"/>
    <description>已對該 RSS 進行格式化操作：中英字符之間插入空格、使用直角引號、標點符號修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-tw</language>
    <lastBuildDate>Mon, 01 Sep 2025 07:44:40 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>Chrome 市佔率突破 70%</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;根據 Statcounter 最新數據，截至 2025 年 8 月，Google Chrome 在桌面瀏覽器市場的份額達到了 &lt;strong&gt;70.25%。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-01c74261cc2dbfb7c696b2470d49ab94115.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-c2f7a0ea50348f5fb173220fa35d6164e8d.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;下面是各主要瀏覽器市場佔比變化&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Microsoft Edge&lt;/strong&gt;：穩居第二，但市場份額僅為 &lt;strong&gt;11.8%&lt;/strong&gt;，比上月略增 0.01 個百分點&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Safari&lt;/strong&gt;：雖為第三，但僅佔 &lt;strong&gt;6.34%&lt;/strong&gt;，月增幅高達 1.04 個百分點&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Firefox&lt;/strong&gt;：市場佔比為 &lt;strong&gt;4.94%&lt;/strong&gt;，環比下降 0.36 個百分點&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Opera&lt;/strong&gt;：排名第五，僅佔 &lt;strong&gt;2.06%&lt;/strong&gt;，環比減少 0.13 個百分點&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;整體來看，Chrome 的市場份額持續上升，而其他競爭瀏覽器則表現相對疲軟，其市場份額普遍有所下滑，尤其是 Firefox 和 Opera。Safari 略有回升，但仍遠遠落後於 Chrome 和 Edge。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369661/chrome-increases-its-overwhelming-market-share-now-over-70</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369661/chrome-increases-its-overwhelming-market-share-now-over-70</guid>
      <pubDate>Mon, 01 Sep 2025 07:32:56 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>螞蟻開源醫學智能體 MedResearcher-R1</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;螞蟻集團聯合研究團隊近日開源發佈了針對醫療領域的知識驅動軌跡合成框架 MedResearcher-R1。旨在解決領域特定 AI 推理的挑戰，通過智能化的數據生成和合成，為醫療研究提供支持。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;MedResearcher-R1 包含三個集成的核心模塊，分別是知識圖譜構建、軌跡生成管道和評估管道。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img alt="" height="455" src="https://oscimg.oschina.net/oscnet/up-aec107f3f7a8b85da21f5a8356da520584b.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;知識圖譜構建模塊是該框架的核心創新。該模塊能夠將領域知識轉化為高質量的問答對，藉助自動推理路徑生成，構建出完整的知識圖譜。此外，系統還提供了交互式網絡可視化，用戶可以通過 D3.js 力導向圖來直觀展示知識圖譜結構。先進的採樣算法和統一的問答生成方法，使得複雜的子圖提取與多種形式的問題合成得以實現。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;接下來是軌跡生成管道。該模塊實現了多輪推理與工具集成的自動化處理，可以將問答對轉換為多輪推理軌跡，並進行質量過濾。通過高效的質量過濾機制，系統能夠檢測到錯誤並進行自動修正，確保生成內容的準確性。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;評估管道為模型的推理性能提供了全面的評估與驗證框架。它不僅支持單問題模式的詳細過程可視化，還可以進行批量數據集評估，提高評估效率。通過這些模塊，MedResearcher-R1 提供了一整套從知識提取到模型訓練數據生成和評估的解決方案，推動醫療領域專用推理模型的開發。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;該框架還開源了由知識圖譜構建模塊生成的高質量問答數據集，包含複雜推理問答對和詳細的推理路徑，為研究者提供了寶貴的資源。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369660</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369660</guid>
      <pubDate>Mon, 01 Sep 2025 07:28:56 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>UltraRAG 2.0 發佈，基於 MCP 架構設計的開源 RAG 框架</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;清華大學 THUNLP 實驗室、東北大學 NEUIR 實驗室、OpenBMB 與 AI9Stars 聯合推出 UltraRAG 2.0 （UR-2.0），這是首個基於 Model Context Protocol (MCP) 架構設計的開源 RAG 框架。&lt;/p&gt; 
&lt;p&gt;UltraRAG 2.0 框架示意圖：&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-e725bd56a6a1b4d875b21297cdeaea9e95d.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;這一設計讓科研人員只需編寫 YAML 文件，就可以直接聲明串行、循環、條件分支等複雜邏輯，從而以極低的代碼量快速實現多階段推理系統。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;UltraRAG 2.0 亮點一覽&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;組件化封裝：將 RAG 的核心組件封裝為標準化的獨立 MCP Server；&lt;/li&gt; 
 &lt;li&gt;靈活調用與擴展：提供函數級 Tool 接口，支持功能的靈活調用與擴展；&lt;/li&gt; 
 &lt;li&gt;輕量流程編排：藉助 MCP Client，建立自上而下的簡潔化鏈路搭建；與傳統框架相比，UltraRAG 2.0 顯著降低了複雜 RAG 系統的技術門檻與學習成本，讓研究者能夠將更多精力投入到實驗設計與算法創新上，而不是陷入冗長的工程實現。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;➤&amp;nbsp;&amp;nbsp;相關鏈接&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Github 鏈接：&lt;/strong&gt;&lt;em&gt;https://github.com/OpenBMB/UltraRAG&lt;/em&gt;&lt;br&gt; &lt;strong&gt;項目主頁&lt;/strong&gt;&lt;em&gt;&lt;strong&gt;：&lt;/strong&gt;https://openbmb.github.io/UltraRAG/&lt;/em&gt;&lt;br&gt; &lt;strong&gt;教程文檔&lt;/strong&gt;&lt;em&gt;&lt;strong&gt;：&lt;/strong&gt;https://ultrarag.openbmb.cn/&lt;/em&gt;&lt;br&gt; &lt;strong&gt;開源數據集：&lt;/strong&gt;&lt;em&gt;https://huggingface.co/datasets/UltraRAG/UltraRAG_Benchmark&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369659</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369659</guid>
      <pubDate>Mon, 01 Sep 2025 07:21:05 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>騰訊 ARC 實驗室發佈 AudioStory 音頻生成技術</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;騰訊 ARC 實驗室發佈了 AudioStory 音頻生成技術，實現複雜敍事場景的好萊塢級音效一鍵生成，可處理視頻配音、音頻續寫和長篇敍事音頻。 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;AudioStory 將大語言模型與文本‑音頻系統結合，能夠把複雜的敍事請求拆分為有順序的子任務，保證場景轉換和情感基調的一致性。 &amp;nbsp;它採用「解耦橋接機制」來分別處理事件內部語義對齊與跨事件一致性，並通過端到端訓練提升理解和生成的協同。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-be8db71e777c33e2c8aacdb79b9d3982b1a.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;用戶通過自然語言描述（如「懸疑追逐戰：腳步濺水，雷聲轟鳴」），系統即可自動分解事件序列，結合大語言模型與文生音頻技術，生成具有時序邏輯與情緒層次的高質量音頻。&lt;/p&gt; 
&lt;p&gt;AudioStory 核心技術突破在於採用語義令牌與殘差令牌雙通道機制，精準協調宏觀敍事與微觀音效細節，並通過三階段漸進訓練解決長音頻連貫性問題。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-9239d4f9913f6c1dcebed99c4ff48ace5f9.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;據瞭解，AudioStory 技術已應用於視頻自動配音、音頻智能續寫等場景，在萬級測試集 AudioStory-10K 中展現領先的指令遵循力與一致性，為有聲書、遊戲音效等領域提供全新創作工具。&lt;/p&gt; 
&lt;p&gt;開源地址：&lt;em&gt;https://github.com/TencentARC/AudioStory&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369654</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369654</guid>
      <pubDate>Mon, 01 Sep 2025 07:09:56 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>亞馬遜 AWS 旗下 AI 編程工具 Kiro 繼續免費至 9 月 15 日</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;AWS 推出的 AI 編程 IDE 工具 Kiro &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fkiro.dev%2Fblog%2Ffree-until-september-15%2F" target="_blank"&gt;宣佈&lt;/a&gt;，鑑於近期定價調整，公司將原定於 9 月 1 日結束的免費使用政策延長至 &lt;strong&gt;9 月 15 日&lt;/strong&gt;。在此期間，所有支付了訂閲費用的用戶都將獲得全額退款，讓用戶可以自由使用至 9 月 15 日。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0901/145841_xWsX_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;與此同時，Kiro 用戶的計劃使用額度已於 &lt;strong&gt;9 月 1 日&lt;/strong&gt;恢復為正常訂閲限額，用戶還可免費啓用 &lt;strong&gt;overages 超額請求&lt;/strong&gt;功能，額外獲贈 1,000 次 vibe 請求與 200 次 spec 請求，且相關費用將被退還。&lt;/p&gt; 
&lt;p&gt;官方強調，所有退款已陸續處理，使用額度也已提前重置，以保障用戶可暢快使用工具。&lt;/p&gt; 
&lt;hr&gt; 
&lt;h3&gt;重要 FAQ 概覽&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;問題&lt;/th&gt; 
   &lt;th&gt;澄清説明&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;用戶何時被收費？何時退款？&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;系統一旦在 9 月 1 日扣款，將於 &lt;strong&gt;9 月 15 日前&lt;/strong&gt;全額退還。&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;超額使用費用是否退款？&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;已於 8 月產生的超額費用將被退還；9 月期間啓用的 overages，如產生費用，同樣會在月底退回。&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;現在訂閲是否算免費？&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;若在 &lt;strong&gt;9 月 14 日之前&lt;/strong&gt;購買訂閲，會立即享受免費使用至 9 月 14 日，並獲得 &lt;strong&gt;9 月費用退款&lt;/strong&gt;。&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;參加 hackathon 的用戶影響如何？&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;參與 9 月 15 日截止的 Kiro hackathon 的開發者，可在截至日前免費使用 Kiro，享有相同退款與使用政策。&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;官方也表示，他們正在聽取用戶反饋，並計劃在 9 月 15 日之後推出更新後的定價方案，屆時將提前通知用戶。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369652</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369652</guid>
      <pubDate>Mon, 01 Sep 2025 06:59:56 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Dante Cloud v3.5.5.2 發佈，企業級技術中台微服務架構</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Dante Cloud v3.5.5.2 已經發布，企業級技術中台微服務架構。&lt;/p&gt; 
&lt;p&gt;♻️ refactor: v3.5.5.2&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;主要更新 
  &lt;ul&gt; 
   &lt;li&gt;[重構] 採用新的 protobuf maven 插件替換原有老舊版本插件，以支持使用最新版 protobuf 4.X 編譯生成 grpc 代碼&lt;/li&gt; 
   &lt;li&gt;[優化] 自定義頁面涉及第三方 js 組件全部替換為 webjars 方式&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;其它更新 
  &lt;ul&gt; 
   &lt;li&gt;[修復] 修復在微服務架構下，當用戶沒有接口權限時，返回的錯誤信息不準確問題&lt;/li&gt; 
   &lt;li&gt;[修復] 修復系統內引用 jquery webjars maven 座標已過時問題&lt;/li&gt; 
   &lt;li&gt;[修復] 修復前端頁面登錄成功後頁面控制枱頁面不顯示問題&lt;/li&gt; 
   &lt;li&gt;[修復] 修復應用合規數據存儲代碼字段長度不足，無法存儲 ipv6 地址問題 fix: #ICVJYM&lt;/li&gt; 
   &lt;li&gt;[優化] 優化自定義授權碼授權模式登錄頁面&lt;/li&gt; 
   &lt;li&gt;[優化] 自定義內嵌授權碼授權模式，登錄頁面，補充缺失的 bootstrap 引用&lt;/li&gt; 
   &lt;li&gt;[優化] 服務 Docker 使用的 liberica 鏡像由 liberica-openjdk-debian 替換為 liberica-openjre-debian，以減少生成 Docker 鏡像的大小&lt;/li&gt; 
   &lt;li&gt;[升級] ip 位置數據庫更新至 2025-08-27&lt;/li&gt; 
   &lt;li&gt;[升級] Nodejs 版本升級至 22.19.0&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;依賴更新 
  &lt;ul&gt; 
   &lt;li&gt;[升級] alipay-sdk-java 版本升級至 4.40.427.ALL&lt;/li&gt; 
   &lt;li&gt;[升級] loki-protobuf 版本升級至 0.0.2_pb4.31.0&lt;/li&gt; 
   &lt;li&gt;[升級] mybatis-plus 版本升級至 3.5.14&lt;/li&gt; 
   &lt;li&gt;[升級] protobuf 版本升級至 4.32.0&lt;/li&gt; 
   &lt;li&gt;[升級] software.amazon.awssdk 版本升級至 2.33.0&lt;/li&gt; 
   &lt;li&gt;[升級] software.amazon.awssdk.crt 版本升級至 0.38.11&lt;/li&gt; 
   &lt;li&gt;[升級] wxjava 版本升級至 4.7.7-20250831.214433&lt;/li&gt; 
   &lt;li&gt;[升級] checker-qual 版本升級至 3.50.0&lt;/li&gt; 
   &lt;li&gt;[升級] hutool 5.X 版升級至 5.8.40&lt;/li&gt; 
   &lt;li&gt;[升級] zookeeper 版本升級至 3.9.4&lt;/li&gt; 
   &lt;li&gt;[升級] bootstrap webjars 版本升級至 5.3.8&lt;/li&gt; 
   &lt;li&gt;[升級] inter-ui webjars 版本升級至 4.1.1&lt;/li&gt; 
   &lt;li&gt;[升級] sweetalert2 webjars 版本升級至 11.22.5&lt;/li&gt; 
   &lt;li&gt;[升級] @tabler/core webjars 版本升級至 1.4.0&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;詳情查看：&lt;a href="https://gitee.com/dromara/dante-cloud/releases/v3.5.5.2"&gt;https://gitee.com/dromara/dante-cloud/releases/v3.5.5.2&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369647</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369647</guid>
      <pubDate>Mon, 01 Sep 2025 06:50:05 GMT</pubDate>
      <author>來源: 資訊</author>
    </item>
    <item>
      <title>Dante OSS v3.5.5.2 發佈，簡化 Minio 操作</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Dante OSS v3.5.5.2 已經發布，簡化 Minio 操作。&lt;/p&gt; 
&lt;p&gt;♻️ refactor: v3.5.5.2&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;主要更新 
  &lt;ul&gt; 
   &lt;li&gt;[重構] 採用新的 protobuf maven 插件替換原有老舊版本插件，以支持使用最新版 protobuf 4.X 編譯生成 grpc 代碼&lt;/li&gt; 
   &lt;li&gt;[優化] 自定義頁面涉及第三方 js 組件全部替換為 webjars 方式&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;其它更新 
  &lt;ul&gt; 
   &lt;li&gt;[修復] 修復在微服務架構下，當用戶沒有接口權限時，返回的錯誤信息不準確問題&lt;/li&gt; 
   &lt;li&gt;[修復] 修復系統內引用 jquery webjars maven 座標已過時問題&lt;/li&gt; 
   &lt;li&gt;[修復] 修復前端頁面登錄成功後頁面控制枱頁面不顯示問題&lt;/li&gt; 
   &lt;li&gt;[修復] 修復應用合規數據存儲代碼字段長度不足，無法存儲 ipv6 地址問題 fix: #ICVJYM&lt;/li&gt; 
   &lt;li&gt;[優化] 優化自定義授權碼授權模式登錄頁面&lt;/li&gt; 
   &lt;li&gt;[優化] 自定義內嵌授權碼授權模式，登錄頁面，補充缺失的 bootstrap 引用&lt;/li&gt; 
   &lt;li&gt;[優化] 服務 Docker 使用的 liberica 鏡像由 liberica-openjdk-debian 替換為 liberica-openjre-debian，以減少生成 Docker 鏡像的大小&lt;/li&gt; 
   &lt;li&gt;[升級] ip 位置數據庫更新至 2025-08-27&lt;/li&gt; 
   &lt;li&gt;[升級] Nodejs 版本升級至 22.19.0&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;依賴更新 
  &lt;ul&gt; 
   &lt;li&gt;[升級] alipay-sdk-java 版本升級至 4.40.427.ALL&lt;/li&gt; 
   &lt;li&gt;[升級] loki-protobuf 版本升級至 0.0.2_pb4.31.0&lt;/li&gt; 
   &lt;li&gt;[升級] mybatis-plus 版本升級至 3.5.14&lt;/li&gt; 
   &lt;li&gt;[升級] protobuf 版本升級至 4.32.0&lt;/li&gt; 
   &lt;li&gt;[升級] software.amazon.awssdk 版本升級至 2.33.0&lt;/li&gt; 
   &lt;li&gt;[升級] software.amazon.awssdk.crt 版本升級至 0.38.11&lt;/li&gt; 
   &lt;li&gt;[升級] wxjava 版本升級至 4.7.7-20250831.214433&lt;/li&gt; 
   &lt;li&gt;[升級] checker-qual 版本升級至 3.50.0&lt;/li&gt; 
   &lt;li&gt;[升級] hutool 5.X 版升級至 5.8.40&lt;/li&gt; 
   &lt;li&gt;[升級] zookeeper 版本升級至 3.9.4&lt;/li&gt; 
   &lt;li&gt;[升級] bootstrap webjars 版本升級至 5.3.8&lt;/li&gt; 
   &lt;li&gt;[升級] inter-ui webjars 版本升級至 4.1.1&lt;/li&gt; 
   &lt;li&gt;[升級] sweetalert2 webjars 版本升級至 11.22.5&lt;/li&gt; 
   &lt;li&gt;[升級] @tabler/core webjars 版本升級至 1.4.0&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;詳情查看：&lt;a href="https://gitee.com/herodotus/dante-oss/releases/v3.5.5.2"&gt;https://gitee.com/herodotus/dante-oss/releases/v3.5.5.2&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369644</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369644</guid>
      <pubDate>Mon, 01 Sep 2025 06:47:05 GMT</pubDate>
      <author>來源: 資訊</author>
    </item>
    <item>
      <title>韓國 2026 財年預算總額創新高，AI 成發力重點</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;據環球網援引韓聯社報道，韓國政府 8 月 29 日召開國務會議審議並通過 2026 財年預算案。預算總支出規模達 728 萬億韓元（1000 韓元約合人民幣 5.1 元），較今年增長 8.1%，遠超今年 2.5% 的增幅，總額創歷年新高。這是李在明政府編制的首份預算，標誌着韓國財政政策正式從前政府時期的「緊縮」轉向「擴張」。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;報道稱，面對該國經濟結構性動力不足的危機，韓國政府將預算重點投向拉動增長的人工智能（AI）和研發領域：研發預算由 29.6 萬億韓元提高至 35.3 萬億韓元，增幅 19.3%，為歷年最大，以加速 AI、生物、文創內容、軍工、能源、製造六大關鍵領域的創新。其中，AI 預算由 3.3 萬億韓元增至 10.1 萬億韓元，增幅逾兩倍。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;韓媒稱，為應對美國政府對國防軍費增額的壓力，韓國防預算由 61.25 萬億韓元增至 66.3 萬億韓元，重點用於改善部隊官兵福利，以及新一代隱形戰機、AI、無人機、機器人等尖端武器研發。據《韓國日報》報道，這是韓國國防預算單年首次增逾 5 萬億韓元，增長率（8.2%）高於總支出，達到 2008 年以來的最高水平。若獲國會通過，國防開支佔國內生產總值（GDP）比重將增至 2.42%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;此外，產業政策支出擴大 14.7% 至 32.3 萬億韓元，以支持受關稅影響的出口商。文化產業支出也將增長 8.8% 至 9.6 萬億韓元。衞生、福利和就業支出達 269.1 萬億韓元。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;為籌措財源，政府將砍掉約 1300 個項目，合計騰挪 27 萬億韓元。即便如此，大部分新增支出仍需依賴大規模舉債，國家債務將突破 1400 萬億韓元，佔 GDP 的 51.6%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;預算案公佈後，韓國國內迅速引發熱議。質疑與批評主要集中在籌資能力與財政可持續性上。《朝鮮日報》社論警告，國債未來 4 年內或增至 1789 萬億韓元，大規模發債可能推高利率、抬升企業與家庭融資成本，並對國家信用評級構成壓力。《韓民族日報》援引高麗大學教授金泰逸的觀點稱，可以理解政府希望讓財政發揮積極作用，但從財政可持續性看「仍有不少不足」，且「看不到對財政支出績效管理與稅基擴充的努力，這一點令人擔憂」。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;韓國《京鄉新聞》社論則強調，健全財政固然重要，財政狀況良好方能增強經濟韌性、抵禦意外衝擊，但健全財政本身不能成為目的。在過去 3 年財政緊縮與富人減稅政策背景下，韓國經濟停滯、民生受損，新政府的擴張預算應以創新增長引導經濟進入良性循環。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;韓國政府表示，將通過擴張性財政刺激經濟，帶動稅收回升，實現「以財養財」的良性循環。韓國經濟副總理兼企劃財政部長官具潤哲稱，新政府上任即面臨「經濟萎縮、民生凍結」的重大課題，財政預算必須發揮「引水」作用，引導經濟恢復成長。據報道，該預算案將在 9 月初提交國會審議，預計 12 月最終確定。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369642</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369642</guid>
      <pubDate>Mon, 01 Sep 2025 06:43:56 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>字節跳動開源 USO，支持統一風格與主體定製的圖像生成模型</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;字節跳動團隊近日發佈並開源&lt;strong&gt;USO（Unified Style-Subject Optimized）&lt;/strong&gt;模型，這是一個「統一風格-主體」定製生成框架，首次把「風格驅動」與「主體驅動」兩類原本對立的圖像生成任務合併到單一模型裏，並在這兩個維度上都達到了開源領域的最佳水平（SOTA）。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1844" src="https://static.oschina.net/uploads/space/2025/0901/143002_VA7Q_2720166.png" width="1440" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;USO&lt;span style="background-color:#ffffff; color:#000000"&gt;通過解耦內容與風格特徵並引入獎勵學習機制，首次實現了風格驅動與主體驅動生成任務的統一框架。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="background-color:#ffffff; color:#000000"&gt;現有方法通常將風格相似性與主體一致性視為對立目標，而 USO 通過構建包含 20 萬組三元組數據（風格參考圖、去風格化主體圖、風格化結果圖）的訓練集，提出跨任務協同解耦範式：利用主體生成模型生成高質量風格化數據，再通過風格獎勵引導的解耦訓練優化主體模型。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-c3f05e9b2f38db1ea199c3afc32dbbedbd8.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="background-color:#ffffff; color:#000000"&gt;技術上採用 SigLIP 多尺度特徵投影實現風格對齊訓練，並通過內容-風格解耦編碼器分離條件特徵，最終結合風格獎勵學習（SRL）進一步提升解耦效果。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;USO 代碼與權重已在 Hugging Face 與 GitHub 公開，並配套在線 Demo 與一鍵安裝腳本。&lt;/p&gt; 
&lt;p&gt;https://huggingface.co/bytedance-research/USO&lt;br&gt; https://huggingface.co/spaces/bytedance-research/USO&lt;br&gt; https://github.com/bytedance/USO&lt;br&gt; https://huggingface.co/papers/2508.18966&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369639</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369639</guid>
      <pubDate>Sun, 31 Aug 2025 06:36:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>中國企業調用大模型日均超 10 萬億 Tokens</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#262626; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;9 月 1 日，國際市場調研機構沙利文（Frost&amp;amp;Sullivan）發佈了最新的《中國 GenAI 市場洞察：企業級大模型調用全景研究，2025》，報告顯示，2025 年上半年，中國企業級市場大模型的日均總消耗量為 10.2 萬億 Tokens，其中，阿里通義佔比 17.7% 位列第一，成為目前中國企業選擇最多的大模型。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#262626; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;img height="349" src="https://oscimg.oschina.net/oscnet/up-ec7a4444a4499da57f2b48d55c388a3bca3.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#262626; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;&lt;em&gt;《&lt;/em&gt;&lt;em&gt;中國 GenAI 市場洞察：企業級大模型調用全景研究，2025》發佈&lt;/em&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#262626; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;沙利文調研國內 700 家企業，領域橫跨金融、製造、互聯網、消費電子、汽車等多個重點行業， 覆蓋不同營收層級和 AI 投入規模的企業，以全面反映中國企業大模型真實使用現狀。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#262626; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;報告顯示，中國大模型企業級市場呈爆發式增長：較 2024 年下半年，2025 年上半年日均調用量暴增 363%，已逾 10 萬億 tokens；其中，阿里通義佔比 17.7%，字節豆包占比 14.1%，DeepSeek 佔比 10.3%，前三名合計佔比超 40%。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#262626; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;公有云上使用大模型成為主流。沙利文報告顯示，七成企業選擇公有云部署或調用大模型，71% 企業還表示未來將增加公有云形態的生成式 AI 服務。報告進一步指出，中國企業正從「追求單⼀最強模型」，轉向「為特定業務場景尋求最優解」，對不同的模態、尺寸和落地場景匹配的需求將進一步爆發。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#262626; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;開源模型成為大模型企業級市場新一輪增長的關鍵驅動力。沙利文報告認為，隨着千問 Qwen、DeepSeek 等國產模型在 2025 年持續開源，開源模型與國際頂級閉源模型的性能差距幾近抹平。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#262626; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;此外，開源模型尺寸、類別豐富，企業還能完全掌握自主權，根據自身業務特點定製模型及應用，報告預測，未來超過 80% 的企業將採用開源大模型，預示着開源模型將在行業應用中佔據主導性增長。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369635</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369635</guid>
      <pubDate>Sun, 31 Aug 2025 06:16:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>微信：將對 AI 生成合成內容添加顯式和隱式標識</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#242424"&gt;微信珊瑚安全發佈「&lt;/span&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fye36XFlR06Ix29CVPqQmsg" target="_blank"&gt;關於進一步規範人工智能生成合成內容標識的公告&lt;/a&gt;&lt;span style="color:#242424"&gt;」指出，&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;根據《人工智能生成合成內容標識辦法》要求，&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;平台&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;應對&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;A&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;I&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;生成&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;合成內容添加&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;顯式標識&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;和&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;隱式標識&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;為&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;保障用戶信息獲取的透明度與可信度，平台進一步優化內容識別能力。用戶通過平台獲取的 AI 生成合成內容，可能帶有顯式標識或隱式標識。平台也會對可能是 AI 生成合成的內容進行&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;相應提示，以便用戶清晰辨識。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="301" src="https://oscimg.oschina.net/oscnet/up-a73a30dc64edfc343477afce40b675857ce.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;為&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;避免&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;發佈&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;的&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;內容&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;在&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;傳播&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;過程&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;中&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;引起&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;混淆&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;或&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;誤&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;認&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;，&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;用戶&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;發佈&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;的內容&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;為&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;A&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;I&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;生成&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;合成&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;的&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;，&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;發佈&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;時&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;需主動&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;進行&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;聲明&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;。&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;標識&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;案例參考&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;：&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="235" src="https://oscimg.oschina.net/oscnet/up-87bc397da078033415bcdbcfdecec3aa6d3.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="156" src="https://oscimg.oschina.net/oscnet/up-a75c22e96cd27be87ad2e22a505a4911d69.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="604" src="https://oscimg.oschina.net/oscnet/up-b493c74959e71a3b436b03d56a2d5085ce2.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;依&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;據《人工智能生成合成內容標識辦法》規定，用戶在發佈或傳播 AI&amp;nbsp;生成合成內容時，不得以任何方式刪除、篡改、偽造或隱匿平台添加的 AI 標識。同時不得利用 AI 技術製作傳播虛假信息、侵權信息以及從事任何違法違規活動。對於違反法律法規及平台規範的行為，平台將視違規情況進行處罰。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;公告稱，&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;微信將不斷優化產品功能，保障平台內容的真實性，鼓勵創作者發佈高質量內容。同時也呼籲廣大創作者嚴格按照《人工智能生成合成內容標識辦法》要求及平台規範，發佈 AI 生成合成內容時主動進行聲明，共創清朗網絡空間。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;相關閲讀：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.oschina.net/news/369234" target="_blank"&gt;9 月 1 日起，AI 生成合成內容必須添加標識&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369634</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369634</guid>
      <pubDate>Sun, 31 Aug 2025 06:09:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Linus 將 Bcachefs 文件系統標記為由外部維護</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgit.kernel.org%2Fpub%2Fscm%2Flinux%2Fkernel%2Fgit%2Ftorvalds%2Flinux.git%2Fcommit%2F%3Fid%3Debf2bfec412ad293a0b118fb1a20a551088ebc9b" target="_blank"&gt;根據 Linux 內核的代碼提交記錄&lt;/a&gt;，Linus Torvalds 近日&lt;span style="background-color:#ffffff; color:#474747"&gt;更新了內核維護者文檔 MAINTAINERS&lt;/span&gt;，已將 Bcachefs 文件系統的維護狀態標記為「外部維護」，&lt;span style="background-color:#ffffff; color:#474747"&gt;發出了他不會接受 Bcachefs 新 PR 的信號。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;這意味着短期內 Bcachefs 的更新不會合併到 Linux 內核主線，該文件系統暫時不會被移除，現有用戶仍可繼續使用，但不排除未來可能被徹底移出內核。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1456" src="https://static.oschina.net/uploads/space/2025/0901/135607_2YlH_2720166.png" width="1450" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;此次變動源於 Linus Torvalds 與 Bcachefs 主要維護者 Kent Overstreet 之間長期存在的開發流程分歧，尤其是在內核候選發佈階段提交新功能補丁，違反了「僅修復 Bug」的規則。&lt;span style="background-color:#ffffff; color:#474747"&gt;Linus Torvalds 當時表態考慮移除 Bcachefs 文件系統。本月早些時候發佈的 Linux 6.17-rc1 就沒有合併來自 Overstreet 的任何拉取請求。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="background-color:#ffffff; color:#474747"&gt;Bcachefs 代碼目前仍然存在於主線 Linux 內核中，可能是為了防止現有用戶在使用 Bcachefs 時遇到問題。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369631</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369631</guid>
      <pubDate>Sun, 31 Aug 2025 06:00:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>OC Auto-POC 開源，一鍵搞定 OS 深度測試</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;OpenCloudOS&amp;nbsp;團隊研發並&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F27jTw0kHKzJ5LkrmccMPZQ" target="_blank"&gt;開源&lt;/a&gt;了&amp;nbsp;Auto-POC (Proof of Concept) 項目。這是一個專為 OS 功能和性能驗證打造的自動化測試套件。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在&amp;nbsp;OpenCloudOS&amp;nbsp;中，Auto-POC&amp;nbsp;通過「開箱即用」的方式，為用戶提供包括基礎功能測試、性能微基準測試、安全專項測試在內的測試能力，並提供報告一鍵生成和 Word 等多文本格式導出的功能。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;目前 Auto-POC 腳本可用於執行一系列的系統測試任務，包括用戶管理、系統配置、YUM 操作、磁盤 I/O 測試、內存穩定性測試、CPU 穩定性測試以及 UnixBench 測試。具體涵蓋以下幾大模塊：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;基礎功能驗證&lt;/strong&gt;：網絡配置（DHCP/靜態）、軟件源可用性、磁盤管理、CPU 基礎能力、內存穩定性等 15 類測試；&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;系統完整性測試&lt;/strong&gt;：驗證系統關鍵組件和文件的完整性；&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;性能壓測引擎：&lt;/strong&gt;&lt;/span&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;span style="color:#000000"&gt;磁盤：集成 FIO，測試磁盤 IOPS、帶寬、延遲等關鍵指標；&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span style="color:#000000"&gt;內存：集成 IOZone，測試內存文件操作性能，覆蓋讀寫、隨機讀寫等；&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span style="color:#000000"&gt;綜合：UnixBench 系統評分&lt;/span&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;安全合規專項：&lt;/strong&gt;&lt;/span&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;span style="color:#000000"&gt;國密算法（SM2/SM3/SM4）支持性驗證&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span style="color:#000000"&gt;文件完整性（AIDE）檢測&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span style="color:#000000"&gt;SELinux 強制訪問控制規則測試&lt;/span&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;Auto-POC 技術設計亮點&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;一鍵啓動，全自動執行：&lt;/strong&gt;&amp;nbsp;Auto-POC 設計了自動化流程。從環境初始化開始，到最終測試報告生成，一是減少了手動配置投入，二是全程極少需要人工幹預。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;標準化輸出，結果清晰可比：&lt;/strong&gt;所有測試結果以結構化、標準化的格式（如 JSON）輸出，同時也支持 Word 等標準文檔格式輸出。方便查看、對比分析和集成到其他系統。性能測試數據具有橫向和縱向可比性。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;高度可擴展：&lt;/strong&gt;1）框架設計靈活，用戶可根據自身需求添加新的測試用例或集成更多測試工具，持續擴展測試能力；2）不僅支持&amp;nbsp;OpenCloudOS&amp;nbsp;系統，也可以支持部署和測試其他類 CentOS 系統。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;擁抱自動化運維/DevOps：&lt;/strong&gt;&amp;nbsp;命令行接口友好，可集成到 CI/CD 流水線、自動化運維平台中，實現 OS 部署後的無人值守自動化驗證。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369629</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369629</guid>
      <pubDate>Sun, 31 Aug 2025 05:58:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>階躍發佈並開源端到端語音大模型 Step-Audio 2 mini</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;階躍星辰正式發佈最強開源端到端語音大模型&amp;nbsp;Step-Audio 2 mini，該模型在多個國際基準測試集上取得 SOTA 成績。&lt;/p&gt; 
&lt;p&gt;它將語音理解、音頻推理與生成統一建模，在音頻理解、語音識別、跨語種翻譯、情感與副語言解析、語音對話等任務中表現突出，並率先支持語音原生的 Tool Calling 能力，可實現聯網搜索等操作。&lt;/p&gt; 
&lt;p&gt;一句話總結，Step-Audio 2 mini 「&lt;strong&gt;聽得&lt;strong&gt;&lt;strong&gt;清楚&lt;/strong&gt;&lt;/strong&gt;、想得明白、説得自然&lt;/strong&gt;」。&lt;/p&gt; 
&lt;p&gt;據介紹，Step-Audio 2 mini 在多個關鍵基準測試中取得 SOTA 成績，在音頻理解、語音識別、翻譯和對話場景中表現突出，綜合性能超越 Qwen-Omni 、Kimi-Audio 在內的所有開源端到端語音模型，並在大部分任務上超越 GPT-4o Audio。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0901/134137_uHCe_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0901/134144_WabG_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;在通用多模態音頻理解測試集 MMAU 上，Step-Audio 2 mini 以 73.2 的得分位列開源端到端語音模型榜首；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;在衡量口語對話能力的 URO Bench 上， Step-Audio 2 mini 在基礎與專業賽道均拿下開源端到端語音模型最高分，展現出優秀的對話理解與表達能力；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;在中英互譯任務上， Step-Audio 2 mini 優勢明顯，在 CoVoST 2 和 CVSS 評測集上分別取得 39.3 和 29.1 的分數，大幅領先 GPT-4o Audio 和其他開源語音模型；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;在語音識別任務上，Step-Audio 2 mini 取得多語言和多方言第一。其中開源中文測試集平均 CER（字錯誤率） 3.19，開源英語測試集平均 WER（詞錯誤率） 3.50，領先其他開源模型 15% 以上。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Step-Audio 2 mini 通過創新架構設計，有效解決了此前語音模型存在的問題，做到「走腦又走心」。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;真端到端多模態架構&lt;/strong&gt;：Step-Audio 2 mini 突破傳統 ASR+LLM+TTS 三級結構，實現原始音頻輸入到語音響應輸出的直接轉換，架構更簡潔、時延更低，並能有效理解副語言信息與非人聲信號。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img height="525" src="https://static.oschina.net/uploads/space/2025/0901/134156_klxf_2720166.png" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;圖：Step-Audio 2 mini 模型架構圖&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;CoT 推理結合強化學習：&lt;/strong&gt;Step-Audio 2 mini 在端到端語音模型中首次引入鏈式思維推理（Chain-of-Thought， CoT）與強化學習聯合優化，能對情緒、語調、音樂等副語言和非語音信號進行精細理解、推理並自然回應。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;音頻知識增強&lt;/strong&gt;：模型支持包括 web 檢索等外部工具，有助於模型解決幻覺問題，並賦予模型在多場景擴展上的能力。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;模型現已上線 GitHub、Hugging Face 等平台。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;GitHub：&lt;em&gt;https://github.com/stepfun-ai/Step-Audio2&lt;/em&gt;&lt;br&gt; Hugging Face：&lt;em&gt;https://huggingface.co/stepfun-ai/Step-Audio-2-mini&lt;/em&gt;&lt;br&gt; ModelScope：&lt;em&gt;https://www.modelscope.cn/models/stepfun-ai/Step-Audio-2-mini&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369623</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369623</guid>
      <pubDate>Sun, 31 Aug 2025 05:43:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>阿里巴巴正在開發一款新 AI 芯片，已進入測試階段</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.wsj.com%2Ftech%2Fai%2Falibaba-ai-chip-nvidia-f5dc96e3" target="_blank"&gt;據報道&lt;/a&gt;，阿里巴巴正在開發一款新的人工智能芯片，意在填補英偉達在中國市場的空白。目前，這款芯片已進入測試階段，主要面向更廣泛的 AI 推理任務，並與英偉達的架構兼容。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0901/114403_3NgP_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;此外，新的芯片不再由台積電代工，轉為由國內一家企業代工。&lt;/p&gt; 
&lt;p&gt;目前，阿里巴巴已經推出了多款自研 AI 芯片以及 AI 大模型，並通過阿里雲將 AI 算力與解決方案服務化，提供給廣泛的企業用戶。值得注意的是，阿里雲 AI 相關產品收入已連續八個季度保持三位數的同比增長，顯示出該業務的持續爆發力。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369611</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369611</guid>
      <pubDate>Sun, 31 Aug 2025 03:44:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>快手開源向量化引擎 Auron 正式加入 Apache 孵化器</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;由快手開源並捐贈的向量化引擎 Auron 項目（原 Blaze 項目）近期&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F9yVMKH7Zc0W_OsYdEBeBUw" target="_blank"&gt;正式&lt;/a&gt;進入全球最大開源基金會（ASF）的孵化器，移交到 Apache 軟件基金會名下。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p style="color:#000000; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#4a4a4a"&gt;加入 Apache 軟件基金會孵化，源於我們對開源生態的深刻認同，以及對項目長期可持續發展的考量。目前 Auron 已在多家公司應用落地，驗證了其實用價值。然而，開源項目的生命力不僅依賴技術先進性，更需要通過開放的社區治理、多元的貢獻者參與和透明的決策機制實現持續進化。我們始終相信，開源的價值在於「共享」與「共創」。&lt;/span&gt;&lt;/p&gt; 
 &lt;p style="color:#000000; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#4a4a4a"&gt;我們將嚴格遵守 Apache 的治理規範，推動項目代碼、文檔與社區的全面透明化，吸引更多開發者參與技術迭代，與 Apache 生態中的其他大數據項目（如 Spark/Flink/Celeborn 等）形成技術互補，共同推動大數據領域的技術持續創新。&lt;/span&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Auron 是基於向量化技術開發的一套 Native 執行引擎，可以充分利用 Native 代碼和 SIMD 指令向量化優勢，以實現減少資源開銷、加速執行的目的。&lt;/p&gt; 
&lt;p&gt;&lt;img height="308" src="https://oscimg.oschina.net/oscnet/up-98c7a3292dbcf3b0b36743234cbae342e56.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;其核心能力包括：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;原生執行：採用 Rust 實現，消除 JVM 開銷以取得更好的性能表現；&lt;/li&gt; 
 &lt;li&gt;向量化計算：基於 Apache Arrow 列式格式構建，充分利用 SIMD 指令優化批處理；&lt;/li&gt; 
 &lt;li&gt;可插拔架構：與 Apache Spark 無縫集成，同時設計上支持未來擴展至其他計算引擎；&lt;/li&gt; 
 &lt;li&gt;生產環境強化優化：開發了多級內存管理、優化 Shuffle 格式及自適應執行策略等優化機制，並在生產環境大規模應用落地。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;基於 Auron，在 TPC-DS 上相比 Spark 可以取得 2+倍的性能提升：&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" height="382" src="https://oscimg.oschina.net/oscnet/up-f70fed02e6787d0c28479a8cf4f008c930d.webp" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h4&gt;&lt;strong&gt;Auron 發展歷史與現狀&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;2022 年 1 月，快手大數據 Spark 引擎團隊正式啓動了 Blaze 並開源。2023 年 9 月，Blaze 在 TPC-H/TPC-DS 等 Beachmark 上取得了顯著的性能提升。同時，團隊也圍繞生產環境進行了大量優化，並在快手內部實現了大規模應用。目前，Blaze 在快手日均運行數十萬個任務、處理 EB 級別數據，展現出卓越的性能和穩定性，為快手每年節約數千萬元服務器成本。&lt;/p&gt; 
&lt;p&gt;在社區運營方面，快手於 2024 年 1 月展開了開源社區的管理工作。自 Blaze 開源以來，已累計發佈了 10 餘個版本。目前，Blaze 已被滴滴、攜程、汽車之家、58 同城、OPPO 等多個行業的公司廣泛使用。&lt;/p&gt; 
&lt;p&gt;2025 年 8 月，Blaze 項目正式進入 ASF 孵化器，並更名為 Auron（發音： [ˈɔːrɑːn] ），其靈感來源 Aura（能量場）。Auron 寓意為大數據引擎所帶來的強大性能，未來，Auron 社區計劃提供更多先進功能，如 Flink 引擎、數據湖系統、GPU/DPU 硬件的集成等。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369610</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369610</guid>
      <pubDate>Sun, 31 Aug 2025 03:39:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Anthropic 承認 Claude Opus 4.1 和 Opus 4 模型近期「降智」</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;8 月 30 日，Anthropic 官方發佈&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fstatus.anthropic.com%2Fincidents%2Fh26lykctfnsz" target="_blank"&gt;事件報告&lt;/a&gt;，確認了 Claude Opus 4.1 和 Opus 4 近期出現質量下降的情況。（官方表示已修復）&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0901/113337_hTvV_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;報告顯示，從 8 月 25 日 17:30 至 8 月 28 日 2:00 期間（世界統一時間），Claude Opus 4.1 對某些請求的回答質量有所下降，用戶端可能會看到較低的智能表現、格式錯誤的響應或 Claude Code 中工具調用出現問題。&lt;/p&gt; 
&lt;p&gt;Anthropic 方面表示，上述情況是由於其推理堆棧的推送所引起，目前已將 Claude Opus 4.1 進行回滾。Anthropic 表示，雖然更新是為了提高模型效率和吞吐量，但其意圖始終是保持相同的模型響應質量。&lt;/p&gt; 
&lt;p&gt;另外，Anthropic 還發現 Claude Opus 4.0 也受到了上述問題影響，目前同樣進行回滾處理。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369609</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369609</guid>
      <pubDate>Sun, 31 Aug 2025 03:34:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>馬斯克承認 xAI 代碼庫遭竊，前員工轉投 OpenAI</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;馬斯克最近爆料稱，他所創立的 xAI 公司的整個代碼庫遭到竊取。xAI 已經對一名前員工提起訴訟，指控他竊取了公司的商業機密，且此人已跳槽至競爭對手 OpenAI。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="328" src="https://oscimg.oschina.net/oscnet/up-cf86b5f71d69bdbb4fa495886290e53f865.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;據悉，涉事的前員工名叫 Xuechen Li，他曾是 xAI 的核心成員之一。根據 xAI 向加州北區聯邦地方法院遞交的起訴書，Li 面臨四項指控，涉及違反保密協議、侵犯商業祕密、違反加州計算機數據法規以及欺詐。xAI 要求法院對 Li 實施禁令，禁止其在 OpenAI 等競爭對手工作，並要求其歸還所有被盜取的數據。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;事件的起因追溯到 Li 於 7 月 28 日從 xAI 辭職，辭職前的三天，他便已將大量公司的數據上傳至個人系統。令人矚目的是，在辭職前夕，Li 還將手中的 xAI 股份套現，獲得了近 700 萬美元的收益。雖然 Li 在離職時簽署了相關文件，承諾歸還公司財產和刪除所有副本，但他仍然採取了一系列手段來掩蓋其竊密行為。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;根據 xAI 的調查，8 月 11 日，公司的安全軟件檢測到數據外泄的跡象，隨即向 Li 發函要求他歸還被盜信息。可 Li 不但沒有配合，反而更改了存儲盜取數據的賬戶密碼，試圖阻止公司的訪問和恢復。在承認竊密的過程中，Li 的律師也在場，但他仍然隱瞞了多項關鍵信息。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;值得一提的是，Li 在離職前已收到 OpenAI 的邀請，並於 8 月 19 日加入該公司。xAI 在起訴書中表示，Li 涉嫌盜取的商業機密包括了 「超越 ChatGPT 及其他競品的&lt;span&gt;尖端&lt;/span&gt;人工智能技術」，這些技術可能為 OpenAI 及其他競爭對手節省數十億美元的研發費用。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;Xuechen Li 的背景同樣引人關注。他是斯坦福大學的計算機博士，曾在 Google 和微軟實習，是 xAI 早期團隊的一員。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369606</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369606</guid>
      <pubDate>Sun, 31 Aug 2025 03:25:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>強化學習的 「GPT-3 時刻」 即將到來</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                                        &lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;編者按：&lt;/strong&gt; 強化學習能否像 GPT-3 改變自然語言處理那樣，通過大規模擴展實現質的飛躍？為什麼強化學習至今仍困在"先預訓練，再微調"的傳統模式中？為什麼即使是最先進的 RL 模型，一旦脫離訓練環境就變得如此脆弱？&lt;/p&gt; 
 &lt;p&gt;無論是自動駕駛、機器人控制，還是複雜系統優化，我們都需要能夠快速適應新任務、具備真正泛化能力的智能體。然而當前的 RL 模型就像是"高分低能"的應試選手 ------ 在熟悉的測試環境中表現優異，但面對真實世界的複雜性時卻束手無策。&lt;/p&gt; 
 &lt;p&gt;本文提出了 replication training 範式，為強化學習的規模化擴展指明瞭全新方向。作者不再拘泥於傳統的遊戲環境或仿真場景，而是大膽提議讓 AI 複製現有的軟件產品。它利用了互聯網上豐富的軟件資源，提供了客觀明確的評估標準，同時訓練了 AI 在長週期項目中保持穩定輸出的能力。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;作者 | Matthew Barnett, Tamay Besiroglu, Ege Erdil&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;編譯 | 嶽揚&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;GPT-3 證明瞭，僅僅通過擴大語言模型的規模，就能帶來強大的、task-agnostic（譯者注：模型不依賴特定任務的設計或微調，就能處理多種不同類型的任務。）、few-shot（譯者注：模型僅需極少量示例，就能快速理解並執行新任務。）的性能，其表現通常優於經過精心微調的模型。在 GPT-3 出現之前，要達到最先進的性能，首先需要在大型通用文本語料庫上對模型進行預訓練，然後再針對特定任務進行微調。&lt;/p&gt; 
&lt;p&gt;如今的強化學習同樣困在類似 GPT-3 之前的範式裏。我們首先是對大模型進行預訓練，然後在高度專業化的環境中，對特定任務進行精細的微調。但這種方法的根本侷限在於：由此獲得的能力難以泛化，導致性能"脆弱"（brittle performance） ------ &lt;strong&gt;模型一旦脫離訓練期間接觸的精確語境，性能便會迅速退化。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-e782b46339412b51a1a128eda50eb48d456.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;我們認為強化學習（RL）即將迎來其"GPT-3 時刻"。相比在有限數量的訓練場景或任務設置上微調模型，我們預計該領域將轉向在數千個多樣化環境上進行大規模訓練。有效實施這一做法將催生出具有 few-shot、task-agnostic 能力的 RL 模型，能夠快速適應全新的任務。但實現這一點需要訓練環境在規模和多樣性上遠超當前任何的可用資源。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;01 究竟需要多少 RL 資源？&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;當前的 RL 數據集相對較小。例如，DeepSeek-R1 在大約 60 萬個數學問題上進行了訓練，這相當於人類連續努力六年的工作量（假設每個任務耗時五分鐘完成）。相比之下，重建 GPT-3 那包含 3000 億 token 的訓練語料庫，若按人類平均書寫速度計算，需要大約數萬年的寫作時間。&lt;/p&gt; 
&lt;p&gt;需要説明的是，&lt;strong&gt;要達到與當前前沿模型預訓練預算相當的 RL 計算支出，按人類完成相同任務所需時長來衡量，可能需要大約上萬年。&lt;/strong&gt; DeepSeek-R1 在 RL 階段使用了約 6e23 FLOP 的計算量[1]，按人類效率折算，對應約 6 年的時長。假設未來的訓練任務使用與 DeepSeek-R1 相似的訓練輪次（epochs）和組大小（group sizes），將此擴展至約 6e26 FLOP 意味着需要人類約 6000 年的工作時長。&lt;/p&gt; 
&lt;p&gt;尚不確定未來的強化學習訓練會需要更大的還是更小的組規模（group sizes）、抑或是更多的訓練輪次（epochs），尤其是隨着任務分佈多樣性的增加。我們在這方面缺乏足夠的數據，因此精確估算等效的人類工作時間仍很困難，儘管 1 萬年左右似乎是一個較為合理的數量級。&lt;/p&gt; 
&lt;p&gt;這一過程要求模型完成的工作量，其規模可與 Windows Server 2008、GTA V 或 Red Hat Linux 7.1 等大型項目相當 ------ 每個項目估計都需要約 1 萬年的累計人類工作量。&lt;/p&gt; 
&lt;p&gt;將強化學習（RL）擴展到這一規模在經濟上是高效的。&lt;strong&gt;由於算力成本在總訓練成本中佔據主導地位，將強化學習的規模提升到與預訓練預算相當的水平，能在不明顯增加總成本的情況下帶來大幅的性能提升。&lt;/strong&gt; 然而，要實現這一目標，就必須大規模擴展強化學習環境（RL environments）的體量，同時確保任務能夠實現自動化評估。這很可能需要開發新的構建強化學習環境的方法。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;02 Replication training&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;想象一下，每次當你想要通過下一個詞預測方法（next-token prediction）預訓練語言模型時，都必須親手創建整個訓練語料庫。顯然，這極其不切實際。因此，我們轉而利用海量的現有內容 ------ 如書籍、學術論文、博客帖子和 Reddit 討論內容來構建訓練語料庫。&lt;/p&gt; 
&lt;p&gt;同樣，&lt;strong&gt;我們推測，RL（強化學習）領域的"GPT-3 時刻"將主要依託於一種稱為 replication training 的新範式來實現。&lt;/strong&gt; 該範式要求 AI 複製現有的軟件產品或其內部特定功能。實現複雜的哈希與加密算法的簡單命令行工具是較為理想的初期目標，這種方案可以輕鬆擴展到更復雜的軟件，例如網站、專業軟件和遊戲。&lt;/p&gt; 
&lt;p&gt;每項複製任務（replication tasks）均包含詳細的説明規範和用於參考的實現方案。其核心思想是，AI 模型經過訓練後能夠生成與用於參考的實現方案完全一致的方案。這種清晰直接的方法極大地簡化了評估過程，因為評分標準客觀且明確：生成的實現方案的行為要麼與用於參考的實現方案完全一致，要麼就是不一致。&lt;/p&gt; 
&lt;p&gt;儘管這些複製任務（replication tasks）可能與日常的軟件工程活動有所不同，但它們專門針對當前 AI 系統難以掌握的關鍵能力。例如，複製一個複雜的算法（如依據詳細規範進行開發的、包含萬行量級代碼的加密/解密 CLI 工具），要求模型必須做到：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;準確閲讀並深度理解詳細指令。&lt;/li&gt; 
 &lt;li&gt;一絲不苟且精確無誤地執行指令。&lt;/li&gt; 
 &lt;li&gt;能夠發現早期錯誤並可靠地恢復。&lt;/li&gt; 
 &lt;li&gt;在長時間週期（相當於人類數月時間的開發工作量）內保持穩定輸出 ------ 在此過程中，質量優劣完全由功能正確性直接判定。&lt;/li&gt; 
 &lt;li&gt;在遇到困難時展現出韌性，而非草率止步於看起來"差不多能用"的方案。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;我們預測，replication training 將成為 AI 領域的下一個範式，因為它順延了我們在 AI 發展過程中已觀察到的趨勢 ------ 利用海量的現有人類生成數據來創建新任務。就像自然語言一樣，軟件在互聯網上同樣資源豐富。因此，replication training 提供了一種可擴展的途徑，能高效生成複雜任務，推動我們實現可端到端完成完整軟件項目的 AI。&lt;/p&gt; 
&lt;p&gt;然而，這種方法也面臨着幾項挑戰。編寫有效且全面的測試仍然是一項非同小可的任務，需要大量的工程投入。此外，複製任務（replication tasks）本身具有一定的人造性，因為精確複製現有軟件並非日常軟件工程的典型工作（儘管在軟件移植、遺留系統重構、淨室重新實現【譯者注：clean-room reimplementations，指在嚴格隔離原始代碼知識的前提下，僅通過分析功能規範或外部行為，重新實現與原有軟件功能相同的程序。該過程需確保開發團隊從未接觸過原始源代碼，以避免法律上的版權/專利侵權風險。】）等場景中確有其例。&lt;/p&gt; 
&lt;p&gt;儘管存在這些挑戰，但我們認為 replication training 為將強化學習環境（RL environments）擴展到實現有意義泛化所需的龐大規模提供了一條清晰明確的路徑。它很可能將成為解鎖強化學習"GPT-3 時刻"的關鍵，為達成穩健的、task-agnostic 的性能提供所需的數萬年量級的經驗積累。&lt;/p&gt; 
&lt;p&gt;replication training 會是解鎖 full automation of labor（譯者注：通過 AI / 機器人系統實現人類所有勞動形式的自動化替代，達到無需人類直接參與即可完成經濟生產活動的終極狀態。）的終極範式嗎？對此我們持懷疑態度。雖然它能催生可在精確設計規範下自主完成高複雜度軟件項目的系統，但我們推測，這些能力仍將遜色於人類所具備的開放式能力。即便 AI 成為高級編程專家，它們在狹窄的軟件領域之外的高層管理（譯者注：high-level management，指組織架構中涉及戰略決策、資源分配和跨部門協調的頂層管理職能。）與自主規劃（agentic planning）方面也未必能勝任。&lt;/p&gt; 
&lt;p&gt;然而，正如我們需要先發明預訓練，才能邁向 replication training，replication training 仍可作為通往下一範式的橋樑。我們對這一新範式的未來潛力充滿期待。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;END&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;本期互動內容 🍻&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;❓您預測 RL 領域的"GPT-3 時刻"會在什麼時間節點出現？3 年內、5-10 年，還是更久？請分享您的判斷依據。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;本文經原作者授權，由&lt;/strong&gt; &lt;strong&gt;Baihai IDP&lt;/strong&gt; &lt;strong&gt;編譯。如需轉載譯文，請聯繫獲取授權。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;原文鏈接：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.mechanize.work%2Fblog%2Fthe-upcoming-gpt-3-moment-for-rl%2F" target="_blank"&gt;https://www.mechanize.work/blog/the-upcoming-gpt-3-moment-for-rl/&lt;/a&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/IDP/blog/18689960</link>
      <guid isPermaLink="false">https://my.oschina.net/IDP/blog/18689960</guid>
      <pubDate>Sun, 31 Aug 2025 03:01:00 GMT</pubDate>
      <author>原創</author>
    </item>
    <item>
      <title>百度「91 助手」將於本月底全面停止所有服務</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2F91zs.soupingguo.com%2Fnotice%2Fservice-shutdown.html" target="_blank"&gt;根據 91 助手官方消息&lt;/a&gt;，該應用將於 2025 年 9 月 27 日 23:59 起全面停止所有服務，包括但不限於手機連接管理、文件傳輸、應用安裝卸載、系統清理等功能。&lt;/p&gt; 
&lt;p&gt;官方建議用戶在此期間及時備份相關數據，服務終止後，用戶在 91 助手的數據將永久丟失，無法再以任何方式找回。此外 91 助手將向仍處於會員期的用戶進行退費，但需要用戶主動申請，未主動申請退費的用戶將無法獲得退款。 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0901/103517_Wl4q_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;作為百度 2013 年以 19 億美元重金收購的標誌性資產，其停服不僅意味着一款產品的生命週期終結，更折射出整個移動應用分發行業的深刻變革，以及移動互聯網早期紅利的終結。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-25f9a25a818ee772e357f2e25fad5bb1098.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;截至發稿，百度方面未作回應。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369597</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369597</guid>
      <pubDate>Sun, 31 Aug 2025 02:38:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
  </channel>
</rss>
