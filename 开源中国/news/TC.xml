<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - news - 繁體中文（台灣）</title>
    <link>https://www.oschina.net/news/project</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news" rel="self" type="application/rss+xml"/>
    <description>已對該 RSS 進行格式化操作：中英字符之間插入空格、使用直角引號、標點符號修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-tw</language>
    <lastBuildDate>Wed, 27 Aug 2025 02:43:53 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>vivo 等提出 DiMo-GUI：模態分治 + 動態聚焦，GUI 智能體推理時擴展的新範式</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;blockquote&gt; 
 &lt;p style="margin-left:0; margin-right:0"&gt;作者：vivo 互聯網算法團隊&lt;/p&gt; 
 &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#40a9ff"&gt;&lt;strong&gt;本文入選 EMNLP 2025 Main Conference&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
 &lt;p style="margin-left:0; margin-right:0"&gt;&lt;strong&gt;EMNLP 會議&lt;/strong&gt;全稱為 Conference on Empirical Methods in Natural Language Processing，由國際計算語言學協會 ACL 舉辦，是自然語言處理和人工智能領域最重要的學術會議之一。EMNLP 2025 會議共有 8174 篇投稿，Main Conference 接收率僅為 22.16%。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;img src="https://oscimg.oschina.net/oscnet//53e017ae0673a706f87b381b014e1ca6.png" referrerpolicy="no-referrer"&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;p style="color:#303030; margin-left:0; margin-right:0; text-align:start"&gt;項目主頁：&lt;/p&gt; 
&lt;p style="color:#303030; margin-left:0; margin-right:0; text-align:start"&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fxie.infoq.cn%2Flink%3Ftarget%3Dhttps%253A%252F%252Fgithub.com%252Fvivo%252FDiMo-GUI" rel="nofollow" target="_blank"&gt;https://github.com/vivo/DiMo-GUI&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p style="margin-left:0; margin-right:0"&gt;&lt;strong&gt;摘要：&lt;/strong&gt;&lt;/p&gt; 
 &lt;p style="margin-left:0; margin-right:0"&gt;本文介紹了一種無需額外訓練的 GUI 定位框架 DiMo-GUI，針對多模態大語言模型（MLLMs）在複雜圖形用戶界面（GUI）定位任務中的挑戰，通過動態視覺推理與模態感知優化顯著提升性能。DiMo-GUI 採用逐級縮放的動態定位機制，迭代裁剪聚焦目標區域，減少視覺冗餘；同時分離文本與圖標模態，獨立推理後結合指令評估確定最終目標，有效平衡多模態處理能力。在 GUI 定位任務最新的基準數據集上，DiMo-GUI 相較基線展現顯著性能提升。作為即插即用框架，DiMo-GUI 適用於網頁導航、移動應用自動化等場景，未來可通過回溯機制進一步提升魯棒性。&lt;/p&gt; 
 &lt;p style="margin-left:0; margin-right:0"&gt;該工作由 vivo 互聯網算法團隊、加州大學默塞德分校、昆士蘭大學共同完成。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;span id="OSC_h1_1"&gt;&lt;/span&gt; 
&lt;h1&gt;一、引言&lt;/h1&gt; 
&lt;p style="color:#303030; margin-left:0; margin-right:0; text-align:start"&gt;隨着&lt;strong&gt;圖形用戶界面（Graphical User Interface, GUI）&lt;/strong&gt;在自動化導航和操作系統控制等領域的廣泛應用，基於自然語言查詢的 &lt;strong&gt;GUI 定位（GUI Grounding）&lt;/strong&gt;成為&lt;strong&gt;多模態大語言模型（multimodal large language models, MLLMs）&lt;/strong&gt;的重要研究方向。然而，GUI 環境的視覺複雜性、語言歧義以及空間雜亂等問題為精準定位帶來了顯著挑戰。&lt;/p&gt; 
&lt;p style="color:#303030; margin-left:0; margin-right:0; text-align:start"&gt;本文基於最新研究《DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning》，介紹了一種無需額外訓練的 GUI 定位框架——&lt;strong&gt;DiMo-GUI&lt;/strong&gt;，通過動態視覺推理和模態感知優化顯著提升了多模態大模型在複雜 GUI 環境中的定位性能，推動了&lt;strong&gt;推理時擴展（test-time scaling）&lt;/strong&gt;在該領域的發展。&lt;/p&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;img src="https://oscimg.oschina.net/oscnet//0dc3cd4bfdb7f8a14c2a2fde07e79ed9.png" referrerpolicy="no-referrer"&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;p style="color:#303030; margin-left:0; margin-right:0; text-align:start"&gt;日常生活中，我們與電腦、手機的交互離不開圖形用戶界面。小到點贊、大到數據分析，我們都希望 AI 能像人一樣，理解屏幕上的每一個按鈕、每一段文字，並準確執行指令。然而，對於飛速發展中的多模態大模型來説，這卻是前所未有的艱鉅挑戰。在一個複雜的 App、網頁或桌面軟件中，用戶可能隨手一句「點擊開始播放」，但對於 AI 來説，準確找到這個指令對應的圖標/按鈕並不簡單：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;strong&gt;模態混雜&lt;/strong&gt;：用戶界面同時包含文本、圖標、背景、裝飾性元素等，幹擾多；並且大多數 VLM 對文字理解更強，圖標處理卻弱，造成嚴重偏差；&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;strong&gt;冗餘信息&lt;/strong&gt;：高分辨率 UI 中，重要區域可能只佔整體的幾十分之一，模型容易定位錯誤區域。&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p style="color:#303030; margin-left:0; margin-right:0; text-align:start"&gt;研究發現，傳統方法如基於文本推理或單次視覺定位的管道在高分辨率、視覺擁擠的 GUI 中表現不佳。例如在最新的 ScreenSpot-Pro 數據集上，大多數通用模型如 GPT-4o, Qwen2-VL 等只有 1% 左右的正確率， 即使是針對於 GUI 定位任務的 ShowUI, Aria-UI 等智能體也只有 10% 左右的正確率。&lt;/p&gt; 
&lt;span id="OSC_h1_2"&gt;&lt;/span&gt; 
&lt;h1&gt;二、關鍵改進：模態分離 + 動態定位&lt;/h1&gt; 
&lt;p style="color:#303030; margin-left:0; margin-right:0; text-align:start"&gt;從上述問題出發，該研究推出零訓練成本的 DiMo-GUI，通過模態感知的視覺推理推進訓練時擴展，顯著提升多模態大模型的圖形界面（GUI）理解能力。主要的改進方式包括以下兩點：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;strong&gt;動態視覺定位&lt;/strong&gt;：DiMo-GUI 採用逐級縮放機制，從粗略預測開始，基於初始座標生成候選焦點區域，並通過迭代裁剪逐步聚焦目標。例如，首次推理後，模型以預測座標為中心裁剪半個圖像大小的區域作為下一輪輸入，顯著減少視覺冗餘。動態迭代機制根據前後預測的座標距離（小於圖像對角線六分之一時停止）實現自適應停止，避免「過度思考」。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;strong&gt;模態感知優化&lt;/strong&gt;：DiMo-GUI 將 GUI 元素分為文本和圖標兩類，分別進行獨立的定位推理，生成文本座標（C_text）和圖標座標（C_icon）。隨後，模型結合原始指令和全分辨率圖像評估兩個候選座標，確定最終目標 （C*），有效平衡文本和圖標的處理能力。&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p style="color:#303030; margin-left:0; margin-right:0; text-align:start"&gt;這樣的方式推動了&lt;strong&gt;推理時拓展（Test-time Scaling）&lt;/strong&gt;在 GUI 定位這一領域的發展，提供了新的思路和方式。&lt;/p&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;img src="https://oscimg.oschina.net/oscnet//e83f73873fbad7e248e1da405b99f189.png" referrerpolicy="no-referrer"&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;p style="color:#303030; margin-left:0; margin-right:0; text-align:start"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;span id="OSC_h1_3"&gt;&lt;/span&gt; 
&lt;h1&gt;三、實驗結果：無需訓練和任何額外數據，只在推理階段就可以大幅提升性能&lt;/h1&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;img src="https://oscimg.oschina.net/oscnet//b3d3d49204e1faa771ec0ad0791dea74.png" referrerpolicy="no-referrer"&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;p style="color:#303030; margin-left:0; margin-right:0; text-align:start"&gt;團隊在最新的高分辨率 GUI 數據集 ScreenSpot-Pro 上驗證發現：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;DiMo-GUI 可以作為即插即用的框架大幅提升多個 GUI 模型的性能。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;其中 OS-Atlas-7B 在引入 DiMo-GUI 之後獲得了超過兩倍的指標提升（18.9% -- 49.7%）, UGround-7B 和 UGround-V1-7B 也均獲得了超過 10% 的指標提升。&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p style="color:#303030; margin-left:0; margin-right:0; text-align:start"&gt;在相對簡單的 ScreenSpot 數據集上，DiMo-GUI 同樣可以提升多個模型的性能。&lt;/p&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;img src="https://oscimg.oschina.net/oscnet//92892c7de106ba9aac72c1532918459f.png" referrerpolicy="no-referrer"&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;p style="color:#303030; margin-left:0; margin-right:0; text-align:start"&gt;定性結果表示，模型加入 DiMo-GUI 之後可以通過動態定位逐步逼近正確結果。&lt;/p&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;img src="https://oscimg.oschina.net/oscnet//b763f01354805dae541a9ab9898a0ae1.png" referrerpolicy="no-referrer"&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;span id="OSC_h1_4"&gt;&lt;/span&gt; 
&lt;h1&gt;四、總結&lt;/h1&gt; 
&lt;p style="color:#303030; margin-left:0; margin-right:0; text-align:start"&gt;&lt;strong&gt;DiMo-GUI&lt;/strong&gt;&amp;nbsp;提供了一種高效、通用且無需訓練的 &lt;strong&gt;GUI 定位框架&lt;/strong&gt;，通過動態視覺推理和模態感知優化顯著提升了多模態大語言模型在複雜 GUI 環境中的表現。其&lt;strong&gt;「即插即用」&lt;/strong&gt;特性使其可無縫集成到現有 &lt;strong&gt;GUI Agent &lt;/strong&gt;中，適用於網頁導航、移動應用自動化等場景。未來研究可探索引入回溯機制以糾正早期錯誤，進一步提升定位魯棒性。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/vivotech/blog/18689577</link>
      <guid isPermaLink="false">https://my.oschina.net/vivotech/blog/18689577</guid>
      <pubDate>Wed, 27 Aug 2025 02:37:52 GMT</pubDate>
      <author>原創</author>
    </item>
    <item>
      <title>谷歌發佈新圖像生成模型 nano banana</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;谷歌正式&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdevelopers.googleblog.com%2Fen%2Fintroducing-gemini-2-5-flash-image%2F" target="_blank"&gt;發佈&lt;/a&gt;了其最先進的圖像生成與編輯模型——Gemini 2.5 Flash Image（又名 nano banana）。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-9cb4376ea1cc064051cdbcd5e04a019d086.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-cd68bfaad4042c2de322bf1198b1e09f449.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;據官方介紹，Gemini 2.5 Flash Image 的主要特點包括下面幾點：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;充分保持角色的一致性：它可以輕鬆地將同一個角色置於不同的環境中，或者從多個角度展示同一款產品，同時完美地保持其核心主體不變。&lt;/li&gt; 
 &lt;li&gt;基於提示的圖片編輯：允許用戶通過簡單的自然語言指令，對圖片進行精準的局部修改 。&lt;/li&gt; 
 &lt;li&gt;利用 Gemini 的現實世界知識：模型可藉助 Gemini 強大的世界知識庫，讓圖像生成變得更加「智能」。&lt;/li&gt; 
 &lt;li&gt;多幅圖像融合：可以將一張圖片中的物體「放」進另一張圖片的場景裏，整個過程只需一條提示指令就能完成。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;性能表現上，Gemini 2.5 Flash Image 在多項基準測試上均為第一名，超越 OpenAI ChatGPT 4o（GPT Image 1 high）、Qwen Image Edit 等模型。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-bfee07407ab38e2b001fd0dbe895d36f242.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;關於調用 API，具體的定價是每百萬輸出 token 30 美元，官方介紹，生成一張圖片大約消耗 1290 個輸出 token，也就是説，每張圖片的成本約為 0.039 美元，換算下來人民幣不到 3 毛錢。&lt;/p&gt; 
&lt;p&gt;目前，Gemini 2.5 Flash Image 已經可以通過 Gemini APP、Gemini API、Google AI Studio 和 Vertex AI 進行訪問。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368669/google-gemini-2-5-flash-image</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368669/google-gemini-2-5-flash-image</guid>
      <pubDate>Wed, 27 Aug 2025 02:21:21 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>蘋果內部正探討收購 Mistral 和 Perlextity 可能性</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;據報道，蘋果公司內部已就收購法國人工智能初創公司 Mistral 以及美國的 Perplexity 展開討論。這一舉措旨在增強其人工智能能力，以應對谷歌和三星等競爭對手的領先優勢。&lt;/p&gt; 
&lt;p&gt;此前，蘋果首席執行官蒂姆・庫克在上個月暗示，公司對大規模人工智能相關收購持開放態度，以加速其人工智能發展路線圖，這與蘋果以往在併購方面的保守姿態有所不同。Mistral 在去年 B 輪融資後估值超過 60 億美元，本月有報道稱該公司正在洽談以 100 億美元估值籌集 10 億美元資金。今年早些時候，彭博社也曾報道，蘋果高管內部討論過對 Perplexity 的潛在收購意向。&lt;/p&gt; 
&lt;p&gt;據《The Information》報道，蘋果服務業務主管埃迪・庫伊是收購人工智能公司以增強蘋果產品實力的主要倡導者，他曾提議收購 Netflix 和特斯拉，但均被庫克否決。而軟件業務主管克雷格・費德里吉則對大規模人工智能收購持謹慎態度，他認為蘋果有能力內部構建人工智能技術。&lt;/p&gt; 
&lt;p&gt;目前，蘋果對這兩起潛在收購仍存顧慮，因其可能涉及鉅額資金，而蘋果歷史上極少有超億美元的收購交易。若聯邦裁決終止蘋果與谷歌 200 億美元的默認搜索引擎合作，蘋果或更有動力收購人工智能搜索初創公司填補空缺。&lt;/p&gt; 
&lt;p&gt;截至目前，蘋果、Mistral 和 Perplexity 均未對此置評。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368665</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368665</guid>
      <pubDate>Wed, 27 Aug 2025 02:13:21 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Opera 二季度營收同比增長 30%，AI 生態開啓新一輪增長週期</title>
      <description/>
      <link>https://www.oschina.net/news/368662</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368662</guid>
      <pubDate>Wed, 27 Aug 2025 02:06:21 GMT</pubDate>
    </item>
    <item>
      <title>阿里開源視頻生成模型 Wan2.2-S2V</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;阿里&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F5FwE7TvjzDYQabnpnMru6Q" target="_blank"&gt;宣佈&lt;/a&gt;開源全新多模態視頻生成模型通義萬相 Wan2.2-S2V，通過一張靜態圖片和一段音頻，可生成電影級數字人視頻，該模型單次生成的視頻時長可達分鐘級，提升數字人直播、影視製作、AI 教育等行業的視頻創作效率。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="256" src="https://oscimg.oschina.net/oscnet/up-da3fb7fdb3af007528ceab6ea0bd26a6902.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;根據介紹，Wan2.2-S2V 可驅動真人、卡通、動物、數字人等多種類型圖片，並支持肖像、半身以及全身等任意畫幅，上傳一段音頻後，模型就能讓圖片中的主體形象完成説話、唱歌和表演等動作。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;通義團隊基於通義萬相的通用視頻生成能力，融合了文本引導的全局運動控制和音頻驅動的細粒度局部運動，實現了複雜場景的音頻驅動視頻生成；引入 AdaIN 和 CrossAttention 兩種控制機制，實現了更準確更動態的音頻控制效果。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;生成時長上，Wan2.2-S2V 單次生成的視頻時長可達業界領先的分鐘級。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;Wan2.2-S2V 通過層次化幀壓縮技術，大幅降低了歷史幀的 Token 數量，通過該方式將 motion frames(歷史參考幀) 的長度從數幀拓展到 73 幀， 從而實現了穩定的長視頻生成效果。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;Wan2.2-S2V 還支持文本控制，輸入 Prompt 後還可對視頻畫面進行控制，實現鏡頭運動、角色軌跡和實體間互動，讓視頻主體的運動和背景的變化更豐富。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在模型訓練上，通義團隊構建了超 60 萬個片段的音視頻數據集，通過混合並行訓練進行全參數化訓練，充分挖掘了 Wan2.2-S2V 模型的性能。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;同時通過多分辨率訓練、支持模型多分辨率的推理，Wan2.2-S2V 可支持不同分辨率場景的視頻生成需求, 如豎屏短視頻、橫屏影視劇。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368660</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368660</guid>
      <pubDate>Wed, 27 Aug 2025 01:59:21 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>國務院：建立健全人工智能開源貢獻評價和激勵機制，鼓勵高校將開源貢獻納入學生學分認證和教師成果認定</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;國務院印發《關於深入實施「人工智能+」行動的意見》。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0826/191343_GXhC_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;其中提到，促進開源生態繁榮。支持人工智能開源社區建設，促進模型、工具、數據集等匯聚開放，培育優質開源項目。建立健全人工智能開源貢獻評價和激勵機制，鼓勵高校將開源貢獻納入學生學分認證和教師成果認定。支持企業、高校、科研機構等探索普惠高效的開源應用新模式。加快構建面向全球開放的開源技術體系和社區生態，發展具有國際影響力的開源項目和開發工具等。&lt;/p&gt; 
&lt;p&gt;原文：&lt;em&gt;https://www.gov.cn/zhengce/content/202508/content_7037861.htm&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368606</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368606</guid>
      <pubDate>Mon, 18 Aug 2025 11:14:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>騰訊遊戲發佈 AI 工具集 VISVISE，動畫製作效率提升 8 倍</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在近日舉行的科隆國際遊戲展上，騰訊遊戲正式發佈了名為 VISVISE 的遊戲創作 AI 工具集，該產品旨在大幅提升遊戲美術師的工作效率，減少重複性勞動。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;據介紹，VISVISE 工具集涵蓋動畫製作、模型製作、數字資產管理和智能 NPC 四個核心領域。其中最受關注的是 MotionBlink 動畫生成工具，該工具能夠根據用戶輸入的關鍵幀自動補全中間幀，快速生成完整的動畫序列。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;騰訊遊戲技術團隊表示，使用 MotionBlink 工具，原本需要數天時間製作的 10 秒動畫，現在僅需 4 秒即可完成 200 幀動畫的生成，效率提升達到 8 倍。該工具不僅能顯著減輕動畫師的工作負擔，生成的動畫質量也能在某些情況下達到光學動作捕捉的水平。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="211" src="https://oscimg.oschina.net/oscnet/up-66204834e57b0c6dc3f5aaa12b702a8cd61.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;傳統動畫製作流程中，手動補幀通常佔據動畫師大部分工作時間。MotionBlink 的推出有效解決了這一痛點。現場體驗的用戶反饋稱，該技術大幅降低了角色動畫製作的門檻，為小型開發團隊和獨立創作者提供了有力的技術支持。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;除 MotionBlink 外，VISVISE 還包含 GoSkinning 工具，專門用於解決 3D 角色蒙皮製作中的效率問題。傳統的蒙皮流程複雜且耗時，而 GoSkinning 通過 AI 技術實現了自動化處理。據測試數據顯示，處理一個包含 2 萬頂點的 3D 模型，使用該工具僅需 30 秒時間，大幅提升了工作效率。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="270" src="https://oscimg.oschina.net/oscnet/up-a489a914edc7b41eb74b895cdf1cb3f2679.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;騰訊遊戲方面表示，VISVISE 工具集的推出不僅是技術層面的突破，更代表了對整個遊戲開發工作流程的重新設計。通過 AI 技術的應用，美術師能夠將更多精力投入到創意設計和藝術創作中，而非繁重的技術操作。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;業內專家認為，隨着 AI 技術在遊戲開發領域的深入應用，傳統的內容製作模式正在發生根本性變化。像 VISVISE 這樣的工具集有望推動整個遊戲行業向更高效、更智能的方向發展。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;目前，騰訊遊戲尚未公佈 VISVISE 工具集的具體發佈時間和使用方式，但表示將繼續優化相關技術，為遊戲開發者提供更完善的 AI 輔助工具。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368600</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368600</guid>
      <pubDate>Mon, 18 Aug 2025 10:26:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>谷歌 Chrome 測試新特性：一鍵設為默認瀏覽器並固定至任務欄</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;谷歌正在測試一項 Chrome 新特性，用戶可以在設置頁面通過 &lt;strong&gt;「一鍵操作」&lt;/strong&gt; 將 Chrome 同時設為 &lt;strong&gt;Windows 11 的默認瀏覽器&lt;/strong&gt; 並 &lt;strong&gt;固定到任務欄上&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-c8ad80599c395d70a2597444696015e6f0c.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-5e646e64658fdffccb2016c1035431da87f.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwindowsreport.com%2Fgoogle-preps-one-click-option-to-make-chrome-default-and-pin-it-to-windows-11-taskbar%2F" target="_blank"&gt;根據科技媒體 Windows Report 的報道&lt;/a&gt;，該功能的開發細節已出現在 Chromium 代碼庫中，按鈕文字將改為 「將 Google Chrome 設為默認瀏覽器並固定到任務欄」（&lt;em&gt;「WIP Add option to pin to taskbar in settings.」&amp;nbsp;&lt;/em&gt;），且直接整合到設置頁面，表明該特性並非短期實驗，而是計劃長期保留 。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-97e9429156e4e8c29003511c9ae4be43892.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;此前，Mozilla 曾在 Firefox 瀏覽器的安裝階段短暫測試過類似提示，但未長期保留；相比之下，谷歌將功能深度融入瀏覽器設置中，未來可能會穩定上線 。&lt;/p&gt; 
&lt;p&gt;此外，該特性的推出可能與歐盟《數字市場法》有關，該法規要求用戶在 Windows 上設置非 Edge 瀏覽器為默認瀏覽器後，除非用戶明確拒絕，系統應自動將其固定到任務欄 。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368599</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368599</guid>
      <pubDate>Mon, 18 Aug 2025 10:24:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>微軟發佈免費 VM 轉換工具，支持 VMware 遷移至 Hyper-V</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;微軟近日推出免費公測版「VM 轉換工具」，可將 VMware 虛擬機遷移至基於 Hyper-V 的 Windows Server。該工具支持一次遷移最多 10 台虛擬機，兼容 BIOS 與 UEFI 系統，並保留引導配置。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-c10d73d298129ec7f5bdbdaeaddfad87663.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;該工具面向因合規或數據治理要求而傾向於本地部署的企業用戶，現已發佈於 Windows 管理中心。作為擴展安裝程序，該工具允許在不借助額外代理程序的情況下，完成 VMware 虛擬機向 Hyper-V 環境的遷移。&lt;/p&gt; 
&lt;p&gt;遷移過程分為多個階段：首先連接到現有虛擬化環境，並預檢查關鍵組件和配置，確保遷移條件達標，若發現問題需由 IT 管理員手動修復。隨後，工具利用變更塊跟蹤（CBT）技術創建源虛擬機的副本，並保持其正常運行。&lt;/p&gt; 
&lt;p&gt;在用戶確認關機後，工具進行第二次複製，將首次複製以來產生的增量數據同步到目標主機，確保遷移過程無中斷，減少停機時間，並在切換完成後保持系統一致性。&lt;/p&gt; 
&lt;p&gt;該工具一次可遷移多達 10 台虛擬機，並根據固件類型自動映射：BIOS 系統映射為 Hyper-V 的第一代（Generation 1），UEFI 系統映射為第二代（Generation 2），且完整保留引導配置。&lt;/p&gt; 
&lt;p&gt;詳情查看&amp;nbsp;&lt;em&gt;https://learn.microsoft.com/zh-cn/windows-server/manage/windows-admin-center/use/migrate-vmware-to-hyper-v&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368595</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368595</guid>
      <pubDate>Mon, 18 Aug 2025 10:14:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Docker Desktop 修復高危漏洞 CVE-2025-9074</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Docker 近日發佈更新，修復了 Docker Desktop 在 Windows 10/11 和 macOS 版本中存在的一處高危漏洞。該漏洞編號為 &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.cve.org%2FCVERecord%3Fid%3DCVE-2025-9074" target="_blank"&gt;CVE-2025-9074&lt;/a&gt;，評分高達 9.3/10，利用難度低，風險極高。官方已在 Docker Desktop v4.44.3 中完成修復，建議所有用戶立即升級。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0826/180337_8exE_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;問題源於容器可以在無需身份驗證的情況下連接至 192.168.65.7:2375 的 Docker Engine API。攻擊者可藉此繞過隔離機制，直接訪問宿主機文件系統。在驗證實驗中，研究團隊證明瞭任何容器只需發起一個 Web 請求，就能觸發漏洞並全面控制安裝 Docker Desktop 的主機。&lt;/p&gt; 
&lt;p&gt;在 macOS 上，由於系統對應用有額外的文件系統限制，漏洞主要影響 Docker Desktop 的控制權，進一步攻陷整個系統的難度較大。但在 Windows 10/11 環境下，由於缺乏類似的限制，攻擊風險更為嚴重。&lt;/p&gt; 
&lt;p&gt;值得注意的是，Linux 版 Docker Desktop 不受影響，因為其並未通過 TCP 連接依賴 Docker Engine API。要避免風險，用戶需儘快升級至 v4.44.3 版本。&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;https://docs.docker.com/desktop/release-notes/#4443&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368593</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368593</guid>
      <pubDate>Mon, 18 Aug 2025 10:03:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>國務院發佈《「人工智能+」行動意見》 2035 年邁入智能社會</title>
      <description/>
      <link>https://www.oschina.net/news/368591</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368591</guid>
      <pubDate>Mon, 18 Aug 2025 09:58:00 GMT</pubDate>
    </item>
    <item>
      <title>百度發佈 AI 智能搜索工具「梯子 AI 」</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;百度旗下的 AI 搜索 App「Tizzy.ai」完成了前期的測試，正式更名為「梯子 AI」。這款應用發佈於 8 月 10 日，當時名稱仍為「Tizzy.ai」，直到 8 月 21 日更新後改名為「梯子 AI」，版本號也直接從 1.0.0 跳到 1.2.0（官網 https://tizzy.baidu.com/）。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-0bf83789061d58fbed4ca9fa0cccf815533.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;「梯子 AI」定位為智能搜索助手，依託多個大模型能力開發而成，主打無廣告智能搜索，整合深度思考、資源檢索及影視娛樂功能（號稱海量資源）。&lt;/p&gt; 
&lt;p&gt;應用描述：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;梯子 AI 是百度融合多個大模型能力開發的 AI 智能搜索工具。提供自動思考和深度思考的雙模式智能引擎，全網分析給出精準答案；打造極簡搜索體驗，搜網址、搜天氣、搜攻略等輸入問題直達結果；提供熱門豐富的海量短劇，無廣告、無會員、流暢播放，打造極致觀劇體驗。&lt;/p&gt; 
 &lt;p&gt;梯子 AI 能幫你做什麼&lt;/p&gt; 
 &lt;p&gt;【AI 雙模智能搜索】支持自動思考與深度思考的雙模式智能引擎，通過全網信息精準分析，結合你的偏好提供個性化答案，搜索結果更高效、更精準。&lt;/p&gt; 
 &lt;p&gt;【極簡的交互體驗】簡潔搜索框，輸入問題直達答案，沒有任何推廣信息，提供極致的搜索體驗。&lt;/p&gt; 
 &lt;p&gt;【豐富的短劇生態】涵蓋都市熱血、玄幻仙俠、逆襲反轉、穿越重生等多種類型，擁有豐富的熱門短劇，暢看無阻。&lt;/p&gt; 
 &lt;p&gt;【沉浸式觀劇體驗】觀看短劇過程無廣告、無會員、加速緩衝，提供邊看邊搜，重新設計每個細節，打造極致的觀劇體驗。&lt;/p&gt; 
&lt;/blockquote&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368590</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368590</guid>
      <pubDate>Mon, 18 Aug 2025 09:55:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>微軟旗下開源文檔數據庫 Do​​cumentDB 加入 Linux 基金會</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;DocumentDB 是微軟基於 PostgreSQL 構建的兼容 MongoDB 的文檔數據庫，目前已用於支持基於 vCore 的 Azure Cosmos DB for MongoDB 實例。今年年初，微軟開源了 DocumentDB，因為它相信一個完全開源、兼容 MongoDB 的文檔數據庫能夠實現。&lt;/p&gt; 
&lt;p&gt;出乎所有人的意料，微軟竟然以最為寬鬆的 MIT 許可證&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdevblogs.microsoft.com%2Fcosmosdb%2Fdocumentdb-is-gaining-momentum-in-the-open-source-database-world%2F" target="_blank"&gt;將其開源&lt;/a&gt;，該許可證允許開發者和組織將其無限制地集成到他們的解決方案中。DocumentDB 項目開源一週內，就獲得了 1000 個 GitHub 星標、近 50 個 fork 以及多個 Pull 請求。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-fe1c0989daaba4a35b2f8e816475b799172.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;DocumentDB 因其原生實現的面向文檔的 NoSQL 數據庫而廣受開發者歡迎，它基於 PostgreSQL 框架，支持對 BSON（二進制 JSON）數據類型進行 CRUD（創建、讀取、更新、刪除）操作。此外，DocumentDB 還支持全文搜索、地理空間查詢和矢量搜索。&lt;/p&gt; 
&lt;p&gt;今天，微軟&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopensource.microsoft.com%2Fblog%2F2025%2F08%2F25%2Fdocumentdb-joins-the-linux-foundation%2F" target="_blank"&gt;宣佈&lt;/a&gt;DocumentDB 將成為 Linux 基金會的一部分。微軟希望此舉能夠為 NoSQL 數據庫創建一個開放標準。微軟表示，它致力於與 MongoDB 驅動程序 100% 兼容，以確保文檔數據庫生態系統蓬勃發展。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0826/174814_Mz6m_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Azure Cosmos DB 副總裁 Kirill Gavrylyuk 表示：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;加入 Linux 基金會將為 DocumentDB 創建一個獨立的身份，併為任何數據庫提供商提供一個渠道來為我們的使命做出貢獻。此外，Postgres 繼續被譽為最受歡迎的平台，並將繼續作為項目的支柱。對於 DocumentDB 而言，開源 Postgres 將比 Postgres 的分支版本更受青睞。Linux 基金會將確保 DocumentDB 遵守這些管理原則，以保持一致性。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Linux 基金會對 DocumentDB 項目的治理將確保供應商中立，並維護 DocumentDB 始終堅持 PostgreSQL 優先的承諾。Linux 基金會執行董事 Jim Zemlin 對 DocumentDB 加入基金會感到非常興奮。他&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.linuxfoundation.org%2Fpress%2Flinux-foundation-welcomes-documentdb-to-advance-open-developer-first-nosql-innovation" target="_blank"&gt;表示&lt;/a&gt;：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;DocumentDB 填補了文檔數據庫生態系統中的一個關鍵空白，吸引了眾多貢獻者、用戶和擁護者。更令人興奮的是，它為基於文檔的應用程序提供了一個開放標準，就像 SQL 為關係數據庫所做的那樣。通過加入 Linux 基金會，DocumentDB 確保了其開源未來，並助力 NoSQL 數據庫標準和社區驅動的創新開闢了一條新道路。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;通過在 Linux 基金會的治理下，DocumentDB 能夠確保其開源未來並推動社區創新。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368589/ms-documentdb-joins-the-linux-foundation</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368589/ms-documentdb-joins-the-linux-foundation</guid>
      <pubDate>Mon, 18 Aug 2025 09:49:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Fenster - 最精簡的跨平台 GUI 庫</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                        
                                                                                    &lt;p style="text-align:start"&gt;&lt;span&gt;&lt;span&gt;&lt;span style="color:#1f2328"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style="background-color:#ffffff"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Fenster /ˈfɛnstɐ/——德語中「window」的意思。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span style="color:#1f2328"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style="background-color:#ffffff"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;這個庫提供了最簡潔、最實用的跨平台 2D 畫布顯示方式。只需幾行代碼，你就能實現跨平台的鍵盤/鼠標輸入和音頻播放。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p style="text-align:start"&gt;&lt;strong&gt;特點：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;具有指定大小和標題的單一應用程序窗口。&lt;/li&gt;
&lt;li&gt;應用程序生命週期和系統事件均自動處理。&lt;/li&gt;
&lt;li&gt;最小 24 位 RGB 幀緩衝區。&lt;/li&gt;
&lt;li&gt;跨平台鍵盤事件（鍵碼）。&lt;/li&gt;
&lt;li&gt;跨平台鼠標事件（X/Y + 鼠標點擊）。&lt;/li&gt;
&lt;li&gt;跨平台計時器具有穩定的 FPS 速率。&lt;/li&gt;
&lt;li&gt;跨平台音頻播放（WinMM、CoreAudio、ALSA）。&lt;/li&gt;
&lt;li&gt;簡單的輪詢 API，無需回調或多線程（如 Arduino/Processing）。&lt;/li&gt;
&lt;li&gt;一個約 300LOC 的 C99 頭文件，易於理解和擴展。&lt;/li&gt;
&lt;li&gt;Go 綁定（&lt;code&gt;import "github.com/zserge/fenster"&lt;/code&gt;，參見&amp;nbsp;&lt;a href="https://pkg.go.dev/github.com/zserge/fenster"&gt;godoc&lt;/a&gt;）&lt;/li&gt;
&lt;li&gt;Zig 綁定（參見&amp;nbsp;&lt;a href="https://github.com/zserge/fenster/blob/main/examples/minimal-zig"&gt;examples/minimal-zig&lt;/a&gt;）&lt;/li&gt;
&lt;li&gt;Lua 綁定（參見&amp;nbsp;&lt;a href="https://github.com/jonasgeiler/lua-fenster"&gt;https://github.com/jonasgeiler/lua-fenster&lt;/a&gt;）&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zserge/fenster/blob/main/examples/doom-c"&gt;可以運行 Doom&lt;/a&gt;！&lt;/li&gt;
&lt;/ul&gt;

&lt;p style="text-align:start"&gt;&lt;strong&gt;示例&lt;/strong&gt;&lt;/p&gt;

&lt;div style="text-align:start"&gt;
&lt;pre&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style="background-color:#f6f8fa"&gt;&lt;span style="color:#1f2328"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style="color:#1f2328"&gt;&lt;span style="background-color:#f6f8fa"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style="color:#59636e"&gt;// main.c&lt;/span&gt;&lt;/span&gt;
&lt;span&gt;&lt;span style="color:#cf222e"&gt;#include&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span style="color:#0a3069"&gt;"fenster.h"&lt;/span&gt;&lt;/span&gt;
&lt;span&gt;&lt;span style="color:#cf222e"&gt;#define&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span style="color:#0550ae"&gt;W&lt;/span&gt;&lt;/span&gt; 320
&lt;span&gt;&lt;span style="color:#cf222e"&gt;#define&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span style="color:#0550ae"&gt;H&lt;/span&gt;&lt;/span&gt; 240
&lt;span&gt;&lt;span style="color:#1f2328"&gt;int&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span style="color:#6639ba"&gt;main&lt;/span&gt;&lt;/span&gt;() {
  &lt;span&gt;&lt;span style="color:#1f2328"&gt;uint32_t&lt;/span&gt;&lt;/span&gt; &lt;span&gt;buf&lt;/span&gt;[&lt;span&gt;&lt;span style="color:#0550ae"&gt;W&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span style="color:#0550ae"&gt;*&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span style="color:#0550ae"&gt;H&lt;/span&gt;&lt;/span&gt;];
  &lt;span&gt;&lt;span style="color:#cf222e"&gt;struct&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span style="color:#1f2328"&gt;fenster&lt;/span&gt;&lt;/span&gt; &lt;span&gt;f&lt;/span&gt; &lt;span&gt;&lt;span style="color:#0550ae"&gt;=&lt;/span&gt;&lt;/span&gt; { .&lt;span&gt;&lt;span style="color:#0550ae"&gt;title&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span style="color:#0550ae"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span style="color:#0a3069"&gt;"hello"&lt;/span&gt;&lt;/span&gt;, .&lt;span&gt;&lt;span style="color:#0550ae"&gt;width&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span style="color:#0550ae"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span style="color:#0550ae"&gt;W&lt;/span&gt;&lt;/span&gt;, .&lt;span&gt;&lt;span style="color:#0550ae"&gt;height&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span style="color:#0550ae"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span style="color:#0550ae"&gt;H&lt;/span&gt;&lt;/span&gt;, .&lt;span&gt;&lt;span style="color:#0550ae"&gt;buf&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span style="color:#0550ae"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span&gt;buf&lt;/span&gt; };
  &lt;span&gt;&lt;span style="color:#6639ba"&gt;fenster_open&lt;/span&gt;&lt;/span&gt;(&lt;span&gt;&lt;span style="color:#0550ae"&gt;&amp;amp;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;f&lt;/span&gt;);
  &lt;span&gt;&lt;span style="color:#cf222e"&gt;while&lt;/span&gt;&lt;/span&gt; (&lt;span&gt;&lt;span style="color:#6639ba"&gt;fenster_loop&lt;/span&gt;&lt;/span&gt;(&lt;span&gt;&lt;span style="color:#0550ae"&gt;&amp;amp;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;f&lt;/span&gt;) &lt;span&gt;&lt;span style="color:#0550ae"&gt;==&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span style="color:#0550ae"&gt;0&lt;/span&gt;&lt;/span&gt;) {
    &lt;span&gt;&lt;span style="color:#cf222e"&gt;for&lt;/span&gt;&lt;/span&gt; (&lt;span&gt;&lt;span style="color:#1f2328"&gt;int&lt;/span&gt;&lt;/span&gt; &lt;span&gt;i&lt;/span&gt; &lt;span&gt;&lt;span style="color:#0550ae"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span style="color:#0550ae"&gt;0&lt;/span&gt;&lt;/span&gt;; &lt;span&gt;i&lt;/span&gt; &lt;span&gt;&lt;span style="color:#0550ae"&gt;&amp;lt;&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span style="color:#0550ae"&gt;W&lt;/span&gt;&lt;/span&gt;; &lt;span&gt;i&lt;/span&gt;&lt;span&gt;&lt;span style="color:#0550ae"&gt;++&lt;/span&gt;&lt;/span&gt;) {
      &lt;span&gt;&lt;span style="color:#cf222e"&gt;for&lt;/span&gt;&lt;/span&gt; (&lt;span&gt;&lt;span style="color:#1f2328"&gt;int&lt;/span&gt;&lt;/span&gt; &lt;span&gt;j&lt;/span&gt; &lt;span&gt;&lt;span style="color:#0550ae"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span style="color:#0550ae"&gt;0&lt;/span&gt;&lt;/span&gt;; &lt;span&gt;j&lt;/span&gt; &lt;span&gt;&lt;span style="color:#0550ae"&gt;&amp;lt;&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span style="color:#0550ae"&gt;H&lt;/span&gt;&lt;/span&gt;; &lt;span&gt;j&lt;/span&gt;&lt;span&gt;&lt;span style="color:#0550ae"&gt;++&lt;/span&gt;&lt;/span&gt;) {
        &lt;span&gt;&lt;span style="color:#6639ba"&gt;fenster_pixel&lt;/span&gt;&lt;/span&gt;(&lt;span&gt;&lt;span style="color:#0550ae"&gt;&amp;amp;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;f&lt;/span&gt;, &lt;span&gt;i&lt;/span&gt;, &lt;span&gt;j&lt;/span&gt;) &lt;span&gt;&lt;span style="color:#0550ae"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span style="color:#6639ba"&gt;rand&lt;/span&gt;&lt;/span&gt;();
      }
    }
  }
  &lt;span&gt;&lt;span style="color:#6639ba"&gt;fenster_close&lt;/span&gt;&lt;/span&gt;(&lt;span&gt;&lt;span style="color:#0550ae"&gt;&amp;amp;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;f&lt;/span&gt;);
  &lt;span&gt;&lt;span style="color:#cf222e"&gt;return&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span style="color:#0550ae"&gt;0&lt;/span&gt;&lt;/span&gt;;
}&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;

&lt;p style="text-align:start"&gt;&lt;span&gt;&lt;span&gt;&lt;span style="color:#1f2328"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style="background-color:#ffffff"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;編譯並運行：&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;div style="text-align:start"&gt;
&lt;pre&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style="background-color:#f6f8fa"&gt;&lt;span style="color:#1f2328"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style="color:#1f2328"&gt;&lt;span style="background-color:#f6f8fa"&gt;&lt;code&gt;# Linux
cc main.c -lX11 -lasound -o main &amp;amp;&amp;amp; ./main
# macOS
cc main.c -framework Cocoa -framework AudioToolbox -o main &amp;amp;&amp;amp; ./main
# windows
cc main.c -lgdi32 -lwinmm -o main.exe &amp;amp;&amp;amp; main.exe&lt;/code&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;

                                                                    &lt;/div&gt;
                                                                </description>
      <link>https://www.oschina.net/p/fenster</link>
      <guid isPermaLink="false">https://www.oschina.net/p/fenster</guid>
      <pubDate>Mon, 18 Aug 2025 09:46:00 GMT</pubDate>
    </item>
    <item>
      <title>Firefox 帶來 PWA 實驗性實現</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Moziilla 宣佈為 Firefox Windows 用戶在 142 版本中帶來一個新的實驗性功能：漸進式 Web 應用（PWA）。該功能自 2018 年 Chrome 70 版本以來，一直就存在於桌面版 Chrome 中。&lt;/p&gt; 
&lt;p&gt;PWA 在 Firefox 中的發展歷程相當坎坷，早在 Firefox 73（Nightly）時期，Mozilla 就嘗試過一個名為 「Site-Specific Browsers（SSB）」 的 PWA 實驗性實現，但這一功能從未完全開發完成，並在 2021 年 1 月被 Mozilla 移除。&lt;/p&gt; 
&lt;p&gt;當時，Mozilla 解釋説，該功能存在 「多個已知問題」，並且保留它會佔用 Firefox 團隊在漏洞排查上的時間。&lt;/p&gt; 
&lt;p&gt;今年 3 月，Mozilla 發佈了 Firefox Nightly 141 版本，重新加入了這一功能，並將其命名為 「Taskbar Tabs」。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-7e987d26d0238edd293a1f64eb53f150ab1.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-8d3309d0755d9440b38d337973c2381c7ca.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Firefox 產品經理 David Rubino 解釋説，這一新的實現方式在設計上有所不同，Web 應用將保留 Firefox 的主要工具欄，包括地址欄、擴展程序和書籤，以確保用戶仍然感覺自己是在瀏覽器中。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368586</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368586</guid>
      <pubDate>Mon, 18 Aug 2025 09:43:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>書生髮布 InternVL 3.5 最新視覺全系列模型</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;書生髮布了最新的視覺模型 InternVL 3.5 全系列模型，從 1B 到 241B 共 8 個尺寸。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-7fb0fd8578a3037d507a1059e08223c437f.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;根據評測結果，書生 3.5 最高尺寸 241B 在視覺模型裏的表現僅次於商業版的 GPT-5 和 Gemini 2.5 Pro。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-595c729823f646e6dbcd8b05be0bb57277f.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-13518e6aab25fa7275c2777b42debccf8de.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;所有模型均已發佈到 Hugging Face&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;https://huggingface.co/collections/OpenGVLab/internvl35-68ac87bd52ebe953485927fb&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;該模型的技術亮點：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Cascade Reinforcement Learning（Cascade RL）&lt;/strong&gt;：採用「離線 RL + 在線 RL」兩階段策略，實現更加穩健收斂和精細對齊，從而顯著增強模型的推理能力，在 MMMU 和 MathVista 等任務上表現提升明顯。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Visual Resolution Router（ViR）&lt;/strong&gt;：動態調整視覺 token 的分辨率，兼顧性能與效率，使視覺理解更加靈活高效。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Decoupled Vision-Language Deployment（DvD）&lt;/strong&gt;：將視覺編碼器與語言模型分開部署至不同 GPU，有效平衡資源負載，提升推理速度。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;在推理性能提升高達 16.0%（整體推理任務中），同時相較於 InternVL3，實現了&amp;nbsp;&lt;strong&gt;4.05× 的推理速度加速。&lt;/strong&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368585</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368585</guid>
      <pubDate>Mon, 18 Aug 2025 09:29:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>騰訊開源 tRPC-Agent-Go：讓 Go 開發者輕鬆構建智能 AI 應用</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;騰訊 tRPC 團隊之前開源了 A2A 開發框架 tRPC-A2A-Go 和 MCP 開發框架 tRPC-MCP-Go，現在進一步&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FrUcJ_9D1gVrdJUmzZP63zQ" target="_blank"&gt;推出&lt;/a&gt;&amp;nbsp;tRPC-Agent-Go&amp;nbsp;開發框架，實現 Go 語言 AI 生態開發框架的閉環。&lt;/p&gt; 
&lt;p&gt;公告稱，當前主流 Agent 框架（AutoGen、CrewAI 、Agno、ADK 等）大部分都是基於 Python，而&amp;nbsp;Go 在微服務、併發與部署方面有天然優勢，Go 在騰訊內部也有大規模應用，業界基於 Go 語言的 Agent 框架較少，大部分都是編排式的 workflow 框架，缺少真正的「去中心化、可協作、能湧現」的自主多 Agent 能力。tRPC-Agent-Go 直接利用 Go 的高併發與 tRPC 生態，把 LLM 的推理、協商和自適應性帶到 Go 場景，滿足複雜業務對「智能+性能」的雙重需求。&lt;/p&gt; 
&lt;p&gt;tRPC-Agent-Go 採用模塊化架構設計，由多個核心組件組成，組件都可插拔，通過事件驅動機制實現組件間的解耦通信，支持 callback 插入自定義邏輯：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Agent：核心執行單元，負責處理用戶輸入並生成響應&lt;/li&gt; 
 &lt;li&gt;Runner：Agent 的執行器，負責管理執行流程，串聯 Session/Memory Service 等能力&lt;/li&gt; 
 &lt;li&gt;Model：支持多種 LLM 模型（OpenAI、DeepSeek 等）&lt;/li&gt; 
 &lt;li&gt;Tool：提供各種工具能力（Function、MCP、DuckDuckGo 等）&lt;/li&gt; 
 &lt;li&gt;Session：管理用戶會話狀態和事件&lt;/li&gt; 
 &lt;li&gt;Memory：記錄用戶的長期記憶和個性化信息&lt;/li&gt; 
 &lt;li&gt;Knowledge：實現 RAG 知識檢索能力&lt;/li&gt; 
 &lt;li&gt;Planner：提供 Agent 的計劃和推理能力&lt;/li&gt; 
 &lt;li&gt;CodeExecutor：提供 Agent 代碼執行能力，支持 Local，Container 等模式&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img height="357" src="https://oscimg.oschina.net/oscnet/up-95bdd4a5582b1c5df0538b32ab59b27150e.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" height="812" src="https://oscimg.oschina.net/oscnet/up-8a1ddd94dd74790efd81c8792dd5ce17680.webp" width="300" referrerpolicy="no-referrer"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h4&gt;&lt;strong&gt;核心特點&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;多樣化 Agent 系統&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;LLMAgent：基於大語言模型，支持工具調用和推理&lt;/li&gt; 
 &lt;li&gt;ChainAgent：鏈式執行，支持多步驟任務分解&lt;/li&gt; 
 &lt;li&gt;ParallelAgent：並行處理，支持多專家協作&lt;/li&gt; 
 &lt;li&gt;CycleAgent：循環迭代，支持自我優化&lt;/li&gt; 
 &lt;li&gt;GraphAgent：圖工作流，兼容現有編排習慣&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;豐富工具生態&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;內置常用工具&lt;/li&gt; 
 &lt;li&gt;支持 Function、MCP 協議等多種擴展方式&lt;/li&gt; 
 &lt;li&gt;靈活的工具組合和調用策略&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;智能會話管理&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;支持 Redis 和內存存儲的會話持久化&lt;/li&gt; 
 &lt;li&gt;長期記憶和個性化信息保持&lt;/li&gt; 
 &lt;li&gt;RAG 檢索增強生成能力&lt;/li&gt; 
 &lt;li&gt;實時事件驅動架構&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;全鏈路可觀測性&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;OpenTelemetry 全鏈路追蹤和性能監控&lt;/li&gt; 
 &lt;li&gt;可視化調試界面和實時監控&lt;/li&gt; 
 &lt;li&gt;結構化日誌和錯誤追蹤&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368583</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368583</guid>
      <pubDate>Mon, 18 Aug 2025 09:19:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>貨拉拉開源兩款三方庫</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;貨拉拉開源了兩款實用三方庫 —— AspectPro Aop Plugin 和 page-spy-harmony，直擊應用開發過程中「代碼耦合高」「遠程調試難」等高頻痛點。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#e6e6e6; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;在複雜的應用開發過程中，日誌記錄、性能監控、權限校驗等功能雖然往往與核心業務邏輯關係不大，但卻必不可少，但這些功能的代碼散佈於代碼架構的各個角落。這種現象易導致代碼耦合度增高、業務邏輯不夠純粹，不僅降低了代碼的可讀性，也為後期的維護帶來了不小的挑戰。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#e6e6e6; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;針對這一典型問題，貨拉拉推出了輕量級鴻蒙運行時 hook 框架 AspectPro Aop Plugin，並同步開源其編譯時代碼修改插件 aspect-pro-plugin，兩者配合使用可實現任意代碼 hook 操作。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#e6e6e6; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;AspectPro Aop Plugin 對齊鴻蒙系統的 AOP 能力，開發者無需關心靜態方法限制，即可對如按鈕點擊事件、鏈式構造類方法、不可寫方法等多類行為進行精準 hook，並靈活更改參數與返回值；而 aspect-pro-plugin 則在編譯階段支持多種代碼掃描、替換與導包策略，支持自定義配置規則。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#e6e6e6; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;通過引入 AspectPro Aop Plugin，開發者可將橫切邏輯獨立封裝、與業務代碼解耦，從而顯著提升代碼結構清晰度與維護效率，避免「邏輯混雜」導致的後期維護成本激增問題，提升開發效率與代碼質量。這一工具特別適閤中大型項目中對代碼規範與開發協作要求較高的團隊使用。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#e6e6e6; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;img height="293" src="https://oscimg.oschina.net/oscnet/up-4888079610c92e960fcf7334fe2f467752e.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#e6e6e6; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;遠程可視化調試解決方案 page-spy-harmony 採用客戶端-服務端的架構：在應用中集成一個輕量級的設備端 SDK，該 SDK 會在運行時採集關鍵數據；同時，開發者可以通過瀏覽器訪問一個功能豐富的 Web 端控制枱，實時接收並可視化展示來自設備端的數據。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#e6e6e6; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;通過 page-spy-harmony，開發者可以一目瞭然地遠程查看應用的運行時信息，包括詳細的日誌、網絡請求往來以及 AppStorage 中的數據等。這不僅極大地提升了調試效率，還簡化了遠程協作的流程。無論是團隊成員異地協作，還是遠程協助用戶或測試人員排查問題，page-spy-harmony 都能提供有力支持，加速問題的定位與修復。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368577</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368577</guid>
      <pubDate>Mon, 18 Aug 2025 08:53:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>OpenZFS 2.3.4 發佈：支持 Linux 6.16 內核、引入 zfs rewrite 子命令</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;OpenZFS 2.3.4 已正式發佈，最大亮點在於支持 Linux 6.16 內核，以及引入&amp;nbsp;zfs rewrite 子命令。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;1. 支持 Linux 6.16 內核&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;OpenZFS 2.3.4 新增對最新 Linux 6.16 穩定內核的支持，而此前 2.3.3 版本僅支持到 6.15。它仍兼容 Linux 4.18 及更高版本，以及 FreeBSD 13.3 及更新版本（包括即將發佈的 FreeBSD 15.0）。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2. 引入 &lt;code&gt;zfs rewrite&lt;/code&gt; 子命令&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;新增的 &lt;code&gt;zfs rewrite&lt;/code&gt; 命令允許以原樣內容重寫指定文件，但可更改其存儲位置、壓縮方式、校驗和、去重策略、鏡像副本數等配置參數。相比傳統的讀寫拷貝、發送/接收、重命名等方案，此命令更高效（無需進入用戶空間數據拷貝），尤其對 &lt;code&gt;sync=always&lt;/code&gt; 數據集效果顯著（無需再寫 ZIL），且在數據鎖正常範圍內可在負載下安全執行，不修改文件的修改時間等元數據。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;3.其他修復與更新&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;本次發佈還包含一些 FreeBSD 平台的修復、打包方面更新以及其他若干較小的 bug 修復。&lt;/p&gt; 
&lt;p&gt;詳情查看&amp;nbsp;&lt;em&gt;https://github.com/openzfs/zfs/releases/tag/zfs-2.3.4&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368576</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368576</guid>
      <pubDate>Mon, 18 Aug 2025 08:44:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>比 Cursor 更快更穩定的 Coding Agent？</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;p&gt;搞了 2 年直播，我也是搞出名堂來了。&lt;/p&gt; 
&lt;p&gt;張宏波説要來我們這裏搞直播，聊一聊 Coding Agent。&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;張宏波是誰？&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;他是編程語言領域的專家，是 OCaml 語言的前核心開發人員，&lt;/span&gt;OCaml 編譯器獲得過 2023 年 ACM SIGPLAN 編程語言軟件獎。&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;此外，他還創造了&lt;/span&gt;&lt;span style="color:#000000"&gt;編程語言&lt;/span&gt;&amp;nbsp;&lt;span style="color:#000000"&gt;ReScript，被 Meta、谷歌、育碧、TinyMCE 等多個公司商用。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;就這成就，已經值得吹一輩子了吧？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;但張宏波不一樣，他覺得很遺憾。&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;因為 ReScript 具備相當的技術實力，並且遠超一些同行，但是相較於微軟的 TypeScript 或者谷歌的 Dart，ReScript 的影響力遠沒有達到它應有的高度。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;他想要打造的，是&lt;span style="background-color:#ffffff; color:#222222"&gt;一款現象級的編程語言&lt;/span&gt;。&lt;/p&gt; 
&lt;p&gt;一直以來，張宏波都不甘平庸。就連他當初考到&lt;span style="background-color:#ffffff; color:#222222"&gt;清華大學電氣工程及自動化系&lt;/span&gt;，都説是因為高考發揮失常才被調劑過去的。他真正想進的，是他一年後&lt;span style="background-color:#ffffff; color:#222222"&gt;成功轉入的清華電子系。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;span style="color:#000000"&gt;所以在 2022 年，張宏波結束了他在 Meta 的 5 年職業生涯，來到了&lt;/span&gt;&lt;span&gt;粵港澳大灣區數字經濟研究院（&lt;/span&gt;IDEA 研究院&lt;span&gt;）&lt;/span&gt;組建了基礎軟件中心，從零開始創立了 MoonBit。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;這裏插一句，張宏波加入 IDEA 研究院，源於沈向洋（Harry Shum）拋出的橄欖枝。&lt;/p&gt; 
&lt;p&gt;早年在曾在微軟亞洲研究院實習時，沈向洋就是他的導師，並在實習結束後力薦張宏波前往美國繼續深造。當然，張宏波之後也踏上了前往美國讀博的旅程（後來還有中止讀博一事今天且按下不表）。&lt;/p&gt; 
&lt;p&gt;彼時，沈向洋已經是粵港澳大灣區數字經濟研究院創始人及理事長。張宏波也從當年的學生，成長為獨當一面的頂尖專家。&lt;/p&gt; 
&lt;p&gt;這就不得不感嘆，命運的迴響如此奇妙。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;説回張宏波一手創立的 MoonBit——一個專門為 AI&amp;nbsp; 設計的開源編程語言。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;換句話來説，這個編程語言的目的，是讓 AI 用起來最得心應手、最不容易出錯。&lt;/p&gt; 
&lt;p&gt;現在，大家都在用 AI Coding 工具來寫代碼，但不論用的是 Java，還是 Python，亦或是 Rust 等其他主流編程語言，基本上已經定型了，只能在現有基礎上「嫁接」AI 能力。所以，最終 AI 確實是把代碼寫出來了，但問題是怎麼維護呢？&lt;/p&gt; 
&lt;p&gt;MoonBit 就不一樣。它的語法、類型系統、錯誤處理機制等，在設計之初就深度考慮瞭如何讓 AI 更容易地理解、生成和驗證代碼，從而保證了代碼的可維護性。&lt;/p&gt; 
&lt;p&gt;就連&amp;nbsp;&lt;span style="background-color:#ffffff; color:#222222"&gt;JavaScript 標準委員會聯席主席 Rob Palmer、Vue 和 Vite 之父尤雨溪等知名技術專家都屢次公開誇讚。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;最近，張宏波還帶領團隊，開發了一個 Coding Agent——MoonBit Pilot。據説比&lt;/strong&gt;&lt;span style="background-color:#ffffff; color:#000000"&gt;&amp;nbsp;Cursor 還更快、更穩定！&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="250" src="https://oscimg.oschina.net/oscnet/up-4a9f06ab1db5e6ac343b22234959e7d805f.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;這是他&lt;span style="background-color:#ffffff; color:#333333"&gt;從底層設計的一整套 AI 原生的開發者工具，包括編程語言的設計、編譯器、調試器、包管理等，並在各個環節給予大模型最有效的反饋。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="background-color:#ffffff; color:#333333"&gt;也許你已經發現了，跟&lt;/span&gt;編程語言&amp;nbsp;MoonBit&amp;nbsp; 一樣，也是 AI 原生。&lt;/p&gt; 
&lt;p&gt;所以，MoonBit Pilot 不僅可以生成所有編程語言的代碼，而且針對 MoonBit 編程語言的優勢極大。畢竟，還有誰能比自己人更清楚 MoonBit 嘛！&lt;/p&gt; 
&lt;p&gt;總之，不管是&amp;nbsp;MoonBit ，還是&amp;nbsp;MoonBit Pilot ，聽來都不簡單。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;現在，張宏波用 MoonBit Pilot 等 AI 工具寫 MoonBit。過去一週，他用閒暇時間寫了 309 個高質量的 Commits，實現了相當於過去一個優秀程序員一年的工作量。&lt;/strong&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;img height="646" src="https://oscimg.oschina.net/oscnet/up-2658c358aed8eef15d5387c10698412aeeb.png" width="500" referrerpolicy="no-referrer"&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;p&gt;他是怎麼做到的？&lt;strong&gt;8 月 29 日晚，&lt;/strong&gt;IDEA 研究院基礎軟件中心首席科學家、MoonBit 團隊負責人張宏波，將做客開源中國《技術領航》欄目直播間：&lt;/p&gt; 
&lt;div&gt; 
 &lt;ul&gt; 
  &lt;li&gt; 
   &lt;div&gt;
     講解 AI 原生編程語言 —— MoonBit 的底層設計 
   &lt;/div&gt; &lt;/li&gt; 
  &lt;li&gt; 
   &lt;div&gt;
     Coding Agent —— MoonBit Pilot 的最新進展 
   &lt;/div&gt; &lt;/li&gt; 
  &lt;li&gt; 
   &lt;div&gt;
     實操演示：MoonBit Pilot 零幹預輔助完成軟件庫 
   &lt;/div&gt; &lt;/li&gt; 
  &lt;li&gt; 
   &lt;div&gt;
     大規模代碼重構關鍵：原生語義查找+分段編碼 
   &lt;/div&gt; &lt;/li&gt; 
  &lt;li&gt; 
   &lt;div&gt;
     Q&amp;amp;A 環節 （15min） 
   &lt;/div&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;div&gt;
   我們還會在直播間，隨機抽取 5 名直播間評論區互動的幸運用戶，贈送網頁版 MoonBit Pilot&amp;nbsp;權限哦~ 
 &lt;/div&gt; 
 &lt;div&gt;
   &amp;nbsp; 
 &lt;/div&gt; 
 &lt;div&gt; 
  &lt;strong&gt;微信掃碼，預約直播：&lt;/strong&gt; 
  &lt;br&gt; &amp;nbsp; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;img height="8795" src="https://oscimg.oschina.net/oscnet/up-6a5dbdd0dde7efa92a215e410cc36f17215.jpg" width="3402" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;p&gt;&lt;strong&gt;直播福利：&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;福袋抽獎：直播中將有 5 輪抽獎，參與就有機會獲得 OSC T 恤、馬建倉蛇年公仔（限量版）、代碼聖盃、馬克杯、冰箱貼、前沿技術書籍等。&lt;/p&gt; 
&lt;/div&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img alt="up-d0ddd08ceeff2b5526d3def6537a6ac649b.png" height="253" src="https://oscimg.oschina.net/oscnet/up-d0ddd08ceeff2b5526d3def6537a6ac649b.png" width="400" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;hr&gt; 
 &lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;《技術領航》是開源中國 OSCHINA 推出的一檔直播欄目，旨在為&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;開源軟件、商業產品、前沿技術、知名品牌活動等各類項目&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;提供一個展示平台，基本上每週五晚上開播&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;欄目邀請項目的創始人、核心團隊成員或資深用戶作為嘉賓，通過路演式直播分享項目的亮點和經驗，有助於提高項目的知名度，吸引更多的用戶和開發者關注。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
 &lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span&gt;&lt;span style="background-color:#ffffff"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;如果你手上也有好的項目，想要跟同行交流分享，歡迎聯繫我，欄目隨時開放～&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
 &lt;p style="color:#333333; margin-left:0px; margin-right:0px; text-align:center"&gt;&lt;img height="537" src="https://oscimg.oschina.net/oscnet/up-4dd54c1b0b817689ceefa15aa66d79cfae8.png" width="400" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;/div&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/3859945/blog/18689590</link>
      <guid isPermaLink="false">https://my.oschina.net/u/3859945/blog/18689590</guid>
      <pubDate>Mon, 18 Aug 2025 08:34:00 GMT</pubDate>
      <author>原創</author>
    </item>
  </channel>
</rss>
