<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - news - 繁體中文（香港）</title>
    <link>https://www.oschina.net/news/project</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news" rel="self" type="application/rss+xml"/>
    <description>已對該 RSS 進行格式化操作：中英字符之間插入空格、使用直角引號、標點符號修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-hk</language>
    <lastBuildDate>Thu, 11 Sep 2025 21:41:52 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>快手發佈開源多模態大模型 Kwai Keye-VL-1.5</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;快手近日正式發佈多模態大語言模型 Keye-VL-1.5-8B。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0911/195624_2HB4_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;https://huggingface.co/Kwai-Keye/Keye-VL-1_5-8B&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;據介紹，與之前的版本相比，Keye-VL-1.5 的綜合性能實現顯著提升，尤其在基礎視覺理解能力方面，包括視覺元素識別、推理能力以及對時序信息的理—表現尤為突出。Keye-VL-1.5 在同等規模的模型中表現出色，甚至超越了一些閉源模型如 GPT-4o。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-99ae906bd3166733efda4ecc20d5895a63d.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Keye-VL-1.5 採用四階段漸進式訓練流水線，以系統化方式提升模型性能。在視覺編碼器預訓練階段，使用 SigLIP-400M 權重初始化 ViT，並通過 SigLIP 對比損失持續預訓練以適應內部數據分佈。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-09d1b4cf72c51b253843c3541b0130725c4.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;第一階段重點優化投影 MLP 層，實現跨模態特徵的穩固對齊；第二階段解凍全部參數進行端到端多任務預訓練，顯著增強基礎視覺理解能力；第三階段進行退火訓練，利用高質量數據微調模型，彌補上一階段中高質量樣本接觸不足的問題，同時將序列長度擴展至 128K、調整 RoPE 逆頻率配置，並引入長視頻、長文本和大尺度圖像等長上下文數據。 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;最終，通過同質-異質融合技術對不同數據混合比例下的模型權重進行平均，減少固定數據比例帶來的內在偏差，在保持多樣化能力的同時提升模型的魯棒性。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371648</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371648</guid>
      <pubDate>Wed, 10 Sep 2025 12:00:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>AI 編程公司 Replit 發佈第三代自主編碼 Agent</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Replit&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Freplit.com%2Fagent3" target="_blank"&gt;宣佈&lt;/a&gt;推出第三代自主編碼 Agent（Agent 3），官方稱其自主性提升至前代的 10 倍，單次可連續運行 200 分鐘，全程無需人工幹預。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;自主性增強&lt;/strong&gt;：Agent 3 可以自主測試和修復代碼，甚至在後台持續改進用户的應用，將用户從重複性工作中解放出來。它能夠像人類一樣在瀏覽器中 「點擊」 和 「操作」，檢查應用中的按鈕、表單和 API，確保一切正常運行。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;持續運行能力&lt;/strong&gt;：該版本能夠持續自主運行超過三小時，相比之前的版本有了很大的進步。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;提升開發效率&lt;/strong&gt;：Agent 3 能夠根據用户需求生成高質量代碼，並主動提供優化建議，從而提升開發效率。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0911/194603_LoIb_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;新系統通過自研測試框架在瀏覽器內自動點擊按鈕、填寫表單、調用 API 並修復錯誤，其速度比主流 Computer Use 模型快 3 倍，成本則降低了 90%。&lt;/p&gt; 
&lt;p&gt;Agent 3 支持自然語言提示，用户可以用簡單描述啓動複雜項目，並在手機端通過 Live Monitoring 實時查看進度。其另一項突破是能夠生成子 Agent 與自動化流程，成品可直接接入 Slack、Notion、郵件等平台，進一步擴展工作流。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371645/replit-agent3</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371645/replit-agent3</guid>
      <pubDate>Wed, 10 Sep 2025 11:48:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>谷歌推遲發佈 Android 16 QPR1 的 AOSP 源代碼，引發擔憂</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.androidauthority.com%2Fandroid-16-qpr1-source-code-delay-3596650%2F"&gt;根據科技媒體 Android Authority 的報道&lt;/a&gt;，谷歌近期已向 Pixel 設備推送 Android 16 QPR1 更新，但遲遲未按慣例在 48 小時內同步開放 AOSP 源碼，引發第三方 ROM 開發者擔憂。&lt;/p&gt; 
&lt;p&gt;Android 16 QPR1（Quarterly Platform Release 1）是谷歌 Android 系統最近發佈的一個更新，其中包含了 Material 3 Expressive 等新特性。&amp;nbsp;通常在發佈類似更新後，谷歌會在 1-2 天內將對應的源代碼上傳至 AOSP，方便第三方定製 ROM 的開發人員同步或使用這些更新特性。&lt;/p&gt; 
&lt;p&gt;但截至目前，一週過去了，AOSP 上的源代碼還沒能找到。 谷歌對外表示，源代碼會在 「接下來幾周內」（「in the coming weeks」）發佈，但未給出更具體的時間表也沒有解釋延遲原因。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-f069cf8e6cb319a979b8b5816b2b9f523c7.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;定製 ROM 社區（如 LineageOS 等）以及使用 AOSP 的開發者對此延遲表示擔憂，因為他們依賴谷歌的及時源代碼發佈來更新他們的系統、添加新特性或修復兼容性問題。&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/news/341310/google-android-development-aosp" target="_blank"&gt;谷歌在年初已經將部分 Android 的開發流程 「完全私有化」&lt;/a&gt;（即不再在公共視線中進行部分開發）以簡化流程。雖然該公司曾多次公開承諾 AOSP 不會取消，但這些動作加劇了對其未來戰略走向的猜測。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371641/android-16-qpr1-source-code-delay</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371641/android-16-qpr1-source-code-delay</guid>
      <pubDate>Wed, 10 Sep 2025 11:31:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>宇樹科技創始人王興興：AI 時代，小組織的爆發力會越來越強</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;9 月 11 日，宇樹科技創始人兼 CEO 王興興出席了 2025 外灘大會，這也是宇樹官宣 IPO 計劃后王興興首次公開發聲。&lt;/p&gt; 
&lt;p&gt;他認為&lt;span&gt;AI 時代的組織管理是一門新課題。王興興表示，宇樹科技是一家以硬件為主要產品的公司，隨着業務快速發展，人員規模更大之後，可能會帶來協作效率的降低，需要花時間探索更高效的組織管理方式。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0911/192019_miGZ_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="font-family:-apple-system,BlinkMacSystemFont,&amp;quot;Apple Color Emoji&amp;quot;,&amp;quot;Segoe UI Emoji&amp;quot;,&amp;quot;Segoe UI Symbol&amp;quot;,&amp;quot;Segoe UI&amp;quot;,&amp;quot;PingFang SC&amp;quot;,&amp;quot;Hiragino Sans GB&amp;quot;,&amp;quot;Microsoft YaHei&amp;quot;,&amp;quot;Helvetica Neue&amp;quot;,Helvetica,Arial,sans-serif"&gt;儘管存在挑戰，但王興興對未來依舊十分樂觀，他認為，現在創新創業的門檻已經大幅降低，年輕創新者迎來了好時代。真正可以用 AI 工具去實現新創意，並且在 AI 時代，小組織的爆發力會越來越強。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;王興興還提到：「頂尖的 AI 人才肯定是缺的，我相信這是每個大公司共同的渴求。」&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371637</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371637</guid>
      <pubDate>Wed, 10 Sep 2025 11:21:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>AgentFly —— 基於記憶增強的在線強化學習框架</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                        
                                                                                    &lt;p&gt;AgentFly 是基於記憶增強的在線強化學習框架，通過記憶庫存儲經驗軌跡並利用神經案例選擇策略實現 LLM 代理的持續適應能力，無需對底層 LLM 參數進行微調。&lt;/p&gt;

&lt;p&gt;該方法將決策過程建模為記憶增強的馬爾可夫決策過程（M-MDP），通過非參數或參數化記憶模塊存儲過往經驗，並基於軟 Q 學習優化案例檢索策略。&lt;/p&gt;

&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-070739b87c472bf439392dc3bc00bb25ddc.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-3e6c7c875e8356c92b9a62c600cca2f9f9e.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

&lt;p&gt;實驗表明，該方法通過記憶庫的持續更新實現高效在線學習，在複雜工具調用和多輪推理任務中展現出顯著優勢，為構建具備持續學習能力的通用型 LLM 代理提供了新範式。&lt;/p&gt;

                                                                    &lt;/div&gt;
                                                                </description>
      <link>https://www.oschina.net/p/agentfly</link>
      <guid isPermaLink="false">https://www.oschina.net/p/agentfly</guid>
      <pubDate>Wed, 10 Sep 2025 11:18:00 GMT</pubDate>
    </item>
    <item>
      <title>小米 Kaldi 團隊開源零樣本語音合成模型模型 ZipVoice</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;近日，小米集團新一代 Kaldi 團隊發佈了基於 Flow Matching 架構的 ZipVoice 系列語音合成（TTS）模型——ZipVoice（零樣本單説話人語音合成模型）與 ZipVoice-Dialog（零樣本對話語音合成模型）。&lt;/p&gt; 
&lt;p&gt;作為 zipformer 在語音生成任務上的應用和探索，ZipVoice 解決了現有零樣本語音合成模型的參數量大、合成速度慢的痛點，在輕量化建模和推理加速上取得了重要突破。ZipVoice-Dialog 則解決了現有對話語音合成模型在穩定性和推理速度上的瓶頸，實現了又快又穩又自然的語音對話合成。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-207702be1087c5d602185a495fa12acd620.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;ZipVoice 系列的模型文件、訓練代碼和推理代碼以及 6.8k 小時的語音對話數據集 OpenDialog 已全部開源：https://github.com/k2-fsa/ZipVoice&lt;/p&gt; 
&lt;p&gt;Zipvoice 論文：https://arxiv.org/pdf/2506.13053&lt;/p&gt; 
&lt;p&gt;樣例體驗請訪問：https://zipvoice.github.io&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371625</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371625</guid>
      <pubDate>Wed, 10 Sep 2025 10:56:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>AI 編程公司 Replit 融資 2.5 億美元，估值達 30 億美元</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;AI 編程公司 Replit &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Freplit.com%2Fnews%2Ffunding-announcement" target="_blank"&gt;宣佈&lt;/a&gt;完成了一輪 2.5 億美元融資，使其估值達到了約 30 億美元，較 2023 年上一輪融資增長近三倍。在過去一年中，Replit 的年收入從 280 萬美元飆升至 1.5 億美元，目前。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;公告稱，Replit 的用户數量已達到 4000 萬。此次融資正值該公司年化收入在不到一年的時間裏從 280 萬美元增長至 1.5 億美元之際（增幅超過 50 倍）。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="291" src="https://oscimg.oschina.net/oscnet/up-37388b8353eaedc1a8426cc5a02204bd9d6.png" width="600" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;此次融資由 Prysm Capital 主導，參與投資的還有 Amex Ventures 和谷歌的 AI Futures Fund。此外，Replit 的現有投資者，包括 Y Combinator、Craft Ventures、Andreessen Horowitz、Coatue Management 和 Paul Graham 等也參與了這一輪融資。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;與此同時，Replit 還宣佈推出了 Agent 3，並聲稱是其迄今為止自主性最強的 agent。Agent 3 的自主性比之前的版本提高了十倍，能夠測試和修復代碼，並構建自定義代理和工作流，從而能夠自動執行任何類型的複雜或重複性任務，而不僅僅是軟件工程。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371618</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371618</guid>
      <pubDate>Wed, 10 Sep 2025 10:31:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>聚焦結構化注意力，探索提升多模態大模型文檔問答性能</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;blockquote&gt; 
 &lt;p&gt;作者：vivo 互聯網算法團隊&lt;/p&gt; 
 &lt;p&gt;本文聚焦多模態大語言模型（MLLMs）在文檔問答（DocQA）任務中的性能提升，提出無需改動模型架構或額外訓練的結構化輸入方法，通過保留文檔層次結構與空間關係（如標題、表格、圖像位置）優化理解能力。研究發現，傳統無結構 OCR 輸入導致注意力分散，性能下降，而 LaTeX 範式結構化輸入顯著提升表現。注意力分析揭示其誘導"結構化注意力"，減少無關區域幹擾，聚焦語義核心。在 MMLongBench、PaperTab 等四個數據集上驗證，該方法尤其在複雜圖表任務中效果顯著，為智能文檔處理與自動問答提供高效的解決方案。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;本文提供配套演示代碼，可下載體驗：&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fvivo%2FStructureMatters" target="_blank"&gt;Github |&lt;/a&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fvivo%2FStructureMatters" target="_blank"&gt;StructureMatters&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;一、引言&lt;/h1&gt; 
&lt;p&gt;多模態大語言模型（Multimodal Large Language Models, MLLMs）蓬勃發展的今天，文檔理解（Document Understanding）作為一項涉及文本、圖表和圖像的複雜任務，依然面臨諸多挑戰。如何高效整合多源信息、理解文檔的層次結構，成為提升 MLLMs 性能的關鍵問題。研究發現了一種無需修改模型架構或額外訓練的新方法：僅通過結構化輸入提升 MLLMs 在文檔問答（DocQA）任務中的表現，同時通過注意力分析實踐探尋結構化輸入帶來性能提升的深層原因。&lt;/p&gt; 
&lt;h1&gt;二、文檔理解的核心挑戰&lt;/h1&gt; 
&lt;p&gt;文檔理解要求模型同時處理文本、圖表、圖像等多模態信息，並準確回答問題。然而，現有方法多依賴於擴展上下文窗口或優化檢索增強生成（RAG），忽略了一個關鍵問題：輸入格式如何影響模型的理解能力？&lt;/p&gt; 
&lt;p&gt;研究發現，傳統的無結構 OCR 文本輸入在某些 case 下未提升模型性能，反而因注意力分散和結構丟失導致性能下降。例如，在 MMLongBench 數據集上，加入無結構 OCR 文本後，模型準確率從 0.389 下降至 0.370。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-12cc41cc30fcd9ed83f9ed46f2f8fc29448.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;當前主流多模態大模型已經具備處理多模態信息的能力，其中 Qwen2.5-VL-7B-Instruct，Phi-3.5-Vision-Instruct，SmolVLM-Instruct 等在多個多模態任務上達到了 SOTA，但在文檔閲讀任務中仍表現不佳。以往文檔閲讀模型通過訓練得到專用模型來進行文檔閲讀理解，並基於文檔回答問題，如 mPLUG-DocOwl，Textmonkey 等模型。但隨着 RAG 的快速發展，像 ColBERT 和 ColPali 這樣的 RAG 方法在分別檢索文本或視覺信息方面已被證明有效，當前主流方法通常基於 RAG 檢索證據頁面，然後將證據信息直接輸入多模態大模型中以便回答 DocQAs。但當問題需要整合來自兩種模態的信息時，它們通常表現不佳。&lt;/p&gt; 
&lt;p&gt;隨着通用大模型的發展和 AGI 概念的普及，如何直接利用通用多模態大模型達到目的，不額外進行訓練成為研究熱點。改變輸入結構能否幫助多模態大模型進行高效推理為本文探討的重點。本文致力於探尋通用多模態大模型在何種條件下能夠具有更加高效的推理理解能力，能否具備在 trainning free 的條件下達到較高的多元素文檔理解能力。&lt;/p&gt; 
&lt;h1&gt;三、創新方法：結構化輸入與注意力分析&lt;/h1&gt; 
&lt;p&gt;為解決這一問題，提出了一種基於 LaTeX 範式的結構保留方法。該方法通過保留文檔的層次結構和空間關係（如標題、表格、圖像的位置），從而為模型提供更清晰的語義引導。&lt;/p&gt; 
&lt;p&gt;具體流程包括：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;結構化編碼&lt;/strong&gt;：將 OCR 文本和圖像輸入 MLLMs，提示模型儘可能保留圖表、表格和文本的結構，生成 LaTeX 格式的表示。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;聯合輸入&lt;/strong&gt;：將結構化文本與原始圖像一同輸入模型，指導其在回答問題時關注關鍵區域。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;注意力分析&lt;/strong&gt;：通過比較僅圖像輸入、圖像加無結構文本、圖像加結構化文本三種情況的注意力分佈，發現結構化輸入顯著減少了注意力浪費，引導模型聚焦於語義相關的文本和圖像區域。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;實驗結果表明，該方法在多個文檔理解基準數據集上顯著提升了模型性能。例如，在 MMLongBench 上，QWEN2.5-VL-7B-INSTRUCT 的準確率從 0.389 提升至 0.435；在 PaperTab 數據集上，準確率提升高達 20%，得益於 LaTeX 格式對錶格和圖表的精準解析。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-51b199d7da4f2b8e482da26b791c77d6d76.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h1&gt;四、通過注意力機制進行深層原因探究&lt;/h1&gt; 
&lt;p&gt;進一步的，通過注意力分析揭示了結構化輸入的內在機制。無結構文本輸入導致模型注意力分佈散亂，浪費在圖像邊緣或無關區域；而結構化文本添加了結構化約束，誘導模型形成"結構化注意力"模式，聚焦於文檔的核心內容（如圖表、文本塊）。例如，在一個案例中，模型需根據圖表回答"西德居民對美俄關係的看法比例"。無結構輸入下，注意力分散在圖像空白區域；結構化輸入後，注意力集中於圖表和相關文本，顯著提高答案准確性。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-c1cf6c7f984dc7670fa8d60b87bd138c1bf.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;結構化輸入幫助減少 MLLMs 對於圖片邊界 token 的關注度，提高了模型對於文章主體部分的注意力得分。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-4266ab22bf20fc556f2d3de92b379f90a11.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;具體實例分析，證明結構化輸入的重要意義。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-8d93f9bca6600aaabfd3cab20c1a73d141c.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h1&gt;五、實驗驗證與數據支持&lt;/h1&gt; 
&lt;p&gt;在四個文檔理解基準數據集（MMLongBench、LongDocUrl、PaperTab、FetaTab）上測試 4 種 MLLMs 模型（如 QWEN2-VL-7B-INSTRUCT、Phi-3.5-Vision-Instruct）。結果顯示，結構化輸入在所有數據集上均提升了模型性能，尤其在包含複雜圖表的 PaperTab 數據集上效果顯著。消融實驗進一步證明，僅用結構化文本或僅用圖像的性能均低於兩者結合，驗證了結構化輸入與圖像聯合使用的必要性。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-41cf00ff446e8353ac63034f7b316b625ac.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-b172ced302ebcd0de4814ef3027cb3a7f41.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h1&gt;六、總結與展望&lt;/h1&gt; 
&lt;p&gt;實踐研究揭示了輸入格式對 MLLMs 文檔理解能力的關鍵影響，提出了一種簡單而高效的結構化輸入方法。未來可進一步探索更先進的結構提取技術或設計注意力控制插件，以進一步釋放 MLLMs 在文檔理解中的潛力。該研究提供了一種無需重訓模型即可提升性能的實用方案，適用於智能文檔處理、自動問答等場景。在沒有額外訓練和架構修改的前提下，通過簡單的結構化文本輸入，可以提升現有多模態大模型在文檔理解任務中的表現。此項研究可以幫助用户分析、工作解析等場景中更準確地提取信息，提升工作效率。同時，RAG（檢索增強生成）系統也能結合結構化輸入來降低信息檢索中的噪聲，從而更高效地利用檢索到的證據頁面，為未來文檔處理與分析提供了新的實踐路徑。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/vivotech/blog/18691398</link>
      <guid isPermaLink="false">https://my.oschina.net/vivotech/blog/18691398</guid>
      <pubDate>Wed, 10 Sep 2025 10:08:00 GMT</pubDate>
      <author>原創</author>
    </item>
    <item>
      <title>支付寶推出國內首個 AI 付</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;支付寶宣佈推出國內首個「AI 付」服務，面向 AI 時代為智能體提供支付服務，並率先在瑞幸咖啡的 AI 點單助手「Lucky AI」上線，用户可在瑞幸支付寶小程序或瑞幸咖啡 App，用説話的方式完成下單並支付。這也是行業首次打通智能體內的下單與支付全鏈路，實現無縫的 AI 服務體驗。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;以點咖啡為例，此前用户通過 AI 智能體下單，仍需跳轉至支付頁面再予手動確認。現在，用户在瑞幸支付寶小程序或瑞幸 App 喚起「Lucky AI」，不僅可以説句話點咖啡，還可以直接説句「下單」，完成身份核驗後即支付。整個過程用户無需離開 AI 對話界面，像日常和店員聊天一樣簡單自然。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="496" src="https://oscimg.oschina.net/oscnet/up-ed3ccbc5548251ac7a86f54e281b209e58a.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;螞蟻集團數字支付事業羣首席技術官朱林表示：支付寶始終致力於通過產品創新，解決支付中存在的安全信任和便捷兩大課題，此次為智能體提供「AI 下單+支付」解決方案，旨在服務好用户、產業和時代的需求，做好激活 AI 產業生態的一把鑰匙。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;朱林認為，伴隨 AI 產業的爆發式增長和智能終端的普及，預計未來 5 年內更自然的新交互支付佔比或超 50%，多樣化的智能設備支付將增長 10 倍，而更智能的 AI 支付市場規模可達萬億級。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;支付寶正着力打造面向 AI 時代的支付服務，構築融入 AI 交互服務的「支付新基建」。目前已推出包括:&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;國內第一個「支付 MCP Server」，讓 AI 智能體可一鍵接入支付寶支付服務;&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;國內第一個「AI 打賞」服務，為在 AI 智能體內有收取讚賞、小費等需求的開發者提供便捷收款能力;&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;國內第一個「AI 訂閲付費」功能，支持開發者在智能體中便捷接入，按服務次數或時長定價並收款;&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;全球第一個智能眼鏡支付「看一下支付」服務，用户佩戴眼鏡看商家收款碼或收款設備即可支付;&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;國內第一個智能體支付服務「支付寶 AI 付」，用户説説話可點單又能支付。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371606</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371606</guid>
      <pubDate>Wed, 10 Sep 2025 09:41:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>微軟再次提醒 Windows 將逐步淘汰 VBScript，分三階段進行</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;近期微軟再次提醒用户，VBScript 將被逐步淘汰，並分享了相關建議。&lt;/p&gt; 
&lt;p&gt;VBScript，即 Visual Basic Script，是微軟在近三十年前開發的腳本語言，曾默認包含在 Windows 中，主要用於自動化任務，不過近年來，該語言也常被黑客利用。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-9c905bf5e0ab3ab7646889588b6b63a0c65.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;早在 2023 年 10 月，微軟就&lt;a href="https://www.oschina.net/news/261322/microsoft-deprecated-vbscript-in-window" target="_blank"&gt;宣佈&lt;/a&gt;將在未來的 Windows 版本中淘汰 VBScript，並在 2024 年 5 月發佈了詳細的時間表，如今微軟又提供了更多細節。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-589957b28bdb1eaa90fcbc1849da2ea8216.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;VBScript 主要用於通過執行外部.vbs 腳本或引用正則表達式庫來擴展 Office 應用程序的功能，不過，隨着 VBScript 的淘汰，這些開發人員和用處將受到影響。&lt;/p&gt; 
&lt;p&gt;微軟將淘汰 VBScript 分為三個階段，第一階段已經啓動，可能會持續到 2026 年或 2027 年。在此期間，VBScript 將作為「按需功能」（FOD）默認啓用，現有項目不會受到影響。&lt;/p&gt; 
&lt;p&gt;當第二階段開始時，該 FOD 將被禁用，最後，在第三階段，VBScript 將被完全移除，這將直接影響之前提到的用處。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;微軟建議客户使用 Office 2508 版本中包含的 RegExp 類，並默認使用 Microsoft 365 訂閲，這將使開發人員能夠在 VBScript 中繼續使用 RegExp。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;微軟還建議客户通過 Microsoft 365 升級到最新版本的 Office，以利用新的 RegExp 實現，這將允許開發人員在 Visual Basic 編輯器（VBE）中使用該功能，而無需添加 vbscript.dll。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371602</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371602</guid>
      <pubDate>Wed, 10 Sep 2025 09:28:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Stability AI 發佈專業音頻生成模型 Stable Audio 2.5</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Stability AI 推出專業音頻生成模型&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fstability.ai%2Fnews%2Fstability-ai-introduces-stable-audio-25-the-first-audio-model-built-for-enterprise-sound-production-at-scale" target="_blank"&gt;Stable Audio 2.5&lt;/a&gt;，藉助 Adversarial Relativistic-Contrastive（ARC）後訓練技術，實現複雜音樂結構的高效生成。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-e2eb31cef809fb7d09a49502cdb9a67f522.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在英偉達 H100 GPU 上，模型可在 2 秒內完成最長 3 分鐘的音頻創作，支持前奏、發展、尾聲等多段落結構，並集成音頻修復功能，允許用户上傳現有音頻進行續寫。&lt;/p&gt; 
&lt;p&gt;該模型同步推出移動端輕量版 Stable Audio Open Small，可在手機端 7 秒內生成 11 秒立體聲。為確保商用合規，Stable Audio 2.5 基於 licensed 數據集訓練，並通過版權識別系統限制用户上傳版權受限內容。&lt;/p&gt; 
&lt;p&gt;Stability AI 希望該技術能應用於廣告、零售、品牌音效等多個領域，與 WPP 旗下的音效品牌代理機構 Amp 合作，為大型客户提供一致的音頻識別服務。&lt;/p&gt; 
&lt;p&gt;Stability AI 的音頻團隊還可以根據公司的音效庫調整模型，打造獨特的音頻標識。Stable Audio2.5 將通過 WPP Open 平台面向 WPP 的全球客户開放。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371600/stability-ai-introduces-stable-audio-2-5</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371600/stability-ai-introduces-stable-audio-2-5</guid>
      <pubDate>Wed, 10 Sep 2025 09:21:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>​字節 Seed 推出全新 AgentGym-RL 框架</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;字節跳動 Seed 研究團隊推出了名為 AgentGym-RL 的新框架，專注於通過強化學習訓練 LLM 代理，使其能夠進行多輪互動決策。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;該框架具有模塊化和解耦的架構，提供了極高的靈活性和擴展性。AgentGym-RL 覆蓋了多種真實場景，能夠支持主流的強化學習算法，幫助代理全面提升其決策能力。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="372" src="https://oscimg.oschina.net/oscnet/up-6607f56f1b2f55c4cb23f3482def33cf8f5.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;為了進一步優化訓練效果，研究團隊還提出了一種名為 ScalingInter-RL 的訓練方法。該方法通過階段性調整交互次數，幫助代理在早期專注於掌握基本技能，隨後逐漸增加交互次數，以鼓勵更多樣化的問題解決策略。這種探索與利用的平衡設計，有助於代理在面對複雜任務時保持穩定的學習和決策能力。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;在實驗過程中，研究者們採用了 Qwen2.5-3B 和 Qwen2.5-7B 作為基礎模型，評估了 AgentGym-RL 和 ScalingInter-RL 在五個不同場景中的表現。結果顯示，使用 AgentGym-RL 的代理在 27 個任務中，表現優於多個商業模型。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;研究團隊計劃將整個 AgentGym-RL 框架，包括代碼和數據集，開源，以支持更多研究者開發智能代理。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;AgentGym-RL 框架涉及的多種場景包括網絡導航、深度搜索、數字遊戲、體感任務和科學實驗等，代理在這些場景中需具備強大的決策能力和適應能力，才能完成複雜的任務。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371591</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371591</guid>
      <pubDate>Wed, 10 Sep 2025 08:40:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Salesforce 開源深度研究 Agent：SFR-DeepResearch (SFR-DR)</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Salesforce &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2FCaimingXiong%2Fstatus%2F1965440617334685886" target="_blank"&gt;發佈&lt;/a&gt;了開源深度研究 Agent：SFR-DeepResearch（SFR-DR）。該模型基於 OpenAI 的小型開源權重模型，通過強化學習進行訓練，具備推理、搜索與代碼執行能力，可自主完成深度研究任務。&lt;/p&gt; 
&lt;p&gt;SFR-DR-20B 版本僅依靠網頁搜索、瀏覽器和 Python 解釋器，在純文本的 Humanity's Last Exam 基準測試中取得了 28.7% 的成績。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-1e558bc3cb2f0faa6a5717b108de0484740.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;SFR-DR 亮點特性如下&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;核心能力&lt;/strong&gt;：基於強化學習（RL）訓練的自主研究代理，能夠獨立推理、搜索和編程，完成深度研究任務。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;性能表現&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;SFR-DR-20B&lt;/strong&gt; 在 Humanity's Last Exam（純文本）上取得 &lt;strong&gt;28.7%&lt;/strong&gt; 的成績&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;僅依賴網絡搜索、網頁瀏覽和 Python 解釋器&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;超越 OpenAI o3 DeepResearch 和 Kimi Researcher&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;訓練方式&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;端到端 RL，從優化推理能力的基礎模型開始訓練&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;保留推理能力的同時提升研究執行能力&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;自主性&lt;/strong&gt;：無需預定義多代理工作流，可自主規劃、推理、提出方案並執行行動&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;論文地址：&lt;em&gt;https://arxiv.org/abs/2509.06283&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371589</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371589</guid>
      <pubDate>Wed, 10 Sep 2025 08:33:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>騰訊開源圖檢索增強生成框架 Youtu-GraphRAG</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;騰訊優圖實驗室開源了 Youtu-GraphRAG，這是一個全新的圖檢索增強生成框架，旨在通過大語言模型+RAG 模式，將知識組織成圖譜，再交給大語言模型進行檢索和推理，從而提高模型在處理複雜問答任務時的準確性和可追溯性。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;Youtu-GraphRAG 特別適用於企業知識庫問答、科研文檔解析、個人知識管理等知識密集型場景。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;Youtu-GraphRAG 通過三大創新實現了從圖構建到索引、再到檢索的垂直統一和認知閉環。首先，它採用了四層知識樹結構，將知識拆解成屬性、關係、關鍵詞和社區四個層次，使得大模型在回答問題時能夠沿着知識樹定位信息，推理路徑清晰可見。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;其次，社區檢測升級不僅關注「誰和誰有關」，還結合語義理解「為什麼它們有關」，生成簡明摘要，幫助用户快速抓住問題本質。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;最後，智能迭代檢索機制允許用户提出複雜問題時，將其拆解成多個子問題並行檢索，並通過迭代反思機制對結果進行補充和修正，最終給出更完整、更可靠的回答。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="314" src="https://oscimg.oschina.net/oscnet/up-ab798039306f65590d7249203cb4394d0d9.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;Youtu-GraphRAG 在實踐檢驗中表現出色。在六個&lt;span&gt;權威&lt;/span&gt;基準測試中，&lt;span&gt;最高&lt;/span&gt;可節省 90.71% 的 Token 成本，複雜推理任務的準確率&lt;span&gt;最高&lt;/span&gt;提升 16.62%。此外，該框架支持中英文雙語，跨領域應用無需重構，具有很高的靈活性。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;使用 Youtu-GraphRAG 非常簡單，只需四步即可上手。首先，通過命令行獲取項目代碼。其次，進行環境配置，包括獲取遠程調用模型的憑證 API key 並創建配置文件。然後，一鍵部署項目。最後，通過 curl 命令體驗交互。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371566</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371566</guid>
      <pubDate>Wed, 10 Sep 2025 07:30:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>字節跳動發佈開源多模態模型 Mini-o3</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;字節跳動發佈開源多模態模型 Mini-o3，通過擴展推理模式和交互輪次提升視覺搜索性能，在複雜場景中實現顯著突破。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0911/151240_xzZ9_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;https://mini-o3.github.io/&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;據介紹，Mini-o3 是一個完全開源的多模態模型，專為「邊看邊想」的視覺搜索任務設計。它通過強化學習將工具調用次數擴展到數十輪，在 VisualProbe、V* Bench、HR-Bench、MME-Realworld 等基準上取得了 7B 量級的最佳成績。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-a111377d25b371aba3ccf675445d406ca6b.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-71b6b52f2c739864eda55692e83e9bbe7ce.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;項目公開了訓練代碼、模型權重以及包含 4,500 條數據的 Visual Probe 數據集，允許研究者在非商業許可下復現 OpenAI o3 風格的深度推理行為。&lt;/p&gt; 
&lt;p&gt;Mini-o3 支持深度優先搜索、試錯等多樣化推理模式，測試時交互輪次可擴展至 32 輪以上，準確率隨輪次增加顯著提升（如 VisualProbe-Hard 任務準確率從 35.1% 提升至 48.0%）。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;核心創新&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;挑戰性數據集構建&lt;/strong&gt;：推出 VisualProbe 數據集，包含高分辨率圖像、小目標和密集幹擾物場景，強制模型進行多輪探索。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;迭代數據收集&lt;/strong&gt;：通過冷啓動數據生成多樣化推理軌跡，覆蓋回溯、假設驗證等策略，解決預訓練模型缺乏多輪交互能力的問題。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Over-Turn Masking 策略&lt;/strong&gt;：在強化學習中避免對超輪次響應的懲罰，支持模型深度探索，訓練時輪次上限設為 6 輪，測試時可擴展至 32 輪以上。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;應用案例&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-32f28f4c28db5eb7a911b5d6fe714e2b11b.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371562</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371562</guid>
      <pubDate>Wed, 10 Sep 2025 07:18:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>🔥🔥AI 能打造盲人的第三隻眼？</title>
      <description/>
      <link>https://www.oschina.net/ai-creation/details/2194</link>
      <guid isPermaLink="false">https://www.oschina.net/ai-creation/details/2194</guid>
      <pubDate>Wed, 10 Sep 2025 07:03:00 GMT</pubDate>
    </item>
    <item>
      <title>🔥🔥智能植物收割？能割韭菜不？</title>
      <description/>
      <link>https://www.oschina.net/ai-creation/details/2195</link>
      <guid isPermaLink="false">https://www.oschina.net/ai-creation/details/2195</guid>
      <pubDate>Wed, 10 Sep 2025 07:03:00 GMT</pubDate>
    </item>
    <item>
      <title>Visual Studio 2026 Insiders 預覽版發佈：深度集成 AI、界面設計煥然一新</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Visual Studio 2026 Insiders 首個預覽版&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdevblogs.microsoft.com%2Fvisualstudio%2Fvisual-studio-2026-insiders-is-here%2F" target="_blank"&gt;已發佈&lt;/a&gt;，帶來了更快的啓動速度、更智能的 AI Agent 和更現代的 UI。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-e324bcd11fdf3e7e28980bdbf8987580a75.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;這是自 2021 年以來 Visual Studio 的首次重大版本更新。本次更新主打更深層的人工智能集成及全新界面設計，為廣大開發者帶來了全新體驗。&lt;/p&gt; 
&lt;p&gt;新版 Visual Studio 採用微軟 Fluent Design 設計體系，並推出數款新主題（包括 「Mango Paradise」「Juicy Plum」 等），同時啓用了新 Logo，並將 Visual Studio Preview 更名為 Visual Studio Insiders。雖然界面煥新，核心依然運行在傳統 .NET Framework 上，許多新功能更側重於提升 AI 輔助開發體驗。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0911/144937_XQRx_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;AI 增強成為亮點。Copilot 可獲取更多上下文，擁有 「Profiler Copilot Agent」 等新功能，能基準測試代碼、查找並自動實現優化建議。「自適應粘貼」 讓 Copilot 能根據已有代碼自動調整粘貼內容，「URL 上下文」 功能支持通過聊天窗口指定網頁規則，讓 AI 輔助更智能。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0911/145432_IwpX_2720166.gif" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;開發者還可自由選擇接入的 AI 大模型及 API 密鑰，目前支持 Anthropic、Google 及 OpenAI 等主流廠商，增強了模型選擇的靈活性。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-5b5a94b7cf0eda7accb37905ed368768183.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;新版設置系統採用可編輯 JSON 配置文件，並支持高級過濾和批量同步，可與項目一同納入版本控制。原僅企業版獨有的代碼覆蓋率功能（用於單元測試覆蓋分析）現向社區及專業版用户開放。&lt;/p&gt; 
&lt;p&gt;對於擴展兼容，Visual Studio 2026 承諾與 2022 版擴展完全兼容，便於用户無痛升級。而 NDepend 開發團隊等第三方工具表示，雖然主進程依舊依賴舊版.NET Framework，但新版已為擴展和子進程引入 .NET Core，多數代碼運行環境得到一定現代化。&lt;/p&gt; 
&lt;p&gt;此外，VS 2026 新增一鍵工具，支持將.NET Framework 應用遷移至.NET 10（預計隨新版同步發佈），並保持傳統 Windows 平台的高適配性。&lt;/p&gt; 
&lt;p&gt;用户社區評論普遍對 AI 功能感興趣，也有呼籲微軟更多關注性能優化與資源效率，以及期盼 Linux 版 Visual Studio 的聲音。微軟方面稱，過去一年團隊共修復了 4489 項問題和 290 項新需求，大部分已納入新版本。&lt;/p&gt; 
&lt;p&gt;目前尚無 Visual Studio 2026 的正式發佈日期，預計將與.NET 10 一同於 11 月發佈。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371552/visual-studio-2026-insiders</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371552/visual-studio-2026-insiders</guid>
      <pubDate>Wed, 10 Sep 2025 06:50:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>9 月線下活動彙總</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;span id="OSC_h4_1"&gt;&lt;/span&gt; 
&lt;h4&gt;&lt;strong&gt;9 月 13 日，廣州，Solar 開發者咖啡&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;👨‍💻熟悉的開發者咖啡活動迴歸，這次搬到了廣州！這也是 Solar 第一次在廣州舉辦社區活動，嘗試在大灣區佈局更多站點。期待開發者們的加入！&lt;/p&gt; 
&lt;p&gt;🗓️時間：9 月 13 日，週六 14:30-18:00&lt;br&gt; 📝現在報名 https://luma.com/xhki98ic&lt;/p&gt; 
&lt;p&gt;分享主題：&lt;/p&gt; 
&lt;p&gt;- Solana 歷史、POH 機制、開發差異；&lt;br&gt; - Solana 現在的開源生態、技術、項目；&lt;br&gt; - 基於 Blinks 開發所用的技術、需求分析、商業模式和收穫；&lt;br&gt; - Solar.zens.one, Solar 的項目導航&lt;br&gt; - Solana DeFi 生態現狀和創新&lt;/p&gt; 
&lt;span id="OSC_h4_2"&gt;&lt;/span&gt; 
&lt;h4&gt;9 月 13–14 日，杭州，RustChinaConf 2025 x Rust Global China&lt;/h4&gt; 
&lt;p&gt;今年正值 Rust 誕生 10 週年 🎉&lt;br&gt; 👉 講師陣容全面升級，海外講師議題比例高達 45%！&lt;br&gt; 👉 Rust Foundation 團隊將首次現場參與交流！&lt;br&gt; 👉 國內大廠創業公司紛紛加入，瞭解 Rust 語言的最近進展&lt;br&gt; 👉 現場設有 Rust 十週年慶典與特別大禮包&lt;/p&gt; 
&lt;p&gt;🔎 查看完整議程： &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Frustcc.cn%2F2025conf%2Fschedule.html" target="_blank"&gt;https://rustcc.cn/2025conf/schedule.html&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;🎟️ 立即購票： &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhangzhou2025.gosim.org%2Ftickets%2F" target="_blank"&gt;https://hangzhou2025.gosim.org/tickets/&lt;/a&gt;&lt;/p&gt; 
&lt;span id="OSC_h4_3"&gt;&lt;/span&gt; 
&lt;h4&gt;9 月 14 日，成都，聚焦 Kiro：體驗下一代 Agentic IDE&lt;/h4&gt; 
&lt;p&gt;生成式 AI 正在重構開發流程，AI Agent 成為人機交互新核心！亞馬遜雲科技成都 User Group 技術沙龍邀您共同探索最前沿的 Agentic 實踐。&lt;/p&gt; 
&lt;p&gt;🔍 聚焦 Kiro：體驗下一代 Agentic IDE，基於 MCP 協議重塑開發環境&lt;br&gt; 🧠 依託 Amazon Bedrock：構建與部署高效 AI Agent（如 Strands Agents）&lt;br&gt; 🛠 實戰指引：掌握亞馬遜雲科技 Builder Cards 中文版，加速生成式 AI 應用開發&lt;/p&gt; 
&lt;p&gt;立即報名，攜手邁進智能體驅動開發的新時代！&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F1mqqg_M34zzVig1QZl0KCg" target="_blank"&gt;https://mp.weixin.qq.com/s/1mqqg_M34zzVig1QZl0KCg&lt;/a&gt;&lt;/p&gt; 
&lt;span id="OSC_h4_4"&gt;&lt;/span&gt; 
&lt;h4&gt;9 月 20 日，成都，Gitee Talk | 模力方舟 AI 應用開發沙龍&lt;/h4&gt; 
&lt;p&gt;📅 9 月 20 日（下週六）13:30-17:00&lt;br&gt; 📍 成都春熙路 voco 酒店（3 號線春熙路 E1 口出直達）&lt;/p&gt; 
&lt;p&gt;✨ 聚焦 AI 開發全鏈路：從模型調用、開源實踐、3D 交互到硬件選型，5 大實戰議題，帶你突破技術邊界！&lt;br&gt; 🙌 現場和大咖交流互動、結識技術同頻人，開源老傳統披薩暢吃，抽模力方舟千元代金券和更多驚喜好禮～&lt;/p&gt; 
&lt;p&gt;🚀 立即報名，鎖定席位：&lt;a href="https://www.oschina.net/event/8598033"&gt;https://www.oschina.net/event/8598033&lt;/a&gt;&lt;/p&gt; 
&lt;span id="OSC_h4_5"&gt;&lt;/span&gt; 
&lt;h4&gt;9 月 20 日，合肥，架構師的 AI 進化論&lt;/h4&gt; 
&lt;p&gt;🚀「架構師的 AI 進化論——從架構升級到行業應用」騰訊雲架構師技術沙龍 · 合肥&lt;/p&gt; 
&lt;p&gt;🤔AI 時代，架構師是被替代？被重構？還是——主動進化？&lt;/p&gt; 
&lt;p&gt;🌟下週六（9 月 20 日）14:00，相約合肥！與多位一線專家面對面，全程硬核乾貨，深入 AI 架構實戰經驗與前沿思考，千萬不要錯過！&lt;/p&gt; 
&lt;p&gt;🎁 現場還有鵝廠限定周邊、公仔、定製好禮抽獎送不停，戳鏈接免費報名👇&lt;br&gt; &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmc.tencent.com%2FD91JjaK9" target="_blank"&gt;https://mc.tencent.com/D91JjaK9&lt;/a&gt;&lt;/p&gt; 
&lt;span id="OSC_h4_6"&gt;&lt;/span&gt; 
&lt;h4&gt;9 月 20 日，青島，奇妙 AI 之旅&lt;/h4&gt; 
&lt;p&gt;免費參與，0 門檻入場！&lt;br&gt; 這是一個真正能帶走方案、解決實際問題的 AI 實戰工作坊！💡&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;聚焦真問題，挖掘問題痛點；&lt;/li&gt; 
 &lt;li&gt;一起頭腦風暴，打破思維限制；&lt;/li&gt; 
 &lt;li&gt;快速搭建方案原型，讓想法能實實在在被看到；&lt;/li&gt; 
 &lt;li&gt;多方面驗證，直到拿出可落地的方案；&lt;/li&gt; 
 &lt;li&gt;現場路演，讓 AI 創新方案被看到。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;👉🏼立即報名：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.huodongxing.com%2Fevent%2F9823342171500%3Fcoupon%3D88ab88" target="_blank"&gt;https://www.huodongxing.com/event/9823342171500?coupon=88ab88&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;全程邊玩邊掌握 AI 技能，更有 3000 元獎金🧧等你來拿～&lt;/p&gt; 
&lt;p&gt;外地朋友別猶豫！機票 / 車票報銷名額等你來抽！&lt;/p&gt; 
&lt;span id="OSC_h4_7"&gt;&lt;/span&gt; 
&lt;h4&gt;9 月 20 日，上海，PyCon China 2025&lt;/h4&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;1 個主會場 + 4 個分會場 + 開源松 + Vibe Coding 黑客松 + 社區開源集市展&lt;/span&gt;&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;彙集國內外 Python 領域專家與實力開發者，立足 AI 新紀元，&lt;/span&gt;&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;為大家呈現一場豐富、精彩、趣味，且充滿活力的 PyCon 盛會。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;開創性地帶來首屆 PyCon China Vibe Coding 黑客松大賽&lt;/span&gt;。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;活動詳情：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FDfU9DaqCMT5nDgHufzzaNw" target="_blank"&gt;https://mp.weixin.qq.com/s/DfU9DaqCMT5nDgHufzzaNw&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/3859945/blog/18691462</link>
      <guid isPermaLink="false">https://my.oschina.net/u/3859945/blog/18691462</guid>
      <pubDate>Wed, 10 Sep 2025 06:37:00 GMT</pubDate>
      <author>原創</author>
    </item>
    <item>
      <title>網信辦副主任王京濤：促進軟件生態、開源代碼等自主生態構建</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;中央網信辦副主任、國家網信辦副主任王京濤在《新安全》雜誌上撰文表示，今年是「十四五」規劃收官之年、「十五五」規劃謀篇佈局之年，也是進一步全面深化改革的重要一年。&lt;/p&gt; 
&lt;p&gt;網絡安全工作機遇與挑戰並存，要以更大力度、更實舉措加快推進國家網絡安全體系和能力現代化，推動我國網絡安全事業發展邁上新台階。進一步建立安全可信的供應鏈安全管理體系。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0911/143425_K3L6_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;https://mp.weixin.qq.com/s/b-rPE4C9CD9-5IEJ7odAXA&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;圍繞集成電路、基礎軟件、人工智能、量子信息等重點領域，加強產業鏈協同創新，加快推動關鍵核心技術研發突破。充分發揮我國超大規模市場優勢，促進軟件生態、開源代碼等自主生態構建，推動人工智能等技術在產業應用上取得更大突破。充分發揮網絡安全審查、雲計算服務安全評估、新技術新應用安全評估等制度機製作用，防範網絡安全風險，提高供應鏈安全水平。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371539</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371539</guid>
      <pubDate>Wed, 10 Sep 2025 06:35:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
  </channel>
</rss>
