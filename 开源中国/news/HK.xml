<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>開源中國-最新資訊</title>
        <link>https://www.oschina.net/news/project</link>
        <atom:link href="http://8.134.148.166:30044/oschina/news" rel="self" type="application/rss+xml"></atom:link>
        <description>開源中國-最新資訊 - Powered by RSSHub</description>
        <generator>RSSHub</generator>
        <webMaster>contact@rsshub.app (RSSHub)</webMaster>
        <language>en</language>
        <lastBuildDate>Tue, 11 Mar 2025 21:37:19 GMT</lastBuildDate>
        <ttl>5</ttl>
        <item>
            <title>梁文鋒拒絕用 DeepSeek 賺快錢，騰訊、阿里近期都曾與其接觸</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcn.wsj.com%2Farticles%2Finvestors-want-a-piece-of-deepseek-its-founder-says-not-now-724386a5%3Fmod%3Dcn_hp_lead_pos2&quot; target=&quot;_blank&quot;&gt;據《華爾街日報》報道&lt;/a&gt;&lt;/u&gt;，&lt;strong&gt;DeepSeek 創始人梁文鋒已經拒絕了通過其大模型賺快錢的投資提議。他告訴潛在投資者，自己希望保持那種致力於科學項目研究的精神&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;702&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0311/184106_oQoG_2720166.png&quot; width=&quot;1800&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;據知情人士透露，梁文鋒告訴身邊的人，他並不急於獲得投資，因為擔心外部投資者會幹預 DeepSeek 的決策。最近幾周，包括騰訊和阿里巴巴在內的中國科技公司高管曾與梁文鋒會面，討論潛在合作機會。&lt;/p&gt; 
&lt;p&gt;知情人士稱，自 2023 年底以來，DeepSeek 曾向多個風險投資基金進行推介，包括一些外國公司，但是這些公司拒絕投資，因為他們看不到明確的資金回報途徑。&lt;/p&gt; 
&lt;p&gt;儘管對 DeepSeek 缺乏明確創收計劃表示擔憂，但最近更多潛在投資者表達了對投資 DeepSeek 的興趣。然而，梁文鋒着眼於公司的長期戰略，拒絕了他們的投資提議。DeepSeek 正在研究如何幫助科技巨頭利用 AI 開發商業應用並分享其中的收益。&lt;/p&gt; 
&lt;p&gt;目前，梁文鋒似乎在堅持他在 2023 年一次罕見採訪中表達的理念。他當時説：「我們不做應用，我們只做研究和探索。」記者問他為什麼這樣做，梁文鋒回答説，原因是好奇心驅動。&lt;/p&gt; 
&lt;p&gt;知情人士還透露，梁文鋒不想對 DeepSeek 的核心 AI 模型收費。這些模型目前是免費的。該公司計劃最早在 4 月發佈其下一個推理模型，旨在解決複雜問題。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338220</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338220</guid>
            <pubDate>Wed, 05 Mar 2025 10:41:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>微信小程序「聊天工具」模式開始內測</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;微信團隊今日發佈公告稱，為更好地支持開發者在微信羣聊場景內服務用户&lt;strong&gt;，小程序「聊天工具」模式開始內測&lt;/strong&gt;。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;功能介紹&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;聊天工具模式是為了幫助小程序更好與微信聊天結合而推出的模式，可用於實現羣問卷、羣拼單、羣任務等功能。其與小程序普通模式相比開放更多與聊天緊密結合的能力：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;聊天成員相關能力：開發者可調用聊天成員選擇器並獲取成員相關 id，通過開放數據域渲染聊天成員的頭像暱稱&lt;/li&gt; 
  &lt;li&gt;發送內容到聊天能力：開發者可發送文本、提醒、圖片、表情、視頻等內容類型到聊天中&lt;/li&gt; 
  &lt;li&gt;動態消息能力：小程序卡片上的輔標題可以動態更新，在用户完成/參與了活動後下發系統消息&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;同時，聊天工具模式需要使用獨立分包基於 skyline 開發，該分包也可被普通模式打開。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;小程序「聊天工具」模式的能力介紹如下：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;聊天成員相關能力&lt;/strong&gt;：開發者可調用聊天成員選擇器並獲取成員相關 id，通過開放數據域渲染聊天成員的頭像暱稱&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://img.ithome.com/newsuploadfiles/2025/3/70dbe39d-824d-4504-9929-aca7c109a807.png?x-bce-process=image/format,f_avif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-681fa9ded30fbe32ccdcc7d158863ac4818.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;發送內容到聊天能力&lt;/strong&gt;：開發者可發送文本、提醒、圖片、表情、視頻等內容類型到聊天中，用户通過內容可進入小程序&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-59fa2aecb7ab97672121ab713e31a28004e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;https://img.ithome.com/newsuploadfiles/2025/3/b38d2ebf-68ab-4a08-ac94-60d09d79681d.png?x-bce-process=image/format,f_avif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;動態消息能力&lt;/strong&gt;：小程序卡片上的輔標題可以動態更新，在用户完成 / 參與了活動後下發系統消息&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-03294ca6c0c39d56a7d7127e664c8534b88.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;詳情查看文檔：&lt;img alt=&quot;&quot; src=&quot;https://img.ithome.com/newsuploadfiles/2025/3/3e6a2e0e-35a8-48c6-bb62-6be791f89764.png?x-bce-process=image/format,f_avif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdevelopers.weixin.qq.com%2Fminiprogram%2Fdev%2Fframework%2Fopen-ability%2FchatTool.html&quot; target=&quot;_blank&quot;&gt;https://developers.weixin.qq.com/miniprogram/dev/framework/open-ability/chatTool.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338202</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338202</guid>
            <pubDate>Wed, 05 Mar 2025 09:04:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>AI 的「臉」該怎麼管？</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;em&gt;人民網記者，趙竹青&lt;/em&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;「靳東兩會建議 AI 換臉立法」「雷軍回應國慶 7 天被 AI 雷軍罵了 8 天」……今年兩會期間，AI 話題頻頻登上熱搜，「AI 換臉」引發的風險問題更是受到代表委員的熱議。 事實上，苦於被 AI 偷走面孔和聲音的，遠不止靳東與雷軍。前有「假張文宏」深夜直播帶貨，後有「假劉曉慶」分享人生雞湯，以及「假古天樂」代言遊戲平台，深度偽造 (Deepfake) 技術自 2017 年問世以來，正以指數級速度滲透日常生活。隨着技術持續進化、門檻不斷降低，「AI 換臉」的不當濫用已成為違法侵權重災區，並有愈演愈烈之勢。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;面對新型侵權形態，現行法律體系正面臨現實挑戰。全國人大代表、小米集團創始人雷軍坦言，現有法律體系仍將 AI 侵權嵌套在隱私權、肖像權、名譽權等傳統框架中，而「被罵 8 天」「因 AI 謠言導致股價下跌」等損失，卻因無法量化舉證而難以維權。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;「這些損害涉及眾多不特定個體，僅靠私益訴訟難以實現有效治理。」最高人民檢察院公益訴訟檢察廳廳長徐向春的觀察一針見血。顯然，面對新技術引發的新問題，仍沿用傳統追責機制，就好比用漁網去攔截數據洪流，是力所不逮的。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;近年來，我國《互聯網信息服務深度合成管理規定》《生成式人工智能服務管理暫行辦法》等法律法規已頒佈實施，《人工智能生成合成內容標識辦法 (徵求意見稿)》完成社會意見徵集，對人工智能治理進行了有益探索。然而，鑑於技術的迅猛發展，治理的「工具箱」仍需不斷擴容。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;如何管好 AI 這張「臉」？全國兩會上，代表委員們圍繞如何更好把握人工智能發展機遇、更好應對新技術帶來的風險挑戰，積極建言獻策。 雷軍建議，加快單行法立法進程，提升立法位階；強化行業自律共治，壓實平台企業等各方的責任和義務；加大普法宣傳的廣度力度，增強民眾的警惕性和鑑別力。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;全國人大代表、TCL 創始人李東生建議，加快人工智能深度合成內容標識管理規章制度的出台，明確懲罰制度，同時還需加強國際合作，形成人工智能生成合成內容的有效監管。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;全國政協委員、中華全國律師協會監事長呂紅兵認為，應當加快推進「小、快、靈」立法，儘早出台行政法規，規範相關領域發展。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;在全國人大代表、四川省律師協會會長李世亮看來，隨着 DeepSeek 在全世界範圍內引發轟動，「AI+」快速融入各行各業，AI 立法「條件已經成熟」。他期待 AI 全面系統性法案的出台，填補我國 AI 法律空白。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;記者注意到，8 日提請審議的全國人大常委會工作報告明確，今年將圍繞人工智能、數字經濟、大數據等新興領域加強立法研究。這也意味着，我國 AI 治理正逐步從「補丁式規範」向「體系化建構」轉型。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;技術狂飆的時代，規則不能只是追趕者。今年全國兩會關於 AI 治理的討論中，一個共識逐漸清晰：唯有構建起「法律劃界、技術鑑偽、平台擔責、全民共治」的多維防護網，才能讓 AI 的「臉」既綻放科技之美，又不失人性之真與善。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338192</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338192</guid>
            <pubDate>Wed, 05 Mar 2025 08:33:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>百度上線 AI 陪伴產品「月匣」App</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;百度近期低調推出情感陪伴類 App「月匣」，主打高自由度 AI 對話與沉浸式劇本互動兩大核心功能。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;img height=&quot;544&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-2b1b8422cc0ee7f9d5a53332bf2f668b7d0.jpg&quot; width=&quot;300&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;根據介紹，這款產品通過構建虛擬角色生態，試圖在泛娛樂社交領域開闢新賽道。與以往百度推出的 AI 社交產品不同的是，百度的這款全新 AI 社交產品，不僅搭載自研的文心一言大模型，還整合了 DeepSeek、豆包、MiniMax abab 三大外部的大模型，構建起「四核驅動」的 AI 社交引擎。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338176</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338176</guid>
            <pubDate>Wed, 05 Mar 2025 07:39:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>TIOBE 3 月榜單：Fortran 和 Delphi 爭奪前十</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;TIOBE 公佈了 2025&amp;nbsp;年 3 月的&lt;/span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.tiobe.com%2Ftiobe-index%2F&quot; target=&quot;_blank&quot;&gt;編程語言排行榜&lt;/a&gt;&lt;span style=&quot;color:#000000&quot;&gt;。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;66&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-538dc8bd513430c8df55d4d61c23929ab13.png&quot; width=&quot;700&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;本月榜單中，一些古老的語言正在悄然上升。其中，Fortran 和 Delphi 正在爭奪前十名的位置，COBOL 和本月新加入的 Ada 則稍稍靠後，但也均成功擠入了 Top 20。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;TIOBE CEO&amp;nbsp;Paul Jansen 認為，這一趨勢與維持世界運轉的許多重要遺留系統有關。&lt;/span&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;它們中的大多數都是藉助這些「dinosaur languages」開發出來的。現在，這些系統的最後一批核心開發人員即將退休，公司為了避免任何風險，選擇保留現有系統，甚至對其進行擴展，而不是用基於更現代語言的更新系統來取代它們。&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;請注意，我們把這些語言稱為「dinosaurs」，但它們已經隨着時間的推移而不斷發展，而且已經相當先進。它們都有新的語言定義。譬如 Fortran 2023、Delphi 12（2024 年發佈）、Ada 2023 和 COBOL 2023。我們可能會對這些語言進入 TIOBE 指數前 20 名感到驚訝，但它們絕對是有作用的，值得稱讚。&lt;/span&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;strong style=&quot;color:#333333&quot;&gt;TIOBE 3 月 TOP 20 編程語言&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;400&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-204e07ccd60ab4b3c568070f91028f34963.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;strong style=&quot;color:#333333&quot;&gt;TOP 10 編程語言 TIOBE 指數走勢（2002-2024）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;221&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-33bfc79079a90aaf3dd4ea7c65b16937581.png&quot; width=&quot;700&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;strong style=&quot;color:#333333&quot;&gt;第 21-50 名編程語言排行&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;413&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-270d625073c800a18b1102408d7f43cbb57.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;第 51-100 名如下，由於它們之間的數值差異較小，僅以文本形式列出（按字母排序）：&lt;/span&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;ActionScript, Algol, Alice, Apex, APL, CFML, CHILL, Clipper, CLIPS, Clojure, Curl, Eiffel, Elm, Erlang, F#, Forth, Groovy, Hack, Icon, Inform, Io, J, JScript, LabVIEW, Ladder Logic, Logo, Maple, Modula-2, Mojo, MQL5, NATURAL, Nim, OCaml, Occam, OpenCL, OpenEdge ABL, PL/I, Q, Raku, Ring, S, Scheme, Simulink, Smalltalk, SPARK, Tcl, Vala/Genie, VHDL, Wolfram, Xojo&lt;/span&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;TIOBE 編程社區指數（The TIOBE Programming Community index）是一個衡量編程語言受歡迎程度的指標，該指數每月更新一次。評判的依據來自世界範圍內的工程師、課程和第三方供應商，包括流行的搜索引擎，如 Google、必應、雅虎、維基百科、亞馬遜、YouTube 和百度都被用於指數計算。值得注意的是，TIOBE 指數並不代表編程語言的好壞或編寫代碼的多少。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;該指數可以用來檢查你的編程技能是否還能跟上時代的步伐，或者在開始建立一個新的軟件系統時，基於指數對採用何種編程語言做出決策。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.tiobe.com%2Ftiobe-index%2Fprogramminglanguages_definition%2F&quot; target=&quot;_blank&quot;&gt;TIOBE 指數&lt;/a&gt;&lt;span style=&quot;color:#000000&quot;&gt;的定義方式，以及詳細榜單信息&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.tiobe.com%2Ftiobe-index%2F&quot; target=&quot;_blank&quot;&gt;均可查看官網&lt;/a&gt;。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338172/tiobe-index-202503</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338172/tiobe-index-202503</guid>
            <pubDate>Wed, 05 Mar 2025 07:30:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>最新消息：DeepSeek-R2 AI 模型將於 3 月 17 日發佈</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#d35400&quot;&gt;&lt;strong&gt;3 月 11 日晚更新&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;針對 DeepSeek 將在 3 月 17 日發佈下一代 R2 模型的傳聞，DeepSeek 官方企業諮詢賬號在用户羣中回應稱，「闢謠：R2 發佈為假消息」。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;1124&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0311/231933_8Rg2_2720166.png&quot; width=&quot;1280&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;據智通財經援引消息人士透露，&lt;strong&gt;DeepSeek 下一代 AI 模型 DeepSeek-R2 或提前於下週一 (3 月 17 日) 正式發佈&lt;/strong&gt;。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;img height=&quot;948&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0311/153001_YozB_2720166.png&quot; width=&quot;1220&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;DeepSeek-R2 在多個關鍵領域實現突破，包括更出色的編程能力、多語言推理能力，以及以更低的成本提供更高的準確性。專業人士認為，這些特性若得以兑現，可能使其在全球 AI 競賽中佔據顯著優勢。這對於資產價格而言，可能又是一次重估機會。&lt;/p&gt; 
&lt;p&gt;據悉，截至目前，DeepSeek 官方尚未正式公佈 R2 的具體日期及技術細節等。早前市場預期 DeepSeek-R2 模型於 5 月發佈。&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;相關閲讀：&lt;a href=&quot;https://www.oschina.net/news/335778/deepseek-rushes-launch-new-ai-model&quot; target=&quot;news&quot;&gt;DeepSeek R2 將提前推出&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338171</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338171</guid>
            <pubDate>Wed, 05 Mar 2025 07:25:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>基於 Megatron 的多模態大模型訓練加速技術解析</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                    
                                                                                                                                                    &lt;p&gt;作者：胡凱文，李鵬，黃俊&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;單位：阿里雲智能集團人工智能平台 PAI 算法團隊&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_1&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;引言&lt;/h1&gt; 
&lt;p&gt;多模態大模型是近期業界關注的熱點，OpenAI 的 GPT4O 以及谷歌 Gemini 等多模態大模型的出現讓人機交互變得更加簡單和自然。這種模型在多種下游任務上表現優異，比如圖文檢索、視覺問答等。通過結合語言理解和視覺感知能力，它能為用户提供更加豐富和自然的人機交互體驗。Pai-Megatron-Patch 是一款由阿里雲人工智能平台 PAI 研發的圍繞英偉達 Megatron 的大模型訓練配套工具，旨在幫助開發者快速上手大模型，打通大模型相關的高效分佈式訓練、有監督指令微調、下游任務評估等大模型開發鏈路。在 Megatron 的基礎之上，Pai-Megatron-Patch 及時追蹤社區最新需求，搭建了包括 LLaMA3、Qwen2 等在內的多種熱門大語言模型，並在此基礎上持續拓展新特性，如支持 Optimizer Offloading 功能的 Distributed Optimizer， 可與 Transformer Engine、MoE 以及流水並行等模塊協同使用，以適應更多場景需求。Pai-Megatron-Patch 的整體技術棧如下圖所示：&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//0965cecdadad37ae040df3b73c232297.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;本文以 Qwen2-VL 為例，從易用性和訓練性能優化兩個方面介紹基於 Megatron 構建的 Pai-Megatron-Patch 多模態大模型訓練的關鍵技術。在易用性方面，最新的 Pai-Megatron-Patch 實現了基於 Mcore 的多模態編碼器和 LLM 解碼器，同時實現了支持高精度低損耗的 Huggingface 和 MCore 多模態模型權重互轉轉換以及並行加載，極大簡化了不同框架間遷移的學習成本和技術障礙。在性能方面，實現了支持高性能的文本/圖像/視頻數據統一加載，實現了高性能的流水並行切分，另外通過引入 CPU 卸載技術以及 Sequence Packing 技術，進一步支持多模態長序列訓練的顯存優化，有效緩解了 GPU 的顯存壓力，提升了大模型訓練的效率與穩定性。這些改進共同作用，為用户提供更加流暢且高效的多模態大模型開發體驗。&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_2&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;支持模型並行的權重轉換&lt;/h1&gt; 
&lt;p&gt;作為開源主流高效大模型訓練工具，用户在嘗試使用 Megatron 訓練開源模型時首先遇到的障礙是 Huggingface 與 Megatron 模型權重格式的差異。下面將以 Qwen2-VL 為例，詳解從 Huggingface 到 Megatron 的模型權重轉換技術。如下圖所示，Qwen2-VL[2] 主要由多模態編碼器和 LLM 解碼器兩個模塊組成。Qwen2-VL 採用了專門設計的多模態編碼器架構，可以同時接受文本/圖像/視頻作為輸入，並將它們轉換成統一形式的表示向量後送入 LLM 解碼器。這種設計使得模型能夠在同一個空間內有效地捕捉到跨模態的信息關聯。與 LLaVA 等多模態模型不同，Qwen2-VL 的視覺輸入不需要縮放到固定分辨率大小，而是根據圖像/視頻數據的原始分辨率，動態生成不同長度的視覺表示向量，這一特性被稱為動態分辨率。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//1eb93737ae6dbd05ddba2e152c5a393b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;通過將 Huggingface 模型轉換到 Megatron 格式，用户可在已經充分訓練好的 Qwen2-VL 基礎大模型或者微調大模型上進行二次訓練。所謂支持模型並行的權重轉換，其核心原理是在轉換時將大型模型權重分割成多個部分，並分配給不同的 GPU 進行並行加載。通過這種方式可以顯著減少單個設備上的顯存佔用量，加快訓練速度。然而，在實際應用中這一技術面臨着若干挑戰。在開發過程中，我們遇到的最常見問題之一是在完成權重轉換後重新加載進行訓練時，step 0 loss 比預期高。造成這種情況的原因主要有兩個方面：一方面是模型實現風格不一致，結構映射關係十分複雜；另一方面是模型權重並行切分算法實現容易出錯。因此，針對上述兩種原因，Pai-Megatron-Patch 分別進行了優化。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;在 Pai-Megatron-Patch 的模型轉換中，針對模型結構映射關係十分複雜的問題，建模分析並構建映射表能有效對比 Huggingface 模型與 Megatron 實現的聯繫。下表總結了兩者分別在各個算子類型上的命名對應關係。在支持模型並行的權重轉換實現過程中，我們首先將 Huggingface 模型的 ckpt 賦值給 Megatron 模型，然後再在 Megatron 模型上進行 TP/PP 切分。這種先轉換再切分的好處是可以提升代碼的可維護性，減少轉換出錯的概率。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//ef715495d37cfd035b51f1b0d64e4bbb.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;除了模塊層面的實現差異，在算子層，由於 3D 並行的存在，Megatron 與 Huggingface 的權重及計算方式也存在很大區別，需要根據實際情況進行轉換。針對模型權重並行切分算法實現容易出錯的問題，逐算子精細化切分保證了自底向上的準確轉換。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//0a7d46cfbd1f0e2bdb39037f639960a9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;上圖表示了一個 Huggingface 實現的 GQA 權重轉換到 Megatron 的正確/錯誤流程，其中每個塊表示一個 head 的權重，邊框和內部的顏色分別代表這個 head 對應的類型及被理解成的類型。在 transformers 庫中，Huggingface 採用 q_proj, k_proj, v_proj 這三個算子來計算 QKV status, 隨後再調用&lt;code&gt;repeat_kv()&lt;/code&gt;函數將同一個 Query Group 內的 KV status 數量與 Query 對齊，進而調用 attention 函數。在 Megatron 中，由於張量並行的存在，Attention Module 採用了 ColumnLinear(linear_qkv) 對 QKV 統一計算，為了降低節點間通信量，這個並行 Linear 將 Query Group 均勻切分到各個 Node 上，使每個節點內就能計算一部分 attention 結果，不需要對 QKV status 進行同步。在這個 case 下，如果直接拼接 QKV 的權重張量（如圖上半部分）再進行 TP 切分，可以明顯看到大部分 head 位置出現問題，因此，我們需要將 QKV 權重進行一定轉換，按 Query Group 的順序進行拼接（如圖下半部分）後再切分才能保證 Attention 的正確性。具體地説，在進行 TP=2 切分時，Pai-Megatron-Patch 先將 query，key 和 value 這三個 tensor 分別 reshape 成一個 4D 的 tensor，reshape 後的 shape 是&amp;lt;num_query_groups, -1, head_dim, hidden_dim&amp;gt;。接着沿着第二個維度拼接在一起，然後沿着 num_query_groups 維度進行算子拆分。才能確保並行加載 ckpt 後的起步 loss 值誤差偏移在合理範圍內。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//13b6f29e544514a59b0fa5f932487c7e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_3&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;用户友好的多模態數據加載&lt;/h1&gt; 
&lt;p&gt;相對於 LLM 預訓練，在數據加載方面，由於引入了多個類型的數據，多模態模型的 DataLoader 尤其複雜。為了支持 LLaVA 等模型，英偉達的 Megatron 進一步推出了 Energon 來實現多模態數據的加載邏輯。儘管如此，由於目前多模態輸入並無統一的格式，在目前的 Megatron 中，對於多模態數據的支持仍比較有限，具體表現在 (1) 僅支持部分格式簡單的數據加載，例如每個樣本至多包含一張圖像或一個視頻，應用場景有限;(2) 現有 TaskEncoder 輸出格式固定，靈活性不大，難以高效支持模型訓練。在實際開發中，Qwen2-VL 同時遇到了上述兩類障礙:與已有的 LLaVA 相比，Qwen2-VL 基於 ChatML 格式設計輸入，應當支持單個樣本中多張圖片視頻、多輪對話等複雜功能，但這類數據無法使用當前 energon 的代碼進行讀取;此外，現有 TaskEncoder 僅支持將圖片縮放到固定大小，再傳入模型訓練，因此難以支持 Qwen2-VL 獨有的動態分辨率特性。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;為瞭解決上述功能性問題，我們首先對內置的 DataLoader 代碼做了大量擴展，使其能基於輸入的原始多模態數據構造用於動態分辨率訓練的圖像切片序列以及位置信息，同時支持可自定義 prompt、多輪、包含任意數量圖像或視頻的對話樣本的數據處理。在此基礎上，我們設計了一套自動化腳本，能基於用户給定的 sharegpt 格式數據集路徑自動轉換成 energon 可讀取的 webdataset 數據集，繞過當前 energon 數據集準備的交互流程，加快數據轉換效率，同時降低用户學習成本。此外，對於數量不定的圖像，為了能在 energon 側自動解碼，webdataset 支持將其以 numpy array 的形式保存，然而，由於 jpg 到 ndarray 間的數據格式差異，在實際測試中，我們發現這將造成最終數據集文件體積出現幾倍到幾十倍的增加。為瞭解決這個問題，我們在運行時向 webdataset 的編解碼模塊增加鈎子函數，使其支持圖像文件列表的自動編解碼，實現在按原始二進制數據保存的同時能被 energon 自動轉換為圖像張量的需求。通過上述優化，在 Pai-Megatron-Patch 中，經過轉換的 sharegpt 格式數據相對於其原始總大小几乎沒有變化。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//6d2c8ff40024e1e3d09c02ef056fc475.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_4&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;視覺特徵處理流程優化&lt;/h1&gt; 
&lt;p&gt;與 LLM 相比，多模態模型先對視覺數據進行編碼，再與文本特徵拼接後送入 LLM 進行推理。與基於靜態分辨率的多模態大模型不同，複雜的多模態數據格式以及動態分辨率共同使得 Qwen2-VL 的這一過程更加複雜，不僅顯著影響訓練效率，也與能否應用 TP Comm Overlap 等技術相關。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;與靜態分辨率使用固定數量視覺 token 不同，動態分辨率技術允許模型依據輸入圖像大小，將其編碼成不定數量的視覺特徵。這一改進使模型對於高分辨率圖像的細節捕捉能力獲得顯著提升，同時也優化了低分辨率圖像的推理性能。下圖是 Qwen2-VL 的訓練數據的簡單示例，針對不同長度的視覺 token，最終它們會被嵌入到文本序列中，與同一個 batch 的數據拼合後送入到 LLM 解碼器。在這個過程中主要存在兩個難點:(1) 如何對同一批次內的多個視覺輸入高效獲得特徵表示;(2) 如何拼合數據以支持原生性能優化開關。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//739d529e86745f50251962405174bef3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;由於視覺輸入的分辨率具有很大不確定性，為了將同一 batch 內的不同長度視覺數據轉換成推理所需的特徵表示，常見的做法是將每個視覺數據填充到相同長度後統一送入視覺模型。儘管這一方式實現簡潔，但填充的 token 不僅造成了顯存的浪費，同時也影響了視覺編碼器的吞吐，當同一批次數據中同時包含高分辨率視頻及低分辨率圖像，由於視覺 token 數量的顯著差異，將造成極為顯著的性能浪費。對此，Pai-Megatron-Patch 借鑑了 Sequence Packing 的做法，將同一批次內的所有視覺輸入打包後調用 varlen attention，來避免填充操作帶來的顯存/性能損耗。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//939334d510ecedf2be878901ae2b15b7.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;在生成視覺特徵表示後，基於預先構造好的掩膜張量，視覺編碼器的輸出會被依次填入文本表示的對應位置，替換掉默認的佔位符。在不使用任何性能優化技術的情況下，這一實現並不會帶來什麼問題。然而，當開啓序列並行後，LanguageEmbedding 模塊的輸出會自動 reduce scatter 到各個 TP rank，導致基於原始輸入構造的完整掩膜張量無法使用。為瞭解決這一問題，同時避免冗餘通信，Pai-Megatron-Patch 針對性修改了 MCore 中實現的 LanguageEmbedding 模塊，在輸入序列拆分前增加利用掩膜張量替換文本表示的操作 ，不幹擾序列並行特性的正常運行。&lt;strong&gt;在實現序列並行機制的基礎上，通過進一步應用 TP Communication Overlap 特性，對於 Qwen2-VL-70B 我們在 4 機 32 卡 a100 上觀察到了 6% 性能提升。（詳見實驗部分）&lt;/strong&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_5&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;基於優化器卸載的多模態長序列顯存優化&lt;/h1&gt; 
&lt;p&gt;相對於大語言模型訓練微調，由於視頻、音頻等模態存在，多模態大模型的輸入序列長度將遠遠大於純的語言模型。因此，在通常情況下，為了完成這類模型的訓練微調，往往需要更多的計算資源。而當獲取更多資源這條路徑不可行時，在資源受限的情況下拉起任務成為這一場景下的首要挑戰。在之前 Pai-Megatron-Patch 實現的優化器卸載特性基礎上，最近，PAI 團隊與英偉達 Megatron 團隊深度合作，共建了一套原生基於 Megatron 的權重卸載優化器。在此基礎上，通過與多種正交的卸載技術結合，Pai-Megatron-Patch 將四機 32 卡 A100 上的 Qwen2-VL 的最長上下文從 4K 提升至 32K。在本章節中，我們將以 BF16 訓練為例，詳細描述最新版權重卸載優化器的原理、實現及我們引入的最新性能優化特性--Optimizer Overlapping。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;如下圖，與激活重計算或激活卸載等顯存優化技術不同，優化器卸載的目的在於將參數更新這一計算負載較低但顯存佔用相對高的步驟全部或部分放置到 CPU 上進行，以達到降低顯存佔用的目的。對於一個參數量為 M 的模型，Adam 會默認分配大小為 12M 字節的顯存以保存全精度的參數 (4M) 及優化器狀態 (8M)，當使用優化器卸載時，優化器則會將這 12M 大小的張量全部分配到內存上，同時額外分配 4M 的內存用於拷貝 GPU 上的全精度梯度數據。因此，當打開 Megatron 的分佈式優化器時，以 4 機 32 卡訓練 70B 為例，每張卡的顯存能進一步減少約 26GB。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//d9895494e5ede2ef2bfd1f6e0b119cdc.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;如下圖所示，在最新的原生優化器卸載特性實現中，我們設計了一個混合設備優化器類 (Hybrid Device Optimizer, HDO) 來支持靜態優化器卸載功能。與先前的深度修改 DistributedOptimizer，難以快速遷移到其他訓練代碼不同，通過在頂層沿用 PyTorch 優化器的 API，HDO 能無縫替換 TorchAdam，同時仍保留一定的優化器卸載能力。我們希望這一設計能使用户在更多場景下體驗到優化器卸載帶來的資源需求下降*注。整個 HDO 大致分為兩部分，包括針對單個參數張量更新的底層優化器集合，以及用於管理優化器參數映射關係、優化器保存/讀取、以及更新過程中子優化器間同步的 HDO 類。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//f44b34f0fc6d5d9c7efc682cbcd49061.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;由於在先前版本的 Megatron-Core 中，優化器內全精度參數創建部分由 DistributedOptimizer 默認分配到 GPU 上，並不能進行卸載，導致最初 HDO 的實際顯存優化僅有 8M 而非舊版深度定製 DO 的 12M。為瞭解決這一問題，我們修改了 DO 的參數創建邏輯，當識別到用户啓用優化器卸載特性時，DO 自動將全精度參數創建這一步驟交由 HDO 進行，避免了冗餘顯存分配。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;注: 由於參數已經被外部創建，為了不對這些參數進行修改，這一場景下僅能將 8M 的 Adam/AdamW 優化器狀態分配及參數更新移動到 CPU 上進行，對於 SGD 優化器，由於無額外的固定顯存分配，HDO 沒有卸載效果。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;優化器初始化&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;與支持單一設備張量的優化器不同，為了實現位於不同設備上的多個張量的參數更新及性能優化，HDO 被實現為多個子優化器的集合，每個子優化器負責單一設備上的數個張量的參數更新，而 HDO 用於實現主優化器與子優化器間的優化狀態同步、Device/Host 間的參數/梯度傳遞以及由此引發的 CUDA 同步問題。在優化器初始化階段，相對於普通的 Adam，HDO 額外要求用户指定 CPU/CUDA 上的子優化器類型以及相應的卸載比例，隨後基於這一比例對 param_groups 進行拆分，再重新構造對應設備上的子優化器。在訓練階段，由於 lr_scheduler 等外部因素可能會對優化器參數產生影響，在子優化器參數更新前，HDO 會將所有狀態同步到子優化器，如果子優化器在 CPU 上更新，也會將 GPU 梯度複製到 CPU 上。由於子優化器也可能修改參數組的狀態，在更新結束後，每個子優化器也會將這些狀態同步到主優化器，從而保證與非優化器卸載訓練的收斂一致性。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;優化器保存/讀取，及 卸載比例變換&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;相對於 Pai-Megatron-Patch 內的優化器卸載模塊，HDO 支持完整的優化器保存/讀取特性，同時也支持加載優化器時切換優化器卸載比例，具有更大的靈活性。如前文所述，HDO 內部由多個子優化器負責實際參數更新，在每個參數更新階段開始前/結束後分別進行 HDO 及子優化器間參數的同步。在保存優化器狀態時，由於 HDO 內數據與子優化器一致，HDO 的 state_dict 即包含恢復優化器所需的全部信息。在加載權重時，由於子優化器需要基於 fp32 參數進行更新，我們引入了 pre_load_state_dict_hook 以及 post_load_state_dict_hook，將 HDO 的 state 中的半精度參數數據臨時用保存的 fp32 替換後同步到子優化器，再替換回來，隨後調用&lt;code&gt;_move_new_state_to_right_device&lt;/code&gt;，將每個子優化器的狀態移動到其對應的設備。通過在運行時更新子優化器的對應設備，我們能將在一個卸載比例下保存的模型在另一個優化器卸載配置上拉起訓練。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Naive Optimizer Overlapping 技術&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;與 CUDA 上的參數更新相比，基於 CPU 的參數更新性能成為優化器卸載技術是否可用的關鍵因素。通常，優化器卸載的時間包含三個部分：將 CUDA 梯度同步到 CPU(D2H)、在 CPU 上進行參數更新 (C)、將更新後的 CPU 參數同步到 CUDA(H2D)。為了儘可能提升性能，我們提出了一項 Naive optimizer Overlapping 技術，通過 PyTorch 的 CUDA Stream 機制，實現通信與計算重疊，將 CPU-CUDA 間數據拷貝的時間儘可能掩蓋在 CPU 計算下。在 Naive optimizer Overlapping 中，我們的一個主要改進是將 CPU 子優化器進一步打散，使一個 CPU 參數對應一個 CPU 優化器，同時令所有拷貝非阻塞化。這一改進允許 HDO 在單個梯度張量同步完成後立即調用 CPU 參數更新，無需等待所有梯度完成拷貝，也允許 CPU 參數更新完成後立刻非阻塞拷貝到 GPU，從而實現重疊。為了保證多個 CUDA Stream 以及 CPU 之間正確的數據同步關係，同時儘可能避免影響性能，我們僅在關鍵位置引入 CUDAEvent，來避免額外流同步帶來的開銷。下圖表示了一個理想的優化器重疊場景，其中每個參數張量的更新時間與兩次數據傳輸接近。當流水線熱身結束後（time step 2），可以看到 HDO 在兩個 CUDA Stream 上進行雙向的數據拷貝同時，CPU 也在同時進行參數更新。這一方式最大化掩蓋了冗長的數據傳輸時間，&lt;strong&gt;在我們測試的 LLaMA2-70B 訓練中，我們觀察到該優化能減少約 1s 的端到端時間。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//5ca7cdd30c2c3af2cf816eeb23f0e253.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_6&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;多模態流水並行性能優化&lt;/h1&gt; 
&lt;p&gt;多模態大模型訓練微調作為英偉達 Megatron 的新特性，目前仍在持續開發中。在 3D 並行方面，目前以 LLaVA 為代表的多模態模型的類型均為 ModelType.encoder_and_decoder，它對 encoder（視覺編碼器）與 decoder（LLM 解碼器）分別引入了一套 TP/PP 參數，同時支持特殊的 Encoder PP 配置：當 Encoder PP&amp;gt;0 時，Megatron 會將 Encoder 放在獨立的 GPU 上，允許兩部分模型使用不同的 TP/PP 參數初始化；當 Encoder PP=0 時，Megatron 將 Encoder 部分按照 Decoder 的 TP 切分後，與 Decoder 的第一個 PP Stage 合併，放置在同一組 GPU 上。其中，前種切分方式一般適用於編碼器/解碼器結構類似、參數量接近的情況；後者則適用於視覺編碼器參數量較少，計算量不大的場景。考慮到 Qwen2-VL 的視覺編碼器大小以及用户體驗，我們最初採用了編碼器與解碼器部分合並的切分方式，來保持和其他 LLM 相似的切分參數配置，減少學習成本。但在實際測試中，通過與基模型 Qwen2 對比，我們觀察到這一合併顯著降低了 Qwen2-VL 的訓練吞吐。這主要由兩方面因素導致：(1) 由於模型結構不一致，視覺編碼器與 LLM 解碼器的前向速度有差異，導致 micro batch 內哥 GPU 間存在負載不均衡;(2) 由於視覺編碼器支持動態分辨率特性，對於每個 micro batch，實際前向的 token 數並不一致，使得每個 mciro batch 計算量略有差異，出現各 GPU 間存在負載不均衡的情況。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;為瞭解決上述負載不均衡的問題，進一步提升訓練吞吐，在原生 Megatron 的基礎上，我們從兩個方向對現有框架進行改進嘗試：(1) 基於非均勻切分策略間的負載均衡;(2) 拓展模型實現以支持虛擬流水線並行特性。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;基於非均勻切分策略間的負載均衡：在模型總參數量較小或 PP 較大的並行配置下，pp rank 0 包含的視覺編碼器帶來的額外計算量相對於原有的 LLM 解碼器的計算量比例有顯著提升。由於在流水線中，最慢的步驟決定了整體的性能，應用 megatron 內的非均勻切分特性，將部分計算量轉移到其他 pp rank 上，能有效提升整體的訓練性能。為了支持基於非均勻切分的繼續預訓練或微調功能，我們同步更新了 Qwen2-VL 的轉換模型，用户可以通過控制 MP_PP0_LAYERS 環境變量，獲得非均勻切分的模型權重文件。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//32be076a3fe4ab8d995a8aa30ddcf555.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;虛擬流水線並行特性：虛擬流水線並行，或 interleaved 1F1B PP 是 Megatron-LM 在 1F1B 的流水線並行實現上的重要改進。它通過將同一個 pp stage 內的 transformer 層分配到多個 GPU，進一步打碎計算，通過增大通信量的方式使負載更加均衡，降低 Global Batch 內的 bubble 比例。然而， 對於 encoder_and_decoder 類型的模型（即英偉達內置的所有多模態模型），MCore 不支持啓用 VPP。考慮到視覺編碼器的整體計算量不需要獨立 GPU，我們進一步改進了 Qwen2-VL 實現，移除了 Encoder PP 的支持，並將 Qwen2-VL 的模型類型設置為 LLM 的 encoder_or_decoder 來啓用 VPP 選項。&lt;strong&gt;與官方論文的結果不同，我們觀察到虛擬流水線並行也能起到訓練加速效果，甚至在 H20 上，我們觀察到相對於非均勻切分，虛擬流水線並行的加速效果更加明顯。&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id=&quot;OSC_h1_7&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;實驗分析&lt;/h1&gt; 
&lt;p&gt;在本節中，我們對上述提到的技術對 Qwen2-VL 模型的性能及收斂情況展開了全面的分析，主要包括三方面：1. 權重轉換精度分析：通過對比轉換前後的模型測試集表現，我們驗證了權重轉換模塊的正確性；2.多模態長序列顯存優化分析：通過引入 HDO 對優化器進行卸載，在相同 GPU 數量的情形下，Pai-Megatron-Patch 大大提升了長序列訓練的可用性；3.多模態模型訓練性能優化：全面測試了各訓練加速技術對於不同設備/模型大小的影響，論證了技術的有效性，同時為用户提供開關設置的指引。&lt;/p&gt; 
&lt;span id=&quot;OSC_h3_8&quot;&gt;&lt;/span&gt; 
&lt;h3&gt;權重轉換精度分析&lt;/h3&gt; 
&lt;p&gt;為了驗證 PAI-Megatron-Patch 內多模態模型轉換的正確性，我們採用 VLMEvalKit 對轉換前後的 Qwen2-VL-7B 模型在多個數據集上的表現進行評估。結果如下圖，其中 Ref 是 VLMEvalKit 提供的在 SEEDBench 及 MMBench 上的評估分數，Official 是官方權重在測試環境內的實測分數，Convert 為將官方原始權重進行兩次轉換得到的新 Huggingface 權重的分數。可以看到，轉換前後的模型評估分數完全一致，且均與開源 leaderboard 指標接近，有效説明瞭轉換代碼的正確性。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//03efee772c4e46b60b013c2d364ef1c2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h3_9&quot;&gt;&lt;/span&gt; 
&lt;h3&gt;基於優化器卸載的多模態長序列顯存優化&lt;/h3&gt; 
&lt;p&gt;在本節中，我們探索了通過 HDO 與其他顯存優化技術複合，進一步提升有限機器場景下可訓練上下文長度上限的可能性。我們分別測試了 Qwen2-VL 7B/72B 在單機 8 卡、四機 32 卡的無 offload 情況下的最長上下文，以及打開所有優化後的上下文長度，結果如下表。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//e1dc0526f3b5d0fc7ccb86254065117d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;可以看到，對於 Qwen2-VL 7B 模型，通過結合激活重計算技術及優化器卸載技術，我們將原本 64K 上下文長度的訓練上限進一步拓展到 128K，且沒有 OOM 風險。*對於 72B 模型，結論類似，通過結合多種正交的顯存優化技術，我們也將四機 32 卡 A100 的可訓練上下文長度從 16K 提升到 128K。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;*注：在單獨開啓重計算的情況下我們也成功在 demo 數據集上實現了 128K 上下文長度的訓練，此時顯存空閒不到 1G，考慮到實際場景下視覺模型佔用的顯存會隨數據變化等原因，不推薦使用這一配置訓練。&lt;/p&gt; 
&lt;span id=&quot;OSC_h3_10&quot;&gt;&lt;/span&gt; 
&lt;h3&gt;多模態模型訓練性能優化&lt;/h3&gt; 
&lt;p&gt;在本節中，我們針對兩種大小的 Qwen2-VL 模型的訓練場景，測試了 PAI-Megatron-Patch 當前支持的訓練優化技術對吞吐的影響，來説明其在不同場景下的優勢。具體地，我們採用預處理的 LLaVA-Pretrain 數據集 (參見 Qwen2-VL 的最佳實踐文檔) 在 4K 上下文長度下對 Qwen2-VL-7B/70B 進行訓練，在最佳並行配置的基礎上，進一步打開各類優化技術以比較模型性能的變化。其中，&lt;code&gt;PP0_layers&lt;/code&gt;表示開啓非均勻切分時，PP Rank0 上的 LLM Decode r 層數，&lt;code&gt;TGP&lt;/code&gt;內的三個字母依次表示&lt;code&gt;tp-comm-overlap&lt;/code&gt;、&lt;code&gt;overlap-grad-reduce&lt;/code&gt;、&lt;code&gt;overlap-param-gather&lt;/code&gt;三個優化開關的啓用狀態。對於每個實驗，我們記錄了其訓練時的每秒 token 數 (TGS) 及 MFU。需要注意的是，由於 megatron-core 暫不支持估計視覺編碼器的 TFLOPs，記錄的 MFU 僅基於 LLM 的運算量計算得到，因此略低於實際值。&lt;/p&gt; 
&lt;span id=&quot;OSC_h4_11&quot;&gt;&lt;/span&gt; 
&lt;h4&gt;overlap 開關對訓練吞吐的影響&lt;/h4&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//8ccde1833962a155ee78229e63d7572e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;相對於 LLM，由於存在多個不同結構的 transformer，多模態模型並不原生支持 megatron-core 的某些 overlap 技術，同時差異化的結構也可能導致 overlap 的收益與 LLM 不同。在本節中，我們在 A100 上測試了&lt;code&gt;tp-comm-overlap&lt;/code&gt;、&lt;code&gt;overlap-grad-reduce&lt;/code&gt;、&lt;code&gt;overlap-param-gather&lt;/code&gt;三個優化開關對 Qwen2-VL 7B/72B 訓練吞吐的影響，結果如上表所示。主要結論如下：&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;對於 Qwen2-VL，可以觀察到&lt;code&gt;overlap-grad-reduce&lt;/code&gt;及&lt;code&gt;overlap-param-gather&lt;/code&gt;無論在 7B/72B 模型規模上均導致了性能出現略微下降。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;相對於 72B 模型 (~8%),7B 模型開啓&lt;code&gt;tp-comm-overlap&lt;/code&gt;的直接收益可以忽略不計。這主要是因為 7B 模型實際運行時的低 TP 數以及未開啓 overlap 的 vision model 計算量佔比相對較高。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;span id=&quot;OSC_h4_12&quot;&gt;&lt;/span&gt; 
&lt;h4&gt;虛擬流水並行及非均勻切分對訓練吞吐的影響&lt;/h4&gt; 
&lt;p&gt;基於 megatron-core 的核心能力，Pai-Megatron-Patch 為 Qwen2-VL 提供了兩種性能優化方式：多模態大模型的虛擬流水並行及解碼器非均勻切分技術。為了比較這兩者對訓練吞吐的影響差異，我們在最佳並行配置的基礎上，比較了兩種技術的性能上限，結果如下表。其中，VP 表示每個虛擬並行塊中的解碼器 Transformer 層數（區別於通常意義的 VPP Size）。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//e941277896d9cc4003214357264a1e77.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;從表中我們能得出的主要結論如下：&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;儘管&lt;code&gt;tp-comm-overlap&lt;/code&gt;僅能為 Qwen2-VL 72B 模型直接帶來約 8% 的性能提升，然而，通過序列並行帶來的顯存優化，模型有進一步降低並行數來提升吞吐的潛力。例如在 H20 上，我們觀察到，通過打開 SP 及 TP Comm overlap 開關，模型可以採用 TP4PP4 的配置進行訓練，通過調整虛擬流水並行，相對於 TP8PP4 的最佳性能有額外 15% 的性能提升。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;由於各 GPU 的機內、機間帶寬差異，在不同的 GPU 上，最佳優化技術有所不同。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;與 Qwen2-VL 論文的結果不同，我們的 VPP（或 1F1B interleaved）實現在 A100/H20 機器上均有一定的性能提升。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;span id=&quot;OSC_h1_13&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;總結&lt;/h1&gt; 
&lt;p&gt;在基於 Megatron 的 Qwen2-VL 多模態大模型最佳實踐開發過程中，我們圍繞大模型訓練測試了以下核心技術的性能：&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;支持模型並行的權重轉換。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;基於 CPU 卸載的多模態長序列顯存優化的魯棒性。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;基於 Megatron 的多重訓練加速技術的穩定性。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;運用綜合加速技術來訓練 Qwen2-VL 過程中的易用性和穩定性。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;後續在 Pai-Megatron-Patch 中還會陸續放出更多高質量的大模型最佳實踐以及最新的訓練加速技術應用示例，敬請期待。&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_14&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;參考文獻&lt;/h1&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Qwen2-VL: Enhancing Vision-Language Model&#39;s Perception of the World at Any Resolution&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;MMBench: Is Your Multi-modal Model an All-around Player?&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fopen-compass%2FVLMEvalKit&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot;&gt;https://github.com/open-compass/VLMEvalKit&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/u/5583868/blog/17876695</link>
            <guid isPermaLink="false">https://my.oschina.net/u/5583868/blog/17876695</guid>
            <pubDate>Wed, 05 Mar 2025 07:09:00 GMT</pubDate>
            <author>原創</author>
        </item>
        <item>
            <title>奇安信：攻擊 X 平台的殭屍網絡與春節攻擊 DeepSeek 的為同一組織</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;美國東部時間 3 月 10 日，埃隆·馬斯克一天內遭遇了雙重打擊：除了特斯拉股價下跌 15%、自高點幾乎腰斬之外，其旗下的社交媒體 X 平台（原 Twitter）還遭遇了大規模網絡攻擊，導致全球範圍內多次宕機，許多用户無法正常使用該應用程序。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-3d867334583b4a067b5d5ac7e28903e2b6f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-c22e078be8628892b35aceafc606813c59c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FBSbnMTxG_piE0Mi_bH4v0A&quot; target=&quot;_blank&quot;&gt;奇安信 Xlab 實驗室監測發現&lt;/a&gt;&lt;/u&gt;，&lt;strong&gt;此次攻擊所使用的殭屍網絡為 Mirai 變種殭屍網絡 RapperBot，&lt;/strong&gt;攻擊時間與 X 平台宕機時間完全吻合，主要集中在北京時間 3 月 10 日晚 10 點至 11 日凌晨 2 點。&lt;strong&gt;與 2025 年春節期間攻擊 DeepSeek 的屬於同一組殭屍網絡&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;據稱該殭屍網絡常年活躍，以高強度的流量攻擊聞名，平均每天攻擊上百個目標，高峯時期指令上千條，能夠迅速癱瘓目標服務器。攻擊目標分佈在巴西、白俄羅斯、俄羅斯、中國、瑞典等地區。此次攻擊規模之大、烈度之猛，直接導致 X 平台在美國東部時間 3 月 10 日遭遇三次明顯服務中斷，最高時有 20538 名用户報告故障。&lt;/p&gt; 
&lt;p&gt;RapperBot 殭屍網絡並非普通黑客組織，而是對外提供有償攻擊服務的「職業打手」。其攻擊規模和資源投入遠超普通網絡攻擊，可能涉及大型組織甚至國家層面的支持。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338164</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338164</guid>
            <pubDate>Wed, 05 Mar 2025 07:04:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>RWKV7-G1 0.1B 推理模型發佈，最適合嵌入式的純血 RNN 模型</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;2025 年 3 月 10 日，RWKV 基金會發布第一個 &lt;strong&gt;RWKV-7 推理模型&lt;/strong&gt;（Reasoning Model）： &lt;strong&gt;RWKV7-G1&lt;/strong&gt; 0.1B。&lt;/p&gt; 
&lt;p&gt;RWKV7-G1 系列模型擁有&lt;strong&gt;傑出的推理能力&lt;/strong&gt;，且原生支持世界 100+ 種語言和代碼。即使是最小的 0.1B 也能回答&lt;strong&gt;開放性和創造性問題&lt;/strong&gt;。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;RWKV7-G1（&quot;GooseOne&quot;）系列推理模型是基於 World v3.5 數據集&lt;strong&gt;繼續訓練&lt;/strong&gt; RWKV-7 &quot;Goose&quot; World 系列模型。&lt;/p&gt; 
 &lt;p&gt;World v3.5 數據集包含更多小説、網頁、數學、代碼和 reasoning 數據，總數據為 5.16T tokens。對於 0.1B 模型，我們會隨機採樣其中的 1T tokens 訓練。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;目前我們已能在手機高通 8gen3 以 62 token/s 推理 RWKV-7 1.5B 模型，而 0.1B 模型在樹莓派也能跑得挺快，歡迎做嵌入式的朋友加入 RWKV 技術羣討論。&lt;/p&gt; 
&lt;h2&gt;模型表現&lt;/h2&gt; 
&lt;p&gt;RWKV7-G1 0.1B 模型回答 &lt;code&gt;simulate SpaceX mars landing using python&lt;/code&gt;（使用 python 模擬 SpaceX 火星着陸）」：&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;RWKV G1 0.1B simulate SpaceX mars landing using python&quot; height=&quot;2276&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-a90a626d3589d084f50ce2836131854b642.jpg&quot; width=&quot;650&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;日本開發者測試 RWKV7-G1 0.1B 的多語言能力：&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;RWKV7-G1-0.1B-jpn&quot; height=&quot;480&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-30c2c03d927be963bcead808ee26ffd2196.png&quot; width=&quot;600&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;如此小的 0.1B 模型，也能同時支持世界 100+ 種語言和代碼。更大參數的 RWKV7-G1 0.4B/1.5B/2.9B &lt;strong&gt;正在同時訓練中&lt;/strong&gt;。&lt;/p&gt; 
&lt;h2&gt;英文和多語言測評&lt;/h2&gt; 
&lt;p&gt;RWKV7-G1 0.1B 的英文和多語言能力相比 RWKV-7-World 0.1B 繼續提升：&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;141&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-c2badec5fa9dba560aa79f167930e033418.png&quot; width=&quot;1441&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;我們也對 RWKV7-G1 0.1B 進行了 「無法作弊的模型評測」 Uncheatable Eval，可見 RWKV7-G1 0.1B 對於多種新數據的壓縮率，顯著超越所有其它同尺寸的開源模型：&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;154&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-d64cc13d9c6b94f4752c404005bbf9b83bb.png&quot; width=&quot;700&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Uncheatable Eval：&lt;/strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fspaces%2FJellyfish042%2FUncheatableEval&quot; target=&quot;_blank&quot;&gt;https://huggingface.co/spaces/Jellyfish042/UncheatableEval&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;訓練中的 RWKV7-G1 1.5B 模型&lt;/h2&gt; 
&lt;p&gt;以下示例基於 &lt;code&gt;RWKV7-G1-1.5B-16%trained&lt;/code&gt;模型，注意這個模型目前只訓練了 16%。後續 100% 訓練完成的 RWKV7-G1 1.5B 會顯著更強：&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;1556&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-4b81465fc759bfe4e90f4c9969ce95b71d2.png&quot; width=&quot;812&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;RWKV7-G1-1.5B-16%trained 的示例二：&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;541&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-a300e868682a29ea61efa75a25523cee5fd.png&quot; width=&quot;660&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;訓練中的 RWKV 模型可在 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2FBlinkDL%2Ftemp-latest-training-models%2Ftree%2Fmain&quot; target=&quot;_blank&quot;&gt;https://huggingface.co/BlinkDL/temp-latest-training-models/tree/main&lt;/a&gt; 下載。&lt;/p&gt; 
&lt;h2&gt;模型試用&lt;/h2&gt; 
&lt;p&gt;可以在 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fspaces%2FBlinkDL%2FRWKV-Gradio-2&quot; target=&quot;_blank&quot;&gt;Hugging Face Gradio Demo&lt;/a&gt; 試用 RWKV7-G1 0.1B 模型。&lt;/p&gt; 
&lt;p&gt;G1 的整體 prompt 格式與 RWKV-7 模型類似，可選使用 &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; 標籤開啓 reasoning 功能：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;User: 你不許參加學術派對！

Assistant: &amp;lt;think&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;RWKV Runner 和 Ai00 等 RWKV 推理工具&lt;strong&gt;正在適配 reasoning 聊天模式&lt;/strong&gt;，因此目前只能在&lt;strong&gt;續寫模式中&lt;/strong&gt;體驗 reasoning 功能。&lt;/p&gt; 
&lt;h2&gt;模型下載&lt;/h2&gt; 
&lt;p&gt;下載已完成訓練的 RWKV7-G1 0.1B 模型：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Hugging Face：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2FBlinkDL%2Frwkv7-g1%2Ftree%2Fmain&quot; target=&quot;_blank&quot;&gt;https://huggingface.co/BlinkDL/rwkv7-g1/tree/main&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;魔搭社區：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmodelscope.cn%2Fmodels%2FRWKV%2Frwkv7-g1%2Ffiles&quot; target=&quot;_blank&quot;&gt;https://modelscope.cn/models/RWKV/rwkv7-g1/files&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;WiseModel：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwisemodel.cn%2Fmodels%2Frwkv4fun%2FRWKV-7-G1%2Ffile&quot; target=&quot;_blank&quot;&gt;https://wisemodel.cn/models/rwkv4fun/RWKV-7-G1/file&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;下載其他訓練中的 RWKV7-G1 模型：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Hugging Face：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2FBlinkDL%2Ftemp-latest-training-models%2Ftree%2Fmain&quot; target=&quot;_blank&quot;&gt;https://huggingface.co/BlinkDL/temp-latest-training-models/tree/main &lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;魔搭社區：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmodelscope.cn%2Fmodels%2FRWKV%2Ftemp-latest-training-models%2Ffiles&quot; target=&quot;_blank&quot;&gt;https://modelscope.cn/models/RWKV/temp-latest-training-models/files&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;G1 模型發佈計劃&lt;/h2&gt; 
&lt;p&gt;當前已發佈 G1 0.1B 模型，正在訓練 G1 0.4B/1.5B/2.9B，具體發佈計劃如下：&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;th&gt;模型&lt;/th&gt; 
   &lt;th&gt;發佈計劃&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;G1 0.1B&lt;/td&gt; 
   &lt;td&gt;3 月 8 日（已發佈）&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;G1 0.4B&lt;/td&gt; 
   &lt;td&gt;3 月下旬&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;G1 1.6B&lt;/td&gt; 
   &lt;td&gt;4 月&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;G1 2.9B&lt;/td&gt; 
   &lt;td&gt;5 月&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;我們也在同時準備更大更優的數據集 &lt;strong&gt;World v3.7&lt;/strong&gt;，用於 G1 7B 訓練。&lt;/p&gt; 
&lt;h2&gt;RWKV-7 學術支持&lt;/h2&gt; 
&lt;p&gt;RWKV 社區近期新增了大量 RWKV 學術研究論文，以下是截至 2025 年 2 月的 RWKV 論文數量統計表格：&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;546&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-ad88e36470c390ace32e587452140fa813c.png&quot; width=&quot;650&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;歡迎大家基於 RWKV-7 進行創業、科研，我們也會為基於 RWKV 的項目提供技術支持。&lt;/p&gt; 
&lt;p&gt;如果您的團隊正在基於 RWKV 創業或開展研究，請聯繫我們！（在「RWKV 元始智能」微信公眾號留言您的聯繫方式，或發送郵件到「contact@rwkvos.com」。）&lt;/p&gt; 
&lt;h2&gt;加入 RWKV 社區&lt;/h2&gt; 
&lt;p&gt;歡迎大家加入 RWKV 社區，可以從 RWKV 中文官網瞭解 RWKV 模型，也可以加入 RWKV 論壇、QQ 頻道和 QQ 羣聊，一起探討 RWKV 模型。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;📖 RWKV 中文文檔：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.rwkv.cn&quot; target=&quot;_blank&quot;&gt;https://www.rwkv.cn&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;💬 RWKV 論壇：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcommunity.rwkv.cn%2F&quot; target=&quot;_blank&quot;&gt;https://community.rwkv.cn/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;🐧 QQ 頻道：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fpd.qq.com%2Fs%2F9n21eravc&quot; target=&quot;_blank&quot;&gt;https://pd.qq.com/s/9n21eravc&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;📺 BiliBili 視頻教程：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fspace.bilibili.com%2F3546689096910933&quot; target=&quot;_blank&quot;&gt;https://space.bilibili.com/3546689096910933&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338160</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338160</guid>
            <pubDate>Wed, 05 Mar 2025 06:53:00 GMT</pubDate>
            <author>來源: 投稿</author>
        </item>
        <item>
            <title>李想：AGI 投資遠超互聯網、一兩年內還不具備更好的商業模式</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;理想汽車 CEO 李想近日在朋友圈分享了對 AGI 的一些觀點，他表示，AGI 所需的投資遠超互聯網，一兩年內還不具備好的商業模式。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;1612&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0311/144714_pwWM_2720166.png&quot; width=&quot;1002&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;李想還説道，&lt;strong&gt;AGI 早期雖然自己不賺錢，但會破壞傳統商業模式賺錢&lt;/strong&gt;。他還提到，新技術往往從通縮開始，AGI 的發展趨勢不可阻擋。&lt;/p&gt; 
&lt;p&gt;理想汽車在 2024 年底舉行 2024 理想 AI Talk ，李想當時曾就 AI 等話題展開對話。&lt;/p&gt; 
&lt;p&gt;他表示，理想汽車一年一百億的研發，一半投在了人工智能上。大模型出現以後，人類會發生根本性的改變。互聯網讓信息平權，人工智能幫助實現認知和知識的平權。&lt;/p&gt; 
&lt;p&gt;李想還談到了人工智能的三個階段：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;第一階段，「增強能力」，主要起輔助作用，決策權在用户。例如 L3 自動駕駛，需要用户監督，並且負責任。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;第二階段，「我的助手」，助手角色，佈置給它任務，它就可以獨立完成，並對結果承擔責任。例如 L4 自動駕駛，可以讓它到學校幫忙接孩子等。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;第三階段，終極階段，「硅基家人」，不需要指示、分配任務，它就是我們的家庭成員，甚至是家庭重要的組織者。它不但瞭解我，它還瞭解我的孩子，瞭解我身邊的朋友，甚至比我還瞭解。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;p&gt;相關閲讀&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/news/285159&quot; target=&quot;news&quot;&gt;理想汽車多模態認知大模型 Mind GPT 正式上線&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/news/270426&quot; target=&quot;news&quot;&gt;理想汽車全自研多模態認知大模型 —— Mind GPT&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338155</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338155</guid>
            <pubDate>Wed, 05 Mar 2025 06:49:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>iPhone 17 系列機模曝光</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;博主 MajinBuOfficial 根據供應鏈獲得的 CAD 數據，以 3D 打印的方式獲得了 iPhone 17 全系列的機模。這批機模包括了 iPhone 17、iPhone 17 Air、iPhone 17 Pro、iPhone 17 Pro Max 四款機型，而外觀信息與此前透露的諜照基本一致。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-2fecc3d4611b83757346306026632dfd575.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;除了 iPhone 17 的後置攝像頭排列沒有太大變化之外，新的 iPhone 17 Air 採用了後置單攝的配置，並將它放置於一個長條形的橫向模塊當中，整機的厚度也有了明顯縮小。&lt;/p&gt; 
&lt;p&gt;Pro 版本的攝像頭同樣變化較大，採用橫向的大尺寸模組，將後置三攝、LiDAR 傳感器、閃光燈、後置麥克風等元器件都收納於其中。至於為什麼蘋果要預留如此巨大的模組位置，還有待觀察。&lt;/p&gt; 
&lt;p&gt;此前，分析師郭明錤透露，新的 iPhone 17 Air 或將採用高密度電池，使其可以在緊湊尺寸之中獲得更勝於以往的電池續航表現。&lt;/p&gt; 
&lt;p&gt;此外，彭博社記者 Mark Gurman 今日發文&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/news/338104/apple-ios-ipados-macos-design-overhaul&quot;&gt;透露&lt;/a&gt;&lt;/u&gt;，蘋果將會統一 iPhone、iPad 以及 Mac 三個設備的系統界面。據 Gurman 預測，本次系統界面統一將涉及圖標、菜單、界面窗口樣式等內容。值得關注的是，統一後的系統界面設計將會與 visionOS 的風格保持一致，同時簡化用户使用的操作步驟、方式。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0311/103415_RHy0_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;據悉，本次改動將會在 iOS19、iPadOS19 和 MacOS16 中出現。報道指出，這也是自 iOS7 後，時隔十年，蘋果再一次為 iPhone 進行 UI 大改變；而 Mac 方面，MacOS16 將是自 2020 年 MacOS Big Sur 發佈以來，蘋果對 MacOS 最大的一次升級。&lt;/p&gt; 
&lt;p&gt;相關閲讀&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/news/335621&quot; target=&quot;news&quot;&gt;iPhone 17 系列 CAD 圖曝光&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/news/338104/apple-ios-ipados-macos-design-overhaul&quot; target=&quot;news&quot;&gt;蘋果計劃對 iOS、iPadOS 和 macOS 系統外觀進行大幅重新設計&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338152</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338152</guid>
            <pubDate>Wed, 05 Mar 2025 06:40:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>越疆發佈 Dobot Atom：全球首款「靈巧操作+直膝行走」具身智能人形機器人</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;深圳越疆科技今日&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.bilibili.com%2Fvideo%2FBV1EkREYbEsg%2F&quot; target=&quot;_blank&quot;&gt;發佈&lt;/a&gt;&lt;/u&gt;了全球首款「靈巧操作 + 直膝行走」具身智能人形機器人 Dobot Atom，可實現跨場景、多台協同勝任複雜操作泛化任務。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;1502&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0311/143353_pHFB_2720166.png&quot; width=&quot;2430&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;據介紹，該款機器人是面向工業級精細操作全尺寸仿生人形機器人，搭載自研操作技能模型 ROM-1，具有神經驅動靈巧操作和仿人直膝步態行走兩大特徵。&lt;/p&gt; 
&lt;p&gt;區別於傳統機器人，該產品通過「腦-手協同」技術突破，結合視覺感知與五指靈巧手閉環操作，無需預編程即可自主完成上百種複雜任務。&lt;/p&gt; 
&lt;p&gt;官方介紹稱，這台為「打工」而生的工業級操作類人形機器人，身高 1.53 米，體重 62 公斤，採用 1:1 仿人手臂構型設計，全身配置 41 個自由度，搭載重複定位精度 ±0.05mm 的 7 自由度工業級仿生協作臂，適應常見 700-1000mm 工作台高度靈巧作業，並具有工業現場穩定通過能力。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-fbf5c225d2fa360687b55084e48cf17f3d8.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;作為中國首款具備工業級雙臂操作與仿人直膝行走能力的人形機器人，其核心搭載越疆自研的神經驅動靈巧操作系統（NDS）與仿人直膝行走系統（AWS），雙臂協同靈巧操作，高還原度仿人直膝行走，這種同時擁有上下肢體高水平運動和控制表現的能力，標誌着人形機器人操作領域取得重要進展。&lt;/p&gt; 
&lt;p&gt;NDS 系統通過端到端自主推理，賦予機器人 28 個上肢自由度及伺服級抖動抑制能力，可完成工具製造、脆弱物料無損抓取（如車釐子）等複雜任務，操作精度高達 ±0.05mm。&lt;/p&gt; 
&lt;p&gt;AWS 系統則基於類人生物力學與強化學習技術，實現高度擬人的直膝行走和靈活轉身，適配短程狹小空間作業需求。其自適應泛化能力尤為突出，僅需 2 小時採集少量數據即可掌握新技能，顯著提升非結構化場景下的操作效率。&lt;/p&gt; 
&lt;p&gt;據官方介紹，這台人形機器人的核心零部件、軟硬件系統採用了自主研發的工業級方案，主要面向數以千計用工的車廠組裝備料環節、咖啡店制飲多台設備的流程操作、連鎖藥店夜間取藥等場景，即設備位置不固定、產品多規格、操作相似度高，並有短程狹小空間通過、靈活轉身操作需求的工業商業連續重複工作場景。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338149</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338149</guid>
            <pubDate>Wed, 05 Mar 2025 06:35:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>馬斯克嘗試用 AI 取代美國公務員</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.theatlantic.com%2Ftechnology%2Farchive%2F2025%2F03%2Fgsa-chat-doge-ai%2F681987%2F&quot; target=&quot;_blank&quot;&gt;大西洋月刊報道稱&lt;/a&gt;&lt;/u&gt;，馬斯克領導的政府效率部（DOGE）正在努力縮減和重組美國公務員隊伍，這一努力已進入新階段。其理念很簡單：&lt;strong&gt;利用生成式人工智能來自動化以前由人完成的工作&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0311/142816_kg1u_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;美國政府正在與美國總務管理局（GSA）的 1,500 名聯邦僱員一起測試一款新型聊天機器人 「GSA Chat」&amp;nbsp;，並可能最早於本週五向整個機構發佈，這意味着超過 10,000 名負責超過 1,000 億美元合同和服務的工作人員可以使用這款機器人。&lt;/p&gt; 
&lt;p&gt;這款聊天機器人被 GSA 領導層視為提升聯邦工作人員生產力的工具，是政府效率部及其盟友更大行動方案的一部分。談到 GSA 的整體計劃時，最近被任命為 GSA 信息技術部門 —— 技術轉型服務局局長的前特斯拉工程師託馬斯・謝德上個月在全體員工會議上表示，&lt;strong&gt;該機構正在推進「人工智能優先戰略」&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;謝德稱，「正如大家所知，隨着我們縮減聯邦政府的整體規模，仍有大量項目需要保留，這為技術和自動化全面發揮作用提供了巨大機遇」。他提出，可以在政府範圍內提供「編碼代理」—— 指的是能夠代替人類編寫並可能部署代碼的 AI 程序。此外，謝德還表示，AI 可以「對合同進行分析」，軟件可用於「自動化」GSA 的「財務職能」。&lt;/p&gt; 
&lt;p&gt;目前，在工作中使用人工智能很常見，GSA 的聊天機器人可能不會對政府運作產生巨大影響。但這只是政府效率部繼續大幅削減公務員體系的一個小例子。據報道，在教育部，政府效率部顧問將有關機構支出的敏感數據輸入 AI 程序，以確定削減開支的方向。據説政府效率部打算利用 AI 來幫助決定政府各部門的員工是否應保住工作。&lt;/p&gt; 
&lt;p&gt;在上週晚些時候的另一場 TTS 會議上，謝德表示，他預計該部門在幾周內規模將「至少縮小 50%」。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338147/musk-replacing-workers-with-ai</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338147/musk-replacing-workers-with-ai</guid>
            <pubDate>Wed, 05 Mar 2025 06:28:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>RISC-V 工委會：徵集《RISC-V 指令集架構矩陣擴展（ME）指令集》等三項團體標準參編單位</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;中國電子工業標準化技術協會 RISC-V 工委會發布「關於公開徵集《RISC-V 指令集架構矩陣擴展（ME）指令集》等三項團體標準參編單位的通知」。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;通知內容指出，由阿里巴巴達摩院（杭州）科技發起制定的《RISC-V 指令集架構矩陣擴展（ME）指令集》、進迭時空（杭州）科技發起制定的《開放精簡指令集（RISC-V）配置文件》，以及由芯昇科技發起制定的《RISC-V 指令集架構無線矢量擴展（Zvw）指令集》三項團體標準已獲批立項。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;為更高質量地完成標準編制工作，保障標準的廣泛性、科學性和實用性，現向全行業及 RISC-V 工委會成員徵集上述三項團體標準參編單位，共同完成標準的制定工作。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;383&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0311/142732_nWVQ_4252687.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;strong&gt;報名條件&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;1、參編單位具備相關領域工作基礎，具有較高的社會影響力，重視標準化工作，能夠提供技術專家作為參編人員參與標準編制。&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;2、參編單位應指定固定的起草人員，能夠確保參與標準制修訂過程中的各項會議，按時完成標準編制組分配的工作任務。&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;3、起草人應具備相應的專業知識、經驗和能力，瞭解產業現狀和技術發展水平。同時較熟悉標準化工作流程，具有標準制修訂相關工作經驗的優先。起草人應及時對標準提出建設性意見建議。&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;4、在可以公開的前提下，參編單位向標準編制工作組提供相關研究成果、經典案例和數據，供編制標準組參考。&lt;/span&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;更多詳情可&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FjAQEyU7kzEWgp9HefHji_g&quot; target=&quot;_blank&quot;&gt;查看官方公告&lt;/a&gt;。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338146</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338146</guid>
            <pubDate>Wed, 05 Mar 2025 06:28:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>Composio —— 適用於 AI 代理的生產就緒工具集</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                                                                        
                                                                                    &lt;p style=&quot;color:#1f2328; text-align:start&quot;&gt;&lt;strong&gt;Composio 為 AI 代理提供可用於生產的工具集&lt;/strong&gt;，提供：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;支持多個類別的 250 多種工具：
&lt;ul&gt;
&lt;li&gt;GitHub、Notion、Linear、Gmail、Slack、Hubspot、Salesforce 等軟件工具 &amp;amp;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;a href=&quot;https://app.composio.dev/apps&quot;&gt;更多&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;操作系統操作, 包括文件工具、shell 工具、代碼分析工具 &amp;amp;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;a href=&quot;https://app.composio.dev/apps&quot;&gt;更多&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;通過 Google、Perplexity、Tavily 和 Exa 實現搜索功能 &amp;amp;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;a href=&quot;https://app.composio.dev/apps&quot;&gt;更多&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;全面的框架支持，包括 OpenAI、 Groq、Claude、LlamaIndex、Langchain、CrewAI、Autogen、Gemini 以及&lt;a href=&quot;https://docs.composio.dev/framework&quot;&gt;更多&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;支持多種協議 (OAuth、API 密鑰、Basic JWT) 的託管身份驗證&lt;/li&gt;
&lt;li&gt;通過優化設計將工具調用準確率提高高達 40%&lt;/li&gt;
&lt;li&gt;用於後端集成的白標解決方案&lt;/li&gt;
&lt;li&gt;支持自定義工具和擴展的可插拔架構&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;500&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0310/163052_S7VK_4252687.gif&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

                                                                    &lt;/div&gt;
                                                                </description>
            <link>https://www.oschina.net/p/composio</link>
            <guid isPermaLink="false">https://www.oschina.net/p/composio</guid>
            <pubDate>Wed, 05 Mar 2025 06:15:00 GMT</pubDate>
        </item>
        <item>
            <title>FreeBSD 13.5 正式發佈：升級驅動程序、修復錯誤</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;FreeBSD 13.5 已發佈，作為&amp;nbsp;FreeBSD 13&amp;nbsp;系列的最終更新。用户應開始制定升級到當前 FreeBSD 14 穩定系列或關注未來 FreeBSD 15.0 版本的計劃。&lt;/p&gt; 
&lt;p&gt;FreeBSD 13.5 帶來了許多小型的軟件更新，例如 XZ、SQLite3、OpenSSH 以及其他應用程序。&lt;/p&gt; 
&lt;p&gt;其他變化&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;增加對 Purism 鍵盤的支持，支持與 Coreboot 搭配使用&lt;/li&gt; 
 &lt;li&gt;將 Realtek 8156/8156B 網絡驅動程序支持移動到 URE 驅動程序&lt;/li&gt; 
 &lt;li&gt;支持 Brainboxes USB-to-serial 轉換器&lt;/li&gt; 
 &lt;li&gt;AGP 驅動程序的手冊頁面也已更新&lt;/li&gt; 
 &lt;li&gt;修復 UFS1 文件系統&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.phoronix.com%2Fnews%2FFreeBSD-13.5-Beta-2&quot; target=&quot;_blank&quot;&gt;錯誤&lt;/a&gt;，實現在 gstate 中的微秒級磁盤延遲&lt;/li&gt; 
 &lt;li&gt;以及各種網絡驅動程序的更新&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;關於 FreeBSD 13.5 穩定版的更多詳細信息，訪問&amp;nbsp;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.freebsd.org%2Freleases%2F13.5R%2Fannounce%2F&quot; target=&quot;_blank&quot;&gt;FreeBSD.org&lt;/a&gt;。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338143/freebsd-13-5</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338143/freebsd-13-5</guid>
            <pubDate>Wed, 05 Mar 2025 06:14:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>OpenZFS 2.3.1 發佈，兼容 Linux 6.13 內核</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;OpenZFS 2.3.1 已發佈，新版本兼容 Linux 6.13 內核，以及修復了多項錯誤。&lt;/p&gt; 
&lt;p&gt;對於在 Linux 上使用 OpenZFS 的用户來説，OpenZFS 2.3.1 現在與 Linux 6.13 上游兼容，而之前的 v2.3.0 版本僅支持到 Linux 6.12。&lt;/p&gt; 
&lt;p&gt;OpenZFS 2.3.1 還包含一些初步的 Linux 6.14 兼容性內容，但尚未官方確認，因為 Linux 6.13 穩定版還需要兩週時間才能最終確定。&lt;/p&gt; 
&lt;p&gt;該版本還帶來了各種 ZFS 文件系統驅動程序修復、更好的填充空元數據塊、文檔更新、各種 ZTS 修復以及其他一些小型的修復。&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;詳情查看&amp;nbsp;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fopenzfs%2Fzfs%2Freleases%2Ftag%2Fzfs-2.3.1&quot; target=&quot;_blank&quot;&gt;https://github.com/openzfs/zfs/releases/tag/zfs-2.3.1&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338141/openzfs-2-3-1</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338141/openzfs-2-3-1</guid>
            <pubDate>Wed, 05 Mar 2025 06:07:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>稚暉君發佈靈犀 X2，具備複雜交互能力的「靈動機器人」</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;智元機器人宣佈其新款人形機器人靈犀 X2 正式上線，具備完善的運動、交互及作業能力。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;靈犀 X2&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;由智元旗下的機器人實驗室 X-Lab 開發，全身共 28 個自由度、體重 33.8 千克，小腦控制器、域控制器、智能電源管理系統、核心關節模組全線自研。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;iframe frameborder=&quot;0&quot; height=&quot;350&quot; scrolling=&quot;no&quot; src=&quot;https://player.bilibili.com/player.html?isOutside=true&amp;amp;aid=114138890110697&amp;amp;bvid=BV1JYRjYoEzE&amp;amp;cid=28800323370&amp;amp;p=1&quot; style=&quot;box-sizing: inherit; font-family: -apple-system, BlinkMacSystemFont, &amp;quot;Apple Color Emoji&amp;quot;, &amp;quot;Segoe UI Emoji&amp;quot;, &amp;quot;Segoe UI Symbol&amp;quot;, &amp;quot;Segoe UI&amp;quot;, &amp;quot;PingFang SC&amp;quot;, &amp;quot;Hiragino Sans GB&amp;quot;, &amp;quot;Microsoft YaHei&amp;quot;, &amp;quot;Helvetica Neue&amp;quot;, Helvetica, Arial, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(0, 0, 0); background-color: rgb(255, 255, 255);&quot; width=&quot;700&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/iframe&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;智元機器人創始人彭志輝（稚暉君）在視頻中，着重介紹了這款機器人的關節、仿生足弓、靈巧手，以及散熱、續航能力，該機器人採用柔性材料，可與手機聯動。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;靈犀 X2 不僅可以像人一樣自然走路，也能跑、能轉、能跳點小舞，會滑板車、玩平衡車、騎自行車。該機器人搭載情感計算引擎。彭志輝稱，靈犀 X2 搭載了多模態交互大模型「硅光動語」，因此它是第一台真正具備複雜交互能力的「靈動機器人」，具備毫秒級交互反應，以及通過視覺理解和認知世界的能力。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;靈犀 X2 可進行遠程裸眼 3D 交流。還可模仿人類呼吸韻律、具備人類好奇心和注意力機制、會一些小動作等肢體語言。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338139</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338139</guid>
            <pubDate>Wed, 05 Mar 2025 05:59:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>中國信通院啓動大模型應用交付生態圖譜編制</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;中國信息通信研究院（簡稱 「中國信通院」）現已正式開展大模型應用交付生態圖譜編制工作，面向產業各界開放報名渠道。該圖譜旨在深入洞察大模型應用生態發展趨勢，全面梳理大模型交付供應商、需求方等產業主體的情況，把握當下大模型實際落地進展，持續推動大模型應用生態的高質量發展，促進產業鏈各環節的深度合作與交流。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;本圖譜從產業供需出發，設定了需求方、供應商兩大視角。一方面需求方可通過圖譜關注供應商可支持的能力，包括交付大模型應用、模型服務、大模型相關平台或私域模型部署、全棧能力交付等多種內容，以及提供項目前期的諮詢服務以及項目上線後的運營管理等。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;另一方面供應商可通過圖譜瞭解需求方市場所傾向的行業場景，把握當下需求熱點，反哺產品迭代更新。本圖譜內容將不斷更新完善，為行動計劃後續工作做好鋪墊，在大模型產業分析研究報告、大模型項目供需對接會等活動中發揮作用，並作為大模型應用交付供應商名錄的重要參考依據。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;現面向產業各界開放大模型應用交付生態圖譜參與渠道，有意參與的企業可於 2025 年 3 月 22 日前完成填寫。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;詳情可&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FMzspp_8L1Bze7DbGiirONA&quot; target=&quot;_blank&quot;&gt;查看官方公告&lt;/a&gt;。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338137</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338137</guid>
            <pubDate>Wed, 05 Mar 2025 05:47:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>網易有道完成翻譯底層技術迭代，宣稱「翻譯質量全球第一」</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;3 月 11 日，網易有道&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fg4SqrMDbb9C41pChnjWGeQ&quot; target=&quot;_blank&quot;&gt;宣佈&lt;/a&gt;&lt;/u&gt;完成翻譯底層技術迭代，基於自主研發的&lt;strong&gt;子曰翻譯大模型 2.0&lt;/strong&gt;，在測試中實現翻譯質量超越國內外主流通用大模型。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0311/114216_wW5E_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;此次突破，也標誌着國產大模型在專業領域取得實質性進展，通過數據、算法等技術創新，使得小參數垂類模型實現性能大幅提升。&lt;/p&gt; 
&lt;p&gt;據瞭解，搭載全新大模型的翻譯已在有道詞典、有道翻譯及有道翻譯官內上線，提供標準模型、高級模型兩種不同參數選擇。另外，有道詞典筆 X7 系列也已升級為最新翻譯大模型，其餘型號將陸續更新。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338122</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338122</guid>
            <pubDate>Wed, 05 Mar 2025 03:43:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
    </channel>
</rss>