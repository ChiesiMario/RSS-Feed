<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - news - 繁體中文（香港）</title>
    <link>https://www.oschina.net/news/project</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news" rel="self" type="application/rss+xml"/>
    <description>已對該 RSS 進行格式化操作：中英字符之間插入空格、使用直角引號、標點符號修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-hk</language>
    <lastBuildDate>Tue, 29 Jul 2025 07:48:36 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>螞蟻 inclusionAI 團隊發佈 Ming-lite-omni v1.5</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;螞蟻集團 inclusionAI 團隊發佈了全面升級版的全模態模型 &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Finclusionai.github.io%2Fzh%2Fblog%2Fming-lite-omni-1_5%2F" target="_blank"&gt;&lt;strong&gt;Ming-Lite-Omni v1.5&lt;/strong&gt;&lt;/a&gt;，基於 &lt;strong&gt;Ling-lite-1.5&lt;/strong&gt; 構建，總參數量為 &lt;strong&gt;203 億&lt;/strong&gt;（其中 MoE 部分活躍參數為 &lt;strong&gt;30 億&lt;/strong&gt;），在圖像-文本理解、文檔理解、視頻理解、語音理解與合成、圖像生成與編輯等全模態能力上顯著提升。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-95012b461af8180f5480bac2f4c85b95949.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Ming-lite-omni v1.5 模型架構如下，主題參考了 Ming-lite-omni v1 版本的結構，區別在於為了增強圖像編輯人物和場景一致性，升級 Vision head 支持參考圖特徵輸入。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-6c48bdf68ea2bcdfc0e8deb440f605297fb.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;關鍵優化&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;增強視頻理解&lt;/strong&gt;：通過 &lt;strong&gt;MRoPE 3D 時空編碼&lt;/strong&gt; 和針對長視頻的 &lt;strong&gt;課程學習策略&lt;/strong&gt;，顯著提升對複雜視覺序列的理解能力 。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;優化多模態生成&lt;/strong&gt;：採用雙分支圖像生成（ID 與場景一致性損失）和新的音頻解碼器及 BPE 編碼，提升生成一致性與感知控制，實現高質量實時語音合成。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;數據全面升級&lt;/strong&gt;：新增結構化文本數據、高質量產品信息及包括方言（如普通話、粵語、四川話等）在內的精細化視覺與語音感知數據。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;性能表現&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;在 &lt;strong&gt;MMVet&lt;/strong&gt;、&lt;strong&gt;MathVista&lt;/strong&gt;、&lt;strong&gt;OCRBench&lt;/strong&gt; 等數據集上表現突出，文檔理解任務（如 &lt;strong&gt;ChartQA&lt;/strong&gt;、&lt;strong&gt;OCRBench&lt;/strong&gt;）取得 10B 以下參數模型中的 &lt;strong&gt;SOTA&lt;/strong&gt; 成績。&lt;/li&gt; 
 &lt;li&gt;視頻理解、語音理解與生成（支持多種方言）及圖像生成（保持人物 ID 一致性編輯）均處於行業領先地位。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;該模型已在 &lt;strong&gt;Hugging Face&lt;/strong&gt; 和 &lt;strong&gt;ModelScope&lt;/strong&gt; 上開放下載，並提供詳細安裝指南、代碼示例和 &lt;strong&gt;Gradio&lt;/strong&gt; 演示。&lt;/p&gt; 
&lt;p&gt;Hugging Face: https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5&lt;br&gt; ModelScope: https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362971</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362971</guid>
      <pubDate>Tue, 29 Jul 2025 07:39:59 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>eBPF 助力 NAS 分鐘級別 Pod 實例溯源</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;span id="OSC_h1_1"&gt;&lt;/span&gt; 
&lt;h1&gt;一、背景&lt;/h1&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;雲存儲 NAS 產品是一個可共享訪問、彈性擴展、高可靠、高性能的分佈式文件系統。 NAS 兼容了 POSIX 文件接口，可支持數千台計算節點共享訪問，可掛載到彈性計算 ECS、容器實例等計算業務上，提供高性能的共享存儲服務。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;鑑於多主機間共享的便利性和高性能， NAS 在得物的算法訓練、應用構建等場景中均成為了基礎支撐。&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img src="https://static001.geekbang.org/infoq/36/36274710c18533ce5fc246ee82e640c2.jpeg" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;在多業務共享的場景中，單個業務流量異常容易引發全局故障。目前，異常發生後需依賴&lt;strong&gt;雲服務廠商 NAS &lt;/strong&gt;的溯源能力，&lt;strong&gt;但只能定位到主機級別，無法識別具體異常服務&lt;/strong&gt;。要定位到服務級別，仍需依賴所有使用方協同排查，並由 SRE 多輪統計分析，&lt;strong&gt;效率低下&lt;/strong&gt;&lt;/span&gt;&lt;span style="color:#6a6a6a"&gt;（若服務實例發生遷移或重建，排查難度進一步增加）&lt;/span&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;&lt;strong&gt;為避免因 NAS 異常或帶寬佔滿導致模型訓練任務受阻&lt;/strong&gt;，因此需構建支持服務級流量監控、快速溯源及 NAS 異常實時感知的能力，以提升問題定位效率並減少業務中斷。&lt;/span&gt;&lt;/p&gt; 
&lt;span id="OSC_h1_2"&gt;&lt;/span&gt; 
&lt;h1&gt;二、流量溯源方案調研和驗證&lt;/h1&gt; 
&lt;span id="OSC_h2_3"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span style="color:#000000"&gt;NAS 工作原理&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;NAS 本地掛載原理&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;在 Linux 平台上，NAS 的產品底層是基於標準網絡文件系統 NFS（Network File System），通過將遠端文件系統掛載到本地，實現用户對遠端文件的透明訪問。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;NFS 協議（主要支持 NFS v3 和 v4，通常以 v3 為主）允許將遠端服務掛載到本地，使用户能夠像訪問本地文件目錄一樣操作遠端文件。文件訪問請求通過 RPC 協議發送到遠端進行處理，其整體流程如下：&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img height="465" src="https://oscimg.oschina.net/oscnet/up-3f3abf4fce2a08639688cf370284cf62cdd.png" width="620" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#6a6a6a"&gt;文件系統訪問時的數據流向示意&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img height="552" src="https://oscimg.oschina.net/oscnet/up-a5b7a533fbca51c726053b4598e79c9786f.jpg" width="507" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#6a6a6a"&gt;Linux 內核中 NFS 文件系統&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;NFS 文件系統讀/寫流程&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;在 Linux NFS 文件系統的實現中，文件操作接口由 nfs_file_operations 結構體定義，其讀取操作對應的函數為:&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;//NFS 文件系統的 VFS 層實現的函數如下所示：
const&amp;nbsp;struct&amp;nbsp;file_operations nfs_file_operations = {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; .llseek &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; = nfs_file_llseek,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; .read_iter &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;= nfs_file_read,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; .write_iter &amp;nbsp; &amp;nbsp; &amp;nbsp; = nfs_file_write,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;// ...
};&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;針對 NFS 文件系統的讀操作涉及到 2 個階段（寫流程類似，只是函數名字有所差異，本文僅以讀取為例介紹）。由於文件讀取涉及到網絡操作因此這兩個階段涉及為異步操作：&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;&lt;u&gt;※ 兩個階段&lt;/u&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;讀取請求階段：&lt;/strong&gt;當應用程序針對 NFS 文件系統發起 read() 讀操作時，內核會在 VFS 層調用 nfs_file_read 函數，然後調用 NFS 層的 nfs_initiate_read 函數，通過 RPC 的 rpc_task_begin 函數將讀請求發送到 NFS Server，至此向 NFS Server 發起的請求工作完成。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;讀響應階段：&lt;/strong&gt;在 NFS Server 返回消息後，會調用 rpc_task_end 和 nfs_page_read_done 等函數，將數據返回到用户空間的應用程序。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img height="415" src="https://oscimg.oschina.net/oscnet/up-fd2c800299c65096f8d6eba7de108c0581f.jpg" width="1280" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;在瞭解 NFS 文件系統的讀流程後，我們回顧一下 NFS Server 為什麼無法區分單機訪問的容器實例或進程實例。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;這是因為 NFS 文件系統的讀寫操作是在內核空間實現的。當容器 A/B 和主機上的進程 C 發起讀請求時，這些請求在進入內核空間後，統一使用主機 IP（如 192.168.1.2）作為客户端 IP 地址。因此，NFS Server 端的統計信息只能定位到主機維度，無法進一步區分主機內具體的容器或進程。&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img height="516" src="https://oscimg.oschina.net/oscnet/up-91366119d285327518f226735b34227dddd.jpg" width="1080" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#6a6a6a"&gt;內核空間實現示意&lt;/span&gt;&lt;/p&gt; 
&lt;span id="OSC_h2_4"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span style="color:#000000"&gt;方案調研和驗證&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;進程對應容器上下文信息關聯&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;內核中進程以 PID 作為唯一編號，與此同時，內核會建立一個 struct task_struct 對象與之關聯，在 struct task_struct 結構會保存進程對應的上下文信息。如實現 PID 信息與用户空間容器上下文的對應（進程 PID 1000 的進程屬於哪個 Pod 哪個 Container 容器實例），我們需基於內核 task_struct 結構獲取到容器相關的信息。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;通過分析內核代碼和資料確認，發現可以通過 task_struct 結構中對應的 cgroup 信息獲取到進程對應的 cgroup_name 的信息，而該信息中包含了容器 ID 信息，例如&lt;strong&gt; docker-2b3b0ba12e92...983.scope &lt;/strong&gt;，完整路徑較長，使用 .... 省略。基於容器 ID 信息，我們可進一步管理到進程所歸屬的 Pod 信息，如 Pod NameSpace 、 Pod Name 、 Container Name 等元信息，最終完成進程 PID 與容器上下文信息元數據關聯。&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;struct&amp;nbsp;task_struct&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;struct&amp;nbsp;css_set&amp;nbsp;__rcu &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;*cgroups;
}


struct&amp;nbsp;css_set&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;struct&amp;nbsp;cgroup_subsys_state&amp;nbsp;*subsys[CGROUP_SUBSYS_COUNT];
}


struct&amp;nbsp;cgroup_subsys_state&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;struct&amp;nbsp;cgroup&amp;nbsp;*cgroup;
}


struct&amp;nbsp;cgroup&amp;nbsp;{
&amp;nbsp;&amp;nbsp;struct&amp;nbsp;kernfs_node&amp;nbsp;*kn; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;/* cgroup kernfs entry */
}


struct&amp;nbsp;kernfs_node&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;const&amp;nbsp;char&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; *name; &amp;nbsp;// docker-2b3b0ba12e92...983.scope
}&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;以某容器進程為例，該進程在 Docker 容器環境中的 cgroup 路徑完整為 /sys/fs/cgroup/cpu/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podefeb3229_4ecb_413a_8715_5300a427db26.slice/docker-2b3b0ba12e925820ac8545f67c8cadee864e5b4033b3d5004d8a3aa742cde2ca.scope 。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;經驗證，我們在內核中讀取 task-&amp;gt;cgroups-&amp;gt;subsys[0]-&amp;gt;kn-&amp;gt;name 的值為 docker-2b3b0ba12e925820ac8545f67c8cadee864e5b4033b3d5004d8a3aa742cde2ca.scope 。&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img src="https://static001.geekbang.org/infoq/92/92735ea140e6e0021584e2c7cc21b0b4.png" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;其中容器 ID 字段為 docker- 與 .scope 間的字段信息，在 Docker 環境中一般取前 12 個字符作為短 ID，如 2b3b0ba12e92 ，可通過 docker 命令進行驗證，結果如下：&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;docker&amp;nbsp;ps -a|grep&amp;nbsp;2b3b0ba
2b3b0ba12e92&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; registry-cn-hangzhou-vpc.ack.aliyuncs.com/acs/pause:3.5&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;NAS 上下文信息關聯&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;NAS 產品的訪問通過掛載命令完成本地文件路徑的掛載。我們可以通過 mount 命令將 NAS 手工掛載到本地文件系統中。&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;mount&amp;nbsp;-t nfs -o vers=3,nolock,proto=tcp,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport \
&amp;nbsp;&amp;nbsp;3f0f3489aa-xxxx.cn-shanghai.nas.aliyuncs.com:/test /mnt/nas&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;執行上述掛載命令成功後，通過 mount 命令則可查詢到類似的掛載記錄：&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;5368 47 0:660 / /mnt/nas rw,relatime shared:1175 \
&amp;nbsp; &amp;nbsp; &amp;nbsp;- nfs 3f0f3489aa-xxxx.cn-shanghai.nas.aliyuncs.com:/test \ &amp;nbsp; &amp;nbsp;
&amp;nbsp; &amp;nbsp; &amp;nbsp;rw,vers=3,rsize=1048576,wsize=1048576,namlen=255,hard,nolock,\
&amp;nbsp; &amp;nbsp; &amp;nbsp;noresvport,proto=tcp,timeo=600,retrans=2,sec=sys, \
&amp;nbsp; &amp;nbsp; &amp;nbsp;mountaddr=192.168.0.91,mountvers=3,mountport=2049,mountproto=tcp,\
&amp;nbsp; &amp;nbsp; &amp;nbsp;local_lock=all,addr=192.168.0.92&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;核心信息分析如下：&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;# 掛載點，父掛載點，掛載設備號 &amp;nbsp; 目錄 &amp;nbsp; &amp;nbsp; 掛載到本機目錄 &amp;nbsp;協議 &amp;nbsp; NAS 地址
5368&amp;nbsp; &amp;nbsp; &amp;nbsp;47&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;0:660&amp;nbsp; &amp;nbsp; &amp;nbsp;/ &amp;nbsp; &amp;nbsp; &amp;nbsp; /mnt/nas &amp;nbsp; &amp;nbsp; nfs &amp;nbsp; &amp;nbsp;3f0f3489aa-xxxx.cn-shanghai.nas.aliyuncs.com:/test
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;maror:minor&amp;nbsp;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;掛載記錄中的&lt;/span&gt;&lt;span style="color:#d92142"&gt;&lt;strong&gt; 0:660 &lt;/strong&gt;&lt;/span&gt;為本地設備編號，格式為 major:minor ， 0 為 major 編號， 660 為 minor 編號，系統主要以 minor 為主。在系統的 NFS 跟蹤點 nfs_initiate_read 的信息中的 dev 字段則為在掛載記錄中的 minor 編號。&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;cat /sys/kernel/debug/tracing/events/nfs/nfs_initiate_read/format
format:
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; field:dev_t dev; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;offset:8; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; size:4; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;signed:0;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;...
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; field:u32 count; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;offset:32; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;size:4; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;signed:0;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;通過用户空間 mount 信息和跟蹤點中 dev_id 信息，則可實現內核空間設備編號與 NAS 詳情的關聯。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;內核空間信息獲取&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;如容器中進程針對掛載到本地的目錄 /mnt/nas 下的文件讀取時，會調用到 nfs_file_read() 和 nfs_initiate_read 函數。通過 nfs_initiate_read 跟蹤點我們可以實現進程容器信息和訪問 NFS 服務器的信息關聯。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;通過編寫 eBPF 程序針對跟蹤點 tracepoint/nfs/nfs_initiate_read 觸發事件進行數據獲取，我們可獲取到訪問進程所對應的 cgroup_name 信息和訪問 NFS Server 在本機的設備 dev_id 編號。&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img height="673" src="https://oscimg.oschina.net/oscnet/up-b7e2eac3eeba85b51ed94f7a84d28a096ea.jpg" width="1280" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#6a6a6a"&gt;獲取 cgroup_name 信息&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;進程容器上下文獲取：&lt;/strong&gt; 通過 cgroup_name 信息，如樣例中的 docker-2b3b0ba12e92...983.scope ，後續可以基於 container_id 查詢到容器對應的 Pod NameSpace 、 Pod Name 和 Container Name 等信息，從而定位到訪問進程關聯的 Pod 信息。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;NAS 上下文信息獲取：&lt;/strong&gt; 通過 dev 信息，樣例中的 660 ，通過掛載到本地的記錄，可以通過 660 查詢到對應的 NAS 產品的地址，比如 3f0f3489aa-xxxx.cn-shanghai.nas.aliyuncs.com 。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;用户空間元信息緩存&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img src="https://static001.geekbang.org/infoq/aa/aa252b079e6ce3cb1e52e02ab5b4052a.png" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;在用户空間中，可以通過解析掛載記錄來獲取 DEV 信息，並將其與 NAS 信息關聯，從而建立以 DevID 為索引的查詢緩存。如此，後續便可以基於內核獲取到 dev_id 進行關聯，進一步補全 NAS 地址及相關詳細信息。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;對於本地容器上下文的信息獲取，最直接的方式是通過 K8s kube-apiserver 通過 list-watch 方法進行訪問。然而，這種方式會在每個節點上啓動一個客户端與 kube-apiserver 通信，顯著增加 K8s 管控面的負擔。因此，我們選擇通過本地容器引擎進行訪問，直接在本地獲取主機的容器詳情。通過解析容器註解中的 Pod 信息，可以建立容器實例緩存。後續在處理指標數據時，則可以通過 container-id 實現信息的關聯與補全。&lt;/span&gt;&lt;/p&gt; 
&lt;span id="OSC_h1_5"&gt;&lt;/span&gt; 
&lt;h1&gt;三、架構設計和實現&lt;/h1&gt; 
&lt;span id="OSC_h2_6"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span style="color:#000000"&gt;整體架構設計&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;內核空間的信息採集採用 Linux eBPF 技術實現，這是一種安全且高效的內核數據採集方式。簡單來説，eBPF 的原理是在內核中基於事件運行用户自定義程序，並通過內置的 map 和 perf 等機制實現用户空間與內核空間之間的雙向數據交換。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;在 NFS 和 RPC 調用事件觸發的基礎上，可以通過編寫內核空間的 eBPF 程序來獲取必要的原始信息。當用户空間程序蒐集到內核指標數據後，會對這些原始信息進行二次處理，並在用户空間的採集程序中補充容器進程信息（如 NameSpace、Pod 和 Container 名稱）以及 NFS 地址信息（包括 NFS 遠端地址）。&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img src="https://static001.geekbang.org/infoq/c6/c687b3f4df8a2ce5d0ab5cd9f287dfd4.png" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;span id="OSC_h2_7"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span style="color:#000000"&gt;內核 eBPF 程序流程&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;以 NFS 文件讀為例，通過編寫 eBPF 程序跟蹤 nfs_initiate_read / rpc_task_begin / rpc_task_end / nfs_page_read_done 等關鍵鏈路上的函數，用於獲取到 NFS 讀取的數據量和延時數據，並將訪問鏈路中的進程上下文等信息保存到內核中的指標緩存中。&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img height="471" src="https://oscimg.oschina.net/oscnet/up-f854a1a8cdf429eff044e715772dc96dfb6.jpg" width="1280" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;如上圖所示， nfs_initate_read 和 rpc_task_begin 發生在同一進程上下文中，而 rpc_task_begin 與 rpc_task_end 是異步操作，儘管兩者不處於同一進程上下文，但可以通過 task_id 進行關聯。同時， page_read_done 和 rpc_task_end 則發生在同一進程上下文中。&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img height="627" src="https://oscimg.oschina.net/oscnet/up-70f989400d6710f021cfa7577375a709030.jpg" width="1280" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;nfs_initiate_read 函數調用觸發的 eBPF 代碼示例如下所示：&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;

SEC("tracepoint/nfs/nfs_initiate_read")
int&amp;nbsp;tp_nfs_init_read(struct&amp;nbsp;trace_event_raw_nfs_initiate_read *ctx)
&amp;nbsp; &amp;nbsp;&amp;nbsp;// 步驟 1 獲取到 nfs 訪問的設備號信息，比如 3f0f3489aa-xxxx.cn-shanghai.nas.aliyuncs.com
&amp;nbsp; &amp;nbsp;&amp;nbsp;// dev_id 則為： 660&amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;dev_t&amp;nbsp;dev_id =&amp;nbsp;BPF_CORE_READ(ctx, dev);
&amp;nbsp; &amp;nbsp; u64 file_id =&amp;nbsp;BPF_CORE_READ(ctx, fileid);
&amp;nbsp; &amp;nbsp; u32 count =&amp;nbsp;BPF_CORE_READ(ctx, count);
&amp;nbsp; &amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;struct&amp;nbsp;task_struct&amp;nbsp;*task = (struct&amp;nbsp;task_struct *)bpf_get_current_task();


&amp;nbsp; &amp;nbsp;&amp;nbsp;// 步驟 2 獲取進程上下文所在的容器 cgroup_name 信息
&amp;nbsp; &amp;nbsp;&amp;nbsp;// docker-2b3b0ba12e925820ac8545f67c8cadee864e5b4033b3d5004d8a3aa742cde2ca.scope
&amp;nbsp; &amp;nbsp;&amp;nbsp;const&amp;nbsp;char&amp;nbsp;*cname =&amp;nbsp;BPF_CORE_READ(task, cgroups, subsys[0], cgroup, kn, name);
&amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(cname)
&amp;nbsp; &amp;nbsp; {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;bpf_core_read_str(&amp;amp;info.container, MAX_PATH_LEN, cname);
&amp;nbsp; &amp;nbsp; }


&amp;nbsp; &amp;nbsp;&amp;nbsp;bpf_map_update_elem(&amp;amp;link_begin, &amp;amp;tid, &amp;amp;info, BPF_ANY);
}


SEC("tracepoint/nfs/nfs_readpage_done")
int&amp;nbsp;tp_nfs_read_done(struct&amp;nbsp;trace_event_raw_nfs_readpage_done *ctx)
{
&amp;nbsp; &amp;nbsp;//... 省略
}


SEC("tracepoint/sunrpc/rpc_task_begin")
int&amp;nbsp;tp_rpc_task_begin(struct&amp;nbsp;trace_event_raw_rpc_task_running *ctx)
{
&amp;nbsp; &amp;nbsp;&amp;nbsp;//... 省略
}


SEC("tracepoint/sunrpc/rpc_task_end")
int&amp;nbsp;tp_rpc_task_done(struct&amp;nbsp;trace_event_raw_rpc_task_running *ctx)
{
&amp;nbsp; &amp;nbsp;//... 省略
}&lt;/code&gt;&lt;/pre&gt; 
&lt;span id="OSC_h2_8"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span style="color:#000000"&gt;用户空間程序架構&lt;/span&gt;&lt;/h2&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img height="606" src="https://oscimg.oschina.net/oscnet/up-b9cf6d19b68dcceaa9f6f459645264baaa8.jpg" width="1280" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;元數據緩存&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;&lt;u&gt;※ NAS 掛載信息緩存&lt;/u&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;通過解析掛載記錄，可以獲取 DEV 信息與 NAS 信息的關聯關係。以下是實現該功能的關鍵代碼詳情：&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;scanner := bufio.NewScanner(mountInfoFile)
count :=&amp;nbsp;0
for&amp;nbsp;scanner.Scan() {
&amp;nbsp; &amp;nbsp; line := scanner.Text()
&amp;nbsp; &amp;nbsp; devID,remoteDir, localDir, NASAddr = parseMountInfo(line)


&amp;nbsp; &amp;nbsp; mountInfo := MountInfo{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;DevID: &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; devID,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;RemoteDir: &amp;nbsp; &amp;nbsp; remoteDir,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;LocalMountDir: localDir,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;NASAddr： NASAddr,
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; mountInfos =&amp;nbsp;append(mountInfos, mountInfo)

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;&lt;u&gt;※ 容器元信息緩存&lt;/u&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;通過 Docker 或 Containerd 客户端，從本地讀取單機的容器實例信息，並將容器的上下文數據保存到本地緩存中，以便後續查詢使用。&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;podInfo := PodInfo{
&amp;nbsp; &amp;nbsp; NameSpace: &amp;nbsp; &amp;nbsp; labels["io.kubernetes.pod.namespace"],
&amp;nbsp; &amp;nbsp; PodName: &amp;nbsp; &amp;nbsp; &amp;nbsp; labels["io.kubernetes.pod.name"],
&amp;nbsp; &amp;nbsp; ContainerName: labels["io.kubernetes.container.name"],
&amp;nbsp; &amp;nbsp; UID: &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; labels["io.kubernetes.pod.uid"],
&amp;nbsp; &amp;nbsp; ContainerID: &amp;nbsp; conShortID,
}&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;數據處置流程&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;用户空間程序的主要任務是持續讀取內核 eBPF 程序生成的指標數據，並對讀取到的原始數據進行處理，提取訪問設備的 dev_id 和 container_id 。隨後，通過查詢已建立的元數據緩存，分別獲取 NAS 信息和容器 Pod 的上下文數據。最終，經過數據合併與處理，生成指標數據緩存供後續使用。&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;func&amp;nbsp;(m *BPFEventMgr)&amp;nbsp;ProcessIOMetric() {
&amp;nbsp; &amp;nbsp;&amp;nbsp;// ...
&amp;nbsp; &amp;nbsp; events := m.ioMetricMap
&amp;nbsp; &amp;nbsp; iter := events.Iterate()


&amp;nbsp; &amp;nbsp;&amp;nbsp;for&amp;nbsp;iter.Next(&amp;amp;nextKey, &amp;amp;event) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;// ① 讀取到的 dev_id 轉化為對應的完整 NAS 信息
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;devId := nextKey.DevId
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;mountInfo, ok := m.mountMgr.Find(int(devId))


&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;// ② 讀取 containerID 格式化並查詢對應的 Pod 上下文信息
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;containerId := getContainerID(nextKey.Container)
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;podInfo, ok = m.criMgr.Find(containerId)
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;// ③ 基於事件信息、NAS 掛載信息和 Pod 上下文信息，生成指標數據緩存&amp;nbsp;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;metricKey, metricValue := formatMetricData(nextKey， mountInfo, podInfo)
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;value, loaded := metricCache.LoadOrStore(metricKey, metricValue)
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp;&amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;// ④ 指標數據緩存，生成最終的 Metrics 指標並更新&amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;var&amp;nbsp;ioMetrics []metric.Counter
&amp;nbsp; &amp;nbsp; metricCache.Range(func(key, value&amp;nbsp;interface{})&amp;nbsp;bool&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;k := key.(metric.IOKey)
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;v := value.(metric.IOValue)


&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;ioMetrics =&amp;nbsp;append(ioMetrics, metric.Counter{"read_count",&amp;nbsp;float64(v.ReadCount),
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;[]string{k.NfsServer, v.NameSpace, v.Pod, v.Container})
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;// ...
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;}
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;return&amp;nbsp;true
&amp;nbsp; &amp;nbsp; })
&amp;nbsp; &amp;nbsp;&amp;nbsp;
&amp;nbsp; &amp;nbsp; m.metricMgr.UpdateIOStat(ioMetrics)
}&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;啓動 Goroutine 處理指標數據：通過啓動一個 Goroutine，循環讀取內核存儲的指標數據，並對數據進行處理和信息補齊，最終生成符合導出格式的 Metrics 指標。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;&lt;u&gt;※ 具體步驟&lt;/u&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;獲取 NAS 信息：&lt;/strong&gt;從讀取的原始數據中提取 dev_id ，並通過 dev_id 查詢掛載的 NAS 信息，例如遠端訪問地址等相關數據。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;查詢 Pod 上下文：&lt;/strong&gt;對 containerID 進行格式化處理，並查詢對應的容器 Pod 上下文信息。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;生成指標數據緩存：&lt;/strong&gt;基於事件數據、NAS 掛載信息和 Pod 上下文信息，生成指標數據緩存。此過程主要包括對相同容器上下文的數據進行合併和累加。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;導出 Metrics 指標：&lt;/strong&gt;根據指標數據緩存，生成最終的 Metrics 指標，並更新到指標管理器。隨後，通過自定義的 Collector 接口對外導出數據。當 Prometheus 拉取數據時，指標會被轉換為最終的 Metrics 格式。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;通過上述步驟，用户空間能夠高效地處理內核 eBPF 程序生成的原始數據，並結合 NAS 掛載信息和容器上下文信息，生成符合 Prometheus 標準的 Metrics 指標，為後續的監控和分析提供了可靠的數據基礎。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;自定義指標導出器&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;在導出指標的場景中，我們需要基於保存在 Go 語言中的 map 結構中的動態數據實時生成，因此需要實現自定義的 Collector 接口。自定義 Collector 接口需要實現元數據描述函數 Describe() 和指標蒐集的函數 Collect() ，其中 Collect() 函數可以併發拉取，因此需要通過加鎖實現線程安全。該接口需要實現以下兩個核心函數：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Describe() ：用於定義指標的元數據描述，向 Prometheus 註冊指標的基本信息。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Collect() ：用於蒐集指標數據，該函數支持併發拉取，因此需要通過加鎖機制確保線程安全。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;type&amp;nbsp;Collector&amp;nbsp;interface&amp;nbsp;{
&amp;nbsp; &amp;nbsp;&amp;nbsp;// 指標的定義描述符
&amp;nbsp; &amp;nbsp; Describe(chan&amp;lt;- *Desc)
&amp;nbsp; &amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;// 並將收集的數據傳遞到 Channel 中返回
&amp;nbsp; &amp;nbsp; Collect(chan&amp;lt;- Metric)
}&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;我們在指標管理器中實現 Collector 接口， 部分實現代碼，如下所示：&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;nfsIOMetric := prometheus.NewDesc(
&amp;nbsp; &amp;nbsp; prometheus.BuildFQName(prometheusNamespace,&amp;nbsp;"",&amp;nbsp;"io_metric"),
&amp;nbsp; &amp;nbsp;&amp;nbsp;"nfs io metrics by cgroup",
&amp;nbsp; &amp;nbsp; []string{"nfs_server",&amp;nbsp;"ns",&amp;nbsp;"pod",&amp;nbsp;"container",&amp;nbsp;"op",&amp;nbsp;"type"},
&amp;nbsp; &amp;nbsp;&amp;nbsp;nil,
)


// Describe and Collect implement prometheus collect interface
func&amp;nbsp;(m *MetricMgr)&amp;nbsp;Describe(ch&amp;nbsp;chan&amp;lt;- *prometheus.Desc) {
&amp;nbsp; &amp;nbsp; ch &amp;lt;- m.nfsIOMetric
}


func&amp;nbsp;(m *MetricMgr)&amp;nbsp;Collect(ch&amp;nbsp;chan&amp;lt;- prometheus.Metric) {
&amp;nbsp; &amp;nbsp;&amp;nbsp;// Note：加鎖保障線程併發安全
&amp;nbsp; &amp;nbsp; m.activeMutex.Lock()
&amp;nbsp; &amp;nbsp;&amp;nbsp;defer&amp;nbsp;m.activeMutex.Unlock()
&amp;nbsp; &amp;nbsp;&amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;for&amp;nbsp;_, v :=&amp;nbsp;range&amp;nbsp;m.ioMetricCounters {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;ch &amp;lt;- prometheus.MustNewConstMetric(m.nfsIOMetric, prometheus.GaugeValue, v.Count, v.Labels...)
&amp;nbsp; &amp;nbsp; }

&lt;/code&gt;&lt;/pre&gt; 
&lt;span id="OSC_h1_9"&gt;&lt;/span&gt; 
&lt;h1&gt;四、總結&lt;/h1&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;當前 NAS 溯源能力已正式上線，以下是主要功能和視圖介紹：&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;&lt;u&gt;※ 單 NAS 實例整體趨勢&lt;/u&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;支持基於環境和 NAS 訪問地址過濾，展示 NAS 產品的讀寫 IOPS 和吞吐趨勢圖。同時，基於內核空間統計的延時數據，提供 P95 讀寫延時指標，用於判斷讀寫延時情況，輔助問題分析和定位。&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img src="https://static001.geekbang.org/infoq/14/14577d6fe8b2876ca7f5138680fc3667.png" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img src="https://static001.geekbang.org/infoq/9c/9c091aeaecc284956b851416de5d313a.png" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;在 NAS 流量溯源方面，我們結合業務場景設計了基於任務和 Pod 實例維度的流量分析視圖：&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;&lt;u&gt;※ 任務維度流量溯源&lt;/u&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;通過聚合具有共同屬性的一組 Pod 實例，展示任務級別的整體流量情況。該視圖支持快速定位任務級別的流量分佈，幫助用户進行流量溯源和多任務錯峯使用的依據。&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img src="https://static001.geekbang.org/infoq/0a/0a504606b05345b52061d2f754c15b51.png" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;&lt;u&gt;※ Pod 實例維度流量溯源&lt;/u&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;以 Pod 為單位進行流量分析和彙總，提供 Pod NameSpace 和 Name 信息，支持快速定位和分析實例級別的流量趨勢，幫助細粒度監控和異常流量的精準定位。&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img src="https://static001.geekbang.org/infoq/90/90765c51493906beaf530c64494abc6a.png" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;在整體能力建設完成後，我們成功構建了 NAS 實例級別的 IOPS、吞吐和讀寫延時數據監控大盤。通過該能力，進一步實現了 NAS 實例的 IOPS 和吞吐可以快速溯源到任務級別和 Pod 實例級別，流量溯源時效從小時級別縮短至分鐘級別，有效提升了異常問題定位與解決的效率。同時，基於任務流量視圖，我們為後續帶寬錯峯複用提供了直觀的數據支持。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;往期回顧&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#586c90"&gt;1.&lt;/span&gt;正品庫拍照 PWA 應用的實現與性能優化｜得物技術&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#586c90"&gt;2.&lt;/span&gt;匯金資損防控體系建設及實踐 | 得物技術&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#586c90"&gt;3.&lt;/span&gt;一致性框架：供應鏈分佈式事務問題解決方案｜得物技術&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#586c90"&gt;4.&lt;/span&gt;得物社區活動：組件化的演進與實踐&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#586c90"&gt;5.&lt;/span&gt;從 CPU 冒煙到絲滑體驗：算法 SRE 性能優化實戰全揭秘｜得物技術&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;文 / 泊明&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;關注得物技術，每週更新技術乾貨&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;要是覺得文章對你有幫助的話，歡迎評論轉發點贊～&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;未經得物技術許可嚴禁轉載，否則依法追究法律責任。&lt;/span&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/5783135/blog/18683994</link>
      <guid isPermaLink="false">https://my.oschina.net/u/5783135/blog/18683994</guid>
      <pubDate>Tue, 29 Jul 2025 07:35:59 GMT</pubDate>
      <author>原創</author>
    </item>
    <item>
      <title>英偉達開源 Llama-3.3-Nemotron-Super-49B-v1.5 模型</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;英偉達發佈了 Llama-3.3-Nemotron-Super-49B-v1.5，這是一款專為推理和 Agentic 任務優化的開源模型，在單個 H100 GPU 上實現高吞吐量。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-1eb8efcbf4188aaa81c53fbda0c23259d86.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;模型介紹&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Llama Nemotron Super v1.5 是 Llama-3.3-Nemotron-Super-49B-V1.5 的簡稱。它是 Llama-3.3-Nemotron-Super-49B-V1 的升級版本（該模型是 Meta 的 Llama-3.3-70B-Instruct 的衍生模型），專為複雜推理和智能體任務設計，支持 128K tokens 的上下文長度。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;模型架構&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Llama Nemotron Super v1.5 採用神經架構搜索（Neural Architecture Search，NAS），使該模型在準確率和效率之間實現了良好的平衡，將吞吐量的提升有效轉化為更低的運行成本。&lt;/p&gt; 
&lt;p&gt;（注：NAS 的目標是通過搜索算法從大量的可能架構中找到最優的神經網絡結構，利用自動化方法替代人工設計神經網絡架構，從而提高模型的性能和效率。）&lt;/p&gt; 
&lt;p&gt;模型經過了多階段後訓練，包括針對數學、代碼、科學和工具調用的監督微調 (SFT)，以及用於聊天對齊的獎勵感知偏好優化 (RPO)、用於推理的帶可驗證獎勵的強化學習 (RLVR) 和用於工具調用能力增強的迭代直接偏好優化 (DPO)。&lt;/p&gt; 
&lt;p&gt;在多個基準測試中，該模型表現出色。例如，在 MATH500 上 pass@1 達到 97.4，在 AIME 2024 上達到 87.5，在 GPQA 上達到 71.97。模型支持 Reasoning On/Off 模式，用户可通過在系統提示中設置 /no_think 來關閉推理模式。官方推薦在推理開啓時使用 temperature=0.6 和 Top P=0.95，在關閉時使用貪心解碼。&lt;/p&gt; 
&lt;p&gt;該模型已準備好用於商業用途，遵循 NVIDIA Open Model License 和 Llama 3.3 社區許可協議。開發者可以通過 NVIDIA build.nvidia.com 或 Hugging Face 下載和試用該模型，並可使用 vLLM（推薦 v0.9.2）進行部署，官方倉庫中提供了支持工具調用的解析器插件。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362966</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362966</guid>
      <pubDate>Tue, 29 Jul 2025 07:26:59 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>GitHub 出現大範圍服務中斷：目前已全部恢復，影響超 8 小時</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;代碼託管平台 GitHub 從 2025 年 7 月 28 日 16:50 UTC（北京時間 7 月 29 日 00:50）起突發&lt;strong&gt;大規模服務中斷&lt;/strong&gt;，受影響服務包括 Git 操作、API 請求、Pull 請求和 Issues 跟蹤等核心功能 。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0729/150643_ZtSu_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;儘管 GitHub 工程團隊&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.githubstatus.com%2Fincidents%2Fs6d4x8c6cvv5" target="_blank"&gt;嘗試了多種修復措施&lt;/a&gt;（如增設服務器容量、調整限流措施），初期效果不佳，直到北京時間 &lt;strong&gt;7 月 29 日 9:23 左右&lt;/strong&gt; 才取得實質性進展。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1416" src="https://static.oschina.net/uploads/space/2025/0729/150606_wHAe_2720166.png" width="1890" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;最終，相關問題已逐步解決，截至目前，API 請求、Pull 請求等服務已全面恢復，整體中斷時間超過 &lt;strong&gt;8 小時&lt;/strong&gt;。GitHub 官方表示正在深入調查具體原因，後續將發佈詳細技術分析報告。&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;https://www.githubstatus.com/history&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362956</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362956</guid>
      <pubDate>Tue, 29 Jul 2025 07:07:59 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Julius AI 完成 1000 萬美元種子輪融資</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        ********************************************************************************************************************
    ********************************************************************************************************************
    ********************************************************************************************************************
    ********************************************************************************************************************
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362957</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362957</guid>
      <pubDate>Tue, 29 Jul 2025 07:07:59 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>中國移動「九天」3.0 發佈，多項核心技術同步開源</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;中國移動發佈了其自主研發的 「九天」基礎大模型 3.0。根據介紹，「九天眾擎語言大模型」實現了架構上的突破性創新，採用可擴展至萬億級的&amp;nbsp;&lt;strong&gt;MoE 架構&lt;/strong&gt;。通過 15T token 的多階段配比預訓練數據與全流程治理體系，其推理能力得到顯著強化。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;該模型還創新構建了 113 域 ×53 能力的二維分級後訓練框架，結合動態強化學習策略，使複雜推理能力提升了&amp;nbsp;&lt;strong&gt;35%&lt;/strong&gt;。測評結果顯示，「九天」語言大模型：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;在&amp;nbsp;&lt;strong&gt;GPQA-Diamond&lt;/strong&gt;&amp;nbsp;評測中，以&amp;nbsp;&lt;strong&gt;77.67 分&lt;/strong&gt;斬獲全球第二，超越 DeepSeekR1 和 Qwen3。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;在&amp;nbsp;&lt;strong&gt;ArenaHard V1.0&lt;/strong&gt;&amp;nbsp;中，以&amp;nbsp;&lt;strong&gt;67.2 分&lt;/strong&gt;位居全球第一。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;在&amp;nbsp;&lt;strong&gt;BFCL V3&lt;/strong&gt;&amp;nbsp;評測中，達到&amp;nbsp;&lt;strong&gt;68 分&lt;/strong&gt;。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;在性能大幅躍升的同時，模型進一步強化了可控生成能力，通過精確流程內置等技術細節，實現了專業場景下的&lt;strong&gt;零幻覺&lt;/strong&gt;，破解了沉浸式角色演繹難題。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;基於最新的語言大模型，中國移動還同步推出了多個專項模型:&lt;/span&gt;&lt;/p&gt; 
&lt;ul style="margin-left:0; margin-right:0"&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;九天代碼大模型：&lt;/strong&gt;採用兩階段持續訓練技術，支持代碼生成、註釋生成、單元測試生成、代碼智能問答等任務，覆蓋 Python、Java、JS、TS、Go、C++ 等 10 餘種主流編程語言。在 EvalPlus、MHPP、LivecodeBenchv6 等多個代碼生成榜單上表現領先。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;九天數學大模型：&lt;/strong&gt;在短思考、長思考模式下均達到業界 SOTA 水平，多項指標超越 Qwen2.5Math、Qwen3、DeepSeek Math、DeepSeek R1-Distill 等同參數量級模型。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;「九天善智多模態大模型」引入複雜時空建模、流匹配圖片視頻漸進式聯合訓練、端到端局部可控注意力機制等創新技術。同時，通過融合多模態理解信息和聯合圖文交織數據訓練，顯著提升了模型對文本指令和輸入條件圖像視頻的感知能力。這意味着模型不僅能生成高質量的圖像視頻，還能進行多輪對話式高可控精確編輯操作，大幅提升了視覺生成的靈活便利性。例如，在圖片生成方面可支持多輪精準局部修改，如修改文字、修改背景、增加元素等。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;模型的圖理解和視頻理解性能也得到了全面提升：&lt;/span&gt;&lt;/p&gt; 
&lt;ul style="margin-left:0; margin-right:0"&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;圖理解方面：&lt;/strong&gt;在 MMStar、HallusionBench 和 OCRBench 等圖理解任務中，九天模型分別獲得了&amp;nbsp;&lt;strong&gt;82.2、64.3 和 94.9 的高分&lt;/strong&gt;，處於業界領先水平。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;視頻理解方面：&lt;/strong&gt;在 Videomme 和 MVbench 兩個任務中均表現領先，超越 Qwen2-VL 和 InternVideo2。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;目前，中國移動已將多項模型及核心技術進行開源：&lt;/span&gt;&lt;/p&gt; 
&lt;ol style="margin-left:0; margin-right:0"&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;開源九天數童結構化數據大模型&lt;/strong&gt;：包括 JT-DA-8B 模型及後續演進版本，支持下載模型權重、微調代碼、推理代碼等。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;開源九天數學大模型&lt;/strong&gt;：包括 JT-Math-8B 系列模型，支持下載模型權重、推理代碼、技術報告。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;開源九天代碼大模型&lt;/strong&gt;：包括 JT-Coder-8B 系列模型，支持下載模型權重、推理代碼、技術報告。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;開源業界首創的結構化數據模型評測數據及 TReB 評測體系&lt;/strong&gt;：涵蓋 6 大任務、34 個能力，包括高質量、全面的數據、推理模式及評價指標，支持下載評測數據集、測試代碼。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;開源 CCR-Bench 行業場景複雜指令遵循評測數據集&lt;/strong&gt;：包含 174 條高質量、多樣化、高難度複雜指令數據，高度模擬健康專家、智能客服、醫療助手等典型工業場景，支持下載數據集。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362949</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362949</guid>
      <pubDate>Tue, 29 Jul 2025 06:45:59 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>特斯拉與三星簽訂 165 億美元 AI 芯片製造協議</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;7 月 28 日，三星電子在提交給監管機構的文件中表示，三星電子與一家全球大型公司簽署了價值 22.8 萬億韓元（注：現匯率約合 1181.72 億元人民幣，約合 165 億美元）的芯片製造協議，但未透露具體客户名稱。&lt;/p&gt; 
&lt;p&gt;據消息人士透露，特斯拉正是這家客户，該公司目前與三星的合同芯片製造部門已有業務往來。&lt;/p&gt; 
&lt;p&gt;有外媒表示，三星電子公司將就新達成的 165 億美元協議，為特斯拉公司生產半導體，這將為其表現不佳的晶圓代工部門提供助力。該合作的合同期 2025 年 7 月 24 日-2033 年 12 月 31 日。&lt;/p&gt; 
&lt;p&gt;對此，特斯拉 CEO 埃隆・馬斯克確認了合作爆料，三星在美國得克薩斯州新建的巨型工廠將專門用於生產特斯拉的下一代 AI6 芯片（注：特斯拉汽車智駕芯片），並稱「其戰略重要性毋庸置疑」。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-99e430011e2ff79c31d84adca382c598c73.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;馬斯克還稱，三星目前正在生產 AI4 芯片。台積電將首先在中國台灣地區生產剛剛完成設計的 AI5 芯片，然後在美國亞利桑那州生產。&lt;/p&gt; 
&lt;p&gt;根據外媒今年 6 月報道，台積電在全球第三方晶圓代工市場的市佔比為 67%，而排名第二的三星則僅佔 11%。另外，有消息人士稱，三星電子 2025 上半年晶圓代工部門獲零獎金。此前，外媒援引供應鏈消息稱，三星已啓動「精選和聚焦」戰略，集中資源提升 2nm 工藝良率，希望通過產量和成本優勢來挑戰台積電。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362947</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362947</guid>
      <pubDate>Thu, 17 Jul 2025 06:41:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>端側原生大模型 SmallThinker 正式開源</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;上海交通大學 IPADS 研究所、上海交通大學人工智能學院聯合初創公司本智激活（Zenergize AI），發佈了開源端側原生大模型 SmallThinker。&lt;/p&gt; 
&lt;p&gt;該系列模型採用為端側算力、內存、存儲特性而原生設計的模型架構，並從零開始預訓練，具體包含兩個尺寸的稀疏模型，分別是 SmallThinker-4B-A0.6B 和 SmallThinker-21B-A3B。&lt;/p&gt; 
&lt;ul&gt; 
&lt;/ul&gt; 
&lt;p&gt;SmallThinker 專為低成本硬件設計，可在百元級國產開發板（如瑞芯微 RK3588）上流暢運行百億參數模型，旨在為資源受限的個人設備帶來強大、私密且低延遲的 AI 能力，無需依賴雲端。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1578" src="https://static.oschina.net/uploads/space/2025/0729/143436_ewWq_2720166.png" width="2044" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;用户可以通過 Transformers（版本需 &amp;gt;= 4.53.3）或 ModelScope 來運行該模型。官方 GitHub 倉庫提供了詳細的設置、模型轉換和運行指南。官方提示，模型使用了稀疏的 lm_head，可能會導致一定的精度損失，但用户可以手動修改代碼禁用此特性。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct&lt;/li&gt; 
 &lt;li&gt;https://github.com/SJTU-IPADS/PowerInfer/tree/main/smallthinker&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362945</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362945</guid>
      <pubDate>Thu, 17 Jul 2025 06:36:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>🔥造物分享：AnyShake Project 開源地震監測系統</title>
      <description/>
      <link>https://www.oschina.net/ai-creation/details/2112</link>
      <guid isPermaLink="false">https://www.oschina.net/ai-creation/details/2112</guid>
      <pubDate>Thu, 17 Jul 2025 06:18:00 GMT</pubDate>
    </item>
    <item>
      <title>谷歌 NotebookLM 即將推出「視頻概覽」功能</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;谷歌 NotebookLM 即將推出一項名為「視頻概覽 (Video Overviews)」的新功能，能以視頻幻燈片形式呈現內容摘要。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0729/141652_Z58B_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Ftestingcatalog%2Fstatus%2F1949120138373914737" target="_blank"&gt;相關爆料稱&lt;/a&gt;，這些視頻概覽將以視頻幻燈片的形式呈現，內容包含文本、圖像和其他視覺元素，並由女性聲音進行旁白解説。&lt;/p&gt; 
&lt;p&gt;谷歌 NotebookLM 功能於去年推出，旨在通過 AI 虛擬主持人根據用户上傳到 NotebookLM 的文檔（如課程閲讀材料或法律摘要）生成播客，幫助用户以另一種方式理解和消化文檔中的信息。&lt;/p&gt; 
&lt;p&gt;用户可以上傳中文 PDF、Google Docs、網頁鏈接或文本，NotebookLM 會生成中文總結或回答基於中文來源的問題。支持高達 50 萬字的單個來源，適合處理長篇中文文檔。NotebookLM 目前支持 130 種語言的輸入來源和聊天功能，包括中文（簡體和繁體）。用户可以上傳中文文檔、網頁鏈接或文本，並以中文與 AI 進行交互。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362937</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362937</guid>
      <pubDate>Thu, 17 Jul 2025 06:18:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>全球首家機器人 6S 店深圳開業</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;全球首家機器人「6S 店」於 7 月 28 日在深圳龍崗星河 WORLD 園區機器人劇場開業，店內集聚數百種機器人及配套零部件。多家企業帶來的產品涵蓋了家庭服務、醫療輔助、工業巡檢、教育陪伴等多個領域。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="358" src="https://oscimg.oschina.net/oscnet/up-5d0777ddd254f3732d7201680e7a9e95e67.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;「機器人 6S 店究竟是什麼？」深圳未來時代機器人有限公司 CEO 兼 6S 店店長林楓解釋稱，其在傳統汽車 4S 店「銷售（Sale）、零配件供應（Sparepart）、售後服務（Service）、信息反饋（Survey）」的基礎上，新增「租賃（Lease）、個性化訂製（Customized）」兩大功能，形成獨特的「6S」模式。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;活動現場數據顯示，已有超 200 家產業鏈上下游企業表達進駐意向，其中人形及服務機器人企業近 50 家，涵蓋從核心零部件研發到場景應用的全產業鏈環節。該店緊扣機器人產業特性，一方面將建立實時數據反饋機制，精準收集用户需求反哺研發；另一方面提供租賃服務，讓用户無需購買即可體驗。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;據介紹，機器人 6S 店內置「機器人零配件超市」，匯聚伺服電機、減速器等核心元器件，能滿足主流機器人維修需求，實現「快速響應、即時維修」。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362936</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362936</guid>
      <pubDate>Thu, 17 Jul 2025 06:12:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>網信辦整治自媒體發佈不實信息，平台需優化 AI 生成內容標識</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;為持續深入整治「自媒體」發佈不實信息亂象，進一步規範「自媒體」信息發佈行為，按照 2025 年「清朗」系列專項行動總體安排，中央網信辦決定自 7 月 24 日起，在全國範圍內啓動為期 2 個月的「清朗·整治‘自媒體’發佈不實信息」專項行動。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="221" src="https://oscimg.oschina.net/oscnet/up-fcdd633462b44ba4d48a4e73cc8356abdca.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;專項行動重點整治四類突出問題：&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;1.惡意蹭炒誤導公眾問題。涉熱點輿情或公眾人物時，假冒當事人、近親屬等，通過賬號名稱、簡介等方式編造身份，蹭炒熱點，混淆視聽。涉重大輿情、突發事件時，假冒知情人士，編造起因進展、傷亡人數等，無中生有，幹擾輿論。發佈財經、軍事、外交等重要領域信息時，虛構所謂「權威報道」「一手數據」「深度揭秘」等信息，胡編亂造，誤導認知。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;2.多種手段歪曲事實問題。利用人工智能生成合成技術，仿冒他人，或編造社會民生等領域虛假信息，欺騙公眾。通過劇情擺拍、拼湊剪輯等方式，編造事件、虛構或誇大情節，引起關注。歪曲解讀關乎公眾利益的政策方針、法規文件，宣揚「即將取消」「重大變動」等不實信息，製造噱頭。對往年社會新聞、政策發佈等舊聞舊事摘頭去尾，掩蓋時間、地點、結果等關鍵要素，惡意炒作。藉助網絡黑灰產等渠道，以刷榜打榜買榜方式，通過熱搜榜單呈現不實信息，操縱榜單。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;3.不做標註以假亂真問題。對涉及國內外時事、公共政策、社會事件等相關信息，未標註或未準確標註信息來源。以「網傳」「網友表示」「來源於互聯網」等方式發佈信息，模糊標註信息來源，發佈無實際依據內容。標註錯誤信息來源，或矩陣賬號互相引用標註，導致公眾無法追溯真實來源。以過小字號、隱蔽位置、進度條遮擋等方式標註，刻意弱化標註標識。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;4.專業領域信息不實問題。不進行專業資質認證，或以虛假認證、過期認證方式，冒用財經專家、醫生、律師等身份。歪曲解讀專業內容，如杜撰或篡改真實案例細節，發佈未經科學驗證或明顯違背科學常識的信息，將不同的歷史人物與事件張冠李戴、篡改史實。借專業知識分享名義，編造同質化文案或虛假故事，藉機引流帶貨。發佈教程，教授通過虛假擺拍、蹭熱引流等方式打造「網紅專家」人設，擾亂傳播秩序。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;中央網信辦要求網站平台建立三大機制：在信息發佈環節強制設置來源標註選項，未標註內容不得進入算法推薦池；細化專業資質認證流程，動態核驗賬號身份與運營業務匹配度;暢通舉報渠道，對首次違規賬號採取提示引導，對惡意編造重點領域信息、仿冒熱點當事人的賬號實施長期禁言或封號。同時，平台需完善負面清單、營利權限管理等制度，對存在突出問題的平台依法採取處罰措施。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;此次行動強調「標本兼治」，既通過技術手段壓縮不實信息生存空間，如優化 AI 生成內容標識功能，又壓實平台主體責任，要求其定期排查隱形變異問題。據網信辦負責人介紹，專項行動將與日常監管形成合力，推動建立「自媒體」行業信用評價體系，引導內容創作者回歸真實、專業的傳播軌道，為公眾營造可信、有序的網絡環境。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362926</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362926</guid>
      <pubDate>Thu, 17 Jul 2025 05:47:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>微軟為 Edge 瀏覽器推出新的 Copilot 模式，支持實時分析屏幕內容</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;微軟正在為其 Edge 瀏覽器推出一種名為&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.microsoft.com%2Fzh-cn%2Fedge%2Fai-powered%2Fcopilot-mode" target="_blank"&gt;「Copilot Mode」&lt;/a&gt;的全新實驗性模式，旨在提供一種由 AI 驅動的網頁瀏覽體驗。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1856" src="https://static.oschina.net/uploads/space/2025/0729/113855_QJwL_2720166.png" width="3360" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;該模式包含多項新功能，包括全新的現代化主頁（New Modern Homepage）、快速撰寫（Quick Compose）、簡單的任務切換（Simple Task Handoff）以及語音導航（Voice Navigation）。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-4177c9b14685e66cd78d1978e425461b2a9.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;其中一個核心功能是「Copilot Vision」，它允許 Copilot 「看到」用户的屏幕，即時掃描和分析屏幕上的內容，並實時提供相關的建議和見解。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362895/microsoft-edge-copilot-mode</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362895/microsoft-edge-copilot-mode</guid>
      <pubDate>Thu, 17 Jul 2025 03:39:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>國內首個農業智能大模型上線，每畝地增收可達 200 元</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;中國中化集團推出了國內首個 「農業種植綜合大模型」。這個大模型不僅依託於全國數百座農業技術服務中心的支持，更整合了超過千萬條農業知識資源，為農民提供精準、科學的種植指導。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;該農業種植綜合大模型貫穿了 「耕、種、管、收」 的整個過程，能夠進行高效、可靠的複雜任務處理。通過大模型的應用，農藝師只需在手機或平板電腦上就能實現線上智能決策，線下為農民提供貼身服務。這意味着，農民能夠通過實時監測作物的生長情況、土壤濕度，以及氣象和病蟲害等重要因素，獲取及時的建議，例如 「每畝需要多少肥料、何時澆水」 等信息。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="282" src="https://oscimg.oschina.net/oscnet/up-b29e4fd6f3ee345ed36aa8353bfd5d8e80a.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;許多網友對此表示興奮，甚至戲稱這項技術讓他們想起了小時候的 QQ 農場。科技的進步，不僅為農事決策帶來了高效和便利，還能夠讓農民在增加收入的同時，享受到更多的樂趣和成就感。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;據悉，使用這一大模型後，農事決策的時間可以縮短 75%，每畝地的增收可達 150 到 200 元。這對廣大農民而言，無疑是一個好消息。農業種植綜合大模型的上線，不僅提升了農業生產的智能化水平，也為全國的農業發展注入了新動力。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362892</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362892</guid>
      <pubDate>Thu, 17 Jul 2025 03:37:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>上海 AI 實驗室開源科學多模態大模型『書生』Intern-S1</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;上海人工智能實驗室（上海 AI 實驗室）&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fd7DfDz4yw_5ktewSpjzVDA" target="_blank"&gt;發佈&lt;/a&gt;並開源『書生』科學多模態大模型 Intern-S1，聲稱多模態能力全球開源第一，文本能力比肩國內外一流模型，科學能力全模態達到國際領先，作為融合科學專業能力的基礎模型，Intern-S1 綜合性能為當前開源模型中最優。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;公告介紹稱，Intern-S1 在同一模型內實現了語言和多模態性能的高水平均衡發展，具備「全能高手」的實力；同時，作為「科學明星」，它還富集多學科專業知識，重點強化了科學能力，在化學、材料、地球等多學科專業任務基準上超越了頂尖閉源模型 Grok-4；此外，Intern-S1 還開創了「多任務的通專融合」的新範式，支持大規模多任務強化學習齊頭並進，在保持能力全面的同時實現專業精通。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="319" src="https://oscimg.oschina.net/oscnet/up-e8b2a446e9dc17a212b7fc9fe2fd02a0f70.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="304" src="https://oscimg.oschina.net/oscnet/up-8d4b59325c224883c411d165c948180e543.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="153" src="https://oscimg.oschina.net/oscnet/up-99ef18ed34310abfc6028e4dc8aa28294d1.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;為了更好地適應科學數據，Intern-S1 新增了動態 Tokenizer 和時序信號編碼器，可支持多種複雜科學模態數據，實現了材料科學與化學分子式、生物製藥領域的蛋白質序列、天文巡天中的光變曲線、天體碰撞產生的引力波信號、地震台網記錄的地震波形等多種科學模態的深度融合。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;Intern-S1 還實現了對科學模態數據的深入理解與高效處理，例如，其對化學分子式的壓縮率相比 DeepSeek-R1 提升 70% 以上；在一系列基於科學模態的專業任務上消耗的算力更少，同時性能表現更優。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;更多詳情可&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fd7DfDz4yw_5ktewSpjzVDA" target="_blank"&gt;查看官方公告&lt;/a&gt;。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362877</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362877</guid>
      <pubDate>Thu, 17 Jul 2025 03:12:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>GPT-5 即將發佈！相關參數、功能與展望預測彙總</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;隨着人工智能領域的競爭日益加劇，OpenAI 的下一代大語言模型 GPT-5 備受關注。根據最新信息，GPT-5 預計將於 2025 年年中至晚些時候發佈，具體時間可能在 8 月或更晚。本文綜合網絡信息，整理了關於 GPT-5 的參數、功能及潛在影響的最新動態，為您呈現最全面的預覽。&lt;/span&gt;&lt;/p&gt; 
&lt;h3 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;發佈日期與開發進展&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;根據 OpenAI 首席執行官 Sam Altman 在 2025 年 2 月發佈的路線圖，GPT-5 預計在 2025 年年中推出，具體可能在 8 月或稍晚。Altman 在近期採訪中表示，GPT-5 的發佈將晚於 GPT-4.5（代號 Orion，已於 2025 年 2 月 27 日發佈），並強調其為「前沿模型」，代表重大技術飛躍。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;然而，開發過程中面臨的技術與資源挑戰可能導致進一步延遲。據報道，GPT-5（代號可能為 Orion 或 Arrakis）的訓練成本高達 5 億美元以上，且需要大規模數據中心支持，訓練時間至少 6 個月。OpenAI 內部也經歷了高管離職等動盪，可能影響開發進度。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;img height="241" src="https://oscimg.oschina.net/oscnet/up-44d549d72cc75a7aa62b1b9593f9f177949.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;h3 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;參數規模與技術架構&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;雖然 OpenAI 尚未公開 GPT-5 的具體參數數量，但業界推測其參數規模將顯著超越前代模型。以下是關鍵信息:&lt;/span&gt;&lt;/p&gt; 
&lt;ul style="margin-left:0; margin-right:0"&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;參數規模&lt;/strong&gt;：GPT-4 據估擁有約 1.5 萬億參數，而 GPT-5 可能達到 3 至 50 萬億參數，具體取決於是否採用混合專家模型（MoE）。有報道稱，GPT-5 可能利用 20，000 個 NVIDIA GB200 芯片或 150，000 個 H100 芯片進行訓練，支持高達 80 萬億參數的模型。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;架構創新&lt;/strong&gt;：GPT-5 將整合 GPT 系列與 o 系列（如 o1、o3）的能力，採用統一架構，消除用户在不同任務間切換模型的需求。可能引入圖神經網絡 (GNN) 和增強注意力機制，提升語言處理效率和複雜情境理解能力。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;訓練數據&lt;/strong&gt;：GPT-5 預計使用更大規模的多樣化數據集，包括公開網絡數據和私有企業數據，可能結合合成數據以應對數據短缺問題。然而，合成數據可能引發反饋循環，增加「幻覺」風險。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;核心功能與改進&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;GPT-5 被設計為多模態、統一智能系統，旨在提供更高效、準確的 AI 體驗。以下是其核心功能的預期亮點:&lt;/span&gt;&lt;/p&gt; 
&lt;ul style="margin-left:0; margin-right:0"&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;多模態能力&lt;/strong&gt;：GPT-5 將進一步增強多模態處理能力，支持文本、圖像、語音和視頻輸入輸出。基於 GPT-4o 的語音和圖像處理基礎，GPT-5 可能集成視頻處理功能，例如通過 OpenAI 的 SORA 技術實現文本到視頻的生成。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;高級推理能力&lt;/strong&gt;：OpenAI 強調 GPT-5 將顯著提升鏈式推理（Chain-of-Thought）能力，擅長多步驟邏輯和決策制定。相比 GPT-4o 的快速響應，GPT-5 將更擅長處理複雜問題，如軟件工程中的代碼生成與調試、數學和物理等科學任務。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;上下文窗口擴展&lt;/strong&gt;：GPT-4o 的上下文窗口為 128，000 個 token，而 GPT-5 可能支持高達 500 萬個 token，足以處理整本書籍或大型企業數據，提升長文本處理能力。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;減少幻覺&lt;/strong&gt;：GPT-5 預計將「幻覺」率降至 10% 以下，顯著提高輸出的準確性和可靠性，特別是在科學和編程領域。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;自主 AI 代理&lt;/strong&gt;：GPT-5 可能引入自主 AI 代理功能，能夠執行現實世界的任務，如管理郵件、預訂日程或根據用户偏好完成購物，減少人工幹預。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;Canvas 工作空間&lt;/strong&gt;：基於 GPT-4o 的 Canvas 功能，GPT-5 將提供更強大的交互式工作空間，優化編碼、數學和分步工作流程的體驗。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;行業影響與應用前景&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;GPT-5 的發佈將對多個領域產生深遠影響:&lt;/span&gt;&lt;/p&gt; 
&lt;ul style="margin-left:0; margin-right:0"&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;軟件開發&lt;/strong&gt;：測試者反饋，GPT-5 在複雜軟件項目中的代碼生成和調試能力超越了 Anthropic 的 Claude4Sonnet，可能成為開發者首選工具。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;科學研究&lt;/strong&gt;：在數學、物理和生物學等學科中，GPT-5 的高級推理能力將加速研究進程，支持複雜數據分析和假設驗證。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;商業與生產力&lt;/strong&gt;：通過自主 AI 代理和個性化功能，GPT-5 可優化客户服務、內容創作和日常任務自動化，提升企業效率。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;教育與醫療&lt;/strong&gt;：GPT-5 的多模態能力和上下文理解將革新教育領域的個性化學習，以及醫療領域的患者交互和文檔處理。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;挑戰與倫理考量&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;儘管前景光明，GPT-5 的開發和部署面臨多重挑戰:&lt;/span&gt;&lt;/p&gt; 
&lt;ul style="margin-left:0; margin-right:0"&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;成本與資源&lt;/strong&gt;：訓練成本高昂，數據中心建設週期長，可能限制 OpenAI 的並行開發能力。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;倫理與安全&lt;/strong&gt;：大規模模型可能引發誤用風險，如生成虛假信息或模擬人類行為。OpenAI 正在進行嚴格的安全測試，推遲了部分功能的發佈。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;競爭壓力&lt;/strong&gt;：Anthropic 的 Claude 系列、Google 的 Gemini 和 Meta 的 LLaMA 等競品正在迅速追趕，迫使 OpenAI 在性能與可靠性之間找到平衡。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;用户反饋與社區期待&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;社交媒體上，開發者對 GPT-5 的期待集中在編程能力和推理性能的提升。部分用户在 X 平台上提到，GPT-5 的早期測試版在軟件工程任務中表現優異，超越 Claude Sonnet4。然而，也有用户擔憂其高昂的訂閲成本和潛在的使用限制，類似 Anthropic 對 Claude Code Max 計劃的調整可能引發的不滿。&lt;/span&gt;&lt;/p&gt; 
&lt;h3 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;總結&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;GPT-5 作為 OpenAI 的下一代旗艦模型，預計將通過更大的參數規模、統一的架構和多模態能力，顯著提升 AI 的推理、準確性和實用性。儘管面臨成本、安全和競爭等多重挑戰，其在編程、科研和商業領域的潛力不容忽視。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362874</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362874</guid>
      <pubDate>Thu, 17 Jul 2025 03:01:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Anthropic 將在 8 月底面向 Claude Pro 和 Max 訂閲用户推出新的每週使用限制</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Anthropic &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2FAnthropicAI%2Fstatus%2F1949898502688903593" target="_blank"&gt;宣佈&lt;/a&gt;，由於 Claude Code 的需求空前增長，將從 8 月 28 日起為 Claude Pro 和 Max 訂閲計劃引入新的每週使用量限制。此舉旨在解決因少數極端使用案例和違反服務條款的行為（如賬户共享和轉售）導致的系統容量問題，以確保為所有用户提供更公平、可靠的服務。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0729/105318_5uv3_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;新的限制措施將在現有的每 5 小時重置限制的基礎上，增加一個每 7 天重置的總體每週限制，以及一個針對 Claude Opus 4 的特定每週限制。Anthropic 估計，根據當前使用情況，新規將影響不到 5% 的訂閲用户。&lt;/p&gt; 
&lt;p&gt;公司透露，一些重度用户在後台 24/7 連續運行模型，其中一個案例是在每月 200 美元的套餐上消耗了價值數萬美元的模型用量。新限制旨在緩解此類高成本使用情況。&lt;/p&gt; 
&lt;p&gt;對於受影響的用户，Anthropic 給出了一些預期使用時長的參考：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;大多數 Pro 用户在每週限制內預計可使用 40-80 小時的 Sonnet 4；&lt;/li&gt; 
 &lt;li&gt;大多數 Max 20x 用户則可使用 240-480 小時的 Sonnet 4 和 24-40 小時的 Opus 4。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;對於超出限制的 Max 計劃用户，Anthropic 將提供以標準 API 價格購買額外用量的選項。公司表示仍在探索支持長期使用案例的最佳方式，並歡迎重度用户提供反饋。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362871</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362871</guid>
      <pubDate>Thu, 17 Jul 2025 02:53:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>快手可靈發佈 Kling Lab</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;快手的 Kling AI 團隊&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2FKling_ai%2Fstatus%2F1949760383692255518" target="_blank"&gt;宣佈&lt;/a&gt;推出 Kling Lab，這是一個旨在簡化創作流程、提高效率並促進協作的新工作空間。該產品目前正處於 Beta 測試階段。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0729/104711_dOxb_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;此外，在 2025 世界人工智能大會（WAIC）上，可靈 AI 還披露了最新用户數據，在全球擁有超過 4500 萬創作者，產品自發布以來迭代升級 30 餘次，累計生成超 2 億個視頻和 4 億張圖片。&lt;/p&gt; 
&lt;p&gt;可靈 AI 產品及運營負責人李楊表示，4 月可靈 2.0 發佈以來，服務的 B 端商家數量迎來爆發式增長。截至目前，全球範圍內已有超過兩萬企業客户及開發者接入了可靈 AI 的 API（應用程序編程接口）接口，覆蓋全球 149 個國家和地區。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362870</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362870</guid>
      <pubDate>Thu, 17 Jul 2025 02:48:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>阿里開源通義萬相 Wan2.2</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;阿里&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FiPL7OLQhwYdoFelHt41N6Q" target="_blank"&gt;宣佈&lt;/a&gt;開源視頻生成模型「通義萬相 Wan2.2」，此次共開源文生視頻（Wan2.2-T2V-A14B）、圖生視頻（Wan2.2-I2V-A14B）和統一視頻生成（Wan2.2-IT2V-5B）三款模型。其中文生視頻模型和圖生視頻模型均為業界首個使用 MoE 架構的視頻生成模型，總參數量為 27B，激活參數 14B。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;根據介紹，通義萬相 2.2 率先在視頻生成擴散模型中引入 MoE 架構，有效解決視頻生成處理 Token 過長導致的計算資源消耗大問題。Wan2.2-T2V-A14B、Wan2.2-I2V-A14B 兩款模型均由高噪聲專家模型和低噪專家模型組成，分別負責視頻的整體佈局和細節完善，在同參數規模下，可節省約 50% 的計算資源消耗，在模型能上，通義萬相 2.2 在複雜運動生成、人物交互、美學表達、複雜運動等維度上也取得了顯著提升。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;Wan2.2 還首創了「電影美學控制系統」，光影、色彩、構圖、微表情等能力媲美專業電影水平。例如，用户輸入「黃昏」、「柔光」、「邊緣光」、「暖色調」「中心構圖」等關鍵詞，模型可自動生成金色的落日餘暉的浪漫畫面；使用「冷色調」、「硬光」、「平衡圖」、「低角度」的組合，則可以生成接近科幻片的畫面效果。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" height="213" src="https://oscimg.oschina.net/oscnet/up-9384df8eada31bdbc196286746847bfe546.gif" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img alt="" height="177" src="https://oscimg.oschina.net/oscnet/up-58b00afb9376a3a0972e10d973fdb82ab5f.gif" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img alt="" height="213" src="https://oscimg.oschina.net/oscnet/up-7c4a2896d24586d24ff41491862261a2aeb.gif" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;通義萬相還開源了一款 5B 小尺寸的統一視頻生成模型，單一模型同時支持文生視頻和圖生視頻，可在消費級顯卡部署。該模型採用了高壓縮率 3D VAE 架構，時間與空間壓縮比達到高達 4×16×16，信息壓縮率提升至 64，均實現了開源模型的最高水平，僅需 22G 顯存（單張消費級顯卡）即可在數分鐘內生成 5 秒高清視頻，是目前 24 幀每秒、720P 像素級的生成速度最快的基礎模型。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362867</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362867</guid>
      <pubDate>Thu, 17 Jul 2025 02:13:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>阿里雲正式開源 LoongSuite：打造 AI 時代的高性能低成本可觀測採集套件</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;p&gt;作者：阿里雲可觀測開源&lt;/p&gt; 
&lt;h2&gt;AI Agent 技術架構演進重塑軟件工程實踐方式&lt;/h2&gt; 
&lt;p&gt;在 AI Agent 開發領域，技術架構的演進正在重塑軟件工程的實踐方式。開發者既可以通過 Cursor、通義靈碼、Claude Code 等智能編程助手提升代碼生成效率，也可依託專業的 AI Agent 開發框架構建完整的智能體系統。技術生態呈現出多維度發展：實現方式既有需要深度編碼的高代碼方案，也有通過可視化組件拖拽的低代碼平台；技術棧維度 Java 生態的 Spring AI Alibaba 與 Python 領域的 Dify、AgentScope 等工具形成跨語言支持體系，其中 Python 憑藉其豐富的 AI 庫生態佔據主導地位。技術演進也催生新型開發範式：AutoGen 的多 Agent 對話框架、LangChain 的模塊化組件體系，都在降低智能體開發的技術門檻。&lt;/p&gt; 
&lt;p&gt;我們把智能體的核心能力體系，總結成四個關鍵構成維度：感知層需要集成多模態交互能力，包括自然語言處理、語音識別和視頻流分析；決策中樞由大模型構成，通過 AI 網關（如 Higress）實現模型調用的統一調度，同時也承擔流量控制與安全防護的關鍵角色；記憶機制存儲用户交互歷史並具備上下文關聯能力；工具集成方面，隨着 MCP 協議的出現，工具的使用逐漸標準化。工具成為 AI Agent 和傳統互聯網時代的數字世界很好的溝通渠道。而 MCP 市場的出現可以將 MCP 工具集中進行管理和發現，高效完成 Agent 和工具的連接。同時，當單體 Agent 的能力邊界被突破時，多 Agent 系統通過 A2A 協議實現協同計算，這種分佈式智能架構能夠處理更復雜的任務場景。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-c3b5e0b94b8ba58ffcbf17953e3222070a3.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;AI 工具鏈全景圖&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;隨着開發工具鏈的持續完善，AI Agent 在完成開發後需要進行部署。Agent 執行環境的差異化需求催生了多樣的架構模式：面向個人用户的桌面端 Agent（如 Cherry Studio, DeepChat）可以通過雲端沙箱環境將運行時延伸到雲端，而面向企業服務的 Agent 則運行在具備資源隔離的雲原生環境中，Serverless 架構（如函數計算）可以為其提供彈性伸縮的基礎設施。在 AI Agent 運行過程中，一些通用的能力也需要由中間件來支撐：通過 Nacos 實現的動態 Prompt 管理以及 MCP 註冊中心、Higress 可以作為 AI 模型和 MCP Server 統一代理、RocketMQ 支撐的異步任務隊列、Redis 提供的狀態存儲等共同構成智能體運行的技術底座。同時，安全體系的構建面臨數據合規與系統防護雙重挑戰。在數據治理層面，需建立敏感信息過濾機制和審計追蹤系統；針對 MCP 協議的安全漏洞，可採用沙箱隔離、工具簽名認證等技術手段構建防禦體系。可觀測性平台通過採集 Agent 與模型的調用、token 消耗、性能指標等關鍵信息，為系統優化和威脅檢測提供數據支撐。&lt;/p&gt; 
&lt;h2&gt;可觀測性：AI Agent 技術發展的重要基石&lt;/h2&gt; 
&lt;p&gt;正如前文提到，AI Agent 的開發已突破傳統軟件工程的邊界，其非確定性決策機制與動態化執行流程對可觀測性提出了革命性要求。一個智能體其背後涉及的多模態數據處理、大模型推理及工具鏈調用的複雜度呈指數級增長。這種複雜性不僅體現在技術架構層面，更深刻影響着系統的穩定性保障、成本控制與合規審計等核心運維環節。&lt;/p&gt; 
&lt;p&gt;AI Agent 的自主決策特性使其區別於傳統軟件應用，涉及多模態數據處理、大模型推理及工具調用等複雜交互。當這種非線性工作流應用於真實業務場景時，任何環節的異常都可能引發連鎖反應。另一方面，當 Agent 與模型進行多輪交互時，中間過程可能產生驚人的 Token 消耗，甚至有可能陷入無休止的狀態，形成所謂的 "Token 黑洞"。在缺乏鏈路追蹤機制的情況下，開發者難以定位服務異常的根源。通過構建端到端的可觀測能力可以提供堅實的決策依據。&lt;/p&gt; 
&lt;p&gt;AI Agent 的迭代升級需要在保持服務連續性的前提下進行，這要求建立完善的迴歸測試評估體系。每一次提示詞、模型的變更都可能引發不可預見的副作用。每一次 AI Agent 的修改和發佈上線，我們都需要對 Agent 執行的結果進行評估，這相當於對 AI Agent 進行"迴歸測試"。通過採集執行過程中的可觀測數據，企業可以構建自動化評估框架，量化新版本對服務質量的影響，避免版本迭代風險失控。&lt;/p&gt; 
&lt;p&gt;隨着生成式 AI 不斷發展，可觀測性正從運維工具進化為 AI 應用架構的核心組件。正是看到了這樣的技術趨勢，OpenTelemetry 社區推動的 GenAI 語義約定，正在構建跨框架、跨供應商的標準化數據規範。也是在這樣的技術背景之下，阿里雲正式開源 LoongSuite 可觀測採集套件，在順應 AI 時代技術發展趨勢的基礎上，幫助更多企業，通過高性能低成本的方式，更高效地利用標準化數據規範模型建立可觀測體系。&lt;/p&gt; 
&lt;h2&gt;LoongSuite ：打造 AI 時代高性能低成本的可觀測採集套件&lt;/h2&gt; 
&lt;p&gt;LoongSuite （/lʊŋ swiːt/）（音譯，龍-sweet），作為下一代可觀測性技術生態的核心載體，核心數據採集引擎實現了主機級探針與進程級插樁的有效結合，進程級探針實現應用內細粒度可觀測數據採集，而主機探針則實現了高效靈活的數據處理和數據上報，以及通過 eBPF 等技術實現了進程外數據採集能力。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-6f16653bf7aa654aa37e5e43c6508112a9a.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;LoongSuite 技術應用架構&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;在進程級數據採集層面，LoongSuite 對 Java、Go、Python 等主流編程語言構建企業級觀測能力。通過語言特性的深度適配，採集器能夠自動捕獲函數調用鏈路、參數傳遞路徑及資源消耗，無需修改業務代碼即可實現運行時狀態的精準採集。這種無侵入式設計特別適用於動態更新頻繁的技術環境，既保障觀測數據的完整性，又避免對核心業務邏輯產生幹擾。當面對複雜工作流時，系統可自動關聯分佈式追蹤上下文，構建完整的執行路徑拓撲。作為核心數據採集引擎，LoongCollector 實現多維度觀測數據的統一處理，從原始數據採集到結構化轉換，再到智能路由分發，整個流程通過模塊化架構實現靈活編排。這種架構使觀測數據既可對接開源分析平台實現自主治理，也可無縫銜接託管服務構建雲原生觀測體系。在技術生態構建方面，阿里雲深度參與國際開源標準制定，其核心組件與 OpenTelemetry 等主流標準兼容。接下來，我們將逐一介紹相關組件。&lt;/p&gt; 
&lt;h3&gt;LoongCollector&lt;/h3&gt; 
&lt;p&gt;LoongCollector 作為新一代可觀測性數據採集器，通過深度性能優化與技術架構創新，為雲原生智算服務提供了高性能、高穩定的可觀測數據採集與預處理解決方案，尤其在 AI 場景中展現出顯著優勢。&lt;/p&gt; 
&lt;p&gt;首先，LoongCollector 具備多維度的可觀測數據採集能力，支持 Logs、Metrics、Traces、Events、Profiles 等多種類型數據的統一採集、處理與傳輸，實現了 All-in-One 的可觀測性管理架構。它融合了實時日誌採集、Prometheus 指標拉取、eBPF 技術等能力，在無需修改系統代碼的前提下完成無侵入式監控，能夠高效獲取各類性能指標，尤其適用於大規模分佈式訓練和推理任務中的一體化可觀測需求。&lt;/p&gt; 
&lt;p&gt;其次，LoongCollector 在性能與穩定性方面表現出色。其採用事件驅動架構、時間片調度、無鎖化設計等技術，確保在高併發、大規模數據採集場景下仍能保持低資源消耗和高吞吐量。同時，其高低水位反饋隊列機制和持久化緩存能力，使其具備良好的流量控制和容錯能力，確保數據不丟失、採集不間斷、服務不抖動，全面滿足 AI 訓練過程中對穩定性、連續性和可靠性的嚴苛要求。&lt;/p&gt; 
&lt;p&gt;再者，在 AI 場景中，LoongCollector 支持多種部署模式，包括 Agent 模式和集羣模式，能夠靈活適應分佈式訓練和推理任務的彈性需求。其具備自動發現容器上下文、關聯 K8s 元信息、多租户隔離等能力，確保在複雜雲原生環境下實現高效、安全的數據採集。同時，通過配置管理服務 ConfigServer，可實現對大規模 Agent 的集中管控與動態配置下發，顯著提升運維效率與系統可控性。&lt;/p&gt; 
&lt;p&gt;此外，LoongCollector 實現了多維度觀測數據的統一處理能力。從原始數據採集到結構化轉換，到數據過濾聚合處理，再到路由分發，全流程模塊化靈活編排、按需擴展。其支持 SPL 查詢語言與多語言插件雙引擎驅動，並內置豐富的數據處理算子，滿足多樣化、高吞吐的數據預處理場景。&lt;/p&gt; 
&lt;p&gt;綜上所述，LoongCollector 憑藉其全面的數據採集能力、卓越的性能表現、靈活的部署方式與強大的可編程性，成為 AI 場景下構建可觀測性體系的核心基礎設施，助力企業實現高效、穩定的智算服務運維。&lt;/p&gt; 
&lt;h3&gt;LoongSuite Python Agent&lt;/h3&gt; 
&lt;p&gt;LoongSuite Python Agent 基於 OpenTelemetry Python Agent 構建，OTel 社區由於還在制定 GenAI 語義規範，很多 AI 框架的支持尚未完全實現，目前基本只有 OpenAI 的插件可以支持可觀測數據採集，和國內的流行 AI 框架相去甚遠。LoongSuite Python Agent OTel GenAI 語義規範的最新實現，在遵循開源語義規範的基礎上，添加了國內流行插件的支持。例如國內流行的 AgentScope, Agno 等 AI 編程框架，目前已經率先提供了支持，更多插件包括 Dify、Langchain、MCP Client 的支持，陸續會開源，並且會將這些插件貢獻回 OTel 社區。通過 Python agent 我們可以輕鬆地採集 AI agent 調用模型和工具過程中的詳細信息，耗時等多方面的數據。藉助 OTel 項目可以將這些數據以標準的 OTLP 協議的方式上報到任意的存儲之中，並且通過可視化的界面進行展示。&lt;/p&gt; 
&lt;h3&gt;LoongSuite Go Agent&lt;/h3&gt; 
&lt;p&gt;LoongSuite Go Agent 通過編譯時插樁技術，為 Go 語言構建的 AI Agent 提供無侵入式的觀測能力。通過深度解析 Go 語言的編譯流程，在 AST 語法樹分析階段植入監控邏輯，實現了在不修改源代碼的前提下完成可觀測性能力的注入。LoongSuite Go Agent 採用編譯增強機制，通過預定義的埋點規則引擎，在編譯階段自動注入 Span 創建、token 消耗等統計邏輯。內置對主流開發框架的完整支持，從基礎通信協議到中間件交互，從微服務治理到數據持久化，系統已覆蓋包括 HTTP、gRPC、數據庫連接等在內的二十多個核心模塊，能夠自動捕獲請求延遲分佈、服務調用拓撲及資源競爭狀態等關鍵指標。這種開箱即用的設計顯著降低了觀測體系的部署門檻，使開發者能夠聚焦業務邏輯而非基礎設施配置。LoongSuite Go Agent 可以精準捕獲大模型調用的輸入輸出特徵、Token 消耗模式及多輪交互的流程軌跡，為優化資源利用率提供了數據基礎。目前支持的 AI Agent 開發框架包括 LangChainGo【1】，MCP Server【2】等，Eino、Ollma 等框架的支持也將陸續發佈。&lt;/p&gt; 
&lt;h3&gt;LoongSuite Java Agent&lt;/h3&gt; 
&lt;p&gt;LoongSuite Java Agent 基於 OpenTelemetry Java Instrumentation 項目，通過字節碼增強技術，為 Java 應用提供全鏈路的可觀測性解決方案。藉助對 Java 字節碼的動態修改能力，實現了無需手動修改業務代碼即可接入分佈式追蹤、指標收集和日誌關聯的觀測體系。在極低性能開銷的前提下，提供細粒度的運行時數據採集能力，適配從傳統單體應用到雲原生微服務的全場景觀測需求。從基礎的 Servlet、Spring、Dubbo 等開發框架，到 Redis、Kafka、MySQL 等中間件，再到 JVM 自身的性能指標採集，系統已覆蓋超過 50+ 常用組件的自動埋點，能夠自動捕獲調用鏈路拓撲、方法執行耗時、異常堆棧及資源消耗等關鍵數據。這種即插即用的特性極大降低了可觀測性接入的技術門檻，使開發者無需關注埋點細節即可獲得全面的系統運行視圖。針對高併發場景，其內置的採樣策略與數據聚合機制可在保證觀測精度的同時，有效控制數據量，滿足生產環境的高可用性要求。目前已經在百鍊大模型平台大規模生產落地，這些過程中積累的在大模型場景下數據採集的優化等方式將陸續發佈到開源倉庫。此外，針對常見的模型訪問 SDK 如 OpenAI, DashScope 等，正在提供自動埋點支持，也歡迎社區貢獻更多的插件實現。&lt;/p&gt; 
&lt;h3&gt;Loongsuite 與 Spring AI Alibaba 共建 AI 應用生態建設&lt;/h3&gt; 
&lt;p&gt;Spring AI 作為 Spring 生態與大模型能力融合的產物，在 Java 語言中提供了對 LLM 的抽象封裝和易用的 API，同時在可觀測性設計上充分擁抱 OpenTelemetry 標準，為關鍵調用提供了原生的可觀測性能力。Spring AI Alibaba【3】 是 Alibaba 在 Spring AI 項目的基礎上構建的 AI Agent 開發框架，深度集成了百鍊大模型平台能力，提供瞭如工作台、Graph 等諸多可視化的白屏能力，以及各種開箱即用的預實現 Agent。Spring AI 的核心目標是讓開發者能夠以 Spring 的方式快速集成和使用 AI 能力。因此像 Spring 一樣，可觀測性被作為重要的組成部分集成在框架內部。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-483e2b3957f208292fdeef0998b8034673d.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在可觀測性上，Spring AI 提供以下關鍵能力：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;自動埋點：Spring AI 對所有涉及 LLM 調用、Prompt 構建、流式響應處理等關鍵路徑進行了自動追蹤埋點，並生成符合 OpenTelemetry 標準的 Span。&lt;/li&gt; 
 &lt;li&gt;上下文傳播：支持在調用鏈中自動注入和提取 Trace ID 和 Span ID，確保與上下游服務的調用鏈路無縫銜接。&lt;/li&gt; 
 &lt;li&gt;指標導出：內置對請求延遲、token 使用量、模型響應長度等關鍵性能指標的採集與導出。&lt;/li&gt; 
 &lt;li&gt;日誌關聯：通過 MDC 或結構化日誌機制，將當前 Span 上下文注入到日誌中，便於問題排查時進行全棧分析。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;這些能力使得 Spring AI 在接入觀測系統時無需額外開發即可實現完整的追蹤、監控與日誌聯動。為了進一步提升可觀測性覆蓋範圍並降低接入成本，Spring AI Alibaba 支持結合 LoongSuite Java Agent 進行部署。Java Agent 可以無侵入地對運行中的 JVM 應用進行字節碼增強，從而實現對 Spring 框架、數據庫訪問、HTTP 請求等通用組件的自動埋點。&lt;/p&gt; 
&lt;h2&gt;LoongSuite 項目未來規劃&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;面對 AI Agent 眾多的框架，LoongSuite 將會針對市面上的主流 AI Agent 提供全面的可觀測性數據採集能力，包括 Python 生態中的低代碼平台 Dify、高代碼框架 AgentScope、Agno、OpenAI Agent 等主流 AI Agent 開發框架，同時也包括 Java 生態中的 Spring AI Alibaba 以及其基礎之上的低代碼以及 0 代碼 Agent JManus 提供強有力的支撐，Golang 生態中的 Eino, Langchain4go 等等，也歡迎有興趣參與社區的同學一起參與貢獻更多的框架。&lt;/li&gt; 
 &lt;li&gt;未來 Agent 會大量使用工具，多智能體的協同也將成為常態，LoongSuite 會打通 MCP 和多 Agent 通訊的觀測盲區，突破 MCP token 黑洞，實現對 MCP 和 A2A 協議的可觀測覆蓋。&lt;/li&gt; 
 &lt;li&gt;AI Agent 開發完在測試和線上運行期間都需要對 AI Agent 的行為進行充分的評估，評估的能力逐步成為 AI Agent 生命週期中不可或缺的一環，和 Spring AI Alibaba 以及 AgentScope 等項目集成，發佈開源可觀測追蹤和評估能力控制枱，形成從採集、存儲到評估的 AI Agent 全週期覆蓋。&lt;/li&gt; 
 &lt;li&gt;實現端到端可觀測能力的覆蓋，打通端側 Agent 到模型內部的整條鏈路，實現 AI Agent 鏈路完整分析和快速診斷。&lt;/li&gt; 
 &lt;li&gt;LoongCollector 通過 eBPF 支持 CPU 和 GPU 場景下的 Profiling 能力，LoongSuite 也將和 SysOM【4】 社區共同推出 AI 場景下的 Profiling 能力。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;開源社區參與&amp;amp;貢獻&lt;/h2&gt; 
&lt;p&gt;作為全球領先的雲服務商，阿里雲始終致力於開源觀測技術的最前沿。我們深度投身於 OpenTelemetry（OTel）社區，堅定不移地參與、支持技術開放生態的構建以及全球技術標準的制定。過去幾年，阿里雲在 OpenTelemetry 社區中積極推動技術共享與代碼貢獻，深度融入社區多個關鍵領域，如 Semantic Conventions（可觀測標準規範建設）、Java Instrumentation（Java 探針）、Go Instrumentation（Go 探針）、Profiling（性能分析）等。截至目前，我們累計向社區貢獻併合並 1000+ PR Reviews 與 400+ Pull Requests。在這一開源貢獻進程中，我們成功培養出 3 位 Maintainer、5 位 Approvers、1 位 Triager 以及 8 位 Member，為社區的技術演進與生態建設注入了強勁動力。&lt;/p&gt; 
&lt;p&gt;除卻技術貢獻，阿里雲亦全力踐行開源文化所倡導的分享與合作精神，積極推動新技術與新思想的蓬勃發展。例如，我們踴躍在 KubeCon、OTel Community Day 等全球性行業會議中分享技術成果，同時在社區內發起設立了面向亞太地區的友好交流時段，有力促進了與社區的跨地域技術交流與深度合作。也歡迎更多的開發者加入 OTel 社區以及 LoongSuite 中。LoongSuite 開源的代碼倉庫如下，歡迎參與貢獻：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;LoongCollector:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Falibaba%2Floongcollector" target="_blank"&gt;https://github.com/alibaba/loongcollector&lt;/a&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;LoongSuite Python Agent:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Falibaba%2Floongsuite-python-agent" target="_blank"&gt;https://github.com/alibaba/loongsuite-python-agent&lt;/a&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;LoongSuite Go Agent:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Falibaba%2Floongsuite-go-agent" target="_blank"&gt;https://github.com/alibaba/loongsuite-go-agent&lt;/a&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;LoongSuite Java Agent:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Falibaba%2Floongsuite-java-agent" target="_blank"&gt;https://github.com/alibaba/loongsuite-java-agent&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;相關鏈接&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;【1】LangChainGo&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Ftmc%2Flangchaingo" target="_blank"&gt;https://github.com/tmc/langchaingo&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;【2】MCP Server&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fmark3labs%2Fmcp-go" target="_blank"&gt;https://github.com/mark3labs/mcp-go&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;【3】Spring AI Alibaba&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Falibaba%2Fspring-ai-alibaba%2Fblob%2Fmain%2FREADME-zh.md" target="_blank"&gt;https://github.com/alibaba/spring-ai-alibaba/blob/main/README-zh.md&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;【4】SysOM&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopenanolis.cn%2Fsig%2Fsysom" target="_blank"&gt;https://openanolis.cn/sig/sysom&lt;/a&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/3874284/blog/18686155</link>
      <guid isPermaLink="false">https://my.oschina.net/u/3874284/blog/18686155</guid>
      <pubDate>Thu, 17 Jul 2025 02:08:00 GMT</pubDate>
      <author>原創</author>
    </item>
  </channel>
</rss>
