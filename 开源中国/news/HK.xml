<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>開源中國-最新資訊</title>
        <link>https://www.oschina.net/news/project</link>
        <atom:link href="http://8.134.148.166:30044/oschina/news" rel="self" type="application/rss+xml"></atom:link>
        <description>開源中國-最新資訊 - Powered by RSSHub</description>
        <generator>RSSHub</generator>
        <webMaster>contact@rsshub.app (RSSHub)</webMaster>
        <language>en</language>
        <lastBuildDate>Thu, 06 Mar 2025 12:49:00 GMT</lastBuildDate>
        <ttl>5</ttl>
        <item>
            <title>開源中國馬越：DeepSeek 不是國運級的創新，年輕人才是</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;blockquote&gt; 
 &lt;p&gt;「你很難要求大家還沒吃飽喝足的情況下，去做開源。」&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;如果説關於 DeepSeek 的討論已經過於氾濫，開源也許是當下依然值得討論的主題。&lt;/p&gt; 
&lt;p&gt;長期以來，在國內談起「開源」，都會無可避免地陷入一種尷尬的語境。&lt;/p&gt; 
&lt;p&gt;它當然是理想主義的。「開源」背後的自由、開放特性，被普遍認為是互聯網精神的外化——源代碼向公眾開放共享，且允許在遵循特定許可證條款的前提下，對軟件進行自由使用、修改和二次分發。&lt;/p&gt; 
&lt;p&gt;最知名的開源項目「Linux」是操作系統的內核，催生了數以千萬計的開源軟件，這是互聯網世界的根基。&lt;/p&gt; 
&lt;p&gt;但它的背後經常跟着一個問題：為什麼要開源？怎麼考慮開源之後的商業化？哪怕到 DeepSeek 爆火的現在，也很難有人給出完美的答案。&lt;/p&gt; 
&lt;p&gt;開源中國董事長馬越，是最有立場談國內開源歷史的人之一，他在這條路上走了 18 年。&lt;/p&gt; 
&lt;p&gt;2008 年，馬越從硅谷回國創業，先是成立了「恆拓開源」——用開源軟件幫助企業擺脱數據庫、ERP 等大型軟件的束縛。&lt;/p&gt; 
&lt;p&gt;但很快他就發現，這種方案很難擺脱 To B 項目制的重投入，還很容易做成外包公司。&lt;/p&gt; 
&lt;p&gt;隨後，馬越選擇收購「開源中國」這個社區，開始了一段曲折的創業路——「開源中國」經歷過數度轉型，從開源社區，拓展到代碼託管、代碼工具鏈，在探索商業化期間，經歷了從母公司剝離獨立發展，2019 年被百度戰略控股，最後，又在中美競爭、國產替代浪潮中決定重新獨立發展，謀求上市。&lt;/p&gt; 
&lt;p&gt;做開源社區需要大量的資源、資金投入，在開源中國最艱難的時候，馬越揹負的個人債務最高達 1.8 億元。&lt;/p&gt; 
&lt;p&gt;1972 年出生的馬越，有着一種老大哥式的坦率。他絕沒有賣苦的意思，但你很容易從他的敍述中，體會到經歷這些坎坷過後的幽默——他表示，在中國做 To B 就是「城市包圍農村」，企業軟件就是管理者智慧的固化。當中國的企業發展階段還在初期，「你很難要求大家還沒吃飽喝足的情況下，去做開源。」&lt;/p&gt; 
&lt;p&gt;但這些時刻都已經過去了。開源中國也已經摸索出一條更適合自己的、中國式的開源道路。&lt;/p&gt; 
&lt;p&gt;現在，開源中國已經成為全球第二大的代碼託管平台，匯聚了超過 1800 萬開發者。其自主研發的 DevOps 工具鏈已在金融、軍工等關鍵領域，達到 80% 的市場滲透率。2024 年，開源中國的營收已超過 2 億元。&lt;/p&gt; 
&lt;p&gt;《智能湧現》獲悉，&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/news/337301/oschina-series-c-funding-round&quot;&gt;「開源中國」近期正式完成數億元 C 輪融資&lt;/a&gt;&lt;/u&gt;，由北京信息產業發展投資基金（北京信產基金）領投，深報一本股權投資基金（深報一本）及北京上河動量私募股權基金（上河動量）跟投。&lt;/p&gt; 
&lt;p&gt;至此，開源中國已累計獲得超 16 億元戰略投資。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0306/194426_EaKH_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;△開源中國董事長馬越&amp;nbsp;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;馬越認為，哪怕在全球範圍內，開源也不是件容易的事情。他以 GitHub 舉例：從 2008 年成立開始直到被微軟收購，在 2022 年 ChatGPT 爆發後推出 Copilot，才算是正式證明瞭商業化潛力。&lt;/p&gt; 
&lt;p&gt;「開源是強者和富人的遊戲。」他説，上一代人都成長在物質更短缺的年代——商業社會也是如此，企業要先賺夠了錢，才有餘裕考慮是否開源，做一些人人為我、我為人人的好事。「吃飽了飯，才能有力氣談開源。」&lt;/p&gt; 
&lt;p&gt;這就不難理解，即使 DeepSeek 的爆火為全中國都打了一記強心針，馬越的觀點依然是冷靜的。他認為，DeepSeek 很難根本性改變國內軟件生態的問題，這是一個時代的侷限。&lt;/p&gt; 
&lt;p&gt;而想要在開源路線上有所成就，這要求新一代的開發者，從 Day 1 就開始出海，像 DeepSeek 一樣去全球市場中競爭。&lt;/p&gt; 
&lt;p&gt;如果説 DeepSeek 改變了什麼，更多的都是文化和價值觀層面的事情。「十年前大家普遍不理解開源，覺得開源是一羣草根做的事，現在全社會都能認識到，開源等於創新。」馬越説。&lt;/p&gt; 
&lt;p&gt;以下為《智能湧現》與開源中國董事長馬越的對話，經編輯：&lt;/p&gt; 
&lt;h2&gt;&lt;span style=&quot;color:#27ae60&quot;&gt;&lt;strong&gt;DeepSeek 不是國運級產品，年輕人才是&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;《智能湧現》：開源中國現在是第二大代碼託管平台，國內最大的開源社區。DeepSeek 的熱潮，對你們的直觀影響，是從什麼時候開始的？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;馬越&lt;/strong&gt;&lt;/span&gt;：就是他在 App Store 登頂那會兒。先是 V3，然後是 R1 發佈，一下子就火起來了。我們春節一直在加班，讓 DeepSeek 首先能在中國生產的 GPU 上運行，這需要大量工作，我們是第一個在沐曦芯片上部署的。&lt;/p&gt; 
&lt;p&gt;我們都在調侃，春節就兩件事：DeepSeek、哪吒。DeepSeek 就是開源圈出了個哪吒。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能湧現》：現在已經形成了一種論調：DeepSeek 是一個國運級的產品。但最近你的公開表達裏，似乎對這一點不太認同。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;馬越&lt;/strong&gt;&lt;/span&gt;：首先，大模型這個事情，你離不開英偉達吧？其次，你離不開 Transformer 架構；第三，你採用了蒸餾的思路，這些都不是國內原創的。&lt;/p&gt; 
&lt;p&gt;DeepSeek 本質上是在現有路線上走得最好，實現了彎道超車，這是值得尊敬的。&lt;/p&gt; 
&lt;p&gt;但是 DeepSeek 能夠不依靠外部資金支持，也不做任何 PR，靠技術就能做到全球頂尖——以梁文鋒為代表的年輕人崛起，這才是國運級的現象。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能湧現》：那什麼才算國運級的產品？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;馬越&lt;/strong&gt;&lt;/span&gt;：完全原創的技術創新。誰説 Transformer 就是算法的終局？如果有人用非 Transformer 方案做出比 DeepSeek 強十倍的成果，那才是真正的突破，那是人類級的進步。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能湧現》：DeepSeek 給開源生態最大的啓示會是什麼？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;馬越&lt;/strong&gt;&lt;/span&gt;：讓全社會認識到：開源等於創新。&lt;/p&gt; 
&lt;p&gt;DeepSeek 最令人唏噓的是，在國內兩年都默默無聞，也不如打廣告的很多大模型公司，直到 2024 年開始，才因為技術，因為開源，被美國人超級關注——雖然一部分人特別支持，一部分人極力貶低，這種關注反而倒逼着國內形成了一種愛國情懷。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能湧現》：以前大家不相信這個觀點嗎？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;馬越&lt;/strong&gt;&lt;/span&gt;：以前很多人認為開源就是一幫草根、烏合之眾，很難和大廠這種正規軍相比。&lt;/p&gt; 
&lt;p&gt;其實二十年前我就在説這些話：開源約等於創新能力，創新能力和國力是映射關係。正是因為我們有錢了、富足了，才會有 DeepSeek 這樣的企業出現。&lt;/p&gt; 
&lt;p&gt;以前沒人聽，現在有人聽了。&lt;/p&gt; 
&lt;p&gt;第二點很重要，就是要對年輕人保持敬畏。不只是尊重，而是要怕年輕人，信任年輕人。每一代人都有自己的時代使命，也有時代侷限性。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能湧現》：這羣年輕人，或者新一代開源貢獻者，為什麼能成長起來？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;馬越&lt;/strong&gt;&lt;/span&gt;：這種質變是建立在之前充分的量變基礎上的。&lt;/p&gt; 
&lt;p&gt;這十年要感謝走在前面的互聯網大廠，事實上國內的主要開源力量集中在這些有實力的企業上。包括百度、阿里、騰訊等組織的開源項目，還有華為的鴻蒙、歐拉等等。他們都是領着工資的員工，在搞這些開源工作，不是純粹基於興趣。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能湧現》：DeepSeek 證明一件最關鍵的事：通過底層技術突破，就能吸引大量用户，以及贏得尊重。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;馬越&lt;/strong&gt;&lt;/span&gt;：現在我們國家最應該做兩件事：一是牽頭一起開發中國的 CUDA；二是讓所有國產 GPU 都能快速支持這些模型。&lt;/p&gt; 
&lt;p&gt;説到生態，生態就是要有更多的人蔘與，而且大家都有高度共識。現在最大的問題不是芯片卡脖子，而是 CUDA 這個生態的制約。中國完全可以開發一套類似 CUDA 的系統，就像我們有自己的 GPU 一樣。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;&lt;span style=&quot;color:#27ae60&quot;&gt;&lt;strong&gt;開源是富人和強者的遊戲&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;《智能湧現》：DeepSeek 爆火之後，找你討論的人多嗎？大家最關心什麼話題？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;馬越&lt;/strong&gt;&lt;/span&gt;：有人問我，DeepSeek 會不會給中國 To B 市場帶來新生機？不可能那麼快。&lt;/p&gt; 
&lt;p&gt;IT 外包的人天價格，20 年來的漲幅還不如按摩師。現在外包人天均價一千就算高的了，還有五六百的。你去按摩，現在一小時都要一兩百塊錢。十年前，IT 的外包時薪就比不上按摩了，現在差距更大，那是因為按摩價格漲得快。&lt;/p&gt; 
&lt;p&gt;中國軟件沒人願意花錢，這是行業發展還不行的核心原因。要等這一代年輕人變成決策者，好時代才會來。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能湧現》：本質還是因為國內企業發展階段還比較早。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;馬越&lt;/strong&gt;&lt;/span&gt;：開源本質上是強者和富人的遊戲。正是因為我們吃飽喝足了，才會有 DeepSeek 這樣的企業出現。上一代互聯網用户普遍不願意為軟件和知識付費，騰訊會議掉線了就重連，也不願意買會員。&lt;/p&gt; 
&lt;p&gt;但這一代年輕人生活富足，你們會改變這個局面。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能湧現》：DeepSeek 會給上一代 To B 創業者帶來什麼啓示嗎？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;馬越&lt;/strong&gt;&lt;/span&gt;：我覺得他們給創業者帶來兩個重要的啓示。第一是要對錢保持敬畏。創業的目的就是為了掙錢，談理想和情懷沒意義。&lt;/p&gt; 
&lt;p&gt;DeepSeek 不太需要考慮商業化的問題，是因為幻方已經解決了這個事情。&lt;/p&gt; 
&lt;p&gt;上一代的軟件創業者有個致命問題，一心想着燒錢，通過標準化產品打市場，這不是中國市場的運行邏輯，中國最有錢的金主都是大型企業，在中國想要賺錢，不做定製化是不現實的。&lt;/p&gt; 
&lt;p&gt;中國軟件行業是城市包圍農村，而美國是農村包圍城市，腰部企業數量很多。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能湧現》：DeepSeek 會改變大家對商業化的看法嗎？開源怎麼考慮商業化，是這個領域的「天問」。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;馬越&lt;/strong&gt;&lt;/span&gt;：如果要開源做創業項目，技術必須得硬邦邦。就是和 DeepSeek 一樣，Day 1 就出海，否則在中國太難賺錢了，時代還不夠成熟。&lt;/p&gt; 
&lt;p&gt;大家總是會舉例，比如紅帽那套模式也能商業化，但是想用這種方式在中國做一個上市公司，還不是這個時代的事。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能湧現》：你們自己也經歷了很長一段商業化探索的時期，是從什麼時候想明白要怎麼做的？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;馬越&lt;/strong&gt;&lt;/span&gt;：2020 年是個重要轉折點。我們那年決定從百度獨立出來，重新謀求 IPO。那段時間因為美國開始在很多尖端技術上斷供，我們想抓住這個機會，真正成為一個獨立的開源平台。&lt;/p&gt; 
&lt;p&gt;想要做真正的本土開源平台，必須要是徹底中立的第三方，這是選擇重新獨立發展的核心原因。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能湧現》：想明白之後，都做了什麼？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;馬越&lt;/strong&gt;&lt;/span&gt;：我們現在從社區發展出了三大產品線。&lt;/p&gt; 
&lt;p&gt;開源中國社區（OSChina）現在已經完全進化成一個 AI 教育平台。我們是中國最大的開源社區，有 1000 多萬用户。現在我們 24 人的團隊能創造約 5000 萬收入，還有淨利潤，這在社區團隊中很少見。&lt;/p&gt; 
&lt;p&gt;第二塊是代碼託管和研發效能平台 Gitee，現在平台有 3600 萬個代碼倉庫，服務 36 萬家企業。主要提供代碼託管私有化倉庫服務，確保很多中小團隊的代碼安全，客單每年 3000 塊左右。&lt;/p&gt; 
&lt;p&gt;從 2020 年到現在，我們已經能夠提供 DevOps 全生命週期國產替代方案，在滿足開發者需求的同時，也建立起一個自主創新、安全可信的本土開源軟件工具與生態。&lt;/p&gt; 
&lt;p&gt;第三塊是 AI 大模型平台「模力方舟」，模型體驗、推理訓練到應用部署等等服務，都會提供。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能湧現》：為什麼會從社區拓展到後來的 DevOps，以及 AI 大模型基座？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;馬越&lt;/strong&gt;&lt;/span&gt;：一個開源公司想要成功，靠社區是不夠的，我們需要找一個閉環的商業模式，像 GitHub 那條路——社區、代碼託管是沒法達到這個目標的。GitHub 也是在大模型浪潮來了之後，推出 Copilot，才把營收做起來。&lt;/p&gt; 
&lt;p&gt;以後沒有淨利潤的公司很難在國內上市，所以我一直強調看毛利率和人效，這兩個指標高了，自然會有淨利潤。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能湧現》：你們現在的主要收入，來自哪裏？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;馬越&lt;/strong&gt;&lt;/span&gt;：我們主要收入來自 100 家左右的銀行、券商、軍工、製造業客户，都走大型私有部署形式。中小客户主要靠 SaaS 服務。&lt;/p&gt; 
&lt;p&gt;2024 年我們全國訂單超過 2 億，是一個突破。前年過 1 億，2024 年翻了一倍，還實現了盈虧平衡，這很不容易。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能湧現》：主要模式靠服務大型企業的話，怎麼避免走到項目制的老路？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;馬越&lt;/strong&gt;&lt;/span&gt;：我們的產品做得很複雜，是因為中國的大型企業的場景複雜。我們的流程引擎、角色引擎、交互界面、流水線都是可定製的，還能做各種插件，就是為了保證靈活性。&lt;/p&gt; 
&lt;p&gt;我們會幫客户做定製化配置，但是不做二次開發。我們現在 330 多人，這塊業務佔 200 多人，但定製化去做開發和交付的不到 10%。&lt;/p&gt; 
&lt;p&gt;第二是我們自己堅決不賣算力，只做第三方，比如給雲廠商導流。&lt;/p&gt; 
&lt;p&gt;我們現在的路線很清晰：前端社區承載大流量，做開發者工具賣給企業，先 To C，再 To B，也算是一種產品驅動增長（Product-Driven Growth）的模式。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;&lt;span style=&quot;color:#27ae60&quot;&gt;&lt;strong&gt;一起發展，比單打獨鬥強&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能湧現》：是否選擇開源，企業的考量到底是什麼？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;馬越&lt;/strong&gt;&lt;/span&gt;：現在大模型不開源很難。蘋果為什麼到今天 iOS 都不開源？因為硬件生態已經形成壟斷。如果沒有類似這樣的護城河，你不開源，憑什麼在市場立足？&lt;/p&gt; 
&lt;p&gt;就像我十幾年來一直説的，開源是創新的最佳方法論，也是市場競爭的方法論，是反強權的方法論。你做得好，我們就開源來和你競爭。當年有 Unix 和 Windows，就有 Linux；有 iOS，後來就有 Android，都一樣。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能湧現》：你做過很多併購，DeepSeek 的成功會改變投資人對開源項目的看法嗎？開源項目的出路會變得寬嗎？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;馬越&lt;/strong&gt;&lt;/span&gt;：這也是我想問所有投資人和創業者的問題：投資的目的是啥？到底你希望怎麼賺錢？&lt;/p&gt; 
&lt;p&gt;上市、被收購、分紅都是一種退出方式。但現在在國內，要麼 IPO，要麼死掉，這很殘酷。&lt;/p&gt; 
&lt;p&gt;中國的開源生態很分散，現在很多創業者缺乏一種共識，就是一起發展比單打獨鬥強。很多人把創業當作獲取情緒價值的方式，就想當老大，寧可公司死也不願意賣給別人。覺得賣了就是投降，這坎兒過不去。&lt;/p&gt; 
&lt;p&gt;如果放不下自己的 ego，最終就會害了自己，也害了客户和投資人。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能湧現》：DeepSeek 大獲成功之後，你怎麼評估現在我們所處的 AI 發展階段？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;馬越&lt;/strong&gt;&lt;/span&gt;：如果類比互聯網那個時代，我們還是在大時代的開端狀態，類似當年的撥號上網階段。我從 1997 年開始上網，下載一張照片要四五天，網速只有 28K。但即便如此，我們也覺得很神奇。&lt;/p&gt; 
&lt;p&gt;現在就像出海探索新大陸，所以創業者只要帶着乾糧上了船，不淹死，就一定有收穫。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能湧現》：DeepSeek 會怎麼改變現在國內的創業格局？你覺得更利好大廠還是創業公司？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;馬越&lt;/strong&gt;&lt;/span&gt;：很難説，可能還是大廠比較有優勢。&lt;/p&gt; 
&lt;p&gt;首先，DeepSeek 不是一個創業公司，人家不用外部資金就能買一萬張卡，某種程度上也算個小大廠了。&lt;/p&gt; 
&lt;p&gt;我覺得 DeepSeek 給創業者帶來兩個重要的啓示。第一是要對錢保持敬畏。創業的目的就是為了掙錢，談理想和情懷沒意義。&lt;/p&gt; 
&lt;p&gt;初創公司除非在算法、技術底層有突破，否則在工程層面，很難跟大廠拼數據，拼流量，這是最終商業化的兩個要素。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能湧現》：現在的大模型初創的轉向都很明顯，方向聚焦，專心做底層技術。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;馬越&lt;/strong&gt;&lt;/span&gt;：這就是開源的可怕之處。&lt;/p&gt; 
&lt;p&gt;我前年就説，預訓練是大廠的遊戲，創業公司應該做垂直領域的訓練，把更多精力放在推理上，燒錢的事情本來就不該做。&lt;/p&gt; 
&lt;p&gt;歷史上都有很多例子，當年開源領域有很多做容器的公司，比如 Docker 剛出來時只是各種容器運行時技術中的一種。結果 K8s 生態起來之後，任何容器技術只要實現 K8s 兼容性，就可以融入雲原生技術棧，這種強大的生態整合能力最終使其它技術方案逐漸邊緣化，相當於前邊都白做了。&lt;/p&gt; 
&lt;p&gt;所以我給大家的建議，包括我們自己的策略，就是產品功能要緊跟隨，但要輕投入，商業模式要做輕一點。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能湧現》：對開源中國來説，未來的目標會是什麼？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;馬越&lt;/strong&gt;&lt;/span&gt;：開源中國這十幾年，積累了用户流量護城河，客户品牌美譽度，現在是通過信創找到了快速增長的收入模式。&lt;/p&gt; 
&lt;p&gt;我們在這輪融資之後，也會開始尋求進一步上市，希望成為 A 股人工智能開源第一股。&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;&lt;em&gt;原文：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FEY_NhbMX94bk6op8GDuG6g&quot; target=&quot;_blank&quot;&gt;《對話開源中國馬越：DeepSeek 不是國運級的創新，年輕人才是》&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/337320</link>
            <guid isPermaLink="false">https://www.oschina.net/news/337320</guid>
            <pubDate>Thu, 06 Mar 2025 11:52:45 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>支持函數導入/導出，新增支持變量賦值節點，MaxKB 知識庫問答系統 v1.10.2 LTS 版本發佈</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p style=&quot;color:#000000; text-align:start&quot;&gt;2025 年 3 月 6 日，MaxKB 開源知識庫問答系統正式發佈 v1.10.2 LTS 版本。&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;在 MaxKB v1.10.2 LTS 版本中，&lt;strong&gt;函數庫&lt;/strong&gt;方面，MaxKB 支持函數的導入/導出；&lt;strong&gt;應用&lt;/strong&gt;方面，新增支持「變量賦值」節點；&lt;strong&gt;模型管理&lt;/strong&gt;方面，MaxKB 新增支持 Ollama 供應商的重排模型。&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;strong&gt;X-Pack 增強包&lt;/strong&gt;方面，MaxKB 應用接入功能支持 Slack。目前，MaxKB 支持對接的第三方應用包括企業微信、公眾號、飛書、釘釘以及最新的 Slack。&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;此外，MaxKB 開源項目組還進行了超過 40 項功能更新和問題修復。&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;感謝廣大社區用户的反饋和支持，MaxKB 期待與您攜手創造更加美好的未來。&lt;/p&gt; 
&lt;h1&gt;亮點更新&lt;/h1&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;strong&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■ 支持函數導入/導出&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;在 v1.10.2 LTS 版本中，MaxKB 新增函數的導入/導出功能，從而實現了函數模塊在不同環境之間的無縫遷移。這一功能進一步方便了用户的函數共享過程，提升了系統的靈活性與實用性。&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-4f196ce183a6e9256792ceea1be65b89695.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;▲圖 1 MaxKB 支持函數導入/導出&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;strong&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■ 新增支持「變量賦值」節點&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;在 v1.10.2 LTS 版本中，MaxKB 新增支持「變量賦值」節點。該節點為用户提供更為便捷的方式來更新工作流編排中的變量值，能夠顯著提升用户在配置和管理流程時的靈活性與效率。&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;626&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-1dfcf722f328519c2abb120eb726d5c46f1.jpg&quot; width=&quot;1280&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;▲圖 2 MaxKB 新增支持「變量賦值」節點&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;strong&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■ 新增支持 Ollama 供應商的重排模型&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;在 v1.10.2 LTS 版本中，MaxKB 新增支持 Ollama 供應商的重排模型。目前 MaxKB 已經支持 Ollama 供應商提供的大語言模型、向量模型、視覺模型和重排模型，為用户提供了豐富的模型選擇。&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-2ad23e7ada247c9dfdcf55de68ffc8212e6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;▲圖 3 MaxKB 支持 Ollama 供應商的重排模型&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;strong&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■ 應用接入支持 Slack（X-Pack 增強包）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;在 v1.10.2 LTS 專業版中，MaxKB 的應用接入功能新增支持接入 Slack。目前，MaxKB 支持對接的第三方應用包括企業微信、公眾號、飛書、釘釘以及最新的 Slack，幫助企業將大模型能力快速注入原有業務系統，加速 AI 賦能業務的進程。&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;637&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-c346e08537ec1f877de06169eb13f652e12.jpg&quot; width=&quot;1280&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;▲圖 4 MaxKB 應用接入 Slack 配置頁面&lt;/span&gt;&lt;/p&gt; 
&lt;h1&gt;功能優化&lt;/h1&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;應用（X-Pack）：開啓思考過程後，優化在企業微信、飛書、釘釘、公眾號中的回覆過程；&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;應用（X-Pack）：企業微信對話時支持上傳圖片；&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;應用（X-Pack）：登錄認證的 OIDC 設置支持配置&lt;em&gt;scope&lt;/em&gt;參數；&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;應用：支持創建空白應用；&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;應用：用户輸入的參數新增支持密碼框和開關組件；&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;應用：用户輸入支持自定義標題；&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;應用：表單收集節點的參數新增支持密碼框組件；&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;應用：上傳音頻文件類型新增 m4a 格式；&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;應用：通過調用應用 API Key 的方式進行對話時，支持上傳文件參數；&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;應用：通過調用應用 API Key 的方式進行對話時，支持輸出思考過程參數；&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;應用：基礎信息節點中的用户輸入表格中的參數，支持拖拽式調整順序；&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;應用：判斷器中的條件值支持變量解析；&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;應用：在多路召回節點的「執行詳情」對話框中，優化分段顯示內容（包含分段標題、文檔和知識庫）；&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;應用：當用户退出工作流編輯時提示用户保存數據；&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;應用：關聯知識庫引用分段數的最大值調整為 10000；&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;應用：高級編排中修改節點名稱的操作修改為「…」→「重命名」；&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;應用：單一圖片生成節點生成多張圖片時橫向排列圖片；&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;問答頁面：優化問答頁面佈局為左右佈局，左側顯示 AI 回答，右側顯示用户問題；&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;問答頁面：優化用户打開問答頁面時，顯示歷史對話記錄；&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;問答頁面：優化語音播放時僅播放最後一個內容；&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;問答頁面：支持修改會話標題；&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;知識庫：支持執行生成問題失敗的分段繼續生成問題；&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;知識庫：支持為文檔列表排序；&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;知識庫：支持在生成問題中使用&lt;em&gt;{title}&lt;/em&gt;變量獲取分段標題；&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;函數庫：查詢函數時忽略大小寫；&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;模型設置：查詢模型時忽略大小寫；&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;團隊成員：查詢成員時忽略大小寫；&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;系統：優化第一次登錄時耗時較長的問題。&lt;/p&gt; 
&lt;h1&gt;問題修復&lt;/h1&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;應用：修復 AI 回覆內容中的圖片無法放大的問題；&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;應用：修復高級編排中的提示詞窗口放大後編輯內容，按 ESC 鍵關閉窗口後不保存提示詞的問題；&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;應用：修復使用 vLLM 供應商大語言模型進行對話時，部分情況下回答無法結束的問題；&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;應用：修復使用 Kimi 供應商的大語言模型進行對話時，Tokens 計算不準確的問題；&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;應用：修復圖片生成節點切換模型後參數設置未更新的問題；&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;應用：修復當 Excel 表格中含有合併單元格的數據時，讀取時會缺少數據的問題；&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;應用：修復 OpenAI 調用格式沒有思考過程參數的問題；&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;應用：修復應用子節點中若有非必填參數，在父級應用中若未設置該參數，對話時會報錯的問題；&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;應用：修復 Ollama 供應商的大語言模型使用&lt;/span&gt;&lt;em&gt;&lt;span&gt;num_ctx&lt;/span&gt;&lt;/em&gt;&lt;span&gt;參數時，進行對話會報錯的問題。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;問答頁面：修復在歷史對話記錄中，無法使用瀏覽器進行語音播放的問題；&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;問答頁面：修復問題框中無法在內容中間插入換行的問題；&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;問答頁面：修復部分情況下會在文檔的 URL 地址後面自動加上「/」，導致無法訪問的問題；&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;問答頁面（X-Pack）：修復顯示設置中關閉歷史記錄後，問答頁面的新建對話不顯示的問題；&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;函數庫：修復函數返回值為 0 時，調試時的輸出結果顯示錯誤的問題；&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;模型設置：修復添加阿里雲百鍊的大語言模型時，如果是全模態模型提交會報錯的問題；&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;模型設置：修復添加 Azure OpenAI 的 DeepSeek-R1 模型報錯的問題；&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;系統設置（X-Pack）：修復 Swagger 文檔中接口參數顯示不全的問題；&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;系統設置（X-Pack）：修復在「外觀設置」中設置「網站名稱」後，問答頁面的標籤處不顯示應用名稱的問題；&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#5a55fa&quot;&gt;■&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;系統設置（X-Pack）：修復登錄頁面加載時，默認 Logo 會閃現的問題。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/337319/java-maxkb-1-10-2-lts-released</link>
            <guid isPermaLink="false">https://www.oschina.net/news/337319/java-maxkb-1-10-2-lts-released</guid>
            <pubDate>Thu, 06 Mar 2025 11:50:14 GMT</pubDate>
            <author>來源: 投稿</author>
        </item>
        <item>
            <title>開源中國完成數億元 C 輪融資，領航 AI 新紀元</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#27ae60&quot;&gt;&lt;strong&gt;開源技術生態領軍企業開源中國&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;（開源共識（上海）網絡技術有限公司）近日完成了數億元 C 輪融資&lt;/strong&gt;，本輪融資由北京信息產業發展投資基金（北京信產基金）領投，深報一本股權投資基金（深報一本）及北京上河動量私募股權基金（上河動量）跟投。&lt;/p&gt; 
&lt;p&gt;此次融資將加速公司 AI 戰略佈局：深化現有產品矩陣的擴展、完善與全面 AI 化，構建軟硬件協同的智能解決方案體系，促進人工智能在產業領域的 AI 應用落地。&lt;/p&gt; 
&lt;p&gt;至此，開源中國已累計獲得超 16 億元戰略投資，投資方包括百度、華為、海望資本、張江科投、中科創星、天際資本、君聯資本、上海國際創投、中移和創投資、瑞壹投資、容億資本、泰達實業、中國互聯網投資基金、國調科改、聯想創投、上海浦東軟件園、上海科創、北京信產基金、深報一本、上河動量等。構建起國有資本、科技大廠、創始團隊&quot;3:3:4&quot;的良性股權結構，形成&quot;國家隊護航、產業方協同、市場化運作&quot;的創新生態。&lt;/p&gt; 
&lt;h3&gt;&lt;span style=&quot;color:#27ae60&quot;&gt;&lt;strong&gt;開發者生態的厚積薄發&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;p&gt;作為中國開源基礎設施奠基者，開源中國運營着 1800 萬開發者聚集的 oschina.net 社區及代碼託管平台 Gitee，服務 36 萬企業級用户。其自主研發的 DevOps 工具鏈已在金融、軍工等關鍵領域實現 80% 市場滲透率，成為信創替代工程的標杆案例，驗證了開源商業化的中國路徑。&lt;/p&gt; 
&lt;h3&gt;&lt;span style=&quot;color:#27ae60&quot;&gt;&lt;strong&gt;AI 轉型的戰略升維&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;2024 年，公司推出對標 HuggingFace 的 AI 大模型平台&quot;模力方舟 (moark.com)&quot;，首創&quot;模型數據-算力調度-應用開發&quot;全棧服務體系。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;平台已實現三大突破：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;生態開放化&lt;/strong&gt;：聚合數千開源模型，打造 AI 應用創新基座；&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;服務一體化&lt;/strong&gt;：提供從模型體驗、推理訓練到應用部署的全生命週期服務；&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;算力國產化&lt;/strong&gt;：完成多家國產 GPU 深度適配，成功運行 DeepSeek-V3 等千億級模型。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;開源中國下一步將以模力方舟為核心，打造全方位的 AI 業務佈局，助力 AI 應用創新、科技人才培養和新質生產力提升。&lt;/p&gt; 
&lt;h3&gt;&lt;span style=&quot;color:#27ae60&quot;&gt;&lt;strong&gt;起航，邁向「開源 AI 第一股」&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;p&gt;開源中國以開發者生態為基座，構建開源領域‌「用户-流量-盈利」‌三重護城河，率先在信創市場完成‌開源商業化閉環驗證‌，實現國產研發工具從技術突破到商業變現的質變。依託本輪戰略投資，加速 AI 戰略升級擴張市場領域，開闢第二增長曲線‌。&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;目前開源中國已開始進入 IPO 倒計時，計劃以&quot;開源 AI 第一股&quot;身份登陸資本市場，通過技術普惠推動新質生產力發展，助力中國在全球 AI 2.0 時代構建核心競爭力。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;北京信息產業發展投資基金表示：&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;開源中國是中國開源生態與 AI 技術融合創新的標杆企業，其以開發者生態為根基、以信創替代為突破、以 AI 戰略為驅動的增長路徑，高度契合國家科技創新與自主可控的戰略方向。領投本輪融資，既是基於對開源中國在國產軟件基礎設施領域不可替代地位的認可，更是看好其通過「模力方舟」平台推動 AI 技術普惠化、算力國產化和應用場景規模化落地的能力。&lt;/p&gt; 
 &lt;p&gt;我們期待通過資源協同與生態賦能，助力開源中國加速構建 AI 時代的技術底座，為全球 AI 2.0 競爭注入中國開源力量。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;深報一本表示：&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;我們高度關注開源中國在國產軟件替代浪潮中展現的商業化前景，憑藉其構建的龐大開發者生態體系，公司有望在 AI 應用層持續釋放開源技術的創新勢能。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;上河動量管理合夥人王欣表示：&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;世界範圍內，開源已經成為軟件開發和人工智能創新的極其重要的推動力量。&lt;/p&gt; 
 &lt;p&gt;開源中國是服務於中國本土開源生態的先行者和堅守者，在地緣科技競爭的背景下，開源中國已經成為中國軟件和人工智能領域具有國家級影響力的科技創新基礎設施。相信開源中國的獨特價值會得到越來越多的行業參與者和資本市場的認可。&lt;/p&gt; 
&lt;/blockquote&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/337301/oschina-series-c-funding-round</link>
            <guid isPermaLink="false">https://www.oschina.net/news/337301/oschina-series-c-funding-round</guid>
            <pubDate>Thu, 06 Mar 2025 10:11:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>個人開發者也能訓練推理模型？GRPO 技術詳解</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                    
                                                                                                                                                    &lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;編者按：&lt;/strong&gt; 還在為訓練推理模型燒光算力預算而發愁？當開源小模型遇上數學題就&quot;智商掉線&quot;，如何低成本突破性能瓶頸？&lt;/p&gt; 
 &lt;p&gt;傳統 RLHF 動輒百萬級算力投入，讓多少團隊在強化學習門前望而卻步；格式混亂、邏輯斷層、答案偏差------這些模型推理的頑疾是否也在阻礙你的 AI 產品落地？&lt;/p&gt; 
 &lt;p&gt;本文深入解析 DeepSeek 團隊突破性的 GRPO（羣組相對策略優化）技術，這項創新將強化學習所需計算資源幾乎減半，甚至可以結合 LoRA 在普通消費級 GPU 上進行模型訓練。作者通過親身實踐，成功在僅需 16GB 顯存的環境下將 1B 參數的 Llama 3.2 轉化為推理模型（後續文章會分享相關細節），完全顛覆了傳統強化學習的資源需求認知。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;作者 | Greg Schoeninger&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;編譯 | 嶽揚&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-71410d1cd55685fc17beb84605c865f5851.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;不久前，我們深入研究了 DeepSeek-R1 背後的技術原理，但是沒有詳細介紹其訓練流程中採用的一項名為&quot;羣組相對策略優化&quot;（Group Relative Policy Optimization, GRPO）的關鍵技術。&lt;/p&gt; 
&lt;p&gt;GRPO 本質上是一種旨在提升模型推理能力的強化學習算法。該技術最早發表於其研究論文《DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models》[1]，隨後也被應用於 DeepSeek-R1 的後訓練階段。&lt;/p&gt; 
&lt;p&gt;在《DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning》這一論文[2]中，研究團隊詳細闡述了從基礎預訓練語言模型到最終推理模型的完整構建路徑。雖然之前我們未深入探討 GRPO 的數學原理和代碼實現，但今天這篇文章將全面解析 GRPO 的技術細節，助力各位讀者掌握這項技術的核心要義並應用於實際工作。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;01 要點回顧：DeepSeek-R1 如何運用 GRPO 技術&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;為幫助理解，我們首先梳理從基礎模型到推理模型的完整訓練流程。該流程通過監督式微調（SFT）與羣組相對策略優化（GRPO）的交替迭代實現模型能力躍升：&lt;/p&gt; 
&lt;p&gt;1.監督式微調（SFT）階段&lt;/p&gt; 
&lt;p&gt;a.&lt;strong&gt;冷啓動訓練&lt;/strong&gt;：採用數千條人工標註的高質量數據微調模型&lt;/p&gt; 
&lt;p&gt;b.&lt;strong&gt;數據驗證&lt;/strong&gt;：所有樣本均通過人工審核確保可靠性&lt;/p&gt; 
&lt;p&gt;2.GRPO 強化學習階段&lt;/p&gt; 
&lt;p&gt;a.&lt;strong&gt;推理軌跡訓練&lt;/strong&gt; ：引導模型生成結構化推理過程（具有標籤的推理軌跡）&lt;/p&gt; 
&lt;p&gt;b.&lt;strong&gt;三重確定性獎勵&lt;/strong&gt;：基於格式規範性、邏輯一致性、答案正確性設計獎勵機制&lt;/p&gt; 
&lt;p&gt;3.增強型 SFT 階段&lt;/p&gt; 
&lt;p&gt;a.&lt;strong&gt;合成數據生成&lt;/strong&gt;：創建 80 萬條合成訓練樣本並進行篩選&lt;/p&gt; 
&lt;p&gt;b.&lt;strong&gt;模型自檢過濾&lt;/strong&gt;：通過&quot;LLM As A Judge&quot;機制剔除錯誤響應&lt;/p&gt; 
&lt;p&gt;4.最終 GRPO 對齊階段&lt;/p&gt; 
&lt;p&gt;a.&lt;strong&gt;價值觀校準&lt;/strong&gt;：確保模型輸出兼具實用性與安全性&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-a1a3d34d2c90323e9660a599d6baf9ba905.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;在這篇文章中，我們將深入探討 GRPO 的細節，助您掌握這項推動大模型推理能力突破的關鍵技術。筆者已開展基於 GRPO 的小模型訓練實驗，後續將發佈完整代碼與工程實踐細節，通過可復現案例串聯理論知識與實際應用。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;02 為什麼 GRPO 很重要？&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;TLDR ~ 大幅降低了計算需求且簡化了強化學習流程。與 ChatGPT（PPO）使用的基於人類反饋的強化學習（RLHF）相比，所需的計算資源幾乎減半。當你結合 LoRA 使用時，即使&quot;GPU poor&quot;（譯者注：GPU 的性能不足）也能進行強化學習訓練。我試過了，確實有效。我成功地將 1B 參數的 Llama 3.2 模型改造成了僅需 16GB 顯存的推理模型。後續文章會分享代碼和硬件要求細節。&lt;/p&gt; 
&lt;p&gt;我們只需在雲 GPU 服務上花不到 100 美元，就能從自家車庫訓練推理模型。如果用自己的硬件跑小模型，基本上算是&quot;免費&quot;。其底層原理是什麼呢？下一節將討論從 PPO 到 GRPO 的演變過程。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;03 從 PPO 到 GRPO&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;傳聞 ChatGPT 背後的強化學習（RL）技術是 PPO（Proximal Policy Optimization，近端策略優化）。該流程在 InstructGPT 論文[3]中被提出，用於創建能夠遵循指令而不僅僅是簡單預測下一個單詞的模型。&lt;/p&gt; 
&lt;p&gt;訓練過程需要收集大量標註數據。對於給定的用户查詢，模型需生成多個候選響應，然後由人類或 AI 在循環中對輸出進行標註並按質量從優到劣排序。這些數據可用於訓練&quot;獎勵模型&quot;，其職責是為新接收的提示詞計算&quot;獎勵值&quot;。該獎勵值應體現給定用户查詢下模型響應的優劣程度。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-7e2d394a6e114e400146d01db6dc67530d8.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;收集完所有這些經過排序和標註的數據後，即可啓動 PPO 來訓練大語言模型（LLM）。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;問題在於 PPO 的訓練成本可能非常高昂。&lt;/strong&gt; GRPO 論文[1]中的相關圖表展示了 PPO 和 GRPO 過程中涉及的不同 LLM。下方藍色和黃色方框中共有 4 個不同的 LLM。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-89e516de6f1f5de6d7805fc499a831b28e8.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;為了幫助大家理解上圖的一些術語，我在這裏給出了一些簡單的定義：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;策略模型（Policy Model）&lt;/strong&gt; - 對當前正在訓練的 LLM 的別稱&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;參考模型（Reference Model）&lt;/strong&gt; - 被訓練原始 LLM 的凍結版本&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;獎勵模型（Reward Model）&lt;/strong&gt; - 基於人類偏好訓練的模型（來自上文提到的 InstructGPT 技術）&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;價值模型（Value Model）&lt;/strong&gt; - 試圖估算特定動作長期獎勵的模型&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h1&gt;&lt;strong&gt;04 通過 GRPO 減少內存使用量&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;在 PPO 算法中，策略模型和價值模型都包含需要通過反向傳播進行優化的可訓練參數。反向傳播過程需要消耗大量內存資源。&lt;/strong&gt; 從上面的架構圖可以看出，GRPO 算法移除了價值模型模塊。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-909a6a3cd9958e6b2b842610e7167f00efc.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;PPO 算法中混合使用了 4 個大語言模型（LLMs），這些模型都需要消耗大量的內存和計算資源。其中價值模型和獎勵模型的參數量通常與正在訓練的目標語言模型相當。參考模型通常是訓練初期的語言模型的凍結副本。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-73a54d4ed060595c351d14255a2a026dcc4.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;這種實現方法不僅帶來高昂的計算成本，還存在諸多需要協調的動態組件，而且還有多個模型需要優化。組件數量越多，通常意味着優化難度越大。GRPO 通過精簡架構有效降低了系統複雜度。&lt;/p&gt; 
&lt;p&gt;出於興趣，我在 H100 上測試了不同參數規模的模型，觀察使用 GRPO 進行微調的難易程度。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-f30692cc7db7719142a8d0c6d7bf4b34cda.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;如果想了解具體技術細節，可以查閲相關文檔：&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.oxen.ai%2Fblog%2Fgrpo-vram-requirements-for-the-gpu-poor&quot; target=&quot;_blank&quot;&gt;https://www.oxen.ai/blog/grpo-vram-requirements-for-the-gpu-poor&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;若您理解了所有系統需求的來源，就可以開始參與開源項目貢獻，或像我最近看到的 trl 倉庫的這個 PR 那樣，動手優化自己的機器學習庫：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-46b6550e48276645bc3fb9fe2f1531ef4a2.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;05 羣組相對優勢（Group Relative Advantages）&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;在強化學習過程中，我們從語言模型（LLMs）中獲取的主要信號是代表&quot;優勢&quot;（Advantage）的&quot;A&quot;。這個信號為更新原始語言模型的權重提供了方向指導：&lt;strong&gt;當優勢值較高時，我們需要鼓勵模型重複當前行為；當優勢值較低時，則需要引導模型嘗試不同的行為。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在 PPO 算法中，傳統價值模型的核心任務是評估生成內容的質量，或者説預測這些內容獲得高獎勵值（high reward）的可能性。為了完成這項評估工作，需要訓練大語言模型作為價值判斷模塊。那麼 GRPO 是如何擺脱對價值模型的依賴的呢？&lt;/p&gt; 
&lt;p&gt;第一個技巧是：&lt;strong&gt;GRPO 不再針對單個查詢生成單一輸出，而是開始生成多個候選回答。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-e4651272806713abc0c71e0e9e4adb90f96.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;具體來説，如果問題是一道數學題，模型可能會嘗試幾種不同的解題方法。以下面這個數學問題為例：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Mr. Curtis has 325 chickens on his farm where 28 are roosters and the rest are hens. Twenty hens do not lay eggs while the rest of the hens do. How many egg-laying hens does Mr. Curtis have on his farm?&lt;/p&gt; 
 &lt;p&gt;Curtis 先生的農場有 325 只雞，其中 28 只是公雞，其餘是母雞。其中有 20 只母雞不下蛋，問有多少隻產蛋母雞？&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;模型可能會嘗試多種解題思路，有的正確（答案為 227），有的不正確（答案為 305）。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-4f8c57fb9d114d8cd19c1c434f42d31ba6e.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;正確推理路徑：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;First, let&#39;s find out how many hens there are. The total number of chickens is 325, and 28 are roosters. So, the number of hens is 325 - 28 = 297. Of these 297 hens, 20 do not lay eggs, so the number of egg-laying hens is 297 - 20 = 277.&lt;/p&gt; 
 &lt;p&gt;277&lt;/p&gt; 
 &lt;p&gt;首先，我們來看看有多少隻母雞。雞的總數是 325 只，公雞有 28 只。因此，母雞的數量是 325 - 28 = 297。在這 297 只母雞中，有 20 只不下蛋，所以下蛋母雞的數量是 297 - 20 = 277。&lt;/p&gt; 
 &lt;p&gt;277&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;錯誤推理路徑：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;You need to subtract the 20 hens that do not lay eggs from the total number of hens to find the number of egg-laying hens. So, the number of egg-laying hens is 325 - 20 = 305.&lt;/p&gt; 
 &lt;p&gt;305&lt;/p&gt; 
 &lt;p&gt;您需要從母雞總數中減去不下蛋的 20 只母雞，才能求出下蛋母雞的數量。因此，產蛋雞的數量為 325 - 20 = 305。&lt;/p&gt; 
 &lt;p&gt;305&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;然後我們對每個輸出根據其回答質量計算&quot;獎勵值&quot;（reward）。可能存在多個評估不同響應屬性的獎勵函數。我們暫時將獎勵函數視為黑盒，但知道它們會返回數值型結果------如果響應質量較好則數值較高，較差則較低，例如：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Formatting（格式規範度）=1.0&lt;/li&gt; 
 &lt;li&gt;Answer（答案正確性）=0.0&lt;/li&gt; 
 &lt;li&gt;Consistency（邏輯一致性）=0.5&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;當獲得所有輸出的獎勵值 (r) 後，GRPO 通過計算獎勵值的均值 μ 和標準差 σ，生成羣組相對優勢 A。具體公式為：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-64e1b0b82899716c405d7c373fad02574d9.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;這個公式在機器學習特徵工程中非常實用，它可以將任意數值歸一化為更易學習的正負信號。&lt;/strong&gt; &lt;strong&gt;其直觀含義是：&quot;這個數據點偏離平均值多少個標準差？&quot;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;讓我們來看幾個例子。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-b9a698704de3094e3837dd1ab2500a2a14a.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;若用原生 numpy 代碼表示可能如下：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-b22895474b1a709438951615cdb36e7923e.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-cba3c65046eea82fccdf170ae3480b63728.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;再試另一組數值：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-cc74f64ff94df742a05791fc54f6d5685c6.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;通過歸一化，將獎勵值轉換為以均值為中心（0.0）的相對優勢值。正值表示優於平均水平，負值表示劣於平均水平。這為我們建立了一套基準：&quot;給定當前提示詞，平均響應的質量如何？&quot;在訓練過程中，強化表現好的輸出（提高其概率），抑制表現差的輸出（降低其概率），從而引導模型優化方向。&lt;/p&gt; 
&lt;p&gt;這與傳統價值模型的目標相似：預測給定響應的獎勵值。由於我們現在訓練的是語言模型，只需調整 temperature 參數即可生成多個候選回答，所有生成回答的平均獎勵值即可作為衡量當前模型表現的良好信號，以及決定是否需要強化該行為。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;06 KL 散度&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;這個方程的最後一項是 KL 散度項。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-31fdc55549c04177692cffd66e30e71153b.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;無需深入數學細節，這就是我們在訓練過程中始終保留&quot;參考模型&quot;的原因。我們不希望新模型偏離原始模型太遠，對於每個詞元（token），都要確保新模型的預測結果不會與原始模型的預測結果產生過大偏差。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-c452d93871b84a20051519ae616d6a561c3.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;強制執行 KL 散度的直接原因是：初始模型已經具備生成連貫語句和遵循指令的能力。我們不希望新模型通過&quot;獎勵欺騙&quot;（reward hack）或利用獎勵信號中某些與原始模型不匹配的特性來取巧。&lt;strong&gt;例如，如果模型發現使用&quot;pamplemousse&quot;（葡萄柚的法語，發音有趣且較罕見）這個詞能獲得高獎勵，但該詞在預訓練階段並不常用，我們就要阻止模型過度依賴這種用詞行為。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;將這些要素整合，就得到了完整的最終方程！&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-9c0e6e909d24b741ba1137deb08f79e43bc.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;或者就像我們值得信賴的&quot;牛人 Eric&quot;説的那樣... 這個數學公式看起來比實際複雜...&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-856291aa6b9db5cbd46f3cbcf9d349927ab.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;07 獎勵信號機制&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;DeepSeek-R1-Zero 研究的突破性在於，他們通過完全棄用&quot;神經獎勵模型&quot;進一步大幅降低了內存消耗。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-6f1e8b8d0e5a2080f1c94fa211bdd425e7c.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;這意味着什麼？簡而言之，他們直接使用正則表達式（regex）和字符串匹配技術生成獎勵信號。&lt;strong&gt;研究團隊認為，這種方法既能規避&quot;獎勵欺騙&quot;（reward hacking）問題，又能簡化整個訓練流程。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;如果將前文提到的&quot;準確性獎勵（Accuracy Rewards）&quot;和&quot;格式獎勵（Format Rewards）&quot;規則轉化為代碼，其代碼實現可能如下所示：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-d0b618e57acda02c9b615ec705c4b7b9119.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;reference:&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgist.github.com%2Fwillccbb%2F4676755236bb08cab5f4e54a0475d6fb&quot; target=&quot;_blank&quot;&gt;https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;訓練過程中完全無需引入額外的獎勵模型 LLM，僅需保留策略模型和參考模型作為主要內存佔用源。將所需 LLM 數量從 4 個削減至 2 個，顯著降低了 GPU 資源需求。&lt;/p&gt; 
&lt;p&gt;若你的直覺此時感到不對勁，質疑&quot;這種獎勵函數是否具備泛化能力？&quot;，那麼你是對的。&lt;strong&gt;這類獎勵機制僅在預設的特定任務（如數學推理和格式規範）上表現良好，但無法擴展到其他實用場景。&lt;/strong&gt; 例如，模型可能擅長生成格式的數學解題過程，卻無法完成開放式對話或創意寫作。&lt;/p&gt; 
&lt;p&gt;我的預測是&quot;苦澀的教訓&quot;（The Bitter Lesson）[4]將在此重現：當計算資源和數據量足夠時，模型更傾向於自主學習。我們越是減少人工編碼規則，讓模型自主探索，其表現就越優異。當前 GRPO 的獎勵機制仍顯人工幹預痕跡 ------ 為何不讓模型自行學習獎勵信號的權重呢？&lt;/p&gt; 
&lt;p&gt;儘管如此，嘗試不同的獎勵機制其實挺有意思的。&lt;strong&gt;GRPO 的亮點在於：&lt;/strong&gt; &lt;strong&gt;只要能用代碼定義獎勵函數（輸入響應、輸出數值），即可基於此進行優化。甚至可以通過外部 API 調用其他 LLM 生成獎勵信號。&lt;/strong&gt; 我預感未來幾周/月內，因為 GRPO 訓練門檻的降低，開發者將開始探索各種創意獎勵機制的設計。&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Thanks for reading!&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Hope you have enjoyed and learned new things from this blog!&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;END&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;本期互動內容 🍻&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;❓對於文中提到的&quot;不到 100 美元訓練推理模型&quot;，你有何看法？歡迎在評論區暢所欲言。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;🔗文中鏈接🔗&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;[1]&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2402.03300&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2402.03300&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[2]&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2501.12948&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2501.12948&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[3]&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2203.02155&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2203.02155&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[4]&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fwww.incompleteideas.net%2FIncIdeas%2FBitterLesson.html&quot; target=&quot;_blank&quot;&gt;http://www.incompleteideas.net/IncIdeas/BitterLesson.html&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;原文鏈接：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fghost.oxen.ai%2Fwhy-grpo-is-important-and-how-it-works%2F&quot; target=&quot;_blank&quot;&gt;https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/&lt;/a&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/IDP/blog/17778588</link>
            <guid isPermaLink="false">https://my.oschina.net/IDP/blog/17778588</guid>
            <pubDate>Thu, 06 Mar 2025 09:49:00 GMT</pubDate>
            <author>原創</author>
        </item>
        <item>
            <title>trustcall —— 基於 LangGraph 的強大工具調用庫</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                                                                        
                                                                                    &lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:#1f2328&quot;&gt;當被要求生成或修改大型 JSON blob 時，LLM 會遇到困難。&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;color:#1f2328&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;background-color:#ffffff&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;code&gt;trustcall&lt;/code&gt;可通過&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;color:#1f2328&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;background-color:#ffffff&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;要求 LLM 生成&amp;nbsp;&lt;a href=&quot;https://datatracker.ietf.org/doc/html/rfc6902&quot;&gt;JSON 補丁&lt;/a&gt;操作來解決這個問題。這使得：&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;更快、更便宜地生成結構化輸出。&lt;/li&gt;
&lt;li&gt;即使對於複雜的嵌套模式（定義為 pydantic、模式字典或常規 python 函數）也可以彈性重試驗證錯誤&lt;/li&gt;
&lt;li&gt;準確更新現有模式，避免不必要的刪除。&lt;/li&gt;
&lt;/ul&gt;

&lt;p style=&quot;text-align:start&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;color:#1f2328&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;background-color:#ffffff&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;可靈活適用於多種常見的 LLM 工作流程，例如：&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Extraction&lt;/li&gt;
&lt;li&gt;LLM routing&lt;/li&gt;
&lt;li&gt;Multi-step agent tool use&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;415&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0306/173555_bjJK_4252687.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

                                                                    &lt;/div&gt;
                                                                </description>
            <link>https://www.oschina.net/p/trustcall</link>
            <guid isPermaLink="false">https://www.oschina.net/p/trustcall</guid>
            <pubDate>Thu, 06 Mar 2025 09:37:00 GMT</pubDate>
        </item>
        <item>
            <title>騰訊混元發佈並開源圖生視頻模型，支持生成背景音效及 2K 視頻</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;3 月 6 日，騰訊混元&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FaOeJoWyQ78o45KlJnAtAkg&quot; target=&quot;_blank&quot;&gt;宣佈&lt;/a&gt;推出圖生視頻模型並對外開源，同時上線對口型與動作驅動等玩法，並支持生成背景音效及 2K 高質量視頻。&lt;/p&gt; 
&lt;p&gt;開源內容包含權重、推理代碼和 LoRA 訓練代碼，支持開發者基於混元訓練專屬 LoRA 等衍生模型。&lt;/p&gt; 
&lt;p&gt;目前在 Github、HuggingFace 等主流開發者社區均可下載體驗。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Github: &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FTencent%2FHunyuanVideo-I2V&quot; target=&quot;_blank&quot;&gt;https://github.com/Tencent/HunyuanVideo-I2V&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Huggingface：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Ftencent%2FHunyuanVideo-I2V&quot; target=&quot;_blank&quot;&gt;https://huggingface.co/tencent/HunyuanVideo-I2V&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;據介紹，基於圖生視頻的能力，用户只需上傳一張圖片，並簡短描述希望畫面如何運動、鏡頭如何調度等，混元即可按要求讓圖片動起來，變成 5 秒的短視頻，還能自動配上背景音效。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-79aaf27253683e0e75fd797b7842f3f77d1.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;此外，上傳一張人物圖片，並輸入希望「對口型」的文字或音頻，圖片中的人物即可「説話」或「唱歌」；使用「動作驅動」能力，還能一鍵生成同款跳舞視頻。&lt;/p&gt; 
&lt;p&gt;目前用户通過混元 AI 視頻官網即可體驗（https://video.hunyuan.tencent.com/），企業和開發者可在騰訊雲申請使用 API 接口使用。&lt;/p&gt; 
&lt;p&gt;騰訊混元表示，此次開源的圖生視頻模型，是混元文生視頻模型開源工作的延續，模型總參數量保持 130 億，模型適用於多種類型的角色和場景，包括寫實視頻製作、動漫角色甚至 CGI 角色製作的生成。&lt;/p&gt; 
&lt;ul&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/337275</link>
            <guid isPermaLink="false">https://www.oschina.net/news/337275</guid>
            <pubDate>Thu, 06 Mar 2025 08:46:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>Manus 邀請碼炒至 6 萬元，官方稱將逐步有序釋放</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;自發布以後，Manus 受到了熱烈追捧，網友紛紛湧向 Manus 官網，從而導致頁面一度因訪問量過大而崩潰。目前，試用 Manus 需要輸入邀請碼，這導致邀請碼一碼難求。在二手交易平台上，邀請碼的價格被炒至幾百元到 6 萬元不等。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;448&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-556805b52b9544d5e92e8dd8809b514dcf6.png&quot; width=&quot;300&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;針對邀請碼炒作問題，Manus AI 合夥人張濤在社交平台做出了回應。他首先感謝了大家對 Manus 的關注，並澄清了幾點重要信息：一是公司從未開設任何付費獲取邀請碼的渠道；二是從未投入任何市場推廣預算；三是內測期間系統容量有限，公司將優先保障現有用户的核心體驗，並逐步有序釋放邀請碼。&lt;/p&gt; 
&lt;p&gt;張濤稱，「目前採取邀請碼機制，是因為此刻服務器容量確實有限，不得已而為之，團隊也熬夜搞了一整天了。希望在接下來的時間裏能讓更多處在 waitlist 中的用户優先體驗 Manus。」&lt;/p&gt; 
&lt;p&gt;「懇請大家對一家幾十人的創業公司多一點包容和理解，團隊正在全力輸出，讓大家早日體驗上更好的產品。」&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;相關閲讀：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p style=&quot;margin-left:0px; margin-right:0px; text-align:start&quot;&gt;&lt;a href=&quot;https://www.oschina.net/news/337193/manus-ai-agent&quot; target=&quot;_blank&quot;&gt;Monica.im 發佈 AI Agent 產品「Manus」&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/337267</link>
            <guid isPermaLink="false">https://www.oschina.net/news/337267</guid>
            <pubDate>Thu, 06 Mar 2025 08:19:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>AMD 發佈完全開源的 3B 參數語言模型 Instella</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;AMD 今天&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Frocm.blogs.amd.com%2Fartificial-intelligence%2Fintroducing-instella-3B%2FREADME.html&quot; target=&quot;_blank&quot;&gt;發佈&lt;/a&gt;&lt;/u&gt;了完全開源的 3B 參數語言模型&amp;nbsp;Instella。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;1614&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0306/155518_eNxW_2720166.png&quot; width=&quot;2188&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;GitHub：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FAMD-AIG-AIMA%2FInstella&quot; target=&quot;_blank&quot;&gt;https://github.com/AMD-AIG-AIMA/Instella&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;AMD 宣稱 Instella 代表着&quot;完全開放的最先進的 30 億參數語言模型 (LM)&quot;。這些模型是在 AMD Instinct MI300X GPU 上訓練的。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;通過完全開源 Instella 模型，包括權重、訓練超參數、數據集和代碼，我們旨在促進人工智能社區內的創新與合作。&lt;/p&gt; 
 &lt;p&gt;我們相信，透明度、可重複性和可訪問性是人工智能研究與開發取得進展的關鍵驅動力。&lt;/p&gt; 
 &lt;p&gt;我們邀請開發人員、研究人員和人工智能愛好者探索 Instella，為其不斷改進獻計獻策，並與我們一起推動語言模型的發展。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;根據 AMD 公佈的數據，其性能與 Llama 3.2 3B、Gemma-2 2B 和 Qwen 2.5 3B 等同類產品相比具有很強的競爭力。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-6b27824412274c03ca53d1b47afaedf5831.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/337263/amd-instella-3b</link>
            <guid isPermaLink="false">https://www.oschina.net/news/337263/amd-instella-3b</guid>
            <pubDate>Thu, 06 Mar 2025 07:56:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>智源開源多模態向量模型 BGE-VL</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;智源研究院宣佈聯合多所高校開發了多模態向量模型 BGE-VL，進一步擴充了原有生態體系。BGE-VL 在圖文檢索、組合圖像檢索等主要多模態檢索任務中均取得了最佳效果。&lt;/p&gt; 
&lt;p&gt;BGE-VL 藉助大規模合成數據 MegaPairs 訓練而成。這一設計具備以下兩大核心優勢:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;優異的可擴展性：&lt;/strong&gt;MegaPairs 結合多模態表徵模型、多模態大模型和大語言模型，在海量圖文語料庫中高效挖掘多模態三元組數據。其算法能夠以極低成本持續生成多樣化且高質量的多模態三元組。本次發佈的版本涵蓋 2600 萬條樣本，為多模態檢索模型的訓練提供了大規模、高價值的數據支持。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;卓越的數據質量：&lt;/strong&gt;相較於傳統人工標註數據，MegaPairs 僅需 1/70 的數據量即可實現更優的訓練效果。利用該合成數據，智源訓練了多模態檢索模型 BGE-VL，顯著提升了多個主流多模態檢索基準的性能。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;BGE-VL 的技術報告已發佈，相關數據、模型及代碼資源將陸續向社區全面開放。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li style=&quot;text-align:justify&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;論文地址：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2412.14475&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2412.14475&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; 
 &lt;li style=&quot;text-align:justify&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;項目主頁：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FVectorSpaceLab%2FMegaPairs&quot; target=&quot;_blank&quot;&gt;https://github.com/VectorSpaceLab/MegaPairs&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; 
 &lt;li style=&quot;text-align:justify&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;模型地址：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2FBAAI%2FBGE-VL-MLLM-S1&quot; target=&quot;_blank&quot;&gt;https://huggingface.co/BAAI/BGE-VL-MLLM-S1&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/337258</link>
            <guid isPermaLink="false">https://www.oschina.net/news/337258</guid>
            <pubDate>Thu, 06 Mar 2025 07:38:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>vivo OS 部門設立 AI 領域板塊</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FbXnSMuj_jA5V9BeIDYuJkw&quot; target=&quot;_blank&quot;&gt;據雷鋒網獨家消息&lt;/a&gt;&lt;/u&gt;，vivo 近日進行了組織架構調整，其中其 AI 領域有了新的變動。&lt;/p&gt; 
&lt;p&gt;具體來看，vivo 原 OS 產品領域下將設立 AI 領域，人工智能一部、人工智能二部劃入 AI 領域。原互聯網平台運營領域總經理張飛被調任 AI 領域總經理，併兼管人工智能一部，無考察期，直接向公司副總裁、OS 產品領域負責人周圍彙報。而原人工智能一部總經理肖方旭已於 1 月份離職。&lt;/p&gt; 
&lt;p&gt;據 vivo 員工透露，公司在 AI 大模型方面投入巨大，前期管理意志幹預很重，可實際看來技術進展緩慢，此事早在去年內部就有過討論，最終結果是暫時不做商業化考核，但暫停了對資金的投入。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;報道指出，目前 vivo 的大模型訓練重心正在向端側轉移，雲端的 700 億參數大語言模型還在微調和優化中，暫停了該模型的預訓練工作&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;公開資料顯示，vivo 每年都會投入 20-30 億用於大模型研發。截至 2024 年 10 月，vivo 在 AI 領域的投入已經超過 230 億元，且 AI 研究院的研發人員數量也從 2019 年的 1 千人增加至 2 千多人，是目前公開披露 AI 投入最高的手機廠商之一。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/337257</link>
            <guid isPermaLink="false">https://www.oschina.net/news/337257</guid>
            <pubDate>Thu, 06 Mar 2025 07:36:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>微信月活突破 10 億</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;QuestMobile 近日發佈了&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F-mdl9gcCNmfLotd87SOIFg&quot; target=&quot;_blank&quot;&gt;2024 年度中國移動互聯網實力價值榜&lt;/a&gt;&lt;/u&gt;，TOP50 賽道用户規模 NO.1 APP 如下。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;2284&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0306/152504_luN0_2720166.png&quot; width=&quot;800&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;本榜單體現了互聯網行業 50 個細分賽道的第一名，微信在即時通訊位列第一，&lt;strong&gt;月活唯一突破 10 億級，達到 10.8 億&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;綜合電商方面，淘寶以 9.6 億月活排名第一。短視頻方面的第一是抖音，月活 8.4 億。&lt;/p&gt; 
&lt;p&gt;從 50 個 APP 所屬的集團來看：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;阿里旗下有 8 款：淘寶、高德地圖、支付寶、釘釘、閒魚、餓了麼、菜鳥、盒馬。&lt;/li&gt; 
 &lt;li&gt;騰訊旗下有 7 款：微信、搜狗輸入法、騰訊視頻、QQ 瀏覽器、酷狗音樂、王者榮耀、QQ 郵箱。&lt;/li&gt; 
 &lt;li&gt;字節旗下有 6 款：抖音、今日頭條、番茄免費小説、剪映、番茄暢聽、豆包。&lt;/li&gt; 
 &lt;li&gt;百度旗下有 2 款：百度、百度網盤。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;App 規模增長千萬級榜單如下：&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;1548&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0306/152718_x9Xf_2720166.png&quot; width=&quot;800&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;前十名的千萬級體量 APP 增速排行榜中，字節旗下產品佔據七夕，分別是：抖音商城、豆包、悟空瀏覽器、紅果免費短劇、抖音精選、汽水音樂、番茄暢聽音樂版，可見字節流量之猛。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/337256</link>
            <guid isPermaLink="false">https://www.oschina.net/news/337256</guid>
            <pubDate>Thu, 06 Mar 2025 07:27:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>谷歌搜索測試「AI Mode」：整合多模態和實時信息、一鍵解答覆雜問題</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;谷歌公司昨日&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.google%2Fproducts%2Fsearch%2Fai-mode-search%2F&quot; target=&quot;_blank&quot;&gt;發佈博文&lt;/a&gt;，邀請谷歌搜索用户測試全新的&lt;strong&gt;「AI 模式」（AI Mode）&lt;/strong&gt;。用户可以提出更復雜的問題，並基於搜索結果，AI 生成更詳細、更直觀的答案。&lt;/p&gt; 
&lt;p&gt;谷歌表示，AI 模式將提供更高級的推理、思考和多模態能力，幫助用户更高效地獲取信息。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;540&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0306/150029_Fzwi_2720166.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;谷歌表示，以往用户在處理複雜問題時，往往需要多次搜索才能解決，而「AI 模式」能夠解決這個痛點。用户只需在桌面或移動設備上輸入查詢，點擊新的「AI 模式」按鈕即可體驗。&lt;/p&gt; 
&lt;p&gt;此外，AI 模式頁面底部還提供了「深入探索」快捷入口，用户可直接跳過常規搜索結果，專注於 AI 生成的內容。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-39952b2fa3ffbf6ae0dc3082083d80b34c1.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;在移動設備上，用户可以通過上傳圖片或語音輸入查詢，但目前僅支持文本輸出。AI 模式還支持歷史搜索記錄，方便用户查看過往查詢。&lt;/p&gt; 
&lt;p&gt;AI 模式由定製版的 Gemini 2.0 驅動，能夠訪問實時數據源和知識圖譜等資源。它通過「查詢擴展」技術，從多個子主題和數據源中提取信息，並綜合呈現。如果信息不足，用户將被引導至網頁搜索結果。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;案例 1：鳥類遷徙路徑&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;用户提問：「候鳥如何知道遷徙路線？」AI 模式會進行多步搜索並組織結果，在移動設備上以輪播形式展示來源網站，隨後提供簡明答案和相關文章。&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;案例 2：户外拍攝最佳時間&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;用户詢問：「本週在波士頓公共花園拍攝户外訂婚照的最佳時間是什麼？」AI 模式結合實時天氣信息，推薦具體日期和黃金時段，並註明日落時間。&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;案例 3：睡眠追蹤設備對比&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;用户提問：「智能戒指、智能手錶和追蹤墊在睡眠追蹤功能上有何區別？」AI 模式以對比表格形式呈現答案，並支持後續問題，如「深度睡眠時心率如何變化？」&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;從早期測試來看，AI 模式的查詢長度是傳統搜索的兩倍，用户有 25% 的時間會進行後續提問。谷歌計劃逐步向所有用户開放這一功能，目前測試主要面向高級用户。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/337252/google-ai-mode-search</link>
            <guid isPermaLink="false">https://www.oschina.net/news/337252/google-ai-mode-search</guid>
            <pubDate>Thu, 06 Mar 2025 07:07:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>MongoDB 終於實現盈利，但股價因業績預期不佳而暴跌</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;MongoDB 公司&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Finvestors.mongodb.com%2Fnews-releases%2Fnews-release-details%2Fmongodb-inc-announces-fourth-quarter-and-full-year-fiscal-2025&quot; target=&quot;_blank&quot;&gt;公佈&lt;/a&gt;了 2025 財年第四季度業績，終於實現了季度盈利，超出了華爾街對盈利和收入的目標。但該公司對新財年的預期卻令人大失所望，導致投資者紛紛逃離，其股價在尾盤交易中暴跌。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;該公司報告稱，扣除股票薪酬等某些成本前的每股收益為 1.28 美元，營收為 5.484 億美元，比去年同期增長 20%。這些數字遠遠超出了分析師的預期，分析師此前預計該公司每股收益僅為 60 美分，銷售額為 5.21 億美元。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;本季度訂閲收入增長了 19%，服務收入增長了 34%，公司繼續以驚人的速度增加新客户，本季度結束時新客户數量已超過 54,500 名。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;總而言之，該公司的淨收入為 1580 萬美元 —— 雖然利潤不高，但要好於一年前的 5550 萬美元淨虧損。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;270&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-09822091507c54b3e53dd83a09a400a41e2.png&quot; width=&quot;700&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;然而儘管這些數據很穩健，該公司對新財年的預期卻令人失望。MongoDB 表示，預計每股收益在 2.44 美元至 2.62 美元之間，遠低於華爾街 3.38 美元的目標。在收入方面，該公司預計收入在 22.4 億美元至 22.8 億美元之間，低於華爾街 23.3 億美元的普遍預期。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;在報告發布之前，MongoDB 股價當天早些時候上漲了 3% 以上，但投資者因預期下調而放棄交易。盤後，該股暴跌逾 16%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;MongoDB 最近還進行了一項重要收購，收購了一家名為 Voyage AI Inc. 的初創公司，「支持下一代 AI 應用的先進嵌入和重新排序模型的先驅」。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;MongoDB 總裁兼首席執行官 Dev Ittycheria 在談到此次收購時表示：「收購 Voyage AI 之後，我們將實時數據、複雜的嵌入和檢索模型以及語義搜索直接結合在數據庫中，簡化了可信賴的人工智能應用程序的開發。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;儘管盤後下跌，但 MongoDB 股價今年迄今仍上漲逾 13%。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/337249/mongodb-fourth-quarter-and-full-year-fiscal-2025</link>
            <guid isPermaLink="false">https://www.oschina.net/news/337249/mongodb-fourth-quarter-and-full-year-fiscal-2025</guid>
            <pubDate>Thu, 06 Mar 2025 06:56:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>微信「史詩級」更新：新增「清理原圖 / 視頻」功能</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;在最新版本的 iOS 和 Android 微信版本中，微信熱更新了一個針對清理存儲空間緩存的功能，在「設置 – 通用 – 存儲空間」當中，可以查看已接收和已發出的原圖、原視頻。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-14c220568032f76d5204db7983e2d087abf.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;該界面頂部還有一行小字寫着「清理原圖、原視頻可以節省存儲空間」。在清理後，用户仍可在聊天中看到普通畫質的圖片、視頻。&lt;/p&gt; 
&lt;p&gt;在原圖、原視頻點開之後右側選項之後，會彈出「按文件大小查看」或「按聊天大小查看」，進而按照大小、時間、類型查看具體的原圖、原視頻，自行選擇需要清理的內容。&lt;/p&gt; 
&lt;p&gt;經網友實測，清理了「原圖」之後，被清理的內容仍會在聊天記錄中顯示，但畫質已降低。該功能既能避免「圖片已過期」，還能騰出存儲空間，實現微信瘦身。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/337248</link>
            <guid isPermaLink="false">https://www.oschina.net/news/337248</guid>
            <pubDate>Thu, 06 Mar 2025 06:44:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>OpenAI 擬推月費 2 萬美元的博士級 AI Agent</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.theinformation.com%2Farticles%2Fopenai-plots-charging-20-000-a-month-for-phd-level-agents&quot; target=&quot;_blank&quot;&gt;The Information&lt;/a&gt;&lt;span style=&quot;color:#000000&quot;&gt;&amp;nbsp;消息稱，OpenAI 計劃對達到博士水平的 AI Agent 每月收取高達 2 萬美元的費用。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;282&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-eab847f1c359ba1e53fa20ee5c01f5d983f.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;據悉，OpenAI 計劃針對不同應用推出幾款不同類型的 AI Agent 產品，包括對銷售線索進行分類和排名以及軟件工程。除了最昂貴的這款每月 2 萬美元，旨在支持 「博士級研究」的；還有一款是「high-income knowledge worker」 agent，每月收費 2,000 美元。另一款是軟件開發人員代理，每月收費 10,000 美元。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;目前還不清楚這些代理工具何時推出，也不清楚哪些客户有資格購買這些工具。但 The Information 指出，OpenAI 的投資者軟銀承諾，僅今年一年就將在 OpenAI 的 agent 產品上投資 30 億美元。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/337244/openai-20000-phd-level-agents</link>
            <guid isPermaLink="false">https://www.oschina.net/news/337244/openai-20000-phd-level-agents</guid>
            <pubDate>Thu, 06 Mar 2025 06:32:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>3 月 22 日，南京，聊聊生成式 AI 應⽤構建</title>
            <description></description>
            <link>https://www.oschina.net/event/2423811</link>
            <guid isPermaLink="false">https://www.oschina.net/event/2423811</guid>
            <pubDate>Thu, 06 Mar 2025 06:29:00 GMT</pubDate>
        </item>
        <item>
            <title>在線老虎機遊戲 UI 漏洞導致博彩公司損失近 100 萬英鎊</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.bbc.co.uk%2Fnews%2Farticles%2Fcx2gl2n2n14o&quot; target=&quot;_blank&quot;&gt;BBC&lt;/a&gt; 報道稱，&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;2020 年 10 月，一位來自英國英格蘭格洛斯特郡的園丁 Corrine Durber 在玩 Wild Hatter（愛爾蘭體育博彩公司 Paddy Power 旗下的一款線上老虎機博彩遊戲）遊戲時，結算頁面顯示她贏得了「Monster Jackpot」獎項，金額高達 1,097,132.71 英鎊。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;但 Paddy Power 卻以該用户實際中只是中了&quot;Daily Jackpot&quot;為由，僅支付了 20,265 英鎊，稱差額歸因於遊戲顯示界面的編程錯誤。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;對此，Durber 以屏幕上顯示的內容為依據，起訴了 Paddy Power 和 Betfair 的母公司 PPB Entertainment Limited，指控其違約並要求支付剩餘的獎金。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;在當地時間本週三的判決中，法官作出了有利於 Durber 的簡易判決：「當商家因自身的魯莽、疏忽、錯誤、數字服務不足和測試不足而將所有風險轉嫁給消費者時，這在我看來是不合理的。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;393&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-402d7ef5c362952bcd0a58a6c7077403ca7.png&quot; width=&quot;700&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Corrine Durber 與丈夫 Colin（左）和律師 Peter Coyle（右）在高等法院外合影&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Durber 在接受採訪時表示，這筆錢將改變她家人的生活。「顯然，這將用於照顧孩子們，我們會幫他們還清抵押貸款，並享受我們的退休生活」。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;PPB 曾辯稱，遊戲結果是由隨機數生成器決定的，該生成器顯示 Durber 只贏得了&quot;Daily Jackpot&quot;；但因為 bug 影響了遊戲結算動畫，導致顯示了錯誤的結果。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;對此，法官則表示，「所見即所得」的理念是遊戲的核心。他在一份 62 頁的裁決中繼續寫道：「客觀地説，顧客會希望並期待屏幕上顯示的內容是準確和正確的。當顧客進入實體賭場玩輪盤賭時，可能也會有同樣的期望。如果他們押注 13 號，而球落在 13 號上，他們期望賭場會支付獎金。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;法官發現，由於軟件映射中的人為錯誤，隨機數生成器的結果與屏幕上的結果不同，這一 bug 共影響了 48 天內的 14 次遊戲。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;裁決作出後，Paddy Power 的一位發言人表示：「我們始終努力提供最佳的客户體驗，並以公平為榮。我們對這起不幸的案件深表遺憾，並正在對判決進行復審。」&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/337229</link>
            <guid isPermaLink="false">https://www.oschina.net/news/337229</guid>
            <pubDate>Thu, 06 Mar 2025 05:52:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>史上最強芯片：蘋果 M3 Ultra 支持 512 GB 統一內存、可本地部署滿血版 DeepSeek R1</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;蘋果昨天&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.apple.com.cn%2Fnewsroom%2F2025%2F03%2Fapple-reveals-m3-ultra-taking-apple-silicon-to-a-new-extreme%2F&quot; target=&quot;_blank&quot;&gt;正式發佈&lt;/a&gt;&lt;/u&gt;了迄今打造的最強芯片&amp;nbsp;&lt;span&gt;M3 Ultra —— 將 Apple 芯片性能提升至新極限。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;新芯片性能比 M1 Ultra 提升最多達 2.6 倍，最高支持 512 GB 統一內存，創個人電腦內存新高，此外還&lt;/span&gt;配備了 Mac 性能最強勁的中央處理器和圖形處理器，神經網絡引擎核心數量翻倍。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-d1d71f48b7a0d7ae41433386bb9e2aa9072.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;M3 Ultra 芯片亮點&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;起步內存 96GB，最高可配置 512GB 內存&lt;/li&gt; 
 &lt;li&gt;內部共集成 1,840 億個晶體管&lt;/li&gt; 
 &lt;li&gt;支持雷靂 5 連接，數據傳輸速度最高可達 120 Gb/s，比雷靂 4 提升達 2 倍以上&lt;/li&gt; 
 &lt;li&gt;配備最多 32 核中央處理器，包括 24 顆性能核心和 8 顆能效核心，性能最高可達 M2 Ultra 的 1.5 倍，M1 Ultra 的 1.8 倍&lt;/li&gt; 
 &lt;li&gt;擁有 Apple 芯片中最強的圖形處理器，包括最多 80 顆圖形處理核心，性能比 M2 Ultra 提升最多達 2 倍，比 M1 Ultra 提升最多達 2.6 倍&lt;/li&gt; 
 &lt;li&gt;採用創新的 UltraFusion 封裝架構，通過超過 10,000 個高速連接點，將兩枚 M3 Max 晶粒整合在一起，可同時傳輸超過 10,000 個信號，帶來超過 2.5TB/s 的低延遲片間帶寬，提供低延遲和高帶寬的傳輸能力&lt;/li&gt; 
 &lt;li&gt;提供了專屬的硬件加速 H.264、HEVC 與四個 ProRes 編解碼引擎，能夠播放最多可達 22 條 8K ProRes 422 視頻流。&lt;/li&gt; 
 &lt;li&gt;顯示引擎支持最多 8 台 Pro Display XDR，呈現超過 1.6 億顆像素&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.apple.com.cn%2Fnewsroom%2F2025%2F03%2Fapple-reveals-m3-ultra-taking-apple-silicon-to-a-new-extreme%2F&quot; target=&quot;_blank&quot;&gt;詳情查看官方介紹&lt;/a&gt;&lt;/u&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;相關閲讀：&lt;a href=&quot;https://www.oschina.net/news/337201/apple-unveils-new-mac-studio-m3-ultra&quot; target=&quot;news&quot;&gt;蘋果發佈「核彈級」 Mac Studio：頂配售價超 10 萬、最強處理器 M3 Ultra 正式亮相&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/337215/apple-m3-ultra-new-extreme</link>
            <guid isPermaLink="false">https://www.oschina.net/news/337215/apple-m3-ultra-new-extreme</guid>
            <pubDate>Thu, 06 Mar 2025 03:56:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>LSM-TREE 從入門到入魔：從零開始實現一個高性能鍵值存儲</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                    
                                                                                                                                                    &lt;h1&gt;一、引，言&lt;/h1&gt; 
&lt;p&gt;LSM-Tree（Log-Structured Merge Tree）是一種高效的鍵值存儲數據結構，廣泛應用於 NoSQL 數據庫和大數據處理系統中。其核心思想是通過分層、有序地利用磁盤順序寫入的性能優勢，優化寫入操作，同時犧牲部分讀取性能以換取更高的寫入吞吐量。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//3983af5521bbb144f98321026a45d75a.jpeg&quot; alt=&quot;引言.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt; &lt;img src=&quot;https://oscimg.oschina.net/oscnet//e78c25208e08de7c918d0a3ff99efdfe.jpeg&quot; alt=&quot;引言 2.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 在互聯網的各個基礎設施中，不論是數據庫還是緩存亦或是大數據框架，LSM-Tree 這個數據結構都是很常見的身影。&lt;/p&gt; 
&lt;p&gt;我每天都在使用這個存儲引擎，但是對它的瞭解還流於表面，所以我想要自己實現一次 LSM-Tree 加深理解。&lt;/p&gt; 
&lt;p&gt;本次實現我們採用了 Zig 語言，簡要的實現 LSM-Tree 的核心功能（讀寫、數據壓縮、持久化，不包含 MVCC 的內容）。&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Zig 是一種新興的系統編程語言，其設計目標是提供現代特性的同時保持低複雜性。&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;本項目極大的受到了 Mini-Lsm 這個項目的啓發，強烈推薦大家學習這個項目！&lt;/em&gt;&lt;/p&gt; 
&lt;h1&gt;二、LSM-Treee 核心功能概述&lt;/h1&gt; 
&lt;p&gt;在開始自己編寫之前，我先簡單介紹一下 LSM-Tree（&lt;strong&gt;Log-Structured Merge Tree&lt;/strong&gt;）的架構以及讀寫流程。&lt;/p&gt; 
&lt;p&gt;LSM-Tree 它結合了日誌和索引的特點，優化了寫入和讀取性能。每次寫入都是採用 append-only 的方式，所以寫入性能很高。&lt;/p&gt; 
&lt;p&gt;而作為代價，追加寫入會造成存儲放大，LSM-Tree 時採用了多層 SSTable 的方式將數據堆疊在硬盤上。所以需要一個合併壓縮的過程來回收過多的空間。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//d49f94616047080aabc5ff8f4d80024f.jpeg&quot; alt=&quot;合併壓縮的過程.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;寫流程&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;預寫日誌&lt;/strong&gt; （&lt;strong&gt;WAL&lt;/strong&gt;） ：寫操作首先寫入預寫日誌（WAL），用於記錄未提交的數據，確保數據的持久性和一致性。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MemTable&lt;/strong&gt;：隨後將數據寫入內存中的 MemTable，MemTable 是一個平衡樹（如 skiplist），支持快速插入和刪除操作。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;觸發 Compaction&lt;/strong&gt;：當 MemTable 達到一定閾值時，會觸發後台線程將 MemTable 中的數據刷入磁盤，生成 SSTable 文件。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SSTable&lt;/strong&gt;：生成的 SSTable 文件是不可變的，存儲在磁盤上，用於後續讀取操作。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;合併操作&lt;/strong&gt; （&lt;strong&gt;Merge&lt;/strong&gt;） ：當多個 SSTable 文件達到一定數量時，會觸發合併操作，將它們合併為一個更大的 SSTable 文件，以減少文件數量。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;讀流程&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;MemTable 優先&lt;/strong&gt;：讀取操作首先從 MemTable 中查找數據，因為 MemTable 是按升序排列的，查找效率較高。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Block Cache&lt;/strong&gt;：如果 MemTable 中未找到數據，則從 Block Cache 中查找。Block Cache 存儲了預先加載到內存中的 SSTable 塊，以提高讀取性能。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SSTable 查找&lt;/strong&gt;：如果 Block Cache 中也未找到數據，則從磁盤上的 SSTable 文件中查找。Lsm-tree 會從最低層（L0）開始查找，逐層向上查找，直到找到目標數據。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;多版本併發控制&lt;/strong&gt; （&lt;strong&gt;MVCC&lt;/strong&gt;） ：Lsm-tree 支持多版本併發控制，允許同時訪問不同版本的數據，從而提高併發性能。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;三、核心功能實現&lt;/h1&gt; 
&lt;h2&gt;MemTable 實現&lt;/h2&gt; 
&lt;p&gt;首先，我們先實現 LSM 存儲引擎的內存結構---Memtable。我們選擇&lt;strong&gt;跳錶&lt;/strong&gt;實現作為 Memtable 的數據結構，因為它支持無鎖的併發讀寫。我們不會深入介紹跳錶的工作原理 (Redis 的同學應該不陌生這個東西)，簡單來説，它是一個易於實現的有序鍵值映射。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//adc116878e613d568440d87fb5500640.jpeg&quot; alt=&quot;有序健值.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt; Skiplist 的實現非常簡單，這裏我利用 Zig 編譯時的能力實現了一個泛型版本的跳錶 src/skiplist.zig，有興趣的小夥伴可以直接去倉庫中參觀代碼。&lt;/p&gt; 
&lt;p&gt;基於 SkipList 的能力，我們即可包裝出 Memtable 的基本功能。&lt;/p&gt; 
&lt;p&gt;我們這個 LSM 支持 WAL 功能的，即寫入內存表之前要先寫入磁盤日誌，方便在意外宕機重啓後可以恢復數據。&lt;/p&gt; 
&lt;p&gt;WAL 的能力我就不想自己再實現了，於是從網上扒了一個 C 的實現（Zig 集成 C 語言非常便捷，可以參考與 C 交互）。&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;map: Map,
lock: RwLock,
wal: ?Wal,
id: usize,
allocator: std.mem.Allocator,
arena: std.heap.ArenaAllocator,
approximate_size: atomic.Value(usize) = atomic.Value(usize).init(0),

fn putToList(self: *Self, key: []const u8, value: []const u8) !void {
    {
        self.lock.lock();
        defer self.lock.unlock();
        try self.map.insert(kk, vv);
    }

    _ = self.approximate_size.fetchAdd(@intCast(key.len + value.len), .monotonic);
}

fn putToWal(self: *Self, key: []const u8, value: []const u8) !void {
    // [key-size: 4bytes][key][value-size: 4bytes][value]

    if (self.wal) |w| {
        var buf = std.ArrayList(u8).init(self.arena.allocator());

        var bw = buf.writer();
        try bw.writeInt(u32, @intCast(key.len), .big);
        _ = try bw.write(key);
        try bw.writeInt(u32, @intCast(value.len), .big);
        _ = try bw.write(value);
        try w.append(buf.items);
    }
}

// 寫入 Memtable，先寫 WAL，再寫 skiplist table
pub fn put(self: *Self, key: []const u8, value: []const u8) !void {
    try self.putToWal(key, value);
    try self.putToList(key, value);
}

pub fn get(self: *Self, key: []const u8, val: *[]const u8) !bool {
    self.lock.lockShared();
    defer self.lock.unlockShared();
    var vv: []const u8 =     ;
    if (try self.map.get(key, &amp;amp;vv)) {
        val.* = vv;
        return true;
    }
    return false;
}

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;注意到這裏我們沒有實現刪除的功能，這裏我仿照了 RocksDB 中的墓碑機制，用空值代表刪除，所以刪除被 put(key, &quot;&quot;) 取代。&lt;/p&gt; 
&lt;h2&gt;SSTable&lt;/h2&gt; 
&lt;p&gt;接下來，我們就着手開始實現 LSM 中另外一個重要元素 --- SSTable。&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;SSTable（Sorted String Table）是一種不可變的（Immutable）磁盤文件，內部按 Key 有序排列，存儲鍵值對數據。每個 SSTable 文件生成後不再修改，更新和刪除操作通過追加新記錄或標記刪除，最終通過合併（Compaction）清理冗餘數據。 每當 LSM-Tree 中的 MemTable 體積超出閾值，就會將內存中的數據寫入 SsTable。&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//441435c815897a51832a32a1157637ae.jpeg&quot; alt=&quot;內存中的數據.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 每個 SSTable 由多個 Block 組成，每個 Block 是一組 KV 的 package。&lt;/p&gt; 
&lt;p&gt;Block 的&lt;strong&gt;編碼格式&lt;/strong&gt;如下：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//ec2724e561fe943e071d00506663be9d.jpeg&quot; alt=&quot;block 的健碼格式.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 為了構建一個 Block，我們實現了一個&lt;strong&gt;BlockBuilder&lt;/strong&gt;的模塊，這部分代碼見 src/block.zig：&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;pub const Block = struct {
    data_v: std.ArrayList(u8),
    offset_v: std.ArrayList(u16),
}

pub const BlockBuilder = struct {
    allocator: std.mem.Allocator,
    offset_v: std.ArrayList(u16),
    data_v: std.ArrayList(u8),
    block_size: usize,
    first_key: []u8,
    
    pub fn add(self: *Self, key: []const u8, value: ?[]const u8) !bool {
        std.debug.assert(key.len &amp;gt; 0); // key must not be empty

        const vSize = if (value) |v| v.len else 0;
        
        if ((self.estimated_size() + key.len + vSize + 3 * @sizeOf(u16) &amp;gt; self.block_size) and !self.is_empty()) {
            return false;
        }
        try self.doAdd(key, value);

        if (self.first_key.len == 0) {
            self.first_key = try self.allocator.dupe(u8, key);
        }
        return true;
    }

    fn doAdd(self: *Self, key: []const u8, value: ?[]const u8) !void {
        // add the offset of the data into the offset array
        try self.offset_v.append(@intCast(self.data_v.items.len));
        const overlap = calculate_overlap(self.first_key, key);

        var dw = self.data_v.writer();
        // encode key overlap
        try dw.writeInt(u16, @intCast(overlap), .big);
        // encode key length
        try dw.writeInt(u16, @intCast(key.len - overlap), .big);

        // encode key content
        _ = try dw.write(key[overlap..]);
        // encode value length
        if (value) |v| {
            try dw.writeInt(u16, @intCast(v.len), .big);
            // encode value content
            _ = try dw.write(v);
        } else {
            try dw.writeInt(u16, 0, .big);
        }
    }

    pub fn build(self: *Self) !Block {
        if (self.isEmpty()) {
            @panic(&quot;block is empty&quot;);
        }
        return Block.init(
            try self.data_v.clone(),
            try self.offset_v.clone(),
        );
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;可能有同學注意到，我們寫 key 的時候沒有直接將 key 值寫入，而且只寫了 key 與當前 block 的第一個 key 不重疊的 suffix 部分。由於 block 中的 key 都是有序的，所以一個 block 中的 key 有很大概率是前綴類似的，所以這裏是一個空間優化的小技巧，例如：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Key: foo, foo1, foo2, foo3 ....&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;我們寫入 block 時，只需要寫：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;foo|1|2|3|....&lt;/strong&gt; 很多有序表的實現中都會用到這個小技巧。&lt;/p&gt; 
&lt;p&gt;有了 block 的實現，我們可以進一步來定義 SSTable 的格式。一個 SSTable 由多個 Block、block 元數據以及布隆過濾器構成。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//e183c694bf3c0e2a6ec6b15651d5fbee.jpeg&quot; alt=&quot;布隆過濾器.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt; &lt;em&gt;布隆過濾器是一種概率性數據結構，用於維護一組鍵。您可以向布隆過濾器中添加鍵，並且可以知道在添加到布隆過濾器中的鍵集中可能存在或必須不存在的鍵。&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;在 SSTable 中添加布隆過濾器可以有效提升查詢 key 的效率。&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;元數據包含了 block 的&lt;strong&gt;第一個與最後一個 key 以及 block 在 sst 中的 offset 信息&lt;/strong&gt;，記錄元數據主要為了在後續的檢索中可以快速定位某個 key 落在哪個 block 中。&lt;/p&gt; 
&lt;p&gt;同樣的套路，為了構建 SSTable，我們先實現一個&lt;strong&gt;SSTableBuilder&lt;/strong&gt;，部分代碼見 src/ss_table.zig&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;pub const SsTableBuilder = struct {
    allocator: std.mem.Allocator,
    builder: BlockBuilder, // 剛才實現的 block 構建裝置
    first_key: ?[]const u8,
    last_key: ?[]const u8,
    meta: std.ArrayList(BlockMeta),
    block_size: usize,
    data: std.ArrayList(u8),
    bloom: BloomFilterPtr, // 布隆過濾器
    
    pub fn add(self: *Self, key: []const u8, value: []const u8) !void {
        try self.setFirstKey(key);
        try self.bloom.get().insert(key); // 寫入布隆過濾器

        if (try self.builder.add(key, value)) {
            try self.setLastKey(key);
            return;
        }
        // block is full
        try self.finishBlock();
        std.debug.assert(try self.builder.add(key, value));
        try self.resetFirstKey(key);
        try self.setLastKey(key);
    }
    
    // 寫入一個 block 的數據
    fn finishBlock(self: *Self) !void {
        if (self.builder.isEmpty()) {
            return;
        }
        var bo = self.builder;
        // reset block
        defer bo.reset();

        self.builder = BlockBuilder.init(self.allocator, self.block_size);
        var blk = try bo.build();
        defer blk.deinit();
        const encoded_block = try blk.encode(self.allocator); // block 序列化
        defer self.allocator.free(encoded_block);
        
        // 記錄 block 的元數據
        try self.meta.append(.{
            .allocator = self.allocator,
            .offset = self.data.items.len,
            .first_key = try self.allocator.dupe(u8, self.first_key.?),
            .last_key = try self.allocator.dupe(u8, self.last_key.?),
        });
        const cksm = hash.Crc32.hash(encoded_block); // 寫入 4b 的校驗值
        try self.data.appendSlice(encoded_block);
        try self.data.writer().writeInt(u32, cksm, .big);
    }
    
    // 構建為一個 SSTable
    pub fn build(
        self: *Self,
        id: usize,
        block_cache: ?BlockCachePtr, // 讀取 block 數據的緩存，減少 block 的反序列化成本
        path: []const u8,
    ) !SsTable {
        var arena = std.heap.ArenaAllocator.init(self.allocator);
        defer arena.deinit();
        const allocator = arena.allocator();

        try self.finishBlock();
        const w = self.data.writer();
        
        // 寫入元數據及其 offset
        const meta_offset = self.data.items.len;
        const meta_b = try BlockMeta.batchEncode(self.meta.items, allocator);
        _ = try w.write(meta_b);
        try w.writeInt(u32, @intCast(meta_offset), .big);

        // 寫入布隆過濾器及其 offset
        const bloom_offset = self.data.items.len;
        const encoded_bloom = try self.bloom.get().encode(allocator);
        _ = try w.write(encoded_bloom);
        try w.writeInt(u32, @intCast(bloom_offset), .big);
        
        
        const file = try FileObject.init(path, self.data.items);
        errdefer file.deinit();

        const fk = self.meta.items[0].first_key;
        const lk = self.meta.getLast().last_key;

        return .{
            .allocator = self.allocator,
            .file = file,
            .block_metas = try self.meta.toOwnedSlice(),
            .meta_offset = meta_offset,
            .block_cache = block_cache,
            .bloom = self.bloom.clone(),
            .id = id,
            .first_key = try self.allocator.dupe(u8, fk),
            .last_key = try self.allocator.dupe(u8, lk),
            .max_ts = 0,
        };
    }
}

&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Write&lt;/h2&gt; 
&lt;p&gt;有了 SSTable 和 MemTable，我們就有了 LSM-Tree 需要的兩個最重要的材料，後續的讀寫不過是對這兩類材料的組合拼裝。&lt;/p&gt; 
&lt;p&gt;在實現寫操作之前，我們先假想一下 LSM-Tree 的數據結構: &lt;img src=&quot;https://oscimg.oschina.net/oscnet//e8b35c69dad1058161979d0761a78d91.jpeg&quot; alt=&quot;lsmtree 的數據結構.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;首先我們需要一個數據結構存儲當前 MemTable、冷 MemTables 和多層的 SST，如下圖所示。 圖片&lt;/li&gt; 
 &lt;li&gt;其次我們需要一個鎖用於同步上述數據結構的讀寫行為。&lt;/li&gt; 
 &lt;li&gt;我們還需要一個 SSTable 的自增 id。&lt;/li&gt; 
 &lt;li&gt;最後還需要一些必要的配置，例如存儲路徑、線程管理器等。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;最終，我們實現的 LSM 數據結構如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;pub const StorageState = struct {
    allocator: std.mem.Allocator,
    mem_table: MemTablePtr, // 當前正在寫的 MemTable
    imm_mem_tables: std.ArrayList(MemTablePtr), // 冷 MemTable 數組
    l0_sstables: std.ArrayList(usize), // 第一層的 SSTable 數組
    levels: std.ArrayList(std.ArrayList(usize)), // 後續多層的 SSTable 數組
    sstables: std.AutoHashMap(usize, SsTablePtr), // sst_id =&amp;gt; SSTable
}

pub const StorageInner = struct {
    const Self = @This();

    allocator: std.mem.Allocator,
    state: StorageState,
    state_lock: std.Thread.RwLock = .{},
    next_sst_id: atomic.Value(usize),
    path: []const u8,
    options: StorageOptions,
    compaction_controller: CompactionController,
    block_cache: BlockCachePtr,
    terminate: std.Thread.ResetEvent = .{},
    wg: std.Thread.WaitGroup = .{},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;先不考慮逐層壓縮的邏輯，只考慮一層 SSTable 的簡單情況，寫邏輯可以簡化為如下流程：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//f5e96cf0290f96c4ce85ccfb3f369951.jpeg&quot; alt=&quot;簡化為如下流程.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;寫入 State 中的 MemTable&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;pub fn writeBatch(self: *Self, records: []const WriteBatchRecord) !void {
   for (records) |record| {
       switch (record) {
           .put =&amp;gt; |pp| {
               try self.state.getMemTable().put(pp.key, pp.value);
           },
           .delete =&amp;gt; |dd| {
               // we use &quot;&quot; as the tombstone value
               try self.state.getMemTable().put(dd, &quot;&quot;);
           },
       }
       // 嘗試把當前 MemTable 壓入冷數據
       try self.tryFreeze(self.state.getMemTable().getApproximateSize());
   }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;當 MemTable 體積超出閾值，壓入冷 MemTable 數組，重置當前 MemTable&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;fn forceFreezeMemtable(self: *Self) !void {
    const next_sst_id = self.getNextSstId();
    
    // 生成一個新的 MemTable
    var new_mm: MemTable =     ;
    {
        if (self.options.enable_wal) {
            const mm_path = try pathOfWal(self.allocator, self.path, next_sst_id);
            defer self.allocator.free(mm_path);
            new_mm = MemTable.init(next_sst_id, self.allocator, mm_path);
        } else {
            new_mm = MemTable.init(next_sst_id, self.allocator, null);
        }
    }
    errdefer new_mm.deinit();

    var old_mm: *MemTable =     ;
    {
        self.state_lock.lock();
        defer self.state_lock.unlock();
        var old_mm_ptr = self.state.mem_table;
        old_mm = old_mm_ptr.get();
        defer old_mm_ptr.release();
        self.state.mem_table = try MemTablePtr.create(self.allocator, new_mm);
        
        // 將寫滿的 MemTable 壓入冷數據
        try self.state.imm_mem_tables.append(old_mm_ptr.clone()); // newer memtable is inserted at the end
    }
    // TIPS：把磁盤同步放在鎖的範圍外面，降低鎖的覆蓋
    try old_mm.syncWal();
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;當冷 MemTable 數組大小超出配置閾值，觸發 SSTable 落盤，彈出最冷的 MemTable，寫入磁盤 SSTable，並記錄在 L0 的 SSTable 數組中。這一過程是在一個線程中定時觸發&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;pub fn flushNextMemtable(self: *Self) !void {
    std.debug.assert(self.state.imm_mem_tables.items.len &amp;gt; 0);
    var to_flush_table: *MemTable =     ;
    {
        self.state_lock.lockShared();
        defer self.state_lock.unlockShared();
        // oldest memtable is at the index 0
        to_flush_table = self.state.imm_mem_tables.items[0].load();
    }

    // 將最冷的 MemTable 構建為 SSTable
    var builder = try SsTableBuilder.init(self.allocator, self.options.block_size);
    defer builder.deinit();

    const sst_id = to_flush_table.id;
    try to_flush_table.flush(&amp;amp;builder);

    const sst_path = try self.pathOfSst(sst_id);
    defer self.allocator.free(sst_path);
    var sst = try builder.build(sst_id, self.block_cache.clone(), sst_path);
    errdefer sst.deinit();

    // add the flushed table to l0_sstables
    {
        self.state_lock.lock();
        defer self.state_lock.unlock();

        var m = self.state.imm_mem_tables.orderedRemove(0);
        defer m.deinit();
        std.debug.assert(m.load().id == sst_id);

        // newest sstable is at the end
        try self.state.l0_sstables.append(sst_id);
        try self.state.sstables.put(sst.id, try SsTablePtr.create(self.allocator, sst));
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;當然，這裏只實現了一半的寫邏輯，數據停留在 L0 的 SST 中，後續的多層 SST 還沒有使用。&lt;/p&gt; 
&lt;p&gt;剩下一半的寫邏輯會在數據壓縮的章節中介紹。&lt;/p&gt; 
&lt;h2&gt;Iterators&lt;/h2&gt; 
&lt;p&gt;寫入的過程比較好理解，但是讀就略微複雜了，以上面我們實現的寫結果為例子，最終我們的數據沉澱在一個 3 層的數據結構中，要如何高效的從其中檢索數據呢？&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//21d6f9e0f2c5dcbdd5570ac5da94ee77.jpeg&quot; alt=&quot;高效檢索數據.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 如同寫過程一般，讀過程也是對各個基礎單元 (MemTable、SSTable、Block) 讀過程的組合，為了方便組合邏輯，我們要先統一各個模塊的讀行為。&lt;/p&gt; 
&lt;p&gt;在 LSM-Tree 中，所有的讀行為都定義為瞭如下的 Interface（Zig 中沒 trait 或者 Interface，所以這裏實例代碼我用 Rust 描述）：&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;pub trait StorageIterator {
    /// Get the current value.
    fn value(&amp;amp;self) -&amp;gt; &amp;amp;[u8];

    /// Get the current key.
    fn key(&amp;amp;self) -&amp;gt; &amp;amp;[u8];

    /// Check if the current iterator is empty.
    fn is_empty(&amp;amp;self) -&amp;gt; bool;

    /// Move to the next position.
    fn next(&amp;amp;mut self) -&amp;gt; anyhow::Result&amp;lt;()&amp;gt;;

    /// Number of underlying active iterators for this iterator.
    fn num_active_iterators(&amp;amp;self) -&amp;gt; usize {
        1
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;我們首先對 MemTable、SSTable、Block 這些模塊實現讀接口，代碼可見：src/MemTable.zig，src/block.zig，src/ss_table.zig，這裏單獨簡單介紹下 SSTable 的讀接口實現思路，其他的模塊實現思路類似，感興趣的直接閲讀源碼即可。&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;pub const SsTableIterator = struct {
    allocator: std.mem.Allocator,
    table: SsTablePtr,
    blk: BlockPtr,
    blk_iterator: BlockIteratorPtr,
    blk_idx: usize,

    const Self = @This();


    pub fn initAndSeekToFirst(allocator: std.mem.Allocator, table: SsTablePtr) !Self {
        const s = try seekToFirstInner(allocator, table);
        return .{
            .allocator = allocator,
            .table = table,
            .blk_iterator = s.blk_iter,
            .blk = s.blk,
            .blk_idx = 0,
        };
    }

    pub fn initAndSeekToKey(allocator: std.mem.Allocator, table: SsTablePtr, k: []const u8) !Self {
        const b = try seekToKeyInner(allocator, table, k);
        return .{
            .allocator = allocator,
            .table = table,
            .blk_iterator = b.blk_iter,
            .blk_idx = b.blk_idx,
            .blk = b.blk,
        };
    }

    fn seekToFirstInner(allocator: std.mem.Allocator, table: SsTablePtr) !struct {
        blk: BlockPtr,
        blk_iter: BlockIteratorPtr,
    } {
        var blk = try table.get().readBlockCached(0, allocator); // 讀取第一個 block
        errdefer blk.release();
        var blk_iter = try BlockIterator.createAndSeekToFirst(allocator, blk.clone());
        errdefer blk_iter.deinit();

        return .{
            .blk = blk,
            .blk_iter = try BlockIteratorPtr.create(allocator, blk_iter), // 從 SSTable 的讀接口轉換為 Block 的讀接口
        };
    }

    fn seekToKeyInner(allocator: std.mem.Allocator, table: SsTablePtr, k: []const u8) !struct {
        blk_idx: usize,
        blk: BlockPtr,
        blk_iter: BlockIteratorPtr,
    } {
        const table_ptr = table.get();
        var blk_idx = try table_ptr.findBlockIndex(k);
        var blk = try table_ptr.readBlockCached(blk_idx, allocator);
        errdefer blk.deinit();
        var blk_iter = try BlockIterator.createAndSeekToKey(allocator, blk.clone(), k);
        errdefer blk_iter.deinit();
        var blk_iter_ptr = try BlockIteratorPtr.create(allocator, blk_iter);
        errdefer blk_iter_ptr.release();

        // 如果當前 block 讀完了，跳到下一個 block，並生成 block 的讀接口
        if (blk_iter.isEmpty()) {
            blk_idx += 1;
            if (blk_idx &amp;lt; table_ptr.numBlocks()) {
                {
                    blk.deinit();
                    blk_iter.deinit();
                }
                var blk2 = try table_ptr.readBlockCached(blk_idx, allocator);
                errdefer blk2.deinit();
                var blk_iter2 = try BlockIterator.createAndSeekToFirst(allocator, blk2.clone());
                errdefer blk_iter2.deinit();

                return .{
                    .blk_idx = blk_idx,
                    .blk_iter = try BlockIteratorPtr.create(allocator, blk_iter2),
                    .blk = blk2,
                };
            }
        }
        return .{
            .blk_idx = blk_idx,
            .blk_iter = blk_iter_ptr,
            .blk = blk,
        };
    }

    pub fn key(self: Self) []const u8 {
        return self.blk_iterator.get().key();
    }

    pub fn value(self: Self) []const u8 {
        return self.blk_iterator.get().value();
    }

    pub fn isEmpty(self: Self) bool {
        return self.blk_iterator.get().isEmpty();
    }

    pub fn next(self: *Self) !void {
        try self.blk_iterator.get().next();
        // 若當前的 Block 讀完了，就跳到下一個 block，並生成 Block 讀接口。
        if (self.blk_iterator.get().isEmpty()) {
            self.blk_idx += 1;
            if (self.blk_idx &amp;lt; self.table.get().numBlocks()) {
                self.reset();
                const blk = try self.table.get().readBlockCached(self.blk_idx, self.allocator);
                const blk_iter = try BlockIterator.createAndSeekToFirst(self.allocator, blk.clone());
                self.blk = blk;
                self.blk_iterator = try BlockIteratorPtr.create(self.allocator, blk_iter);
            }
        }
    }
};

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;有了幾個基本元素的讀接口之後，我們便遇到第一個問題：&lt;strong&gt;我們如何對多個 MemTable 做讀檢索？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//32630b4228aeed16aa135674cfff65a1.jpeg&quot; alt=&quot;如何對多個 m 做檢索.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 這個時候，我們需要一個新的數據結構來實現多個讀實例的合併檢索---- &lt;strong&gt;MergeIterator&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;MergeIterator 在內部維護一個二叉堆。堆中數據的優先級如下：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;當各個迭代器 key 不同時，具有最小 key 的迭代器最優。當多個迭代器有相同的當前 key 時，最新的迭代器一個最優。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;假設我們有如下 MemTable（iter1 最新，iter3 最舊）:&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;iter1: b-&amp;gt;del, c-&amp;gt;4, d-&amp;gt;5 iter2: a-&amp;gt;1, b-&amp;gt;2, c-&amp;gt;3 iter3: e-&amp;gt;4&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;經過合併後迭代器結果應該為：&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;a 最小，iter2 優先迭代 iter2 迭代一次後，iter1 與 iter2 key 相同，iter1 優先迭代，b-&amp;gt;2 跳過 c 最小，iter1 優先迭代，iter2 中 c-&amp;gt;3 跳過 d 最小，iter1 優先迭代，只剩 iter3，迭代 iter3&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;最終結果：a-&amp;gt;1, b-&amp;gt;del, c-&amp;gt;4, d-&amp;gt;5, e-&amp;gt;4&lt;/p&gt; 
&lt;p&gt;實現代碼如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;// 標準庫中有二叉堆實現
const IteratorHeap = std.PriorityQueue(*HeapWrapper, Comparer.Context, Comparer.cmp);

allocator: std.mem.Allocator,
q: IteratorHeap,
current: ?*HeapWrapper,

pub fn init(allocator: std.mem.Allocator, iters: std.ArrayList(StorageIteratorPtr)) !Self {
    var q = IteratorHeap.init(allocator, .{});
    if (iters.items.len == 0) {
        return Self{
            .allocator = allocator,
            .q = q,
            .current = null,
        };
    }

    // PS: the last iter has the highest priority
    // 按順序寫入二叉堆
    for (iters.items, 0..) |sp, i| {
        if (!sp.load().isEmpty()) {
            const hw = try allocator.create(HeapWrapper);
            errdefer allocator.destroy(hw);
            hw.* = HeapWrapper.init(i, sp.clone());
            try q.add(hw);
        }
    }

    const cc = q.removeOrNull();
    return Self{
        .allocator = allocator,
        .q = q,
        .current = cc,
    };
}

pub fn key(self: Self) []const u8 {
    return self.current.?.key();
}

pub fn value(self: Self) []const u8 {
    return self.current.?.value();
}

pub fn isEmpty(self: Self) bool {
    if (self.current) |cc| {
        return cc.isEmpty();
    }
    return true;
}

pub fn next(self: *Self) !void {
    const cc = self.current.?;
    while (true) {
        if (self.q.peek()) |ii| {
            std.debug.assert(!ii.isEmpty());
            // 如果優先堆頭部迭代器 A 和當前正在生效的迭代器 B 的 key 相同，讓迭代器 A 跳過重複 key
            if (std.mem.eql(u8, cc.key(), ii.key())) {
                try ii.next();
                if (ii.isEmpty()) {
                    _ = self.q.remove();
                    ii.deinit();
                    self.allocator.destroy(ii);
                }
            } else {
                break;
            }
        }
        break;
    }

    try cc.next(); // 迭代當前迭代器

    // 如果當前優先迭代器迭代完了，就從堆中彈出最優迭代器
    if (cc.isEmpty()) {
        defer {
            cc.deinit();
            self.allocator.destroy(cc);
        }
        if (self.q.removeOrNull()) |h| {
            self.current = h;
        } else {
            self.current = null;
        }
        return;
    }

    // 將當前迭代器寫回二叉堆，重新計算最優迭代器
    try self.q.add(cc); 
    self.current = self.q.removeOrNull();
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;有了 MergeIterator 這個工具，我們具備了在多個 MemTable 和多個 SSTable 中迭代檢索的能力，但是還有個問題，我們當前有兩個 MergeIterator，應該如何在兩個迭代器中執行迭代任務？&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//d030afd564de14e802e21ca38a1e58ce.jpeg&quot; alt=&quot;執行迭代任務.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 此時，我們再引入一個新的數據結構：&lt;strong&gt;TwoMergeIterator&lt;/strong&gt;，這個是 MergeIterator 在元素只有兩個的情況下的簡化版。&lt;/p&gt; 
&lt;p&gt;TwoMergeIterator 由兩個迭代器構成，一個高優一個低優，每次迭代優先迭代高優，當 key 相同時，優先迭代高優。實現如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;pub const TwoMergeIterator = struct {
    a: StorageIteratorPtr,
    b: StorageIteratorPtr,
    choose_a: bool,

    // 選擇兩個迭代器中 key 更小的迭代器
    fn chooseA(a: *StorageIterator, b: *StorageIterator) bool {
        if (a.isEmpty()) {
            return false;
        }
        if (b.isEmpty()) {
            return true;
        }
        return std.mem.lessThan(u8, a.key(), b.key());
    }

    // key 相同時，跳過低優中的 key
    fn skipB(self: *TwoMergeIterator) !void {
        const ap = self.a.load();
        const bp = self.b.load();
        if (!ap.isEmpty() and !bp.isEmpty() and std.mem.eql(u8, ap.key(), bp.key())) try bp.next();
    }

    pub fn init(a: StorageIteratorPtr, b: StorageIteratorPtr) !TwoMergeIterator {
        var iter = TwoMergeIterator{
            .a = a,
            .b = b,
            .choose_a = false,
        };
        try iter.skipB();
        iter.choose_a = chooseA(iter.a.load(), iter.b.load());
        return iter;
    }

    pub fn deinit(self: *TwoMergeIterator) void {
        self.a.release();
        self.b.release();
    }

    pub fn key(self: TwoMergeIterator) []const u8 {
        if (self.choose_a) {
            std.debug.assert(!self.a.load().isEmpty());
            return self.a.load().key();
        }
        std.debug.assert(!self.b.load().isEmpty());
        return self.b.load().key();
    }

    pub fn value(self: TwoMergeIterator) []const u8 {
        if (self.choose_a) {
            std.debug.assert(!self.a.load().isEmpty());
            return self.a.load().value();
        }
        std.debug.assert(!self.b.load().isEmpty());
        return self.b.load().value();
    }

    pub fn isEmpty(self: TwoMergeIterator) bool {
        if (self.choose_a) {
            return self.a.load().isEmpty();
        }
        return self.b.load().isEmpty();
    }

    pub fn next(self: *TwoMergeIterator) !void {
        if (self.choose_a) {
            try self.a.load().next();
        } else {
            try self.b.load().next();
        }
        try self.skipB();
        self.choose_a = chooseA(self.a.load(), self.b.load());
    }
};

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;至此，我們讀行為所需要的武器就完備了！&lt;/p&gt; 
&lt;h2&gt;Read/Scan&lt;/h2&gt; 
&lt;p&gt;讓我們再來看看 LSM 的架構圖：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//04dd1b2d4cde764a23b28d78259cf343.jpeg&quot; alt=&quot;LSM 的架構圖.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 我們將每個數據層中的數據標上優先級，由於 LSM-Tree 是 append-only 的，所以優先級越高的數據層中數據越新。&lt;/p&gt; 
&lt;p&gt;所以我們的讀策略也很明顯：按照上圖中 P0 至 P2 依次檢索，這部分代碼實現見 src/storage.zig。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;讀 MemTable&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;// search in memtable
if (try self.state.getMemTable().get(key, value)) {
    if (value.*.len == 0) {
        // tomestone
        return false;
    }
    return true;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;讀 Immutable MemTable&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;// search in imm_memtables

self.state_lock.lockShared();
defer self.state_lock.unlockShared();
for (self.state.imm_mem_tables.items) |imm_table| {
    if (try imm_table.load().get(key, value)) {
        if (value.*.len == 0) {
            // tomestone
            return false;
        }
        return true;
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;讀 LV0~LVmax SSTables&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;// 收集 L0 中的迭代器
var l0_iters = std.ArrayList(StorageIteratorPtr).init(self.allocator);
defer {
    for (l0_iters.items) |iter| {
        var ii = iter;
        ii.release();
    }
    l0_iters.deinit();
}
{
    self.state_lock.lockShared();
    defer self.state_lock.unlockShared();
    for (self.state.l0_sstables.items) |sst_id| {
        const sst = self.state.sstables.get(sst_id).?;
        if (try sst.load().mayContain(key)) {
            var ss_iter = try SsTableIterator.initAndSeekToKey(self.allocator, sst.clone(), key);
            errdefer ss_iter.deinit();
            try l0_iters.append(try StorageIteratorPtr.create(self.allocator, .{ .ss_table_iter = ss_iter }));
        }
    }
}

// 收集 Levels 中的迭代器
var level_iters: std.ArrayList(StorageIteratorPtr) =     ;
{
    self.state_lock.lockShared();
    defer self.state_lock.unlockShared();
    level_iters = try std.ArrayList(StorageIteratorPtr).initCapacity(
        self.allocator,
        self.state.levels.items.len,
    );
    for (self.state.levels.items) |level| {
        var level_ssts = try std.ArrayList(SsTablePtr).initCapacity(self.allocator, level.items.len);
        errdefer level_ssts.deinit();
        for (level.items) |sst_id| {
            const sst = self.state.sstables.get(sst_id).?;
            if (try mayWithinTable(key, sst)) {
                try level_ssts.append(sst.clone());
            }
        }
        if (level_ssts.items.len &amp;gt; 0) {
            var level_iter = try SstConcatIterator.initAndSeekToKey(
                self.allocator,
                level_ssts,
                key,
            );
            errdefer level_iter.deinit();
            try level_iters.append(try StorageIteratorPtr.create(self.allocator, .{ .sst_concat_iter = level_iter }));
        }
    }
}

// 將多個迭代器合併為一個 TwoMergeIterator
var l0_merge_iter = try MergeIterators.init(self.allocator, l0_iters);
errdefer l0_merge_iter.deinit();

var levels_merge_iter = try MergeIterators.init(self.allocator, level_iters);
errdefer levels_merge_iter.deinit();

var iter = try TwoMergeIterator.init(
    try StorageIteratorPtr.create(self.allocator, .{ .merge_iterators = l0_merge_iter }),
    try StorageIteratorPtr.create(self.allocator, .{ .merge_iterators = levels_merge_iter }),
);
defer iter.deinit();

if (iter.isEmpty()) {
    return false;
}

if (std.mem.eql(u8, iter.key(), key) and iter.value().len &amp;gt; 0) {
    value.* = iter.value();
    return true;
}

&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;壓縮&lt;/h2&gt; 
&lt;p&gt;在上一節的寫過程中，我們實現了從內存表到 Level0 的 SSTable 堆疊。&lt;/p&gt; 
&lt;p&gt;隨着寫入的持續，Lv0 的 SSTable 會越來越多，這個時候就需要我們將 Lv0 中的數據合併寫入至 Lv2，並依次類推重複這個過程，直到堆疊到最深的層數，這個逐層合併數據的過程就是&lt;strong&gt;數據壓縮&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//982033f774dbf9693963f20f07b42b5b.jpeg&quot; alt=&quot;數據壓縮.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt; LSM-Tree 中數據壓縮的過程大致如下：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//cb153ea8c68e5038d9c6b02da7837645.jpeg&quot; alt=&quot;過程大致如下.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 具體的實現代碼可見 src/compact.zig，src/storage.zig。&lt;/p&gt; 
&lt;p&gt;簡單分層壓縮與原始 LSM 論文中的壓縮策略相似。它為 LSM 樹維護多個層級。當一個層級太大時，它會將此層級的所有 SST 與下一層合併。壓縮策略由 3 個參數控制：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;size_ratio_percent：【文件低級數量/文件高級數量】，當實際計算的值低於此閾值時觸發壓縮。假設這裏我們設置為 60%，當 L0 中 SST 數量為 2，L1 中 SST 數量為 1，此時 ratio 為 1/2 = 50% &amp;lt; 60%，此時我們應該將 L0 壓縮合並至 L1。&lt;/li&gt; 
 &lt;li&gt;level0_file_num_compaction_trigger: 第一層 SSTable 達到多少後觸發壓縮。因為這是最高層，沒法與更高層比較，只能固定觸發壓縮。&lt;/li&gt; 
 &lt;li&gt;max_levels: 顧名思義，最大的層數限制。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;做好這些準備工作，我們可以逐步實現壓縮邏輯：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;生成壓縮任務：&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;pub const SimpleLeveledCompactionController = struct {
    options: SimpleLeveledCompactionOptions,

    pub fn generateCompactionTask(self: SimpleLeveledCompactionController, state: *storage.StorageState) !?SimpleLeveledCompactionTask {
        if (self.options.max_levels == 1) {
            return null;
        }

        var level_sizes = std.ArrayList(usize).init(state.allocator);
        defer level_sizes.deinit();

        try level_sizes.append(state.l0_sstables.items.len);
        for (state.levels.items) |level| {
            try level_sizes.append(level.items.len);
        }

        // 如果 Lv0 中 SST 數量超出閾值，觸發 L0 級別壓縮
        if (state.l0_sstables.items.len &amp;gt;= self.options.level0_file_num_compaction_trigger) {
            std.debug.print(&quot;compaction of L0 to L1 because L0 has {d} SSTS &amp;gt;= {d}\n&quot;, .{ state.l0_sstables.items.len, self.options.level0_file_num_compaction_trigger });
            return .{
                .upper_level = null,
                .upper_level_sst_ids = try state.l0_sstables.clone(),
                .lower_level = 1,
                .lower_level_sst_ids = try state.levels.items[0].clone(),
                .is_lower_level_bottom = false,
            };
        }

        // 計算 Lv[n+1]/lv[n]，如果比例小於閾值，觸發 Lv[n]級別壓縮
        for (1..self.options.max_levels) |level| {
            const lower_level = level + 1;
            if (level_sizes.items[level] == 0) {
                continue;
            }
            const size_ration = level_sizes.items[lower_level] * 100 / level_sizes.items[level];
            if (size_ration &amp;lt; self.options.size_ration_percent) {
                std.debug.print(&quot;compaction of L{d} to L{d} because L{d} size ratio {d} &amp;lt; {d}\n&quot;, .{ level, lower_level, level, size_ration, self.options.size_ration_percent });
                return .{
                    .upper_level = level,
                    .upper_level_sst_ids = try state.levels.items[level - 1].clone(),
                    .lower_level = lower_level,
                    .lower_level_sst_ids = try state.levels.items[lower_level - 1].clone(),
                    .is_lower_level_bottom = lower_level == self.options.max_levels,
                };
            }
        }

        return null;
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;執行壓縮任務：&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;有了上一小節中讀過程的介紹，多層數據的壓縮過程就很好理解了。&lt;/p&gt; 
&lt;p&gt;例如我們想將 L1 與 L2 的 SSTable 合併壓縮至 L2，我們只需要把 L1 和 L2 的數據放在一起創造一個迭代器，再持續從該迭代器中讀出數據寫入新的 SSTable 中，這個過程保證了新的 SSTable 中數據不重複且有序。&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;fn compactSimple(self: *Self, task: SimpleLeveledCompactionTask) !std.ArrayList(SsTablePtr) {
    if (task.upper_level) |_| {
        var upper_ssts = try std.ArrayList(SsTablePtr).initCapacity(
            self.allocator,
            task.upper_level_sst_ids.items.len,
        );
        var lower_ssts = try std.ArrayList(SsTablePtr).initCapacity(
            self.allocator,
            task.lower_level_sst_ids.items.len,
        );

        self.state_lock.lockShared();
        for (task.upper_level_sst_ids.items) |sst_id| {
            const sst = self.state.sstables.get(sst_id).?;
            try upper_ssts.append(sst.clone());
        }
        for (task.lower_level_sst_ids.items) |sst_id| {
            const sst = self.state.sstables.get(sst_id).?;
            try lower_ssts.append(sst.clone());
        }
        self.state_lock.unlockShared();

        var upper_iter = try SstConcatIterator.initAndSeekToFirst(self.allocator, upper_ssts);
        errdefer upper_iter.deinit();

        var lower_iter = try SstConcatIterator.initAndSeekToFirst(self.allocator, lower_ssts);
        errdefer lower_iter.deinit();

        var iter = try TwoMergeIterator.init(
            try StorageIteratorPtr.create(self.allocator, .{ .sst_concat_iter = upper_iter }),
            try StorageIteratorPtr.create(self.allocator, .{ .sst_concat_iter = lower_iter }),
        );
        defer iter.deinit();
        return self.compactGenerateSstFromIter(&amp;amp;iter, task.is_lower_level_bottom);
    } else {
        // compact l0_sstables to l1_sstables
        // ..... 代碼邏輯大致與上面 LvN 層壓縮一致，只是 Lv0 層的 SSTable 是無序的需要特殊考慮
        return self.compactGenerateSstFromIter(&amp;amp;iter, task.is_lower_level_bottom);
    }
}


fn compactGenerateSstFromIter(self: *Self, iter: *TwoMergeIterator, compact_to_bottom_level: bool) !std.ArrayList(SsTablePtr) {
    var builder: SsTableBuilder = try SsTableBuilder.init(self.allocator, self.options.block_size);
    defer builder.deinit();
    var new_ssts = std.ArrayList(SsTablePtr).init(self.allocator);
    
    // 持續迭代此迭代器
    while (!iter.isEmpty()) {
        // 如果壓縮至最後一層，可以不保留墓碑值 key 了
        if (compact_to_bottom_level) {
            if (iter.value().len &amp;gt; 0) {
                try builder.add(iter.key(), iter.value());
            }
        } else {
            try builder.add(iter.key(), iter.value());
        }
        // 當寫滿一個 SSTable 後，就清空 builder，把寫滿的 SSTable 入列
        if (builder.estimatedSize() &amp;gt;= self.options.target_sst_size) {
            // reset builder
            defer builder.reset() catch unreachable;
            const sst_id = self.getNextSstId();
            const path = try self.pathOfSst(sst_id);
            defer self.allocator.free(path);
            var sst = try builder.build(sst_id, self.block_cache.clone(), path);
            errdefer sst.deinit();

            var sst_ptr = try SsTablePtr.create(self.allocator, sst);
            errdefer sst_ptr.deinit();

            try new_ssts.append(sst_ptr);
        }
        try iter.next();
    }
    // 剩餘的數據單獨一個 SSTable
    if (builder.estimatedSize() &amp;gt; 0) {
        const sst_id = self.getNextSstId();
        const path = try self.pathOfSst(sst_id);
        defer self.allocator.free(path);
        var sst = try builder.build(sst_id, self.block_cache.clone(), path);
        errdefer sst.deinit();
        var sst_ptr = try SsTablePtr.create(self.allocator, sst);
        errdefer sst_ptr.deinit();
        try new_ssts.append(sst_ptr);
    }
    return new_ssts;
}

&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;替換壓縮後的 SST&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;這部分邏輯並不複雜，即刪除此次壓縮任務中的原有兩層數據，用新合併的 SSTable 替換至較低層數據。&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;這裏有個需要注意的點，即壓縮過程是在一個線程中單獨執行的，壓縮過程中 LSM-Tree 的原數據可能發生了改變，所以這裏執行 SSTable 刪除時要注意過濾掉新數據，不能覆蓋了有效數據。&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;併發問題是軟件中的 Bug 集散地！&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;pub fn applyCompactionResult(
    _: SimpleLeveledCompactionController,
    state: *storage.StorageState,
    task: SimpleLeveledCompactionTask,
    output: []usize,
) !std.ArrayList(usize) {
    var files_to_remove = std.ArrayList(usize).init(state.allocator);
    errdefer files_to_remove.deinit();

    if (task.upper_level) |upper_level| {
        // 刪除高層 SSTable 數據，這層數據不會在壓縮過程中變更，放心刪
        std.debug.assert(sliceEquals(
            task.upper_level_sst_ids.items,
            state.levels.items[upper_level - 1].items,
        ));
        try files_to_remove.appendSlice(task.upper_level_sst_ids.items);
        state.levels.items[upper_level - 1].clearAndFree();
    } else {
        // 刪除 L0 數據，需要小心
        try files_to_remove.appendSlice(task.upper_level_sst_ids.items);
        var new_l0_sstables = std.ArrayList(usize).init(state.allocator);
        errdefer new_l0_sstables.deinit();

        {
            var l0_sst_compacted = std.AutoHashMap(usize, struct {}).init(state.allocator);
            defer l0_sst_compacted.deinit();
            for (task.upper_level_sst_ids.items) |sst_id| {
                try l0_sst_compacted.put(sst_id, .{});
            }

            for (state.l0_sstables.items) |sst_id| {
                if (!l0_sst_compacted.remove(sst_id)) { // 不在壓縮任務中的 SST 不能刪除
                    try new_l0_sstables.append(sst_id);
                }
            }
            std.debug.assert(l0_sst_compacted.count() == 0);
        }
        state.l0_sstables.deinit();
        state.l0_sstables = new_l0_sstables;
    }
    // 低層 SSTable 數據，直接刪除
    try files_to_remove.appendSlice(task.lower_level_sst_ids.items);
    state.levels.items[task.lower_level - 1].clearAndFree();
    try state.levels.items[task.lower_level - 1].appendSlice(output);

    return files_to_remove;
}


// sst to remove
var ssts_to_remove = std.ArrayList(SsTablePtr).init(self.allocator);

{
    var new_sst_ids = std.ArrayList(usize).init(self.allocator);
    defer new_sst_ids.deinit();

    self.state_lock.lock();
    defer self.state_lock.unlock();

    for (sstables.items) |sst| {
        const id: usize = @intCast(sst.get().sstId());
        try new_sst_ids.append(id);
        try self.state.sstables.put(id, sst.clone());
    }

    var file_to_remove = try self.compaction_controller.applyCompactionResult(
        &amp;amp;self.state,
        task,
        output.items,
    );
    defer file_to_remove.deinit();

    for (file_to_remove.items) |id| {
        if (self.state.sstables.fetchRemove(id)) |kv| {
            try ssts_to_remove.append(kv.value);
        }
    }
    try self.syncDir();
}

for (ssts_to_remove.items) |sst| {
    const path = try self.pathOfSst(sst.get().sstId());
    defer self.allocator.free(path);
    try std.fs.cwd().deleteFile(path);
}
try self.syncDir();
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;四、總結&lt;/h1&gt; 
&lt;p&gt;我們使用 Zig 語言實現了一個 LSM-Tree 的核心功能，包括 MemTable、SSTable、寫流程、各類 Iterator 與數據壓縮能力。通過這個項目，我收穫了很多心得體會。&lt;/p&gt; 
&lt;h3&gt;瞭解了 LSM-Tree 的核心流程&lt;/h3&gt; 
&lt;p&gt;以往對 LSM 這個數據結構的多層 SST 設計與寫過程早有耳聞，但是讀流程的實現不太理解。這個項目解答了我疑惑很久的讀流程的實現，特別是 MergeIterator 的算法設計非常巧妙。&lt;/p&gt; 
&lt;h3&gt;摸索了個 zig 語言的智能指針&lt;/h3&gt; 
&lt;p&gt;Zig 語言沒有內存安全的保證，為了不想指針亂飛到處泄露，在 Deepseek 的幫助下實現了一個簡單的智能指針，極大降低了內存管理的心智負擔。&lt;/p&gt; 
&lt;h3&gt;工程經驗&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;儘可能多的做 assertion 的工作，可以提前暴露很多 bug。&lt;/li&gt; 
 &lt;li&gt;大型多模塊的項目，一定要寫單元測試，不然出了 bug 無法分塊定位問題。&lt;/li&gt; 
 &lt;li&gt;千萬不要把 IO 過程放在鎖的範圍裏，極大的影響性能！&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;文 / 酒米&lt;/p&gt; 
&lt;p&gt;關注得物技術，每週更新技術乾貨&lt;/p&gt; 
&lt;p&gt;要是覺得文章對你有幫助的話，歡迎評論轉發點贊～&lt;/p&gt; 
&lt;p&gt;未經得物技術許可嚴禁轉載，否則依法追究法律責任。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/u/5783135/blog/17821037</link>
            <guid isPermaLink="false">https://my.oschina.net/u/5783135/blog/17821037</guid>
            <pubDate>Thu, 06 Mar 2025 03:24:00 GMT</pubDate>
            <author>原創</author>
        </item>
        <item>
            <title>Visual Studio Code 1.98 發佈</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#333333&quot;&gt;Visual Studio Code 1.98 已&lt;/span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcode.visualstudio.com%2Fupdates%2Fv1_98&quot; target=&quot;_blank&quot;&gt;發佈&lt;/a&gt;&lt;span style=&quot;color:#333333&quot;&gt;，具體更新內容如下：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcode.visualstudio.com%2Fupdates%2Fv1_98%23_collapsed-mode-for-next-edit-suggestions-preview&quot; target=&quot;_blank&quot;&gt;Next Edit Suggestions（預覽）&lt;/a&gt;&amp;nbsp;- Copilot 預測你可能進行的下一步編輯。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;281&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-019e10d4849c9dd6fc68e2e48607e7bdd4b.gif&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcode.visualstudio.com%2Fupdates%2Fv1_98%23_agent-mode-improvements-experimental&quot; target=&quot;_blank&quot;&gt;Agent mode（預覽）&lt;/a&gt;- Copilot 自主完成任務。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;281&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-518b6c0edf8f8db2235e98351573e6ccdbd.gif&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcode.visualstudio.com%2Fupdates%2Fv1_98%23_notebook-support-in-copilot-edits-preview&quot; target=&quot;_blank&quot;&gt;Copilot Edits for notebooks&lt;/a&gt;&amp;nbsp;- 快速迭代編輯你的 notebooks。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style=&quot;color:#333333&quot;&gt;現在可以使用 Copilot 編輯 notebook 文件，其直觀體驗與編輯代碼文件相同。從頭開始創建新 notebook、修改多個單元格的內容、插入和刪除單元格以及更改單元格類型。此預覽功能在使用數據科學或文檔筆記本時提供了無縫的工作流程。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;260&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-f810400f60fd4fcf741bff71103f68a23b3.gif&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcode.visualstudio.com%2Fupdates%2Fv1_98%23_more-advanced-codebase-search-in-copilot-chat&quot; target=&quot;_blank&quot;&gt;代碼搜索&lt;/a&gt;&amp;nbsp;- 讓 Copilot 找到與你的聊天提示相關的文件。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;當你在 Copilot 聊天查詢中添加 #codebase 時，Copilot 會幫助你在工作區中找到相關代碼，以用於聊天提示。#codebase 現在可以運行文本搜索和文件搜索等工具，從你的工作區中獲取更多上下文。通過&lt;span style=&quot;color:#333333&quot;&gt;設置&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;color:var(--keybinding-foreground)&quot;&gt;github.copilot.chat.codesearch.enabled&amp;nbsp;啓用該功能。&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcode.visualstudio.com%2Fupdates%2Fv1_98%23_terminal-intellisense-preview&quot; target=&quot;_blank&quot;&gt;Terminal IntelliSense（預覽）&lt;/a&gt;&amp;nbsp;- 為你的終端提供豐富的補全支持。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;通過添加補全規範（例如 git）、改進命令行解析以提供更好的建議以及增強文件和文件夾補全功能，大幅改進了 bash、zsh、fish 和 PowerShell 的終端 shell 補全功能。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcode.visualstudio.com%2Fupdates%2Fv1_98%23_peek-references-drag-and-drop-support&quot; target=&quot;_blank&quot;&gt;Drag &amp;amp; drop references&lt;/a&gt;&amp;nbsp;- 在新編輯器中快速打開 peek references。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;329&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-153d8c57cc0e6c208cad9acb97f691fc2a6.gif&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcode.visualstudio.com%2Fupdates%2Fv1_98%23_custom-title-bar-on-linux&quot; target=&quot;_blank&quot;&gt;Linux 自定義標題欄&lt;/a&gt;&amp;nbsp;- 默認啓用對 Linux 的自定義標題欄支持。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img height=&quot;151&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-9e84c065a811004cd37ad56202d7dabd0e8.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;161&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-a2909fabdb45b4fb759242c8dc0ebf83d6f.png&quot; width=&quot;300&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcode.visualstudio.com%2Fupdates%2Fv1_98%23_diagnostics-commit-hook-experimental&quot; target=&quot;_blank&quot;&gt;未解決的診斷（預覽）&lt;/a&gt;&amp;nbsp;- 提交時提示未解決的診斷。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;234&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-153c2b99fc26f5a7171b1b02c51c915a39c.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcode.visualstudio.com%2Fupdates%2Fv1_98%23_discard-untracked-changes-improvements&quot; target=&quot;_blank&quot;&gt;Soft-delete in source control&lt;/a&gt;&amp;nbsp;- 將未跟蹤的文件移至垃圾箱而不是刪除它們。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img height=&quot;285&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-d2ae775ec5b09d887bf514d7c0a6066137e.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcode.visualstudio.com%2Fupdates%2Fv1_98%23_custom-instructions-generally-available&quot; target=&quot;_blank&quot;&gt;自定義指令 GA&lt;/a&gt;&amp;nbsp;- 使用自定義指令來調整 Copilot 以滿足你的需求。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;詳情可&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcode.visualstudio.com%2Fupdates%2Fv1_98&quot; target=&quot;_blank&quot;&gt;查看官方公告&lt;/a&gt;。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/337206/vs-code-1-98-released</link>
            <guid isPermaLink="false">https://www.oschina.net/news/337206/vs-code-1-98-released</guid>
            <pubDate>Thu, 06 Mar 2025 03:14:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
    </channel>
</rss>