<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>開源中國-最新資訊</title>
        <link>https://www.oschina.net/news/project</link>
        <atom:link href="http://8.134.148.166:30044/oschina/news" rel="self" type="application/rss+xml"></atom:link>
        <description>開源中國-最新資訊 - Powered by RSSHub</description>
        <generator>RSSHub</generator>
        <webMaster>contact@rsshub.app (RSSHub)</webMaster>
        <language>en</language>
        <lastBuildDate>Thu, 02 Jan 2025 07:36:58 GMT</lastBuildDate>
        <ttl>5</ttl>
        <item>
            <title>開源日報 | Qwen-VL 大模型全面降價；華為輪值董事長孟晚舟新年致辭；「技術債務就像是倖存者的戰鬥傷痕」；國產 AI 舞台站滿了「90 後天才」</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;歡迎閲讀 OSCHINA 編輯部出品的開源日報，每天更新一期。&lt;/p&gt; 
&lt;h3&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;strong&gt;# 2024.12.31&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;h2&gt;&lt;span style=&quot;color:#27ae60&quot;&gt;今日要聞&lt;/span&gt;&lt;/h2&gt; 
&lt;h3&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.pingwest.com%2Fw%2F301378&quot; target=&quot;_blank&quot;&gt;IBM 計劃收購 HashiCorp，遭英國反壟斷監管機構審查&lt;/a&gt;&lt;/h3&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;p&gt;據 TechCrunch 報道，英國反壟斷監督機構競爭與市場管理局（CMA）已開始調查 IBM 計劃收購雲軟件廠商 HashiCorp 是否會影響競爭。&lt;/p&gt; 
  &lt;p&gt;CMA 週一表示，它將在 1 月 16 日前邀請有關各方就這一併購發表評論。該監管機構暫定 2 月 25 日為最後期限，以決定是批准該交易還是將其提交進一步審查。&lt;/p&gt; 
  &lt;p&gt;IBM 於今年 4 月宣佈同意以約 64 億美元的價格收購 HashiCorp。如果收購繼續進行，將擴大 IBM 在雲計算和人工智能領域的推進力度，並讓該公司獲得 HashiCorp 約 4400 家客户的名冊。&lt;/p&gt; 
  &lt;p&gt;CMA 於 8 月通知 HashiCorp 將對合並進行審查。美國聯邦貿易委員會也在調查這一交易。&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;h3&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.pingwest.com%2Fw%2F301401&quot; target=&quot;_blank&quot;&gt;阿里雲再度降價：Qwen-VL 大模型全面降價&lt;/a&gt;&lt;/h3&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;p&gt;阿里雲今天宣佈，Qwen-VL 大模型全面降價。這是阿里雲本年度的第三輪降價。&lt;/p&gt; 
  &lt;p&gt;Qwen-VL-Plus 模型價格直降 81%，輸入價格僅為 0.0015 元/千 tokens，創下全網最低價格；而更高性能的 Qwen-VL-Max 降價至 0.003 元/千 tokens，降幅達到 85%。根據新的定價，1 元錢可以最多處理大約 600 張 720P 圖片，或者 1700 張 480P 圖片。&lt;/p&gt; 
  &lt;p&gt;Qwen-VL 系列大模型是阿里雲推出的多模態大模型，已成為開源社區最受歡迎的模型之一，具備強大的視覺推理能力。該模型不僅能夠識別不同分辨率和長寬比的圖片，還能理解 20 分鐘以上的長視頻，並具備自主操作手機和機器人等智能體的視覺理解能力。&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;h3&gt;&lt;span style=&quot;color:#245bdb&quot;&gt;&lt;a href=&quot;https://www.oschina.net/news/327292&quot;&gt;智譜深度推理模型 GLM-Zero 預覽版上線&lt;/a&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;div&gt; 
   &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;智譜宣佈發佈本年度最後一個模型 GLM-Zero 的初代版本 GLM-Zero-Preview，這是智譜首個基於擴展強化學習技術訓練的推理模型。&lt;/p&gt; 
   &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;根據介紹，GLM-Zero-Preview 是 GLM 家族中專注於增強 AI 推理能力的模型，擅長處理數理邏輯、代碼和需要深度推理的複雜問題。同基座模型相比，GLM-Zero-Preview 在不顯著降低通用任務能力的情況下，在專家任務能力方面的表現大幅提升，其在 AIME 2024、MATH500 和 LiveCodeBench 評測中，效果與 OpenAI o1-preview 相當。&lt;/p&gt; 
   &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;模型表現如下：&lt;/span&gt;&lt;/p&gt; 
   &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;img height=&quot;247&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-4dd02998897ee2c465a04b26d597ad65ae3.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;h3&gt;&lt;span style=&quot;color:#d83931&quot;&gt;&lt;a href=&quot;https://www.oschina.net/news/327323&quot;&gt;Altman 公佈 OpenAI 2025 年將發佈的技術產品&lt;/a&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span&gt;OpenAI 首席執行官薩姆・奧特曼（Sam Altman）發帖公佈了該公司 2025 年即將發佈的技術產品，分別是：&lt;/span&gt;&lt;/p&gt; 
  &lt;ul style=&quot;list-style-type:disc; margin-left:0; margin-right:0&quot;&gt; 
   &lt;li&gt;&lt;span&gt;AGI（通用人工智能）&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span&gt;Agents（智能體）&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span&gt;更好的 GPT-4o 升級版&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span&gt;更好的記憶存儲&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span&gt;更長的上下文窗口&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span&gt;「Grow up mode」（成人模式）&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span&gt;深度研究特色功能&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span&gt;更好的 Sora 以及更好的個性化定製&lt;/span&gt;&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-8b88947df326c888e90ad352129eac9d6b6.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;h3&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.pingwest.com%2Fw%2F301379&quot; target=&quot;_blank&quot;&gt;華為輪值董事長孟晚舟新年致辭：2024 年是原生鴻蒙關鍵一年，一年走過其它操作系統十多年發展之路&lt;/a&gt;&lt;/h3&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;p&gt;據華為官網顯示，華為輪值董事長孟晚舟今日發佈新年致辭，對客户、生態夥伴、產業鏈夥伴、員工和家屬等表達了感謝。&lt;/p&gt; 
  &lt;p&gt;她在致辭中提到，在萬物智聯的賽道上，2024 年是原生鴻蒙的關鍵一年，鴻蒙生態建設千帆起航。鴻蒙千帆計劃得到了眾多行業夥伴的積極響應，短短一年時間，我們就走過其它操作系統十多年的發展之路，創造了「鴻蒙速度」。&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;hr&gt; 
&lt;h2&gt;&lt;span style=&quot;color:#27ae60&quot;&gt;今日觀察&lt;/span&gt;&lt;/h2&gt; 
&lt;div&gt; 
 &lt;h3&gt;&lt;span style=&quot;color:#ffffff&quot;&gt;&lt;span style=&quot;background-color:#ff9900&quot;&gt;社交觀察&lt;/span&gt;&lt;/span&gt;&lt;/h3&gt; 
 &lt;div&gt; 
  &lt;h4&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fweibo.com%2F3146485692%2FP6Fdbs7pg&quot; target=&quot;_blank&quot;&gt;「全球互聯網上中文內容比例很低」是一個誤讀&lt;/a&gt;&lt;/h4&gt; 
  &lt;p&gt;有人用圖一來説明全球互聯網上中文內容比例很低，只佔 1.4%，實際上這是一個誤讀。我以前説過一次，這個數據統計方法並不是計算文字量或者網頁數量，而是計算使用某種語言的網站數量。&lt;/p&gt; 
  &lt;p&gt;舉個例子，微博網站在這個統計中，只能將樣本數字+1，別管微博上邊有多少中文內容，在這個統計方法中，微博跟萬年沒人看的某些個人站沒有區別，都只算一個網站。同樣是 W3Techs 提供的數據，圖二就很能解釋這個問題，只是中文網站數量少，並不是中文內容少。&lt;/p&gt; 
  &lt;div&gt; 
   &lt;div&gt;
    &lt;img alt=&quot;&quot; height=&quot;766&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-482daac5671878f6a85c04be877532dd134.png&quot; width=&quot;400&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    &lt;img alt=&quot;&quot; height=&quot;463&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-6f886a14ad542db607c9f12aece90f15540.png&quot; width=&quot;400&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
   &lt;/div&gt; 
   &lt;div&gt;
    &amp;nbsp;
   &lt;/div&gt; 
  &lt;/div&gt; 
  &lt;div&gt; 
   &lt;div style=&quot;text-align:right&quot;&gt;
    &lt;span style=&quot;color:#000000&quot;&gt;- 微博&amp;nbsp;&lt;strong&gt;BugOS 技術組&lt;/strong&gt;&lt;/span&gt;
   &lt;/div&gt; 
  &lt;/div&gt; 
  &lt;div&gt; 
   &lt;h4&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fweibo.com%2F2194035935%2FP6DrHa8Ob&quot; target=&quot;_blank&quot;&gt;一個大模型需要多大 GPU 內存才能跑起來的計算公式&lt;/a&gt;&lt;/h4&gt; 
   &lt;p&gt;一個大模型需要多大 GPU 內存才能跑起來的計算公式： M = &amp;nbsp;( (P * 4B) / (32 / Q) &amp;nbsp;) * 1.2&lt;/p&gt; 
   &lt;p&gt;M: 所需的 GPU 顯存，單位是 GB。&lt;br&gt; P: 模型的參數數量。例如，7B 模型有 70 億個參數。&lt;br&gt; 4B: 每個參數佔用的字節數，這裏假設每個參數佔用 4 個字節（通常指 FP32 或 Float32 格式）。&lt;br&gt; 32: 4 個字節等於 32 位。&lt;br&gt; Q: 加載模型時使用的位數。例如，16 位 (FP16/BF16)，8 位 (INT8) 或 4 位 (INT4)。這通常稱為量化。&lt;br&gt; 1.2: 表示額外開銷的係數，通常為 20%。這考慮了除了模型權重之外還需要加載到 GPU 顯存中的其他數據，例如優化器狀態、梯度等。&lt;/p&gt; 
   &lt;p&gt;如使用 FP16 量化加載 Llama 70B 模型，計算過程就是&lt;br&gt; M = &amp;nbsp;( (70,000,000,000 * 4) / (32 / 16) &amp;nbsp;)* 1.2 = 168 GB&lt;/p&gt; 
   &lt;p style=&quot;text-align:right&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;- 微博&lt;strong&gt; 蟻工廠&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
   &lt;h4&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fweibo.com%2F2169039837%2FP7e1GcOVX&quot; target=&quot;_blank&quot;&gt;大模型導航資源&lt;/a&gt;&lt;/h4&gt; 
   &lt;p&gt;分享個大模型導航資源，裏面收集了幾乎全部的模型，具有里程碑意義的論文，排行榜，測試集，訓練框架，部署，應用，書籍等&lt;/p&gt; 
   &lt;p&gt;github.com/Hannibal046/Awesome-LLM&lt;/p&gt; 
   &lt;p style=&quot;text-align:right&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;- 微博&lt;strong&gt; karminski-牙醫&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
   &lt;h4&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fweibo.com%2F1706699904%2FP6mWLnFlv&quot; target=&quot;_blank&quot;&gt;英偉達雖然欠下來了大量的「技術債務」，但在他看來「技術債務就像是倖存者的戰鬥傷痕。」&lt;/a&gt;&lt;/h4&gt; 
   &lt;div&gt; 
    &lt;div&gt; 
     &lt;div&gt; 
      &lt;p&gt;關於先做個垃圾出來，讀《英偉達之芯》又看到了一個好例子：&lt;/p&gt; 
      &lt;p&gt;3dfx 破產之後，一個加入英偉達的員工被英偉達的代碼庫震驚到了，「簡直就像是癌症」「代碼寫得一塌糊塗，開發工具鏈也是一團亂麻，最重要的是，他們對此毫不在意」「他們一心只想着下一塊芯片流片，其他什麼都不顧。」&lt;/p&gt; 
      &lt;p&gt;而之前 3dfx 的工作方式則是追求完美，他在那裏寫出的程序優雅，開發的系統條理清晰、註釋詳盡，但結果卻是一敗塗地。&lt;/p&gt; 
      &lt;p&gt;他給的總結相當精闢，英偉達雖然欠下來了大量的「技術債務」，但在他看來「技術債務就像是倖存者的戰鬥傷痕。」&lt;/p&gt; 
     &lt;/div&gt; 
    &lt;/div&gt; 
   &lt;/div&gt; 
   &lt;p style=&quot;text-align:right&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;- 微博&amp;nbsp;&lt;strong&gt;i 陸三金&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
 &lt;h3&gt;&lt;span style=&quot;color:#ffffff&quot;&gt;&lt;strong&gt;&lt;span style=&quot;background-color:#ff9900&quot;&gt;媒體觀察&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
 &lt;h4&gt;&lt;strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbaijiahao.baidu.com%2Fs%3Fid%3D1819939622972077660%26wfr%3Dspider%26for%3Dpc&quot; target=&quot;_blank&quot;&gt;AI 發展：訓練數據即將遭遇瓶頸&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt; 
 &lt;div&gt; 
  &lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:#222222&quot;&gt;訓練數據即將遭遇的瓶頸已悄然浮現。有研究機構預測，到 2028 年左右，用於訓練 AI 模型的數據集典型規模將達到公共在線文本總估計量的規模。換句話説，AI 可能會在大約 4 年內耗盡訓練數據。與此同時，數據所有者（如報紙出版商）開始打擊對其內容的濫用行為，進一步收緊了訪問權限，這將引發「數據共享」規模上的危機。為此，開發人員必須尋找變通之道。&lt;/span&gt;&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;p style=&quot;text-align:right&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;-&lt;/span&gt;&lt;strong&gt;&lt;span style=&quot;color:#000000&quot;&gt;&amp;nbsp;科技日報&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;h4&gt;&lt;strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fwww.ce.cn%2Fcysc%2Ftech%2Fgd2012%2F202412%2F30%2Ft20241230_39251115.shtml&quot; target=&quot;_blank&quot;&gt;全面擁抱人工智能——訪 360 集團創始人周鴻禕&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:#000000&quot;&gt;我國人工智能大模型具有廣闊發展前景，但要在全球大模型產業競爭中贏得主動，一是要充分發揮我國制度優勢，與國外通用大模型展開競爭；二是充分用好我國工業種類齊全、場景眾多的優勢，將大模型和各種應用場景結合，推動一場新型工業革命，這是實現發展「彎道超車」的關鍵。&lt;/span&gt;&lt;/p&gt; 
 &lt;p style=&quot;margin-left:0px; margin-right:0px; text-align:right&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;-&lt;/span&gt;&lt;strong&gt;&lt;span style=&quot;color:#000000&quot;&gt;&amp;nbsp;經濟日報&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;div&gt; 
  &lt;h4&gt;&lt;strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbaijiahao.baidu.com%2Fs%3Fid%3D1819927511172343210%26wfr%3Dspider%26for%3Dpc&quot; target=&quot;_blank&quot;&gt;國產 AI 舞台，站滿了「90 後天才」&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt; 
  &lt;div&gt; 
   &lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:#222222&quot;&gt;從資本到產業對人才的大手筆搶先押注現狀來看，有關 AI 的比拼，無疑不止算力，而更在於人才。&lt;/span&gt;&lt;/p&gt; 
   &lt;p style=&quot;text-align:right&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;-&lt;strong&gt;&amp;nbsp;科創板日報&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
 &lt;h4&gt;&lt;strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbaijiahao.baidu.com%2Fs%3Fid%3D1819926514777138655%26wfr%3Dspider%26for%3Dpc&quot; target=&quot;_blank&quot;&gt;「國產英偉達」們，扎堆上市&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;因此，GPU 企業想要快速發展，必然離不開資本的助力，衝擊上市仍是「國產英偉達」們獲取資金彈藥的重要途徑。&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;而在等待資本市場的大門開啓之前，它們也需要直面生存的考驗。張建中曾直言，「摩爾線程目標為至少先存活 10 年」。在這場「國產替代」光榮而艱辛的征途中，中國算力企業的競逐才剛剛開始。&lt;/span&gt;&lt;/p&gt; 
 &lt;p style=&quot;margin-left:0px; margin-right:0px; text-align:right&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;- &lt;/span&gt;&lt;strong&gt;&lt;span style=&quot;background-color:#ffffff; color:rgba(0, 0, 0, 0.9)&quot;&gt;財經天下 WEEKLY&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;h4&gt;&lt;strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbaijiahao.baidu.com%2Fs%3Fid%3D1819915034550649526%26wfr%3Dspider%26for%3Dpc&quot; target=&quot;_blank&quot;&gt;冷眼與嘲諷之後，谷歌的 AI 大模型翻盤之路&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt; 
 &lt;div&gt; 
  &lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:#262626&quot;&gt;谷歌正在逐漸奪回大模型競賽的行業關注度和開發者認同，反壟斷大錘還尚未真正落下，谷歌獲得了一個難得的發展窗口來在新的技術革新潮流中暫時站穩腳跟，為下一個人工智能時代真正到來前做好準備。&lt;/span&gt;&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;p style=&quot;text-align:right&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;- &lt;/span&gt;&lt;span style=&quot;color:#252525&quot;&gt;&lt;strong&gt;錦緞研究院&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
 &lt;h4&gt;&lt;strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftech.ifeng.com%2Fc%2F8fjJTSFA8ou&quot; target=&quot;_blank&quot;&gt;AI「爆改」快遞行業的第二年&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt; 
 &lt;div&gt; 
  &lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:#222222&quot;&gt;從簡單的寄件、查件入手，到面向快遞小哥打造「知識庫」、再到幫助完成業務信息的彙總整理，甚至到供應鏈的智慧控制，大模型在快遞行業的能力正在被逐步釋放。選擇私有化部署模型、自研大模型的快遞公司們都相信一點：大模型是值得的長期投資，它在快遞行業的應用上限仍然有一個廣闊空間等待發掘。&lt;/span&gt;&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;p style=&quot;margin-left:0px; margin-right:0px; text-align:right&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;-&lt;strong&gt;&amp;nbsp;光錐智能&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
 &lt;hr&gt; 
 &lt;h2&gt;&lt;span style=&quot;color:#27ae60&quot;&gt;&lt;strong&gt;今日推薦&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt; 
 &lt;h3&gt;&lt;span style=&quot;color:#ffffff&quot;&gt;&lt;strong&gt;&lt;span style=&quot;background-color:#ff9900&quot;&gt;開源項目&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
 &lt;h4&gt;&lt;strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fliriliri%2Faya&quot; target=&quot;_blank&quot;&gt;liriliri/aya&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;333&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-3ebd9d1bb174c2b67c92d4e694388364cd8.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fliriliri%2Faya&quot; target=&quot;_blank&quot;&gt;https://github.com/liriliri/aya&lt;/a&gt;&lt;/u&gt;&lt;/em&gt;&lt;/p&gt; 
 &lt;p style=&quot;color:#1f2328; text-align:start&quot;&gt;&lt;span style=&quot;background-color:#ffffff; color:#333333&quot;&gt;AYA 是一款內置 ADB 並基於其功能編寫用户界面的桌面應用。相比於原始的 ADB 命令行輸入，AYA 安裝傻瓜，功能齊全，全圖形化界面，一鍵操作，極大地提高用户效率。&lt;/span&gt;&lt;/p&gt; 
 &lt;h3&gt;&lt;span style=&quot;color:#ffffff&quot;&gt;&lt;strong&gt;&lt;span style=&quot;background-color:#ff9900&quot;&gt;每日一博&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
 &lt;h4&gt;&lt;strong&gt;&lt;a href=&quot;https://my.oschina.net/u/4939618/blog/16883119&quot; target=&quot;_blank&quot;&gt;網頁多模態建模思考&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span style=&quot;background-color:#ffffff; color:#333333&quot;&gt;本文從網頁理解業務出發，從多模態信息融合，預訓練任務構建角度，探討通用網頁建模方案。首先，指出網頁的特殊性，即從不同觀察視角下，網頁存在富文本、樹形結構，和圖層堆疊三種形態。在此基礎上，對比了多種多模態融合思路的優缺點，給出一種較好的方案。進一步，提出多粒度、多維度的網頁預訓練方案；最後，探索了大模型時代，利用現有多模態模型，低成本的適配到網頁的一種可行思路。&lt;/span&gt;&lt;/p&gt; 
 &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;img alt=&quot;圖片&quot; height=&quot;173&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-42f7e135775e8f8238f20f5e042a576e488.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
 &lt;hr&gt; 
 &lt;div&gt; 
  &lt;h2&gt;&lt;span style=&quot;color:#27ae60&quot;&gt;&lt;strong&gt;開源之聲&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt; 
 &lt;/div&gt; 
 &lt;h3&gt;&lt;span style=&quot;color:#ffffff&quot;&gt;&lt;strong&gt;&lt;span style=&quot;background-color:#ff9900&quot;&gt;用户觀點&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
 &lt;h4&gt;&lt;strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FjS984AtnzvXfNwjPVFakZg&quot; target=&quot;_blank&quot;&gt;最強開源終端模擬器 Ghostty 正式發佈 1.0：原生 UI 體驗、採用 Zig 編寫、速度飛快、支持 Mac 和 Linux、支持 GPU 加速&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt; 
 &lt;blockquote&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 1：什麼玩意？不支持 windows？我今晚就去提 issue，炮轟作者&lt;/span&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 2：不至於，README&amp;nbsp;裏有寫是有計劃支持&amp;nbsp;Windows&amp;nbsp;的。終端模擬器不支持&amp;nbsp;Windows&amp;nbsp;是非常常見的情況&lt;/span&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 3：zig&amp;nbsp;比 rust 吹實在&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 4：只要 C&amp;nbsp;ABⅠ在行業上佔大頭，zig 就永遠實在。zig 直接調用 C 真的很爽！&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 5：完全可以理解，等下就去試試。Who&amp;nbsp;care&amp;nbsp;Windows?&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 6：和 Rust 寫的 Warp 比如何？Zig 應用越來越多，好事。&lt;/span&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 7：Warp&amp;nbsp;性能不太行，輸出多了卡，&amp;nbsp;不知道後續的版本會不會優化&lt;/span&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 8：目前在用 wezterm，感覺真正的 killer&amp;nbsp;feature 是 multiplexing，tmux 快捷鍵記不住。目前看 ghost 沒有 multiplexing，也沒有 tmux&amp;nbsp;integration，期待。&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 9：好吧，我還是用 WinTerm 吧&lt;/span&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 10：你不覺得這玩意反應要慢半拍麼，而且偽開源不讓人放心。&lt;/span&gt; 
      &lt;ul&gt; 
       &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 11：我看不懂源代碼，所以不存在放心與否～&lt;/span&gt;&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 12：不知道跟 wezterm 比起來怎麼樣&lt;/span&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 13：用上了，之前用 wezterm，個人感覺比 wezterm 更簡潔高效。兩個都很好。&lt;/span&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 14：可以替換掉&amp;nbsp;iTerm2 了&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 15：我用 powershell7.5&lt;/span&gt;&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/blockquote&gt; 
 &lt;h4&gt;&lt;strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FLAiX2TcypVQbdG8s9Y2OMA&quot; target=&quot;_blank&quot;&gt;中國 AI 的進步之快，讓美國人開始懷疑現實了&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt; 
 &lt;blockquote&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 1：飄了&lt;/span&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 2：哪來這麼多反思哥反思姐？作為機器學習領域的從業者，國內 ai 領域實際上就是在突飛猛進的發展，海外各類先進模型和理論也至少一半是大陸出海華人的貢獻。現在大環境不好，但不是國內從業者夜以繼日的努力是網民一句話所能掩蓋的！&lt;/span&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 3：當然 DeepSeek 不太一樣的是，它不太缺卡，2021 年就囤了 1 萬張英偉達 A100，那會兒 ChatGPT 還沒影呢，和 Meta 為了元宇宙囤卡卻陰差陽錯的趕上 AI 浪潮很像，DeepSeek 買那麼多卡，是為了做量化交易⋯⋯&lt;/span&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 4：陰差陽錯，就好比你買了一把鍋鏟，本來打算是用來炒菜的，後來發現打老公也挺好使！&lt;/span&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;觀點 5：中美 ai 發展的確差不多，期待落地&lt;/span&gt;&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/blockquote&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;em&gt;&lt;strong&gt;---END---&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2024/0912/150800_DfGR_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/327414</link>
            <guid isPermaLink="false">https://www.oschina.net/news/327414</guid>
            <pubDate>Tue, 31 Dec 2024 11:26:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>RWKV 生態內容徵集大賽（2025 年）</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;很高興告訴大家：RWKV 社區推出&quot;&lt;strong&gt;RWKV 生態內容徵集大賽&lt;/strong&gt; &quot;，此活動在 &lt;strong&gt;2025 年全年內&lt;/strong&gt;公開徵集 RWKV 相關的內容，包括：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;與 RWKV 相關的論文&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;講解 RWKV 的教程，例如文章、視頻、動畫&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;基於 RWKV 的應用&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;我們會根據&lt;strong&gt;內容的質量、新穎度、與 RWKV 的相關度&lt;/strong&gt;，發放生態獎勵：&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;th&gt;獎項&lt;/th&gt; 
   &lt;th&gt;獎金&lt;/th&gt; 
   &lt;th&gt;參考論文&lt;/th&gt; 
   &lt;th&gt;參考教程&lt;/th&gt; 
   &lt;th&gt;參考應用&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;鉑獎&lt;/td&gt; 
   &lt;td&gt;6888 元&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2406.06973&quot; target=&quot;_blank&quot;&gt;RWKV-CLIP&lt;/a&gt;，&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2406.19369&quot; target=&quot;_blank&quot;&gt;RWKV-SAM&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fpoloclub.github.io%2Ftransformer-explainer%2F&quot; target=&quot;_blank&quot;&gt;鉑金教程參考&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FOpenMOSE%2FRWKV-LM-RLHF&quot; target=&quot;_blank&quot;&gt;RWKV-LM-RLHF&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;金獎&lt;/td&gt; 
   &lt;td&gt;4888 元&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2412.19535&quot; target=&quot;_blank&quot;&gt;StyleRWKV&lt;/a&gt;，&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2412.10856&quot; target=&quot;_blank&quot;&gt;RWKV-edge&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2FProfTomYeh%2Fstatus%2F1839706195508208089&quot; target=&quot;_blank&quot;&gt;金獎教程參考&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FOpenMOSE%2FRWKV-Infer&quot; target=&quot;_blank&quot;&gt;RWKV-Infer&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;銀獎&lt;/td&gt; 
   &lt;td&gt;2888 元&lt;/td&gt; 
   &lt;td&gt;---&lt;/td&gt; 
   &lt;td&gt;---&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fshengxia%2FRWKV_Role_Playing&quot; target=&quot;_blank&quot;&gt;RWKV_Role_Playing&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;銅獎&lt;/td&gt; 
   &lt;td&gt;1888 元&lt;/td&gt; 
   &lt;td&gt;---&lt;/td&gt; 
   &lt;td&gt;---&lt;/td&gt; 
   &lt;td&gt;---&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;鐵獎&lt;/td&gt; 
   &lt;td&gt;888 元&lt;/td&gt; 
   &lt;td&gt;---&lt;/td&gt; 
   &lt;td&gt;---&lt;/td&gt; 
   &lt;td&gt;---&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;除了獲獎作品，其他所有&lt;strong&gt;符合條件的投稿&lt;/strong&gt; 均可獲得 &lt;strong&gt;RWKV 周邊&lt;/strong&gt;一套，包括 RWKV T 恤、帆布袋、徽章、冰箱貼各 1 個。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-77625c3ead2e14d462793268eba2d3cb448.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;投稿規則&lt;/h2&gt; 
&lt;h3&gt;RWKV 論文&lt;/h3&gt; 
&lt;p&gt;我們徵集任何與 RWKV 相關的論文，&lt;strong&gt;無論是誰寫的，無論是否中會，只要內容新穎。&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;RWKV 相關的論文&lt;strong&gt;不限發佈平台&lt;/strong&gt;，支持所有可公開查閲的論文平台。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;RWKV 文章&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;RWKV 相關的文章以&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fzhuanlan.zhihu.com%2Fwrite&quot; target=&quot;_blank&quot;&gt;知乎-文章&lt;/a&gt;為默認發佈平台，允許多平台分發。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;在知乎發佈文章時，請添加 &lt;code&gt;rwkv&lt;/code&gt; 話題：&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-e980b9c31863b214fac301b41edaa9fcd3a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;發佈後，應當可以在&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.zhihu.com%2Ftopic%2F27422569%2Fnewest&quot; target=&quot;_blank&quot;&gt;知乎- RWKV 話題&lt;/a&gt;中查看您的投稿。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;投稿的 RWKV 文章需要滿足以下要求：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;文章正文不少於 300 字，每個章節或操作步驟需要有合理的配圖&lt;/li&gt; 
 &lt;li&gt;文章應有合理的結構，示例結構：準備微調數據 -&amp;gt; 微調的調參等配置過程 -&amp;gt; 遇到的問題和解決方案 -&amp;gt; 微調效果&lt;/li&gt; 
 &lt;li&gt;文章語句清晰易懂，內容新穎&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;RWKV 視頻和動畫&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;RWKV 相關的視頻和動畫以 &lt;strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.bilibili.com%2F&quot; target=&quot;_blank&quot;&gt;bilibili&lt;/a&gt;&lt;/strong&gt; 為默認投稿平台。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;在 Bilibili 發佈 RWKV 視頻或動畫時，需要帶上 &lt;code&gt;RWKV&lt;/code&gt; 標籤。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-e5881d6a993bc1f88f77a388cf6aa70284c.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;投稿的 RWKV 視頻和動畫需要滿足以下條件：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;視頻時長不低於 1 分鐘&lt;/li&gt; 
 &lt;li&gt;視頻畫質不低於 720P&lt;/li&gt; 
 &lt;li&gt;視頻音頻無限制，可以是 AI 或真人配音，如無音頻則需要配字幕&lt;/li&gt; 
 &lt;li&gt;內容新穎&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;RWKV 應用&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;RWKV 應用需要開源發佈，以 &lt;strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2F&quot; target=&quot;_blank&quot;&gt;GitHub&lt;/a&gt;&lt;/strong&gt; 為默認發佈平台。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;在 GitHub 開源發佈 RWKV 應用時，需要在 GitHub 倉庫的設置中加上 &lt;code&gt;rwkv&lt;/code&gt; 標籤。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-1cfbe9e8859dff7e3a4dafc4cffd2ec2bda.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;倉庫添加話題後，應當可以在 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Ftopics%2Frwkv%3Fo%3Ddesc%26s%3Dupdated&quot; target=&quot;_blank&quot;&gt;GitHub-rwkv 話題最新項目&lt;/a&gt;中查看您的 RWKV 應用。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;投稿的 RWKV 應用需要滿足以下條件：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;應用需具有清晰的核心功能，並能夠在真實場景中運行，內容新穎&lt;/li&gt; 
 &lt;li&gt;代碼具備可讀性，包含必要的註釋&lt;/li&gt; 
 &lt;li&gt;README 等文檔中包含&lt;strong&gt;依賴版本&lt;/strong&gt; 和&lt;strong&gt;操作步驟&lt;/strong&gt;等用户指南&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;活動規則&lt;/h2&gt; 
&lt;h3&gt;活動時間&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;用户投稿時間：2025 年全年（2025.01.01 ~ 2025.12.31）&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;投稿反饋通道&lt;/h3&gt; 
&lt;p&gt;投稿後，請加入 &lt;strong&gt;RWKV 社區活動&lt;/strong&gt; QQ 羣：858016738 ，聯繫管理員登記投稿。&lt;/p&gt; 
&lt;p&gt;任何關於本活動的疑問，也可以在羣內討論。&lt;/p&gt; 
&lt;h3&gt;評審規則&lt;/h3&gt; 
&lt;p&gt;由彭博等 RWKV 社區核心成員、大模型專家組成評審團，對參賽作品進行評審。&lt;/p&gt; 
&lt;p&gt;評審結果會&lt;strong&gt;在 2025 年每個自然月的下旬公佈&lt;/strong&gt;，作品獎勵會在次月發放。&lt;/p&gt; 
&lt;h3&gt;投稿內容範圍&lt;/h3&gt; 
&lt;p&gt;所有投稿內容需要&lt;strong&gt;與 RWKV 架構或模型相關&lt;/strong&gt;，其中：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;論文&lt;/strong&gt;：基於 RWKV 架構或其變體，在語言、多模態、序列、強化學習等等領域的論文，也包括可解釋性、理論分析、量化壓縮、下游任務等等&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;教程（文章、視頻、動畫等）&lt;/strong&gt;：RWKV 架構解析、代碼解讀、微調案例等教程&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;應用&lt;/strong&gt;：基於 RWKV 架構或模型的應用，例如訓練和推理框架，也包括角色扮演、助手、寫作、遊戲等等具體應用&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;獎品發放方式&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;現金獎勵的幣種為人民幣，以匯款或轉賬方式發出，獎金為含税金額&lt;/li&gt; 
 &lt;li&gt;RWKV 周邊獎勵為實物，以快遞方式發出&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;p&gt;&lt;strong&gt;本活動最終解釋權歸元始智能所有。&lt;/strong&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/327403</link>
            <guid isPermaLink="false">https://www.oschina.net/news/327403</guid>
            <pubDate>Tue, 31 Dec 2024 09:21:00 GMT</pubDate>
            <author>來源: 投稿</author>
        </item>
        <item>
            <title>網頁多模態建模思考</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                    
                                                                                                                                                    &lt;span id=&quot;OSC_h1_1&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;導讀&lt;/h1&gt; 
&lt;p&gt;本文從網頁理解業務出發，從多模態信息融合，預訓練任務構建角度，探討通用網頁建模方案。首先，指出網頁的特殊性，即從不同觀察視角下，網頁存在富文本、樹形結構，和圖層堆疊三種形態。在此基礎上，對比了多種多模態融合思路的優缺點，給出一種較好的方案。進一步，提出多粒度、多維度的網頁預訓練方案；最後，探索了大模型時代，利用現有多模態模型，低成本的適配到網頁的一種可行思路。&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_2&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;01 綜述&lt;/h1&gt; 
&lt;p&gt;網頁本質上是一種超文本，一般由超文本標記語言來定義（例如 HTML）。HTML 是一種基礎技術，常與 CSS、JavaScript 一起被眾多網站用於設計網頁、網頁應用程序以及移動應用程序的用户界面 。網頁瀏覽器內核通過解釋 HTML 文件，通過視覺引擎將其渲染成可視化網頁。&lt;/p&gt; 
&lt;p&gt;由於 HTML 的複雜性特點，使得網頁體現出多模態性，多粒度性，並且這些模態內部存在複雜的對應關係。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;多模態性&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;所謂多模態性，即從不同視角下，網頁體現出不同的形態。從信息載體角度看，它是文本、圖像、視頻等多媒體元素集合。從視覺層面看，它擁有圖層的概念，是各層圖像堆疊起來形成了一張完整的「圖片」。從底層代碼邏輯看，它是一種特殊的類 XML 語言，定義了一棵具有層次關係的樹（dom-tree）。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;多粒度性&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;所謂多粒度性，即網頁無論從哪種模態看，都是由粒度不等的元素組成的。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;以資訊類網頁舉例，從信息載體模態看，網頁由段落組成，段落又由句子組成，句子由 tokens 組成；&lt;/p&gt; 
 &lt;p&gt;從視覺層面，網頁由不同尺寸的圖層，依次堆疊而成；&lt;/p&gt; 
 &lt;p&gt;從底層代碼邏輯看，html 由不同高度以及大小的子樹構成。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;內在的對齊邏輯&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;多種模態的基本元素之間，存在多對多的對齊關係。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;對於 dom-tree 的結點，對應視覺層面的一個圖層，亦可對應着一個或者多個句子&lt;/p&gt; 
 &lt;p&gt;一個句子，可能對應着一個結點，也可能對應着多個結點&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;一個例子：多模態的網頁表示&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-9a68799619487d4d13e1e65530c502edaaa.png&quot; alt=&quot;圖片&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;△語義：句子{圖像、視頻等可文本化）的集合&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-1c057353f56b7591a592066fd456df918a0.png&quot; alt=&quot;圖片&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;△結構：dom 結點構成的樹&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-de73382e2ab59c30101daf9bbaa02660652.png&quot; alt=&quot;圖片&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;△視覺：圖層的疊加（輪廓圖）&lt;/p&gt; 
&lt;p&gt;由於網頁的多模態性，多粒度性，以及潛在對齊關係的特點，使得對網頁的建模，與對富文本的建模思路有着顯著的不同。如果將網頁作為富文本處理，會丟失大量的信息&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;舉一個簡單的例子，一個文本位於網頁的不同位置（譬如正文區域，推薦區域），它的重要性是完全不一樣的。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;而複雜的業務下游應用，例如網頁質量甄別，網頁結構化分塊等，僅依賴文本的語義信息是遠遠不夠的，需要綜合考慮多模態的信息，以及多模態間的對齊信息。&lt;/p&gt; 
&lt;p&gt;下面，結合業界研究和我們的探索，從多模態信息融合、預訓練方案方面展開。最後，探討 LLM 時代，網頁多模態模型的可能的探索方向。&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_3&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;02 多模態多粒度特徵融合&lt;/h1&gt; 
&lt;p&gt;如何將多粒度，多模態的特徵融合，是一個複雜的問題。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;1. 多粒度信息的表示&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;這個部分，業內解決方案較多。&lt;/p&gt; 
&lt;p&gt;對&lt;strong&gt;語義信息建模&lt;/strong&gt;，基於 hierarchical attention 的有較多方案。一種方式，是通過 bert 等編碼器，輸入 tokens，取 CLS 單元輸出作為句子的向量表示；再通過 transformer 結構，輸入句子向量，計算句子間 attention，得到句子以及篇章的稠密向量表示，用於下游任務。&lt;/p&gt; 
&lt;p&gt;對於&lt;strong&gt;結構建模&lt;/strong&gt;，以 html-dom 為基本單元。通過全連接層，融合 bounding box 座標、webkit 提取的 css style 信息等。&lt;/p&gt; 
&lt;p&gt;對於&lt;strong&gt;視覺建模&lt;/strong&gt;，可以基於 vit，以 patch 為基本單元。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2. 多對多對應關係下，多模態信息融合&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;這部分，相關研究主要有四種方案：頂層融合，底層融合、多模態統一建模以及多模態交叉 attention 的方案。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-800824f2678ef96daec39a396908620c116.png&quot; alt=&quot;圖片&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;底層融合即在輸入層將多模態信息對齊後拼接作為 transformer 層的輸入。這種方案對多任務預訓練方案設計（包括任務類型，多任務 loss 權重設計）要求較高，不利於多種模態的信息的平衡。&lt;/p&gt; 
&lt;p&gt;頂層融合即在頂層獲取一個結點或者語句對應的多模態向量，拼接後用於下游分類或迴歸任務。缺點在於，各模態獨立建模的時候，缺少了相關信息交互，不能充分利用多模態之間的對齊信息。&lt;/p&gt; 
&lt;p&gt;多模態統一建模，以 LayoutV2 為例，將文本、圖片信息通過不同的編碼器定長編碼後，輸入統一的 transformer 中。對於多模態綜合理解任務來説，很難在輸入層顯示的注入多模態的對齊信息；網頁結構的單元向量與語義、視覺（網頁輪廓圖）的單元向量亦很難通過淺層的編碼器投影到相同的語義空間內。&lt;/p&gt; 
&lt;p&gt;通過對比，認為在網頁建模場景下，多模態交叉 attention 是一個較好的方案。具體來説，各模態分域表示，域之間相互獨立；通過多模態交叉 attention 層完成多模態間信息交互。DocFormer 提出的 multi-modal attention，Beit3 提出的 MultiWay Transformer 等本質上均為這種思路。&lt;/p&gt; 
&lt;p&gt;在多對多對齊關係下（即一個模態的基本單元，可能對應另外一個模態多個基本單元）。可使用聚合函數（例如 average-pooling，lstm 等），將對應模態上一層 transformer-encode 輸出的對應單元序列，通過聚合操作後變換為定長向量，輸入到多模態 attention 層計算。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-42f7e135775e8f8238f20f5e042a576e488.png&quot; alt=&quot;圖片&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-d458c22d4438625ff4b8a868228cd254a5b.png&quot; alt=&quot;圖片&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_4&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;03 預訓練任務設計&lt;/h1&gt; 
&lt;p&gt;如何設計針對多個模態，不同粒度的任務，以及如何低成本的構建偽標籤，是訓練網頁基座模型的關鍵。&lt;/p&gt; 
&lt;p&gt;分析業務中下游任務的特點，在預訓練階段設計瞭如下 4 個類別的的預訓練任務。 &lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-4623962da781deb0d521bd0b838298fb8b2.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;對於細粒度的語義預訓練場景，借鑑 token 粒度 MLM 的思路，mask 完整的句子，並且通過 decoder 重建句子。&lt;/p&gt; 
&lt;p&gt;對於粗粒度的篇章預訓練場景，結合搜索點擊日誌，mask 掉 title 後，通過 decoder 去噪重建 title 以及生成用户點擊的 query。&lt;/p&gt; 
&lt;p&gt;對於細粒度的 html-dom 粒度預訓練場景，通過 html_tag mask/重建，結點亂序重排進行訓練。&lt;/p&gt; 
&lt;p&gt;對於篇章粒度的結構任務，通過 GPT 等生成頁面類型的偽標籤，作為監督信號訓練。&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_5&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;04 展望：LLM 時代的網頁基座模型探索方向&lt;/h1&gt; 
&lt;p&gt;現階段，多模態大模型發展到新的階段，已經可以將圖片（視頻）、文本通過統一的 decoder 模型處理&amp;nbsp;。如何有效利用已有大模型的能力，低成本適配到網頁，是當前研究的熱點和難點。&lt;/p&gt; 
&lt;p&gt;一個樸素的思想，是將整個 html 源碼輸入給大模型，做進一步 postpretrain 使得模型適配網頁。但是，由於網頁源碼的平均長度非常大（根據我們對百度網頁庫的統計，平均源碼長度在 160k），如果再將節點的樣式以 style 標籤形式注入，源碼長度預計會翻數十倍。面向海量網頁計算極難落地。再者，針對網頁場景下做若干輪 post-pretrain 成本亦很高。&lt;/p&gt; 
&lt;p&gt;一個可行思路是，通過 adaptor 網絡，將網頁 html-dom 的結構、位置以及視覺信息變換到已有多模態大模型的空間中，壓縮成若干定長向量表示。&lt;/p&gt; 
&lt;p&gt;通過 adaptor 網絡與 LLM 聯合訓練，調低 LLM 的學習率（儘量不擾動已有 LLM 的參數，保留 LLM 的泛化性）；通過特殊標籤，注入 adaptor 產出的 tokens 向量，讓 LLM 解釋隱式向量代表的含義，訓練 adaptor 網絡。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;例如，構建 prompt：&lt;/p&gt; 
 &lt;p&gt;以下是一個網頁的 dom 結點表示&amp;lt;STRUCT&amp;gt;adaptor_tokens&amp;lt;/STRUCT&amp;gt;，輸出 css 描述文本：style=&quot;xxxxx&quot;&lt;/p&gt; 
 &lt;p&gt;訓練 adator 網絡，使得產出的 tokens 向量能夠與 LLM 的語義空間打平&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;———— END————&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;推薦閲讀&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247603657%26idx%3D1%26sn%3D6ba08a7cf4a124c94c4cd51786217499%26scene%3D21%23wechat_redirect&quot; target=&quot;_blank&quot;&gt;百度垂搜一站式研發平台演進實踐&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247603628%26idx%3D1%26sn%3Df75ddec65ee183dc0c3d48b7e1fdb1ea%26scene%3D21%23wechat_redirect&quot; target=&quot;_blank&quot;&gt;初探圖譜 Embedding 用於異常檢測（一）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247603616%26idx%3D1%26sn%3D6f18533697c0a083f9c58373ac6b1a85%26scene%3D21%23wechat_redirect&quot; target=&quot;_blank&quot;&gt;AIAPI - 轉向 AI 原生檢索&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247603585%26idx%3D1%26sn%3D1ea31a1565c49bc466ddb99b6d9e63d9%26scene%3D21%23wechat_redirect&quot; target=&quot;_blank&quot;&gt;學校新來了一位 AI 作文老師：能看、會評、還教改寫&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247603457%26idx%3D1%26sn%3Db3a0dcf00cb7a38bf62729c279619824%26scene%3D21%23wechat_redirect&quot; target=&quot;_blank&quot;&gt;搞定十萬卡集羣！貧窮限制了我的想象力…&lt;/a&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/u/4939618/blog/16883119</link>
            <guid isPermaLink="false">https://my.oschina.net/u/4939618/blog/16883119</guid>
            <pubDate>Tue, 31 Dec 2024 08:37:00 GMT</pubDate>
            <author>原創</author>
        </item>
        <item>
            <title>「AI 為伍，重啓征程」2024 OSC 源創會年終盛典在珠海圓滿落幕</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                    
                                                                                                                                                    &lt;div&gt;
  12 月 28 日， 
 &lt;strong&gt;「&lt;/strong&gt; 
 &lt;strong&gt;AI&lt;/strong&gt; 
 &lt;strong&gt; 為伍，重啓征程」2024 OSC 源創會年終盛典&lt;/strong&gt;在珠海嘉遠世紀酒店圓滿落下帷幕。本次活動由開源中國、Gitee 主辦，華為聯合主辦，珠海市香洲區科技和工業信息化局、廣東省科學院珠海產業技術研究院、珠海市軟件行業協會、珠海市科技發展促進會、澳門亞太 IT 協會提供支持。 
&lt;/div&gt; 
&lt;div&gt;
  &amp;nbsp; 
&lt;/div&gt; 
&lt;div&gt;
  本次活動秉承「自由、開放、分享」的宗旨，自開啓報名後就受到了全國各地開發者和 IT 企業的關注，吸引到行業內的頂尖專家、技術領袖和一線開發者積極報名， 
 &lt;strong&gt;現場觀眾達 400&lt;/strong&gt; 
 &lt;strong&gt;餘人，會場座無虛席，參會人數再創新高。&lt;/strong&gt; 
&lt;/div&gt; 
&lt;div&gt;
  &amp;nbsp; 
&lt;/div&gt; 
&lt;div&gt;
  2024 年，源創會走過不同城市，舉辦了 8 場城市沙龍，1 場年終盛典，匯聚上千位開發者、近 70 位優秀講師。與此同時，開源中國和 Gitee AI 社區生態的發展也離不開業界專家與合作伙伴的支持。為了感謝各位合作伙伴的支持與貢獻，本次大會組委會特別頒發 
 &lt;strong&gt;「源創會 2024 年度技術領航者」&lt;/strong&gt; 
 &lt;strong&gt;、&lt;/strong&gt; 
 &lt;strong&gt;「開源中國 2024 年度突出貢獻專家」&lt;/strong&gt; 、 
 &lt;strong&gt;「 Gitee AI 年度最佳合作伙伴」&lt;/strong&gt;三大獎項。 
&lt;/div&gt; 
&lt;div&gt;
  &amp;nbsp; 
&lt;/div&gt; 
&lt;div&gt;
  大會現場巧妙設置了一系列精彩紛呈、趣味盎然的活動，如「可樂滾滾樂」、「展台互動集章」、「尋找神秘人」等小遊戲，讓參會者門在繁忙的學習交流之餘，也能盡情享受活動帶來的歡樂時光。 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;img height=&quot;757&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-60c6f827a4c838c5c35d9126e2b007ab682.png&quot; width=&quot;1140&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;img height=&quot;582&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-ea03f4327186e36e640b473448e4f40f4fe.png&quot; width=&quot;1132&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span id=&quot;OSC_h1_1&quot;&gt;&lt;/span&gt; 
 &lt;h1&gt;&lt;span style=&quot;color:#27ae60&quot;&gt;大會精彩內容集錦&lt;/span&gt;&lt;/h1&gt; 
 &lt;span id=&quot;OSC_h2_2&quot;&gt;&lt;/span&gt; 
 &lt;h2&gt;「AI 為伍，開源同行」主論壇&lt;/h2&gt; 
 &lt;blockquote&gt; 
  &lt;div&gt;
    聚焦開源與大模型技術的融合與發展 
  &lt;/div&gt; 
 &lt;/blockquote&gt; 
 &lt;div&gt; 
  &lt;img height=&quot;625&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-f6288527b46f51a02af66a1db7f29a28fd7.png&quot; width=&quot;942&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
 &lt;/div&gt; 
 &lt;div&gt;
   上午，在「AI 為伍，開源同行」的主論壇現場，華為資深開源工程師李佳偉發表了題為《主流開源軟件原生支持昇騰：大模型訓練與推理的輕鬆之選》的精彩演講，詳細闡述了華為昇騰在對主流開源軟件，諸如 vLLM 、ONNXRuntime 、ollama 、llama.cpp 等進行原生支持方面所取得的顯著進展以及當前的實際狀況，旨在為廣大開發者搭建起更為便捷、高效的大模型訓練與推理平台，助力其在 AI 領域的探索與創新之路更加順暢無阻。 
 &lt;/div&gt; 
 &lt;div&gt;
   李佳偉指出，在當今時代的科技浪潮中，AI 軟件領域正呈現出爆發式增長的強勁態勢，不斷突破傳統邊界，實現着顛覆性的成長與跨越，同時，代碼規模朝着更加精簡高效的方向發展，已成為不可逆轉的趨勢。面對智能計算領域開源軟件如雨後春筍般蓬勃湧現的局面，華為昇騰秉持着開放、包容的態度，誠摯歡迎各路賢才精英踴躍加入，共同挖掘技術潛力，拓展創新邊界，攜手推動 AI 技術邁向新的高峯，為全球科技產業的發展貢獻力量，共繪智能未來的宏偉藍圖。 
 &lt;/div&gt; 
 &lt;div&gt;
   &amp;nbsp; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;img height=&quot;615&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-9bc4705e402f42be1a8d343736695b058c6.png&quot; width=&quot;928&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
 &lt;/div&gt; 
 &lt;div&gt;
   紅帽大中華區首席架構師張家駒帶來題為《大模型技術創新與合作——在人工智能領域擁抱開源價值觀》的分享。步入 AI 時代，開源概念亦需順勢革新，秉持 100% 開源價值觀成為必然要求，這意味着不僅代碼要開源、權重需開放，訓練數據以及訓練方法等方面同樣要實現開源共享。基於這樣的理念，紅帽精心發起了 InstructLab 項目。InstructLab 志在打造一個開放包容的社區平台，讓每一個人都能擁有平等參與大模型開發的契機，使 AI 真正化作普惠大眾的技術力量。 
 &lt;/div&gt; 
 &lt;div&gt;
   &amp;nbsp; 
 &lt;/div&gt; 
 &lt;div&gt; 
  &lt;img height=&quot;603&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-012129eb7b9d5c29e6c8ccb962d26c18c78.png&quot; width=&quot;928&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
 &lt;/div&gt; 
 &lt;div&gt;
   在《大模型在研發安全的應用實踐》的分享中，騰訊代碼安全負責人張棟強調，代碼安全已成為大企業推進安全左移的核心點。傳統代碼安全方案在效率與能力上存在明顯瓶頸，騰訊混元大模型通過其卓越的語義理解與泛化能力，在存量場景中突破傳統能力上限，有效提升高危風險檢出的準確率（質）、檢出數（量）和修復效率。更重要的是在增量場景中，大模型為邏輯類漏洞和自動審計提供了落地的可能，使傳統技術較難解決的複雜問題得以推進，實現從「提質提量」到「擴邊增效」，推動代碼安全實現質的突破與應用領域的拓展。 
 &lt;/div&gt; 
 &lt;div&gt;
   &amp;nbsp; 
 &lt;/div&gt; 
 &lt;div&gt; 
  &lt;img height=&quot;616&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-a015e8fd1330ce4668420c5dfd1b510175c.png&quot; width=&quot;927&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
 &lt;/div&gt; 
 &lt;div&gt;
   螞蟻集團高級算法專家餘航則是分享了 CodeFuse 基座模型。CodeFuse 源於螞蟻自身的開發場景及代碼庫沉澱，基於海量高質量代碼數據和代碼領域特色詞表，以及多任務微調技術 MFT ，已從單環節智能化演進到企業級端到端的研發智能體探索，並開源了多個自研和微調的代碼大模型，總下載量近 200 萬。 
 &lt;/div&gt; 
 &lt;div&gt;
   餘航詳細介紹了，CodeFuse 旗下極具特色的倉庫級代碼圖大模型 CGM，在行業權威的 SWE-Bench Lite 榜單上表現卓越，成功解決了 41.67% 的問題，在競爭激烈的 SWE-Bench Lite 開源榜單中脱穎而出，榮登榜首之位。這一成績的取得，不僅彰顯了 CodeFuse 模型的高超性能與精準能力，更為整個代碼大模型領域樹立了新的標杆，為後續的研究與應用提供了極具價值的參考與借鑑，有望引領行業朝着更加高效、智能的方向發展。 
 &lt;/div&gt; 
 &lt;div&gt;
   &amp;nbsp; 
 &lt;/div&gt; 
 &lt;div&gt; 
  &lt;img height=&quot;610&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-199292b7ce5c5f0cd5d7393c7a66ffc5dc5.png&quot; width=&quot;919&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
 &lt;/div&gt; 
 &lt;div&gt;
   Vivo 高級系統架構專家徐海波在題為《 vivo 藍河操作系統的 AI 技術探索與前沿實踐》的分享。他介紹，BlueOS 藍河操作系統是 vivo 自研面向通用人工智能時代的智能操作系統，具備更智慧的 AI 交互、更流暢的性能、更安全的內核及框架等特點。 
 &lt;/div&gt; 
 &lt;div&gt; 
  &lt;span id=&quot;OSC_h2_3&quot;&gt;&lt;/span&gt; 
  &lt;h2&gt;「GenAI 開發關鍵技術」主論壇&lt;/h2&gt; 
  &lt;blockquote&gt; 
   &lt;div&gt;
     聚焦 GenAI 開發中的關鍵技術 
   &lt;/div&gt; 
  &lt;/blockquote&gt; 
  &lt;div&gt; 
   &lt;img height=&quot;604&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-08cb237f3c5cb587ad110e4b2e7f94810ee.png&quot; width=&quot;897&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
  &lt;/div&gt; 
  &lt;div&gt;
    在下午「GenAI 開發關鍵技術」主論壇中，英飛流創始人兼 CEO 張穎峯發表題為《新一代企業級多模態 RAG 引擎》的演講。張穎峯表示，隨着 LLM 多模態能力的增強，RAG 也需要步入多模態時代，它並不限於對日常圖片，音視頻的檢索增強，還應該涵蓋當下佔據大部分的非結構化文檔，發掘出這些數據的商業價值。 
  &lt;/div&gt; 
  &lt;div&gt;
    &amp;nbsp; 
  &lt;/div&gt; 
  &lt;div&gt; 
   &lt;img height=&quot;618&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-e6a80737e9be486a7d34acaf407bf736f4c.png&quot; width=&quot;913&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
  &lt;/div&gt; 
  &lt;div&gt;
    Gitee 私有云產品總監林靖靖發表《數據智能跟蹤體系的構建》分享，深入闡述了 Gitee DevOps 如何打破信息孤島，形成研發管理全域智能的產品組合，結合企業過程資產庫和研發過程資產信息庫，基於 AI 大模型 multiagents 和 RAG 技術，實現企業組織研發過程智能化、體系化，加速體系成熟，構築智能化軟件工廠。 
  &lt;/div&gt; 
  &lt;div&gt;
    &amp;nbsp; 
  &lt;/div&gt; 
  &lt;div&gt; 
   &lt;img height=&quot;604&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-4cec210f483ef6a15b932c90d0eb7aeed78.png&quot; width=&quot;915&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
  &lt;/div&gt; 
  &lt;div&gt;
    文心快碼 Baidu Comate 架構師徐曉強發佈題為《文心快碼在代碼生成場景下的知識豐富探索與實踐》的演講。為了提供給開發者更加準確的生成結果，文心快碼這兩年不斷豐富上下文的探索，在代碼續寫場景下做「準確度」和「速度」的平衡。也探索了基於 Agent 的代碼改寫能力。隨着模型能力的提升，文心快碼已經能夠在更多場景和更模糊的指令下完成更困難的任務。 
  &lt;/div&gt; 
  &lt;div&gt;
    &amp;nbsp; 
  &lt;/div&gt; 
  &lt;div&gt; 
   &lt;img height=&quot;607&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-10b5d6472c3fa89cd02d73699ad65380c7e.png&quot; width=&quot;922&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
  &lt;/div&gt; 
  &lt;div&gt;
    IDEA 基礎軟件中心高級工程師費浩祥發佈題為《MoonBit 和 AI 的協同設計》的演講。會上，費浩祥為大家介紹了 MoonBit 是如何在編程語言和工具鏈的上針對 AI 代碼生成進行協同設計，並介紹這些設計是如何改善模型的性能，從而幫助 MoonBit 用户完成常見的編碼任務。 
  &lt;/div&gt; 
  &lt;div&gt;
    &amp;nbsp; 
  &lt;/div&gt; 
  &lt;div&gt; 
   &lt;img height=&quot;615&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-7ad18a4d35ede9554bee7a8bde8c201b7e7.png&quot; width=&quot;915&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
  &lt;/div&gt; 
  &lt;div&gt;
    華為開源生態專家楊滔發表《大模型時代的昇騰 AI 》主題分享。楊滔指出，人工智能時代，昇騰基礎軟硬件平台提供從底層算力、算子、框架、套件等層面對人工智能從模型開放到應用的全流程支持。 
  &lt;/div&gt; 
  &lt;div&gt;
    在人工智能框架方面，昇思 MindSpore 持續創新，通過易用性提示，對大模型訓推的支持，擁抱 AI 時代的創新，降低用户開發和應用成本。 
  &lt;/div&gt; 
  &lt;div&gt;
    AI 應用使能套件作為昇騰生態領域的關鍵窗口，專注於賦予開發者圍繞模型的全方位的能力，涵蓋模型訓練與推理一體的高效流程，有力地降低了昇騰硬件開發的技術門檻。 
  &lt;/div&gt; 
  &lt;div&gt;
    &amp;nbsp; 
  &lt;/div&gt; 
  &lt;div&gt; 
   &lt;img height=&quot;616&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-9323e5b39bf9903dfaf010be001731182d1.png&quot; width=&quot;915&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
  &lt;/div&gt; 
  &lt;div&gt;
    矩陣起源研發 VP 趙晨陽在題為《如何利用多模態模型構建適用於 LLM 搜索的數據》的分享中表示，智能體表現好壞依賴於數據，也進一步應證了高質量「知識」對於 LMM 的重要性。隨後，趙晨陽進一步闡述在多模態數據融合階段，更是需要創新性的算法和模型架構，來打破不同模態之間的語義鴻溝，實現數據的有機整合和協同表達。 
  &lt;/div&gt; 
  &lt;div&gt;
    &amp;nbsp; 
  &lt;/div&gt; 
  &lt;div&gt; 
   &lt;img height=&quot;618&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-2158984da12de417f86ea183d0f0092ba11.png&quot; width=&quot;927&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
  &lt;/div&gt; 
  &lt;div&gt;
    Java 開發者應該如何構建 Agent？會上，Spring AI Alibaba 項目負責人劉軍則向大家介紹了基於百鍊模型服務的 AI 應用開發框架「 Spring AI Alibaba 」及其開發框架的架構與基本使用。Spring AI Alibaba 開源項目基於 Spring AI 構建，是阿里雲通義系列模型及服務在 Java AI 應用開發領域的最佳實踐，提供高層次的 AI API 抽象與雲原生基礎設施集成方案，可以幫助開發者快速構建 AI 應用。 
  &lt;/div&gt; 
  &lt;div&gt;
    &amp;nbsp; 
  &lt;/div&gt; 
  &lt;div&gt; 
   &lt;img height=&quot;616&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-e129d7547d9c85e6c0c4a4b7b8d5b4968d3.png&quot; width=&quot;921&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
  &lt;/div&gt; 
  &lt;div&gt;
    Alluxio 首席架構師傅正佳帶來題為《構建大模型時代的高性能 AI 數據底座》的分享。傅正佳介紹，Alluxio 是一個位於數據存儲和計算框架之間，提供數據抽象、統一訪問、分佈式緩存加速、數據親和性調度等功能的開源數據編排平台。Alluxio 通過幫助企業構建大模型時代的高性能 AI 數據底座以應對 I/O 挑戰，提升 AI 算力的效率與性能，被廣泛應用於模型訓練與推理、自動駕駛、AI 製藥、金融量化以及視頻渲染等場景。 
  &lt;/div&gt; 
  &lt;div&gt; 
   &lt;span id=&quot;OSC_h2_4&quot;&gt;&lt;/span&gt; 
   &lt;h2&gt;「昇騰 AI 大模型與應用開發」分論壇&lt;/h2&gt; 
   &lt;blockquote&gt; 
    &lt;div&gt;
      聚焦昇騰 AI 大模型與應用開發 
    &lt;/div&gt; 
   &lt;/blockquote&gt; 
   &lt;div&gt; 
    &lt;img height=&quot;649&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-09f43971f063549241591e17cf938f1dd4a.png&quot; width=&quot;919&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
   &lt;/div&gt; 
   &lt;div&gt;
     在下午的「昇騰 AI 大模型與應用開發」分論壇上，華為昇思生態總監王神迪博士帶來題為《昇思 MindSpore AI 框架使能大模型原生創新》的分享。昇思 MindSpore 作為大模型時代 AI 框架的新選擇，作為中國乃至世界的框架「新勢力」，引領技術創新，加速全面智能化時代到。目前，社區下載量 1000 萬+，社區核心貢獻者 3.5 萬，認證企業數超 1500+ 家。 
   &lt;/div&gt; 
   &lt;div&gt;
     &amp;nbsp; 
   &lt;/div&gt; 
   &lt;div&gt; 
    &lt;img height=&quot;687&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-8d5d0f602b6d8542509a415f14e4183c21c.png&quot; width=&quot;916&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
   &lt;/div&gt; 
   &lt;div&gt;
     華為主任工程師張俊怡發表了題為《昇騰大模型 MindSpeed 訓練加速庫系列介紹》的演講。張俊怡圍繞 MindSpeed 向大家介紹訓練加速庫系列，深入闡釋了其核心技術架構與獨特優勢。MindSpeed 訓練加速庫旨在應對當前人工智能領域對高效、快速訓練日益增長的需求，通過優化算法、改進內存管理以及充分利用硬件並行計算能力等手段，顯著提升了模型訓練的速度與效率。 
   &lt;/div&gt; 
   &lt;div&gt;
     &amp;nbsp; 
   &lt;/div&gt; 
   &lt;div&gt; 
    &lt;img height=&quot;814&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-c0d7b41b6e29813fd062bffb97d3e009cf8.png&quot; width=&quot;1084&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
   &lt;/div&gt; 
   &lt;div&gt;
     華為昇騰生態套件項目架構師潘邵武帶來題為《昇騰生態開發套件，模型訓推新體驗》的分享。為提升昇騰平台的模型開發效率，加速開發者 AI 應用創新，華為計算產品線牽頭開發了 AI 應用使能套件，已適配 LLaMa-Factory 、Stable Diffusion WebUI 等開源生態套件，覆蓋了微調訓練、推理部署、模型評測等模型開發應用全流程。會上，潘邵武圍繞昇騰生態，向大家展示了 AI 應用使能套件生態全景，以及 OpenI 啓智社區所開展的各類活動，希望與廣大開發者共建昇騰 AI 生態。 
   &lt;/div&gt; 
   &lt;div&gt;
     &amp;nbsp; 
   &lt;/div&gt; 
   &lt;div&gt; 
    &lt;img height=&quot;627&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-4b9ffa343341260036dfd60c7ce1a2a362e.png&quot; width=&quot;921&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
   &lt;/div&gt; 
   &lt;div&gt;
     迅龍軟件系統開發工程師徐洋帆為大家帶來題為《香橙派：開源+ AI ，探索無限可能》的分享。徐洋帆介紹，香橙派與華為昇騰目前聯合研發的高算力人工智能產品，包括 OrangePi Alpro、OrangePi Al Studio 等，具有強大的計算能力和高效的運算速度，能滿足市場上各行各業及個人開發者對 AI 推理應用開發的需求，能讓企業以更低的門檻嘗試 A，推動企業的智能化升級。 
   &lt;/div&gt; 
   &lt;div&gt;
     &amp;nbsp; 
   &lt;/div&gt; 
   &lt;div&gt; 
    &lt;img height=&quot;654&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-aca8e06a98a5ebc243f4a2bb5fba04fd894.png&quot; width=&quot;922&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
   &lt;/div&gt; 
   &lt;div&gt;
     魔樂社區負責人、天翼雲專家李寶龍為大家帶來題為《與魔樂一起，繁榮國產 AI 生態》的分享。魔樂社區（Modelers）是全新的人工智能社區，擁有包容的工具鏈體系，已託管和展示昇思、DeepSpeed、AI 應用使能套件等框架或平台。他還表示，魔樂社區堅持走開源、公益的路線，免費、長期支撐應用創新。值得一提的是，魔樂社區對用户制定了成長激勵計劃，鼓勵用户在不同領域深入學習和實踐，從而實現個人和專業上的成長與發展。 
   &lt;/div&gt; 
   &lt;div&gt; 
    &lt;img height=&quot;625&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-935398b1541ccdf5c2d97900708b27e5cc9.png&quot; width=&quot;912&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
   &lt;/div&gt; 
   &lt;div&gt;
     開源中國 Gitee AI 負責人彭博則為大家分享《 Gitee AI 如何在國產算力上構建 Serverless API 及其應用場景》。彭博指出，模型引擎和應用引擎已經暴露出一些問題，如模型引擎體驗失敗率高，應用引擎要編寫跟 GPU 推理相關的代碼門檻高等等。因此 Gitee AI 推出 Serverless API，直接調用 API，無須關心底層的 GPU 推理代碼；同時兼容 OpenAl 接口，門檻低；體驗穩定，部署簡單；按次付費，價格實惠。 
   &lt;/div&gt; 
   &lt;div&gt;
     &amp;nbsp; 
   &lt;/div&gt; 
   &lt;div&gt; 
    &lt;img height=&quot;685&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-e2bd0c60f0082318248c3626e8197027977.png&quot; width=&quot;913&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
   &lt;/div&gt; 
   &lt;div&gt;
     落到具體，情感機器（北京）科技有限公司 AI 生態負責人陳少宏則是為大家帶來題為《 SwanLab+openMind 打造國產 AI 開發者工具鏈》的分享。他介紹，情感機器（北京）科技有限公司是一家專注於人工智能和機器學習底層工具研發的高科技企業。旗下 SwanLab 是一款專為 AI 訓練設計的過程記錄工具，幫助開發者發掘出最具潛力的 AI 模型，將與 AI 應用使能套件共同打造全球領先的人工智能研發工具鏈。 
   &lt;/div&gt; 
   &lt;div&gt;
     &amp;nbsp; 
   &lt;/div&gt; 
   &lt;div&gt;
     在本次 
    &lt;strong&gt;2024 &lt;/strong&gt; 
    &lt;strong&gt;OSC&lt;/strong&gt; 
    &lt;strong&gt; 源創會年終盛典&lt;/strong&gt;的推進過程中，我們心懷無盡感激，向一路同行的贊助商、支持單位、合作伙伴們致以最誠摯的鳴謝。 
   &lt;/div&gt; 
   &lt;ul&gt; 
    &lt;li&gt; 
     &lt;div&gt; 
      &lt;strong&gt;贊助商及支持單位&lt;/strong&gt; 
     &lt;/div&gt; &lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;div&gt; 
    &lt;img height=&quot;526&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-76342f33f173a817318980d182400389d1e.png&quot; width=&quot;904&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
   &lt;/div&gt; 
   &lt;ul&gt; 
    &lt;li&gt; 
     &lt;div&gt; 
      &lt;strong&gt;合作伙伴&lt;/strong&gt; 
     &lt;/div&gt; &lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;div&gt; 
    &lt;img height=&quot;358&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-e25b48eb839592ed279e28923d77b51b260.png&quot; width=&quot;925&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
   &lt;/div&gt; 
   &lt;div&gt;
     &amp;nbsp; 
   &lt;/div&gt; 
   &lt;div&gt; 
    &lt;em&gt;&lt;strong&gt;我們明年源創會再見&lt;/strong&gt;&lt;/em&gt; 
   &lt;/div&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/oschinaofficial/blog/16928110</link>
            <guid isPermaLink="false">https://my.oschina.net/oschinaofficial/blog/16928110</guid>
            <pubDate>Tue, 31 Dec 2024 06:29:00 GMT</pubDate>
            <author>原創</author>
        </item>
        <item>
            <title>smart-chatRoom —— 分佈式簡易聊天系統</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                                                                        
                                                                                    &lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:#40485b&quot;&gt;基於 Redis/RocketMQ + SpringBoot + Vue +Websocket 實現分佈式、跨服務器節點共享的分佈式簡易聊天系統，通過該核心實例實現了跨服務器節點無法共享 WebSocket 會話 Session，無法跨節點查詢用户會話信息的痛點； 藉助 Redis 實現了跨通道 WebSocket 通信，按通道進行會話交流的亮點; 藉助 RocketMQ 實現高併發場景下的會話消費堆積、會話丟失問題。&lt;/span&gt;&lt;/p&gt;

&lt;p style=&quot;color:#252b3a; margin-left:0; margin-right:0; text-align:start&quot;&gt;主要亮點如下：&lt;/p&gt;

&lt;ul style=&quot;margin-left:0; margin-right:11px&quot;&gt;
&lt;li style=&quot;list-style-type:disc&quot;&gt;支持基於 Redis 多通道的訂閲發送，按通道實現 Session 共享&lt;/li&gt;
&lt;li style=&quot;list-style-type:disc&quot;&gt;支持基於 Redis 多通道的消息會話隔離，保證消息交流的安全性&lt;/li&gt;
&lt;li style=&quot;list-style-type:disc&quot;&gt;支持基於 Redis 多通道消息監控、消息存儲，實現離線消息的暫存&lt;/li&gt;
&lt;li style=&quot;list-style-type:disc&quot;&gt;支持基於 RocketMQ 中間件的消息發佈、訂閲，按消息主題區分消息會話&lt;/li&gt;
&lt;li style=&quot;list-style-type:disc&quot;&gt;支持基於 RocketMQ 中間件的消息分類處理，實現按標籤共享 Session、消費 Session&lt;/li&gt;
&lt;li style=&quot;list-style-type:disc&quot;&gt;支持基於 RocketMQ 中間件的消息離線發送，用户上線按照上次消費位點接收離線消息&lt;/li&gt;
&lt;li style=&quot;list-style-type:disc&quot;&gt;支持基於 RocketMQ 中間件的消息監聽，處理重複消息，實現消息發佈的高效率&lt;/li&gt;
&lt;li style=&quot;list-style-type:disc&quot;&gt;支持單聊、羣聊、廣播、按指定通道交流&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

                                                                    &lt;/div&gt;
                                                                </description>
            <link>https://www.oschina.net/p/smart-chatroom</link>
            <guid isPermaLink="false">https://www.oschina.net/p/smart-chatroom</guid>
            <pubDate>Tue, 31 Dec 2024 06:01:00 GMT</pubDate>
        </item>
        <item>
            <title>百度 25 週年李彥宏發全員信：AI 應用將在 2025 年井噴</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;2025 年 1 月 1 日是百度成立 25 週年，百度創始人李彥宏晚間發出全員信表示，「25 年來，我們始終走在技術的最前沿，始終相信技術創新才是百度的核心競爭力。」&lt;/p&gt; 
&lt;p&gt;李彥宏在信中表明瞭對 2025 年的期待，「雖然超級應用尚未出現，但 AI 的實際滲透率已經不低，並且將在 2025 年繼續井噴式增長。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;附全員信原文：&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;各位百度同學，今天是百度成立 25 週年的日子。25 年前的今天，七個懷揣着技術改變世界的夢想的年輕人在中關村北大資源賓館的兩間小屋子裏開始了一段創業旅程。中國互聯網的歷史從此發生了改變。今天，超過一半的中國人每月都要使用百度獲取信息，找到所求，「百度」這兩個原本毫無意義的漢字成了一個家喻户曉的名字。&lt;/p&gt; 
 &lt;p&gt;25 年來，我們不忘初心，風雨兼程，先後經歷了 PC 互聯網時代，移動互聯網時代，現在已經基本進入了人工智能時代。我們從簡單的網頁搜索功能開始，逐步發展出了貼吧、知道、百科、地圖、文庫、網盤等明星產品，我們依託強大的用户基礎，歷時十餘年的時間，逐步打造出了人工智能時代從芯片、框架到模型、應用等四層全棧技術，為中國互聯網和世界 AI 領域培養了一批又一批的科學技術人員、開發者和創業者。&lt;/p&gt; 
 &lt;p&gt;25 年來，我們始終走在技術的最前沿，始終相信技術創新才是百度的核心競爭力，我們多年來一直把超過收入 20% 的資金投入到研發上，並且不遺餘力地嘗試把最前沿的技術產品化，讓更多的人從中受益，因為我們相信只有規模化的應用才能讓技術發揮它的價值，甚至近年來在人工智能方面的實踐表明，重大的技術突破，顛覆式的創新往往是規模化應用的結果，而不是原因。沒有萬卡集羣就不會有大模型的智能湧現，就不會有這次生成式 AI 的浪潮；沒有數以億計的運營公里數，無人駕駛就不可能比有人駕駛安全十倍；沒有大量的 AI 原生應用的推動，國產 AI 芯片就不可能真正成熟！&lt;/p&gt; 
 &lt;p&gt;當然，走在技術的最前沿也意味着我們要冒更大的風險，要承受高於同行的失敗概率，要耐得住寂寞，要忍受別人的不理解甚至白眼，要不斷試錯，要知道哪一天方向走錯了需要迅速調整方向，重新出發，甚至要對自己的能力邊界有清醒的認知，並且不斷總結經驗教訓，以利再戰！&lt;/p&gt; 
 &lt;p&gt;剛剛過去的 2024 年也是過去 25 年的一個縮影，充滿了機遇和挑戰，時而令人興奮，時而令人沮喪，有些工作一直到最後一天才知道成或不成。如同過去一樣，這一年我們堅定地在 AI 技術上探索創新，我們在全球首創了基於圖片的檢索增強技術 iRAG，大大降低了圖片生成的幻覺問題；我們致力於讓不會寫程序的素人具備程序員的能力，為此我們發佈了秒噠，這與全球主流的代碼輔助生成形成鮮明的對比；我們也在大模型應用領域獨樹一幟，為 4000 萬文庫的付費用户提供無與倫比的內容創作和思想碰撞能力！&lt;/p&gt; 
 &lt;p&gt;對於 2025 年，我們充滿期待。我們意識到今天的人工智能領域，競爭比任何時候都更加激烈，技術迭代的速度比以往任何時候都更快，我們面臨的挑戰也是前所未有的。但是我們堅信，大模型賦能的 AI 原生應用正在各行各業各種場景迅速普及，雖然超級應用尚未出現，AI 的實際滲透率已經不低，並且將在 2025 年繼續井噴式增長。我們也期待，我們在 2023、2024 種下的種子能夠在 2025 生根發芽，開花結果，並且不斷獲得市場的驗證和認可。&lt;/p&gt; 
 &lt;p&gt;感謝每一位同學一直以來對百度使命的忠誠陪伴，全情投入，願百度在未來的日子裏，繼續乘風破浪，勇往直前，創造更加輝煌的明天！&lt;/p&gt; 
 &lt;p&gt;Robin&lt;/p&gt; 
&lt;/blockquote&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/327634</link>
            <guid isPermaLink="false">https://www.oschina.net/news/327634</guid>
            <pubDate>Tue, 31 Dec 2024 05:59:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>歡迎 PaliGemma 2 – 來自 Google 的新視覺語言模型</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                    
                                                                                                                                                    &lt;p&gt;我們很高興迎來 Google 全新的視覺語言模型 &lt;strong&gt;PaliGemma 2&lt;/strong&gt;，這是 PaliGemma 的一個新版本。與其前代產品一樣，PaliGemma 2 使用強大的 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fcollections%2Fgoogle%2Fsiglip-659d5e62f0ae1a57ae0e83ba&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;SigLIP&lt;/strong&gt;&lt;/a&gt; 進行視覺處理，但在文本解碼部分升級到了最新的 &lt;strong&gt;Gemma 2&lt;/strong&gt;。&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_1&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;模型規模和輸入分辨率&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;PaliGemma 2 提供了新的預訓練模型，參數規模包括 &lt;strong&gt;3B&lt;/strong&gt; 、 &lt;strong&gt;10B&lt;/strong&gt; 和 &lt;strong&gt;28B&lt;/strong&gt;。所有模型均支持以下多種輸入分辨率:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;224x224&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;448x448&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;896x896&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;這種多樣化的組合為不同的使用場景提供了極大的靈活性，使實踐者能夠根據質量和效率需求之間的平衡進行選擇。與之相比，上一代 PaliGemma 僅提供 &lt;strong&gt;3B&lt;/strong&gt; 版本。&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_2&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;預訓練和微調能力&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;這些預訓練模型被設計為更容易適配下游任務。首個 PaliGemma 模型因其廣泛適配性被社區用於多種任務。本次迭代引入了更高質量的預訓練模型和更多選擇，進一步增強了靈活性。&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_3&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;DOCQI 數據集示例&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;Google 此次發佈了一些基於 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fdatasets%2Fgoogle%2Fdocci&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;DOCCI&lt;/strong&gt;&lt;/a&gt; 數據集的微調模型，展現了長篇、細緻和富有表現力的圖像描述能力。這些微調模型提供 &lt;strong&gt;3B&lt;/strong&gt; 和 &lt;strong&gt;10B&lt;/strong&gt; 兩個版本，支持輸入分辨率 &lt;strong&gt;448x448&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;此次發佈包含了所有開放的模型倉庫、Transformers 框架的集成、微調腳本，以及我們基於 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fdatasets%2FHuggingFaceM4%2FVQAv2&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;VQAv2 數據集&lt;/strong&gt;&lt;/a&gt; 微調的視覺問答模型演示。這些資源為用户提供了全面的工具支持，助力探索和開發更多創新應用。&lt;/p&gt; 
&lt;hr&gt; 
&lt;span id=&quot;OSC_h2_4&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;資源鏈接&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;本次發佈包括開源模型庫、transformers 集成、微調腳本以及視覺問答演示。以下是相關資源鏈接:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fcollections%2Fgoogle%2Fpaligemma-2-release-67500e1e1dbfdd4dee27ba48&quot; target=&quot;_blank&quot;&gt;發佈合集&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fmerveenoyan%2Fsmol-vision%2Fblob%2Fmain%2FFine_tune_PaliGemma.ipynb&quot; target=&quot;_blank&quot;&gt;微調腳本&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fspaces%2Fmerve%2Fpaligemma2-vqav2&quot; target=&quot;_blank&quot;&gt;微調模型演示 Demo&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fpapers%2F2412.03555&quot; target=&quot;_blank&quot;&gt;技術報告&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;span id=&quot;OSC_h1_5&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;&lt;span&gt;PaliGemma 2 介紹&lt;/span&gt;&lt;/h1&gt; 
&lt;p&gt;PaliGemma 2 是 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fblog%2Fpaligemma&quot; target=&quot;_blank&quot;&gt;PaliGemma 視覺語言模型&lt;/a&gt; 的一個新迭代，由 Google 於五月發佈。&lt;/p&gt; 
&lt;p&gt;PaliGemma 2 將強大的 SigLIP 圖像編碼器與 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fblog%2Fgemma2&quot; target=&quot;_blank&quot;&gt;Gemma 2&lt;/a&gt; 語言模型連接起來。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;PaliGemma2 Architecture&quot; src=&quot;https://eastwind-cdn.dongs.xyz/image/202412180054322.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;PaliGemma2 Architecture&lt;/p&gt; 
&lt;p&gt;新的模型基於 &lt;strong&gt;Gemma 2&lt;/strong&gt; 的 &lt;strong&gt;2B&lt;/strong&gt; 、&lt;strong&gt;9B&lt;/strong&gt; 和 &lt;strong&gt;27B&lt;/strong&gt; 語言模型，分別對應 &lt;strong&gt;3B&lt;/strong&gt; 、&lt;strong&gt;10B&lt;/strong&gt; 和 &lt;strong&gt;28B&lt;/strong&gt; 的 PaliGemma 2 變體。這些模型的名稱考慮了緊湊圖像編碼器的附加參數。正如上文所述，這些模型支持三種不同的分辨率，為下游任務的微調提供了很大的靈活性。&lt;/p&gt; 
&lt;p&gt;PaliGemma 2 根據 &lt;strong&gt;Gemma 許可證&lt;/strong&gt; 分發，該許可證允許重新分發、商業使用、微調以及創建模型衍生品。&lt;/p&gt; 
&lt;p&gt;此版本包含以下基於 &lt;strong&gt;bfloat16&lt;/strong&gt; 精度的檢查點:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;9 個預訓練模型&lt;/strong&gt;: 3B、10B 和 28B，分辨率支持&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;224x224&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;448x448&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;896x896&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;2 個在 DOCCI 數據集上的微調模型&lt;/strong&gt;: 基於 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fdatasets%2Fgoogle%2Fdocci&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;DOCCI&lt;/strong&gt;&lt;/a&gt; 數據集 (圖像-文本配對)，支持 &lt;strong&gt;3B&lt;/strong&gt; 和 &lt;strong&gt;10B&lt;/strong&gt; 的 PaliGemma 2 變體，輸入分辨率為 &lt;strong&gt;448x448&lt;/strong&gt;。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id=&quot;OSC_h2_6&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;模型能力&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;如同之前的 PaliGemma 發佈一樣，預訓練 (pt) 模型在下游任務的微調中表現出色。&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_7&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;預訓練數據集&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;pt 模型在以下數據混合集上進行了預訓練。這些多樣化的預訓練數據集使模型能夠在相似領域的下游任務中使用更少的示例進行微調。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;WebLI&lt;/strong&gt;: 一個基於公共網絡構建的大規模多語言圖像 - 文本數據集。WebLI 數據集的多樣化分割使模型具備了多方面的能力，如視覺語義理解、物體定位、視覺文本理解和多語言能力。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;CC3M-35L&lt;/strong&gt;: 從網頁上精心挑選的英語圖像 - 替代文本數據集 (&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Faclanthology.org%2FP18-1238%2F&quot; target=&quot;_blank&quot;&gt;Sharma et al., 2018&lt;/a&gt;)。數據集的標籤通過 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcloud.google.com%2Ftranslate&quot; target=&quot;_blank&quot;&gt;Google Cloud Translation API&lt;/a&gt; 翻譯成了 34 種額外的語言。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Visual Question Generation with Question Answering Validation (VQ2A)&lt;/strong&gt;: 一個改進的問題回答數據集。該數據集也被翻譯成了相同的 34 種語言，使用了 Google Cloud Translation API。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;OpenImages&lt;/strong&gt;: 檢測和物體感知的問答數據集 (Piergiovanni et al., 2022)，通過手動規則生成，基於 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fstorage.googleapis.com%2Fopenimages%2Fweb%2Ffactsfigures_v7.html&quot; target=&quot;_blank&quot;&gt;OpenImages 數據集&lt;/a&gt;。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;WIT&lt;/strong&gt;: 從 Wikipedia 收集的圖像和文本數據集 (Srinivasan et al., 2021)。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;span id=&quot;OSC_h2_8&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;微調模型與基準測試&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;PaliGemma 2 團隊在多種視覺語言理解任務上對 PT 模型進行了內部微調，並提供了這些微調模型的基準測試結果。詳細信息可以在 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fgoogle%2Fpaligemma2-28b-pt-896%23paligemma-2-results-by-model-resolution-and-size&quot; target=&quot;_blank&quot;&gt;模型卡&lt;/a&gt; 和 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fpapers%2F2412.03555&quot; target=&quot;_blank&quot;&gt;技術報告&lt;/a&gt; 中找到。&lt;/p&gt; 
&lt;p&gt;PaliGemma 2 基於 &lt;strong&gt;DOCQI 數據集&lt;/strong&gt; 微調，可以實現多種圖像描述任務，包括文本渲染、捕捉空間關係以及包含世界知識的描述。&lt;/p&gt; 
&lt;hr&gt; 
&lt;span id=&quot;OSC_h2_9&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;性能比較&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;以下表格展示了 DOCQI 微調模型與其他模型的性能對比 (數據來自 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fpapers%2F2412.03555&quot; target=&quot;_blank&quot;&gt;技術報告&lt;/a&gt; 中的 Table 6):&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;th&gt;模型&lt;/th&gt; 
   &lt;th&gt;參數量&lt;/th&gt; 
   &lt;th&gt;字符數 (#char)&lt;/th&gt; 
   &lt;th&gt;句子數 (#sent)&lt;/th&gt; 
   &lt;th&gt;NES ↓&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniGPT-4&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;484&lt;/td&gt; 
   &lt;td&gt;5.6&lt;/td&gt; 
   &lt;td&gt;52.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;mPLUG-Owl2&lt;/td&gt; 
   &lt;td&gt;8B&lt;/td&gt; 
   &lt;td&gt;459&lt;/td&gt; 
   &lt;td&gt;4.4&lt;/td&gt; 
   &lt;td&gt;48.4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;InstructBLIP&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;510&lt;/td&gt; 
   &lt;td&gt;4.0&lt;/td&gt; 
   &lt;td&gt;42.6&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LLAVA-1.5&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;395&lt;/td&gt; 
   &lt;td&gt;4.2&lt;/td&gt; 
   &lt;td&gt;40.6&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;VILA&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;871&lt;/td&gt; 
   &lt;td&gt;8.6&lt;/td&gt; 
   &lt;td&gt;28.6&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PaliGemma&lt;/td&gt; 
   &lt;td&gt;3B&lt;/td&gt; 
   &lt;td&gt;535&lt;/td&gt; 
   &lt;td&gt;8.9&lt;/td&gt; 
   &lt;td&gt;34.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PaLI-5B&lt;/td&gt; 
   &lt;td&gt;5B&lt;/td&gt; 
   &lt;td&gt;1065&lt;/td&gt; 
   &lt;td&gt;11.3&lt;/td&gt; 
   &lt;td&gt;32.9&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;PaliGemma 2&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;3B&lt;/td&gt; 
   &lt;td&gt;529&lt;/td&gt; 
   &lt;td&gt;7.7&lt;/td&gt; 
   &lt;td&gt;28.4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;PaliGemma 2&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;10B&lt;/td&gt; 
   &lt;td&gt;521&lt;/td&gt; 
   &lt;td&gt;7.5&lt;/td&gt; 
   &lt;td&gt;20.3&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr&gt; 
&lt;span id=&quot;OSC_h3_10&quot;&gt;&lt;/span&gt; 
&lt;h3&gt;&lt;span&gt;指標説明:&lt;/span&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;#char&lt;/strong&gt;: 生成的描述中平均字符數。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;#sent&lt;/strong&gt;: 平均句子數。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;NES&lt;/strong&gt;: 非藴含句子數 (數值越低越好)，用於衡量事實不準確性。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;您可以在下面找到 DOCQI 檢查點的部分模型輸出，展示模型的多樣性和靈活性。&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Input Image&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Caption&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img alt=&quot;Image 1&quot; src=&quot;https://eastwind-cdn.dongs.xyz/image/202412180054547.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;折線圖展示了 ImageNet 模型在微調後的 Top-1 準確率表現。圖中有四條不同顏色的線條: 藍色、橙色、綠色和黑色。&lt;strong&gt;藍色線條是四條線中最低的一條&lt;/strong&gt; ，它代表了表現最差的模型結果。&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img alt=&quot;Image 2&quot; src=&quot;https://eastwind-cdn.dongs.xyz/image/202412180054606.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;一張白紙的特寫鏡頭，上面用黑色的文字打印着內容。紙張中間稍微彎曲，文字使用打字機字體呈現。紙張頂部寫着 &quot;&lt;strong&gt;Ashley Hotel West Coast&lt;/strong&gt;&quot;，其下是 &quot;&lt;strong&gt;WiFi Internet Service&lt;/strong&gt;&quot;。再下面是 &quot;&lt;strong&gt;Username: fqpp&lt;/strong&gt;&quot;，最後是 &quot;&lt;strong&gt;Password: aaeu&lt;/strong&gt;&quot;。&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img alt=&quot;Image 3&quot; src=&quot;https://eastwind-cdn.dongs.xyz/image/202412180055484.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;一幅描繪大衞·鮑伊「Ziggy Stardust」造型的壁畫被畫在一面白牆上。壁畫展示了三張並排的面孔，每張都有紅色的頭髮，眼睛上畫着藍色的閃電圖案。面孔的妝容包括藍色眼影、粉紅色腮紅和紅色嘴唇。中間的面孔上方有一個黑色的方形窗口，窗口內用白色文字寫着 &quot;&lt;strong&gt;JAM&lt;/strong&gt;&quot;，字體為藍色。畫面的一側停着一輛銀色汽車。&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img alt=&quot;Image 4&quot; src=&quot;https://eastwind-cdn.dongs.xyz/image/202412180055346.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;從上方俯瞰一張白色大理石台面，枱面上放着四個咖啡杯。左邊有兩個灰色的杯子，左下角有一個白色的杯子，右側則是另一個灰色的杯子。右上角放着一個帶木質底座的金屬水果籃，裏面裝滿了橙子。左邊還有一個裝有水的透明玻璃水壺，畫面中僅顯示了部分內容。&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img alt=&quot;Image 5&quot; src=&quot;https://eastwind-cdn.dongs.xyz/image/202412180055610.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;一張白色書本的特寫，上半部分是白色區域，底部有一條藍色條紋。白色部分印有黑色文字，內容為: &quot;&lt;strong&gt;Visual Concept Learning from User-tagged Web Video&lt;/strong&gt;&quot; 。黑色文字下方有一個白色框，框內包含五張小圖片。最左邊的圖片是一名站在草地中的人，右側緊接的是一張藍色海洋的圖片。&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;span id=&quot;OSC_h2_11&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;演示&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;為了演示效果，Hugging Face 團隊對 &lt;strong&gt;PaliGemma 2 3B&lt;/strong&gt; 模型進行了微調，輸入分辨率為 448x448，數據集使用的是 &lt;strong&gt;VQAv2&lt;/strong&gt; 的一小部分。我們採用了 &lt;strong&gt;LoRA 微調&lt;/strong&gt; 和 &lt;strong&gt;PEFT&lt;/strong&gt; 方法，具體細節將在微調部分進行講解。&lt;/p&gt; 
&lt;p&gt;下面的演示展示了最終結果。您可以自由查看 Space 中的代碼瞭解其工作原理，或者克隆代碼以適配您的自定義微調需求。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://eastwind-cdn.dongs.xyz/image/202412180057439.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_12&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;如何與 Transformers 一起使用&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;您可以使用 🤗 Transformers 庫對 &lt;strong&gt;PaliGemma 2&lt;/strong&gt; 模型進行推理，通過 &lt;strong&gt;PaliGemmaForConditionalGeneration&lt;/strong&gt; 和 &lt;strong&gt;AutoProcessor&lt;/strong&gt; APIs 實現操作。請確保您安裝的 Transformers 版本為 &lt;strong&gt;4.47 或更高&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip&amp;nbsp;install&amp;nbsp;transformers&amp;gt;=4.47
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;在安裝完成後，您可以按照以下示例運行推理。同樣重要的是，請確保遵循用於訓練模型的任務提示格式，以獲得最佳效果:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&lt;span&gt;from&lt;/span&gt;&amp;nbsp;transformers&amp;nbsp;&lt;span&gt;import&lt;/span&gt;&amp;nbsp;AutoProcessor,&amp;nbsp;PaliGemmaForConditionalGeneration
&lt;span&gt;from&lt;/span&gt;&amp;nbsp;PIL&amp;nbsp;&lt;span&gt;import&lt;/span&gt;&amp;nbsp;Image
&lt;span&gt;import&lt;/span&gt;&amp;nbsp;requests

model_id&amp;nbsp;=&amp;nbsp;&lt;span&gt;&quot;google/paligemma2-10b-ft-docci-448&quot;&lt;/span&gt;
model&amp;nbsp;=&amp;nbsp;PaliGemmaForConditionalGeneration.from_pretrained(model_id)
model&amp;nbsp;=&amp;nbsp;model.to(&lt;span&gt;&quot;cuda&quot;&lt;/span&gt;)
processor&amp;nbsp;=&amp;nbsp;AutoProcessor.from_pretrained(model_id)

prompt&amp;nbsp;=&amp;nbsp;&lt;span&gt;&quot;&amp;lt;image&amp;gt;caption&amp;nbsp;en&quot;&lt;/span&gt;
image_file&amp;nbsp;=&amp;nbsp;&lt;span&gt;&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/cats.png&quot;&lt;/span&gt;
raw_image&amp;nbsp;=&amp;nbsp;Image.open(requests.get(image_file,&amp;nbsp;stream=&lt;span&gt;True&lt;/span&gt;).raw).convert(&lt;span&gt;&quot;RGB&quot;&lt;/span&gt;)

inputs&amp;nbsp;=&amp;nbsp;processor(prompt,&amp;nbsp;raw_image,&amp;nbsp;return_tensors=&lt;span&gt;&quot;pt&quot;&lt;/span&gt;).to(&lt;span&gt;&quot;cuda&quot;&lt;/span&gt;)
output&amp;nbsp;=&amp;nbsp;model.generate(**inputs,&amp;nbsp;max_new_tokens=&lt;span&gt;200&lt;/span&gt;)

input_len&amp;nbsp;=&amp;nbsp;inputs[&lt;span&gt;&quot;input_ids&quot;&lt;/span&gt;].shape[&lt;span&gt;-1&lt;/span&gt;]
print(processor.decode(output[&lt;span&gt;0&lt;/span&gt;][input_len:],&amp;nbsp;skip_special_tokens=&lt;span&gt;True&lt;/span&gt;))
&lt;span&gt;#&amp;nbsp;A&amp;nbsp;medium&amp;nbsp;shot&amp;nbsp;of&amp;nbsp;two&amp;nbsp;cats&amp;nbsp;laying&amp;nbsp;on&amp;nbsp;a&amp;nbsp;pile&amp;nbsp;of&amp;nbsp;brown&amp;nbsp;fishing&amp;nbsp;nets.&amp;nbsp;The&amp;nbsp;cat&amp;nbsp;in&amp;nbsp;the&amp;nbsp;foreground&amp;nbsp;is&amp;nbsp;a&amp;nbsp;gray&amp;nbsp;tabby&amp;nbsp;cat&amp;nbsp;with&amp;nbsp;white&amp;nbsp;on&amp;nbsp;its&amp;nbsp;chest&amp;nbsp;and&amp;nbsp;paws.&amp;nbsp;The&amp;nbsp;cat&amp;nbsp;is&amp;nbsp;laying&amp;nbsp;on&amp;nbsp;its&amp;nbsp;side&amp;nbsp;with&amp;nbsp;its&amp;nbsp;head&amp;nbsp;facing&amp;nbsp;the&amp;nbsp;bottom&amp;nbsp;right&amp;nbsp;corner&amp;nbsp;of&amp;nbsp;the&amp;nbsp;image.&amp;nbsp;The&amp;nbsp;cat&amp;nbsp;in&amp;nbsp;the&amp;nbsp;background&amp;nbsp;is&amp;nbsp;laying&amp;nbsp;on&amp;nbsp;its&amp;nbsp;side&amp;nbsp;with&amp;nbsp;its&amp;nbsp;head&amp;nbsp;facing&amp;nbsp;the&amp;nbsp;top&amp;nbsp;left&amp;nbsp;corner&amp;nbsp;of&amp;nbsp;the&amp;nbsp;image.&amp;nbsp;The&amp;nbsp;cat&#39;s&amp;nbsp;body&amp;nbsp;is&amp;nbsp;curled&amp;nbsp;up,&amp;nbsp;its&amp;nbsp;head&amp;nbsp;is&amp;nbsp;slightly&amp;nbsp;turned&amp;nbsp;to&amp;nbsp;the&amp;nbsp;right,&amp;nbsp;and&amp;nbsp;its&amp;nbsp;front&amp;nbsp;paws&amp;nbsp;are&amp;nbsp;tucked&amp;nbsp;underneath&amp;nbsp;its&amp;nbsp;body.&amp;nbsp;There&amp;nbsp;is&amp;nbsp;a&amp;nbsp;teal&amp;nbsp;rope&amp;nbsp;hanging&amp;nbsp;from&amp;nbsp;the&amp;nbsp;fishing&amp;nbsp;net&amp;nbsp;in&amp;nbsp;the&amp;nbsp;top&amp;nbsp;right&amp;nbsp;corner&amp;nbsp;of&amp;nbsp;the&amp;nbsp;image.&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;您還可以使用 transformers 集成中的 &lt;strong&gt;&lt;code&gt;bitsandbytes&lt;/code&gt;&lt;/strong&gt; 來加載具有量化的模型。以下示例使用了 &lt;strong&gt;4-bit &lt;code&gt;nf4&lt;/code&gt;&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&lt;span&gt;from&lt;/span&gt;&amp;nbsp;transformers&amp;nbsp;&lt;span&gt;import&lt;/span&gt;&amp;nbsp;BitsAndBytesConfig

bnb_config&amp;nbsp;=&amp;nbsp;BitsAndBytesConfig(
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;load_in_4bit=&lt;span&gt;True&lt;/span&gt;,
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;bnb_4bit_quant_type=&lt;span&gt;&quot;nf4&quot;&lt;/span&gt;,
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;bnb_4bit_compute_dtype=torch.bfloat16
)
model&amp;nbsp;=&amp;nbsp;PaligemmaForConditionalGeneration.from_pretrained(
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;model_id,
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;quantization_config=bnb_config,
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;device_map={&lt;span&gt;&quot;&quot;&lt;/span&gt;:&lt;span&gt;0&lt;/span&gt;}
)

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;我們快速測試了量化對性能的影響，通過評估一個 3B 微調檢查點在 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fdatasets%2Flmms-lab%2Ftextvqa&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;textvqa&lt;/strong&gt;&lt;/a&gt; 數據集上的表現，使用 224x224 輸入圖像。這是我們在 5,000 個驗證集條目上獲得的結果:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;bfloat16&lt;/strong&gt;，無量化: &lt;strong&gt;60.04%&lt;/strong&gt; 準確率。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;8-bit&lt;/strong&gt;: **59.78%**。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;4-bit&lt;/strong&gt;，使用上面代碼片段中的配置: **58.72%**。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;這些結果非常鼓舞人心！當然，量化對於更大的檢查點更有意義，我們建議您始終在您所使用的領域和任務上測量結果。&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_13&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;微調&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;如果您之前已經微調過 &lt;strong&gt;PaliGemma&lt;/strong&gt;，那麼用於微調 &lt;strong&gt;PaliGemma 2&lt;/strong&gt; 的 API 是相同的，您可以直接使用現有代碼。我們提供了 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fmerveenoyan%2Fsmol-vision%2Fblob%2Fmain%2Fpaligemma.py&quot; target=&quot;_blank&quot;&gt;微調腳本&lt;/a&gt; 和一個 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fmerveenoyan%2Fsmol-vision%2Fblob%2Fmain%2FFine_tune_PaliGemma.ipynb&quot; target=&quot;_blank&quot;&gt;notebook&lt;/a&gt; 來幫助您微調模型，凍結模型部分參數，或應用內存高效的微調技術，如 &lt;strong&gt;LoRA&lt;/strong&gt; 或 &lt;strong&gt;QLoRA&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;我們使用 &lt;strong&gt;LoRA&lt;/strong&gt; 對 PaliGemma 2 模型在 VQAv2 驗證集的一半進行了微調，以供演示。這項任務使用了 &lt;strong&gt;3 塊 A100&lt;/strong&gt; 顯卡 (80GB VRAM)，耗時半小時。&lt;/p&gt; 
&lt;p&gt;您可以在 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fmerve%2Fpaligemma2-3b-vqav2&quot; target=&quot;_blank&quot;&gt;這裏&lt;/a&gt; 找到模型，此外 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fspaces%2Fmerve%2Fpaligemma2-vqav2&quot; target=&quot;_blank&quot;&gt;這個 Gradio 演示&lt;/a&gt; 展示了模型的效果。&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_14&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;&lt;span&gt;結論&lt;/span&gt;&lt;/h1&gt; 
&lt;p&gt;新發布的 &lt;strong&gt;PaliGemma 2&lt;/strong&gt; 比之前的版本更加令人興奮，具有不同的規模以滿足各種需求，並提供更強大的預訓練模型。我們期待看到社區能夠構建出什麼樣的成果！&lt;/p&gt; 
&lt;p&gt;我們感謝 Google 團隊發佈了這一令人驚歎且開放的模型系列。特別感謝 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2FMolbap&quot; target=&quot;_blank&quot;&gt;Pablo Montalvo&lt;/a&gt; 將模型集成到 Transformers 中，以及 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Flysandre&quot; target=&quot;_blank&quot;&gt;Lysandre&lt;/a&gt;、&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2FRaushanTurganbay&quot; target=&quot;_blank&quot;&gt;Raushan&lt;/a&gt;、&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2FArthurZ&quot; target=&quot;_blank&quot;&gt;Arthur&lt;/a&gt;、&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fydshieh&quot; target=&quot;_blank&quot;&gt;Yieh-Dar&lt;/a&gt; 和團隊其他成員的努力，他們迅速完成了模型的評審、測試和合並工作。&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_15&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;資源&lt;/span&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fcollections%2Fgoogle%2Fpaligemma-2-release-67500e1e1dbfdd4dee27ba48&quot; target=&quot;_blank&quot;&gt;發佈合集&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fblog%2Fpaligemma&quot; target=&quot;_blank&quot;&gt;PaliGemma 博客文章&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fmerveenoyan%2Fsmol-vision%2Fblob%2Fmain%2FFine_tune_PaliGemma.ipynb&quot; target=&quot;_blank&quot;&gt;微調腳本&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fmerve%2Fpaligemma2-3b-vqav2&quot; target=&quot;_blank&quot;&gt;在 VQAv2 上微調模型&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fspaces%2Fmerve%2Fpaligemma2-vqav2&quot; target=&quot;_blank&quot;&gt;微調模型演示&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fpapers%2F2412.03555&quot; target=&quot;_blank&quot;&gt;技術報告&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;英文原文: &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhf.co%2Fblog%2Fpaligemma2&quot; target=&quot;_blank&quot;&gt;https://hf.co/blog/paligemma2&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;原文作者: Merve Noyan, Andreas P. Steiner, Pedro Cuenca, Aritra Roy Gosthipaty&lt;/p&gt; 
 &lt;p&gt;譯者: xiaodouzi666&lt;/p&gt; 
&lt;/blockquote&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/HuggingFace/blog/17019541</link>
            <guid isPermaLink="false">https://my.oschina.net/HuggingFace/blog/17019541</guid>
            <pubDate>Tue, 31 Dec 2024 05:39:00 GMT</pubDate>
            <author>原創</author>
        </item>
        <item>
            <title>IBM 收購 HashiCorp 交易面臨英國反壟斷審查</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftechcrunch.com%2F2024%2F12%2F30%2Fuk-antitrust-watchdog-launches-review-of-ibms-hashicorp-takeover%2F&quot; target=&quot;_blank&quot;&gt;據 TechCrunch 報道&lt;/a&gt;，英國反壟斷監督機構競爭與市場管理局（CMA）已開始調查 IBM 計劃收購雲軟件廠商 HashiCorp 是否會影響競爭。&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-699fccb664555c8767ef7615467cd7deac2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;CMA 週一表示，它將在 1 月 16 日前邀請有關各方就這一併購發表評論。該監管機構暫定 2 月 25 日為最後期限，以決定是批准該交易還是將其提交進一步審查。&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;IBM 於今年 4 月宣佈同意以約 64 億美元的價格收購 HashiCorp。如果收購繼續進行，將擴大 IBM 在雲計算和人工智能領域的推進力度，並讓該公司獲得 HashiCorp 約 4400 家客户的名冊。&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;CMA 於 8 月通知 HashiCorp 將對合並進行審查。美國聯邦貿易委員會也在調查這一交易。&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;相關閲讀：&lt;a href=&quot;https://www.oschina.net/news/289379/ibm-acquire-hashicorp-6-4-billion&quot; target=&quot;news&quot;&gt;IBM 以 64 億美元收購 HashiCorp&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/327615/ibms-acquire-hashicorp-takeover-faces-uk-antitrust-scrutiny</link>
            <guid isPermaLink="false">https://www.oschina.net/news/327615/ibms-acquire-hashicorp-takeover-faces-uk-antitrust-scrutiny</guid>
            <pubDate>Tue, 31 Dec 2024 03:59:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>一個大模型需要多大 GPU 內存才能跑起來的計算公式</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;一個大模型需要多大 GPU 內存才能跑起來的計算公式： M = &amp;nbsp;((P * 4B) / (32 / Q) &amp;nbsp;) * 1.2&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li style=&quot;color: rgb(51, 51, 51); margin-left: 0px; margin-right: 0px; text-align: left;&quot;&gt;M: 所需的 GPU 顯存，單位是 GB。&lt;/li&gt; 
 &lt;li style=&quot;color: rgb(51, 51, 51); margin-left: 0px; margin-right: 0px; text-align: left;&quot;&gt;P: 模型的參數數量。例如，7B 模型有 70 億個參數。&lt;/li&gt; 
 &lt;li style=&quot;color: rgb(51, 51, 51); margin-left: 0px; margin-right: 0px; text-align: left;&quot;&gt;4B: 每個參數佔用的字節數，這裏假設每個參數佔用 4 個字節（通常指 FP32 或 Float32 格式）。&lt;/li&gt; 
 &lt;li style=&quot;color: rgb(51, 51, 51); margin-left: 0px; margin-right: 0px; text-align: left;&quot;&gt;32: 4 個字節等於 32 位。&lt;/li&gt; 
 &lt;li style=&quot;color: rgb(51, 51, 51); margin-left: 0px; margin-right: 0px; text-align: left;&quot;&gt;Q: 加載模型時使用的位數。例如，16 位 (FP16/BF16)，8 位 (INT8) 或 4 位 (INT4)。這通常稱為量化。&lt;/li&gt; 
 &lt;li style=&quot;color: rgb(51, 51, 51); margin-left: 0px; margin-right: 0px; text-align: left;&quot;&gt;1.2: 表示額外開銷的係數，通常為 20%。這考慮了除了模型權重之外還需要加載到 GPU 顯存中的其他數據，例如優化器狀態、梯度等。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;如使用 FP16 量化加載 Llama 70B 模型，計算過程就是&lt;br&gt; &lt;strong&gt;M = &amp;nbsp;( (70,000,000,000 * 4) / (32 / 16) &amp;nbsp;)* 1.2 = 168 GB&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-0ecd94df1c281b114f9cbb19306dc7f358e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;——&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fweibo.com%2F2194035935%2FP6DrHa8Ob&quot; target=&quot;_blank&quot;&gt;蟻工廠&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/327612</link>
            <guid isPermaLink="false">https://www.oschina.net/news/327612</guid>
            <pubDate>Tue, 31 Dec 2024 03:53:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>2025 立個 Flag，先賺它一個小目標</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;div&gt; 
 &lt;div&gt;
  嘿，小夥伴們，你們 2025 的「小目標」是啥？買個温馨的 house？牽手心中佳人？賺它一個億？炒掉 boss？還是來一場説走就走的旅行？還是寫出全球最火的開源項目？還是搞定那個讓人頭大的 bug？
  &lt;br&gt; &amp;nbsp;
 &lt;/div&gt; 
 &lt;div&gt;
  別光想，來立個 Flag，告訴全世界！ 現在參與，前 3 位最具創意、最能打動人心的 Flag，將贏得我們精心準備的定製揹包+T 恤套裝 1 套，助你成為開源界的時尚先鋒，互動數（點贊+評論）最多前 6 位將獲得定製揹包一個。
  &lt;br&gt; &amp;nbsp;
 &lt;/div&gt; 
 &lt;div&gt;
  快來
  &lt;a href=&quot;https://www.oschina.net/oscTweet/topic/%232025%E7%AB%8B%E4%B8%AAFlag&quot;&gt;【動彈】&lt;/a&gt;發佈你的 Flag，發佈在話題
  &lt;a href=&quot;https://www.oschina.net/oscTweet/topic/%232025%E7%AB%8B%E4%B8%AAFlag&quot;&gt;#2025 立個 Flag#&lt;/a&gt;下，讓全世界都看到你的決心和勇氣！ 記得，Flag 立得響，獎品拿得爽，説不定還能吸引一羣志同道合的夥伴，一起賺它一個小目標呢！
  &lt;br&gt; 
  &lt;br&gt; 別讓 Flag 只是説説而已，行動起來，讓 2025 因你而精彩！PC 或者掃碼下載 APP 參與。
 &lt;/div&gt; 
 &lt;div&gt;
  &lt;img alt=&quot;&quot; height=&quot;300&quot; src=&quot;https://static.oschina.net/uploads/space/2024/1231/114955_fkkn_7282530.png&quot; width=&quot;300&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
  &lt;br&gt; 
  &lt;strong&gt;活動時間：&lt;/strong&gt;2024 年 12 月 31 日~2025 年 1 月 15 日
 &lt;/div&gt; 
&lt;/div&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/327335</link>
            <guid isPermaLink="false">https://www.oschina.net/news/327335</guid>
            <pubDate>Tue, 31 Dec 2024 03:53:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>X.Org Server 的代碼提交次數創 10 年新高</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.phoronix.com%2Fmisc%2Fxserver-eoy-2024%2Findex.html&quot; target=&quot;_blank&quot;&gt;根據 X.Org Server 的&amp;nbsp;&lt;/a&gt;&lt;span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.phoronix.com%2Fmisc%2Fxserver-eoy-2024%2Findex.html&quot; target=&quot;_blank&quot;&gt;Git 提交記錄&lt;/a&gt;，在剛剛過去的 2024 年，&lt;/span&gt;X.Org Server 的代碼&lt;span&gt;提交次數達到了 2014 年以來的最高峯。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;雖然提交次數比前幾年多了不少，但這並不意味着 &lt;/span&gt;X.Org Server&amp;nbsp;&lt;span&gt;的復興，因為 Wayland 仍在 Linux 桌面上佔據主導地位。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-d93cec2031e1ac95173fc30bde92b9e8801.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;據統計，X.Org Server 去年有 708 次提交... 比起 2018 年的 535 次提交（每年 200~300 次）要多得多。但即使是在 2010 年代中期，Wayland 還在積極開發的時候，每年也只有 400~500 次提交... 在 2024 年之前，提交次數最多的是 2014 年，當時有 952 次提交。&lt;/p&gt; 
&lt;p&gt;按行數計算，2024 年 X.Org Server 新增了 11998 行代碼，刪除了 14680 行代碼。這比近幾年每年 X.Org Server 代碼庫中通常 5-6 千行代碼的變化要多。但仍遠低於 X.Org Server 積極開發的 2000 年代更高的代碼更新量。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-a555233d997eb149ec68424bd0390abacf2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;2024 年的 X.Org Server 之所以如此活躍主要有兩個原因。&lt;/p&gt; 
&lt;p&gt;首先，X.Org Server 中的 XWayland 代碼仍在繼續積極開發，用於支持新的 Wayland 協議和其他修復/添加內容... XWayland 是 X.Org Server 中繼續開發新功能的主要領域，也是唯一的領域。&lt;/p&gt; 
&lt;p&gt;另一個原因是開源開發者 Enrico Weigelt。&lt;/p&gt; 
&lt;p&gt;Enrico Weigelt 主要負責 X.Org Server 的修復和改進工作，圍繞 X.Org Server 的測試和更好的 CI BSD 覆蓋範圍。在紅帽或英特爾等主要廠商都沒有投資 X.Org Server 開發的情況下，Enrico 幾乎是單槍匹馬地完成了一些 X.Org Server 修復和其他小功能工作。&lt;/p&gt; 
&lt;p&gt;Enrico Weigelt 負責了今年 X.Org Server&amp;nbsp; 63% 的&amp;nbsp;&lt;span&gt;Git 提交... 其他活躍的開發者包括 Olivier Fourdan、Michel Dänzer、Alan Coopersmith、Peter Hutterer 和 Erik Kurzinger，他們大多隻關注 XWayland 的改進/修正。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;除此之外，去年 X.Org Server 的許多提交都是為了安全修復。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-b87e1a0baefae75b023d31ef6fe6bafe102.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0102/111918_vgsn_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;就代碼行數而言，X.Org Server 多年來基本處於停滯狀態，唯一的主要功能工作是圍繞 XWayland 進行的。&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.phoronix.com%2Fmisc%2Fxserver-eoy-2024%2Findex.html&quot; target=&quot;_blank&quot;&gt;點此查看 X.Org Server 的更多 GitStats 數據&lt;/a&gt;。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/327607/xorg-server-2024-gitstats</link>
            <guid isPermaLink="false">https://www.oschina.net/news/327607/xorg-server-2024-gitstats</guid>
            <pubDate>Tue, 31 Dec 2024 03:19:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>Altman 公佈 OpenAI 2025 年將發佈的技術產品</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span&gt;OpenAI 首席執行官薩姆·奧特曼（Sam Altman）發帖公佈了該公司 2025 年即將發佈的技術產品，分別是：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span&gt;AGI（通用人工智能）、Agents（智能體）、更好的 GPT-4o 升級版、更好的記憶存儲、更長的上下文窗口、「Grow up mode」（成人模式）、深度研究特色功能、更好的 Sora 以及更好的個性化定製。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img height=&quot;418&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-8b88947df326c888e90ad352129eac9d6b6.jpg&quot; width=&quot;300&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/327323</link>
            <guid isPermaLink="false">https://www.oschina.net/news/327323</guid>
            <pubDate>Tue, 31 Dec 2024 03:09:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>用 BISHENG 構建企業 LLM 應用，簡直不要太簡單！</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                    
                                                                                                                                                    &lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:#333333&quot;&gt;現在大模型 workflow 產品有很多，比如 BISHENG 、MaxKB、Dify、FastGPT、RagFlow 等等。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:#333333&quot;&gt;其中，畢昇 BISHENG 是一個開源的&lt;/span&gt;&lt;span style=&quot;background-color:#ffffff; color:#40485b&quot;&gt;大模型應用開發平台，專門&lt;/span&gt;&lt;span style=&quot;background-color:#ffffff; color:#333333&quot;&gt;面向企業場景，具備高精度文檔解析 ETL4LLM 能力。自去年 8 月份開源以來， GitHub 上的 Star 數已經超過 9k 了。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;background-color:#ffffff; color:#333333&quot;&gt;對於 AI 小白來説，畢昇 BISHENG &lt;/span&gt;平台也很友好。&lt;/strong&gt;因為平台上有很多現成的模板，開發一個大模型應用簡直不要太簡單！編程、開發測試、合同審核、招投標、高級翻譯、會議紀要等等，只需要根據自己需求，調整參數就能用。全程都是可視化界面操作。&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;img height=&quot;788&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-f6995a9ece3a8a4f407abd152f1b17f921d.png&quot; width=&quot;1015&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;1 月 3 日，OSCHINA 開源中國將邀請 BISHENG 產品負責人魯力，做客直播欄目《開源項目老牌與新秀》第 3 期，分享其對 BISHENG—— 大模型 workflow 產品設計思考、技術方案。談一談它，與 Dify、Coze 有什麼差異。同時，還將分享他在企業大模型應用落地過程中踩過的那些坑。&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;strong&gt;本次直播中有一個重點環節，就是實操演示 —— 手把手教你怎麼用 BISHENG 搭建各種大模型應用，例如合同審核、報告生成等。想體驗的小夥伴可以點擊鏈接：&lt;/strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbisheng.dataelem.com%2F&quot; target=&quot;_blank&quot;&gt;https://bisheng.dataelem.com/&lt;/a&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;微信掃碼，趕緊預約直播吧~&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;img height=&quot;2388&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-d72821bf78b0b3f33f49b2d818702279e20.png&quot; width=&quot;600&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;strong&gt;另外，本次直播得到了諸多社區或組織的大力支持，在此特別表示感謝：&lt;/strong&gt;&lt;/p&gt; 
&lt;div style=&quot;text-align:left&quot;&gt; 
 &lt;div&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;strong&gt;Gitee&lt;/strong&gt;&lt;/p&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;Gitee（碼雲）是開源中國於 2013 年推出的基於 Git 的代碼託管平台、企業級研發效能平台，提供中國本土化的代碼託管服務。&lt;/p&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;目前，Gitee 已經有超過 1200 萬名開發者，累計託管超過 2800 萬個代碼倉庫，是中國境內規模最大的代碼託管平台。同時，旗下企業級 DevOps 研發效能管理平台 Gitee 企業版已服務超過 30 萬家企業。&lt;/p&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;網址：&lt;a href=&quot;https://gitee.com/&quot;&gt;https://gitee.com/&lt;/a&gt;&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;渠成開源社區&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;span&gt;&lt;span&gt;渠成開源社區由禪道項目管理軟件團隊發起，社區的經營主體為青島渠成開源計算機網絡技術研究中心，是非營利性社會服務活動的社會組織。 渠成開源社區主要面向一線開源軟件生產者、貢獻者、組織者、贊助商和用户，以解決具體實際問題為宗旨，旨在打造以開源軟件為核心紐帶的開源生態系統，真正做到讓每一個優秀的開源軟件都能實現商業化。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;span&gt;&lt;span&gt;官網：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fwww.qucheng.cc&quot; target=&quot;_blank&quot;&gt;www.qucheng.cc&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;hr&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;「&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;開源項目老牌與新秀&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;」&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;是開源中國 OSCHINA 推出的一檔直播欄目，旨在為開源項目提供一個展示平台，每週五晚上開播&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;欄目邀請開源項目的作者、核心團隊成員或資深用户作為嘉賓，通過路演式直播分享項目的亮點和經驗，有助於提高項目的知名度，吸引更多的用户和開發者關注。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span&gt;&lt;span style=&quot;background-color:#ffffff&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;如果你手上也有好的開源項目，想要跟同行交流分享，歡迎聯繫我，欄目隨時開放～&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:center&quot;&gt;&lt;img height=&quot;537&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-aeeec97bc896bc61c715698bd6acc1f83b3.jpg&quot; width=&quot;400&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/u/3859945/blog/16971110</link>
            <guid isPermaLink="false">https://my.oschina.net/u/3859945/blog/16971110</guid>
            <pubDate>Tue, 31 Dec 2024 02:55:00 GMT</pubDate>
            <author>原創</author>
        </item>
        <item>
            <title>用 BISHENG 構建企業 LLM 應用，簡直不要太簡單！</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                    
                                                                                                                                                    &lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:#333333&quot;&gt;現在大模型 workflow 產品有很多，比如 BISHENG 、MaxKB、Dify、FastGPT、RagFlow 等等。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:#333333&quot;&gt;其中，畢昇 BISHENG 是一個開源的&lt;/span&gt;&lt;span style=&quot;background-color:#ffffff; color:#40485b&quot;&gt;大模型應用開發平台，專門&lt;/span&gt;&lt;span style=&quot;background-color:#ffffff; color:#333333&quot;&gt;面向企業場景，具備高精度文檔解析 ETL4LLM 能力。自去年 8 月份開源以來， GitHub 上的 Star 數已經超過 9k 了。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;background-color:#ffffff; color:#333333&quot;&gt;對於 AI 小白來説，畢昇 BISHENG &lt;/span&gt;平台也很友好。&lt;/strong&gt;因為平台上有很多現成的模板，開發一個大模型應用簡直不要太簡單！編程、開發測試、合同審核、招投標、高級翻譯、會議紀要等等，只需要根據自己需求，調整參數就能用。全程都是可視化界面操作。&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;img height=&quot;788&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-f6995a9ece3a8a4f407abd152f1b17f921d.png&quot; width=&quot;1015&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;1 月 3 日，OSCHINA 開源中國將邀請 BISHENG 產品負責人魯力，做客直播欄目《開源項目老牌與新秀》第 3 期，分享其對 BISHENG—— 大模型 workflow 產品設計思考、技術方案。談一談它，與 Dify、Coze 有什麼差異。同時，還將分享他在企業大模型應用落地過程中踩過的那些坑。&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;strong&gt;本次直播中有一個重點環節，就是實操演示 —— 手把手教你怎麼用 BISHENG 搭建各種大模型應用，例如合同審核、報告生成等。想體驗的小夥伴可以點擊鏈接：&lt;/strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbisheng.dataelem.com%2F&quot; target=&quot;_blank&quot;&gt;https://bisheng.dataelem.com/&lt;/a&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;微信掃碼，趕緊預約直播吧~&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;img height=&quot;2388&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-d72821bf78b0b3f33f49b2d818702279e20.png&quot; width=&quot;600&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;strong&gt;另外，本次直播得到了諸多社區或組織的大力支持，在此特別表示感謝：&lt;/strong&gt;&lt;/p&gt; 
&lt;div style=&quot;text-align:left&quot;&gt; 
 &lt;div&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;strong&gt;Gitee&lt;/strong&gt;&lt;/p&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;Gitee（碼雲）是開源中國於 2013 年推出的基於 Git 的代碼託管平台、企業級研發效能平台，提供中國本土化的代碼託管服務。&lt;/p&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;目前，Gitee 已經有超過 1200 萬名開發者，累計託管超過 2800 萬個代碼倉庫，是中國境內規模最大的代碼託管平台。同時，旗下企業級 DevOps 研發效能管理平台 Gitee 企業版已服務超過 30 萬家企業。&lt;/p&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;網址：&lt;a href=&quot;https://gitee.com/&quot;&gt;https://gitee.com/&lt;/a&gt;&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;渠成開源社區&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;span&gt;&lt;span&gt;渠成開源社區由禪道項目管理軟件團隊發起，社區的經營主體為青島渠成開源計算機網絡技術研究中心，是非營利性社會服務活動的社會組織。 渠成開源社區主要面向一線開源軟件生產者、貢獻者、組織者、贊助商和用户，以解決具體實際問題為宗旨，旨在打造以開源軟件為核心紐帶的開源生態系統，真正做到讓每一個優秀的開源軟件都能實現商業化。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;span&gt;&lt;span&gt;官網：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fwww.qucheng.cc&quot; target=&quot;_blank&quot;&gt;www.qucheng.cc&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;hr&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;「&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;開源項目老牌與新秀&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;」&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;是開源中國 OSCHINA 推出的一檔直播欄目，旨在為開源項目提供一個展示平台，每週五晚上開播&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;欄目邀請開源項目的作者、核心團隊成員或資深用户作為嘉賓，通過路演式直播分享項目的亮點和經驗，有助於提高項目的知名度，吸引更多的用户和開發者關注。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span&gt;&lt;span style=&quot;background-color:#ffffff&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;如果你手上也有好的開源項目，想要跟同行交流分享，歡迎聯繫我，欄目隨時開放～&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:center&quot;&gt;&lt;img height=&quot;537&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-aeeec97bc896bc61c715698bd6acc1f83b3.jpg&quot; width=&quot;400&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/u/3859945/blog/16971110</link>
            <guid isPermaLink="false">https://my.oschina.net/u/3859945/blog/16971110</guid>
            <pubDate>Tue, 31 Dec 2024 02:55:00 GMT</pubDate>
            <author>原創</author>
        </item>
        <item>
            <title>另闢蹊徑打造高精度的 VL 文字提取工具</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                    
                                                                                                                                                    &lt;span id=&quot;OSC_h2_1&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;一、應用場景：設備銘牌識別環境&lt;/h2&gt; 
&lt;p&gt;在工業和醫療等領域的設備管理中，設備銘牌的信息提取是一項常見但極具挑戰性的任務。這些銘牌通常包含關鍵信息如型號、序列號、製造日期等，對於資產管理、維護記錄和故障排除至關重要。然而，實際拍攝的照片往往存在以下難題：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;角度與透視問題&lt;/strong&gt;：由於拍攝角度各異，上傳的照片中設備銘牌可能出現嚴重的透視變形，導致文字傾斜或扭曲。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;光線與反射干擾&lt;/strong&gt;：現場光線條件複雜，強光反射或陰影遮擋使得銘牌上的文字難以清晰辨認。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;背景雜亂&lt;/strong&gt;：周圍環境複雜，背景中的其他物體可能幹擾文本區域的識別。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;這些問題直接使用視覺語言（VL）模型進行識別時，會導致極低的準確率和可靠性。為瞭解決這些問題，最近和 Gitee AI 團隊進行了深度友好的溝通，最終得到了一套完整的解決方案，通過 UVDoc 圖像校正工具預處理圖片，再利用 QwenVL 進行信息識別，並最終使用大型語言模型（LLM）實現結構化數據提取，顯著提升了銘牌文字的提取效果。&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;202&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-f0fe8c7dbf79cea800ebd5fd45eacd503ea.jpg&quot; width=&quot;250&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&amp;nbsp;&amp;nbsp;&lt;img alt=&quot;&quot; height=&quot;202&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-c0a5b16fd9f82586a0a5e5b09bb17c64735.jpg&quot; width=&quot;141&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&amp;nbsp;&lt;img alt=&quot;&quot; height=&quot;202&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-4f047384ddb6dfa3a43fd16be34037953c4.jpg&quot; width=&quot;269&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&amp;nbsp;&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_2&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;二、技術方法&lt;/h2&gt; 
&lt;span id=&quot;OSC_h3_3&quot;&gt;&lt;/span&gt; 
&lt;h3&gt;1. UVDoc 圖像校正工具：提升輸入質量&lt;/h3&gt; 
&lt;p&gt;針對上述難題，我們首先採用 GiteeAI 團隊最新發布的&lt;a href=&quot;https://ai.gitee.com/serverless-api?model=UVDoc&quot; rel=&quot;nofollow&quot;&gt;UVDoc 圖像校正工具&lt;/a&gt;對原始照片進行預處理。該工具利用先進的計算機視覺算法，自動檢測並糾正圖像中的透視變形，恢復銘牌的真實形狀。同時，它還可以調整圖像的亮度和對比度，減少光線和反射帶來的幹擾。經過 UVDoc 校正後的圖像不僅提高了文本的可讀性，還為後續的文字識別提供了更佳的基礎。&lt;br&gt; 地址：&lt;a href=&quot;https://ai.gitee.com/serverless-api?model=UVDoc&quot; rel=&quot;nofollow&quot;&gt;https://ai.gitee.com/serverless-api?model=UVDoc&lt;/a&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h3_4&quot;&gt;&lt;/span&gt; 
&lt;h3&gt;2. QwenVL：強大的信息識別引擎&lt;/h3&gt; 
&lt;p&gt;完成圖像預處理後，接下來是關鍵的信息識別階段。我們選擇了 QwenVL 作為核心識別引擎，其融合了最新的視覺語言模型技術，能夠在複雜背景條件下精準定位並識別出文本內容。QwenVL 不僅可以處理常規印刷體文字，還能應對手寫體以及多種語言混合的情況，極大地拓寬了應用範圍。此外，QwenVL 還支持多模態輸入，可以同時解析圖像中的其他非文本元素，如圖標、表格等，為用户提供更加全面的信息提取服務。&lt;br&gt; 地址：&lt;a href=&quot;https://ai.gitee.com/serverless-api?model=Qwen2-VL-72B&quot; rel=&quot;nofollow&quot;&gt;https://ai.gitee.com/serverless-api?model=Qwen2-VL-72B&lt;/a&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h3_5&quot;&gt;&lt;/span&gt; 
&lt;h3&gt;3. LLM 結構化數據提取：智能化處理結果&lt;/h3&gt; 
&lt;p&gt;最後一步是將 QwenVL 輸出的結果進一步轉化為結構化的數據格式。這一步驟依賴於 Qwen2.5-72B-Instruct，它具備強大的自然語言理解能力，可以從非結構化的文本中抽取出有價值的結構化信息。例如，在設備銘牌識別場景中，Qwen2.5-72B-Instruct 可以自動識別並分類不同的字段，如型號、序列號、製造日期等；同時生成易於檢索和分析的結構化數據，極大地方便了後續的數據管理和應用。&lt;br&gt; 地址：&lt;a href=&quot;https://ai.gitee.com/serverless-api?model=Qwen2.5-72B-Instruct&quot; rel=&quot;nofollow&quot;&gt;https://ai.gitee.com/serverless-api?model=Qwen2.5-72B-Instruct&lt;/a&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_6&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;三、結果展示&lt;/h2&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;img height=&quot;957&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-e1ac616d8505ee8718c66b5aa216a66688f.png&quot; width=&quot;1370&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;為了驗證我們的方案的有效性，我們進行了實驗，共同處理了 30 張具有不同角度和光線條件的設備銘牌照片。實驗分為兩組：一組直接使用 QwenVL 進行識別（直接 VL 組），另一組先使用 UVDoc 工具預處理後再使用 QwenVL 識別（聯合處理組）。以下是兩組的數據對比及更深入的統計分析：&lt;/p&gt; 
&lt;span id=&quot;OSC_h3_7&quot;&gt;&lt;/span&gt; 
&lt;h3&gt;（一）數據對比表&lt;/h3&gt; 
&lt;table&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;th&gt;識別情況&lt;/th&gt; 
   &lt;th&gt;直接 VL 組 (張)&lt;/th&gt; 
   &lt;th&gt;聯合處理組 (張)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;正確識別&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;28&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;部分識別&lt;/td&gt; 
   &lt;td&gt;12&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;完全不能識別&lt;/td&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;總計&lt;/td&gt; 
   &lt;td&gt;30&lt;/td&gt; 
   &lt;td&gt;30&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;span id=&quot;OSC_h3_8&quot;&gt;&lt;/span&gt; 
&lt;h3&gt;（二）進一步統計分析&lt;/h3&gt; 
&lt;p&gt;最近我在做科研項目，所以簡單按照科研項目的分析邏輯做了一下進一步的數據分析，相關內容就截圖了。&lt;/p&gt; 
&lt;span id=&quot;OSC_h4_9&quot;&gt;&lt;/span&gt; 
&lt;h4&gt;1. 準確率提升&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;正確識別率&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;直接 VL 組：8/30 = 26.7%&lt;/li&gt; 
   &lt;li&gt;聯合處理組：28/30 = 93.3%&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;部分識別率&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;直接 VL 組：12/30 = 40%&lt;/li&gt; 
   &lt;li&gt;聯合處理組：2/30 = 6.7%&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;完全不能識別率&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;直接 VL 組：10/30 = 33.3%&lt;/li&gt; 
   &lt;li&gt;聯合處理組：0/30= 0%&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id=&quot;OSC_h4_10&quot;&gt;&lt;/span&gt; 
&lt;h4&gt;2. 平均準確度&lt;/h4&gt; 
&lt;p&gt;平均準確度定義為每個樣本被正確識別的比例。計算方法如下：&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-196c8ee868b55cd221c9cb66160298a7693.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;直接 VL 組平均準確度：8/30 = 26.7%&lt;/li&gt; 
 &lt;li&gt;聯合處理組平均準確度：28/30 = 93.3%&lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id=&quot;OSC_h4_11&quot;&gt;&lt;/span&gt; 
&lt;h4&gt;3. Kappa 係數（Cohen&#39;s Kappa）&lt;/h4&gt; 
&lt;p&gt;Kappa 係數用於衡量分類系統的可靠性，考慮了偶然一致性。其公式為：&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-f2688c169d186ec03283ab51948f08da1ec.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Kappa 係數表明聯合處理組的一致性遠高於直接 VL 組，説明前者在實際應用中更為可靠。&lt;/p&gt; 
&lt;span id=&quot;OSC_h3_12&quot;&gt;&lt;/span&gt; 
&lt;h3&gt;四、結論&lt;/h3&gt; 
&lt;p&gt;從以上數據分析可以看出，聯合處理組的表現顯著優於直接 VL 組：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;準確性大幅提升&lt;/strong&gt;：聯合處理組的正確識別率從 26.7% 提高到了 93.3%，幾乎達到了完全正確識別。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;部分識別減少&lt;/strong&gt;：聯合處理組部分識別的比例從 40% 降低到 6.7%，表明大多數情況下都能實現完全正確的識別。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;無法識別消除&lt;/strong&gt;：聯合處理組實現了零失敗，所有照片均能至少部分識別，而直接 VL 組有 10 張照片完全不能識別。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;可靠性更高&lt;/strong&gt;：Kappa 係數顯示聯合處理組的一致性遠高於直接 VL 組，證明瞭其在實際應用中的優越性能。&lt;br&gt; &amp;nbsp;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;img height=&quot;514&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-e116320452808f0eb70d24e1a6e9b74d503.png&quot; width=&quot;1781&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;綜上所述，在高精度的圖文識別場景中，通過 UVDoc 圖像校正工具預處理圖片、QwenVL 進行信息識別以及 LLM 進行結構化數據提取，成功解決了設備銘牌識別中的難題，構建了一個高效且精確的文字提取系統。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/bojinzhu/blog/17003148</link>
            <guid isPermaLink="false">https://my.oschina.net/bojinzhu/blog/17003148</guid>
            <pubDate>Tue, 31 Dec 2024 02:36:00 GMT</pubDate>
            <author>原創</author>
        </item>
        <item>
            <title>智元機器人重磅開源百萬真機數據集 AgiBot World</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;智元機器人重磅開源全球首個基於全域真實場景、全能硬件平台、全程質量把控的百萬真機數據集 AgiBot World。這一里程碑式的開源項目，標誌着具身智能領域 「ImageNet 時刻」 已到來。&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2024/1231/101437_MPC7_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;智元機器人介紹稱，AgiBot World 是全球首個基於全域真實場景、全能硬件平台、全程質量把控的百萬真機數據集。相比谷歌開源的 Open X-Embodiment 數據集，AgiBot World 長程數據規模高出 10 倍，場景範圍覆蓋面擴大 100 倍，數據質量從實驗室級上升到工業級標準。&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;未來，智元機器人將陸續開源千萬仿真數據，以支持更泛化和更通用的大模型訓練；將發佈具身基座大模型，可支持模型微調；發佈全套工具鏈，實現採集、訓練和評測閉環。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;GitHub：https://github.com/OpenDriveLab/agibot-world&lt;/li&gt; 
 &lt;li&gt;項目主頁：https://agibot-world.com/&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/327297</link>
            <guid isPermaLink="false">https://www.oschina.net/news/327297</guid>
            <pubDate>Tue, 31 Dec 2024 02:15:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>想開啓 Wine 開發？看這篇就夠了！</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;説起 Wine，稍微資深一點的 Linux 用户應該都聽過，但是真要説起 Wine 到底是怎麼回事，可能大多數人不見得説得清。&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.deepin.org%2Fzh%2Fgetting-started-with-wine%2F&quot; target=&quot;_blank&quot;&gt;這篇文章會簡單地介紹 Wine 的工作原理，以及如何開始 Wine 的開發。&lt;/a&gt;所以如果您屬於以下三類讀者之一：&lt;/p&gt; 
&lt;p&gt;* 想參與 Wine 開發，但是不知如何開始的。&lt;/p&gt; 
&lt;p&gt;* 僅僅想大致瞭解 Wine 是如何工作的。&lt;/p&gt; 
&lt;p&gt;* 只是想能夠愉快的用上最新版本 Wine 的。&lt;/p&gt; 
&lt;p&gt;希望在看完本文後，能夠有一些收穫。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;Part 1&amp;nbsp; Wine 是什麼&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;Wine 是 &quot;Wine Is Not an Emulator&quot; 的遞歸縮寫，如同 &quot;GNU&quot; 一樣（GNU&#39;s Not Unix），字面意思就是 Wine 不是一個模擬器。這裏的模擬器主要是指 Wine 並不是一個虛擬機，而是一個 Windows API 實現兼容層。這麼説可能不太好理解，大家可以把 Windows 應用程序類比成 Android 應用程序，而 Wine 的角色就和 Android 很像了，將操作系統提供的各種功能封裝成 API，並讓應用程序在隔離的環境內運行。至於 API 長成啥樣，隔離的力度如何，這些都是實現相關的，和本質並不衝突。另一方面，Wine 其實又是一個模擬器，不過模擬的對象不是硬件 CPU，而是 Windows 的行為。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;Part 2&amp;nbsp; Wine 原理介紹&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;本節內容相對來説稍顯基礎和單一啦，並且閲讀時最好對操作系統有一定程度的瞭解哦。如果只是想編譯、運行，對原理不敢興趣的同學，可以跳過，不影響後面的閲讀。&lt;/p&gt; 
&lt;p&gt;Wine 的目的是運行 Windows 上的可執行程序（PE，portable executable）。我們知道，可執行程序的本質其實就是按照某一規則排列的機器碼，而機器碼是指令集相關的。得益於常見的 PC 機一般是 x86/x64 的，因此 Windows 應用程序從指令集的角度看，是完全可以在 x86/x64 的 Linux 機器上直接運行，而不需要硬件層模擬的。&lt;/p&gt; 
&lt;p&gt;但是為了能夠直接加載運行 PE 文件，需要滿足一些 ABI 兼容。最基本的，Windows PE 程序會假定自己被加載到地址 0x400000 處，因此 Wine 實現自己的 loader 時，需要保證將 PE 鏡像加載到同樣的位置。對於靜態鏈接的程序，需要做的事情可能不是太多，但是對於動態鏈接的程序，Wine 需要模仿 Windows loader 的行為，加載依賴的庫，並進行相應的重定位工作。&lt;/p&gt; 
&lt;p&gt;為了最大程度上減少對二進制層面的依賴，Wine 決定實現至少 GDI32，KERNEL32，USER32 三個動態庫，因為其他庫都是建立在這三個庫的基礎之上的。所以理論上來説，除此之外的其他動態庫是可以直接使用 Windows 上面現有的庫，但由於各種原因，Wine 還是傾向於儘量實現所有的 API。我們把 Wine 自己實現的 API 庫稱作 builtin，把 Windows 上現成的庫稱作 native。當 Wine 在加載 builtin 動態庫的同時，還會在內存中建立 PE header，用來模仿 Windows 上的內存佈局。更加詳細的實現，有機會可以寫一篇 Wine loader 相關的文章來介紹 Wine 本身、PE 程序和動態庫是如何被加載的。&lt;/p&gt; 
&lt;p&gt;除開 loader 的功能外，Wine 還需要解決進程間通信（IPC）的問題。Wine 的實現方式是將所有跨進程的對象和機制，比如 GDI 對象，比如信號量，全部實現在 Wine server 中。同時 Wine 允許系統運行多個 Wine server 的實例。這樣存在於同一個 Wine server 中的對象自然是可以相互通信，好像在同一個空間內；而不同 Wine server 下的對象，是相互隔離的，這種架構使得不同容器之間的程序相互沒有影響。Wine server 的具體實現是通過 unix socket，實現了一套 IPC 機制，完成和 API 層的交互。&lt;/p&gt; 
&lt;p&gt;有了以上這些基礎，Wine 實現起各種功能就可以按部就班了，只需理解 Windows 下 API 的行為和含義，然後再重新實現一遍就行了。聽起來雖然簡單，實際難度不小，特別是一些未公開的行為，必須對整體有相當的瞭解後才能下手。甚至一些差異，比如 UI 相關的內容，由於 Windows 窗口系統和 X 在設計哲學上的不同，實現上需要有所舍取。目前 Wine 支持的平台不僅包括 Linux，還包括 BSD、Mac OS X 和 Android。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;Part 3&amp;nbsp; 環境&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;下面的開發環境都以 deepin 為例進行説明。&lt;/p&gt; 
&lt;p&gt;首先獲取代碼。Wine 官方代碼倉庫地址為&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&amp;gt;&amp;nbsp;&lt;em&gt;git://source.winehq.org/git/wine.git&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;如果你想方便打包給別人使用，又不太想折騰打包的一些細節，可以用各個發行版自己維護的 Wine。比如 Debian 維護的 Wine 倉庫地址為&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&amp;gt;&amp;nbsp;&lt;em&gt;https://salsa.debian.org/wine-team/wine.git&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;這裏以官方的 Wine 為例：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;git clone git://source.winehq.org/git/wine.git&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;然後安裝開發的依賴。為了簡單起見，我們只編譯 32 位的 Wine，因為 64 位的 Wine 只支持 64 位的 PE 程序，而目前 Windows 上仍有大量的程序只提供了 32 位的版本。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;sudo apt install\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;gcc-multilib\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;flex\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;bison\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libx11-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libfreetype6-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libxcursor-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libxi-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libxshmfence-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libxxf86vm-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libxrandr-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libxfixes-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libxinerama-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libxcomposite-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libglu1-mesa-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libosmesa6-dev:i386\&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;ocl-icd-opencl-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libpcap-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libdbus-1-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libgnutls28-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libncurses-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libsane-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libv4l-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libgphoto2-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;liblcms2-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libpulse-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libgstreamer-plugins-base1.0-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libudev-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libcapi20-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libcups2-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libfontconfig1-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libgsm1-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libkrb5-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libtiff-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libmpg123-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libopenal-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libldap2-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libxrandr-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libxml2-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libxslt1-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libjpeg62-turbo-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libusb-1.0-0-dev:i386\&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;gettext\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libsdl2-dev:i386\&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;libvulkan-dev:i386&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;接着運行腳本：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;./configure --&lt;span&gt;with&lt;/span&gt;-gnutls --without-hal --without-oss&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;根據不同的 Wine 版本，此時可能會提示不同的 feature 支持情況。我們可以根據需求，對上面的依賴庫和傳入的參數進行調整，具體可以查看 configure.ac 的內容。&lt;/p&gt; 
&lt;p&gt;Wine 的源碼比較大，編譯有些耗時，可以根據 CPU 情況增加並行參數，比如 make -j8，進行編譯。&lt;/p&gt; 
&lt;p&gt;編譯完成後，運行：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;./wine --version&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;可以查看版本號。如果想安裝到系統，可以運行：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;sudo&amp;nbsp; make install&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;但是注意，安裝後可能會修改一些文件的默認打開方式。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;Part 4&amp;nbsp; 使用&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;運行：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;./wine winecfg&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;可以對默認容器進行設置，默認的容器位於 HOME 目錄下的 .wine，環境變量 WINEPREFIX 用來修改當前的容器路徑。比如有一個叫 demo.exe 的可執行文件，我們想測試能否正常運行，可以運行。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;WINEPREFIX=~/.demo_exe ./wine demo.exe&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;HOME 目錄下的`demo_exe`就會作為其容器目錄。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;Part 5&amp;nbsp; 開發&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;編譯過後的 Wine 源碼目錄結構如下：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;├── aclocal.m4&lt;br&gt; ├── ANNOUNCE&lt;br&gt; ├── AUTHORS&lt;br&gt; ├── config.log&lt;br&gt; ├── config.status&lt;br&gt; ├── configure&lt;br&gt; ├── configure.ac&lt;br&gt; ├── COPYING.LIB&lt;br&gt; ├── dlls&lt;br&gt; ├── documentation&lt;br&gt; ├── fonts&lt;br&gt; ├── include&lt;br&gt; ├── libs&lt;br&gt; ├── LICENSE&lt;br&gt; ├── LICENSE.OLD&lt;br&gt; ├── loader&lt;br&gt; ├── MAINTAINERS&lt;br&gt; ├── Makefile&lt;br&gt; ├── Makefile.in&lt;br&gt; ├── po&lt;br&gt; ├── programs&lt;br&gt; ├── README&lt;br&gt; ├── server&lt;br&gt; ├── tools&lt;br&gt; ├── VERSION&lt;br&gt; └── wine -&amp;gt; tools/winewrapper&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;* 目錄 dlls 按照模塊存放了所有 API 的實現。&lt;/p&gt; 
&lt;p&gt;* 目錄 loader 是和 Wine 啓動、加載相關的代碼。&lt;/p&gt; 
&lt;p&gt;* 目錄 programs 存放了外部程序的代碼，比如註冊表管理工具 regedit 。&lt;/p&gt; 
&lt;p&gt;* 目錄 server 顧名思義，是 Wine server 的實現。&lt;/p&gt; 
&lt;p&gt;接下來需要做的就和普通開發沒什麼兩樣了。比如説我們發現某個應用存在字體相關的 BUG，可以首先根據經驗判斷在 Windows 上，該程序是如何實現的，然後查看對應的實現。例如 GDI 相關的字體實現，位於 dlls/gdi32/font.c 和 dlls/gdi32/freetype.c 。修改完代碼後，在所在模塊的目錄，比如上例就是 dlls/gdi32 下重新 make 就可以快速驗證了。&lt;/p&gt; 
&lt;p&gt;對於複雜的問題，不太好直接定位的，可以通過輸出日誌的方式來調試，環境變量 WINEDEBUG 指定了需要輸出的日誌。&lt;/p&gt; 
&lt;p&gt;有時我們可能需要把複雜的情況簡單化，這時候難免會寫一些小的 demo 程序來重現問題。如果不想到 Windows 上面編譯，可以使用 mingw 直接在 deepin 下編譯出 exe 文件。方法很簡單，首先安裝 mingw：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;sudo apt install mingw-w64&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;接着正常利用 Windows API 實現程序，最後利用 mingw 編譯工具鏈生成文件即可，Makefile 示例：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;hello.exe: hello.c&lt;br&gt; i686-w64-mingw32-g++ -o hello.exe hello.c -DUNICODE -D_UNICODE -municode -lgdi32&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;上面的例子，定義了 UNICODE，所以使用的 UNICODE 版本的 API，入口函數為 wmain，-lgdi32 表示需要鏈接庫 gdi32 。生成出來的 hello.exe，可以同時在 Windows 和 deepin 下運行。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;Part 6&amp;nbsp; 其他&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;本文介紹的內容只涉及到 Wine 開發的基礎，Wine 本身還有很多東西值得去探索。比如 Wine 是如何使用 driver 機制讓接口和實現分離的，再比如 Wine 是如何使用純 C 實現 COM 機制的。雖然 Wine 的出現已經有一些年頭了，但是目前的開發仍然比較活躍，感興趣的同學可以加入進來，為 Linux 生態添磚加瓦，讓大家能用到更多的優質應用，也算是曲線救國了。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;Part 7&amp;nbsp; 更新&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;7.1 新的 WOW64&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;上面編譯的例子中，我們只編譯了 32 位的 Wine，忽略了編譯 64 位 Wine 的細節。從 Wine 8 開始，Wine 開始了一個稱為 PE &amp;nbsp;Convertion 的開發過程，重新實現了 WOW64 的機制。此模式下只需要編譯出一份 Wine，既可以運行 32 位程序，也能運行 64 位程序。同時也不再依賴 32 位的運行時庫。雖然此模式還被認為處於實驗階段，但是從 deepin-wine8 開始，我們已經默認使用此方式。&lt;/p&gt; 
&lt;p&gt;編譯新的 WOW64 方式的 Wine，步驟如下：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Wine 官方的源碼地址已經更改，在&amp;nbsp;&lt;em&gt;https://gitlab.winehq.org/wine/wine.git&lt;/em&gt;&amp;nbsp;下載源代碼。&lt;/li&gt; 
 &lt;li&gt;依舊需要安裝編譯時依賴的開發庫，不同在於我們這次只需要安裝 amd64 的版本，在安裝開發庫時不再需要指定 ：i386 後綴。此外 mingw-w64 在此模式下成為必須項。&lt;/li&gt; 
 &lt;li&gt;在 Wine 源碼目錄的同級目錄下新建 build-wine 目錄，並進入此目錄。&lt;/li&gt; 
 &lt;li&gt;運行 ../wine/configure --prefix=/opt/wine-newwow64 --enable-archs=i386,x86_64。這一步如果有任何提示，比如缺少編譯期依賴，可以在安裝後再次運行。&lt;/li&gt; 
 &lt;li&gt;make -j8，進行編譯。&lt;/li&gt; 
 &lt;li&gt;sudo make install 安裝，Wine 會安裝在第 4 步通過 prefix 指定的位置，即 /opt/wine-newwow64 。當然也可以選擇不安裝，直接在編譯目錄下運行。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;7.2 其他架構&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在關於原理一節的描述中我們提到過，x86/x64 機器的 Linux 系統可以通過 Wine 運行 x86/x64 的 PE 程序。實際上如今通過 DBT（dynamic binary translation）技術，在 ARM 及其他架構上已經有了運行 Wine 的方案。感興趣的讀者可以查看 [Box64](&lt;em&gt;https://github.com/ptitSeb/box64&lt;/em&gt;) 項目。&lt;/p&gt; 
&lt;p&gt;11 月份，微信在 deepin 商店上架了 Linux 原生版本，功能與 Windows、MacOS 相差無幾。Wine 版本的微信終於可以&quot;功成身退&quot;了，我們也期待今後有更多軟件推出原生版本，讓 Wine 早點完成&quot;歷史使命&quot;。&lt;/p&gt; 
&lt;div&gt; 
 &lt;h1&gt;&lt;strong&gt;相關閲讀：&lt;/strong&gt;&lt;/h1&gt; 
 &lt;p&gt;（1）&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdistrowatch.com%2Ftable.php%3Fdistribution%3Ddeepin&quot; target=&quot;_blank&quot;&gt;支持 deepin（深度）社區&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;（2）&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.deepin.org%2Fzh%2Fcategory%2Ftechnology-sharing%2F&quot; target=&quot;_blank&quot;&gt;deepin 技術分享合集&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/327293</link>
            <guid isPermaLink="false">https://www.oschina.net/news/327293</guid>
            <pubDate>Tue, 31 Dec 2024 02:14:00 GMT</pubDate>
            <author>來源: 投稿</author>
        </item>
        <item>
            <title>百度網頁版新增「AI 搜」功能，基於文心大模型打造</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;百度近日在百度搜索 Web 端首頁上線了百度「AI 搜」入口，「AI 搜」基於原百度搜索 AI 夥伴改版升級而來，在此前的基礎上做功能升級。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0102/100753_fgNl_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;據介紹，百度「AI 搜」是基於百度文心大模型打造的桌面端 AI 搜索引擎，目前內容側已經打通百度搜索引擎、百度健康、百度律臨、百度文庫、百度教育等內容生態，可確保搜索結果可靠、權威。&lt;/p&gt; 
&lt;p&gt;目前百度「AI 搜」主要提供包括話題探索、問題解決、決策輔助、知識答疑、主題研究、學習創作等功能，覆蓋文生圖、文生文、邏輯推理、多輪對話、智能摘要、AI 修圖等 AI 技術。&lt;/p&gt; 
&lt;p&gt;此外，百度「AI 搜」也提供了文心智能體入口，在對話框中可通過@方式與不同智能體進行交互，方便用户使用和創建智能體。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/327593</link>
            <guid isPermaLink="false">https://www.oschina.net/news/327593</guid>
            <pubDate>Tue, 31 Dec 2024 02:09:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>智譜深度推理模型 GLM-Zero 預覽版上線</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;智譜宣佈發佈本年度最後一個模型 GLM-Zero 的初代版本 GLM-Zero-Preview，這是智譜首個基於擴展強化學習技術訓練的推理模型。&lt;/p&gt; 
&lt;p&gt;根據介紹，GLM-Zero-Preview 是 GLM 家族中專注於增強 AI 推理能力的模型，擅長處理數理邏輯、代碼和需要深度推理的複雜問題。同基座模型相比，GLM-Zero-Preview 在不顯著降低通用任務能力的情況下，在專家任務能力方面的表現大幅提升，其在 AIME 2024、MATH500 和 LiveCodeBench 評測中，效果與 OpenAI o1-preview 相當。&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;模型表現如下：&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;247&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-4dd02998897ee2c465a04b26d597ad65ae3.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;目前 GLM-Zero-Preview 已經上線使用：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;用户可以在「智譜清言」（&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fchatglm.cn&quot; target=&quot;_blank&quot;&gt;chatglm.cn&lt;/a&gt;）中的「Zero 推理模型」智能體免費使用，支持上傳文字或圖片，模型會輸出完整推理過程；&lt;/li&gt; 
 &lt;li&gt;開發者可以在「智譜開放平台」（&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fbigmodel.cn&quot; target=&quot;_blank&quot;&gt;bigmodel.cn&lt;/a&gt;）中，通過 API 進行調用。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;智譜方面表示，目前的 GLM-Zero-Preview 與 OpenAI 的 o3 模型還有不少的差距。接下來，他們計劃推出正式版 GLM-Zero，將深度思考的能力從數理邏輯擴展到更多更通用的技術。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/327292</link>
            <guid isPermaLink="false">https://www.oschina.net/news/327292</guid>
            <pubDate>Tue, 31 Dec 2024 02:09:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>你好，2025！</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:#3e3e3e&quot;&gt;飛致雲恭祝大家新年快樂！&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;1920&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-cc8718206c51fbfc57a88fe25d62b1bbcee.jpg&quot; width=&quot;1081&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/327509</link>
            <guid isPermaLink="false">https://www.oschina.net/news/327509</guid>
            <pubDate>Mon, 30 Dec 2024 06:24:00 GMT</pubDate>
            <author>來源: 投稿</author>
        </item>
    </channel>
</rss>