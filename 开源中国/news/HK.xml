<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>開源中國-最新資訊</title>
        <link>https://www.oschina.net/news/project</link>
        <atom:link href="http://8.134.148.166:30044/oschina/news" rel="self" type="application/rss+xml"></atom:link>
        <description>開源中國-最新資訊 - Powered by RSSHub</description>
        <generator>RSSHub</generator>
        <webMaster>contact@rsshub.app (RSSHub)</webMaster>
        <language>en</language>
        <lastBuildDate>Mon, 17 Feb 2025 07:36:58 GMT</lastBuildDate>
        <ttl>5</ttl>
        <item>
            <title>最新屍檢報告認定 OpenAI「吹哨人」死因為自殺</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;2024 年 11 月 26 日，前 OpenAI 員工 Suchir Balaji 在舊金山的公寓中被發現死亡，年僅 26 歲。時至今日，舊金山法醫部門在最新公佈的屍檢報告裁定 Balaji 的死因為開槍自殺，駁斥了 Balaji 家人有關他殺的懷疑。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;344&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-1a05ba4a3131262dbb151b1411f9a3d8cd9.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;資料顯示，Balaji 是一名印度裔美國人，曾在加州大學伯克利分校學習並獲得了計算機科學學士學位。大學期間，他於 2019 年在 Scale AI 實習，並於 2021 年畢業後加入 OpenAI，參與過 WebGPT 的研發，後來又加入 GPT-4 的預訓練團隊，o1 的推理團隊以及 ChatGPT 的後訓練團隊。2024 年 8 月，他因對公司的商業行為感到失望後離職，並公開表達了自己的擔憂：「如果你相信我所相信的，你就必須離開公司」。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;10 月份，Balaji 因指控 OpenAI 非法使用受版權保護的材料來訓練其 AI 模型而廣受關注。《紐約時報》後來將他列為該報對 OpenAI 的訴訟中「擁有獨特和相關文件」的關鍵人物。彼時，OpenAI 正在被眾多著名作家和新聞出版商起訴侵犯版權。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;離開 OpenAI 後，Balaji 表示自己一直在從事「個人項目」。據他母親説，他計劃創建一個以機器學習和神經科學為中心的非營利組織。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334332/death-of-openai-suchir-balaji</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334332/death-of-openai-suchir-balaji</guid>
            <pubDate>Mon, 17 Feb 2025 07:13:04 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>Android 16 第二個 Beta 版本發佈</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Android 16 第二個 Beta 版本現已發佈，增加了對專業相機體驗、圖形效果的新支持，擴展了性能框架，並繼續改進與隱私、安全和後台任務相關的功能。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Android 16 增強了對專業相機用户的支持，允許混合自動曝光以及精確的色温和色調調整。使用新的 Intent 操作拍攝動態照片比以往任何時候都更容易，並且繼續改進 UltraHDR 圖像，支持 HEIC 編碼和 ISO 21496-1 草案標準中的新參數。&lt;/span&gt;&lt;/p&gt; 
&lt;pre style=&quot;margin-left:0; margin-right:0; text-align:left !important&quot;&gt;&lt;strong style=&quot;color:#008000&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;fun&lt;/span&gt;&lt;/strong&gt;&lt;span style=&quot;color:#000000&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color:#0000ff&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;setISOPriority&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;()&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;{&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;
   &lt;/span&gt;&lt;em&gt;&lt;span style=&quot;color:#006600&quot;&gt;// ...&lt;/span&gt;&lt;/em&gt;&lt;span style=&quot;color:#000000&quot;&gt;

    &lt;/span&gt;&lt;strong style=&quot;color:#008000&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;val&lt;/span&gt;&lt;/strong&gt;&lt;span style=&quot;color:#000000&quot;&gt; availablePriorityModes &lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt; mStaticInfo&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;characteristics&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;.&lt;/span&gt;&lt;strong style=&quot;color:#008000&quot;&gt;&lt;span style=&quot;color:#000088&quot;&gt;get&lt;/span&gt;&lt;/strong&gt;&lt;span style=&quot;color:#666600&quot;&gt;(&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;
        &lt;/span&gt;&lt;span style=&quot;color:#660066&quot;&gt;CameraCharacteristics&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;CONTROL_AE_AVAILABLE_PRIORITY_MODES
    &lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;)&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;
    &lt;/span&gt;&lt;em&gt;&lt;span style=&quot;color:#006600&quot;&gt;// ...&lt;/span&gt;&lt;/em&gt;&lt;span style=&quot;color:#000000&quot;&gt;
    
    &lt;/span&gt;&lt;em&gt;&lt;span style=&quot;color:#006600&quot;&gt;// Turn on AE mode to set priority mode&lt;/span&gt;&lt;/em&gt;&lt;span style=&quot;color:#000000&quot;&gt;
    reqBuilder&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;[&lt;/span&gt;&lt;span style=&quot;color:#660066&quot;&gt;CaptureRequest&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;CONTROL_AE_MODE&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;]&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color:#660066&quot;&gt;CameraMetadata&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;CONTROL_AE_MODE_ON
    reqBuilder&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;[&lt;/span&gt;&lt;span style=&quot;color:#660066&quot;&gt;CaptureRequest&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;CONTROL_AE_PRIORITY_MODE&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;]&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color:#660066&quot;&gt;CameraMetadata&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;CONTROL_AE_PRIORITY_MODE_SENSOR_SENSITIVITY_PRIORITY
    reqBuilder&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;[&lt;/span&gt;&lt;span style=&quot;color:#660066&quot;&gt;CaptureRequest&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;SENSOR_SENSITIVITY&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;]&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt; TEST_SENSITIVITY_VALUE
    &lt;/span&gt;&lt;strong style=&quot;color:#008000&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;val&lt;/span&gt;&lt;/strong&gt;&lt;span style=&quot;color:#000000&quot;&gt; request&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;:&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color:#660066&quot;&gt;CaptureRequest&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt; reqBuilder&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;build&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;()&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;

    &lt;/span&gt;&lt;em&gt;&lt;span style=&quot;color:#006600&quot;&gt;// ...&lt;/span&gt;&lt;/em&gt;&lt;span style=&quot;color:#000000&quot;&gt;

&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;}&lt;/span&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;385&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-b23fc5ea06a5a11a06d173580a172064ded.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Android 16 還將添加 RuntimeColorFilter 和 RuntimeXfermode，允許用户在繪製調用中添加圖形效果，例如閾值、棕褐色和色相飽和度。&lt;/span&gt;&lt;/p&gt; 
&lt;pre style=&quot;margin-left:0; margin-right:0; text-align:left !important&quot;&gt;&lt;strong style=&quot;color:#008000&quot;&gt;&lt;span style=&quot;color:#000088&quot;&gt;private&lt;/span&gt;&lt;/strong&gt;&lt;span style=&quot;color:#000000&quot;&gt; &lt;/span&gt;&lt;strong style=&quot;color:#008000&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;val&lt;/span&gt;&lt;/strong&gt;&lt;span style=&quot;color:#000000&quot;&gt; thresholdEffectString &lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color:#ba2121&quot;&gt;&lt;span style=&quot;color:#008800&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;color:#008800&quot;&gt;
    uniform half threshold;
    half4 &lt;/span&gt;&lt;span style=&quot;color:#0000ff&quot;&gt;&lt;span style=&quot;color:#008800&quot;&gt;main&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;color:#008800&quot;&gt;(half4 c) {
        half luminosity = dot(c.rgb, half3(&lt;/span&gt;&lt;span style=&quot;color:#666666&quot;&gt;&lt;span style=&quot;color:#008800&quot;&gt;0.2126&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;color:#008800&quot;&gt;, &lt;/span&gt;&lt;span style=&quot;color:#666666&quot;&gt;&lt;span style=&quot;color:#008800&quot;&gt;0.7152&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;color:#008800&quot;&gt;, &lt;/span&gt;&lt;span style=&quot;color:#666666&quot;&gt;&lt;span style=&quot;color:#008800&quot;&gt;0.0722&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;color:#008800&quot;&gt;));
        half bw = step(threshold, luminosity);
        &lt;/span&gt;&lt;strong style=&quot;color:#008000&quot;&gt;&lt;span style=&quot;color:#008800&quot;&gt;return&lt;/span&gt;&lt;/strong&gt;&lt;span style=&quot;color:#008800&quot;&gt; bw.xxx1 * c.a;
    }&lt;/span&gt;&lt;span style=&quot;color:#ba2121&quot;&gt;&lt;span style=&quot;color:#008800&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;

&lt;/span&gt;&lt;strong style=&quot;color:#008000&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;fun&lt;/span&gt;&lt;/strong&gt;&lt;span style=&quot;color:#000000&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color:#0000ff&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;setCustomColorFilter&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;(&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;paint&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;:&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color:#660066&quot;&gt;Paint&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;)&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;{&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;
   &lt;/span&gt;&lt;strong style=&quot;color:#008000&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;val&lt;/span&gt;&lt;/strong&gt;&lt;span style=&quot;color:#000000&quot;&gt; filter &lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color:#660066&quot;&gt;RuntimeColorFilter&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;(&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;thresholdEffectString&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;)&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;
   filter&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;setFloatUniform&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;(&lt;/span&gt;&lt;span style=&quot;color:#666666&quot;&gt;&lt;span style=&quot;color:#006666&quot;&gt;0.5&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;)&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;
   paint&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;colorFilter &lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt; filter
&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;}&lt;/span&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;此外，新版本中的一些變化還包括&amp;nbsp;R.attr#windowOptOutEdgeToEdgeEnforcement 將被棄用並禁用、Health and fitness permissions&amp;nbsp;的新 API targets、針對意圖重定向攻擊的默認安全強化等等。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Android 16 計劃於 2025 年第二季度發佈，這將是今年發佈的唯一包含功能變更的 Android 版本。預計第四季度將發佈另一個包含新開發者 API、優化和錯誤修復的版本。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;133&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-26a04b0db45f01253cbf0b8249a6864b748.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;117&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-5ac252c1d5405e7554865078867a3e4708b.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;更多詳情可&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fandroid-developers.googleblog.com%2F2025%2F02%2Fsecond-beta-android16.html&quot; target=&quot;_blank&quot;&gt;查看官方公告&lt;/a&gt;。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334329/second-beta-android16</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334329/second-beta-android16</guid>
            <pubDate>Mon, 17 Feb 2025 06:54:33 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>Asahi Linux 創始人宣佈辭去項目負責人職務</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;上週，Hector Martin 辭去了 Linux 內核 Apple Silicon 代碼的上游維護工作。當時他仍然計劃為 Asahi Linux 項目的下游內核做出貢獻，但就在前兩天，&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmarcan.st%2F2025%2F02%2Fresigning-as-asahi-linux-project-lead%2F&quot; target=&quot;_blank&quot;&gt;他出人意料地決定辭去 Asahi Linux 項目負責人的職位&lt;/a&gt;。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/143821_rzmV_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Asahi Linux 項目創始人 Hector Martin 在博客宣佈，他將辭去項目負責人的職務。Martin 説道，隨着時間的推移，參與項目變得越來越沒有樂趣，並注意到了關於 Asahi Linux 在 Apple Silicon 上缺乏 Apple M3/M4 支持以及其他缺失功能（如 Thunderbolt 和 USB-C 顯示器）的用户投訴。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;由於圍繞 Apple 芯片硬件上 Asahi Linux 的用户期望感到沮喪，並且最近還與 Linux 內核中 Rust 代碼的上游挫折/爭論/挑戰以及其他因素相關，Hector Martin 決定辭職&lt;/strong&gt;。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;「我立即辭去 Asahi Linux 項目負責人的職務。Asahi Linux&amp;nbsp;項目將繼續進行，我正在與團隊的其他成員一起處理職責和行政憑證的移交。我的個人 Patreon 將暫停，那些曾向我個人捐贈的用户建議轉移到 Asahi Linux OpenCollective（GitHub Sponsors 不允許我單方面暫停付款，但我的贊助者將被告知這一變化，以便他們可以手動取消贊助）。&lt;/p&gt; 
 &lt;p&gt;我想感謝整個 Asahi Linux 團隊，沒有你們，我獨自一人根本無法取得任何進展。我還對我的所有 Patreon 和 GitHub 贊助者表示最深切的感激，是你們讓這個項目從一開始就成為一個可行的現實。」&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Martin 在博客中也表達了對 Linus 的失望：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Rust for Linux 作為一個上游 Linux 項目所遇到的問題已經有詳細的記錄，我就不在此贅述了。我只想説，我認為 Linus 在處理將 Rust 整合到 Linux 中的問題上是其作為領導者的一大敗筆。&lt;strong&gt;這樣一個大型項目需要得到主要利益相關者的大力支持才能生存下去，而他的做法似乎只是靜觀其變&lt;/strong&gt;。&lt;/p&gt; 
 &lt;p&gt;與此同時，在他下游的多個子系統維護者卻竭力阻撓或妨礙項目的進行，發出令人無法接受的辱罵，並普遍打擊士氣。幾個月前，一位主要的 Rust for Linux 維護者已經辭職。&lt;/p&gt; 
 &lt;p&gt;當蘋果發佈 M1 時，Linus Torvalds 希望它能運行 Linux，但並不抱太大希望。我們實現了這一願望，Linux 5.19 從運行 Asahi Linux 的 M2 MacBook Air 上發佈。我曾希望他的熱情能轉化為對我們社區的支持，並幫助我們解決上游問題。&lt;/p&gt; 
 &lt;p&gt;遺憾的是，這一切都沒有實現。2023 年 11 月，我向他發出邀請，與他討論內核貢獻和維護方面的挑戰，看看我們能提供什麼幫助。他從未回覆。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Asahi Linux 博客也&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fasahilinux.org%2F2025%2F02%2Fpassing-the-torch%2F&quot; target=&quot;_blank&quot;&gt;已確認了 Hector 的辭職&lt;/a&gt;，而剩餘的開發者計劃繼續推動 Linux 在 Apple Silicon 硬件上的發展。&lt;/p&gt; 
&lt;p&gt;當前 Asahi Linux 成員包括 Alyssa Rosenzweig、chaos_princess、Davide Cavalca、Neal Gompa、James Calligeros、Janne Grunau 和 Sven Peter。剩餘的開發者表示他們仍將專注於將代碼提交到 Linux 內核。預計 Apple M3 和 M4 硬件支持將在他們更多的代碼被提交到上游以及持續集成取得進展之後才會實現。&lt;/p&gt; 
&lt;p&gt;對於今年的 Apple M1/M2 硬件，他們希望實現 DP Alt Mode、Vulkan 驅動程序中的稀疏圖像以及內置麥克風支持。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334317/marcan-resigning-as-asahi-linux-project-lead</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334317/marcan-resigning-as-asahi-linux-project-lead</guid>
            <pubDate>Mon, 17 Feb 2025 06:44:04 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>百度測試社區 APP 「次遇」</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;百度近期正在測試一款名為「次遇」的 App。據悉，這是一款基於興趣的原創社區 App，產品會在近日上線。 另據企查查顯示，百度關聯公司正在申請註冊相關商標。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-0e92cc7029db9ff18aad093ab328066e5d5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;整個社區的覆蓋人羣主要以動漫用户、OC 興趣用户、遊戲用户和追星用户為主，性別上，以年輕女用户為主，預計佔比 90%。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-15d3184f96d57ee30a3e73f05e989f68ea3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;為了讓產品在發展初期能夠吸引更多用户，以及保證平台的內容質量，「次遇」邀請了不少高質量的原創作者。一位入駐次遇的二次元畫師表示，百度正在從 B 站、LOFTER、抖音、微博和小紅書等平台，邀請二次元創作者加入，入駐門檻除了作品質量的要求外，粉絲數也需要幾萬以上。&lt;/p&gt; 
&lt;p&gt;與此同時，平台還為創作者提供流量扶持、創作激勵和個人 IP 孵化。具體實施上，將會提供千萬級流量扶持，主要依靠百度系 App 矩陣，包括百度 App、百度地圖、貼吧、百度輸入法和百度文庫。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333783</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333783</guid>
            <pubDate>Fri, 14 Feb 2025 11:44:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>OpenAI 官方發佈推理類模型的最佳實踐</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;OpenAI 官方博客發佈了推理類模型的最佳實踐，指導大家如何更好的使用 o1、o3 這類推理模型，當然也可以應用在 deepseek r1 上。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-264a714cd2f2a9ba42f99650890334024a0.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;這裏摘錄一下比較重要的原則：&lt;/p&gt; 
&lt;h4&gt;⭐&lt;strong&gt;什麼時候適合用推理模型？&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;1. 處理模糊任務&lt;/strong&gt;&lt;br&gt; 推理模型特別擅長利用有限的信息或不同的信息片段，並通過簡單的提示理解用户的意圖，並處理指令中的任何空白。 事實上，推理模型通常會在做出不成熟的猜測或試圖填補信息空白之前提出澄清問題。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2. 大海撈針&lt;/strong&gt;&lt;br&gt; 當您傳遞大量非結構化信息時，推理模型非常擅長理解並僅提取最相關的信息來回答問題。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;3.在大型數據集中尋找關係和細微差別&lt;/strong&gt;&lt;br&gt; 我們發現推理模型特別擅長對具有數百頁密集、非結構化信息的複雜文檔進行推理——例如法律合同、財務報表和保險索賠。 這些模型特別擅長在文檔之間建立聯繫，並根據數據中未言明的真相做出決策。&lt;br&gt; 推理模型還擅長對細緻的政策和規則進行推理，並將它們應用於手頭的任務，以得出合理的結論。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;4. 多步驟智能體規劃&lt;/strong&gt;&lt;br&gt; 推理模型對於智能體規劃和戰略制定至關重要。 當推理模型用作「規劃者」時，我們已經看到了成功，它可以為問題生成詳細的多步驟解決方案，然後根據高智能還是低延遲最重要來選擇和分配正確的 GPT 模型（「執行者」）用於每個步驟。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;5.視覺推理 （o1、QvQ 等視覺推理模型專享功能）&lt;/strong&gt;&lt;/p&gt; 
&lt;h4&gt;⭐&lt;strong&gt;怎麼有效地用推理模型？&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;1. &lt;strong&gt;保持提示簡單直接&lt;/strong&gt;： 這些模型擅長理解和響應簡潔、清晰的指令。&lt;br&gt; 2. &lt;strong&gt;避免思維鏈提示&lt;/strong&gt;： 由於這些模型在內部執行推理，因此提示它們「逐步思考」或「解釋你的推理過程」是不必要的。&lt;br&gt; 3. &lt;strong&gt;使用分隔符以提高清晰度&lt;/strong&gt;： 使用分隔符（如 Markdown、XML 標籤和章節標題）來清楚地指示輸入的不同部分，這有助於模型正確地解釋各個部分。&lt;br&gt; 4. &lt;strong&gt;首先嚐試零樣本&lt;/strong&gt;，如果需要再嘗試少樣本： 推理模型通常不需要少樣本示例（few-shot examples）就能產生好的結果，所以首先嚐試編寫沒有示例的提示。 如果你對期望的輸出有更復雜的要求，在提示中包含一些輸入和期望輸出的示例可能會有所幫助。但要確保示例與你的提示指令非常一致，因為兩者之間的差異可能會導致不良結果。&lt;br&gt; 5. &lt;strong&gt;提供具體的指導方針&lt;/strong&gt;： 如果你想明確地限制模型的響應（例如「提出一個預算低於 500 美元的解決方案」），請在提示中明確地列出這些約束條件。&lt;br&gt; 6. &lt;strong&gt;非常明確地説明你的最終目標&lt;/strong&gt;： 在你的指令中，嘗試為成功的響應提供非常具體的參數，並鼓勵模型持續推理和迭代，直到符合你的成功標準。&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;原文：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fplatform.openai.com%2Fdocs%2Fguides%2Freasoning-best-practices&quot; target=&quot;_blank&quot;&gt;https://platform.openai.com/docs/guides/reasoning-best-practices&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333767/reasoning-best-practices-by-openai</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333767/reasoning-best-practices-by-openai</guid>
            <pubDate>Fri, 14 Feb 2025 09:36:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>DeepSeek 等大模型私有化服務器快速上升，近九成在裸奔</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;經濟參考網報道稱，隨着 DeepSeek 大模型的迅速流行，越來越多的公司和個人選擇將該開源大模型私有化部署。奇安信資產測繪鷹圖平台監測發現，8971 個 Ollama 大模型服務器中，有 6449 個活躍服務器，其中 88.9% 都「裸奔」在互聯網上，導致任何人不需要任何認證即可隨意調用、在未經授權的情況下訪問這些服務，有可能導致數據泄露和服務中斷，甚至可以發送指令刪除所部署的 DeepSeek、Qwen 等大模型文件。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;公開信息顯示，運行 DeepSeek R1 大模型的服務器正在快速上升，上述 8971 個服務器中有 5669 個在中國。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;308&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-621c404ca9e7daa19db425427e2fa23b30b.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;span style=&quot;color:#000000&quot;&gt;奇安信資產測繪鷹圖平台顯示大概有 8971 個 IP 運行了 Ollama&lt;/span&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;為了應對這些問題，專家建議，所有部署 DeepSeek 服務的企業和個人應立即採取有效的安全防護措施。具體措施如下：&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color:#000000&quot;&gt;儘快修改配置&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;建議立即修改 Ollama 配置，加入身份認證手段。同時及時修改防火牆、WAF、入侵檢測等相關安全配置，例如制定 IP 白名單限制訪問，確保只有授權人員能夠訪問模型服務。定期檢查和關閉不必要的端口、限制計算資源的使用、加強監控等措施也是提高安全性的關鍵。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color:#000000&quot;&gt;確保數據傳輸加密&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;在大模型運行中，需要對所有傳輸的數據進行加密，避免在遭遇攻擊及數據竊取時泄露敏感信息。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color:#000000&quot;&gt;部署專業安全產品&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;通過部署奇安信大模型衞士等產品，可以有效的抵禦針對應用服務的傳統網絡攻擊，尤其對大模型應用特有的越獄、提示詞注入等攻擊進行全面有效的防護；部署奇安信 API 安全衞士等產品，對大模型應用的 API 接口訪問做好全面監測與防護。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;此外，個人用户更應警惕不知名廠商提供的 DeepSeek 大模型服務，一些不良廠商使用被盜資源對外售賣，騙取錢財的同時，還可實時監控用户提交的所有數據，可造成隱私泄露。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;隨着大模型技術的不斷發展，安全問題將變得愈發複雜。行業專家呼籲，使用 DeepSeek 及類似大模型的用户應儘快採取預防措施，確保技術的安全部署和穩定運行。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333761</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333761</guid>
            <pubDate>Fri, 14 Feb 2025 09:18:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>Ubuntu 24.04.2 LTS 延期至下週發佈</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p style=&quot;color:#000000; text-align:start&quot;&gt;Ubuntu 24.04.2 LTS 及其衍生版本原定於本週四發佈，但一個臨時的技術問題導致此次發佈&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flists.ubuntu.com%2Farchives%2Fubuntu-release%2F2025-February%2F006310.html&quot; target=&quot;_blank&quot;&gt;被推遲&lt;/a&gt;。&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0214/163047_Nn1o_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;由於某些 Ubuntu 24.04.2 LTS 版本在製作時&lt;strong&gt;沒有包含硬件啓用「HWE」內&lt;/strong&gt;核，Ubuntu 24.04.2 LTS 的發佈被推遲了一週，以便有時間重新制作。&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;Ubuntu 24.04.2 的重要意義在於它是 Ubuntu 24.04 LTS 系列中第一個採用 HWE 內核的版本，而 HWE 內核是 Ubuntu 24.10 的向後移植內核和其他組件。由於 Ubuntu 24.04.2 LTS 具有 Linux 6.11 內核選項和其他硬件驅動程序升級，它比一年前 Ubuntu 24.04 推出時提供了更好的硬件支持。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333744/ubuntu-24-04-2-lts-delayed</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333744/ubuntu-24-04-2-lts-delayed</guid>
            <pubDate>Fri, 14 Feb 2025 08:32:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>中國信通院：正式啓動 DeepSeek 國產化適配測評工作</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;中國信息通信研究院（簡稱「中國信通院」）宣佈正式啓動 DeepSeek 國產化適配測評工作，旨在為 DeepSeek 系列模型在多硬件多場景下的適配部署提供參考。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;一是評價模型在包括硬件芯片、計算設備、智算集羣等軟硬件系統中的適配效果；二是反映模型在軟硬件系統適配過程中軟件棧及工具的適配易用性及開發部署成本。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;本次測評將依託由中國信通院人工智能軟硬件協同創新與適配驗證中心（亦莊）、人工智能關鍵技術和應用評測工業和信息化部重點實驗室聯合推進的 AISHPerf（Performance Benchmarks of Artificial Intelligence Software and Hardware,以下簡稱 AISHPerf）人工智能軟硬件基準體系及測試工具，面向包括芯片、服務器、集羣、開發框架及工具鏈、智算設施及平台等在內的人工智能軟硬件產品及系統開展。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;測試將主要圍繞表 1 所示的 DeepSeek 不同模態、不同尺寸的系列模型，面向推理、微調、訓練過程，低成本使用測試工具 AISHPerf，從適配成本、功能完備性、優化效果、性能指標等多方面開展測試評估。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;img height=&quot;172&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-f31d6d384d3e96b7b27f04655df7e6d0a52.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.qq.com%2Fform%2Fpage%2FDVnh1bVFndmt1bnFM%3Fu%3Df50706e2c08c43acb447aeaa0ec26470%23%2Ffill&quot; target=&quot;_blank&quot;&gt;報名錶&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333734</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333734</guid>
            <pubDate>Fri, 14 Feb 2025 08:07:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>GNOME 官網全新改版</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;GNOME 全新官網已&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.gnome.org%2F&quot; target=&quot;_blank&quot;&gt;上線&lt;/a&gt;&lt;/u&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;screenshot&quot; src=&quot;https://static.oschina.net/uploads/img/202502/17142818_JmM4.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;新的設計看起來既時尚又現代，它簡化了頭部設計、空間更寬敞，色彩更鮮豔，還有簡單而有效的動畫，等等，比之前（相對單調）的舊版本更能傳達 GNOME 充滿活力、以用户為中心的理念。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://static.oschina.net/uploads/img/202502/17142819_RWzj.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;文檔方面，GNOME 開發文檔和設計指南現在各自擁有專門的章節，並附上了相關鏈接，還有一個部分展示了支持 GNOME 的組織列表（包括 Canonical），以強調 GNOME 在更廣泛的 Linux 生態中扮演的關鍵角色。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://static.oschina.net/uploads/img/202502/17142820_bW25.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;詳情訪問 GNOME 官網：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.gnome.org%2F&quot; target=&quot;_blank&quot;&gt;https://www.gnome.org/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334308/gnome-website-revamp-goes-live</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334308/gnome-website-revamp-goes-live</guid>
            <pubDate>Sat, 08 Feb 2025 06:28:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>「天工」成為全球首例登百級台階的人形機器人</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;國地共建具身智能機器人創新中心宣佈，在户外真實地形測試中，「天工」機器人連續攀爬多級階梯，成功登上北京通州區海子牆公園最高點，成為全球首例可在室外連續攀爬多級階梯的人形機器人。&lt;/span&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;國創中心持續提升具身小腦能力，實現了基於視覺的感知行走，可實現無磕碰、不踩稜、不踏空地跨越連續多級樓梯和 35 釐米大高差台階，奔跑時速提高至 12km/h，並且能在雪地進行高速奔跑，同時具備更強的抗幹擾能力，大外力衝擊下仍可保持平衡。應對複雜地形的移動能力提升，將成為人形機器人走出實驗室，在真實環境執行任務，甚至在山地、雪地救援、廢墟等極端環境下作業的基礎，為具身智能機器人規模化應用夯實技術底座。&lt;/span&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;282&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-fb4868df102f12e166908215d6ce18410f2.gif&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;此外，升級後的「天工」能夠輕鬆應對超 10KG 重物落下所造成的高達 45Ns 衝量，相當於一名職業拳擊手以 450 N 的力，重擊對手的一瞬間打出的力道，即使在光滑的雪地上從各個方向突然出現的各類幹擾等，「天工」均能保持穩定平衡不發生摔倒，達到業內領先水平。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;通過具身小腦所帶來的全身控制能力升級，「天工」面對複雜環境的移動能力再次大幅提升，首次真正發揮出雙足結構為人形機器人帶來的多地形通用性優勢，在實現全地形場景技術閉環的同時，更為行業確立了複雜環境移動能力的全新標杆。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;未來，該技術也將納入國創中心所打造的開源開放生態彙總，通過技術共享降低行業創新門檻將加速具身智能機器人在千行百業的規模化落地，為具身智能產業化開闢更具想象力的落地路徑。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color:#000000&quot;&gt;相關閲讀：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/news/289801&quot; target=&quot;_blank&quot;&gt;北京人形機器人創新中心發佈全球首個純電驅擬人奔跑的全尺寸人形機器人 「天工」&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334302</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334302</guid>
            <pubDate>Sat, 08 Feb 2025 06:00:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>ReasonFlux：通過分層模板縮放提升 LLM 推理</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;大型語言模型（LLMs）已經展現出了卓越的問題解決能力，然而，複雜的推理任務——例如競技級別的數學問題或複雜的代碼生成——仍然具有挑戰性。這些任務需要精確地穿越龐大的解空間，並進行細緻的逐步思考。現有的方法雖然在提高準確性方面有所改進，但往往面臨着高計算成本、僵化的搜索策略以及難以跨不同問題進行泛化的難題。&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.marktechpost.com%2F2025%2F02%2F15%2Freasonflux-elevating-llm-reasoning-with-hierarchical-template-scaling%2F&quot; target=&quot;_blank&quot;&gt;https://www.marktechpost.com/2025/02/15/reasonflux-elevating-llm-reasoning-with-hierarchical-template-scaling/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;在這篇論文中，研究人員介紹了一個新的框架，&lt;strong&gt;ReasonFlux&lt;/strong&gt;，它通過重新構想 LLMs 如何使用分層、模板引導的策略來規劃和執行推理步驟，從而解決了這些侷限性。 最近用於增強大型語言模型推理的方法分為兩大類：&lt;em&gt;深思熟慮的搜索_和_獎勵引導的方法&lt;/em&gt;。像思維樹（ToT）這樣的技術使 LLM 能夠探索多個推理路徑，而蒙特卡洛樹搜索（MCTS）則將問題分解為步驟，這些步驟由過程獎勵模型（PRM）引導。&lt;/p&gt; 
&lt;p&gt;儘管這些方法有效，但由於採樣過多和手動搜索設計，它們的可擴展性較差。例如，MCTS 需要遍歷成千上萬的潛在步驟，這使得它在實際應用中計算成本過高。與此同時，像思維緩衝（BoT）這樣的檢索增強生成 RAG 方法利用存儲的問題解決模板，但在適應性地整合多個模板方面存在困難，這限制了它們在複雜場景中的效用。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;1066&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0217/135909_MMin_3820517.png&quot; width=&quot;1750&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;ReasonFlux 引入了一個結構化的框架，該框架結合了精選的高層次思維模板庫與分層強化學習（HRL），以動態規劃和優化推理路徑。它不是優化單個步驟，而是專注於配置最優的 &lt;em&gt;模板軌跡&lt;/em&gt;——從結構化知識庫中檢索出的抽象問題解決策略序列。這種方法簡化了搜索空間，並使高效適應子問題成為可能。該框架由三個主要組件組成：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;結構化模板庫&lt;/strong&gt;：研究團隊構建了一個包含 500 個思維模板的庫，每個模板封裝了一種問題解決策略（例如，「三角代換優化積分」）。模板包含元數據——名稱、標籤、描述和應用步驟——以實現高效的檢索。例如，一個標記為「有理函數優化」的模板可能會指導大型語言模型（LLM）應用特定的代數替換。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;分層強化學習&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;基於結構的微調&lt;/strong&gt;：將基本 LLM（例如，Qwen2.5-32B）微調以將模板元數據與其功能描述關聯起來，確保它理解何時以及如何應用每個模板。&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;模板軌跡優化&lt;/strong&gt;：利用偏好學習，該模型學會根據效果對模板序列進行排序。對於給定的問題，會採樣多個軌跡，並根據它們在類似問題上的成功率來確定獎勵。這訓練模型優先考慮高獎勵序列，從而提高其規劃能力。&lt;/p&gt; &lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;自適應推理縮放&lt;/strong&gt;：在推理過程中，ReasonFlux 充當「導航員」，分析問題以檢索相關模板，並根據中間結果動態調整軌跡。例如，如果一個涉及「多項式因式分解」的步驟產生了意外的約束，系統可能會轉向「約束傳播」模板。這種規劃和執行之間的迭代互動反映了人類的解決問題方式，其中部分解決方案會指導後續步驟。&lt;/p&gt; &lt;img height=&quot;376&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0217/135928_eZy6_3820517.png&quot; width=&quot;1686&quot; referrerpolicy=&quot;no-referrer&quot;&gt; &lt;p&gt;&amp;nbsp;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;ReasonFlux 在 MATH、AIME 和 OlympiadBench 等競爭級基準測試中進行了評估，超越了前沿模型（GPT-4o、Claude）以及專業開源模型（DeepSeek-V3、Mathstral）。關鍵結果包括：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;MATH 準確率達到 91.2%，超過 OpenAI 的 o1-preview 6.7%。&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;AIME 2024 準確率為 56.7%，超出 DeepSeek-V3 45%，與 o1-mini 相當。&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;OlympiadBench 準確率為 63.3%，比先前方法提高了 14%。&lt;/strong&gt;此外，結構化模板庫展示了強大的泛化能力：當應用於不同的問題時，它將小型模型（例如，7B 參數）的能力提升至能夠通過直接推理超越大型模型。此外，ReasonFlux 實現了更好的探索-利用平衡，在複雜任務上比 MCTS 和 Best-of-N 需要少 40% 的計算步驟（見圖 5）。 總結來説，ReasonFlux 重新定義了 LLMs 處理複雜推理的方式，通過將高級策略與逐步執行解耦。其分層模板系統減少了計算開銷，同時提高了準確性和適應性，解決了現有方法中的關鍵差距。通過利用結構化知識和動態規劃，該框架為高效、可擴展的推理設定了新的標準——證明即使是小型、有良好指導的模型也能與最大的前沿系統相媲美。這一創新為在資源受限的環境中部署高級推理開闢了道路，從教育到自動化代碼生成。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334301/reasonflux-llm</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334301/reasonflux-llm</guid>
            <pubDate>Sat, 08 Feb 2025 06:00:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>微軟開源「專業領域知識-推理能力 RAG」 —— PIKE-RAG</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;近年來，大語言模型（LLM）憑藉強大的文本生成能力在各個領域引起了廣泛關注。它們不僅能寫文章、翻譯語言，還能完成創作任務。但當遇到需要專業領域知識支持的工業級問題時，比如半導體設計、製藥研發或法律條文解讀，這些模型往往力不從心。這不僅因為訓練數據中缺少足夠的專業信息，還因為單靠「生成」能力，難以構建嚴謹的邏輯推理和多層次的信息整合。&lt;/p&gt; 
&lt;h2&gt;為什麼傳統方法會遇到瓶頸？&lt;/h2&gt; 
&lt;p&gt;目前，為瞭解決這一問題，業界提出了「檢索增強生成」（Retrieval-Augmented Generation，簡稱 RAG）的思路。其核心理念是在生成答案之前，先從一個龐大的外部知識庫中檢索出相關信息，再將這些信息融入生成的上下文中，從而使回答更準確、更有事實依據。&lt;/p&gt; 
&lt;p&gt;然而，傳統 RAG 方法存在以下幾個問題：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;知識來源複雜&lt;/strong&gt;：現實中的數據不僅僅是純文本，還包括表格、圖表、圖片等多種格式。單一的文本檢索難以捕捉這些多樣信息。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;專業領域知識不足&lt;/strong&gt;：工業應用中的專業知識具有特定術語和邏輯，普通模型難以準確提取和理解，從而導致回答不夠嚴謹。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;「一刀切」的策略&lt;/strong&gt;：不同類型的問題（如簡單事實問答與需要多步推理的複雜問題）要求不同的處理策略，而傳統方法往往採用統一流程，無法兼顧所有需求。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;PIKE-RAG 的創新之處&lt;/h2&gt; 
&lt;p&gt;為瞭解決上述不足，微軟亞洲研究院提出了 PIKE-RAG —— 一種專注於「知識」和「推理」增強的生成框架。PIKE-RAG 不僅幫助模型檢索相關知識，更注重如何理解、拆解和合理組織這些信息，從而構建出嚴謹的推理鏈。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;788&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0217/121127_pF8y_3820517.png&quot; width=&quot;2072&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;792&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0217/121200_n64m_3820517.png&quot; width=&quot;2058&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;810&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0217/121040_68c7_3820517.png&quot; width=&quot;2088&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;下面我們來看看它的核心設計：&lt;/p&gt; 
&lt;h3&gt;1. 分級任務設計&lt;/h3&gt; 
&lt;p&gt;論文將問題大致分為四類：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;事實型問題&lt;/strong&gt;：例如「這款 LED 產品的額定電流是多少？」&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;鏈式推理問題&lt;/strong&gt;：需要跨多個信息點進行關聯，比如比較多個產品的性能。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;預測型問題&lt;/strong&gt;：例如「未來 5 年半導體技術可能有哪些突破？」&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;創造型問題&lt;/strong&gt;：要求模型發揮創造力，提出新見解。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;這種分類使得系統能根據問題的難度和性質，採用針對性的處理策略，從而「量體裁衣」地提升答案的準確性和邏輯性。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;1246&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0217/120535_p84F_3820517.png&quot; width=&quot;1124&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;2. 知識「原子化」與任務分解&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;知識原子化&lt;/strong&gt;：面對複雜問題，系統會將長文檔或複雜數據拆分成最基本的信息單元（知識原子）。這種拆分類似於把大問題拆成小問題，每個小單元便於獨立檢索和理解。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;知識感知的任務分解&lt;/strong&gt;：系統根據問題需求，動態分解任務，並利用已提取的知識原子構建邏輯推理鏈。這樣一來，即使是多步推理的問題，系統也能循序漸進地「拼湊」出最終答案。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;任務分解器訓練&lt;/strong&gt;：為實現高效分解，系統還引入了可訓練的任務分解模塊，通過大量領域數據學習如何將問題正確拆解併合理組合各個知識點。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;3. 分階段系統構建&lt;/h3&gt; 
&lt;p&gt;PIKE-RAG 採用了分階段的開發策略，逐步提升系統的處理能力：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;初級階段&lt;/strong&gt;：專注於構建一個多模態知識庫。系統會從文本、表格、圖像等多種格式中抽取信息，並利用解析算法將它們統一組織成一個結構化、關聯緊密的知識網絡。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;中級階段&lt;/strong&gt;：在事實型問題上引入多粒度檢索技術，結合增強型文本切分和自動標記機制，確保能精確提取出關鍵信息。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;高級階段&lt;/strong&gt;：逐步引入鏈式推理模塊、知識原子化處理和任務分解器，使系統不僅能夠檢索信息，更能在多跳推理、預測和創造性回答等複雜任務中表現優異。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;實現原理：如何讓系統「知曉」與「推理」&lt;/h2&gt; 
&lt;p&gt;在 PIKE-RAG 系統中，設計者採用了層次化、分階段的實現策略，確保系統能逐步提升對複雜問題的處理能力。下面詳細介紹各個主要環節的實現原理：&lt;/p&gt; 
&lt;h3&gt;1. 知識庫構建（Level-0）&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;文件解析&lt;/strong&gt;：系統首先從各種格式的數據中抽取信息，將非結構化數據（如掃描文檔、表格、圖片中的文字）經過專門算法轉換為統一的文本數據。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;知識組織&lt;/strong&gt;：解析後的信息被組織成一個多層次的異構圖，各類數據節點（例如產品技術規格、圖表、説明文字等）通過超鏈接、引用關係等方式互相連接，形成結構化的知識庫，便於後續的高效檢索和利用。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;2. 專門模塊針對不同問題&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;事實型問題模塊（Level-1）&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;增強型切分與自動標記&lt;/strong&gt;：長文檔被切分成更小的信息塊，並自動為每個信息塊打上標籤，以便在檢索時更精確地匹配查詢內容。&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;多粒度檢索&lt;/strong&gt;：系統在檢索時不僅搜索全文，還能在不同層級和粒度上查找相關信息，提高檢索的準確性。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;鏈式推理問題模塊（Level-2）&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;知識原子化&lt;/strong&gt;：將大塊複雜知識拆解成最小的基本單元，使得每個單元都能獨立檢索並參與推理。&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;任務分解&lt;/strong&gt;：針對複雜問題，系統動態分解成多個子任務，每個子任務依次解決後再組合成最終答案。&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;訓練可調的任務分解器&lt;/strong&gt;：通過大量領域數據訓練，系統學會如何針對不同專業問題設計合適的分解策略和推理流程。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;預測型與創造型問題模塊（Level-3 &amp;amp; Level-4）&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;在高級階段，系統不僅能處理已知信息，還能在已有數據基礎上推演預測未來趨勢或提出創造性觀點，從而滿足更高層次的應用需求。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img height=&quot;684&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0217/120551_g5tH_3820517.png&quot; width=&quot;828&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;3. 分階段開發策略&lt;/h3&gt; 
&lt;p&gt;整個系統從構建基礎知識庫開始，逐步引入不同層次的檢索與推理模塊。每個階段的開發都以解決特定問題為目標：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;初級階段&lt;/strong&gt;確保系統在簡單事實問答上表現出色；&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;中級階段&lt;/strong&gt;引入多跳推理和任務分解，處理更復雜的問題；&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;高級階段&lt;/strong&gt;則針對預測和創造性任務進行優化，使系統具備更強的靈活性和適應性。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;實驗效果與應用前景&lt;/h2&gt; 
&lt;p&gt;通過大量實驗驗證，PIKE-RAG 在開放領域和法律領域的問答任務中均展現了卓越的性能。得益於知識原子化、任務分解以及多粒度檢索技術，該系統在處理多步推理和複雜查詢時表現尤為出色。這不僅為工業級問答系統的發展提供了新思路，也為未來在更多複雜場景中的應用奠定了基礎。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2FPIKE-RAG&quot; target=&quot;_blank&quot;&gt;https://github.com/microsoft/PIKE-RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fpike-rag.azurewebsites.net%2F&quot; target=&quot;_blank&quot;&gt;https://pike-rag.azurewebsites.net&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334286/microsoft-pike-rag</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334286/microsoft-pike-rag</guid>
            <pubDate>Sat, 08 Feb 2025 04:12:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>OBS Studio 批評 Fedora 的 Flatpak 打包，稱其是惡意分支</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;開源屏幕錄製和直播應用 OBS Studio 近日向 Fedora 提出了批評，指出它對該應用程序的 Flatpak 打包存在問題，並威脅説如果不加以解決，將採取法律行動。&lt;/p&gt; 
&lt;p&gt;三週前 OBS Studio 團隊就提交了&amp;nbsp;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgitlab.com%2Ffedora%2Fsigs%2Fflatpak%2Ffedora-flatpaks%2F-%2Fissues%2F39%23note_2344970813&quot; target=&quot;_blank&quot;&gt;Fedora Flatpak SIG 工單&lt;/a&gt;&amp;nbsp;—— 關於 Fedora 提供「損壞」的 OBS Studio Flatpak 被視為官方軟件包：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;「Fedora Flatpaks 應用商店提供的非官方 OBS Studio Flatpak 似乎打包不佳且已損壞，導致用户向上遊投訴，因為他們認為這是 OBS Studio 的官方軟件包。這種情況在 OBS Studio 之外也存在多個例子，許多用户對 Fedora Flatpaks 被強制推廣，缺少或沒有明確的選項退出感到不滿。&lt;/p&gt; 
 &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgitlab.gnome.org%2FGNOME%2Fgnome-software%2F-%2Fissues%2F2754&quot; target=&quot;_blank&quot;&gt;https://gitlab.gnome.org/GNOME/gnome-software/-/issues/2754&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fpagure.io%2Ffedora-workstation%2Fissue%2F463&quot; target=&quot;_blank&quot;&gt;https://pagure.io/fedora-workstation/issue/463&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;我們希望請求將該軟件包移除，或者明確指出它是一個第三方軟件包。&lt;strong&gt;確保下游軟件包正常工作不應是上游的責任，尤其是當它們覆蓋官方軟件包時&lt;/strong&gt;。&lt;/p&gt; 
 &lt;p&gt;我還想了解為什麼有人認為將一個運行得非常完美的 Flatpak 版本破壞後，以更高的優先級發佈到我們的官方構建中是一個好主意。我們在官方 Flatpak 上投入了大量的努力，以確保它們在 Flathub 上發佈時能儘可能地正常運行。」&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;但然後在過去的一天裏，Fedora 不但沒有刪除，似乎還和 OBS Studio 團隊對罵起來，這讓後者非常不爽，因此認定 Fedora Flatpak 上的 OBS Studio 是個惡意分支，並威脅採取法律行動：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;由於目前很明顯 Fedora 對此沒有興趣進行理性討論，並決定訴諸於人身攻擊，我們現在將 Fedora Flatpaks 分發的 OBS Studio 視為惡意分支。&lt;/p&gt; 
 &lt;p&gt;這是一個正式請求，要求從您的分發中移除我們所有的品牌標識，包括但不限於我們的名稱、我們的標誌、屬於 OBS 項目的任何附加知識產權。&lt;/p&gt; 
 &lt;p&gt;如果不遵守，可能會導致採取進一步的法律行動。我們期望在接下來的 7 個工作日內收到回覆（截至 2025 年 2 月 21 日星期五）。&lt;/p&gt; 
&lt;/blockquote&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334282/obs-studio-poor-fedora-flatpak</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334282/obs-studio-poor-fedora-flatpak</guid>
            <pubDate>Sat, 08 Feb 2025 03:54:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>2024 年 Rust 社區調查報告</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;Rust 調查團隊很高興與大家分享我們關於 Rust 編程語言的 2024 年調查結果，該調查於 2024 年 12 月 5 日至 2024 年 12 月 23 日進行。與往年一樣，2024 年的 Rust 狀態調查旨在收集 Rust 用户以及更廣泛地關注 Rust 未來的所有人的見解和反饋。&lt;/p&gt; 
&lt;p&gt;這份調查的第九版揭示了來自全球 Rust 語言社區的全新見解和學習機會，以下我們將進行總結。除了這篇博客文章外，&lt;strong&gt;我們還&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fraw.githubusercontent.com%2Frust-lang%2Fsurveys%2Fmain%2Fsurveys%2F2024-annual-survey%2Freport%2Fannual-survey-2024-report.pdf&quot; target=&quot;_blank&quot;&gt;準備了一份報告&lt;/a&gt;&lt;/u&gt;&lt;/strong&gt;，其中包含了調查中所有問題的彙總結果圖表。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;我們對每一位在過去一年中抽出時間表達對 Rust 看法和體驗的社區成員表示最誠摯的感謝。您的參與將幫助我們使 Rust 對每個人來説都變得更好。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;下文包含了大量數據，所以請坐穩，享受閲讀！&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;參與&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111550_gaCL_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;如上圖所示，2024 年，我們收到的調查查看次數比上一年少。這可能是由於調查僅進行了兩週，而上一年調查進行了近一個月。然而，完成率也有所下降，這似乎表明調查可能有點太長了。我們將考慮這一點，為下一次調查的版本進行調整。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;社區&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Rust 狀態調查不僅為我們提供了關於世界各地有多少 Rust 用户在使用和體驗該語言的寶貴見解，而且還讓我們瞭解了我們全球社區的結構。這些信息讓我們瞭解到語言的使用情況以及隨着時間的推移，我們可能需要解決的接入差距。我們希望這些數據和我們的相關分析能進一步促進關於我們如何繼續優先考慮 Rust 社區的全球接入和包容性的重要討論。&lt;/p&gt; 
&lt;p&gt;與往年一樣，我們詢問了受訪者他們居住在哪個國家。排名前十的國家依次是：美國（22%）、德國（14%）、英國（6%）、法國（6%）、中國（5%）、加拿大（3%）、荷蘭（3%）、俄羅斯（3%）、澳大利亞（2%）和瑞典（2%）。我們很高興看到 Rust 受到世界各地用户的喜愛！您可以在下面的圖表中嘗試找到您的國家：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111604_Xkme_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;我們也詢問了受訪者是否認為自己屬於一個邊緣化社區的一員。在回答者中，74.5% 選擇了「否」，15.5% 選擇了「是」，10% 選擇不願意透露。&lt;/p&gt; 
&lt;p&gt;我們詢問了選擇「是」的羣體，他們將自己識別為哪些特定羣體的成員。將自己視為技術領域中被代表性不足或邊緣化羣體成員的大多數人將自己識別為女同性戀、男同性戀、雙性戀或其他非異性戀。其次是神經多樣性羣體，佔比 46%，其次是跨性別羣體，佔比 35%。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111617_hrzo_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;每年，我們必須承認 Rust 社區和開源整體在多樣性、公平性和包容性（DEI）方面的差距。我們相信，Rust 基金會在推進 Rust 社區聚會全球訪問和在每個週期向多元化的維護者羣體分配補助金方面正在開展出色的工作，您可以在這裏瞭解更多信息。即便如此，全球包容性和訪問性只是 DEI 的一個要素，調查工作組將繼續在這個領域推動進步。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Rust 使用情況&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;自認是 Rust 用户的人數與去年相當，大約為 92%。這個高比例並不令人驚訝，因為我們主要針對現有的 Rust 開發者進行這項調查。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111627_7qhc_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;同樣地，像去年一樣，大約 31% 的未將自己標識為 Rust 用户的人士將難度感知作為不使用 Rust 的主要原因。不使用 Rust 的最常見原因是受訪者們還沒有機會嘗試它。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111639_1Cns_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;在參與 2024 年調查的前 Rust 用户中，36% 的人士將不可控因素列為他們不再使用 Rust 的原因，這比去年下降了 10 個百分點。&lt;/p&gt; 
&lt;p&gt;今年，我們還詢問受訪者如果有機會，他們是否會考慮再次使用 Rust，結果發現很大一部分受訪者（63%）會這麼做。這真是令人欣慰！&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111652_RnJ5_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;標記為 N/A 的封閉答案在調查的前一個版本中並未出現。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;不再使用 Rust 的人告訴我們，這主要是因為他們實際上並不需要它（或他們公司的目標發生了變化），或者因為它不是這項工作的合適工具。少數人報告稱，他們被這種語言或其生態系統整體所壓倒，或者認為轉向或引入 Rust 在人力成本上過於昂貴。&lt;/p&gt; 
&lt;p&gt;在 2024 年使用 Rust 的人中，有 53% 的人是每天（或幾乎每天）使用它——比上一年增加了 4 個百分點。我們可以觀察到，在過去的幾年中，Rust 的使用頻率呈上升趨勢，這表明 Rust 在工作場所的使用越來越多。這一點也由下文「Rust at Work」部分中提到的其他答案所證實。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111737_L7g6_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Rust 的專業技能在我們的受訪者中也持續增長！20% 的受訪者能夠編寫（僅）簡單的 Rust 程序（相比 2023 年下降了 3 個百分點），而 53% 的人認為自己使用 Rust 是高效的——這一比例在 2023 年為 47%。雖然這項調查只是衡量 Rust 整體技能變化的一個工具，但這些數字令人鼓舞，因為它們代表了每年迴歸調查的許多 Rustaceans 的知識增長。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111747_VI2v_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;不出所料，最受歡迎的 Rust 版本是最新穩定版，無論是最新版本還是與用户的 Linux 發行版一起提供的版本。幾乎三分之一的用户也使用最新的夜間版本，由於各種原因（見下文）。然而，似乎 beta 工具鏈的使用並不多，這有點遺憾。我們希望鼓勵 Rust 用户更多地使用 beta 工具鏈（例如在 CI 環境中），以幫助測試即將穩定化的 Rust 版本。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111759_RPoz_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;人們使用夜間工具鏈主要是為了獲取特定的不穩定語言功能。也有幾位用户提到，他們對夜間版本的 rustfmt 更滿意，或者他們使用夜間編譯器是因為編譯速度更快。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111809_jJfZ_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;學習 Rust&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;要使用 Rust，程序員首先必須學習它，所以我們總是對他們是怎樣學習的很感興趣。根據調查結果，似乎大多數用户通過 Rust 文檔以及《Rust 編程語言》這本書來學習，這本書長期以來一直是新 Rustaceans 最喜歡的學習資源。許多人似乎也通過閲讀 Rust crate 的源代碼來學習。事實上，成千上萬 Rust crate 的文檔和源代碼都可在 docs.rs 和 GitHub 上找到，這使得學習變得更加容易。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111822_KHvR_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;關於屬於「其他」類別的回答，它們可以歸納為三個類別：使用 LLM（大型語言模型）助手（如 Copilot、ChatGPT、Claude 等）、閲讀官方 Rust 論壇（Discord、URLO）或在貢獻 Rust 項目時接受指導的人。我們想向那些使我們的空間對新來者友好和歡迎的人表示衷心的感謝，因為這是一項重要的工作，而且它是有回報的。有趣的是，相當數量的人通過「做中學」來學習，並使用 rustc 錯誤信息和 clippy 作為指南，這是 Rust 診斷質量的良好指標。&lt;/p&gt; 
&lt;p&gt;至於正規教育，似乎 Rust 尚未滲透到大學課程中，因為這是一個通常發展緩慢的領域。只有極少數受訪者（大約 3%）曾上過大學的 Rust 課程或使用過大學學習材料。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111833_l4Zn_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;編程環境&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;關於 Rustaceans 使用的操作系統，Linux 是最受歡迎的選擇，而且它似乎每年都在變得越來越受歡迎。其次是 macOS 和 Windows，它們的使用份額非常相似。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111844_giEi_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-9695c9f7f5975eb79647c3db5c61467af25.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
 &lt;p&gt;順便提一下，如您在詞雲中看到的，還有一些用户更喜歡 Arch。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Rust 程序員使用他們的 Rust 程序針對一系列的平台。我們發現針對嵌入式和移動平台的目標用户有所增加，但除此之外，平台分佈與去年大致相同。由於 WebAssembly 目標相當多樣化，我們這次將其分為兩個單獨的類別。根據結果，很明顯，在使用 WebAssembly 時，它主要是在瀏覽器（23%）的上下文中，而不是其他用例（7%）。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111901_JLWt_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;當然，我們不能忘記許多程序員最喜愛的主題：他們使用哪個 IDE（開發環境）。儘管 Visual Studio Code 仍然是最受歡迎的選擇，但今年的市場份額下降了 5 個百分點。另一方面，Zed 編輯器似乎最近獲得了相當大的關注度。選擇「其他」的少數人正在使用各種各樣的不同工具：從 CursorAI 到經典如 Kate 或 Notepad++。特別提一下使用「ed」的 3 個人，這真是一項了不起的成就。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111912_hDw1_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-e87b497c85dbd1dced8a457b81fc3d05e46.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
 &lt;p&gt;您還可以查看詞雲，它總結了對此問題的開放性回答（「其他」類別），以瞭解其他哪些編輯器也受歡迎。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Rust 在工作中的使用&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;我們很高興看到越來越多的人在工作時使用 Rust 進行大部分編碼，從去年的 34% 上升到 38%。在過去幾年中，這一指標呈現出明顯的上升趨勢。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111924_MVQm_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Rust 在公司中的使用似乎也在增加，因為 45% 的受訪者表示他們的組織在 Rust 上的使用並非微不足道，這比 2023 年增加了 7 個百分點。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111934_YhGk_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;再次強調，我們調查受訪者僱主投資 Rust 的首要原因是可以構建相對正確且無 bug 的軟件。其次受歡迎的原因是 Rust 的性能特性。21% 在工作中使用 Rust 的受訪者這麼做是因為他們已經熟悉它，因此它是他們的默認選擇，比 2023 年增加了 5 個百分點。這似乎表明，Rust 正成為越來越多公司選擇的基礎語言之一。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111945_PfzP_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;與上一年相似，很大比例的受訪者（82%）報告説 Rust 幫助他們的公司實現了目標。總的來説，似乎程序員和公司對他們在 Rust 上的使用感到非常滿意，這真是太好了！&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111956_tivA_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;在技術領域，情況與前一年相當相似。Rust 似乎特別受歡迎，用於創建服務器後端、Web 和網絡服務以及雲計算技術。它似乎也在嵌入式用例方面獲得了更多的關注。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/112012_w7WV_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;您可以向右滾動圖表以查看更多領域。請注意，在 2023 年的調查中，汽車領域並未作為封閉答案提供（它只是通過開放式答案輸入的），這或許可以解釋為什麼會有如此大的跳躍。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;看到專業 Rust 使用的持續增長以及許多用户對其性能、控制、安全性、安全性、愉悦性等方面的信心，這令人興奮！&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;挑戰&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;正如往常一樣，State of Rust 調查的主要目標之一是揭示過去一年 Rustaceans 心中的挑戰、擔憂和優先事項。&lt;/p&gt; 
&lt;p&gt;我們詢問了用户關於限制他們生產力的 Rust 方面。不出所料，緩慢的編譯速度位列榜首，這似乎一直是 Rust 用户的永久性擔憂。一如既往，有努力正在進行中以提高編譯器的速度，例如啓用並行前端或默認切換到更快的鏈接器。我們邀請您測試這些改進，並告訴我們如果您遇到任何問題。&lt;/p&gt; 
&lt;p&gt;其他挑戰包括對 Rust 調試的支持不佳以及 Rust 編譯器工件的高磁盤使用量。另一方面，大多數 Rust 用户似乎對它的運行時性能、編譯器的正確性和穩定性以及 Rust 的文檔都非常滿意。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/112026_HY9X_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;關於 Rust 用户希望穩定（或實現）的具體不穩定（或缺失）功能，最希望的是異步閉包和 if/let while 鏈。嗯，好消息是！異步閉包將在 Rust 的下一個版本（1.85）中穩定，而 if/let while 鏈有望在 Edition 2024 發佈後不久跟進很快之後，這次發佈也將發生在 Rust 1.85 中。&lt;/p&gt; 
&lt;p&gt;其他備受渴望的功能包括生成器（同步和異步）以及更強大的泛型常量表達式。您可以關注 Rust 項目目標以跟蹤這些（以及其他）功能的進展。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/112038_zfN3_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;在對此問題的公開回答中，人們真的很有幫助，並盡力描述限制他們生產力的最顯著問題。我們看到了關於異步編程（永恆的寵兒）的挑戰，錯誤的可調試性（人們普遍喜歡，但並不適合每個人）或 Rust 工具緩慢或資源密集（rust-analyzer 和 rustfmt）的提及。一些用户還希望有更好的 IDE 故事和與其他語言的改進互操作性。&lt;/p&gt; 
&lt;p&gt;今年，我們還增加了一個關於 Rust 進化速度的新問題。雖然大多數人似乎對現狀感到滿意，但回答此問題的人中有超過四分之一的人希望 Rust 能夠更快地穩定和/或添加新功能，只有 7% 的受訪者希望 Rust 放慢或完全停止添加新功能。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/112052_mhgg_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;有趣的是，當我們詢問受訪者關於他們對 Rust 未來發展的主要擔憂時，其中一個最常提到的答案是擔心 Rust 會變得過於複雜。這似乎與上一個問題的答案形成了對比。也許 Rust 用户仍然認為 Rust 的複雜性是可控的，但他們擔心有一天它可能會變得過於複雜。&lt;/p&gt; 
&lt;p&gt;我們很高興地看到，對 Rust 項目治理和 Rust 基金會支持不足的擔憂在 2023 年下降了約 6 個百分點。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/112103_2fAx_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;展望未來&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;每年，Rust 狀態調查的結果都有助於揭示 Rust 項目和生態系統中許多需要改進的領域，以及對我們社區運作良好的方面。&lt;/p&gt; 
&lt;p&gt;如果您對 Rust 年度調查有任何建議，請告訴我們！&lt;/p&gt; 
&lt;p&gt;我們非常感謝參與 2024 年 Rust 狀態調查並幫助其創建的人們。雖然開發和維護一種編程語言總是伴隨着挑戰，但今年我們很高興看到高水平的調查參與和坦率的反饋，這將真正幫助我們讓 Rust 更好地服務於每個人。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334273/2024-state-of-rust-survey-results</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334273/2024-state-of-rust-survey-results</guid>
            <pubDate>Sat, 08 Feb 2025 03:27:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>RWKV 首屆全球開發者大會定檔 2 月 21 日，研討 RWKV-7 架構與未來趨勢</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;新一代大模型架構 RWKV 將於 &lt;strong&gt;2025 年 2 月 22 日&lt;/strong&gt;在&lt;strong&gt;上海&lt;/strong&gt;舉辦首屆主題為 &lt;strong&gt;《RWKV-7 架構與未來趨勢》&lt;/strong&gt; 的開發者大會，大會將深入探討 RWKV-7 的獨家技術亮點、應用場景以及未來趨勢，展示 RWKV 在推動全球 AI 發展中的前瞻性與領導力。&lt;/p&gt; 
&lt;p&gt;RWKV-7 架構採用動態狀態演化（dynamic state evolution）機制，超越了傳統的 attention/linear attention 範式，擁有強大的上下文學習（in-context learning）能力和持續學習能力。RWKV-7 模型在推理過程中就能不斷自動根據新的數據進行自我優化和改進（test-time training），從而顯著提升了模型的理解力和處理能力。例如 RWKV-7 2.9B 模型的英文和多語言能力（英文評測 71.1%，多語言評測 62.3%），均顯著超越所有同尺寸模型，包括 Llama 3.2 3B（英文評測 68.7%，多語言評測 57.3%）、Qwen2.5 3B（英文評測 68.6%，多語言評測 57.0%）等知名優秀開源模型。且 RWKV-7 2.9B 只訓練了 3T tokens，另兩者訓練了接近 20T tokens。更大規模的 RWKV-7 也在訓練中。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-0dde119f43dfd830ac5ecc616e6a70e1783.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;此次大會將匯聚來自全球的技術專家、頂尖大學教授、行業領袖與創業者，預計超過 3000 名開發者和 AI 技術愛好者將參與其中。大會將設有多個&lt;strong&gt;分享和互動環節&lt;/strong&gt;，為參與者提供一個寶貴的交流與合作平台，幫助全球開發者共同探索 AI 的未來發展方向。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;RWKV 開發者論壇演講嘉賓及議程：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-a9aff24ab7e5cc3158b5d66b43b45612a83.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;RWKV 開發者大會 | 大會信息：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;時間：2025 年 2 月 22 日 14:00&lt;/li&gt; 
 &lt;li&gt;地點：上海漕河涇現代服務園大廈 A6 號樓&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;RWKV 開發者大會 | 報名二維碼：&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;200&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-1646ff713189f4ad7d2790a64066d4124a6.jpg&quot; width=&quot;200&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;未來，RWKV 將繼續通過持續創新和生態建設，致力於為全球開發者提供強大的技術支持與資源，推動 AI 技術的普及與應用，敬請期待！&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334263</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334263</guid>
            <pubDate>Sat, 08 Feb 2025 02:51:00 GMT</pubDate>
            <author>來源: 投稿</author>
        </item>
        <item>
            <title>x-easypdf v3.3.0 發佈，擁有 AI 加持的 pdf 框架</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;x-easypdf v3.3.0 發佈，擁有 AI 加持的 pdf 框架&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;319&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-58a18b3315bcc283bc5eab13ddfa051a4dd.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;x-easypdf 是一個 java 語言簡化處理 pdf 的框架，包含 fop 模塊與 pdfbox 模塊，fop 模塊以創建功能為主，基於 xsl-fo 模板生成 pdf 文檔，以數據源的方式進行模板渲染；pdfbox 模塊以編輯功能為主，對標準的 pdfbox 進行擴展，添加了成噸的功能。&lt;/p&gt; 
&lt;p&gt;本次更新內容如下：&lt;/p&gt; 
&lt;p&gt;新特性：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;【pdfbox】新增 jpeg2000 格式圖像支持&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增大模型解析文檔的支持&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增開源中國（gitee）AI 解析器&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增智譜（glm）AI 解析器&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增騰訊（hunyuan）AI 解析器&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增阿里（qwen）AI 解析器&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增深度求索（deepseek）AI 解析器&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增字節跳動（doubao）AI 解析器&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增崑崙萬維（tiangong）AI 解析器&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增月之暗面（kimi）AI 解析器&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增訊飛（spark）AI 解析器&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增線性化支持&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增 office 文件轉換 pdf 的支持（依賴 office 服務）&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增 word 轉換器&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增 excel 轉換器&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增 ppt 轉換器&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增 html 轉換器&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增 rtf 轉換器&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增附件處理器&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增加載 awt 字體的支持&lt;/li&gt; 
 &lt;li&gt;【fop】新增條形碼無白邊配置&lt;/li&gt; 
 &lt;li&gt;【fop】新增設置條形碼緩存的方法&lt;/li&gt; 
 &lt;li&gt;【fop】新增權限配置&lt;/li&gt; 
 &lt;li&gt;【fop】新增從資源路徑加載 awt 字體的支持&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;原有變更：&lt;/p&gt; 
&lt;p&gt;maven 座標變更，原 &lt;code&gt;groupId 「org.dromara.x-easypdf」&lt;/code&gt; 變更為 &lt;code&gt;org.dromara&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;問題修復：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;【pdfbox 模塊】修復表格組件單元格添加多組件換行錯誤問題&lt;/li&gt; 
 &lt;li&gt;【pdfbox 模塊】修復表格重疊問題&lt;/li&gt; 
 &lt;li&gt;【pdfbox 模塊】修復空文本錯誤問題&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334262/x-easypdf-3-3-0-released</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334262/x-easypdf-3-3-0-released</guid>
            <pubDate>Sat, 08 Feb 2025 02:50:00 GMT</pubDate>
            <author>來源: 投稿</author>
        </item>
        <item>
            <title>Zadig：首個深度集成 DeepSeek 的 DevOps 平台</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                    
                                                                                                                                                    &lt;p&gt;&lt;img height=&quot;383&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-2230cda1042253217386ec9e74ab4b9bf7b.png&quot; width=&quot;898&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_1&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;引言：當工程效能遭遇數據迷霧&lt;/h1&gt; 
&lt;p&gt;在微服務與雲原生架構普及的今天，DevOps 團隊正面臨雙重挑戰：日均千次的流水線執行產生 TB 級數據，卻難以轉化為有效洞見；K8s 生產環境複雜度指數級增長，人工巡檢如同大海撈針。Zadig 與 DeepSeek 的深度協同，首次將 AGI 技術注入 DevOps 全生命週期，推出「&lt;strong&gt;AI 效能分析&lt;/strong&gt;」與「&lt;strong&gt;AI 環境巡檢&lt;/strong&gt;」兩大核心能力，實現從經驗驅動到智能決策的範式轉移。現已面向社區用户全面開放，開源力量再進化！&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_2&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;AI 效能診斷：讓數據説話，精準定位效能瓶頸&lt;/h1&gt; 
&lt;p&gt;傳統工程效能分析往往依賴人工統計與經驗判斷，效率低且易受主觀因素影響，而 Zadig 沉澱了研發過程的構建、部署、測試等大量效能數據，基於 DeepSeek 的 AI 能力，通過智能分析數據，為團隊提供客觀、可操作的改進建議。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;核心能力：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;智能數據分析&lt;/strong&gt;：通過自然語言交互（Prompt 方式），AI 可快速分析流水線、構建、測試等環節的效能數據，識別瓶頸問題。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img height=&quot;1530&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-74e2b55272658d6609a7f07b0434738960b.png&quot; width=&quot;2942&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;問題精準定位&lt;/strong&gt;：無論是構建耗時過長、測試通過率低，還是資源利用率不足，AI 都能清晰指出問題所在。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img height=&quot;1486&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-888422976ca2585f16a7cfc4a352681b7ad.png&quot; width=&quot;2948&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;科學改進建議&lt;/strong&gt;：基於分析結果，AI 提供具體的優化建議，例如並行測試策略、資源分配調整等，幫助團隊快速提升效能。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img height=&quot;1486&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-d89c13f6e249b6d1517a5d28bd169d63f7d.png&quot; width=&quot;2948&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;場景價值：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;無需手動分析海量數據，AI 自動生成效能報告，節省大量時間。&lt;/li&gt; 
 &lt;li&gt;通過數據驅動的優化建議，團隊可快速落地改進措施，提升交付效率。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id=&quot;OSC_h1_3&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;AI 環境巡檢：全天候守護，讓環境問題無所遁形&lt;/h1&gt; 
&lt;p&gt;面對複雜的 Kubernetes 生產環境，傳統人工巡檢耗時費力，且難以覆蓋潛在風險。Zadig 的 AI 環境巡檢功能，通過定時巡檢與智能告警，確保環境穩定性。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;核心能力：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;定時自動巡檢&lt;/strong&gt;：AI 定期對 Kubernetes 環境進行全方位檢查，覆蓋資源狀態、服務健康度等關鍵指標。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img height=&quot;2170&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-6351737e713118924456d4aa5a02c16d82b.png&quot; width=&quot;3410&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;智能問題識別&lt;/strong&gt;：自動識別常見環境問題，如 Pod 異常、資源不足、配置錯誤等，並給出相應的解決方案。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img height=&quot;1530&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-f291d5d35f8a63fbb52c7f1a73cd20d7696.png&quot; width=&quot;2942&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;即時告警推送&lt;/strong&gt;：巡檢結果通過 IM 工具（如飛書、釘釘、企業微信等）實時通知相關責任人，確保問題第一時間被處理。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img height=&quot;1666&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-c8ddc2f1472da2e9cebbc2efb6f9a52cdef.png&quot; width=&quot;2234&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;場景價值：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;無需手動巡檢，AI 自動完成環境健康檢查，大幅降低人力成本。&lt;/li&gt; 
 &lt;li&gt;通過即時告警，團隊可快速響應環境問題，避免小問題演變為大故障。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id=&quot;OSC_h1_4&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;結語&lt;/h1&gt; 
&lt;p&gt;Zadig 通過集成 DeepSeek 的 AI 能力，將智能技術深度融入 DevOps 流程，為研發運維團隊帶來了前所未有的效能提升和環境穩定性保障。未來，隨着 AI 技術的不斷發展，Zadig 將繼續探索更多創新應用場景，助力企業實現數字化轉型，提升核心競爭力。&lt;/p&gt; 
&lt;p&gt;Zadig 免費基礎版已全面支持 AI 能力，0 成本解鎖智能 DevOps！&lt;/p&gt; 
&lt;p style=&quot;color:#ff4c88; margin-left:0; margin-right:0; text-align:center&quot;&gt;&lt;strong&gt;即日起，Zadig 新版發佈&lt;br&gt; 掃碼諮詢搶先體驗&lt;/strong&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#191b1f; margin-left:0; margin-right:0; text-align:center&quot;&gt;&lt;img height=&quot;943&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-0c7876673ed701ed97107bb53b607d661dd.png&quot; width=&quot;1797&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:center&quot;&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fkoderover%2Fzadig&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;Zadig 在 Github&lt;/a&gt;&amp;nbsp;/&amp;nbsp;&lt;a href=&quot;https://gitee.com/koderover/zadig&quot; rel=&quot;nofollow&quot;&gt;Zadig 在 Gitee&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;strong&gt;&lt;span&gt;推薦閲讀：&lt;/span&gt;&lt;/strong&gt; 
 &lt;/div&gt; 
 &lt;div&gt; 
  &lt;p style=&quot;color:#002a64; margin-left:0; margin-right:0&quot;&gt;&lt;a href=&quot;https://my.oschina.net/koderover/blog/11210095&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;Zadig 官網博客正式發佈，技術乾貨實踐管飽&lt;/a&gt;&amp;nbsp;/&amp;nbsp;&lt;a href=&quot;https://my.oschina.net/koderover/blog/16492101&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;流水線早已 out 了？你需要更高效能的工作流&lt;/a&gt;&amp;nbsp;/&lt;span style=&quot;background-color:#ffffff; color:#002a64&quot;&gt;&amp;nbsp;&lt;/span&gt;&lt;a href=&quot;https://my.oschina.net/koderover/blog/10316143&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;Jenkins 遷移 Zadig，新項目實施上線效率提升 6 倍&lt;/a&gt;&amp;nbsp;/&amp;nbsp;&lt;a href=&quot;https://my.oschina.net/koderover/blog/16507771&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;🚀 重大更新！Zadig V3.2.0 重塑工作流體驗，強勢推出迭代管理&lt;/a&gt;&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/div&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/koderover/blog/17622087</link>
            <guid isPermaLink="false">https://my.oschina.net/koderover/blog/17622087</guid>
            <pubDate>Sat, 08 Feb 2025 02:44:00 GMT</pubDate>
            <author>原創</author>
        </item>
        <item>
            <title>月之暗面因 DeepSeek 調整工作重心，內部人士：強化學習或許會是個方向</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;據媒體報道，月之暗面內部已經將「持續拿到 SOTA 結果」確定為當下最重要的工作目標。&lt;/p&gt; 
&lt;p&gt;2025 年，月之暗面圍繞模型能力的關鍵方向除了繼續強化多模態部分外，還會繼續強化長文本推理能力。報道分析稱，DeepSeek 爆火後，DeepSeek 與月之暗面存在的路線差異，讓外界面臨重新審視月之暗面技術模式、用户增長模式的情況。&lt;/p&gt; 
&lt;p&gt;而今，DeepSeek 採用區別與月之暗面的路線，也取得了現階段更為出色的效果。業內人士認為，月之暗面如果想守住生態位，「需要做一些改變或者嘗試，比如開源，比如調整引流策略等。」&lt;/p&gt; 
&lt;p&gt;不過目前，月之暗面尚未明確是否「接入」DeepSeek，對於接下來是否「開源」，公司也未置評媒體問詢。&lt;/p&gt; 
&lt;p&gt;對於月之暗面是否會因 DeepSeek 而調整工作重心一事，向月之暗面方面求證，截止發稿公司暫無回應。不過有內部人士透露稱，&lt;strong&gt;「RL（強化學習）大概率會是一個（工作重點）方向」&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;去年 11 月份月之暗面宣佈推出&lt;a href=&quot;https://www.oschina.net/news/320859&quot; target=&quot;_blank&quot;&gt;新一代數學推理模型 k0-math &lt;/a&gt;之際，Kimi 探索版便通過運用強化學習技術創新了搜索體驗，在意圖增強、信源分析和鏈式思考三大推理能力上實現突破。彼時，月之暗面 Kimi 創始人楊植麟便對強化學習這一技術路線帶來的模型能力提升給予了高度評價。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-f3ef9f71486f2898a14d0b17103cbd4308a.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;而在近日 OpenAI 發佈關於推理模型在競技編程中應用的研究論文報告《Competitive Programming with Large Reasoning Models》中，論文也特別提到，「中國的 DeepSeek-R1 和 Kimi k1.5 通過獨立研究顯示，利用思維鏈學習（COT）方法，可顯著提升模型在數學解題與編程挑戰中的綜合表現。其中 k1.5 便是 DeepSeek 和 Kimi 在 1 月 20 日同時發佈的新型推理模型。」&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334255</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334255</guid>
            <pubDate>Sat, 08 Feb 2025 02:28:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>微信搜索接入 DeepSeek，正在灰度測試中</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;2 月 15 日，部分微信用户發現，微信搜索已經上線「AI 搜索」功能，並接入 DeepSeek-R1 提供的「深度思考」服務。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2 月 16 日，記者從騰訊集團確認，微信搜一搜在調用混元大模型豐富 AI 搜索的同時，正式灰度測試接入 DeepSeek&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/101747_uyUN_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;騰訊方面表示，部分測試用户可在微信對話框頂部搜索入口，看到「AI 搜索」字樣，點擊進入後，可免費使用 DeepSeek-R1 滿血版模型，獲得更多元化的搜索體驗。若未顯示該入口，説明此次灰度測試暫未覆蓋到該用户賬號，可耐心等待後續開放。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/101729_EaEQ_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;有用户表示，通過在微信 AI 搜索「如何在微信上使用 DeepSeek 的 R1 模型」問題得到的答案是，該功能正灰度測試中，僅部分用户可見，微信版本需更新至最新版本。若暫未獲得測試方案，微信團隊正逐步擴大測試範圍，建議定期檢查更新及搜索功能變化。&lt;/p&gt; 
&lt;p&gt;從功能附帶的開源與鳴謝聲明能看出，&lt;strong&gt;微信中內置的 DeepSeek R1 基於開源版本構建而來，但其中並未明確提及其使用的模型體積，是否是 671B 的「滿血」R1 版本&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/101712_mjq7_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;據證券時報今日消息，對於一些相關細節，騰訊方面還作了進一步説明：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;1、AI 搜索的數據源包含公眾號嗎？&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;微信 AI 搜索接入的 DeepSeek 支持聯網搜索（用户無需手動選擇），基於公眾號等豐富的微信生態內容，以及全網優質內容，能為用户提供更全面的高質量回答。&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;2、AI 搜索已經全量嗎？&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;目前該能力還在灰度測試中，將根據用户體驗和反饋持續優化。&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;3、微信的搜索場景為什麼要接入大模型？&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;大模型可以提升搜索的智能化和精準度，如更好地理解用户的搜索意圖，分析和處理複雜的查詢內容等。&lt;/p&gt; 
 &lt;p&gt;結合用户需求，騰訊在搜索場景中接入了包括混元、DeepSeek 在內的大模型，進一步豐富用户的搜索體驗。&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;4、AI 搜索會用我微信內的朋友圈、聊天等個人信息嗎？&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;AI 搜索僅整合公眾號及互聯網其他公開信息，不會使用用户的個人信息和相關隱私信息。&lt;/p&gt; 
&lt;/blockquote&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334252</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334252</guid>
            <pubDate>Sat, 08 Feb 2025 02:18:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>百度搜索宣佈將全面接入 DeepSeek</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;百度搜索發文宣佈，&lt;span&gt;&lt;span&gt;&lt;span&gt;為豐富更多元化的搜索體驗，&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;百度搜索將全面接入&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;DeepSeek&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;和文心大模型最新的深度搜索功能。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;搜索用户可免費使用 DeepSeek 和文心大模型深度搜索功能，文心智能體平台的開發者也將能隨時調用 DeepSeek 模型創建並調優智能體。&lt;/p&gt; 
&lt;p&gt;根據介紹，文心大模型深度搜索功能於 2 月 13 日上線，具備更強大的思考規劃和工具調用能力，可為用户提供專家級內容回覆，並處理多場景任務，實現多模態輸入與輸出。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;161&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-f7a3cff12b27c30d05def83a2c2f4477122.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334250</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334250</guid>
            <pubDate>Sat, 08 Feb 2025 02:03:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
    </channel>
</rss>