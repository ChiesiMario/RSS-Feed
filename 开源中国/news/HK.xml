<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - news - 繁體中文（香港）</title>
    <link>https://www.oschina.net/news/project</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news" rel="self" type="application/rss+xml"/>
    <description>已對該 RSS 進行格式化操作：中英字符之間插入空格、使用直角引號、標點符號修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-hk</language>
    <lastBuildDate>Fri, 01 Aug 2025 07:49:12 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>Vercel 發佈 AI SDK 5，構建全棧 AI 應用的開發工具包</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;Vercel 發佈了 &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fvercel.com%2Fblog%2Fai-sdk-5" target="_blank"&gt;AI SDK 5&lt;/a&gt;，這是一個用於構建全棧 AI 應用的開發工具包，它在前代基礎上進行了全面升級，提供了更強大的功能、更高的靈活性和更好的開發體驗。&lt;/p&gt; 
&lt;p&gt;&lt;img height="565" src="https://static.oschina.net/uploads/space/2025/0801/153828_B1BV_2720166.png" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;以下是 AI SDK 5 的主要更新內容：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;聊天功能&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;徹底重建&lt;/strong&gt;：引入了兩種不同的消息類型——&lt;code&gt;UIMessage&lt;/code&gt; 和 &lt;code&gt;ModelMessage&lt;/code&gt;，解決了開發者在狀態管理和聊天曆史持久化方面的挑戰。&lt;code&gt;UIMessage&lt;/code&gt; 是應用程序狀態的「真實來源」，包含所有消息、元數據和工具結果，推薦用於持久化存儲；&lt;code&gt;ModelMessage&lt;/code&gt; 則是為語言模型優化的簡化表示。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;類型安全&lt;/strong&gt;：開發者可以創建自定義的 &lt;code&gt;UIMessage&lt;/code&gt; 類型，並在服務器和客户端之間傳遞，實現端到端的完全類型安全。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;流式傳輸&lt;/strong&gt;：引入了 &lt;code&gt;Data Parts&lt;/code&gt; 功能，允許開發者發送自定義數據塊，如狀態更新或部分工具結果，同時保持代碼的可維護性。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Agent 構建&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;精確的執行流控制&lt;/strong&gt;：&lt;code&gt;stopWhen&lt;/code&gt; 參數允許開發者定義工具調用循環的停止條件，例如達到特定步數或調用了某個特定工具；&lt;code&gt;prepareStep&lt;/code&gt; 鈎子則允許在每一步執行前動態調整參數，如更換模型、修改系統提示或啓用/禁用特定工具。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;面向對象封裝&lt;/strong&gt;：新增的 &lt;code&gt;Agent&lt;/code&gt; 類為構建 Agent 提供了面向對象的封裝。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;語音功能&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;統一提供商抽象&lt;/strong&gt;：通過 &lt;code&gt;experimental_generateSpeech&lt;/code&gt; 和 &lt;code&gt;experimental_transcribe&lt;/code&gt; API，為 OpenAI、ElevenLabs、DeepGram 等提供商的語音生成和轉錄服務提供了統一、類型安全的接口。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;工具調用增強&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;動態工具&lt;/strong&gt;：支持動態工具、提供商執行的工具（如 OpenAI 的網頁搜索）以及更精細的生命週期鈎子。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;其他更新&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;流媒體協議&lt;/strong&gt;：將 &lt;code&gt;SSE&lt;/code&gt; 作為標準的流媒體協議。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;全局提供商&lt;/strong&gt;：引入全局提供商（默認為 Vercel AI Gateway），簡化模型 ID 的使用。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;訪問原始數據&lt;/strong&gt;：支持訪問原始請求和響應數據以增強調試和控制能力。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Zod 4 支持&lt;/strong&gt;：增加了對 Zod 4 的支持。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;遷移工具&lt;/strong&gt;：為幫助用户平滑遷移，Vercel 提供了自動化的代碼修改工具（&lt;code&gt;codemods&lt;/code&gt;）。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fvercel.com%2Fblog%2Fai-sdk-5" target="_blank"&gt;點此查看發佈説明&lt;/a&gt;。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/363628/vercel-ai-sdk-5</link>
      <guid isPermaLink="false">https://www.oschina.net/news/363628/vercel-ai-sdk-5</guid>
      <pubDate>Fri, 01 Aug 2025 07:42:49 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>微軟向報告 .NET 漏洞的用户支付高達 40000 美元的報酬</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;許多公司提供漏洞賞金計劃，鼓勵人們搜索並發現軟件中的安全漏洞，並私下向供應商報告，以便在惡意攻擊者利用漏洞之前實施並應用修復程序。安全研究人員和其他公眾會獲得金錢獎勵，從而獲得經濟激勵。&lt;/p&gt; 
&lt;p&gt;現在，微軟已宣佈對其 .NET Bounty 計劃進行重大更新。&lt;/p&gt; 
&lt;p&gt;獎勵金額現從 7000 美元起，最高可達令人垂涎的 40000 美元。請注意，最高獎勵僅適用於私下披露遠程代碼執行 (RCE) 或特權提升 (EoP) 漏洞，並提供完整文檔且造成嚴重影響的情況。&lt;/p&gt; 
&lt;p&gt;各獎勵等級的細分如下：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0801/153421_aSuq_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;值得注意的是，.NET 賞金計劃主要圍繞 .NET 和 ASP.NET&amp;nbsp;Core 展開，包括 Blazor 和 Aspire。但新的產品類別現在涵蓋了所有受支持的 .NET 和 ASP.NET 版本、適用於 .NET Framework 的 ASP.NET&amp;nbsp;Core、上述內容提供的模板、其存儲庫中的 GitHub Actions 以及 F# 等相關技術。&lt;/p&gt; 
&lt;p&gt;更新後的獎勵結構確保了嚴重性等級的明確定義，以便高影響的問題獲得更高的獎勵，同時還提供了關於如何將報告視為「完整」的指南。您可以在微軟的&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmsrc.microsoft.com%2Fblog%2F2025%2F07%2F.net-bounty-program-now-offers-up-to-40000-in-awards%2F" target="_blank"&gt;專門博客文章&lt;/a&gt;中找到更多信息。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/363626/dotnet-bounty-program-now-offers-up-to-40000-in-awards</link>
      <guid isPermaLink="false">https://www.oschina.net/news/363626/dotnet-bounty-program-now-offers-up-to-40000-in-awards</guid>
      <pubDate>Fri, 01 Aug 2025 07:35:49 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>沒有套路，真的免費：模力方舟全免費的模型都在這了</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;想找無套路，真免費，不限額度的大模型？別再滿世界找了——來模力方舟就夠。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;立即訪問模力方舟 AI 模型廣場：&lt;a href="https://ai.gitee.com/serverless-api"&gt;https://ai.gitee.com/serverless-api&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;本文整理了目前模力方舟上完全免費開放使用的模型， 不是「每天送你 10 次體驗」，也不是「註冊送額度」，更不是「邀請獲禮金」，而是&lt;strong&gt;不限次數、毫無限制、直接免費用&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;無論你是想&lt;strong&gt;生成文本、寫代碼、合成語音、做推理，還是想過濾下內容風險&lt;/strong&gt;，模力方舟都準備好了免費的相關模型，&lt;strong&gt;全部 0 元接入、不限次數&lt;/strong&gt;。&lt;/p&gt; 
&lt;h4&gt;通用語言模型：超能聊，跑得快&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Qwen3-8B / Qwen3-4B / Qwen3-0.6B&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0801/151643_pivg_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;國產開源的 Qwen3 系列，從輕量級到中型參數都有，支持「思考模式」與「對話模式」自由切換，還能寫代碼、講英文、做推理。模型權重與 API 已全面開放，商用也不用擔心授權問題。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Qwen2-7B-Instruct&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0801/151659_Ojcp_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;升級版 Qwen 架構，專為「你讓它幹啥它就幹啥」的指令模式設計，適合結構化問答、信息抽取等任務，照樣免費用。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;InternLM3-8B-Instruct&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;書生·浦語第三代，和 Qwen 一樣屬於國產頭部陣營，指令跟隨能力強、推理不拉胯，在 8B 量級裏非常能打。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0801/151711_9iVU_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;GLM-4-9B / GLM-4-9B-Chat&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0801/151725_ZpJL_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;來自智譜的通用語言模型，性能紮實，特別適閤中文語境下的多輪問答和對話場景。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;DeepSeek-R1-Distill-Qwen 系列（14B / 7B / 1.5B）&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0801/151739_89cE_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;DeepSeek-R1 模型的輕量蒸餾版，覆蓋大中小三種參數體型，推理性能不錯但定位仍是通用語言模型，適合對資源有要求的部署場景。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h4&gt;數學 / 定理證明：專精模型上場&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;DeepSeek-Prover-V2&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0801/151756_uzEG_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;專為 Lean 4 定理證明設計，具備將複雜問題拆解為子目標、合成鏈式推理過程的能力。模型通過 DeepSeek-V3 驅動的遞歸證明管線進行冷啓動訓練，融合非形式與形式數學推理，顯著提升了定理證明效率與泛化能力。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h4&gt;代碼生成模型：小身材，大能力&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;codegeex4-all-9b&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0801/151809_32Qh_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;多語言代碼生成模型，支持包括代碼補全和生成、代碼解釋器、網絡搜索、函數調用、倉庫級代碼問答在內的全面功能，覆蓋軟件開發的各種場景。是參數少於 10B 的頂尖代碼生成模型。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h4&gt;語音識別/合成模型：小體積，大用途&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;SenseVoiceSmall&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0801/151839_AoYq_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;來自通義千問團隊的輕量級語音模型，結合高效算法和小型化設計，提供低延遲和高準確度的語音處理能力，適用於資源受限的應用場景。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Spark-TTS-0.5B&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0801/151851_iZPk_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;來自 Spark Audio 團隊的文本轉語音模型，能夠實現高精度、自然的語音合成。它高效、靈活、功能強大，適用於研究和生產環境。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h4&gt;風控識別模型：確保內容合規&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;nsfw-classifier / Security-semantic-filtering&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0801/151909_OgHB_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;模力方舟提供了兩種風控識別模型，分別面向圖片分類和文本敏感內容過濾，確保內容合規與敏感信息保護。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h4&gt;檢索增強開發者福利：向量 + 重排模型也全免費&lt;/h4&gt; 
&lt;p&gt;模力方舟攜手國產 GPU 夥伴，已將模型廣場中 Embedding 與 Reranker 模型&lt;strong&gt;全部開放免費使用&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;RAG 架構必備的檢索向量和重排序能力，即刻零成本上手！&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Embedding 向量模型：支持中文、英文、多語言編碼，適配主流語義檢索任務；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Reranker 重排序模型：優化文檔排序效果，讓你的檢索結果相關且可信。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0801/151936_fUHW_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;所有模型支持 API 調用，無需申請、無需授權、直接接入，非常適合搜索類應用、知識庫問答、RAG 業務開發等場景使用。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h3&gt;真免費，不限次，現在就能用&lt;/h3&gt; 
&lt;p&gt;馬建倉再強調一次：上面這些模型&lt;strong&gt;沒有「體驗額度」，是「完全免費」&lt;/strong&gt;。不限制調用次數、不限制模型功能，可以直接接入使用。&lt;/p&gt; 
&lt;p&gt;立即訪問模力方舟 AI 模型廣場：&lt;strong&gt;&lt;a href="https://ai.gitee.com/serverless-api"&gt;https://ai.gitee.com/serverless-api&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/363622</link>
      <guid isPermaLink="false">https://www.oschina.net/news/363622</guid>
      <pubDate>Fri, 01 Aug 2025 07:20:49 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>騰訊混元 3D 世界模型技術亮點速覽</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;騰訊近日正式發佈&lt;strong&gt;混元 3D 世界模型 1.0（HunyunWorld-1.0）&lt;/strong&gt;並全面開源。據稱這是首個開源並且兼容傳統 CG 管線的可漫遊世界生成模型，為遊戲開發、VR、數字內容創作等領域帶來了全新的可能性。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0801/150655_8uFO_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;根據該模型的技術報告，HunyunWorld-1.0 採用生成式架構，結合全景圖像合成與分層 3D 重建技術，實現了高質量、沉浸式的可漫遊 3D 場景生成。&lt;/p&gt; 
&lt;p&gt;該模型通過語義分層的 3D 場景表徵與生成算法，同時支持"文生世界"和"圖生世界"兩種生成方式。主要技術框架包括三部分，即全景世界代理生成、基於語義的世界分層與分層世界重建。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0801/150932_iXbY_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;混元 3D 世界模型 1.0（HunyunWorld-1.0）是融合兩類方法優勢的創新框架，能夠依據文本或圖像輸入生成沉浸式、可探索、可交互的 3D 場景。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;360°沉浸體驗 ：通過全景圖將複雜的 3D 世界高效地表徵為 360 度覆蓋的 2D 圖像代理，為後續生成完整的 3D 世界建模提供了豐富的空間信息；&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0801/151052_YWxE_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;工業級兼容性 ：生成的世界場景支持導出標準的 3D 網格格式，能夠無縫導入現有 3D 建模軟件和主流遊戲引擎，用於二次開發；&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0801/151131_DzWQ_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;原子級交互&amp;nbsp;：通過物體解耦的 3D 建模方式，生成物體和背景可分離的 3D 世界，支持精準的物體級交互控制，提升了生成世界的操作自由度。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0801/151149_qD74_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2507.21809" target="_blank"&gt;點此查看更多技術細節&lt;/a&gt;&lt;/em&gt;。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/363621</link>
      <guid isPermaLink="false">https://www.oschina.net/news/363621</guid>
      <pubDate>Fri, 01 Aug 2025 07:12:49 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>字節跳動 Seed 助力清華獲機器人足球世界盃冠軍</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;&lt;span style="color:#000000"&gt;字節跳動 Seed 發文&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F4e6maGpWaEtWfj1rxkhI0w" target="_blank"&gt;宣佈&lt;/a&gt;，其與清華大學趙明國教授團隊聯合研發的人形機器人運動算法 「HumanoidKick」 在 2025RoboCup 機器人世界盃人形組成人組比賽中，成功幫助清華火神隊獲得冠軍。這也是中國機器人足球隊首次在機器人世界盃該組別奪冠。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;根據介紹，此次奪冠的關鍵之一是&lt;strong&gt;基於視覺的&lt;strong&gt;&lt;strong&gt;端到端&lt;/strong&gt;&lt;/strong&gt;自主踢球算法 HumanoidKick&lt;/strong&gt;，由字節跳動 Seed 團隊與清華大學趙明國教授團隊聯合提出並驗證。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;HumanoidKick 算法面向人形機器人硬件，通過基於視覺的深度強化學習，實現了「找球 - 追球 - 踢球」全過程的統一策略，在實際足球比賽中驗證有效。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="368" src="https://oscimg.oschina.net/oscnet/up-28b9adf735a26eea28efb96fd06fe8b5e72.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;HumanoidKick 算法嘗試解決以下三項實際挑戰：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;從視覺到行動的實時聯動&lt;/strong&gt;：傳統機器人足球方案依賴手動編程的預設策略，面對場上變化，常陷入動作卡頓的困境，錯失進攻的有利時機。該算法通過端到端深度強化學習方法，構建了視覺感知與機器人運動控制的毫秒響應機制，讓機器人能像人類球員一樣邊「看」邊「動」。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;從動作整合到自主決策&lt;/strong&gt;：經過數千個環境的並行訓練，機器人得以將多種分散的動作技能整合為統一連貫的端到端策略，構建起「單個動作」 「賽場行為」 「競技策略」之間的關聯。面對實時變化的賽場動態，機器人可以自主決策行動，並泛化出自適應的踢球能力。這種能力進化讓機器人擺脱了被動執行預設動作的侷限，實現了從零散技能到完整策略的突破。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;從仿真到真實環境的適應&lt;/strong&gt;：為了讓算法在真實世界中保持穩定，團隊採用精確建模和域隨機化結合的訓練方案——在仿真環境中，建模真實世界的感知噪聲和物理擾動（如地面條件、關節噪聲），讓機器人在仿真環境中「經歷」各種現實極端場景，實現從仿真環境到真機的無縫應用。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;依託仿真環境中的海量訓練，HumanoidKick 算法在現實賽場中成功幫助機器人脱離既定動作流程束縛，依據瞬息萬變的賽場態勢，迅速自主規劃行動方案。在比賽中，機器人展現出了較好的反應速度，最終取得勝利。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/363620</link>
      <guid isPermaLink="false">https://www.oschina.net/news/363620</guid>
      <pubDate>Fri, 01 Aug 2025 07:09:49 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>谷歌開源 LangExtract，從非結構化文本提取結構化信息的 Python 庫</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;谷歌開源了名為&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fgoogle%2Flangextract" target="_blank"&gt;LangExtract&lt;/a&gt;的 Python 庫，該庫使用 LLMs 根據用户定義的指令從非結構化文本文檔中提取結構化信息（諸如臨牀筆記或報告之類的材料），識別並整理關鍵細節，同時確保提取的數據與源文本相對應。&lt;/p&gt; 
&lt;p&gt;LangExtract 的核心優勢在於其強大的功能特性。首先是「精確的源文本溯源」，它能將每一個提取出的信息精確映射回其在原始文本中的位置，並支持交互式高亮可視化，便於用户追溯和驗證。&lt;/p&gt; 
&lt;p&gt;項目主頁提供了快速上手的代碼示例，演示瞭如何定義提示、提供示例、運行提取，並將結果保存為.jsonl 文件，最後生成交互式 HTML 可視化報告。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0801/145804_AbcO_2720166.gif" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;LangExtract 適用於任何領域，用户僅需提供少量示例即可定義提取任務，無需模型微調。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/363619</link>
      <guid isPermaLink="false">https://www.oschina.net/news/363619</guid>
      <pubDate>Fri, 01 Aug 2025 07:00:49 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>一張圖解釋上下文工程（Context Engineering ）</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;製圖： Victoria Slocum&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-38450d8633e8576132b7439b4dd873cf590.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;以下為解釋&lt;/p&gt; 
&lt;p&gt;---------------------------&lt;br&gt; 提示工程（Prompt Engineering）已死，上下文工程（𝗖𝗼𝗻𝘁𝗲𝘅𝘁 𝗘𝗻𝗴𝗶𝗻𝗲𝗲𝗿𝗶𝗻𝗴） 萬歲！&lt;/p&gt; 
&lt;p&gt;（好吧，也沒完全死掉——但它無疑正在進化成一種遠為更強大的形態）&lt;/p&gt; 
&lt;p&gt;讓我們來認識一下上下文工程（𝗖𝗼𝗻𝘁𝗲𝘅𝘁 𝗘𝗻𝗴𝗶𝗻𝗲𝗲𝗿𝗶𝗻𝗴）——這是一門藝術，致力於構建動態系統，從而精準地為大語言模型（LLM）提供成功完成任務所需的一切。&lt;/p&gt; 
&lt;p&gt;隨着我們從簡單的聊天機器人轉向複雜的 AI 代理（Agent），我們正逐漸意識到，僅僅靠巧妙的提示語是不夠的。真正重要的是精心編排一個完整的信息生態系統，並將這些信息輸入到你的大語言模型中。&lt;/p&gt; 
&lt;p&gt;那麼，這具體意味着什麼呢？&lt;/p&gt; 
&lt;p&gt;它的核心在於構建動態系統，以正確的格式提供正確的信息和工具，從而讓大語言模型能夠切實地完成任務。&lt;/p&gt; 
&lt;p&gt;一個經過上下文工程設計的系統的構成解析：&lt;/p&gt; 
&lt;p&gt;✨用户信息 (𝗨𝘀𝗲𝗿 𝗜𝗻𝗳𝗼𝗿𝗺𝗮𝘁𝗶𝗼𝗻): 偏好、歷史記錄和個性化數據。&lt;br&gt; ✨工具使用 (𝗧𝗼𝗼𝗹 𝗨𝘀𝗲): API、計算器、搜索引擎——任何大語言模型完成工作所需的工具。&lt;br&gt; ✨RAG 上下文 (𝗥𝗔𝗚 𝗖𝗼𝗻𝘁𝗲𝘅𝘁): 從像 Weaviate 這樣的向量數據庫中檢索出的信息。&lt;br&gt; ✨用户輸入 (𝗨𝘀𝗲𝗿 𝗜𝗻𝗽𝘂𝘁): 當前實際的查詢或任務。&lt;br&gt; ✨代理推理 (𝗔𝗴𝗲𝗻𝘁 𝗥𝗲𝗮𝘀𝗼𝗻𝗶𝗻𝗴): 大語言模型的思考過程和決策鏈。&lt;br&gt; ✨聊天曆史 (𝗖𝗵𝗮𝘁 𝗛𝗶𝘀𝘁𝗼𝗿𝘆): 提供對話連續性的先前交互記錄。&lt;/p&gt; 
&lt;p&gt;那麼，它的記憶架構是怎樣的呢？&lt;/p&gt; 
&lt;p&gt;✨短期記憶 (𝗦𝗵𝗼𝗿𝘁-𝘁𝗲𝗿𝗺 𝗺𝗲𝗺𝗼𝗿𝘆): 存在於上下文窗口中，處理當前對話。&lt;br&gt; ✨長期記憶 (𝗟𝗼𝗻𝗴-𝘁𝗲𝗿𝗺 𝗺𝗲𝗺𝗼𝗿𝘆): 存儲在向量數據庫（如 Weaviate）中，跨會話持久化存儲用户偏好和過去的交互記錄。&lt;/p&gt; 
&lt;p&gt;這為什麼重要？&lt;br&gt; 因為當代理系統（agentic systems）失敗時，很少是因為模型本身不夠聰明，而是因為我們沒有給它提供正確的上下文。&lt;/p&gt; 
&lt;p&gt;信息的格式同樣重要。一條結構清晰的錯誤信息，永遠勝過一大堆雜亂的 JSON 數據。就像人類一樣，大語言模型也需要清晰、易於理解的溝通方式。&lt;/p&gt; 
&lt;p&gt;轉載自：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fweibo.com%2F2194035935%2FPDFOm8vhG" target="_blank"&gt;蟻工廠，微博&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/363615</link>
      <guid isPermaLink="false">https://www.oschina.net/news/363615</guid>
      <pubDate>Fri, 01 Aug 2025 06:52:49 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>美國上訴法院維持谷歌應用商店壟斷裁決</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;&lt;span style="color:#000000"&gt;位於舊金山的美國聯邦第九巡迴上訴法院 31 日裁定，駁回谷歌公司上訴，維持此前地方法院的陪審團裁決和法官指令。根據相關裁決和指令，谷歌將不得不改變其 Play 應用商店的一些重要管理方針，包括長期以來不允許其他應用商店在 Play 商店正常運營。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img alt="" height="300" src="https://oscimg.oschina.net/oscnet/up-097faa551a242c0ea617e6e9f397b03bcec.webp" width="300" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;Epic 遊戲公司此前在其開發的《堡壘之夜》遊戲中，通過技術手段讓玩家進行應用內購買時可選擇繞過谷歌的支付系統，從而避免向谷歌支付 30% 的佣金，谷歌隨後將該遊戲從 Play 商店中下架。Epic 遊戲公司在 2020 年就此提起針對谷歌的反壟斷訴訟。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;陪審團 2023 年裁決，谷歌違反了聯邦和加利福尼亞州反壟斷法，故意獲取或維持市場壟斷地位，不合理地限制交易，並非法將 Play 商店的使用與該公司的結算服務捆綁在一起。地方法官 2024 年簽發針對谷歌的指令，要求該公司整改 Play 商店，消除反競爭的行為。谷歌隨後提起上訴，上訴期間該指令暫緩實施。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;聯邦第九巡迴上訴法院法官的裁決駁回了谷歌的上訴，稱地方法院在審理時並未濫用自由裁量權，所發佈的指令得到陪審團裁決以及地方法院自身調查結果的支持。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;這一最新的裁決公佈後，Epic 首席執行官蒂姆·斯威尼在社交媒體平台上表示，安卓版 Epic 遊戲商店將登陸 Play 商店。谷歌方面則表示，會繼續上訴。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;按照相關法律程序，谷歌可以選擇要求第九巡迴上訴法院全體法官複審此案，也可以直接向最高法院上訴。（新華社）&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/363614</link>
      <guid isPermaLink="false">https://www.oschina.net/news/363614</guid>
      <pubDate>Fri, 01 Aug 2025 06:50:49 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>谷歌發佈面向程序員的開源字體：Google Sans Code</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;谷歌發佈了一款面向程序員的開源字體：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fgooglefonts%2Fgooglesans-code" target="_blank"&gt;Google Sans Code&lt;/a&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-94ed522f81ee53d97857a1fa258ac6bd4a3.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;據介紹，Google Sans Code 是一個等寬字體系列，為代碼帶來清晰度、可讀性以及一絲谷歌獨特的品牌特色。&lt;/p&gt; 
&lt;p&gt;主要特性&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;增強可讀性：專為代碼編輯器和終端中的最佳可讀性而設計&lt;/li&gt; 
 &lt;li&gt;支持腳本：擴展拉丁文，支持多種語言&lt;/li&gt; 
 &lt;li&gt;變體字體：提供從 300 到 800 的廣泛字重軸範圍&lt;/li&gt; 
 &lt;li&gt;OpenType 功能：樣式集、本地化形式&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;該字體源自谷歌的品牌字體設計風格，併為 Gemini 和 Android Studio 等產品開發，它確保每個字符即使在較小尺寸下也能保持清晰可辨。 此外，它還針對編程語言語法的獨特排版需求進行了精細調整。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/363609/googlesans-code</link>
      <guid isPermaLink="false">https://www.oschina.net/news/363609/googlesans-code</guid>
      <pubDate>Thu, 17 Jul 2025 06:45:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>DeepSeek 關聯公司公佈大語言模型部署方法專利</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;天眼查 App 顯示，DeepSeek 關聯公司杭州深度求索人工智能基礎技術研究有限公司申請的「一種大語言模型的部署方法及系統」專利公佈。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-0afa30888a89708e3df22bbcdb8c9e1f8b2.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-9a97b9c49fa395a1f8e80a766f8cdd92565.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;摘要顯示，該發明涉及人工智能領域，有益效果在於將預填充階段和解碼階段分別部署在高性能計算能力和大內存的機器上，均衡負載任務，實現最大化的硬件利用，減少閒置算力，降低整體延遲，提高吞吐量，增強系統的擴展性和容錯性。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/363608</link>
      <guid isPermaLink="false">https://www.oschina.net/news/363608</guid>
      <pubDate>Thu, 17 Jul 2025 06:37:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Apache Flink 2.1.0：面向實時 Data + AI 全面升級</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                                        &lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-4c6f1a9969d29f16f70840096693dfe8b0c.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;作者：達龍&lt;a href="https://my.oschina.net/u/941947"&gt;@阿里雲&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Apache&amp;nbsp;Flink&amp;nbsp;PMC&amp;nbsp;（項目管理委員會）很高興地宣佈&amp;nbsp;Apache&amp;nbsp;Flink&amp;nbsp;2.1.0 版本正式發佈，這標誌着實時數據處理引擎向&lt;strong&gt;統一&amp;nbsp;Data&amp;nbsp;+&amp;nbsp;AI&amp;nbsp;平台&lt;/strong&gt;的里程碑式演進。本次版本匯聚全球&amp;nbsp;116&amp;nbsp;位貢獻者，完成&amp;nbsp;16&amp;nbsp;項改進提案（FLIPs），解決&amp;nbsp;220&amp;nbsp;多個問題，重點強化了&lt;strong&gt;實時&amp;nbsp;AI&amp;nbsp;與智能流處理的深度融合&lt;/strong&gt;：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;實時&amp;nbsp;AI&amp;nbsp;能力突破&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;新增&amp;nbsp;AI&amp;nbsp;模型&amp;nbsp;DDL，支持通過&amp;nbsp;Flink&amp;nbsp;SQL&amp;nbsp;與&amp;nbsp;Table&amp;nbsp;API&amp;nbsp;創建和修改&amp;nbsp;AI&amp;nbsp;模型，實現&amp;nbsp;AI&amp;nbsp;模型的靈活管理。&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;擴展&lt;code&gt;ML_PREDICT&lt;/code&gt;表值函數，支持通過&amp;nbsp;Flink&amp;nbsp;SQL&amp;nbsp;實時調用&amp;nbsp;AI&amp;nbsp;模型，為構建端到端實時&amp;nbsp;AI&amp;nbsp;工作流奠定基礎。&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;實時數據處理增強&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;通過開放&amp;nbsp;Flink&amp;nbsp;核心能力——託管狀態、事件時間流處理及表變更日誌，&lt;code&gt;Process&amp;nbsp;Table&amp;nbsp;Functions(PTFs)&lt;/code&gt;使&amp;nbsp;Flink&amp;nbsp;&amp;nbsp;SQL&amp;nbsp;解鎖了更強大的事件驅動型應用開發能力&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;引入&lt;code&gt;VARIANT&lt;/code&gt;數據類型，高效處理&amp;nbsp;JSON&amp;nbsp;等半結構化數據，結合&lt;code&gt;PARSE_JSON&lt;/code&gt;函數與湖倉格式（如&amp;nbsp;Paimon），實現動態&amp;nbsp;Schema&amp;nbsp;數據分析。&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;重點優化流式&amp;nbsp;Join，創新性的引入&lt;code&gt;DeltaJoin&lt;/code&gt;與&lt;code&gt;MultiJoin&lt;/code&gt;策略，消除狀態瓶頸，提升資源利用率與作業穩定性。&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Flink&amp;nbsp;2.1.0&amp;nbsp;將實時數據處理與&amp;nbsp;AI&amp;nbsp;模型無縫集成，推動企業從實時分析邁向實時智能決策，以滿足現代數據應用不斷變化的需求。感謝各位貢獻者的支持！&lt;/p&gt; 
&lt;h1&gt;Flink&amp;nbsp;SQL&amp;nbsp;提升&lt;/h1&gt; 
&lt;h2&gt;Model&amp;nbsp;DDLs&amp;nbsp;支持&lt;/h2&gt; 
&lt;p&gt;自&amp;nbsp;Flink&amp;nbsp;2.0&amp;nbsp;引入&amp;nbsp;AI&amp;nbsp;模型相關&amp;nbsp;SQL&amp;nbsp;語法後，用户可通過類似創建&amp;nbsp;Catalog&amp;nbsp;對象的方式定義模型，並在&amp;nbsp;Flink&amp;nbsp;SQL&amp;nbsp;中調用&amp;nbsp;AI&amp;nbsp;&amp;nbsp;模型。Flink&amp;nbsp;2.1 進一步擴展支持通過&amp;nbsp;Table&amp;nbsp;API（Java/Python）&amp;nbsp;定義模型，通過編程實現模型的靈活管理。&lt;/p&gt; 
&lt;p&gt;示例：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;使用&amp;nbsp;Flink&amp;nbsp;SQL&amp;nbsp;創建模型&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sql"&gt;CREATE MODEL my_model
INPUT (f0 STRING)
OUTPUT (label STRING)
WITH (
  'task' = 'classification',
  'type' = 'remote',
  'provider' = 'openai',
  'openai.endpoint' = 'remote',
  'openai.api_key' = 'abcdefg',
);
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;使用&amp;nbsp;Java&amp;nbsp;API&amp;nbsp;創建模型&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-java"&gt;tEnv.createModel(
    "MyModel", 
    ModelDescriptor.forProvider("OPENAI")
      .inputSchema(Schema.newBuilder()
        .column("f0", DataTypes.STRING())
        .build())
      .outputSchema(Schema.newBuilder()
        .column("label", DataTypes.STRING())
        .build())
      .option("task", "classification")
      .option("type", "remote")
      .option("provider", "openai")
      .option("openai.endpoint", "remote")
      .option("openai.api_key", "abcdefg")
      .build(),
    true);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;更多信息：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcwiki.apache.org%2Fconfluence%2Fdisplay%2FFLINK%2FFLIP-437%253A%2BSupport%2BML%2BModels%2Bin%2BFlink%2BSQL" target="_blank"&gt;FLIP-437:&amp;nbsp;Support&amp;nbsp;ML&amp;nbsp;Models&amp;nbsp;in&amp;nbsp;Flink&amp;nbsp;SQL&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcwiki.apache.org%2Fconfluence%2Fdisplay%2FFLINK%2FFLIP-507%25253A%2BAdd%2BModel%2BDDL%2Bmethods%2Bin%2BTABLE%2BAPI" target="_blank"&gt;FLIP-507:&amp;nbsp;Add&amp;nbsp;Model&amp;nbsp;DDL&amp;nbsp;methods&amp;nbsp;in&amp;nbsp;TABLE&amp;nbsp;API&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;實時&amp;nbsp;AI&amp;nbsp;函數&lt;/h2&gt; 
&lt;p&gt;基於模型&amp;nbsp;DDL，Flink&amp;nbsp;2.1 擴展了&lt;code&gt;ML_PREDICT&lt;/code&gt;表值函數（TVF），支持在&amp;nbsp;Flink&amp;nbsp;SQL&amp;nbsp;中實時調用機器學習模型，提供內置兼容&amp;nbsp;OpenAI&amp;nbsp;API&amp;nbsp;的模型調用支持，同時開放自定義模型接口，標誌着&amp;nbsp;Flink&amp;nbsp;從實時數據處理引擎向統一的實時&amp;nbsp;Data&amp;nbsp;+&amp;nbsp;AI&amp;nbsp;平台演進。未來將繼續擴展&amp;nbsp;&lt;code&gt;ML_EVALUATE&lt;/code&gt;、&lt;code&gt;VECTOR_SEARCH&lt;/code&gt;&amp;nbsp;等函數，用户可以使用&amp;nbsp;Flink&amp;nbsp;SQL&amp;nbsp;更方便地構建端到端實時&amp;nbsp;AI&amp;nbsp;工作流。&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sql"&gt;-- Declare a AI model
CREATE MODEL `my_model`
INPUT (text STRING)
OUTPUT (response STRING)
WITH(
  'provider' = 'openai',
  'endpoint' = 'https://api.openai.com/v1/llm/v1/chat',
  'api-key' = 'abcdefg',
  'system-prompt' = 'translate to Chinese',
  'model' = 'gpt-4o'
);

-- Basic usage
SELECT * FROM ML_PREDICT(
  TABLE input_table,
  MODEL my_model,
  DESCRIPTOR(text)
);

-- With configuration options
SELECT * FROM ML_PREDICT(
  TABLE input_table,
  MODEL my_model,
  DESCRIPTOR(text)
  MAP['async', 'true', 'timeout', '100s']
);

-- Using named parameters
SELECT * FROM ML_PREDICT(
  INPUT =&amp;gt; TABLE input_table,
  MODEL =&amp;gt; MODEL my_model,
  ARGS =&amp;gt; DESCRIPTOR(text),
  CONFIG =&amp;gt; MAP['async', 'true']
);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;更多信息：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnightlies.apache.org%2Fflink%2Fflink-docs-release-2.1%2Fdocs%2Fdev%2Ftable%2Fsql%2Fqueries%2Fmodel-inference%2F" target="_blank"&gt;模型推理&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcwiki.apache.org%2Fconfluence%2Fdisplay%2FFLINK%2FFLIP-437%253A%2BSupport%2BML%2BModels%2Bin%2BFlink%2BSQL" target="_blank"&gt;FLIP-437:&amp;nbsp;Support&amp;nbsp;ML&amp;nbsp;Models&amp;nbsp;in&amp;nbsp;Flink&amp;nbsp;SQL&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Process&amp;nbsp;Table&amp;nbsp;Functions(PTFs)&lt;/h2&gt; 
&lt;p&gt;Apache&amp;nbsp;Flink&amp;nbsp;現已支持&amp;nbsp;&lt;strong&gt;Process&amp;nbsp;Table&amp;nbsp;Functions（PTFs）&lt;/strong&gt;，這是&amp;nbsp;Flink&amp;nbsp;SQL&amp;nbsp;與&amp;nbsp;Table&amp;nbsp;API&amp;nbsp;中最強大的函數類型。從概念上講，PTF&amp;nbsp;是所有&amp;nbsp;UDF&amp;nbsp;的「超集」，能夠將&lt;strong&gt;零個、一個或多張表&lt;/strong&gt;映射為&lt;strong&gt;零個、一個或多行數據&lt;/strong&gt;。藉助&amp;nbsp;PTFs，你可以實現與內置算子一樣功能豐富的自定義算子，並直接訪問&amp;nbsp;Flink&amp;nbsp;的託管狀態、事件時間、定時器以及表級變更日誌。&lt;/p&gt; 
&lt;p&gt;PTFs&amp;nbsp;支持如下操作：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;對錶的每一行執行轉換。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;可以把一張表從邏輯上拆分成不同子集，並對每個子集分別轉換。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;緩存已見事件，供後續多次訪問。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;在未來某個時間點繼續處理，實現等待、同步或超時機制。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;利用複雜狀態機或基於規則的條件邏輯對事件進行緩存與聚合。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;這顯著縮小了&amp;nbsp;Flink&amp;nbsp;SQL&amp;nbsp;與&amp;nbsp;DataStream&amp;nbsp;API&amp;nbsp;之間的差距，同時保留了&amp;nbsp;Flink&amp;nbsp;SQL&amp;nbsp;生態的健壯性與易用性。&lt;br&gt; 關於&amp;nbsp;PTFs&amp;nbsp;的語法與語義細節，詳見：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnightlies.apache.org%2Fflink%2Fflink-docs-master%2Fdocs%2Fdev%2Ftable%2Ffunctions%2Fptfs%2F" target="_blank"&gt;Process&amp;nbsp;Table&amp;nbsp;Functions&lt;/a&gt;。&lt;/p&gt; 
&lt;p&gt;示例如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-java"&gt;// Declare a ProcessTableFunction for memorizing your customers
public static class GreetingWithMemory extends ProcessTableFunction&amp;lt;String&amp;gt; {
    public static class CountState {
        public long counter = 0L;
    }

    public void eval(@StateHint CountState state, @ArgumentHint(SET_SEMANTIC_TABLE) Row input) {
        state.counter++;
        collect("Hello " + input.getFieldAs("name") + ", your " + state.counter + " time?");
    }
}

TableEnvironment env = TableEnvironment.create(...);

// Call the PTF in Table API
env.fromValues("Bob", "Alice", "Bob")
   .as("name")
   .partitionBy($("name"))
   .process(GreetingWithMemory.class)
   .execute()
   .print();

// Call the PTF in SQL
env.executeSql("SELECT * FROM GreetingWithMemory(TABLE Names PARTITION BY name)")
   .print();
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;更多信息&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcwiki.apache.org%2Fconfluence%2Fpages%2Fviewpage.action%3FpageId%3D298781093" target="_blank"&gt;FLIP-440:&amp;nbsp;User-defined&amp;nbsp;SQL&amp;nbsp;operators&amp;nbsp;/&amp;nbsp;ProcessTableFunction&amp;nbsp;(PTF)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Variant&amp;nbsp;類型&lt;/h2&gt; 
&lt;p&gt;新增&lt;code&gt;VARIANT&lt;/code&gt;半結構化數據類型（如&amp;nbsp;JSON），支持存儲任意嵌套結構（包括基本類型、&lt;code&gt;ARRAY&lt;/code&gt;、&lt;code&gt;MAP&lt;/code&gt;）並保留原始類型信息。相比&amp;nbsp;&lt;code&gt;ROW&lt;/code&gt;、&lt;code&gt;STRUCTURED&lt;/code&gt;&amp;nbsp;類型，&lt;code&gt;VARIANT&lt;/code&gt;&amp;nbsp;在處理動態&amp;nbsp;Schema&amp;nbsp;數據時更靈活。配合&amp;nbsp;&lt;code&gt;PARSE_JSON&lt;/code&gt;&amp;nbsp;函數及&amp;nbsp;Apache&amp;nbsp;Paimon&amp;nbsp;等表格式，可實現湖倉中半結構化數據的高效分析。&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sql"&gt;CREATE TABLE t1 (
  id INTEGER,
  v STRING -- a json string
) WITH (
  'connector' = 'mysql-cdc',
  ...
);
 
CREATE TABLE t2 (
  id INTEGER,
  v VARIANT
) WITH (
  'connector' = 'paimon'
  ...
);
 
-- write to t2 with VARIANT type
INSERT INTO t2 SELECT id, PARSE_JSON(v) FROM t1;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;更多信息：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnightlies.apache.org%2Fflink%2Fflink-docs-release-2.1%2Fdocs%2Fdev%2Ftable%2Ftypes%2F%23other-data-types" target="_blank"&gt;Variant&amp;nbsp;Type&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcwiki.apache.org%2Fconfluence%2Fdisplay%2FFLINK%2FFLIP-521%25253A%2BIntegrating%2BVariant%2BType%2Binto%2BFlink" target="_blank"&gt;FLIP-521:&amp;nbsp;Integrating&amp;nbsp;Variant&amp;nbsp;Type&amp;nbsp;into&amp;nbsp;Flink:&amp;nbsp;Enabling&amp;nbsp;Efficient&amp;nbsp;Semi-Structured&amp;nbsp;Data&amp;nbsp;Processing&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Structured&amp;nbsp;類型增強&lt;/h2&gt; 
&lt;p&gt;支持在&lt;code&gt;CREATE&amp;nbsp;TABLE&lt;/code&gt;語句中直接聲明用户定義結構體類型（STRUCTURED），解決類型兼容性問題並提升易用性。&lt;/p&gt; 
&lt;p&gt;示例：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sql"&gt;CREATE TABLE UserTable (
    uid BIGINT,
    user STRUCTURED&amp;lt;'User', name STRING, age INT&amp;gt;
);

-- Casts a row type into a structured type
INSERT INTO UserTable 
SELECT 1, CAST(('Alice', 30) AS STRUCTURED&amp;lt;'User', name STRING, age INT&amp;gt;);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;更多信息&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcwiki.apache.org%2Fconfluence%2Fdisplay%2FFLINK%2FFLIP-520%253A%2BSimplify%2BStructuredType%2Bhandling" target="_blank"&gt;FLIP-520:&amp;nbsp;Simplify&amp;nbsp;StructuredType&amp;nbsp;handling&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnightlies.apache.org%2Fflink%2Fflink-docs-release-2.1%2Fdocs%2Fdev%2Ftable%2Ftypes%2F%23user-defined-data-types" target="_blank"&gt;Structured&amp;nbsp;Type&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Delta&amp;nbsp;Join&lt;/h2&gt; 
&lt;p&gt;引入一種新的&lt;code&gt;DeltaJoin&lt;/code&gt;算子，相比傳統的雙流&amp;nbsp;Join&amp;nbsp;方案，配合&amp;nbsp;Apache&amp;nbsp;Fluss&amp;nbsp;等流存儲，可以實現&amp;nbsp;Join&amp;nbsp;算子無狀態化，解決了大狀態導致的資源瓶頸、檢查點緩慢和恢復延遲等問題。該特性已默認啓用，同時需要依賴&amp;nbsp;Apache&amp;nbsp;Fluss&amp;nbsp;存儲相應版本支持，敬請關注&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fapache%2Ffluss%2Fissues%2F1143" target="_blank"&gt;Support&amp;nbsp;DeltaJoin&amp;nbsp;on&amp;nbsp;Flink&amp;nbsp;2.1&lt;/a&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;更多信息&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcwiki.apache.org%2Fconfluence%2Fdisplay%2FFLINK%2FFLIP-486%25253A%2BIntroduce%2BA%2BNew%2BDeltaJoin" target="_blank"&gt;FLIP-486:&amp;nbsp;Introduce&amp;nbsp;A&amp;nbsp;New&amp;nbsp;DeltaJoin&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;級聯雙流&amp;nbsp;Join&amp;nbsp;優化&lt;/h2&gt; 
&lt;p&gt;使用多個級聯流式&amp;nbsp;Join&amp;nbsp;的&amp;nbsp;Flink&amp;nbsp;作業經常會因&amp;nbsp;State&amp;nbsp;過大而導致運行不穩定和性能下降。在這個版本，我們引入了一種新的&lt;code&gt;MultiJoin&lt;/code&gt;算子，&amp;nbsp;通過在單個算子內直接進行多流&amp;nbsp;Join，消除中間結果存儲，每條流輸入的記錄最多存儲一份，從而實現實現&amp;nbsp;"零中間狀態"&amp;nbsp;，顯著提升了資源利用率與作業穩定性。目前僅支持&amp;nbsp;INNER、LEFT&amp;nbsp;JOIN&amp;nbsp;，並且需要通過參數&lt;code&gt;table.optimizer.multi-join.enabled=true&lt;/code&gt;啓用。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;基準測試&lt;/strong&gt;：我們進行了一項基準測試，比較了&lt;code&gt;MultiJoin&lt;/code&gt;與雙流&amp;nbsp;Join&amp;nbsp;的優勢，更多詳情請參見&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnightlies.apache.org%2Fflink%2Fflink-docs-release-2.1%2Fdocs%2Fdev%2Ftable%2Ftuning%2F%23multiple-regular-joins" target="_blank"&gt;MultiJoin&amp;nbsp;Benchmark&lt;/a&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;更多信息&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcwiki.apache.org%2Fconfluence%2Fdisplay%2FFLINK%2FFLIP-516%253A%2BMulti-Way%2BJoin%2BOperator" target="_blank"&gt;FLIP-516:&amp;nbsp;Multi-Way&amp;nbsp;Join&amp;nbsp;Operator&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;異步&amp;nbsp;Lookup&amp;nbsp;Join&amp;nbsp;增強&lt;/h4&gt; 
&lt;p&gt;異步&amp;nbsp;Lookup&amp;nbsp;Join&amp;nbsp;在之前的版本中，即使用户將&amp;nbsp;&lt;code&gt;table.exec.async-lookup.output-mode&lt;/code&gt;&amp;nbsp;設置為&amp;nbsp;&lt;code&gt;ALLOW_UNORDERED&lt;/code&gt;，引擎在處理更新流時仍會強制回退到&amp;nbsp;&lt;code&gt;ORDERED&lt;/code&gt;&amp;nbsp;以保證正確性。從&amp;nbsp;2.1&amp;nbsp;版本開始，引擎允許並行處理無關的更新記錄，同時保證正確性，從而在處理更新流時實現更高的吞吐。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;更多信息：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcwiki.apache.org%2Fconfluence%2Fdisplay%2FFLINK%2FFLIP-519%253A%2B%2BIntroduce%2Basync%2Blookup%2Bkey%2Bordered%2Bmode" target="_blank"&gt;FLIP-519:&amp;nbsp;Introduce&amp;nbsp;async&amp;nbsp;lookup&amp;nbsp;key&amp;nbsp;ordered&amp;nbsp;mode&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Sink&amp;nbsp;節點複用&lt;/h2&gt; 
&lt;p&gt;當單個作業中多個&amp;nbsp;&lt;code&gt;INSERT&amp;nbsp;INTO&lt;/code&gt;&amp;nbsp;語句更新目標表相同列時（下版本將支持不同列），優化器將自動合併&amp;nbsp;Sink&amp;nbsp;節點實現複用，該特性可以極大提升&amp;nbsp;Apache&amp;nbsp;Paimon&amp;nbsp;等湖倉格式的部分更新場景使用體驗。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;更多信息：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcwiki.apache.org%2Fconfluence%2Fdisplay%2FFLINK%2FFLIP-506%253A%2BSupport%2BReuse%2BMultiple%2BTable%2BSinks%2Bin%2BPlanner" target="_blank"&gt;FLIP-506:&amp;nbsp;Support&amp;nbsp;Reuse&amp;nbsp;Multiple&amp;nbsp;Table&amp;nbsp;Sinks&amp;nbsp;in&amp;nbsp;Planner&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Compiled&amp;nbsp;Plan&amp;nbsp;支持&amp;nbsp;Smile&amp;nbsp;格式&lt;/h2&gt; 
&lt;p&gt;新增&amp;nbsp;Smile&amp;nbsp;二進制格式（兼容&amp;nbsp;JSON&amp;nbsp;格式）用於執行計劃序列化，較&amp;nbsp;JSON&amp;nbsp;更節省內存。默認仍使用&amp;nbsp;JSON&amp;nbsp;格式，需通過顯示調用&amp;nbsp;&lt;code&gt;CompiledPlan#asSmileBytes&lt;/code&gt;&amp;nbsp;和&amp;nbsp;&lt;code&gt;PlanReference#fromSmileBytes&lt;/code&gt;&amp;nbsp;方法啓用。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;更多信息：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcwiki.apache.org%2Fconfluence%2Fdisplay%2FFLINK%2FFLIP-508%253A%2BAdd%2Bsupport%2Bfor%2BSmile%2Bformat%2Bfor%2BCompiled%2Bplans" target="_blank"&gt;FLIP-508:&amp;nbsp;Add&amp;nbsp;support&amp;nbsp;for&amp;nbsp;Smile&amp;nbsp;format&amp;nbsp;for&amp;nbsp;Compiled&amp;nbsp;plans&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FFasterXML%2Fsmile-format-specification" target="_blank"&gt;Smile&amp;nbsp;格式規範&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;運行時提升&lt;/h1&gt; 
&lt;h2&gt;異步&amp;nbsp;Sink&amp;nbsp;可插拔的批量處理&lt;/h2&gt; 
&lt;p&gt;支持自定義批量寫入策略，用户可根據業務需求靈活擴展異步&amp;nbsp;Sink&amp;nbsp;的批量寫入邏輯。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;更多信息：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcwiki.apache.org%2Fconfluence%2Fdisplay%2FFLINK%2FFLIP-509%2BAdd%2Bpluggable%2BBatching%2Bfor%2BAsync%2BSink" target="_blank"&gt;FLIP-509:&amp;nbsp;Add&amp;nbsp;pluggable&amp;nbsp;Batching&amp;nbsp;for&amp;nbsp;Async&amp;nbsp;Sink&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;分片級水位線指標&lt;/h2&gt; 
&lt;p&gt;在&amp;nbsp;Flink&amp;nbsp;2.1&amp;nbsp;版本，我們新增細粒度分片監控指標，涵蓋水位線進度與狀態統計：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;currentWatermark&lt;/code&gt;：該分片最新接收到的水位線值&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;activeTimeMsPerSecond&lt;/code&gt;：該分片每秒處於數據處理狀態的時間（毫秒）&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;pausedTimeMsPerSecond&lt;/code&gt;：因水位線對齊該分片每秒的暫停時間（毫秒）&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;idleTimeMsPerSecond&lt;/code&gt;：每秒空閒時長（毫秒）&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;accumulatedActiveTimeMs&lt;/code&gt;：累計活躍時長（毫秒）&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;accumulatedPausedTimeMs&lt;/code&gt;：累計暫停時長（毫秒）&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;accumulatedIdleTimeMs&lt;/code&gt;：累計空閒時長（毫秒）&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;更多信息：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcwiki.apache.org%2Fconfluence%2Fdisplay%2FFLINK%2FFLIP-513%253A%2BSplit-level%2BWatermark%2BMetrics" target="_blank"&gt;FLIP-513:&amp;nbsp;Split-level&amp;nbsp;Watermark&amp;nbsp;Metrics&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;連接器提升&lt;/h1&gt; 
&lt;h2&gt;Keyed&amp;nbsp;State&amp;nbsp;連接器&lt;/h2&gt; 
&lt;p&gt;在&amp;nbsp;Flink&amp;nbsp;2.1&amp;nbsp;中，我們為&amp;nbsp;Keyed&amp;nbsp;State&amp;nbsp;引入了一個新的&amp;nbsp;SQL&amp;nbsp;連接器。該連接器允許用户使用&amp;nbsp;Flink&amp;nbsp;SQL&amp;nbsp;直接查詢 Checkpoint&amp;nbsp;和&amp;nbsp;Savepoint&amp;nbsp;&amp;nbsp;中的&amp;nbsp;Keyed&amp;nbsp;State，從而使得更容易探查、調試和驗證&amp;nbsp;Flink&amp;nbsp;作業狀態，無需定製工具，該功能對於分析長期運行的作業和驗證狀態遷移尤其有用。&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-plaintext"&gt;CREATE TABLE keyed_state (
    k INT,
    user_id STRING,
    balance DOUBLE
) WITH (
    'connector' = 'savepoint',
    'path' = 'file:///savepoint/path'
);

-- 直接查詢狀態快照
SELECT * FROM keyed_state;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;更多信息：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcwiki.apache.org%2Fconfluence%2Fdisplay%2FFLINK%2FFLIP-496%25253A%2BSQL%2Bconnector%2Bfor%2Bkeyed%2Bsavepoint%2Bdata" target="_blank"&gt;FLIP-496:&amp;nbsp;SQL&amp;nbsp;connector&amp;nbsp;for&amp;nbsp;keyed&amp;nbsp;savepoint&amp;nbsp;data&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;其他重要更新&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;PyFlink：新增支持&amp;nbsp;Python&amp;nbsp;3.12，移除&amp;nbsp;Python&amp;nbsp;3.8 支持。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;依賴升級：flink-shaded&amp;nbsp;升級至&amp;nbsp;20.0&amp;nbsp;以支持&amp;nbsp;Smile&amp;nbsp;格式，Parquet&amp;nbsp;升級至&amp;nbsp;1.15.3&amp;nbsp;修復安全漏洞（CVE-2025-30065）。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;升級説明&lt;/h1&gt; 
&lt;p&gt;Apache&amp;nbsp;Flink&amp;nbsp;社區努力確保升級過程儘可能平穩,&amp;nbsp;但是升級到&amp;nbsp;2.1&amp;nbsp;版本可能需要用户對現有應用程序做出一些調整。請參考&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnightlies.apache.org%2Fflink%2Fflink-docs-master%2Frelease-notes%2Fflink-2.1%2F" target="_blank"&gt;Release&amp;nbsp;Notes&lt;/a&gt;&amp;nbsp;獲取更多的升級時需要的改動與可能的問題列表細節。&lt;/p&gt; 
&lt;h1&gt;貢獻者列表&lt;/h1&gt; 
&lt;p&gt;Apache&amp;nbsp;Flink&amp;nbsp;社區感謝對此版本做出貢獻的每一位貢獻者：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://img.alicdn.com/imgextra/i2/O1CN01mRiFtD1TgkOCxzNkm_!!6000000002412-2-tps-1600-758.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;更多內容&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://img.alicdn.com/imgextra/i4/O1CN01vtIK1x1QErMoUKWNY_!!6000000001945-0-tps-2590-912.jpg" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;h3&gt;活動推薦&lt;/h3&gt; 
&lt;p&gt;阿里雲基於 Apache Flink 構建的企業級產品-實時計算 Flink 版現開啓活動： 新用户複製點擊下方鏈接或者掃描二維碼即可 0 元免費試用 Flink + Paimon &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ffree.aliyun.com%2F%3Futm_content%3Dg_1000395379%26productCode%3Dsc" target="_blank"&gt;實時計算 Flink 版&lt;/a&gt;（3000CU*小時，3 個月內） 瞭解活動詳情：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ffree.aliyun.com%2F%3Futm_content%3Dg_1000395379%26productCode%3Dsc" target="_blank"&gt;https://free.aliyun.com/?utm_content=g_1000395379&amp;amp;productCode=sc&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://img.alicdn.com/imgextra/i4/O1CN015MhRmC1tylxDOo5RG_!!6000000005971-0-tps-1200-600.jpg" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/2828172/blog/18686708</link>
      <guid isPermaLink="false">https://my.oschina.net/u/2828172/blog/18686708</guid>
      <pubDate>Thu, 17 Jul 2025 06:37:00 GMT</pubDate>
      <author>原創</author>
    </item>
    <item>
      <title>字節 AI 編程工具 Trae 發佈關於數據隱私與安全的説明</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;近日，字節跳動旗下的 AI 編程工具 Trae IDE&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgigazine.net%2Fnews%2F20250729-bytedance-trae-ide%2F" target="_blank"&gt; 被指存在數據隱私問題&lt;/a&gt;。&lt;/p&gt; 
&lt;p&gt;開發者發現即使在關閉遙測功能後，Trae 仍會持續向字節跳動服務器上傳數據，且數據量較大。此外，Trae 還被指出存在可遠程激活的「熱更新」機制，數據加密傳輸機制不透明，隱私政策中未明確列舉具體收集的數據類型。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-a970824d275e33e0557fe63610723284865.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-3499b1d590ebc12f5c824e414436afeca85.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;對此，字節跳動 Trae IDE 團隊回應稱，為了持續優化產品體驗，Trae 和業內通用做法類似，會採集部分非敏感的產品統計數據及性能監測數據，如頁面點擊、功能使用頻率等。這類數據不涉及用户的個人身份或隱私信息。&lt;/p&gt; 
&lt;p&gt;Trae IDE 團隊還解釋道，Trae 採集的數據僅為非敏感的統計和性能指標，用於產品優化與性能分析，且嚴格遵循全球數據保護法規。Trae 的遙測機制獨立於 VSCode 的遙測控制項，用户關閉的僅是 VSCode 原生模塊，這導致了誤解。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-a3069e937ea960b8374dec97f972eb90505.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/363605</link>
      <guid isPermaLink="false">https://www.oschina.net/news/363605</guid>
      <pubDate>Thu, 17 Jul 2025 06:30:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Kimi K2 高速版發佈，輸出速度提升至每秒 40 Tokens</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;&lt;span style="color:#000000"&gt;Kimi 開放平台&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FqHE09ndQzz-gvd8RaymN5A" target="_blank"&gt;發文&lt;/a&gt;宣佈推出 Kimi K2 高速版 —— kimi-k2-turbo-preview，參數規模與現有 kimi-k2 保持一致，但輸出速度由每秒 10 Tokens 提升至每秒 40 Tokens。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;同時，官方宣佈特別推出限時 5 折特惠活動，該優惠將持續至 9 月 1 日，之後將恢復原價。在折扣期間：模型每百萬&amp;nbsp;tokens&amp;nbsp;輸入價格（緩存命中）¥2.00，輸入價格（緩存未命中）¥8.00，輸出價格&amp;nbsp;¥32.00。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;「我們還將繼續努力優化，進一步提升 kimi-k2 模型的輸出速度。」&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="322" src="https://oscimg.oschina.net/oscnet/up-af3e364d7b9ae7f30a4699694498dd0d792.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/363603</link>
      <guid isPermaLink="false">https://www.oschina.net/news/363603</guid>
      <pubDate>Thu, 17 Jul 2025 06:29:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Meta 合成孔徑波導全息術新進展，微美全息加速 AI+AR 全息技術融合穩步前行</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;獲悉，日前，Meta（META.US）與斯坦福大學攜手開展的「合成孔徑波導全息術」研究，正朝着打造總光學堆棧厚度不足 3 毫米的「VR 眼鏡」這一目標穩步推進。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;strong&gt;&lt;span&gt;合成孔徑波導全息術新突破&lt;/span&gt;&lt;/strong&gt;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;其團隊集結了 Meta 顯示系統研究團隊的兩名研究員、斯坦福大學的一位副教授等，將相關研究成果已發表於題為《用於大光展量緊湊型混合現實顯示器的合成孔徑波導全息術》的論文中。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;div&gt; 
   &lt;div&gt; 
    &lt;div&gt; 
     &lt;div&gt; 
      &lt;div&gt;
       &lt;img src="https://oscimg.oschina.net/oscnet//2daa37e10675d715ad88326541afa3fd.png" width="659" referrerpolicy="no-referrer"&gt; 
       &lt;div&gt;
        &amp;nbsp;
       &lt;/div&gt; 
      &lt;/div&gt; 
     &lt;/div&gt; 
    &lt;/div&gt; 
   &lt;/div&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;據稱，此顯示系統的關鍵創新點之一，是它支持較大的有效光學擴展量。同時，該系統還與一個全新的基於 AI 人工智能的算法框架協同工作，該框架融合了隱式大光學擴展量波導模型、用於部分相干互強度的高效波傳播模型，以及一個新穎的計算機生成全息框架。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;早前，英偉達（NVDA.US）的研究人員藉助光瞳複製波導、空間光調製器（SLM）和幾何相位透鏡，實現了厚度為 2.5 毫米的真 3D 全息技術。不過該技術的視場角僅為 23°，在未啓用眼動追蹤時，眼區僅為 2.3 毫米。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;Meta 表示，當前，VR/MR 頭顯的厚度幾乎完全由光學元件和顯示器所決定。這無疑需要一套與現有市面上任何顯示系統都截然不同的全新系統，而此次的研究正是 Meta 向這一目標邁進的重要一步。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;div&gt; 
   &lt;div&gt; 
    &lt;div&gt; 
     &lt;div&gt; 
      &lt;div&gt;
       &lt;img src="https://oscimg.oschina.net/oscnet//80898212b9cc539f66143e0f374387b3.png" width="659" referrerpolicy="no-referrer"&gt; 
       &lt;div&gt;
        &amp;nbsp;
       &lt;/div&gt; 
      &lt;/div&gt; 
     &lt;/div&gt; 
    &lt;/div&gt; 
   &lt;/div&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;strong&gt;&lt;span&gt;市場藍海崛起&lt;/span&gt;&lt;/strong&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;現如今，全息顯示技術無疑已經更加成熟。在剛閉幕的 2025 世界人工智能大會上，各企業攜「硬科技」成果亮相，用全息「無屏而有形，隔空卻可觸」的全景式體驗，呈現在人機交互領域的技術實力，為全球觀眾解鎖未來智能的無限潛能。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;根據海外市場研究機構 Verified Market Reports 發佈的報告，2024 年全球裸眼 3D 顯示器的市場規模達到 12 億美元，預計 2026-2033 年間的複合年增長率為 12.5%，到 2033 年市場將達到 35 億美元。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;div&gt; 
   &lt;div&gt; 
    &lt;div&gt; 
     &lt;div&gt; 
      &lt;div&gt;
       &lt;img src="https://oscimg.oschina.net/oscnet//1534a110f7d8fe850c6ffd10de99c2e1.png" width="659" referrerpolicy="no-referrer"&gt; 
       &lt;div&gt;
        &amp;nbsp;
       &lt;/div&gt; 
      &lt;/div&gt; 
     &lt;/div&gt; 
    &lt;/div&gt; 
   &lt;/div&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;對此，相關業內人士指出，全息顯示技術的研究正在穩步推進，而全息顯示市場的高速發展，無疑將繼續給行業帶來積極的影響和全新的視角，也為企業對市場內容的呈現方式帶去新的思路。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;strong&gt;&lt;span&gt;微美全息呈現全新技術視角&lt;/span&gt;&lt;/strong&gt;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;讓在此背景下，資料顯示，微美全息（WIMI.US）作為全息 AR 技術引領者，常年來專注於計算機視覺全息雲服務，業務涵蓋全息 AI 合成、視覺呈現、互動軟件開發、AR 廣告投放、5G 全息通訊等多個環節，持續推進全息技術場景化應用。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;為突破多場景適配、跨終端聯動與虛實融合協同等技術瓶頸，微美全息研發團隊構建了集全息成像、編碼、傳輸和顯示於一體的系統，結合 5G 技術、AI 技術實現低延遲、高清晰度的全息通信，可應用於虛擬課堂、遠程協作、工業檢測等場景成熟解決方案，實現從單點深耕到多元化場景的佈局跨越。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;strong&gt;&lt;span&gt;結尾&lt;/span&gt;&lt;/strong&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;當前，全球智能化浪潮風起雲湧，隨着長久以來相關技術的蓬勃發展，全息顯示效果早就不可同日而語，同時，人工智能成為驅動時代變革的核心引擎，當全息技術與 AI 虛實融合將重構人類交互體驗的底層邏輯，一場關於未來智能的探索正在全球進行，而這更將是一次成熟技術的結合與嘗試。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/363602</link>
      <guid isPermaLink="false">https://www.oschina.net/news/363602</guid>
      <pubDate>Thu, 17 Jul 2025 06:28:00 GMT</pubDate>
      <author>來源: 資訊</author>
    </item>
    <item>
      <title>階躍星辰新一代基礎大模型 Step 3 正式開源，專注多模態推理</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;階躍星辰宣佈正式開源其最新一代基礎大模型 Step3。該模型採用專家混合（MoE）架構，總參數量為 321B，激活參數量為 38B，旨在為企業和開發者提供性能與成本極致均衡的推理方案。&lt;/p&gt; 
&lt;p&gt;Step3 模型在設計上專注於多模態推理，通過端到端的設計最小化解碼成本，在視覺語言推理任務中表現出色。&lt;/p&gt; 
&lt;p&gt;技術上，模型採用了自研的 MFA（Multi-matrix Factorization Attention）注意力機制和 AFD（Attention-FFN Disaggregation）系統架構。MFA 旨在降低 KV 緩存開銷和計算消耗，而 AFD 則將 Attention 和 FFN 計算解耦為兩個子系統，通過流水線並行調度提升吞吐效率。&lt;/p&gt; 
&lt;p&gt;為支持 AFD，階躍星辰還開源了專用的通信庫 StepMesh，以實現跨卡的低延遲高帶寬數據傳輸。&lt;/p&gt; 
&lt;p&gt;在性能評測方面，Step3 在 MMMU、MathVision、AIME 2025 等多個基準上，表現優於同類開源模型。在社區測試中，該模型也展現了不錯的指令遵循和生成能力。vLLM 項目宣佈已支持 Step3 模型，並報告在 Hopper GPU 上實現了高達 4,039 tok/sec/GPU 的吞吐量。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0801/141828_z9Od_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-ef1a09fb7cb19300f347b50db7de9afe917.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Step3 模型權重已在 Hugging Face 和魔搭社區發佈，支持 bf16 和 block-fp8 格式。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Github：https://github.com/stepfun-ai/Step3&lt;/li&gt; 
 &lt;li&gt;Hugging Face：https://huggingface.co/stepfun-ai/step3&lt;/li&gt; 
 &lt;li&gt;魔搭 ModelScope：&lt;br&gt; https://www.modelscope.cn/models/stepfun-ai/step3&lt;br&gt; https://www.modelscope.cn/models/stepfun-ai/step3-fp8&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;用户可以通過階躍星辰開放平台（platform.stepfun.com）訪問其 OpenAI 兼容的 API，上下文長度為 64K，目前提供折扣價格，輸入為每百萬 token 1.5 元，輸出為 4 元。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/363599</link>
      <guid isPermaLink="false">https://www.oschina.net/news/363599</guid>
      <pubDate>Thu, 17 Jul 2025 06:18:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>青雲科技暫停 KubeSphere 開源版產品下載，停止提供免費技術支持</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;青雲&lt;u&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fkubesphere%2Fkubesphere%2Fissues%2F6550"&gt;發佈&lt;/a&gt;&lt;/u&gt;了暫停 KubeSphere 開源版下載和停止提供免費技術支持的公告：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;若貴司目前仍在使用 KubeSphere 開源版，或有後續使用計劃，為保障業務正常運轉，敬請及時與我司客户服務團隊取得聯繫。我們將為您量身定製商業版解決方案，涵蓋專屬技術支持、漏洞修復、版本升級等多項增值服務，確保您的業務系統在高效、安全的環境中穩定運行。&lt;/p&gt; 
 &lt;p&gt;以上調整僅涉及 KubeSphere 開源版產品，不涉及 KubeSphere 核心代碼的開源項目，其仍將在原有開源模式下持續迭代，並保持使用 Apache License 2.0 with additional conditions。&lt;/p&gt; 
 &lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0801/135922_dIql_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;KubeSphere 是青雲科技開源的多租户容器平台，提供全棧的 IT 自動化運維的能力，簡化企業的 DevOps 工作流。KubeSphere 提供了運維友好的嚮導式操作界面，幫助企業快速構建一個強大和功能豐富的容器雲平台。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/363593</link>
      <guid isPermaLink="false">https://www.oschina.net/news/363593</guid>
      <pubDate>Thu, 17 Jul 2025 06:01:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>AutoMQ 1.5.0 開源版正式發佈：構建 kafka 實時數據湖新範式</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                                        &lt;p&gt;重磅發佈｜AutoMQ 1.5.0 開源版正式發佈：構建 kafka 實時數據湖新範式！&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;引領雲原生創新之路&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;AutoMQ 開源版本 1.5.0 正式發佈&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="Image" src="https://oscimg.oschina.net/oscnet/up-4d1dbc53c3c358d60c4bc5ac2f1828359a1.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;實時數據湖，為何成為數據架構的下一個風口？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;隨着數據實時化趨勢席捲全球，&lt;strong&gt;"實時數據湖"（Streaming Lakehouse）正迅速成為雲計算與大數據領域最受關注的技術方向&lt;/strong&gt;。Snowflake、Databricks、Confluent、AWS 等雲廠商紛紛推出相關方案，希望打通從數據採集、處理到分析的一體化鏈路，構建下一代數據基礎設施。&lt;/p&gt; 
&lt;p&gt;實時數據湖是一種支持數據實時寫入與即時分析的新型架構，將流式數據直接接入數據湖，實現低延遲、低成本的數據處理與消費。相比傳統的數據湖與數據倉庫分離架構，&lt;strong&gt;實時數據湖融合了數據湖的彈性與數據倉的實時性，具備以下優勢&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;無需複雜 ETL 作業&lt;/strong&gt;，數據可從 Kafka 等流引擎直接寫入湖中&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;延遲更低&lt;/strong&gt;，支持秒級甚至毫秒級分析查詢&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;架構更簡&lt;/strong&gt;，避免 Flink/Spark 頻繁調度與維護開銷&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;一致性強&lt;/strong&gt;，能與主業務系統保持 Schema 與數據同步&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;在這場變革中，Kafka 被視為實時數據的入口，但其本身並不具備"入湖"能力。為此，AutoMQ 在最新發布的 &lt;strong&gt;1.5.0 開源版本中推出了重磅特性------Table Topic&lt;/strong&gt;：&lt;/p&gt; 
&lt;p&gt;通過將 Kafka Topic 與 Iceberg 表原生綁定，實現真正意義上的 &lt;strong&gt;Zero-ETL 流式入湖&lt;/strong&gt;，讓 Kafka 成為實時數據湖的核心引擎。&lt;/p&gt; 
&lt;p&gt;這也意味着，開發者可以用最少的工具鏈、最少的運維成本，搭建出真正實時、開放、低成本的 Lakehouse 架構。&lt;/p&gt; 
&lt;span id="OSC_h2_1"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;一站式雲原生 Kafka：開源、可控、零 ETL&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;AutoMQ 開源版本 &lt;strong&gt;1.5.0&lt;/strong&gt; 正式發佈！作為&lt;strong&gt;首個運行於 Amazon S3 的開源 Kafka 發行版&lt;/strong&gt; ，本次版本不僅全面強化了 "雲原生、低成本、強可控" 的核心能力，更以創新特性 &lt;strong&gt;Table Topic&lt;/strong&gt; 正式加入實時數據湖陣營，為企業構建 &lt;strong&gt;Zero-ETL 流式入湖能力&lt;/strong&gt;提供了真正可用、可控、開放的開源方案。&lt;/p&gt; 
&lt;p&gt;AutoMQ 基於 Apache 2.0 協議完全開源，無需商業授權即可在生產環境運行，100% 兼容 Kafka 協議，支持現有工具鏈無縫遷移，是真正面向未來的數據基礎設施構建工具。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;本次版本重點包括：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Table Topic：Kafka 與 Iceberg 表的原生綁定，真正實現 Zero-ETL 流式入湖&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;零跨 AZ 架構：全面消除跨可用區流量，Kafka 雲上成本可降低高達 70%&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;支持 Bitnami Helm Chart：一鍵部署至 Kubernetes，輕鬆搭建雲原生 Kafka 集羣&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;通過持續深化開源能力，AutoMQ 正在讓雲上 Kafka 更輕、更強、更開放。&lt;/p&gt; 
&lt;span id="OSC_h2_2"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;01 零 ETL，Kafka 秒變 Iceberg 表&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;數據平台構建中，如何將 Kafka 實時數據寫入數據湖 Iceberg，一直是工程難題。傳統做法需依賴 Spark、Flink 等複雜 ETL 作業，不僅成本高，還帶來延遲、失敗率、Schema 演化等問題。&lt;/p&gt; 
&lt;p&gt;AutoMQ 提出的 &lt;strong&gt;Table Topic&lt;/strong&gt; 是一種全新機制------將 Kafka Topic 原生綁定至 Iceberg 表，無需任何中間件或外部任務，實現 Kafka 到 Iceberg 的零 ETL 流式寫入：&lt;/p&gt; 
&lt;p&gt;&lt;img alt="Image" src="https://oscimg.oschina.net/oscnet/up-7986bc5eb8dbffa9f36630a7749b989ab0d.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;單配置即可啓動，自動完成 Schema 註冊、演化、字段映射與 Upsert&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;支持 GiB/s 級別的實時寫入&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;完整保留 Kafka 流特性，適用於 CDC、審計日誌、實時分析等場景&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Kafka 不再只是消息通道，而是直接成為湖倉的實時數據源。這意味着你可以用最簡化的架構，構建出高效、實時、低成本的數據系統。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;📖 推薦閲讀下方相關文章，瞭解更多技術細節：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzkxNzY0ODE2Ng%3D%3D%26mid%3D2247488409%26idx%3D1%26sn%3Db2bc366bf8d66434880dfa8e43eceda0%26scene%3D21%23wechat_redirect" target="_blank"&gt;為什麼越來越多企業放棄 Flink/Spark，用 AutoMQ 替代傳統 ETL？&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;span id="OSC_h2_3"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;02 零跨區流量，Kafka 雲上運行也能不"燒錢"&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;雲上 Kafka 最大的隱形成本就是&lt;strong&gt;跨 AZ 流量&lt;/strong&gt; 。據統計，在三可用區部署的 Kafka 集羣中，&lt;strong&gt;跨區數據傳輸費用可佔總雲成本的 50%&lt;/strong&gt; ，其中包括副本同步、消費者拉取和跨區 Produce 請求等高頻操作。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;AutoMQ&lt;/strong&gt; 通過基於 &lt;strong&gt;S3 的共享存儲架構&lt;/strong&gt; ，徹底移除了生產、複製、消費路徑中的跨區流量，實現真正意義上的&lt;strong&gt;跨 AZ 流量歸零&lt;/strong&gt;：&lt;/p&gt; 
&lt;p&gt;&lt;img alt="Image" src="https://oscimg.oschina.net/oscnet//06402ccf96142e4f368e54a18fb34ff1.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;生產路徑&lt;/strong&gt;：通過智能代理和跨區代理通道，實現跨 AZ Produce 請求的就近接入&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;複製路徑&lt;/strong&gt;：使用 S3 + 奇偶校驗代替傳統 Broker 間複製，完全消除複製帶寬開銷&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;消費路徑&lt;/strong&gt;：每個 AZ 提供只讀分區，消費者本地讀取，無需跨區讀流&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="OSC_h3_4"&gt;&lt;/span&gt; 
&lt;h3&gt;成本對比&lt;/h3&gt; 
&lt;p&gt;AutoMQ vs Apache Kafka 在，多 AZ 場景下 30MiB/s 寫入吞吐的成本對比&lt;/p&gt; 
&lt;p&gt;&lt;img alt="Image" src="https://oscimg.oschina.net/oscnet/up-914401f80c6e4e14510617fc78cc305a8e3.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;以典型 30MiB/s 寫入負載為例，&lt;strong&gt;Kafka 每月跨 AZ 成本可達 $4,050&lt;/strong&gt; ，而 AutoMQ 架構下僅需 &lt;strong&gt;約 $158 的 S3 存儲與 API 成本&lt;/strong&gt; ，&lt;strong&gt;節省超 95% 的流量費用&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;這一創新架構顯著提升可擴展性和穩定性，同時降低運維複雜度，真正實現 Kafka 在雲上的"降本增效"，是企業級 Kafka 雲部署的理想方案。&lt;/p&gt; 
&lt;span id="OSC_h2_5"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;03 一鍵部署 Kafka？AutoMQ × Bitnami 讓它變簡單了！&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;AutoMQ 1.5.0 現已原生支持 Bitnami Helm Charts，用户可零門檻接入 AutoMQ，在 Kubernetes 中通過熟悉的 Helm 流程實現一鍵部署，無需額外修改配置或編寫 Operator 腳本，即可運行一個無狀態、彈性伸縮、S3 原生存儲的 Kafka 集羣。&lt;/p&gt; 
&lt;p&gt;&lt;img height="274" src="https://oscimg.oschina.net/oscnet/up-9cf5420b017b80fe16c2c1e5d71e5499fba.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;這一集成為用户帶來了更高的部署效率和更低的學習成本，特別適合希望快速在生產環境中驗證或上線 Kafka 流處理平台的技術團隊。&lt;/p&gt; 
&lt;p&gt;💡 適用於：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;AWS EKS、Google GKE、Azure AKS、阿里雲 ACK、騰訊雲 TKE 等主流雲平台&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;私有化部署與混合雲場景&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;從 Apache Kafka 平滑遷移至 AutoMQ 的替代方案&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;無論你是 DevOps 工程師、平台架構師，還是正在尋找開源替代方案的技術負責人，AutoMQ 與 Bitnami 的結合，都能為你提供穩定、標準化的部署體驗，並顯著降低維護複雜度。&lt;/p&gt; 
&lt;span id="OSC_h2_6"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;AutoMQ：開源、自主、真正雲原生的 Kafka&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;自開源以來，AutoMQ 已獲得眾多雲原生團隊與企業技術負責人認可，並在多種生產環境中成功驗證。如今，越來越多公司選擇與我們合作，擁抱 Kafka 的雲原生未來。&lt;/p&gt; 
&lt;p&gt;AutoMQ 完全開源，免費可用，無需修改原有工具鏈，即可替代傳統 Kafka 集羣部署。&lt;/p&gt; 
&lt;p&gt;👉 下方一鍵快速部署指南 | 快速啓動你的雲原生 Kafka 之旅！&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.automq.com%2Fdocs%2Fautomq%2Fgetting-started%2Fdeploy-multi-nodes-test-cluster-on-docker%3Futm_source%3Dautomq_1_5_launch" target="_blank"&gt;https://www.automq.com/docs/automq/getting-started/deploy-multi-nodes-test-cluster-on-docker?utm_source=automq_1_5_launch&lt;/a&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/6990971/blog/18686688</link>
      <guid isPermaLink="false">https://my.oschina.net/u/6990971/blog/18686688</guid>
      <pubDate>Thu, 17 Jul 2025 05:56:00 GMT</pubDate>
      <author>原創</author>
    </item>
    <item>
      <title>谷歌 DeepMind 推出虛擬衞星 AI 模型 AlphaEarth Foundations</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;谷歌 DeepMind 近日推出了名為 AlphaEarth Foundations 的人工智能系統，該系統旨在將海量的衞星數據轉化為統一的數字表示，以提高環境分析的準確性，支持食物安全、森林砍伐和水資源等問題的決策。AlphaEarth Foundations 可以被視作一種 「虛擬衞星」，它以每 10x10 米的分辨率對地球的所有陸地和沿海水域進行描繪。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;img height="397" src="https://oscimg.oschina.net/oscnet/up-76da4876193012eb1829ee5227d378659b8.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;這一模型整合了多種數據來源，包括光學衞星圖像、雷達、3D 激光測繪和氣候模擬。通過將這些輸入數據壓縮為 64 維嵌入（embedding），DeepMind 實現了數據的高效表示。其訓練過程中，AlphaEarth Foundations 使用了來自全球超過 500 萬個地點的超過 30 億條觀測數據，數據來源涵蓋了 Sentinel-2 和 Landsat 等衞星任務，還結合了維基百科文章和物種觀察等文本信息。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;該系統的目標是解決數據過載和信息不一致的兩個核心挑戰。AlphaEarth Foundations 能夠穿透持續的雲層，繪製南極洲的複雜地表，並揭示加拿大小麥種植中的微小變化，這些細節是人眼所無法捕捉到的。在與傳統方法及其他 AI 繪圖系統的對比測試中，AlphaEarth Foundations 的錯誤率平均低了 24%。該模型在土地利用分類、生物物理變量估算和變化檢測等 15 個評估數據集上表現優異。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;AlphaEarth Foundations 還能夠在處理數據稀缺的情況下進行有效工作，其持續的時間分析功能使得系統可以對不完全對齊的時間段進行精確預測。該模型的 「時空精度」（STP）架構將來自同一地點的不同時期的衞星圖像視作視頻中的幀，這樣的處理方式使系統能夠學習空間、時間和測量之間的關係，從而生成捕捉局部環境和時間軌跡的嵌入表示。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;目前，已有 50 多家組織在現實應用中測試這一系統。全球生態系統地圖（Global Ecosystems Atlas）利用該數據將以前未映射的生態系統分類，包括沿海灌叢和超乾旱沙漠等。巴西的 MapBiomas 則藉助這些數據深入分析農業和環境變化，尤其是亞馬遜雨林等關鍵生態系統。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;此外，谷歌還將在 Google Earth Engine 上發佈名為衞星嵌入數據集（Satellite Embedding Dataset）的年度嵌入數據。根據 Google Earth Engine 的數據，該數據集每年生成超過 1.4 萬億個嵌入足跡，為識別全球相似環境條件、變化檢測、自動聚類和更智能的分類提供了多種應用場景。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;為了加速科學研究，谷歌還提供最高 5000 美元的研究資助，以支持基於衞星嵌入的應用案例研究。DeepMind 的開發團隊認為，AlphaEarth Foundations 是理解我們不斷變化的星球狀態和動態的重要一步，並期待將其與通用推理大型語言模型（LLM）結合，創造出更強大的應用。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/363560</link>
      <guid isPermaLink="false">https://www.oschina.net/news/363560</guid>
      <pubDate>Thu, 17 Jul 2025 03:30:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>小米瀏覽器升級 AI 搜索功能，接入豆包大模型</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;&lt;span style="color:#000000"&gt;小米瀏覽器&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FK4dYPsQQDmK8m3aM5iekXQ" target="_blank"&gt;宣佈&lt;/a&gt;已升級 「AI 搜索」 功能，通過接入豆包大模型及火山方舟高代碼智能體產品，進一步提升了 AI 搜索的效率與服務豐富度。&lt;/span&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;span style="color:#000000"&gt;作為數億小米終端的系統級瀏覽器，小米瀏覽器推出「AI 搜索」功能，通過接入豆包大模型 1.5 和豆包視覺理解大模型，帶來自然語言交互的搜索體驗，同時進一步解鎖更多服務。&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;span style="color:#000000"&gt;現在，點擊該瀏覽器的「AI 搜索」，一個問題，直達結果。&lt;/span&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img height="634" src="https://oscimg.oschina.net/oscnet/up-6f3d3bed8cba7a28b2216c0b7855d9bdf4c.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;同時，小米應用商店接入火山引擎釦子，用户通過釦子搭建的智能體，可以上傳到小米應用商店。公告稱，小米應用商店今年計劃大力推廣智能體模塊，通過打通火山引擎釦子發佈渠道至小米應用商店，共同探索更多智能體分發場景。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;「&lt;span style="color:#000000"&gt;未來，小米應用商店、小米瀏覽器將持續攜手火山引擎，深入探索大模型與智能體在終端場景中的應用，拓展智能交互邊界，為用户帶來更高效、便捷、個性化的服務。&lt;/span&gt;」&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/363556</link>
      <guid isPermaLink="false">https://www.oschina.net/news/363556</guid>
      <pubDate>Thu, 17 Jul 2025 03:23:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Manus 上線 Wide Research，多智能體併發處理大規模任務，月費 199 美元起</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;Manus 上線了一項名為 &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmanus.im%2Fzh-cn%2Fblog%2Fintroducing-wide-research" target="_blank"&gt;Wide Research &lt;/a&gt;的新功能，允許系統調用大量 AI 智能體並行處理任務，實現大規模數據的同步運算處理。這將是自今年 3 月平台發佈以來最大的一次功能更新，首發版本將面向月費 199 美元的高級訂閲用户開放。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0801/112140_q2ms_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;這項功能的核心在於「智能體集羣協作」：用户可以同時指派數十個智能體協同工作，完成例如「生成 50 張海報設計稿」、「評選全球前 100 MBA 項目」或「分析 1000 支股票表現」等複雜任務——這些通常對 OpenAI 的 Deep Research 等現有工具構成挑戰。知情人士表示，Wide Research 將顯著提升產品在通用研究和自動執行任務方面的能力。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0801/112120_zAdV_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;據稱，該公司技術團隊過去兩個月一直專注研發 Wide Research，旨在通過多智能體並行計算建立產品的差異化優勢。聯合創始人季逸超計劃以視頻形式展示該功能，方式將類似於他當初發佈 Manus 時的演示。目前，Manus 已將運營總部從中國遷往新加坡、東京和加州聖馬特奧。其產品構建於如 Anthropic Claude 等大語言模型之上，尚未在中國市場上線。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/363554/manus-wide-research</link>
      <guid isPermaLink="false">https://www.oschina.net/news/363554/manus-wide-research</guid>
      <pubDate>Thu, 17 Jul 2025 03:22:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
  </channel>
</rss>
