<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - news - 繁體中文（香港）</title>
    <link>https://www.oschina.net/news/project</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news" rel="self" type="application/rss+xml"/>
    <description>已對該 RSS 進行格式化操作：中英字符之間插入空格、使用直角引號、標點符號修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-hk</language>
    <lastBuildDate>Wed, 27 Aug 2025 07:45:42 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>主權 AI 現狀報告</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#000000; margin-left:0; margin-right:0; text-align:justify"&gt;Futurewei、LF AI &amp;amp; Data 和 LF Research 聯手，&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.linuxfoundation.org%2Fresearch%2Fstate-of-sovereign-ai" target="_blank"&gt;調研&lt;/a&gt;&lt;span style="color:#000000"&gt;了全球主權 AI 的發展狀況。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;本研究基於對 233 名受訪者的調查及行業領袖的專家見解，揭示主權 AI（即在開發 AI 能力時最大限度減少對外部主體的依賴）已成為各國及組織機構的戰略優先事項，79% 的受訪者認為其具有重要價值且具有戰略意義，原因包括數據控制（72%）、國家安全（69%）、經濟競爭力（48%）和合規性與文化契合度（分別佔 44% 和 31%）。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;開源軟件、開放標準和開放數據構成了 AI 主權的基礎。開源軟件尤其受到重視，90% 的受訪者認為其至關重要或非常重要，因為它具備靈活性、透明度和可控性。開源技術對主權 AI 的核心價值包&lt;/span&gt;括：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li style="text-align:justify"&gt;&lt;span style="color:#000000"&gt;透明度與可審計性（69%，歐洲地區達 80%）&lt;/span&gt;&lt;/li&gt; 
 &lt;li style="text-align:justify"&gt;&lt;span style="color:#000000"&gt;安全與信任（60%）&lt;/span&gt;&lt;/li&gt; 
 &lt;li style="text-align:justify"&gt;&lt;span style="color:#000000"&gt;定製化與微調的靈活性（69%）&lt;/span&gt;&lt;/li&gt; 
 &lt;li style="text-align:justify"&gt;&lt;span style="color:#000000"&gt;通過協作開發加速創新（41%）&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#000000; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;img height="390" src="https://oscimg.oschina.net/oscnet/up-3f203ac16daff4c7ca1ca49d117d74837ec.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;儘管主權與開源領域的協作看似矛盾，但 94% 的受訪者認為全球協作對實現主權 AI 至關重要。基礎模型與數據集成為協作首要領域（均佔 59%），開發工具與平台緊隨其後（39%）。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;然而，通往開源主權 AI 的道路仍面臨諸多障礙，包括數據質量與可用性問題（44%）以及技術人才短缺（35%）。參與全球 AI 發展的障礙包括：資源限制（35%）、知識產權顧慮（34%）、地緣政治緊張局勢（28%）、國家安全限制（26%）以及合規挑戰（26%）。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;報告指出，展望未來，主權 AI 治理的未來在於開放、社區驅動的框架。戰略建議包括投資開源 AI 基礎設施、培養主權 AI 人才、支持開源基金會及社區驅動的治理模式與標準、應對數據挑戰，以及促進戰略性國際合作。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;研究強調，主權 AI 並非孤立，而是在保持自主權的同時，參與開放、社區驅動的創新網絡。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;詳情&lt;/span&gt;可&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.linuxfoundation.org%2Fhubfs%2FResearch%2520Reports%2Flfr_sovereign_ai25_082525a.pdf" target="_blank"&gt;查看完整報告&lt;/a&gt;&lt;span style="color:#000000"&gt;。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368749/state-of-sovereign-ai</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368749/state-of-sovereign-ai</guid>
      <pubDate>Wed, 27 Aug 2025 07:41:01 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Emacs 真乃編輯器之神，你甚至可以用它來剪輯視頻</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;開發者&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fxenodium.com%2Femacs-as-your-video-trimming-tool" target="_blank"&gt;分享&lt;/a&gt;了基於開源編輯器 Emacs 剪輯視頻的經驗。他使用 Emacs 作為一個視頻剪輯工具接口——對接 &lt;code&gt;ffmpeg&lt;/code&gt;（命令行工具）來完成視頻片段的裁剪工作，並提供直觀的交互方式。&lt;/p&gt; 
&lt;p&gt;&lt;img height="928" src="https://oscimg.oschina.net/oscnet/up-23b7573270c6eabc63a664cb09a8278f681.png" width="1258" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;核心原理&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Emacs 只是一個「前端」&lt;/strong&gt;，實際處理視頻的任務仍由 &lt;code&gt;ffmpeg&lt;/code&gt; 完成。Emacs 充當一個文本驅動、鍵盤操作友好的界面，讓這類操作更流暢、更整潔。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;簡潔的實現&lt;/strong&gt;：整個功能模塊只有幾百行 Emacs Lisp（Elisp）代碼（不包括依賴庫和 &lt;code&gt;ffmpeg&lt;/code&gt;）即可實現，這突顯了 Emacs Lisp 的強大表達力。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;對於這番操作，有網友評價稱「看似荒誕，卻很有意義」：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;「修剪視頻本質上就是寫下開始/結束時間，有時還加個註釋。然後讓這些文本直接轉成剪輯操作，無需跳到別的編輯器。」&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Emacs 用户強調，他們的整個工作流程幾乎都圍繞純文本展開：寫筆記、搜索、翻譯、調用外部工具。在這個環境下，&lt;strong&gt;視頻剪輯的啓動與控制也可以文本化、鍵盤化&lt;/strong&gt;，無需跳出當前編輯器。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368742/emacs-as-your-video-trimming-tool</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368742/emacs-as-your-video-trimming-tool</guid>
      <pubDate>Wed, 27 Aug 2025 07:30:01 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>xAI 發佈編程模型 Grok Code Fast 1，限時免費使用一週</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;xAI 正式發佈其首個專為編程優化的模型 Grok Code Fast 1，內部研發代號為 「Sonic」。這是一個文本推理模型，支持 256k 上下文窗口。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-2caf13f938833e4dc01e61e7bf10e220b3c.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-9b4bfd50c51ed384083516ddd81f13b5707.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="1384" src="https://static.oschina.net/uploads/space/2025/0827/145337_yAlO_2720166.png" width="1270" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;th&gt;類型&lt;/th&gt; 
   &lt;th&gt;價格&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;輸入&lt;/td&gt; 
   &lt;td&gt;每 100 萬 tokens 收費 $0.20&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;緩存輸入&lt;/td&gt; 
   &lt;td&gt;每 100 萬 tokens 收費 $0.02&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;輸出&lt;/td&gt; 
   &lt;td&gt;每 100 萬 tokens 收費 $1.50&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;該模型現已同步登陸 Windsurf、Cursor、GitHub Copilot、Roo Code、Kilo Code 及 opencode 等多個 AI 編程工具，並提供限時免費使用。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368734</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368734</guid>
      <pubDate>Wed, 27 Aug 2025 06:56:01 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>面壁小鋼炮 MiniCPM-V 4.5 開源：8B 性能超越 72B</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;面壁智能&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fbl-u8zXmydHQkeKb06TS2w" target="_blank"&gt;宣佈&lt;/a&gt;正式開源 8B 參數的面壁小鋼炮 MiniCPM-V 4.5 多模態旗艦模型。&lt;/p&gt; 
&lt;p&gt;「行業首個具備高刷視頻理解能力的多模態模型，看得準、看得快，看得長。高刷視頻理解、長視頻理解、OCR、文檔解析能力同級 SOTA，且性能超過 Qwen2.5-VL 72B，堪稱最強端側多模態模型。」&lt;/p&gt; 
&lt;p&gt;&lt;img height="407" src="https://oscimg.oschina.net/oscnet/up-e6b0ad545a5abfce279446d4a97ece3a6c2.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;MiniCPM-V 4.5 亮點一覽&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;首個「高刷視頻理解」多模態模型，兼顧性能與效率：高刷視頻理解同級 SOTA 且超過 Qwen2.5-VL 72B 越級領先；同等視覺 token 開銷下， MiniCPM-V 4.5 可接收 6 倍視頻幀數量，達到 96 倍視覺壓縮率，是同類模型的 12-24 倍；&lt;/li&gt; 
 &lt;li&gt;最強多模態 SOTA 模型：圖片理解、長視頻理解、OCR、文檔解析同級 SOTA，且超過 Qwen2.5-VL 72B 達到，越級領先；&lt;/li&gt; 
 &lt;li&gt;端側友好：提供 SOTA 級多模態表現的同時，帶來最佳的推理效率，顯存佔用、平均推理時間等領先優勢顯著；&lt;/li&gt; 
 &lt;li&gt;支持長思考、短思考可控混合推理，性能好、速度快。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;根據介紹，MiniCPM-V 4.5 通過將模型結構從 2D-Resampler 拓展為 3D-Resampler，進行三維視頻片段的高密度壓縮，在同等視覺 Token 量開銷下的情況下，最大可接收 6 倍視頻幀數量，達到 96 倍視覺壓縮率，是同類模型的 12-24 倍。&lt;/p&gt; 
&lt;p&gt;MiniCPM-V 4.5 通過顯著增加抽幀頻次，從看「PPT」變成理解「動態畫面」。面對一閃而過的畫面，MiniCPM-V 4.5 要比 Gemini-2.5-Pro、GPT-5、GPT-4o 等代表性雲端模型看得更準、更細。&lt;/p&gt; 
&lt;p&gt;在 MotionBench、FavorBench 兩項體現高刷視頻理解能力的榜單中，MiniCPM-V 4.5 達到同尺寸 SOTA，且超過 Qwen2.5-VL 72B，實現越級領先水平。圖片理解性能上，MiniCPM-V 4.5 在 OpenCompass 測評中，領先 GPT-4o、GPT-4.1、Gemini-2.0-Pro 等眾多閉源模型 ，甚至超過 Qwen2.5-VL 72B，實現越級領先。&lt;/p&gt; 
&lt;p&gt;視頻理解性能上，MiniCPM-V 4.5 在 LVBench、MLVU、Video-MME、LongVideoBench 等榜單中，均達到同級最佳水平。在複雜文檔識別任務中，MiniCPM-V 4.5 在 OmniDocBench 榜單的 OverallEdit、TextEdit、TableEdit 三項指標上，均取得了通用多模態模型同級別的 SOTA 表現。&lt;/p&gt; 
&lt;p&gt;此外，MiniCPM-V 4.5 同時支持常規模式和深度思考模式，實現了性能與響應速度的有效平衡，常規模式在絕大部分場景下提供出色的多模態理解能力，深度思考模式則專注於應對複雜與複合型推理任務。&lt;/p&gt; 
&lt;p&gt;在視頻理解榜單 VideoMME、以及單圖 OpenCompass 測試中，MiniCPM-V 4.5 達到同級 SOTA 水平，顯存佔用、平均推理時間等方面領先優勢顯著。&lt;/p&gt; 
&lt;p&gt;其中，在覆蓋短、中、長三種類型的視頻理解評測集 Video-MME 上，MiniCPM-V 4.5 採用 3 幀打包策略進行推理，時間開銷（未計算模型抽幀時間）僅為同級模型的 1/10 。&lt;/p&gt; 
&lt;h4&gt;&lt;strong&gt;&lt;strong&gt;技術解析&lt;/strong&gt;&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;MiniCPM-V 4.5 作為多模態模型的新旗艦，之所以具備「高刷」視頻理解能力、並取得圖片理解、OCR、長視頻理解等 SOTA ，主要得益於在模型結構、訓練範式等領域的創新。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;3D-Resampler 實現高密度視頻壓縮&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;當前，制約多模態模型視頻理解能力的最核心挑戰是性能和效率的 Trade-off，即模型需要看更多視頻幀，才能獲取更精細的信息以提高理解上限；而模型融入太多視頻幀，又會造成顯存、推理速度等開銷爆炸。由於局部片段的不同視頻幀之間存在信息冗餘性，即大部分視覺信息不變，僅有少部分信息發生變化，存在很大的信息壓縮空間。MiniCPM-V 4.5 將模型結構從 2D-Resampler 拓展為 3D-Resampler，進行三維視頻片段的高密度壓縮。具體來説，視頻會按照每 N 個視頻幀一組分組（分組尺寸最大為 6 ），3D-Resampler 會對每個視頻組壓縮編碼，得到 64 個視覺 token（與編碼單圖視覺 token 數量相同），最終在推理開銷不變的情況下，最大 10 fps 抽幀，實現了模型高刷視頻理解能力。得益於 Resampler 機制的靈活性，我們可以在推理階段靈活調整視頻分組尺寸，同時支持單圖、多圖、視頻的統一編碼（即單圖編碼視為 3D 視頻編碼的 2D 特例），方便知識和能力遷移。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;面向多頁文檔圖片的統一&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;OCR&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;和知識推理學習&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;「對文字的識別解析」和「從文檔中學習知識」是多模態大模型的兩個重要課題，但這兩個方向的學習範式割裂，並且分別受到圖像樣例難度和解析準確性的限制。提升 OCR 能力往往需要補充更豐富且難的數據，數據增廣（例如對圖像中文字加高斯噪音）往往會被採用，以提升數據的難度和多樣性，但是過大的增廣會使得文字不可讀，反而會導致模型幻覺。在文檔知識學習方面，大部分工作會將文檔解析成為圖文交替數據進行學習，嚴重受到文解析工具錯誤的影響。&lt;/p&gt; &lt;p&gt;MiniCPM-V 4.5 連續控制圖像中的「文字信息可見度」，使得多模態模型在 OCR 和知識學習兩種模式之間無縫切換，首次實現了 OCR 和知識學習兩種學習範式的有效融合，且不會受到過難增廣和解析錯誤的影響。具體來説，我們首先提取出文檔中的文字框（這通常是非常準確的，大部分解析錯誤來源於排版、閲讀順序、低信息量圖文噪音錯誤），然後對文字框內區域進行不同程度的噪音增廣。當施加噪音較小，文字處於尚可辨別範圍內時，模型會 OCR 學習識別文字；當施加噪音較大，文字無法辨認時，模型會自動進入知識學習，根據文檔的多模態上下文還原文字；當噪音介於兩者之間，模型會進行混合能力的學習。基於上述技術，MiniCPM-V 4.5 低成本實現了領先的 OCR 能力和多模態知識能力。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;通用域混合推理強化學習&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;深度思考推理能力極大拓展了多模態大模型的推理能力邊界，但在常見問題場景中，也往往伴隨過高的推理延遲。MiniCPM-V 4.5 同時支持常規模式和深度思考模式，實現了性能與響應速度的有效平衡：常規模式在絕大部分場景下提供出色的多模態理解能力，深度思考模式則專注於應對複雜與複合型推理任務。為了讓模型在兩種模式下都具備出色性能，MiniCPM-V 4.5 提出了混合推理的強化學習訓練方案，在強化學習過程中同時激活常規和深度思考模式，實現在相同訓練時長下顯著更強的常規模式性能和相似的深度思考模式性能。藉助 RLPR 技術，MiniCPM-V 4.5 進一步從通用域多模態推理數據上獲得高質量的獎勵信號，以提升廣泛通用領域的推理能力。最終，通過輕量化的 RLAIF-V 訓練階段，模型在保持推理能力的同時顯著降低了幻覺。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368729</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368729</guid>
      <pubDate>Wed, 27 Aug 2025 06:48:01 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>YouTube 承認未經許可使用 AI 修改視頻</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;近期，音樂領域的知名視頻創作者 Rick Beato 在 YouTube 上發現了一個令人困惑的現象：他的視頻形象出現了異常，頭髮看起來不自然，彷彿被不經意間「化妝」了。這一發現引起了他的警覺。&lt;/p&gt; 
&lt;p&gt;&lt;img height="267" src="https://oscimg.oschina.net/oscnet/up-b19b11e08e5f1976dd09c557d491f44bb15.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;實際上，Rick Beato 並非個例。過去數月間，不少 YouTube 用户悄然發現自己的視頻被平台利用 AI 技術進行了調整。這些調整包括讓襯衫的褶皺更加清晰、皮膚質感更加鋭利或平滑，甚至有時會導致耳朵形狀出現微妙的扭曲。雖然這些變化並不顯著，但對於追求原生質感的創作者而言，卻足以讓他們的內容帶上一種不受歡迎的 AI 生成痕跡。&lt;/p&gt; 
&lt;p&gt;另一位音樂領域的視頻創作者 Rhett Shull 也遭遇了類似的問題。他在發現視頻形象被篡改後，迅速發佈了一段視頻表達了自己的不滿。Rhett 認為，這種未經同意的修改不僅會歪曲他的個人形象，更會侵蝕他與觀眾之間建立的信任基礎。&lt;/p&gt; 
&lt;p&gt;早在今年六月，社交媒體上就出現了關於 YouTube 視頻被 AI 修改的投訴。面對用户的質疑，YouTube 終於承認正在對 YouTube Shorts 中的少量視頻進行技術調整。YouTube 編輯和創作者聯絡主管 Rene Ritchie 解釋稱，他們正在實驗使用傳統的機器學習技術來去除視頻中的模糊、降噪並提高清晰度。他還表示，YouTube 將繼續考慮創作者和觀眾的反饋，以改進這一功能。&lt;/p&gt; 
&lt;p&gt;然而，並非所有人都對 YouTube 的解釋買賬。美國匹茲堡大學的 Samuel Woolley 就指出，YouTube 在未經視頻製作者同意的情況下就擅自修改內容，這種做法不僅不尊重創作者的權益，而且對技術的描述也存在誤導性。他認為，YouTube 應該更加透明地告知用户這些調整的存在，並徵求他們的同意。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368722</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368722</guid>
      <pubDate>Tue, 19 Aug 2025 06:07:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>全球首現 AI 勒索軟件 PromptLock</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;知名網絡安全公司 ESET 揭露了全球首個 AI 勒索軟件 ——PromptLock。這款惡意軟件使用了 OpenAI 開源的 gpt-oss:20b 語言模型，能夠在感染的設備上本地生成惡意 Lua 代碼，令人擔憂的是，它不僅支持 Windows 系統，還能夠在 Linux 和 macOS 平台上運行。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;根據 ESET 的最新報告，PromptLock 利用預設的文本提示詞調用 gpt-oss:20b 模型，在受害設備上直接生成能夠搜索、竊取和加密文件的惡意代碼。該程序的設計使得其能在各種操作系統中靈活適配，具備高度的隱蔽性和適應性。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;目前，雖然還未發現 PromptLock 具備直接刪除文件的功能，但其潛在威脅性顯而易見，黑客未來可能會對其進行進一步的升級和完善。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="449" src="https://oscimg.oschina.net/oscnet/up-062d9f1c3a96ae45e365d70504f7ffe172c.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在運行機制方面，gpt-oss:20b 模型本身體積龐大，約為 13GB，對於顯存的要求較高。不過，ESET 指出，攻擊者可以通過構建內部智能體或隧道，將受害網絡與外部服務器連接，藉此使用運行在外部服務器上的模型，從而繞過本地顯存的限制，使用 Ollama API 進行訪問。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;安全專家對此表示，PromptLock 可能只是一個概念驗證程序，或者仍在開發中，但也不容忽視的是，這一事件可能是惡意利用本地或私有 AI 的早期信號。Citizen Lab 的研究員 John Scott-Railton 警告稱，當前的防禦措施尚未做好應對這種新型威脅的準備。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;OpenAI 對這一事件做出了回應，感謝研究人員的通報，並表示他們已經採取了相應措施，以降低模型被惡意利用的風險。OpenAI 表示將持續努力，完善防護機制，確保其技術不會被用於不法行為。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368714</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368714</guid>
      <pubDate>Tue, 19 Aug 2025 05:45:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>全球芯片市場預計將突破 1 萬億美元大關</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;根據分析公司 Counterpoint Research 發佈的一份新報告，人工智能的迅速發展正在推動全球半導體市場迎來前所未有的增長。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;該報告預測，隨着代理 AI 和物理 AI 的興起，全球芯片市場的收入預計將在未來數年內突破 1 萬億美元。這一數字幾乎是當前市場規模的兩倍，預計到 2024 年，市場收入將達到 6560 億美元。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;報告指出，當前我們正處於人工智能基礎設施開發的第二階段。在這一階段，應用程序如對話式人工智能、語義搜索和多媒體內容生成等正在快速增長。這些新技術對計算、內存和網絡等資源的需求也在不斷上升。此外，&lt;span&gt;第一&lt;/span&gt;階段的基礎文本應用程序已經逐漸轉向處理圖像、音頻和視頻的多模態生成 AI。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;Counterpoint Research 的研究副總裁 Niel Shah 表示，未來的物理人工智能將標誌着基礎設施發展的第三次浪潮。隨着工業機器人、人形機器人和智能車輛的崛起，半導體芯片將成為不可或缺的一部分。這些芯片將廣泛應用於 GPU、內存、智能手機以及其他人工智能基礎設施組件中。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;到 2030 年，服務器將對萬億美元收入做出重要貢獻，預計市場規模將從 2024 年的 1530 億美元增至 4640 億美元。同時，智能手機目前的收入為 1720 億美元，預計將在未來幾年內增至 2590 億美元。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;科技公司紛紛投入巨資開發人工智能數據中心和基礎設施。例如，亞馬遜的機器人團隊已經成立了一個專注於物理人工智能的團隊，並推出了&lt;span&gt;首款&lt;/span&gt;具有觸覺的機器人。此外，NVIDIA 展示了全新的 Blackwell Ultra GPU，專為大規模人工智能推理和代理人工智能工作負載設計。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在未來，四分之一的筆記本電腦出貨量將把生成式 AI 作為賣點，Google、三星和蘋果等科技巨頭也在積極佈局這一領域。隨着收入來源的轉變，未來的市場將更多關注由人工智能驅動的應用和服務，以實現更高的利潤。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368697</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368697</guid>
      <pubDate>Tue, 19 Aug 2025 03:43:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>JetBrains 成為 Godot Foundation 白金贊助商</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;JetBrains &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FQSUe7nRs1v3YKERulwcWxw" target="_blank"&gt;宣佈&lt;/a&gt;已成為 Godot Foundation 白金贊助商。&lt;/p&gt; 
&lt;p&gt;Godot 是由社區推動的開源遊戲引擎。Godot Foundation 是一個非營利組織，旨在為 Godot Project 提供資金支持，從而幫助實現這些目標。它為此僱用開發者兼職或全職從事 Godot Engine 和相關項目、僱用藝術家創作高質量演示藝術品、購買硬件和幫助支付其他類似費用。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0827/113556_iPfr_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;JetBrains 表示，他們將作為白金贊助商為這項事業做出貢獻，並相信更多的工具選擇會使生態系統更加豐富。&lt;/p&gt; 
&lt;p&gt;JetBrains Rider 在 2020.1 版本中首次引入了對 Godot 的支持，並在 2024.2 版本中捆綁，直接提供強大的 Godot 功能。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0827/113648_9XwK_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在即將發佈的 2025.2 版本中，我們還將通過一款最初由 David Horacek 在社區中創建、現在由 JetBrains 維護的插件捆綁改進的 GDScript 支持。&amp;nbsp;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368696</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368696</guid>
      <pubDate>Tue, 19 Aug 2025 03:36:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>抖音副總裁李亮回應 「人均 7 個月離職」 傳聞：稱大多是 AI 創作</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;抖音集團副總裁李亮近日通過個人微博對有關 「人均 7 個月離職」 的傳聞進行了澄清。他指出，該傳聞源於一篇在《中歐商業評論》上發表的文章，而這篇文章的內容存在許多不實之處，李亮甚至表示這些內容可能是由人工智能（AI）創作的。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="377" src="https://oscimg.oschina.net/oscnet/up-87e372d390ff076ac71f6fa846551f0c9be.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;李亮在微博中提到，文章提到的 「抖音 2016 年春節計劃上線‘拜年&lt;span&gt;特效&lt;/span&gt;’」 的説法並不準確。他指出，抖音實際上是在 2016 年 9 月才正式上線，因此不可能在當年春節期間推出&lt;span&gt;特效&lt;/span&gt;。此外，李亮強調，抖音在之後的春節營銷中並沒有因為跨部門協調延遲而錯失機會，文章中的表述顯然是錯誤的。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;文章還提到字節跳動的全球業務被拆分為 217 個 「創業單元」，每個單元員工不足 150 人。李亮對此表示，這完全是造謠，依據此説法，字節員工總數不應超過 3.2 萬人，而實際員工數量遠遠高於此。他還指出，文中提及的 「35% 內外部流動率」、「項目存活率需大於 45%」 等數據幾乎都是憑空想象，毫無事實依據。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;李亮特別提到，這篇文章的作者在《中歐商業評論》上發表了多篇疑似由 AI 撰寫的文章，涉及多個企業。他呼籲該媒體對類似 AI 創作的文章應註明其真實性，避免誤導讀者。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;並建議《中歐商業評論》在未來發布類似內容時，附上 「此為 AI 創作，內容不實，請謹慎閲讀」 的提醒，以確保信息的真實性。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368690</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368690</guid>
      <pubDate>Tue, 19 Aug 2025 03:27:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>谷歌翻譯新增 AI 功能「Live Translate」，支持實時對話翻譯</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;谷歌為旗下翻譯工具 Google Translate 正式&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.google%2Fproducts%2Ftranslate%2Flanguage-learning-live-translate%2F" target="_blank"&gt;發佈兩項全新 AI 功能&lt;/a&gt;：實時翻譯與學習。&lt;/p&gt; 
&lt;p&gt;實時對話翻譯方面，新版 Live Translate 在原有實時會話基礎上擴展至 70 多，種語言（含中文）。用户只需在 Android 或 iOS 端打開 Translate 應用，點擊 「Live translate」，選定語言後即可開始説話；系統同步輸出語音譯文與雙語文字記錄，並智能識別停頓、口音與語調，實現自然對話。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-61ec1454df144a013fd03ebb668674126c2.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;該功能今日起率先在美國、印度及墨西哥，上線，基於先進的語音分離模型，確保機場、咖啡館等嘈雜環境仍保持高質量。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-4e76c62c82a9a1bc9f2fc58f308ebe515d9.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;語言學習方面，谷歌同步啓動名為 Practice 的 Beta 體驗。該功能依據用户設定的水平與目標，即時生成個性化聽説練習場景。課程由語言習得專家設計，實時追蹤每日進度，當前優先面向英語母語者練習西班牙語與法語，以及西班牙語、法語、葡萄牙語母語者練習英語。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368689</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368689</guid>
      <pubDate>Tue, 19 Aug 2025 03:21:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Java 開源工具集 Hutool-5.8.40 發佈，常規 bug 修復</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img src="https://static.oschina.net/uploads/img/201803/21114512_tLDC.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhutool.cn%2F" target="_blank"&gt;https://hutool.cn/&lt;/a&gt;&lt;/p&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="background-color:#f6f6f6"&gt;Hutool&lt;/span&gt;&lt;span style="background-color:#ffffff; color:#40485b"&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;是一個功能豐富且易用的&lt;/span&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;Java 工具庫&lt;/strong&gt;&lt;span style="background-color:#ffffff; color:#40485b"&gt;，通過諸多實用工具類的使用，旨在幫助開發者快速、便捷地完成各類開發任務。 這些封裝的工具涵蓋了字符串、數字、集合、編碼、日期、文件、IO、加密、數據庫 JDBC、JSON、HTTP 客户端等一系列操作， 可以滿足各種不同的開發需求。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;------------------------------------------------------------------------------&lt;/p&gt; 
&lt;p&gt;&lt;span style="background-color:#ffffff; color:#333333"&gt;此次更新為常規 bug 修復，此次有幾個比較「坑」的 bug，到最近才發現並修復，如 StrBuilder 的 insert 插入錯誤，可能會踩坑，用到這個類的請更新注意，其它更新如下：&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;🐣新特性&lt;br&gt; 【captcha】 MathGenerator 四則運算方式支持不生成負數結果（pr#1363@Gitee）&lt;br&gt; 【core 】 增加 MapValueProvider 和 RecordConverter 並支持 Record 轉換（issue#3985@Github）&lt;br&gt; 【core 】 CalendarUtil 增加 isSameYear 和 calendar 方法（issue#3995@Github）&lt;br&gt; 【core 】 DateUtil 增加 yyyy-MM-dd'T'HH:mmXXX 格式支持（pr#1367@Gitee）&lt;br&gt; 【core 】 MapUtil 增加 flatten 方法（pr#1368@Gitee）&lt;br&gt; 【extra 】 getClientIP 優先獲取傳入的請求頭信息（pr#1373@Gitee）&lt;br&gt; 【db 】 增加 Gbase8s 驅動支持（issue#ICSFAM@Gitee）&lt;br&gt; 【db 】 增加 TDSQL PostgreSQL 版本、TDSQL-H LibraDB、Snowflake、Teradata 的驅動支持（pr#4024@Github）&lt;br&gt; 【core 】 EnumUtil 增加緩存支持（pr#1376@Gitee）&lt;br&gt; 🐞Bug 修復&lt;br&gt; 【extra 】 Sftp``reconnectIfTimeout 方法改為捕獲所有異常（issue#3989@Github）&lt;br&gt; 【core 】 修復 ChineseDate 閏年閏月節日獲取問題（issue#ICL1BT@Gitee）&lt;br&gt; 【core 】 修復 TreeBuilderappend 重複向 idTreeMap 中 put 問題（pr#3992@Github）&lt;br&gt; 【extra 】 修復 QLExpressEngineallowClassSet 無效問題（issue#3994@Github）&lt;br&gt; 【core 】 修復 StrBuilderinsert 插入計算錯誤問題（issue#ICTSRZ@Gitee）&lt;br&gt; 【cron 】 修復 CronPatternUtil.nextDateAfter 計算下一個匹配表達式的日期時，計算錯誤問題（issue#4006@Github）&lt;br&gt; 【cache 】 ReentrantCache 修改 get 邏輯 key 鎖改為全局鎖，保證安全（issue#4022@Github）&lt;br&gt; 【core 】 修復 NumberWordFormatterformatSimple 輸出錯誤問題（pr#4034@Github）&lt;/p&gt; 
&lt;p&gt;具體更新請關注：https://hutool.cn/docs/#/CHANGELOG&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368685/hutool-5-8-40</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368685/hutool-5-8-40</guid>
      <pubDate>Tue, 19 Aug 2025 03:09:00 GMT</pubDate>
      <author>來源: 資訊</author>
    </item>
    <item>
      <title>rsyslog 8.2508.0 發佈</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;rsyslog 8.2508.0 版本現已&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.rsyslog.com%2Frsyslog-8-2508-0-2025-08-release-announcement%2F%3Futm_source%3Drss%26utm_medium%3Drss%26utm_campaign%3Drsyslog-8-2508-0-2025-08-release-announcement" target="_blank"&gt;發佈&lt;/a&gt;。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;公告稱，「這是一個規模宏大且意義非凡的 rsyslog 版本。此版本將進一步推進我們負責任的 AI First 戰略，並堅定地邁向雲原生運營。此外，它還在質量、安全性和文檔方面進行了重大改進。」&lt;/span&gt;&lt;/p&gt; 
&lt;p style="margin-left:0px; margin-right:0px; text-align:start"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;亮點&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;雲原生進展：原生 Prometheus metrics、health checks、Docker artifacts 合併到 monorepo 中，以及強化的 HTTP output。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;強大的網絡：對 TCP 服務器進行深度重構，並修復了 imtcp 和相關代碼路徑中的幾個競爭條件。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;更安全的默認配置和更清晰的錯誤提示：權限修復、更好的 TLS 診斷、改進的 omelasticsearch 身份驗證處理。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;語言和管道增強：新的 RainerScript 在 pmrfc3164 中具有 headerless 檢測功能、新的 PCRE 匹配模塊和 AI tagging PoC。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;開發人員體驗：格式規範化、類型安全回調、Doxygen 文檔和內部接口清晰化。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;Packaging change：文檔現集中於主倉庫 doc /目錄下，不再提供獨立的文檔 tarball。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="margin-left:0px; margin-right:0px; text-align:start"&gt;&lt;strong&gt;&lt;span style="color:#000000"&gt;Responsible&amp;nbsp;&lt;/span&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.rsyslog.com%2Fclarifying-ai-first-what-it-really-means-for-rsyslog%2F" target="_blank"&gt;AI First&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;AI 生成或 AI 輔助的變更將與所有其他代碼一樣嚴格地進行審查和測試。維護人員對代碼的最終驗收做出決定。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;拒絕低質量的 AI 輸出。優化提示詞、添加防護措施、在必要時簡化代碼模式，並擴展了文檔以確保人類和工具都能清晰理解意圖。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;mmaitag 是正在研發的實用型人工智能的早期 PoC。其重點在於安全、增量的價值。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;更多詳情可&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.rsyslog.com%2Frsyslog-8-2508-0-2025-08-release-announcement" target="_blank"&gt;查看官方公告&lt;/a&gt;。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368684/rsyslog-8-2508-0</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368684/rsyslog-8-2508-0</guid>
      <pubDate>Tue, 19 Aug 2025 03:08:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Anthropic 推出 Claude for Chrome，支持在網頁中執行實際操作</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Anthropic &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.anthropic.com%2Fnews%2Fclaude-for-chrome" target="_blank"&gt;宣佈&lt;/a&gt;啓動 &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fclaude.ai%2Fchrome" target="_blank"&gt;&lt;strong&gt;Claude for Chrome&lt;/strong&gt;&lt;/a&gt; 瀏覽器擴展的測試，讓 AI 不再侷限於對話框，而是能在網頁中執行實際操作，例如查看頁面、填寫表單、點擊按鈕。這一功能有望顯著提升工作效率，但也帶來前所未有的安全挑戰。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-52d27989ae7625c22073a75b3d2e4158efc.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在紅隊測試中，Claude 在未加防護的情況下，面對 123 種攻擊方式，有 &lt;strong&gt;23.6%&lt;/strong&gt; 被成功利用，甚至曾因被偽裝成「安全通知」的郵件欺騙而刪除整份郵箱內容。對此，Anthropic 已引入多層防護機制，包括：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;權限控制&lt;/strong&gt;：用户可選擇授予或撤銷 Claude 對特定網站的訪問；敏感操作必須經過確認。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;風險隔離&lt;/strong&gt;：屏蔽金融、成人、侵權等高風險站點。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;安全引導與檢測&lt;/strong&gt;：通過系統提示和異常行為分類器，攔截潛在的惡意指令。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;目前，Claude for Chrome 僅向 &lt;strong&gt;Max 計劃的首批 1,000 名用户&lt;/strong&gt;開放測試。Anthropic 表示，將在收集用户反饋後逐步擴大範圍，並持續強化安全機制。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368683/anthropic-claude-for-chrome</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368683/anthropic-claude-for-chrome</guid>
      <pubDate>Tue, 19 Aug 2025 03:05:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>百度 AI 搜索 APP「梯子 AI」發佈：由 Tizzy.ai 改名而來，主打無廣告</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span&gt;百度旗下 AI 搜索應用在完成前期緊密測試後，於近日正式以全新名稱「梯子 AI」上架應用市場。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;據悉，這款應用最初於 8 月 10 日以「Tizzy.ai」之名發佈，迅速吸引了眾多科技愛好者的關注。經過一系列優化與迭代，其版本號從初版的 1.0.0 直接躍升至 1.2.0，不僅名稱煥然一新，功能與服務也實現了全面升級。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;「梯子 AI」被定位為智能搜索助手，它依託百度強大的多個大模型能力開發而成，主打無廣告智能搜索服務。這一創新定位旨在打破傳統搜索模式，通過整合深度思考、資源檢索及影視娛樂等多功能於一體，為用户提供「簡單搜索，一觸即達」的&lt;span&gt;極致&lt;/span&gt;體驗。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="619" src="https://oscimg.oschina.net/oscnet/up-a8ff6784ed63b704c5b4658895674e30b1a.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在功能設計上，「梯子 AI」展現了其獨特的優勢。其 AI 雙模智能搜索功能支持自動思考與深度思考的雙模式智能引擎，能夠精準分析&lt;span&gt;全網&lt;/span&gt;信息，並結合用户偏好提供個性化答案，使搜索結果更加高效、精準。同時，應用還注重交互體驗的極簡設計，簡潔的搜索框讓用户輸入問題後即可直達答案，全程無任何推廣信息幹擾，真正實現了「純淨搜索」。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;除了智能搜索外，「梯子 AI」還構建了豐富的短劇生態，網羅了全球熱門電影、電視劇、短劇等海量影視資源，讓用户暢看無阻。更令人驚喜的是，在觀看短劇過程中，用户將享受無廣告、無會員、加速緩衝的沉浸式觀劇體驗。應用還重新設計了每個細節，確保用户在邊看邊搜的過程中也能感受到&lt;span&gt;極致&lt;/span&gt;的舒適與便捷。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368680</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368680</guid>
      <pubDate>Tue, 19 Aug 2025 02:55:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>vivo 等提出 DiMo-GUI：模態分治 + 動態聚焦，GUI 智能體推理時擴展的新範式</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;blockquote&gt; 
 &lt;p style="margin-left:0; margin-right:0"&gt;作者：vivo 互聯網算法團隊&lt;/p&gt; 
 &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#40a9ff"&gt;&lt;strong&gt;本文入選 EMNLP 2025 Main Conference&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
 &lt;p style="margin-left:0; margin-right:0"&gt;&lt;strong&gt;EMNLP 會議&lt;/strong&gt;全稱為 Conference on Empirical Methods in Natural Language Processing，由國際計算語言學協會 ACL 舉辦，是自然語言處理和人工智能領域最重要的學術會議之一。EMNLP 2025 會議共有 8174 篇投稿，Main Conference 接收率僅為 22.16%。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;img src="https://oscimg.oschina.net/oscnet//53e017ae0673a706f87b381b014e1ca6.png" referrerpolicy="no-referrer"&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;p style="color:#303030; margin-left:0; margin-right:0; text-align:start"&gt;項目主頁：&lt;/p&gt; 
&lt;p style="color:#303030; margin-left:0; margin-right:0; text-align:start"&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fxie.infoq.cn%2Flink%3Ftarget%3Dhttps%253A%252F%252Fgithub.com%252Fvivo%252FDiMo-GUI" rel="nofollow" target="_blank"&gt;https://github.com/vivo/DiMo-GUI&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p style="margin-left:0; margin-right:0"&gt;&lt;strong&gt;摘要：&lt;/strong&gt;&lt;/p&gt; 
 &lt;p style="margin-left:0; margin-right:0"&gt;本文介紹了一種無需額外訓練的 GUI 定位框架 DiMo-GUI，針對多模態大語言模型（MLLMs）在複雜圖形用户界面（GUI）定位任務中的挑戰，通過動態視覺推理與模態感知優化顯著提升性能。DiMo-GUI 採用逐級縮放的動態定位機制，迭代裁剪聚焦目標區域，減少視覺冗餘；同時分離文本與圖標模態，獨立推理後結合指令評估確定最終目標，有效平衡多模態處理能力。在 GUI 定位任務最新的基準數據集上，DiMo-GUI 相較基線展現顯著性能提升。作為即插即用框架，DiMo-GUI 適用於網頁導航、移動應用自動化等場景，未來可通過回溯機制進一步提升魯棒性。&lt;/p&gt; 
 &lt;p style="margin-left:0; margin-right:0"&gt;該工作由 vivo 互聯網算法團隊、加州大學默塞德分校、昆士蘭大學共同完成。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;span id="OSC_h1_1"&gt;&lt;/span&gt; 
&lt;h1&gt;一、引言&lt;/h1&gt; 
&lt;p style="color:#303030; margin-left:0; margin-right:0; text-align:start"&gt;隨着&lt;strong&gt;圖形用户界面（Graphical User Interface, GUI）&lt;/strong&gt;在自動化導航和操作系統控制等領域的廣泛應用，基於自然語言查詢的 &lt;strong&gt;GUI 定位（GUI Grounding）&lt;/strong&gt;成為&lt;strong&gt;多模態大語言模型（multimodal large language models, MLLMs）&lt;/strong&gt;的重要研究方向。然而，GUI 環境的視覺複雜性、語言歧義以及空間雜亂等問題為精準定位帶來了顯著挑戰。&lt;/p&gt; 
&lt;p style="color:#303030; margin-left:0; margin-right:0; text-align:start"&gt;本文基於最新研究《DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning》，介紹了一種無需額外訓練的 GUI 定位框架——&lt;strong&gt;DiMo-GUI&lt;/strong&gt;，通過動態視覺推理和模態感知優化顯著提升了多模態大模型在複雜 GUI 環境中的定位性能，推動了&lt;strong&gt;推理時擴展（test-time scaling）&lt;/strong&gt;在該領域的發展。&lt;/p&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;img src="https://oscimg.oschina.net/oscnet//0dc3cd4bfdb7f8a14c2a2fde07e79ed9.png" referrerpolicy="no-referrer"&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;p style="color:#303030; margin-left:0; margin-right:0; text-align:start"&gt;日常生活中，我們與電腦、手機的交互離不開圖形用户界面。小到點贊、大到數據分析，我們都希望 AI 能像人一樣，理解屏幕上的每一個按鈕、每一段文字，並準確執行指令。然而，對於飛速發展中的多模態大模型來説，這卻是前所未有的艱鉅挑戰。在一個複雜的 App、網頁或桌面軟件中，用户可能隨手一句「點擊開始播放」，但對於 AI 來説，準確找到這個指令對應的圖標/按鈕並不簡單：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;strong&gt;模態混雜&lt;/strong&gt;：用户界面同時包含文本、圖標、背景、裝飾性元素等，幹擾多；並且大多數 VLM 對文字理解更強，圖標處理卻弱，造成嚴重偏差；&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;strong&gt;冗餘信息&lt;/strong&gt;：高分辨率 UI 中，重要區域可能只佔整體的幾十分之一，模型容易定位錯誤區域。&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p style="color:#303030; margin-left:0; margin-right:0; text-align:start"&gt;研究發現，傳統方法如基於文本推理或單次視覺定位的管道在高分辨率、視覺擁擠的 GUI 中表現不佳。例如在最新的 ScreenSpot-Pro 數據集上，大多數通用模型如 GPT-4o, Qwen2-VL 等只有 1% 左右的正確率， 即使是針對於 GUI 定位任務的 ShowUI, Aria-UI 等智能體也只有 10% 左右的正確率。&lt;/p&gt; 
&lt;span id="OSC_h1_2"&gt;&lt;/span&gt; 
&lt;h1&gt;二、關鍵改進：模態分離 + 動態定位&lt;/h1&gt; 
&lt;p style="color:#303030; margin-left:0; margin-right:0; text-align:start"&gt;從上述問題出發，該研究推出零訓練成本的 DiMo-GUI，通過模態感知的視覺推理推進訓練時擴展，顯著提升多模態大模型的圖形界面（GUI）理解能力。主要的改進方式包括以下兩點：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;strong&gt;動態視覺定位&lt;/strong&gt;：DiMo-GUI 採用逐級縮放機制，從粗略預測開始，基於初始座標生成候選焦點區域，並通過迭代裁剪逐步聚焦目標。例如，首次推理後，模型以預測座標為中心裁剪半個圖像大小的區域作為下一輪輸入，顯著減少視覺冗餘。動態迭代機制根據前後預測的座標距離（小於圖像對角線六分之一時停止）實現自適應停止，避免「過度思考」。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;strong&gt;模態感知優化&lt;/strong&gt;：DiMo-GUI 將 GUI 元素分為文本和圖標兩類，分別進行獨立的定位推理，生成文本座標（C_text）和圖標座標（C_icon）。隨後，模型結合原始指令和全分辨率圖像評估兩個候選座標，確定最終目標 （C*），有效平衡文本和圖標的處理能力。&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p style="color:#303030; margin-left:0; margin-right:0; text-align:start"&gt;這樣的方式推動了&lt;strong&gt;推理時拓展（Test-time Scaling）&lt;/strong&gt;在 GUI 定位這一領域的發展，提供了新的思路和方式。&lt;/p&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;img src="https://oscimg.oschina.net/oscnet//e83f73873fbad7e248e1da405b99f189.png" referrerpolicy="no-referrer"&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;p style="color:#303030; margin-left:0; margin-right:0; text-align:start"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;span id="OSC_h1_3"&gt;&lt;/span&gt; 
&lt;h1&gt;三、實驗結果：無需訓練和任何額外數據，只在推理階段就可以大幅提升性能&lt;/h1&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;img src="https://oscimg.oschina.net/oscnet//b3d3d49204e1faa771ec0ad0791dea74.png" referrerpolicy="no-referrer"&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;p style="color:#303030; margin-left:0; margin-right:0; text-align:start"&gt;團隊在最新的高分辨率 GUI 數據集 ScreenSpot-Pro 上驗證發現：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;DiMo-GUI 可以作為即插即用的框架大幅提升多個 GUI 模型的性能。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;其中 OS-Atlas-7B 在引入 DiMo-GUI 之後獲得了超過兩倍的指標提升（18.9% -- 49.7%）, UGround-7B 和 UGround-V1-7B 也均獲得了超過 10% 的指標提升。&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p style="color:#303030; margin-left:0; margin-right:0; text-align:start"&gt;在相對簡單的 ScreenSpot 數據集上，DiMo-GUI 同樣可以提升多個模型的性能。&lt;/p&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;img src="https://oscimg.oschina.net/oscnet//92892c7de106ba9aac72c1532918459f.png" referrerpolicy="no-referrer"&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;p style="color:#303030; margin-left:0; margin-right:0; text-align:start"&gt;定性結果表示，模型加入 DiMo-GUI 之後可以通過動態定位逐步逼近正確結果。&lt;/p&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;img src="https://oscimg.oschina.net/oscnet//b763f01354805dae541a9ab9898a0ae1.png" referrerpolicy="no-referrer"&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;span id="OSC_h1_4"&gt;&lt;/span&gt; 
&lt;h1&gt;四、總結&lt;/h1&gt; 
&lt;p style="color:#303030; margin-left:0; margin-right:0; text-align:start"&gt;&lt;strong&gt;DiMo-GUI&lt;/strong&gt;&amp;nbsp;提供了一種高效、通用且無需訓練的 &lt;strong&gt;GUI 定位框架&lt;/strong&gt;，通過動態視覺推理和模態感知優化顯著提升了多模態大語言模型在複雜 GUI 環境中的表現。其&lt;strong&gt;「即插即用」&lt;/strong&gt;特性使其可無縫集成到現有 &lt;strong&gt;GUI Agent &lt;/strong&gt;中，適用於網頁導航、移動應用自動化等場景。未來研究可探索引入回溯機制以糾正早期錯誤，進一步提升定位魯棒性。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/vivotech/blog/18689577</link>
      <guid isPermaLink="false">https://my.oschina.net/vivotech/blog/18689577</guid>
      <pubDate>Tue, 19 Aug 2025 02:37:00 GMT</pubDate>
      <author>原創</author>
    </item>
    <item>
      <title>谷歌發佈新圖像生成模型 nano banana</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;谷歌正式&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdevelopers.googleblog.com%2Fen%2Fintroducing-gemini-2-5-flash-image%2F" target="_blank"&gt;發佈&lt;/a&gt;了其最先進的圖像生成與編輯模型——Gemini 2.5 Flash Image（又名 nano banana）。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-9cb4376ea1cc064051cdbcd5e04a019d086.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-cd68bfaad4042c2de322bf1198b1e09f449.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;據官方介紹，Gemini 2.5 Flash Image 的主要特點包括下面幾點：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;充分保持角色的一致性：它可以輕鬆地將同一個角色置於不同的環境中，或者從多個角度展示同一款產品，同時完美地保持其核心主體不變。&lt;/li&gt; 
 &lt;li&gt;基於提示的圖片編輯：允許用户通過簡單的自然語言指令，對圖片進行精準的局部修改 。&lt;/li&gt; 
 &lt;li&gt;利用 Gemini 的現實世界知識：模型可藉助 Gemini 強大的世界知識庫，讓圖像生成變得更加「智能」。&lt;/li&gt; 
 &lt;li&gt;多幅圖像融合：可以將一張圖片中的物體「放」進另一張圖片的場景裏，整個過程只需一條提示指令就能完成。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;性能表現上，Gemini 2.5 Flash Image 在多項基準測試上均為第一名，超越 OpenAI ChatGPT 4o（GPT Image 1 high）、Qwen Image Edit 等模型。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-bfee07407ab38e2b001fd0dbe895d36f242.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;關於調用 API，具體的定價是每百萬輸出 token 30 美元，官方介紹，生成一張圖片大約消耗 1290 個輸出 token，也就是説，每張圖片的成本約為 0.039 美元，換算下來人民幣不到 3 毛錢。&lt;/p&gt; 
&lt;p&gt;目前，Gemini 2.5 Flash Image 已經可以通過 Gemini APP、Gemini API、Google AI Studio 和 Vertex AI 進行訪問。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368669/google-gemini-2-5-flash-image</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368669/google-gemini-2-5-flash-image</guid>
      <pubDate>Tue, 19 Aug 2025 02:21:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>蘋果內部正探討收購 Mistral 和 Perlextity 可能性</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;據報道，蘋果公司內部已就收購法國人工智能初創公司 Mistral 以及美國的 Perplexity 展開討論。這一舉措旨在增強其人工智能能力，以應對谷歌和三星等競爭對手的領先優勢。&lt;/p&gt; 
&lt;p&gt;此前，蘋果首席執行官蒂姆・庫克在上個月暗示，公司對大規模人工智能相關收購持開放態度，以加速其人工智能發展路線圖，這與蘋果以往在併購方面的保守姿態有所不同。Mistral 在去年 B 輪融資後估值超過 60 億美元，本月有報道稱該公司正在洽談以 100 億美元估值籌集 10 億美元資金。今年早些時候，彭博社也曾報道，蘋果高管內部討論過對 Perplexity 的潛在收購意向。&lt;/p&gt; 
&lt;p&gt;據《The Information》報道，蘋果服務業務主管埃迪・庫伊是收購人工智能公司以增強蘋果產品實力的主要倡導者，他曾提議收購 Netflix 和特斯拉，但均被庫克否決。而軟件業務主管克雷格・費德里吉則對大規模人工智能收購持謹慎態度，他認為蘋果有能力內部構建人工智能技術。&lt;/p&gt; 
&lt;p&gt;目前，蘋果對這兩起潛在收購仍存顧慮，因其可能涉及鉅額資金，而蘋果歷史上極少有超億美元的收購交易。若聯邦裁決終止蘋果與谷歌 200 億美元的默認搜索引擎合作，蘋果或更有動力收購人工智能搜索初創公司填補空缺。&lt;/p&gt; 
&lt;p&gt;截至目前，蘋果、Mistral 和 Perplexity 均未對此置評。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368665</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368665</guid>
      <pubDate>Tue, 19 Aug 2025 02:12:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Opera 二季度營收同比增長 30%，AI 生態開啓新一輪增長週期</title>
      <description/>
      <link>https://www.oschina.net/news/368662</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368662</guid>
      <pubDate>Tue, 19 Aug 2025 02:06:00 GMT</pubDate>
    </item>
    <item>
      <title>阿里開源視頻生成模型 Wan2.2-S2V</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;阿里&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F5FwE7TvjzDYQabnpnMru6Q" target="_blank"&gt;宣佈&lt;/a&gt;開源全新多模態視頻生成模型通義萬相 Wan2.2-S2V，通過一張靜態圖片和一段音頻，可生成電影級數字人視頻，該模型單次生成的視頻時長可達分鐘級，提升數字人直播、影視製作、AI 教育等行業的視頻創作效率。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="256" src="https://oscimg.oschina.net/oscnet/up-da3fb7fdb3af007528ceab6ea0bd26a6902.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;根據介紹，Wan2.2-S2V 可驅動真人、卡通、動物、數字人等多種類型圖片，並支持肖像、半身以及全身等任意畫幅，上傳一段音頻後，模型就能讓圖片中的主體形象完成説話、唱歌和表演等動作。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;通義團隊基於通義萬相的通用視頻生成能力，融合了文本引導的全局運動控制和音頻驅動的細粒度局部運動，實現了複雜場景的音頻驅動視頻生成；引入 AdaIN 和 CrossAttention 兩種控制機制，實現了更準確更動態的音頻控制效果。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;生成時長上，Wan2.2-S2V 單次生成的視頻時長可達業界領先的分鐘級。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;Wan2.2-S2V 通過層次化幀壓縮技術，大幅降低了歷史幀的 Token 數量，通過該方式將 motion frames(歷史參考幀) 的長度從數幀拓展到 73 幀， 從而實現了穩定的長視頻生成效果。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;Wan2.2-S2V 還支持文本控制，輸入 Prompt 後還可對視頻畫面進行控制，實現鏡頭運動、角色軌跡和實體間互動，讓視頻主體的運動和背景的變化更豐富。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在模型訓練上，通義團隊構建了超 60 萬個片段的音視頻數據集，通過混合並行訓練進行全參數化訓練，充分挖掘了 Wan2.2-S2V 模型的性能。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;同時通過多分辨率訓練、支持模型多分辨率的推理，Wan2.2-S2V 可支持不同分辨率場景的視頻生成需求, 如豎屏短視頻、橫屏影視劇。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368660</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368660</guid>
      <pubDate>Tue, 19 Aug 2025 01:59:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>國務院：建立健全人工智能開源貢獻評價和激勵機制，鼓勵高校將開源貢獻納入學生學分認證和教師成果認定</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;國務院印發《關於深入實施「人工智能+」行動的意見》。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0826/191343_GXhC_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;其中提到，促進開源生態繁榮。支持人工智能開源社區建設，促進模型、工具、數據集等匯聚開放，培育優質開源項目。建立健全人工智能開源貢獻評價和激勵機制，鼓勵高校將開源貢獻納入學生學分認證和教師成果認定。支持企業、高校、科研機構等探索普惠高效的開源應用新模式。加快構建面向全球開放的開源技術體系和社區生態，發展具有國際影響力的開源項目和開發工具等。&lt;/p&gt; 
&lt;p&gt;原文：&lt;em&gt;https://www.gov.cn/zhengce/content/202508/content_7037861.htm&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368606</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368606</guid>
      <pubDate>Mon, 18 Aug 2025 11:14:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
  </channel>
</rss>
