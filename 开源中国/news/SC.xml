<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - news - 简体中文</title>
    <link>https://www.oschina.net/news/project</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news" rel="self" type="application/rss+xml"/>
    <description>已对该 RSS 进行格式化操作：中英字符之间插入空格、使用直角引号、标点符号修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-cn</language>
    <lastBuildDate>Thu, 24 Jul 2025 07:49:57 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>从频繁告警到平稳发布：服务冷启动 CPU 风暴优化实践</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;blockquote&gt; 
 &lt;p&gt;作者：vivo 互联网服务器团队- Xie Xiaopeng&lt;/p&gt; 
 &lt;p&gt;本文针对服务启动后几分钟内 CPU 持续处于高峰状态的问题，提出了自己的分析思路与解决方案。最终线上效果比较显著，成功解决了每次发版过程中频繁告警、业务受损以及用户体验不佳的问题，为服务的高可用性增添了一道重要保障。本文的重点在于问题的发现、分析及解决思路。对于 CPU 相关的问题，火焰图和 Arthas 是非常有效的工具，建议大家在遇到类似情况时，积极尝试使用这些工具进行排查和解决。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;1 分钟看图抓住核心观点👇&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//ac1252a6e0f78995afecc8da9af3fe0b.gif" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h1_1"&gt;&lt;/span&gt; 
&lt;h1&gt;一、背景&lt;/h1&gt; 
&lt;p&gt;最近我们的服务在发布或重启时频繁产生告警，这种情况从发版开始一直持续到发版结束后几分钟内，规律非常明显。&lt;/p&gt; 
&lt;p&gt;起初，我们怀疑是流量接入过快导致了此问题。在服务启动后，CICD 会检测 &lt;a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fcheck.do" rel="nofollow" target="_blank"&gt;check.do&lt;/a&gt; 接口，以确认服务是否准备就绪。我们推测，&lt;a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fcheck.do" rel="nofollow" target="_blank"&gt;check.do&lt;/a&gt; 接口成功返回后，CICD 立即接入线上流量，这才引发非常多的异常告警。&lt;/p&gt; 
&lt;p&gt;为了解决这一问题，我们与运维团队进行了沟通，决定将流量接入的时机延迟 30 秒。延迟 30 秒后问题还是没有得到解决，告警依然持续不断。&lt;/p&gt; 
&lt;span id="OSC_h1_2"&gt;&lt;/span&gt; 
&lt;h1&gt;二、 问题表象&lt;/h1&gt; 
&lt;p&gt;以线上某一台机器为例，它的启动步骤如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;2024-09-0416:09:50&amp;nbsp;INFO - 启动应用 ，执行成功。
2024-09-0416:12:36&amp;nbsp;WARN - 检查接口：check.do，响应结果：ok
2024-09-0416:13:07&amp;nbsp;INFO - 启动后等待时间（秒）：30
2024-09-0416:13:07&amp;nbsp;INFO - 恢复 Dubbo 流量成功
2024-09-0416:13:39&amp;nbsp;INFO - 恢复 HTTP 流量成功！
&lt;/code&gt;&lt;/pre&gt; 
&lt;span id="OSC_h2_3"&gt;&lt;/span&gt; 
&lt;h2&gt;2.1 Dubbo 接口超时严重&lt;/h2&gt; 
&lt;p&gt;恢复 HTTP 流量后，很多调用下游的 Dubbo 接口发生超时，以画像接口为例，告警开始时间为：2024-09-04 16:14:07.251，结束时间为：2024-09-04 16:17:31.224，期间超时请求数为：578。&lt;/p&gt; 
&lt;span id="OSC_h2_4"&gt;&lt;/span&gt; 
&lt;h2&gt;2.2 HTTP 接口超时严重&lt;/h2&gt; 
&lt;p&gt;大部分 HTTP 接口也超时严重，P95 响应时间从正常的几十毫秒飙升至几秒钟，16:17:30 后逐渐恢复至正常水平。&lt;/p&gt; 
&lt;span id="OSC_h2_5"&gt;&lt;/span&gt; 
&lt;h2&gt;2.3 CPU 异常&lt;/h2&gt; 
&lt;p&gt;服务发布前后 CPU 表现异常，启动过程 CPU 存在突刺，接入线上流量后一段时间内 CPU 使用率将近 100%，16:17:30 后逐步下降，恢复到正常水平。下图为服务发布前后 CPU 的使用率截图。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//e1cf61c214438c27647181f67523a5bc.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;服务发布前后 CPU 使用率&lt;/p&gt; 
&lt;span id="OSC_h2_6"&gt;&lt;/span&gt; 
&lt;h2&gt;2.4 Runnable、Blocked 线程突刺&lt;/h2&gt; 
&lt;p&gt;下图为线程数的相关监控指标，我们可以看到：在服务发布期间，活跃线程数持续增加，启动期间线程剧增，接入线上流量后线程逐步增加，16:17 分之后趋于平稳，其中 16:12:30-16:12:40 期间活跃线程数从 249 增加到 1026（启动期间业务侧有很多任务均会创建线程池，不影响本次分析）&lt;/p&gt; 
&lt;p&gt;Runnable 线程数与 Blocked 线程数也有突刺，时间是 16:13:30-16:17:30，与接入 HTTP 流量时间相符，与 CPU 突刺时间完全一致。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//0284bd6e471cdeab5825a6b1a4076566.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Runnable 线程&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//997e2e5b9b6042e8231d8768f6ac37b6.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Blocked 线程&lt;/p&gt; 
&lt;span id="OSC_h2_7"&gt;&lt;/span&gt; 
&lt;h2&gt;2.5 老年代上涨快&lt;/h2&gt; 
&lt;p&gt;在查看 GC 老年代内存使用情况时，我们发现启动后未接入流量时，老年代内存为 985.84MB。而在接入流量后，截止到 16:17:30，内存使用量已经上升至 1.36GB，期间老年代的内存增长速度较快。&lt;/p&gt; 
&lt;span id="OSC_h2_8"&gt;&lt;/span&gt; 
&lt;h2&gt;2.6 下游依赖正常&lt;/h2&gt; 
&lt;p&gt;从上游视角查看下游依赖的情况，随便挑一个 Dubbo 接口超时严重的下游依赖，我们查看一下服务的监控指标，发现服务的请求量在启动期间有突刺（业务侧在启动期间会主动发起调用，刷新一些缓存，正常现象），启动后流量几乎没变，但是成功率却有明显下降，且最大响应时间显著增加。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//c15914b39be50c378c9e269f81b7c872.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;上游视角&lt;/p&gt; 
&lt;p&gt;但是从下游视角再看服务相关指标，接口成功率正常，且最大响应时间也正常，说明不是下游服务的问题。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//3138ef8521191df0a25e0843279fb18d.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;下游视角&lt;/p&gt; 
&lt;span id="OSC_h1_9"&gt;&lt;/span&gt; 
&lt;h1&gt;三、原因初步判断&lt;/h1&gt; 
&lt;p&gt;从监控数据来看，在线上流量恢复后，我们的服务当前拥有的线程数不足以处理这些业务请求，因此导致系统大量创建业务线程。由于 CPU 的时间片调度策略，线程之间会频繁发生上下文切换，从而引发 CPU 负载的剧烈上升，甚至达到饱和状态。&lt;/p&gt; 
&lt;p&gt;因此通过初步分析，我们得到以下&lt;strong&gt;结论&lt;/strong&gt;：&lt;/p&gt; 
&lt;p&gt;引起 CPU 飙升的原因主要是由于过多的 Runnable 状态线程以及频繁的线程上下文切换所导致。我们观察到系统中存在大量已启动的线程，这些线程的状态在 Blocked（锁等待、IO 等待等）和 Runnable 之间不断变化。当锁竞争激烈时，CPU 飙升的现象就很容易出现。&lt;/p&gt; 
&lt;span id="OSC_h1_10"&gt;&lt;/span&gt; 
&lt;h1&gt;四、尝试初步解决&lt;/h1&gt; 
&lt;span id="OSC_h2_11"&gt;&lt;/span&gt; 
&lt;h2&gt;4.1 流量逐步灰度&lt;/h2&gt; 
&lt;p&gt;既然我们怀疑是流量全部接入后，线程不足导致的问题，因此需要尝试流量缓慢接入是否能解决这个问题。&lt;/p&gt; 
&lt;p&gt;我们与运维同学线上随机找了一台机器进行流量灰度实验，具体时间节点如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;2024-09-0509:55:21&amp;nbsp;启动成功
2024-09-0509:56:17&amp;nbsp;灰度 1%
2024-09-0509:57:19&amp;nbsp;灰度 5%
2024-09-0509:58:31&amp;nbsp;灰度 44%
2024-09-0510:03:51&amp;nbsp;开始操作全量
2024-09-0510:08:10&amp;nbsp;全量完成
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;再观察一下相关指标，我们发现各项指标均正常：CPU 使用率不再有突刺，Runnable 线程数和 Blocked 线程数也保持稳定，之前的负载尖峰现象已消失。同时异常超时的日志记录也不再出现，老年代内存的增长速度缓慢，HTTP 接口、Dubbo 接口 P95 响应时间正常。由此可见，流量灰度可以解决该问题。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;① CPU 使用率情况如下：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//45fcaa86e0dd82eed37f4645725774ae.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;CPU 使用率&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;② 线程指标情况如下：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//cd1e221cd3022fbb8e17ac382f336365.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Runnable 线程&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//75a065693cd049c3c8d50ca0f2807af2.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Blocked 线程&lt;/p&gt; 
&lt;span id="OSC_h2_12"&gt;&lt;/span&gt; 
&lt;h2&gt;4.2 缓存预热&lt;/h2&gt; 
&lt;p&gt;在前文中提到，接入线上流量后，老年代的内存增长较快，因此我们推测在服务启动初期，由于尚未加载相关的缓存数据，当大量请求涌入时，未命中缓存的情况频繁发生，这迫使系统不断向下游请求以加载缓存，从而导致接口响应变慢。为此，我们需要验证预热缓存的有效性，以确定是否能够改善这一问题。&lt;/p&gt; 
&lt;p&gt;缓存预热的&lt;strong&gt;主要作用和目的&lt;/strong&gt;如下：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;提高缓存命中率&lt;/strong&gt;：通过预先加载热点数据，能够显著提升缓存的命中率，从而减少对后端数据源（如数据库）的访问，降低系统负载。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;保持服务性能稳定&lt;/strong&gt;：在服务启动或缓存失效之后，缓存预热可以有效防止请求对后端数据源施加突发压力，从而确保服务性能的稳定性。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;优化用户体验&lt;/strong&gt;：由于热点数据已被预先加载到缓存中，用户在请求这些数据时能够获得更快的响应速度，从而显著提升用户体验。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;方案&lt;/strong&gt;如下：&lt;/p&gt; 
&lt;p&gt;我们将梳理本地缓存信息，根据访问量和缓存大小区分数据的重要程度，定期将重要的缓存信息刷新至 Redis 中。在服务启动后，未接入线上流量之前，我们将优先从 Redis 中进行数据的预加载。通过这一措施，确保系统在高流量环境下的稳定性和性能。&lt;/p&gt; 
&lt;p&gt;实验结果显示，增加缓存预热后，问题并未得到有效解决，表现差异微乎其微。&lt;/p&gt; 
&lt;p&gt;仅仅预热重要缓存无法解决当前问题。系统在启动时需要预热的内容相对较多，同时各类中间件也有自身的缓存需要预热。因此仅预热业务自定义的内存缓存，效果非常有限。&lt;/p&gt; 
&lt;p&gt;回顾之前的原因分析，我们仅仅关注了表面现象，如 CPU 的上涨和线程数的增加，而未深入挖掘问题的本质。我们需要探讨线程数为何上升、CPU 为何飙升，接下来将进行更深入的分析，以找出问题的根本原因。&lt;/p&gt; 
&lt;span id="OSC_h1_13"&gt;&lt;/span&gt; 
&lt;h1&gt;五、分析问题&lt;/h1&gt; 
&lt;span id="OSC_h2_14"&gt;&lt;/span&gt; 
&lt;h2&gt;5.1 初步分析堆栈与 CPU 火焰图&lt;/h2&gt; 
&lt;p&gt;我们选择了一台线上机器进行服务重启，并在接入线上流量后的几分钟内导出了程序的线程堆栈信息，进行分析：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Runnable 线程数显著增多，占比达到 29%，通常情况下，Runnable 线程数约为 70 个，而此时却激增至 462 个；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;进一步查看 Runnable 线程，发现大部分线程为 catalina-exec 线程（380 个），这是 Tomcat 用于执行 Spring MVC 应用中 Servlet 请求的线程。正常情况下，这类线程的数量仅有几个；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;在这些 Runnable 线程中，有 201 个线程均被阻塞在 org.springframework.beans.PropertyMatches.calculateStringDistance(PropertyMatches.java:170) 这个方法上，我们需要进一步分析其原因。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;看了堆栈信息后，应该有个疑问：为什么启动了这么多的 tomcat 线程？&lt;/p&gt; 
&lt;p&gt;我们推测原因在于服务刚启动时，系统尚未加载任何缓存，所有数据都需要进行首次加载。在这种情况下，服务无法快速响应用户请求，导致接口的响应时间（RT）显著上升。在相同的 QPS 的情况下，为了处理不断增加的业务请求，系统不得不创建更多的 Tomcat 线程。&lt;/p&gt; 
&lt;p&gt;接下来我们接入 Arthas 工具，采集 CPU 火焰图以进行深入分析，CPU 火焰图如下：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//aa098319277f41bc34e91ca21c250e33.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;异常 CPU 火焰图&lt;/p&gt; 
&lt;p&gt;分析结果显示，CPU 耗时主要集中在 calculateStringDistance 方法，这与我们之前的线程堆栈分析结果一致。在服务启动时的 CPU 火焰图中，calculateStringDistance 方法的 CPU 消耗占比高达 16.68% + 39.09% + 8.38% = 64.15%，整体 CPU 使用率接近 97%。&lt;/p&gt; 
&lt;p&gt;经过一段时间的运行后，再观察正常情况下的 CPU 火焰图，calculateStringDistance 方法的 CPU 消耗占比降至 3.39% + 8.57% + 1.78% = 13.74%，整体 CPU 使用率则徘徊在 25% 至 42% 之间。&lt;/p&gt; 
&lt;p&gt;这一变化表明，随着系统的稳定运行，CPU 负载逐渐得到缓解，但 calculateStringDistance 方法仍然是性能瓶颈之一。它虽然不是 CPU 使用率飙升的根因，但它在服务启动后进一步加剧了 CPU 的负载。&lt;/p&gt; 
&lt;span id="OSC_h3_15"&gt;&lt;/span&gt; 
&lt;h3&gt;5.1.1 calculateStringDistance 加剧 CPU 暴涨&lt;/h3&gt; 
&lt;p&gt;在相同 QPS 的情况下，为什么在服务启动后的几分钟内 calculateStringDistance 方法消耗的 CPU 资源严重，而经过一段时间后，这一消耗又有所减小？&lt;/p&gt; 
&lt;p&gt;前文的分析指出，服务刚启动时，流量瞬间恢复，导致系统需要创建大量的业务线程。这些线程在处理请求时，都会执行 calculateStringDistance 方法。由于该方法本身的计算开销较大，且并发执行的线程数量越多，CPU 的消耗就会越显著。因此在服务启动初期，CPU 的负载急剧上升。随着运行时间的延长，业务线程的创建和执行也趋于平衡，并发执行的线程数量大大减小，CPU 消耗也随之减小。&lt;/p&gt; 
&lt;span id="OSC_h3_16"&gt;&lt;/span&gt; 
&lt;h3&gt;5.1.2 calculateStringDistance 源码分析&lt;/h3&gt; 
&lt;p&gt;calculateStringDistance 方法的功能是根据 Levenshtein 算法计算给定两个字符串之间的距离或相似度。通过分析其源代码，我们可以发现，在比较两个字符串时，该方法采用了嵌套的 for 循环结构。在这些循环中，涉及到 length、chatAt 和 Math.min 函数的调用，这使得该方法的计算复杂度相对较高。调用量越大，CPU 消耗就会越严重。根据 CPU 火焰图的分析，发现这三个函数的 CPU 消耗占比与 calculateStringDistance 方法的 CPU 消耗占比之间的比例高达 78%。因此在调用该方法时要小心，在高并发场景下，该方法很有可能成为系统的性能瓶颈，对 CPU 产生影响。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;private&amp;nbsp;static&amp;nbsp;int&amp;nbsp;calculateStringDistance(String s1, String s2){
&amp;nbsp;&amp;nbsp;if&amp;nbsp;(s1.isEmpty()) {
&amp;nbsp; &amp;nbsp;&amp;nbsp;return&amp;nbsp;s2.length();
&amp;nbsp; }
&amp;nbsp;&amp;nbsp;if&amp;nbsp;(s2.isEmpty()) {
&amp;nbsp; &amp;nbsp;&amp;nbsp;return&amp;nbsp;s1.length();
&amp;nbsp; }

&amp;nbsp;&amp;nbsp;int[][] d = newint[s1.length() +&amp;nbsp;1][s2.length() +&amp;nbsp;1];
&amp;nbsp;&amp;nbsp;for&amp;nbsp;(int&amp;nbsp;i =&amp;nbsp;0; i &amp;lt;= s1.length(); i++) {
&amp;nbsp; &amp;nbsp; d[i][0] = i;
&amp;nbsp; }
&amp;nbsp;&amp;nbsp;for&amp;nbsp;(int&amp;nbsp;j =&amp;nbsp;0; j &amp;lt;= s2.length(); j++) {
&amp;nbsp; &amp;nbsp; d[0][j] = j;
&amp;nbsp; }

&amp;nbsp;&amp;nbsp;for&amp;nbsp;(int&amp;nbsp;i =&amp;nbsp;1; i &amp;lt;= s1.length(); i++) {
&amp;nbsp; &amp;nbsp;&amp;nbsp;char&amp;nbsp;c1 = s1.charAt(i -&amp;nbsp;1);
&amp;nbsp; &amp;nbsp;&amp;nbsp;for&amp;nbsp;(int&amp;nbsp;j =&amp;nbsp;1; j &amp;lt;= s2.length(); j++) {
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;int&amp;nbsp;cost;
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;char&amp;nbsp;c2 = s2.charAt(j -&amp;nbsp;1);
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(c1 == c2) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; cost =&amp;nbsp;0;
&amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;else&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; cost =&amp;nbsp;1;
&amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp; d[i][j] = Math.min(Math.min(d[i -&amp;nbsp;1][j] +&amp;nbsp;1, d[i][j -&amp;nbsp;1] +&amp;nbsp;1), d[i -&amp;nbsp;1][j -&amp;nbsp;1] + cost);
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; }

&amp;nbsp;&amp;nbsp;return&amp;nbsp;d[s1.length()][s2.length()];
}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;calculateStringDistance 方法是如何触发的？通过查询堆栈信息并查看源代码，我们发现这是 Spring 框架在解析请求参数并注入属性的过程中所触发的。堆栈信息如下，从上到下逐步分析堆栈，我们重点分析 setPropertyValues 和 createNotWritable-&lt;/p&gt; 
&lt;p&gt;PropertyException 这两个方法。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;"catalina-exec-485"&amp;nbsp;#975 daemon prio=5 os_prio=0 tid=0x00007f50e825f000 nid=0x3375 runnable [0x00007f5043ea4000]
&amp;nbsp; &amp;nbsp;java.lang.Thread.State: RUNNABLE
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.beans.PropertyMatches.calculateStringDistance(PropertyMatches.java:170)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.beans.PropertyMatches.access$100(PropertyMatches.java:44)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.beans.PropertyMatches$BeanPropertyMatches.calculateMatches(PropertyMatches.java:208)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.beans.PropertyMatches$BeanPropertyMatches.&amp;lt;init&amp;gt;(PropertyMatches.java:193)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.beans.PropertyMatches.forProperty(PropertyMatches.java:68)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.beans.PropertyMatches.forProperty(PropertyMatches.java:58)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.beans.BeanWrapperImpl.createNotWritablePropertyException(BeanWrapperImpl.java:237)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.beans.AbstractNestablePropertyAccessor.processLocalProperty(AbstractNestablePropertyAccessor.java:435)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.beans.AbstractNestablePropertyAccessor.setPropertyValue(AbstractNestablePropertyAccessor.java:290)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.beans.AbstractNestablePropertyAccessor.setPropertyValue(AbstractNestablePropertyAccessor.java:278)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.beans.AbstractPropertyAccessor.setPropertyValues(AbstractPropertyAccessor.java:95)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.validation.DataBinder.applyPropertyValues(DataBinder.java:860)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.validation.DataBinder.doBind(DataBinder.java:756)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.bind.WebDataBinder.doBind(WebDataBinder.java:192)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.bind.ServletRequestDataBinder.bind(ServletRequestDataBinder.java:106)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.servlet.mvc.method.annotation.ServletModelAttributeMethodProcessor.bindRequestParameters(ServletModelAttributeMethodProcessor.java:152)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.method.annotation.ModelAttributeMethodProcessor.resolveArgument(ModelAttributeMethodProcessor.java:111)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.method.support.HandlerMethodArgumentResolverComposite.resolveArgument(HandlerMethodArgumentResolverComposite.java:121)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.method.support.InvocableHandlerMethod.getMethodArgumentValues(InvocableHandlerMethod.java:158)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest$original$3Q7HrFjh(InvocableHandlerMethod.java:128)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest$original$3Q7HrFjh$accessor$ykGmQRZT(InvocableHandlerMethod.java)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.method.support.InvocableHandlerMethod$auxiliary$Wny4v5BZ.call(Unknown Source)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:97)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:849)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:760)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:967)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:901)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:872)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at javax.servlet.http.HttpServlet.service(HttpServlet.java:650)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at javax.servlet.http.HttpServlet.service(HttpServlet.java:731)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:303)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:347)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:263)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:347)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:263)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:197)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.StandardWrapperValve.invoke$original$Y7IhKDGv(StandardWrapperValve.java:219)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.StandardWrapperValve.invoke$original$Y7IhKDGv$accessor$4IDmuys6(StandardWrapperValve.java)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.StandardWrapperValve$auxiliary$1SL1DIkO.call(Unknown Source)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:110)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:494)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:169)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:104)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:445)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1136)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:637)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1775)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1734)
&amp;nbsp; &amp;nbsp; &amp;nbsp; - locked &amp;lt;0x000000070f1dc100&amp;gt; (a org.apache.tomcat.util.net.NioChannel)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at java.lang.Thread.run(Thread.java:748)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;先分析 setPropertyValues 方法，该方法负责将请求中的参数映射到目标对象的属性上，主要是遍历属性列表进行赋值并进行异常统一处理，单个属性的注入继续看 setPropertyValue 方法。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;public&amp;nbsp;void&amp;nbsp;setPropertyValues(PropertyValues pvs,&amp;nbsp;boolean&amp;nbsp;ignoreUnknown,&amp;nbsp;boolean&amp;nbsp;ignoreInvalid)
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;throws&amp;nbsp;BeansException {
&amp;nbsp; &amp;nbsp;&amp;nbsp;// 声明 PropertyAccessException 集合，保存单个属性注入时抛出的 PropertyAccessException 异常
&amp;nbsp; List&amp;lt;PropertyAccessException&amp;gt; propertyAccessExceptions =&amp;nbsp;null;
&amp;nbsp; &amp;nbsp;&amp;nbsp;// 获取属性列表
&amp;nbsp; List&amp;lt;PropertyValue&amp;gt; propertyValues = (pvs&amp;nbsp;instanceof&amp;nbsp;MutablePropertyValues ?
&amp;nbsp; &amp;nbsp; &amp;nbsp; ((MutablePropertyValues) pvs).getPropertyValueList() : Arrays.asList(pvs.getPropertyValues()));
&amp;nbsp;&amp;nbsp;for&amp;nbsp;(PropertyValue pv : propertyValues) {
&amp;nbsp; &amp;nbsp;&amp;nbsp;try&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;// 单个属性的注入，注意：此方法可能会引发任意的 BeansException，如果存在严重故障（例如没有匹配的字段），则不会在此处捕获该异常。我们可以尝试只处理不太严重的异常。
&amp;nbsp; &amp;nbsp; &amp;nbsp; setPropertyValue(pv);
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp;&amp;nbsp;catch&amp;nbsp;(NotWritablePropertyException ex) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;// 默认是 true，忽略未知属性，因此不会抛异常
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(!ignoreUnknown) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;throw&amp;nbsp;ex;
&amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;// Otherwise, just ignore it and continue...
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp;&amp;nbsp;catch&amp;nbsp;(NullValueInNestedPathException ex) {
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(!ignoreInvalid) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;throw&amp;nbsp;ex;
&amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;// Otherwise, just ignore it and continue...
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp;&amp;nbsp;catch&amp;nbsp;(PropertyAccessException ex) {
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(propertyAccessExceptions ==&amp;nbsp;null) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; propertyAccessExceptions =&amp;nbsp;new&amp;nbsp;LinkedList&amp;lt;PropertyAccessException&amp;gt;();
&amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp; propertyAccessExceptions.add(ex);
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; }

&amp;nbsp;&amp;nbsp;// 如果 propertyAccessExceptions 不为空，需要整合起来，抛一个复合异常 PropertyBatchUpdateException
&amp;nbsp;&amp;nbsp;if&amp;nbsp;(propertyAccessExceptions !=&amp;nbsp;null) {
&amp;nbsp; &amp;nbsp; PropertyAccessException[] paeArray =
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; propertyAccessExceptions.toArray(new&amp;nbsp;PropertyAccessException[propertyAccessExceptions.size()]);
&amp;nbsp; &amp;nbsp; thrownew&amp;nbsp;PropertyBatchUpdateException(paeArray);
&amp;nbsp; }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;propertyValues 属性的结构如下，它包含了从上游传递过来的所有参数。这些参数被封装成一个集合，便于后续的处理和注入。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//cc70849606402c94a225c47243073954.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;propertyValues 属性&lt;/p&gt; 
&lt;p&gt;分析 setPropertyValue 方法，该方法主要作用是解析属性值，如果存在嵌套属性，则递归解析设置最终对应的属性值，方法最后都会调用 setPropertyValue(tokens, pv) 方法。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;public&amp;nbsp;void&amp;nbsp;setPropertyValue(PropertyValue pv)&amp;nbsp;throws&amp;nbsp;BeansException {
&amp;nbsp;&amp;nbsp;PropertyTokenHolder&amp;nbsp;tokens&amp;nbsp;=&amp;nbsp;(PropertyTokenHolder) pv.resolvedTokens;
&amp;nbsp;&amp;nbsp;if&amp;nbsp;(tokens ==&amp;nbsp;null) {
&amp;nbsp; &amp;nbsp;&amp;nbsp;String&amp;nbsp;propertyName&amp;nbsp;=&amp;nbsp;pv.getName();
&amp;nbsp; &amp;nbsp; AbstractNestablePropertyAccessor nestedPa;
&amp;nbsp; &amp;nbsp;&amp;nbsp;try&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;// 确定给定属性路径中的第一个嵌套属性分隔符，忽略键中的点（如 「map[my.key]」）。
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;// 当配置的属性名 propertyName 中包含'.'这样字符时，代表需要设置嵌套属性
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;// 如果存在嵌套属性，Spring 会递归向下获取最终设置的属性，比如：a.b.c，Spring 会递归调用获取到 b，c 是需要设置的属性
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;// 如果没有嵌套属性的话。会返回自身
&amp;nbsp; &amp;nbsp; &amp;nbsp; nestedPa = getPropertyAccessorForPropertyPath(propertyName);
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp;&amp;nbsp;catch&amp;nbsp;(NotReadablePropertyException ex) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; thrownew&amp;nbsp;NotWritablePropertyException(getRootClass(),&amp;nbsp;this.nestedPath + propertyName,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"Nested property in path '"&amp;nbsp;+ propertyName +&amp;nbsp;"' does not exist", ex);
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;// 将给定的属性名称解析为相应的属性名称令牌，如果没有[]，则 tokens 中的 keys 为空，且 actualName、canonicalName 都等于 propertyName&amp;nbsp;
&amp;nbsp; &amp;nbsp; tokens = getPropertyNameTokens(getFinalPath(nestedPa, propertyName));
&amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(nestedPa ==&amp;nbsp;this) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; pv.getOriginalPropertyValue().resolvedTokens = tokens;
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;// 设置属性
&amp;nbsp; &amp;nbsp; nestedPa.setPropertyValue(tokens, pv);
&amp;nbsp; }
&amp;nbsp;&amp;nbsp;else&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;// 设置属性
&amp;nbsp; &amp;nbsp; setPropertyValue(tokens, pv);
&amp;nbsp; }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;分析 setPropertyValue(tokens, pv) 方法，该方法是用来区分数组类型跟非数组类型的，大部分属性都是非数组类型，我们分析非数组类型方法。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;protected&amp;nbsp;void&amp;nbsp;setPropertyValue(PropertyTokenHolder tokens, PropertyValue pv)&amp;nbsp;throws&amp;nbsp;BeansException {
&amp;nbsp; &amp;nbsp;&amp;nbsp;// 如果属性中存在[]，说明是数组，则进入该方法
&amp;nbsp;&amp;nbsp;if&amp;nbsp;(tokens.keys !=&amp;nbsp;null) {
&amp;nbsp; &amp;nbsp; processKeyedProperty(tokens, pv);
&amp;nbsp; }
&amp;nbsp;&amp;nbsp;else&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;// 大部分都走这个方法
&amp;nbsp; &amp;nbsp; processLocalProperty(tokens, pv);
&amp;nbsp; }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;processLocalProperty 方法的作用就是获取属性值，利用反射完成属性注入。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;private&amp;nbsp;void&amp;nbsp;processLocalProperty(PropertyTokenHolder tokens, PropertyValue pv){
&amp;nbsp; &amp;nbsp;&amp;nbsp;// 获取属性对应的 PropertyHandler
&amp;nbsp;&amp;nbsp;PropertyHandler&amp;nbsp;ph&amp;nbsp;=&amp;nbsp;getLocalPropertyHandler(tokens.actualName);
&amp;nbsp; &amp;nbsp;&amp;nbsp;// 如果不存在对应的 handler 或者，属性是不可写的（没有 setter 方法）
&amp;nbsp;&amp;nbsp;if&amp;nbsp;(ph ==&amp;nbsp;null&amp;nbsp;|| !ph.isWritable()) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;// 如果属性是 optional 类型，则直接返回
&amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(pv.isOptional()) {
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(logger.isDebugEnabled()) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; logger.debug("Ignoring optional value for property '"&amp;nbsp;+ tokens.actualName +
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"' - property not found on bean class ["&amp;nbsp;+ getRootClass().getName() +&amp;nbsp;"]");
&amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;return;
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp;&amp;nbsp;else&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;// 其他情况则抛出不可写属性异常，占用 CPU 较多的方法由此进入
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;throw&amp;nbsp;createNotWritablePropertyException(tokens.canonicalName);
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; }

&amp;nbsp;&amp;nbsp;Object&amp;nbsp;oldValue&amp;nbsp;=&amp;nbsp;null;
&amp;nbsp;&amp;nbsp;try&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;// 获取属性值
&amp;nbsp; &amp;nbsp;&amp;nbsp;Object&amp;nbsp;originalValue&amp;nbsp;=&amp;nbsp;pv.getValue();
&amp;nbsp; &amp;nbsp;&amp;nbsp;Object&amp;nbsp;valueToApply&amp;nbsp;=&amp;nbsp;originalValue;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;// 如果需要转换，则进入此分支
&amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(!Boolean.FALSE.equals(pv.conversionNecessary)) {
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;// 如果已经完成类型转换，则直接使用
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(pv.isConverted()) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; valueToApply = pv.getConvertedValue();
&amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;else&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;// 如果需要读取旧值，默认是 false &amp;amp;&amp;amp; 值可读（有 getter 方法）
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(isExtractOldValueForEditor() &amp;amp;&amp;amp; ph.isReadable()) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;try&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; oldValue = ph.getValue();
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;catch&amp;nbsp;(Exception ex) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(ex&amp;nbsp;instanceof&amp;nbsp;PrivilegedActionException) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; ex = ((PrivilegedActionException) ex).getException();
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(logger.isDebugEnabled()) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; logger.debug("Could not read previous value of property '"&amp;nbsp;+
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;this.nestedPath + tokens.canonicalName +&amp;nbsp;"'", ex);
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;// 类型转换
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; valueToApply = convertForProperty(
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; tokens.canonicalName, oldValue, originalValue, ph.toTypeDescriptor());
&amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp; pv.getOriginalPropertyValue().conversionNecessary = (valueToApply != originalValue);
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp;&amp;nbsp;// 完成属性注入
&amp;nbsp; &amp;nbsp; ph.setValue(this.wrappedObject, valueToApply);
&amp;nbsp; }
&amp;nbsp;&amp;nbsp;catch&amp;nbsp;(TypeMismatchException ex) {
&amp;nbsp; &amp;nbsp;&amp;nbsp;throw&amp;nbsp;ex;
&amp;nbsp; }
&amp;nbsp;&amp;nbsp;catch&amp;nbsp;(InvocationTargetException ex) {
&amp;nbsp; &amp;nbsp;&amp;nbsp;PropertyChangeEvent&amp;nbsp;propertyChangeEvent&amp;nbsp;=&amp;nbsp;new&amp;nbsp;PropertyChangeEvent(
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;this.rootObject,&amp;nbsp;this.nestedPath + tokens.canonicalName, oldValue, pv.getValue());
&amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(ex.getTargetException()&amp;nbsp;instanceof&amp;nbsp;ClassCastException) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; thrownew&amp;nbsp;TypeMismatchException(propertyChangeEvent, ph.getPropertyType(), ex.getTargetException());
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp;&amp;nbsp;else&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;Throwable&amp;nbsp;cause&amp;nbsp;=&amp;nbsp;ex.getTargetException();
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(cause&amp;nbsp;instanceof&amp;nbsp;UndeclaredThrowableException) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;// May happen e.g. with Groovy-generated methods
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; cause = cause.getCause();
&amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp; thrownew&amp;nbsp;MethodInvocationException(propertyChangeEvent, cause);
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; }
&amp;nbsp;&amp;nbsp;catch&amp;nbsp;(Exception ex) {
&amp;nbsp; &amp;nbsp;&amp;nbsp;PropertyChangeEvent&amp;nbsp;pce&amp;nbsp;=&amp;nbsp;new&amp;nbsp;PropertyChangeEvent(
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;this.rootObject,&amp;nbsp;this.nestedPath + tokens.canonicalName, oldValue, pv.getValue());
&amp;nbsp; &amp;nbsp; thrownew&amp;nbsp;MethodInvocationException(pce, ex);
&amp;nbsp; }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;在该方法中，我们注意到堆栈信息中 createNotWritablePropertyException 方法的调用。实际上 calculateStringDistance 方法的高 CPU 消耗正是由此引发的。当抛出不可写属性异常时，系统会计算字符串的相似度，主要目的是为了向用户提供更友好的提示，帮助他们识别哪些属性与当前属性相似，从而判断是否在传递参数时出现了错误。&lt;/p&gt; 
&lt;p&gt;Spring 这种设计不仅提升了用户体验，还降低了因参数错误而导致的调试难度。通过提供相似属性的建议，用户能够更快速地发现并纠正输入错误，确保请求的正确性。以下为调试过程中的部分提示：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Bean property&amp;nbsp;'questionValidatorInterface'&amp;nbsp;is not writable or has an invalid&amp;nbsp;setter&amp;nbsp;method. Does the parameter type of the&amp;nbsp;setter&amp;nbsp;match the&amp;nbsp;return&amp;nbsp;type of the&amp;nbsp;getter?
bean property&amp;nbsp;'users'&amp;nbsp;is not writable or has an invalid&amp;nbsp;setter&amp;nbsp;method. did you mean&amp;nbsp;'user'?
&lt;/code&gt;&lt;/pre&gt; 
&lt;span id="OSC_h3_17"&gt;&lt;/span&gt; 
&lt;h3&gt;5.1.3 calculateStringDistance 流程总结&lt;/h3&gt; 
&lt;p&gt;结合 Spring MVC 解析 HTTP 的请求流程，calculateStringDistance 方法的进入流程如下：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//c5a198998b410036c75859840afa1f60.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;解析参数流程&lt;/p&gt; 
&lt;p&gt;Spring MVC 在解析 HTTP 请求参数时会找到对应的参数解析器，因为我们的项目中大部分都是自定义的复杂对象，因此采用的参数解析器为 ServletModelAttributeMethodProcessor。该解析器在数据绑定过程中，会循环遍历每个参数，通过反射完成属性注入。但是我们自定义的复杂对象在某些接口下，定义的属性不合理，导致抛出 createNotWritablePropertyException 异常。&lt;/p&gt; 
&lt;p&gt;我们深入分析一下源码，看看怎样避免抛出 createNotWritablePropertyException 异常。&lt;/p&gt; 
&lt;p&gt;根据源码，我们发现抛出不可写属性异常的条件是（属性不存在对应的 handler 或者，属性不可写）并且属性不是 optional 类型，只要我们保证不满足这个条件，那么就可以有效避免抛出该异常。&lt;/p&gt; 
&lt;p&gt;说明一下这三个条件：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;属性不存在对应的 handler 即 request 中不存在该属性。比如请求参数中带 version 字段，但是服务端在接受 request 中并未定义 version 字段，那么此处 ph == null 判断条件就成立&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;属性不可写，即属性没有对应的 setter 方法&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;属性是 optional 类型，即属性的数据类型是 Optional 类型&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;通过查看业务侧的代码，我们发现请求（request）中的所有属性都已经定义了相应的 setter 方法，而且不存在 optional 类型的属性。因此我们只需要关注请求中是否存在未定义的属性。&lt;/p&gt; 
&lt;span id="OSC_h3_18"&gt;&lt;/span&gt; 
&lt;h3&gt;5.1.4 排查大流量及核心接口参数&lt;/h3&gt; 
&lt;p&gt;由于服务提供的接口非常多，因此仅排查流量较高和核心的接口。经过分析，我们发现几乎所有接口都存在未定义的属性。&lt;/p&gt; 
&lt;p&gt;这主要是因为客户端很多参数都是公参，在传参时会将这些公参全部透传给服务端，但是服务端并不需要处理所有的参数，因此没有在 request 中定义。特别备注：接口若未定义请求参数接收，则不会走上述流程。&lt;/p&gt; 
&lt;span id="OSC_h3_19"&gt;&lt;/span&gt; 
&lt;h3&gt;5.1.5 解决方案&lt;/h3&gt; 
&lt;p&gt;既然已经明确问题的根源是请求中存在未定义的属性，那么接下来我们将针对这一情况进行优化。方案主要有两个：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;在底层请求中加入客户端公参：对所有公参进行接收，确保它们能够被正确处理。需要注意的是，参数接收将会涉及属性注入，而属性注入是通过反射机制实现的。这一过程可能对 CPU 和接口性能产生影响，因此我们也需要进行实验，以评估这些参数解析的实际效果。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;在 filter 层针对接口去除相关字段：通过在过滤器层面过滤掉不必要的字段，避免接口中出现未定义的属性。&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;最终我们混合两种方案：对于大部分公共参数，定义到底层 request 中；对于非公共参数，针对接口进行移除。&lt;/p&gt; 
&lt;p&gt;我们针对大流量接口及核心接口进行了优化，优化后效果如下：&lt;/p&gt; 
&lt;p&gt;结论：整体效果显著，但仍存在一些不足之处。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;CPU 使用情况&lt;/strong&gt;：在高峰期重启应用时，CPU 的突发情况明显减弱，持续时间从 5 分钟缩短至 1 分钟。同时 CPU 和 Runnable 线程数仍会出现小幅波动，但 Runnable 线程数的波动持续时间已从 6 分钟缩减至 40 秒，波动峰值也由 600 降低至 280。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;接口性能&lt;/strong&gt;：接口的 P95 和 P99 耗时均有所降低，其中 P95 峰值从 53 秒降至 3.4 秒，P99 峰值从 1 分 50 秒降至 50 秒。此外，响应时间较长的时间段也得到了缩短，持续时间从 7 分钟减少到不到 2 分钟。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;发版及日常运行&lt;/strong&gt;：在发版期间及日常运行中，CPU 峰值普遍降低。与前 1 天和前 7 天的平均 CPU 使用率相比，最大和最小使用率均有所下降，幅度明显。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;① 启动后 CPU 使用率情况如下：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//4912336ab2d7bdc8083d05cf3d17cd1a.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;CPU 使用率&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;② 线程数情况如下：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//91e04839dc6184825966b004e62ffa91.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Runnable 线程数&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//f1879456a68bab5f04dd6cefc79aaa5a.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Blocked 线程数&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;③ 接口响应时间情况如下：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//9de3e1b9a261f06dbfd86fd6a64c5ab9.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;接口响应时间&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;④ 运行一段时间后，CPU 使用率情况如下：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//c3f85b69c1dd77a5861070bdd0f90c38.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;CPU 使用率&lt;/p&gt; 
&lt;span id="OSC_h2_20"&gt;&lt;/span&gt; 
&lt;h2&gt;5.2 优化后再次分析 CPU 火焰图&lt;/h2&gt; 
&lt;p&gt;优化后效果虽然好了很多，但是 CPU 和 Runnable 线程数仍会出现小幅波动，接口的响应时间在 1 分钟内仍有上涨。这是我们接下来要继续优化的目标。&lt;/p&gt; 
&lt;span id="OSC_h3_21"&gt;&lt;/span&gt; 
&lt;h3&gt;5.2.1 编译阶段消耗 CPU 占比高&lt;/h3&gt; 
&lt;p&gt;再次使用 arthas 进行监测，查看正常情况与启动后（异常情况）的 CPU 消耗情况，我们可以观察到：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;runWoker 部分：&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;该部分的 CPU 占用比例正常，与平时的表现一致，未见异常波动。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;编译相关的 CPU 占用：&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;CompileBroker::invoke_compiler_on_method(CompileTask*) 占用 CPU 较大，特别是 C2Compiler::compile_method(ciEnv*, ciMethod*, int) 的占比显著&lt;/p&gt; 
&lt;p&gt;由此我们得出结论：编译阶段的 CPU 消耗占比异常，可能是导致 CPU 负载突刺的重要因素。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;① 异常情况下 CPU 火焰图：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//358b54cf05ca85c707e7a9b636e4ba11.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;异常 CPU 火焰图&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;② 正常情况下 CPU 火焰图：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//8736bb5cd5d70f06959491e2d9729028.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;正常 CPU 火焰图&lt;/p&gt; 
&lt;span id="OSC_h3_22"&gt;&lt;/span&gt; 
&lt;h3&gt;5.2.2 用 arthas 换个角度验证&lt;/h3&gt; 
&lt;p&gt;CPU 火焰图是基于启动后 3 分钟内的综合数据采集而生成的，虽然能够提供整体的 CPU 使用情况，但无法反映 CPU 的实时变化。因此，为了更准确地验证编译阶段是否确实消耗了 CPU，我们需要登录到机器上，使用 Arthas 进行实时监测。&lt;/p&gt; 
&lt;p&gt;机器启动后，运行 dashboard 命令，重点关注屏幕上方的进程信息，以识别哪些线程占据了较高的 CPU 资源，以下为其中一次波动的截图，前几次波动 CPU 占比都差不多：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//e26f168e8937e370278c49578b95b790.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;dashboard 命令&lt;/p&gt; 
&lt;p&gt;从图中可以看到， CompilerThread 的三个线程占用了较高的 CPU 资源，尤其是 C2 CompilerThread 的占比明显，这与之前通过火焰图所反映的情况一致。&lt;/p&gt; 
&lt;span id="OSC_h3_23"&gt;&lt;/span&gt; 
&lt;h3&gt;5.2.3 CompilerThread 是什么&lt;/h3&gt; 
&lt;p&gt;C1 C2 CompilerThread 是 Java HotSpot 虚拟机中的两个即时编译器，主要作用是将 Java 字节码在运行时编译成本地机器码，以提高程序的执行效率。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;C1 Compiler（也称为客户端编译器），主要用于快速编译，优化较少，适合需要快速启动的应用。它的编译速度较快，但生成的机器码执行效率相对较低。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;C2 Compiler（也称为服务端编译器），主要用于高性能的编译，优化程度较高，适合长时间运行的应用。C2 编译器会花费更多时间进行优化，以生成更高效的机器码，适合对性能要求较高的场景。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;在 HotSpot 虚拟机中，Java 程序最初都是通过解释器（Interpreter）进行解释执行的，解释器的优点是启动快，省去编译的时间，能够快速运行代码。但随着程序的执行，某些方法或代码块可能会被多次调用，这些被频繁调用的代码被称为「热点代码」（Hot Spot Code）。当虚拟机识别到热点代码时，它会启动 JIT 编译器（C1 或 C2）将这些代码编译成本地机器码，以提高执行效率。&lt;/p&gt; 
&lt;p&gt;HotSpot 虚拟机是解释器与即时编译器并存的架构，两者经常是相辅相成地配合工作。由于即时编译器编译本地代码需要占用程序运行时间，编译出优化程度更高的代码所需的时间也会相应增加。此外为了实现更高的优化，解释器需要为编译器收集性能监控信息，这在一定程度上也会影响解释执行阶段的速度。为了解决这一问题，并在程序启动响应速度与运行效率之间达到最佳平衡，HotSpot 虚拟机在其编译子系统中引入了分层编译的功能。通过这一机制，HotSpot 能够根据代码的执行频率和性能需求，逐步将字节码编译为本地机器码，从而在保证快速启动的同时，优化长时间运行的代码性能。&lt;/p&gt; 
&lt;span id="OSC_h3_24"&gt;&lt;/span&gt; 
&lt;h3&gt;5.2.4 解决方案&lt;/h3&gt; 
&lt;p&gt;截止到现在，问题的原因就变得十分清晰了：当流量涌入时，HotSpot 虚拟机启动了分层编译机制，期间大部分代码迅速转变为热点代码。在这个过程中，C2 编译器需要频繁占用 CPU 资源进行编译，导致 CPU 使用率显著上升。随着大部分热点代码的优化完成，C2 编译器对 CPU 的占用将逐渐减少，CPU 使用率也会随之下降。这一编译过程的持续时间与监控图上的 CPU 波动情况高度一致。&lt;/p&gt; 
&lt;p&gt;C1 和 C2 编译器虽然提供了关闭的参数选项，但关闭这些编译器无疑会对服务的运行性能产生负面影响。网络上也有相关实验案例表明，对于需要长期运行的 Java 后端服务，禁用这些编译器将导致性能显著下降。因此这种关闭编译器的方式并不值得尝试。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;解决方案一：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在前文中，我们已经验证了流量逐步放量对机器的影响：采用灰度发布，对机器几乎没什么影响，各项指标表现都很平稳。由于历史原因，我们的服务当前无法支持灰度发布，因此还需要探索其他有效的解决方案。&lt;/p&gt; 
&lt;p&gt;我们可以换个角度思考：是否可以通过降低接口的请求 QPS，并将发版时间固定在每天流量最低的时段，以观察对服务启动的影响。&lt;/p&gt; 
&lt;p&gt;首先，我们可以优先关注大流量接口，并尝试减少这些接口的 QPS。通过优化接口请求的频率，我们或许能够在发版过程中减轻对系统的压力。&lt;/p&gt; 
&lt;p&gt;降低接口 QPS，调整重启服务的时间（非高峰期），12:13:59 恢复流量成功，实验效果如下：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;恢复流量后 CPU 最高峰值为 61.5%（依旧有小突刺，但是对业务无影响）&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Runnable、Blocked 线程数不再有突刺&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;接口响应时间（RT）也比较平稳&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;日志不再告警，无 error 错误日志&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;① CPU 使用率情况如下：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//626e46d5af3e6130174792c68bea26c6.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;CPU 使用率&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;② 线程指标如下：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//c4b821d566b0c09cb9848e68b198ad1b.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Runnable 线程数&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//a8f80feebc04db763c19655bede37681.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Blocked 线程数&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;③ 接口响应时间情况如下：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//41d2850bb678d230ac1c1add16b70335.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;接口响应时间&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;解决方案二：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;方案一虽然能够在一定程度上缓解问题，但治标不治本。此外我们也无法固定发版时间，因此最有效的策略是进行预热。与前文不同的点在于，此处是 JVM 预热。&lt;/p&gt; 
&lt;p&gt;方案为：在系统成功启动（监测 check.do 返回成功）后，接入线上 HTTP 流量之前，针对大流量接口及核心接口进行 HTTP 接口调用，频控次数为配置项，方便线上动态调整。特别注意：在刚启动时，如果机器或下游依赖出现故障，此处的额外调用会加剧系统或下游的负担，因此调用次数需要合理配置。&lt;/p&gt; 
&lt;p&gt;此方式可以让 C2 编译器提前对热点代码进行优化，在系统在系统稳定后再将流量接入生产环境，从而避免对用户造成任何影响。&lt;/p&gt; 
&lt;p&gt;观察启动后的各项指标，14:56:25 恢复 HTTP 流量成功，实验效果如下：&lt;/p&gt; 
&lt;p&gt;整体表现与之前的方案一相似，但是有一个显著的区别：在恢复 HTTP 流量之前，Runnable 线程数出现了明显的突刺，而在流量恢复后，这种突刺现象则不再出现，线程数已经趋于平稳。我们注意到突刺出现的时间节点是 14:55:25，这个时间点正好是我们预热时发起 HTTP 接口调用的时间。&lt;/p&gt; 
&lt;p&gt;这表明通过预热策略，我们有效地前置了系统的负载波动。当真正的用户请求到达时，系统已经趋于平稳，服务响应速度保持稳定，从而为用户提供了更加流畅的体验。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;① CPU 使用率情况如下：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//3ede52f825b29ea6085afd0dbf6c32c2.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;CPU 使用率&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;② 线程指标如下：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//b4e11728f36cf7ed5f7490ab024b857f.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Runnable 线程数&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//32152718fa8265f6e8e7524b94e66a5b.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Blocked 线程数&lt;/p&gt; 
&lt;span id="OSC_h1_25"&gt;&lt;/span&gt; 
&lt;h1&gt;六、总结&lt;/h1&gt; 
&lt;p&gt;本文针对服务启动后几分钟内 CPU 持续处于高峰状态的问题，提出了自己的分析思路与解决方案。最终线上效果比较显著，成功解决了每次发版过程中频繁告警、业务受损以及用户体验不佳的问题，为服务的高可用性增添了一道重要保障。最初的分析不够深入，导致在内存缓存预热方面的努力未能产生预期效果。因此在未来遇到类似问题时，我们必须深入挖掘，直至找到问题的根本原因。&lt;/p&gt; 
&lt;p&gt;本文的重点在于问题的发现、分析及解决思路。对于 CPU 相关的问题，火焰图和 Arthas 是非常有效的工具，建议大家在遇到类似情况时，积极尝试使用这些工具进行排查和解决。&lt;/p&gt; 
&lt;p&gt;此外 HTTP 请求未定义属性的问题普遍存在，特别是在服务未进行预热启动时，会加剧 CPU 的负载。对于大流量服务而言，遇到此类问题时，需规范请求参数，以减轻 CPU 负担。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/vivotech/blog/18685693</link>
      <guid isPermaLink="false">https://my.oschina.net/vivotech/blog/18685693</guid>
      <pubDate>Thu, 24 Jul 2025 07:12:24 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>特朗普希望重命名「人工智能」术语，改为「天才智能」</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;当地时间 7 月 23 日，美国总统特朗普在华盛顿特区举行的人工智能峰会上&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2FAcyn%2Fstatus%2F1948137088945369220" target="_blank"&gt;发言&lt;/a&gt;，他表示自己不喜欢「人工智能」（ Artificial Intelligence）这个术语表述，建议改名「天才智能」。&lt;/p&gt; 
&lt;p&gt;特朗普在讲话中称，自己不喜欢 AI 中「Artificial」一词，「我忍不了任何造作的东西，我甚至不喜欢人造东西这个名字」。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0724/145318_T5L4_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;他还表示，「&lt;strong&gt;AI 是天才的，是纯粹的天才，我建议把它改名天才智能。&lt;/strong&gt;」特朗普在讲话中强调，自己这个想法是「认真的」。&lt;/p&gt; 
&lt;p&gt;根据英国牛津英语词典，Artificial 一词除了「人工、人造」外，也有「矫揉造作」的含义。&lt;/p&gt; 
&lt;p&gt;值得一提的是，据外媒相关报道：&lt;a href="https://www.oschina.net/news/362050"&gt;美国政府将签署大量有关 AI 产业的行政令&lt;/a&gt;。&lt;/p&gt; 
&lt;p&gt;据介绍，这些行政令内容涵盖多个方面，包括建立支持数据中心、半导体制造工厂建设的举措，完善国家电力网络，以及消除 AI 大模型对话中所谓的「意识形态偏见」等。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362115</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362115</guid>
      <pubDate>Thu, 24 Jul 2025 06:54:24 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>脉脉：超四成国内 AI 头部公司员工欲跳槽</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="background-color:#ffffff; color:#242424"&gt;脉脉平台&lt;/span&gt;&lt;span style="background-color:#ffffff; color:#242424"&gt;最新&lt;/span&gt;&lt;span style="background-color:#ffffff; color:#242424"&gt;数据显示，截至 2025 年 7 月，国内 AI 头部公司员工的跳槽意愿显著高于其他行业。高达&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;strong style="color:#242424"&gt;41.07%&lt;/strong&gt;&lt;span style="background-color:#ffffff; color:#242424"&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;的 AI 从业者目前处于「正在看机会」的求职状态，这一比例远超互联网行业的&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;strong style="color:#242424"&gt;14.65%&lt;/strong&gt;&lt;span style="background-color:#ffffff; color:#242424"&gt;。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;自今年 2 月以来，每月新增上万名 AI 人才将其求职状态更新为「正在看机会」，这充分体现了 AI 人才市场的高度活跃性。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="216" src="https://oscimg.oschina.net/oscnet/up-3e5948b9574d255f2fff9c5a87be1ffdfe1.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;与此同时，企业间的「抢人大战」已进入白热化阶段。目前已有超过&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;strong&gt;1000 家 AI 公司&lt;/strong&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;在脉脉平台发布了 AI 相关岗位。为了吸引&lt;span&gt;顶尖&lt;/span&gt;人才，包括华为、小红书、DeepSeek 等在内的知名企业高管也亲自上阵，在个人主页签名中明确标注「长期招人」。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;此外，HR 和猎头在平台上的活跃度达到「分钟级」，AI 人才的个人主页访问量也因此激增，显示出市场对 AI 人才的迫切需求。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362114</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362114</guid>
      <pubDate>Thu, 24 Jul 2025 06:53:24 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>基于 Python-use 范式的开源 Agent</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;p style="text-align:left"&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;「传统 Agent 框架更像是用低代码拖拽的「机器人编排器」，&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;Python-use 则是直接用 Python 把 Agent 逻辑实现出来，让代码就是 Agent。」&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;当大多数 Agent 框架还在把「工具」当作黑盒 API 时，知道创宇 AI 业务部总经理王利伟和其团队在思考另一种方式——如果代码就是工具，而 LLM 恰好擅长写代码，为什么不干脆让 AI 自己用 Python 把任务跑出来？&lt;/p&gt; 
&lt;p&gt;在这篇访谈中，王利伟系统阐述了「Python-use 范式」——一种把 Agent 逻辑直接写成可执行 Python 的极简思路。它抛弃繁复的 Schema 注册、Workflow 编排和多 Agent 协商，实现细粒度代码控制，逻辑可控、可调试、最少 Token 浪费。&lt;/p&gt; 
&lt;p&gt;本周六，王利伟将出席【Al Agent：从工具助手到自主行动】OSC 源创会·杭州站活动，并发表《基于 Python-use 范式的开源 Agent》主题演讲，介绍如何撮合 LLM ➕Python 生态形成强大的智能体，通过独创的 Python-use 范式，让 AI 不光会调用工具，也会自己造工具。&lt;/p&gt; 
&lt;p&gt;即刻报名：&lt;a href="https://www.oschina.net/event/8597955"&gt;https://www.oschina.net/event/8597955&lt;/a&gt;&lt;/p&gt; 
&lt;p style="text-align:center"&gt;&lt;img height="1067" src="https://oscimg.oschina.net/oscnet/up-f2c70b12c7d35ded92d220bd2c56ab5987b.png" width="800" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&lt;span style="color:#27ae60"&gt;&lt;strong&gt;问：您提出「Python-use 范式」与传统 Agent 开发框架的核心差异是什么？它如何解决现有 Agent 工具调用能力的局限性？&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;答：&lt;/p&gt; 
&lt;p style="text-align:left"&gt;回答这个问题之前，我们先定义一下什么是「工具」，众所周知「工具」调用是 Agent 的基本能力之一。工具到底是什么呢？是各种应用程序，接口对吧。从根本上来讲都是代码，代码组成了 MCP 工具、API 工具以及各类应用程序。Python use 范式是回归第一性原理，把 code 当成工具，code 是所有工具的最基本构成，code 可以组成各种各样的工具，而 LLM 对 code 的理解和编写能力都足够强，相比依赖于现成的工具，Python use 是从代码出发，具有灵活性、扩展性。当然，在这过程 Python use 也是支持现有工具的调用的，比如 MCP、browser use 等等。而对于一些碎片化的场景，没有标准工具、现成工具可以用的场景，Python use 可以依赖于 Python 编码自行找到更具创造性的方案。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;一句话总结：&lt;/p&gt; 
&lt;p style="text-align:left"&gt;传统 Agent 框架更像是用低代码拖拽的「机器人编排器」；&lt;/p&gt; 
&lt;p style="text-align:left"&gt;Python-use 则是直接用 Python 把 Agent 逻辑实现出来，让代码就是 Agent&lt;/p&gt; 
&lt;div&gt; 
 &lt;table cellspacing="0" style="border-collapse:collapse; border:none; table-layout:fixed; width:500px"&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td style="border-color:#dee0e3; border-style:solid; border-width:1px; height:39px; vertical-align:top"&gt; &lt;p style="text-align:left"&gt;维度&lt;/p&gt; &lt;/td&gt; 
    &lt;td style="border-color:#dee0e3; border-style:solid; border-width:1px; height:39px; vertical-align:top"&gt; &lt;p style="text-align:left"&gt;传统 Agent 开发框架&lt;/p&gt; &lt;/td&gt; 
    &lt;td style="border-color:#dee0e3; border-style:solid; border-width:1px; height:39px; vertical-align:top"&gt; &lt;p style="text-align:left"&gt;Python-use 范式&lt;/p&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td style="border-color:#dee0e3; border-style:solid; border-width:1px; height:39px; vertical-align:top"&gt; &lt;p style="text-align:left"&gt;任务驱动逻辑&lt;/p&gt; &lt;/td&gt; 
    &lt;td style="border-color:#dee0e3; border-style:solid; border-width:1px; height:39px; vertical-align:top"&gt; &lt;p style="text-align:left"&gt;通过「规划 → 调度 → 工具调用 →反馈」的多层 Agent、子-Agent、workflow 实现任务拆解和执行。往往是图状、嵌套、多 Agent。&lt;/p&gt; &lt;/td&gt; 
    &lt;td style="border-color:#dee0e3; border-style:solid; border-width:1px; height:39px; vertical-align:top"&gt; &lt;p style="text-align:left"&gt;直接写出「任务目标 → 代码逻辑 → 执行」的 Python 脚本来解决任务，代码即规划+工具调用+执行的统一体。&lt;/p&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td style="border-color:#dee0e3; border-style:solid; border-width:1px; height:39px; vertical-align:top"&gt; &lt;p style="text-align:left"&gt;工具调用&lt;/p&gt; &lt;/td&gt; 
    &lt;td style="border-color:#dee0e3; border-style:solid; border-width:1px; height:39px; vertical-align:top"&gt; &lt;p style="text-align:left"&gt;工具通常封装为 function calling / Tool 类、API schema，由 Agent 通过有限的模板化调用（受限于预定义接口和框架支持的函数集合）。&lt;/p&gt; &lt;/td&gt; 
    &lt;td style="border-color:#dee0e3; border-style:solid; border-width:1px; height:39px; vertical-align:top"&gt; &lt;p style="text-align:left"&gt;直接调用 Python 生态中任意库、API、命令行、HTTP、数据库等，甚至动态生成和运行代码，无需提前注册工具。&lt;/p&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td style="border-color:#dee0e3; border-style:solid; border-width:1px; height:39px; vertical-align:top"&gt; &lt;p style="text-align:left"&gt;灵活性&lt;/p&gt; &lt;/td&gt; 
    &lt;td style="border-color:#dee0e3; border-style:solid; border-width:1px; height:39px; vertical-align:top"&gt; &lt;p style="text-align:left"&gt;强调框架内一致性和安全性，但牺牲了灵活性。增加一个新工具需要写 schema、注册、重训练或适配。&lt;/p&gt; &lt;/td&gt; 
    &lt;td style="border-color:#dee0e3; border-style:solid; border-width:1px; height:39px; vertical-align:top"&gt; &lt;p style="text-align:left"&gt;由于直接写 Python 代码，可以随时引入任何新工具、任意组合库、甚至嵌入 shell/JS 等。灵活性最大。&lt;/p&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td style="border-color:#dee0e3; border-style:solid; border-width:1px; height:39px; vertical-align:top"&gt; &lt;p style="text-align:left"&gt;执行粒度&lt;/p&gt; &lt;/td&gt; 
    &lt;td style="border-color:#dee0e3; border-style:solid; border-width:1px; height:39px; vertical-align:top"&gt; &lt;p style="text-align:left"&gt;依赖大量 LLM 推理+中间规划，执行粒度粗，容易浪费 token、出错。&lt;/p&gt; &lt;/td&gt; 
    &lt;td style="border-color:#dee0e3; border-style:solid; border-width:1px; height:39px; vertical-align:top"&gt; &lt;p style="text-align:left"&gt;细粒度代码控制，逻辑可控、可调试、最少 token 浪费。&lt;/p&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;至于如何解决现有 Agent 工具调用能力的局限性大体分析如下：&lt;/p&gt; 
&lt;p style="text-align:left"&gt;现有 Agent 框架在工具调用上主要有两个局限：&lt;/p&gt; 
&lt;p style="text-align:left"&gt;1、工具注册繁琐且封闭：需要开发者把工具写成符合接口的形式并注册进 Agent 系统。灵活性低、扩展慢。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;2、推理成本高+错误多：每次工具调用都可能需要 LLM 去推理哪个工具+如何填参数，容易出错，且慢。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;Python-use 通过：&lt;/p&gt; 
&lt;p style="text-align:left"&gt;代码即接口：不需要任何预定义 schema、function calling 注册。Python 里能 import / pip install 的库、调用的 API，都是工具。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;动态生成工具：Python 里可以即时生成函数、类、模块，甚至临时下载或拼接代码然后执行，完全不受限。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;全栈生态：Python 能调用系统命令、数据库、网络请求、爬虫、机器学习、云 API… 不再被框架内置的工具集限制。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;例如：&lt;/p&gt; 
&lt;p style="text-align:left"&gt;传统 Agent 框架里，你要增加对某个第三方 CRM 的支持，得写 Tool 类、注册 schema、让 LLM 学会调用。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;Python-use 里，你直接用 requests 或 SDK 写个接口调用完事。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;传统 Agent 范式假设：&lt;/p&gt; 
&lt;p style="text-align:left"&gt;人类用自然语言说「你去干 X」，AI 负责拆解成多步计划+调度各种工具完成。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;Python-use 范式更像：&lt;/p&gt; 
&lt;p style="text-align:left"&gt;人类写出一段 Python 程序告诉 AI 怎么干，或者 AI 直接生成出一段 Python 程序来干。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;即：&lt;/p&gt; 
&lt;p style="text-align:left"&gt;传统是 LLM+流程编排器+有限工具集&lt;/p&gt; 
&lt;p style="text-align:left"&gt;Python-use 是 LLM+Python 解释器+全 Python 生态&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&lt;span style="color:#27ae60"&gt;&lt;strong&gt;问：「让 AI 自己造工具」是演讲的亮点。能否解释 LLM 在 Python-use 范式中如何完成从「使用工具」到「生成工具」的跨越？关键技术难点是什么？&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&lt;strong&gt;答：&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;使用工具其实只是一个思维方式的差别生产工具，只是一张窗户纸，只是大家对 LLM 的理解以及应用方式的差别。使用工具 tool use 是假定要处理的任务都有各种现成的工具可以使用，Python use 一样也具备这个能力，并不是说它就不支持现有工具的调用，Python use 认为 code is agent ，code is everything，Python 可以 use network、use computer 可以 use 各类工具，它可以 use code 去编码、写工具。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;在传统 Agent 中：&lt;/p&gt; 
&lt;p style="text-align:left"&gt;LLM 能做到的通常是：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;选择一个已有工具&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;正确填写参数调用&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;（最多）按照文档组合几个已有工具完成目标&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p style="text-align:left"&gt;它的「能力边界」被框架里预定义的 function/schema 限死了。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;在 Python-use 中：&lt;/p&gt; 
&lt;p style="text-align:left"&gt;LLM 不光能调用库和工具，还可以：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;根据任务需要动态生成代码段（工具）&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;把这段代码封装成函数/类/模块/脚本&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;并且可以即时运行、测试、调试它&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p style="text-align:left"&gt;也就是说，它不只是「调用工具」，它还能写工具！&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;举个例子：&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&lt;span style="color:#d83931"&gt;「帮我把一堆 Excel 按部门拆分成不同的 PDF 并发邮件」&lt;/span&gt;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;传统 Agent：找不到现成的「拆 Excel 发 PDF」工具，任务失败或需要人手扩展工具。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;Python-use：LLM 生成一个函数&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&lt;span style="color:#2ea121"&gt;def split_excel_and_send():&lt;/span&gt;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&lt;span style="color:#2ea121"&gt;# pandas, fpdf, smtplib 逻辑&lt;/span&gt;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;运行测试、修复 bug、保存。这段代码就是一个新造出来的「工具」，下次还能直接用。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;为什么 Python-use 能支持「造工具」？关键在于：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;LLM 生成的就是代码，代码本身就是工具&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Python 解释器支持动态定义、动态执行、动态 import 模块&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;全 Python 生态的库让「造工具」成本极低&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;人类可以随时 review、微调、持久化新工具&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;当然，这个跨越不是轻易做到的，主要有几个挑战：&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&lt;strong&gt;代码生成的正确性&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;LLM 写出的代码可能语法正确但逻辑错误&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;对外部库版本/接口调用不熟导致出错&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;没有即时验证的环境，bug 率高&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="text-align:left"&gt;&lt;strong&gt;上下文管理&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;造出来的工具需要有清晰的输入输出和作用域&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;如果任务复杂，代码的组织结构（函数拆分、模块化）很容易混乱&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="text-align:left"&gt;&lt;strong&gt;安全性&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;动态生成的代码有潜在的安全风险（注入恶意代码、破坏环境、泄露数据）&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;需要沙箱或审核机制&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="text-align:left"&gt;��&lt;/p&gt; 
&lt;p style="text-align:left"&gt;怎么克服这些难点也有对应的思路和方案，比如：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;内置单元测试和验证，让 LLM 顺便生成测试用例或自动运行测试，提高正确性。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;设计合理的 prompt 模式，指导 LLM 输出模块化、注释良好、易维护的代码。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;用虚拟环境+沙箱，让生成和执行的代码不破坏主环境，保障安全。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;em&gt;版本控制+注册，把造出来的工具保存到 Git、注册到私有 PyPI 或工具库中。&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;总结一句话：&lt;/p&gt; 
&lt;p style="text-align:left"&gt;在 Python-use 中，LLM 不只是「选工具」，而是可以直接写出满足当前任务的新工具、即写即用；&lt;/p&gt; 
&lt;p style="text-align:left"&gt;而传统 Agent 则停留在「调用已有工具」阶段，受限于框架的工具集。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&lt;span style="color:#27ae60"&gt;&lt;strong&gt;问：Python 生态有海量开源库，但 LLM 常因依赖、环境问题调用失败。Python-use 如何实现 LLM 与本地 Python 环境的高效安全交互？&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&lt;strong&gt;答：&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;这个问题提的非常好，确实是有各类的版本问题、兼容性、依赖关系问题等等。解决方案是它在执行任务的时候不局限一个方案，一个不行会切换到另外的方案，大模型知道怎么解决。如今 vibe coding 都是差不多的思路，有错误，再重新丢给大模型去分析提出修正就好了，直到运行成功。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;另外一个方法是，在执行任务的时候会把用户系统相关的版本信息、环境信息做收集，发给模型和 TrusToken-也就是我们的 token 分发平台及网关，TrusToken 上会集成很多场景的「最佳实践」形成经验库、知识库，从而帮会根据用户环境做最优匹配，可以理解是 TrusToken 上面做了很多优化。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;至于安全问题，上个问题也提到过，理论上确实存在安全风险，我们也有考虑安全模块，也有方案，还没来得及做。一个安全公司在做产品的时候并没有把安全机制放在首位是有其他考虑，我们完全可以做个沙盒，但是为什么不做沙盒，放到沙盒限制了太多功能，实质上我们电脑上大多数软件都是运行在本机，并没有沙盒，只有杀毒软件才会有。理论上安全风险干什么都存在，与安全风险共舞，不因噎废食。实质上，从现在几万注册用户的使用反馈来讲，还没有安全问题被提出。当然，随着项目的成熟会把响应的机制逐渐完善，现在是有想法没精力，从技术上来讲不是不可解决的难题。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&lt;span style="color:#27ae60"&gt;&lt;strong&gt;问：在操作物联网设备中，智能体如何统一处理不同品牌/协议设备的接口差异？是否依赖预设插件？&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&lt;strong&gt;答：&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;充分信任和利用大模型，他对现有的品牌协议他都懂，主流的接口标准、协议他都学习过的，这些知识他比人熟。如果是定制化的软件它没有学习过，直接写到 API 描述里，大模型通过 API 描述学习，当然对 api 描述就有一定的要求，实在它不懂的就给他外挂说明。AiPy 操作物联网设备并不是依赖插件，主要是通过 API Calling ，当然有插件可以调用也是极好的，实际上我们也在准备发布插件商城。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;提到这个问题不得不提一下我们团队的另外一个产品 ZoomEye.org，它是全球领先的网络空间资产测绘平台，它通过对全球 IPv4 和 IPv6 地址进行探查，能够识别数十亿联网设备的开放端口、服务类型、协议栈、操作系统、硬件厂商、固件版本等关键资产信息。换句话说，ZoomEye 就像是整个网络世界的「显微镜」或「地图系统」，让你可以一眼看清某个 IP 背后部署了哪些设备、跑着什么服务、使用了什么协议。它支持的协议识别范围极广，涵盖操作系统、网络设备等传统 IT 系统、工业控制系统（如 Modbus、BACnet）、摄像头设备（如 ONVIF）、网络存储（如 NAS）、IoT 中控网关、智能家居等，这些恰恰是大多数传统 Agent 系统难以应对的「黑盒」。我们正在探索将 ZoomEye 的识别能力与 AiPy 结合：AI 可以在执行任务前，通过 ZoomEye 自动识别目标设备类型、开放接口、固件版本，进一步提高调用准确率和安全性。这种从「识别 → 理解 → 控制」的闭环，将极大提升 AI 操控物联网设备的普适性与稳定性。现在 ZoomEye 也已经发布了 MCP 和 API，大家可以去体验。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&lt;span style="color:#27ae60"&gt;&lt;strong&gt;问：如何吸引开发者加入 Python-use 生态？会提供哪些 SDK 或工具链降低接入成本？&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&lt;strong&gt;答：&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;因为项目还在初期，暂时还没有 SDK 之类的工具，为了方便开发者调试，给大家的支持就是提供了大量 Token 进行试错调试，默认 1000 万 token，开发者可以凭贡献持续兑换。我们后续会开放商城，商城里可以发布各种插件、成果、知识库、角色、API、MCP 等等，开发者也可以贡献各类插件或应用到商城，优秀的成果我们也会做一些激励措施。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;随着项目的推进我们会持续优化改进生态，也欢迎大家提意见。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&lt;strong&gt;问：对于想尝试 Agent 开发的团队，您认为切入此领域最应优先掌握的三大能力是什么？&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&lt;strong&gt;答：&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;说实话这个问题我并不太敢回答，一是因为我们走的路和别人不一样，二我们自己还并没有成功，没有资格去给别人指点什么。只能单纯的分享自己的几个感受：&lt;/p&gt; 
&lt;p style="text-align:left"&gt;模型能力足够强，有很大的挖掘潜力。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;以前是语料驱动模型，现在是数据驱动 Agent，对要做的场景 know how 掌握了多少是关键。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;不管你啥范式，啥技术，不出 1 个月时间大家都能做到，大家也看到了现在大模型之间的能力差距差别是越来越小了，技术之外的优势可能才是竞争力。&lt;/p&gt; 
&lt;p style="text-align:center"&gt;&lt;em&gt;&lt;img height="2676" src="https://oscimg.oschina.net/oscnet/up-aeeb1d545d5e5ab92f8a3b5d959269e8da3.png" width="800" referrerpolicy="no-referrer"&gt;&lt;/em&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/4489239/blog/18685742</link>
      <guid isPermaLink="false">https://my.oschina.net/u/4489239/blog/18685742</guid>
      <pubDate>Thu, 24 Jul 2025 06:48:24 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>中国证明开放权重模型优于 GPU 算力资源</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;国外科技媒体 The Register 近日&lt;u&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.theregister.com%2F2025%2F07%2F19%2Fopenai_us_china%2F" target="_blank"&gt;发文&lt;/a&gt;&lt;/u&gt;讨论了开放权重模型对 AI 技术进步的正面影响，称中国企业通过开放分享和底层创新，比如 DeepSeek 和 Kimi 系列模型，展现了更高的效率和更强的竞争力。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0724/143507_tWgV_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;文章标题十分坦诚——&lt;em&gt;《&lt;strong&gt;China proves that open models are more effective than all the GPUs in the world&lt;/strong&gt;》&lt;/em&gt;，直接提出「&lt;strong&gt;中国证明开放权重模型比 GPU 更有效&lt;/strong&gt;」。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;核心内容&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;1. OpenAI 延迟发布「开放权重」模型&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;自 GPT‑2 以来，OpenAI 已多年未对外开源其模型。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;原计划在本周发布一个社区友好型开源模型，但因安全审查推迟。CEO Sam Altman 表示，「一旦权重公布，就无法撤回，我们必须确保万无一失」。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2. 美国虽投资重金，但开放模型依然乏善可陈&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;美国在 GPU、计算资源上投入数百亿美元，却仅涌现出少数效率和实力不足的开源模型。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;譬如 Meta 发布的 Llama 4&amp;nbsp;遭遇争议与冷淡反响；微软、IBM、谷歌亦推出体量较小、功能局限的模型&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;3. 中国在开源领域反超&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;中国开发者不仅率先发布公开可用的大规模模型，而且算法创新表现突出。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;DeepSeek R1（DeepSeek）早在年初便问世，后来 Moonshot AI 于 7 月推出的 Kimi 2 更声称已实现万亿参数规模 MoE（专家专家模型），并宣布超越包括西方顶尖私有模型在内的技术水平。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;文章强调，尽管美企掌控大量计算资源，但因开源保守与发布缓慢，在社区驱动的模型研发上落后于中国。从战略上看，美国若想保持 AI 领导力，除了硬件投入，更应适当开放、加快社区驱动的模型生态——否则将继续被中国「公开优先」（open-first）的路线追赶。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362109</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362109</guid>
      <pubDate>Thu, 17 Jul 2025 06:37:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>开源鸿蒙机器人操作系统 M-Robots OS 正式开源</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;深开鸿宣布 &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fatomgit.com%2Fm-robots" target="_blank"&gt;M-Robots&lt;/a&gt; 开源项目正式启动。该项目由开放原子开源基金会孵化、深开鸿牵头发起，旨在以开源共建的方式打造基于开源鸿蒙的统一机器人操作系统 M-Robots OS，推动机器人行业生态融合、能力复用、智能协同。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;根据介绍，M-Robots OS 是全国首个基于开源鸿蒙构建的分布式异构多机协同机器人操作系统，具备多机实时协同、多硬件形态兼容、AI 原生以及丰富 API 与开发工具链四大核心能力，为行业提供「底层统一、场景多元」的全栈式系统平台。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="333" src="https://oscimg.oschina.net/oscnet/up-16538ff9cdf050ddcfc6cd2069b9c349cfe.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;M-Robots OS 开源计划将以分阶段、全栈式策略推进，逐步释放关键能力，推动机器人生态融合：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;2025 年 7 月 24 日：首期开源， 已上线开源鸿蒙机器人核心子系统、核心三方中间件库、包管理器、可视化开发／调试工具；&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;2025 年 12 月：发布公板芯片适配、专属驱动框架、分布式反控机制、DFX 能力；&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;2026 年 6 月：开放混合部署架构、超级设备支持、软总线增强、融合组网技术及 AI 训练工具；&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;2026 年 12 月：推出 M-DDS 分布式通信框架、分布式算力调度、多机协同支持、AI-Agent 框架与仿真工具链；&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;2027 年：实现基于 Agent 的群体智能协作体系，推出统一机器人 IDE 开发环境。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;M-Robots OS 将按照「每年两大版本」的节奏持续演进，开源范围可能会根据技术发展、场景变化、需求优先级进行调整。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;目前，该项目已汇聚包括深开鸿、中软国际、数字华夏、乐聚机器人、哈工大重庆研究院、北京工业大学、北京理工大学在内的 21 家「产学研用」成员单位，成立项目管理委员会（PMC）与多个 SIG 技术组，覆盖架构、运动控制、具身智能等关键方向，推动跨厂商协作与产业场景落地。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362103</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362103</guid>
      <pubDate>Thu, 17 Jul 2025 06:21:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>OpenAI 应用部门迎来新任首席执行官</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Instacart 首席执行官 Fidji Simo&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Ffidjissimo%2Fstatus%2F1947341053209501716" target="_blank"&gt;宣布&lt;/a&gt;，将在 8 月 18 日正式加入 OpenAI，并担任新部门的 CEO。Simo 同时在 OpenAI 官网发布了一篇&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopenai.com%2Findex%2Fai-as-the-greatest-source-of-empowerment-for-all%2F" target="_blank"&gt;深度长文&lt;/a&gt;，主要阐述了她对 AI 如何赋能和改变人类的看法。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1040" src="https://static.oschina.net/uploads/space/2025/0724/141245_kGM8_2720166.png" width="2244" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;几周后，我将加入 OpenAI 担任应用部门首席执行官，致力于让 OpenAI 的技术惠及全球更多人群。&lt;/p&gt; 
 &lt;p&gt;我一直认为自己是一名务实的技术从业者我热爱技术，并非因其本身，而是因其能对人们的生活产生直接影响。这正是这份工作令人兴奋之处，因为我相信，AI 将比历史上任何其他技术为更多人带来更多机遇。如果我们能正确运用 AI，它将赋予每个人前所未有的力量。&lt;/p&gt; 
 &lt;p&gt;但我也明白，这些机遇不会凭空出现。&lt;/p&gt; 
 &lt;p&gt;每一次重大的技术变革，都可能拓宽人们获取权力的渠道这种权力包括做出更明智决策、塑造周遭世界以及以新方式掌控自身命运的能力。但与此同时，技术变革也可能导致财富和权力进一步集中在少数人手中通常是那些本就拥有金钱、资历和人脉的人。&lt;/p&gt; 
 &lt;p&gt;因此，我们必须有意识地规划这些技术的构建与共享方式，以确保它们能为更多人带来更多机遇和繁荣。我们当下的选择，将决定这场即将到来的变革会让所有人都获得更多赋能，还是让少数人进一步集中财富和权力。&lt;/p&gt; 
 &lt;p&gt;我们可以从确保赋能与机遇的关键要素被广泛获取做起，这些要素包括知识、健康、创造性表达、经济自由、时间和支持。下文将详细阐述 AI 在改变人们生活的这些方面所具有的潜力。&lt;/p&gt; 
 &lt;p&gt;如果我们能让智能无处不在、人人可用且通俗易懂，就能打造出世界上最强大的机遇引擎，帮助更多人过上更美好的生活。我期待与 OpenAI 才华横溢的新同事们共同构建这样的未来，也会在不久后分享更多内容。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;公开资料显示，Simo 职业生涯始于 eBay，曾担任战略团队成员，专注于本地商务和分类广告项目的开发。2011 年，她加入了 Facebook，并逐步晋升为 Facebook 应用的负责人，领导包括 NewsFeed、Stories、Groups、Video、Marketplace、Gaming、News、Dating 和广告等核心产品的开发。在她的推动下，Facebook 的视频战略取得了显著进展，推出了自动播放视频、FacebookLive 和 FacebookWatch 等功能。她还带领团队构建了 Facebook 的移动广告业务，为公司的发展做出了重要贡献。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362101</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362101</guid>
      <pubDate>Thu, 17 Jul 2025 06:14:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>预计 2029 年中国数据仓库软件市场规模将达 20.9 亿美元</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;国际数据公司（IDC）于近日发布了《&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmy.idc.com%2Fgetdoc.jsp%3FcontainerId%3DprCHC53700025" target="_blank"&gt;2024 年下半年中国数据仓库软件市场跟踪报告&lt;/a&gt;》。IDC 数据显示，2024 下半年中国数据仓库软件市场规模为 5.5 亿美元，同比增长 8.7%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;其中，本地部署数据仓库软件规模 2.8 亿美元，同比增长 6.8%；公有云数据仓库软件规模 2.6 亿美元，同比增长 10.9%。IDC 预测， 到 2029 年，中国数据仓库软件市场规模将达到 20.9 亿美元，2024-2029 的 5 年市场年复合增长率（CAGR）为 15.5%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="391" src="https://oscimg.oschina.net/oscnet/up-eb766774b570abb458036289ca15326fbde.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;2024 下半年，中国数据仓库&lt;strong style="color:#01010f"&gt;本地部署模式&lt;/strong&gt;市场前五大厂商总计占比 57.7%。出于数据安全和合规性的考虑，金融、政府、能源等行业，以及大型企更倾向于本地部署模式的数据仓库产品。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="406" src="https://oscimg.oschina.net/oscnet/up-05482245a6774b1ed848792b16fadf3762d.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;与本地部署市场相比，&lt;strong style="color:#01010f"&gt;公有云&lt;/strong&gt;数据仓库服务的市场集中度更高，2024 下半年，前五大厂商份额共计达到 90.2%。随着中国泛互联网行业和传统企业的互联网业务的快速发展，企业已经在公有云上积累了大量的数据，为云上数仓的使用创造了前提和基础。2024 年，公有云数据仓库市场规模已超过本地部署市场，占比 50.3%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="455" src="https://oscimg.oschina.net/oscnet/up-6402c7dfdd4a171b4ece232ca18651694c4.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong style="color:#01010f"&gt;IDC 中国企业软件市场研究经理王楠表示&lt;/strong&gt;，存算分离架构、实时分析能力以及湖仓一体技术已经成为数据仓库产品应具备的基础能力，也是客户进行数仓产品选型时考察和评估的重点。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;拥抱生成式 AI 和大模型已经成为数据仓库下一步产品能力升级的核心，在 AI for DB 层面实现自然语言交互式查询、智能调优、智能诊断等能力，使数仓产品的使用和运维更加便捷；在 DB for AI 层面支撑向量引擎、库内机器学习能力，实现正真的智能问数 AI 加持下的数仓产品将使企业的数据分析能力进一步提高， 获得更精确的预测和洞察能力。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362100</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362100</guid>
      <pubDate>Thu, 17 Jul 2025 06:11:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>字节发布端到端同声传译模型 Seed LiveInterpret 2.0</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;字节跳动 Seed 团队宣布正式推出端到端同声传译模型 Seed LiveInterpret 2.0 —— 首个延迟&amp;amp;准确率接近人类水平的产品级中英语音同传系统，在中英同传翻译质量达到业界 SOTA 的同时，实现了极低的语音延迟水平。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;公告称，Seed LiveInterpret 2.0 基于全双工端到端语音生成理解框架，支持中英互译，可实时处理多人语音输入，像人类同传译员一样以极低的延迟 「边听边说」，一边接收源语言语音输入，一边直接输出目标语言的翻译语音。同时，Seed LiveInterpret 2.0 还支持 0 样本声音复刻，让沟通更加流畅自然。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在测试中，Seed LiveInterpret 2.0 面对 40 秒的大段中文表达，能够低延迟地丝滑输出同款音色的英语翻译。Seed LiveInterpret 2.0 还能快速学习音色，即便此前未「听」过角色的声音，依然能通过实时交互进行现场演绎。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;相比传统机器同传系统，Seed LiveInterpret 2.0 模型具备以下优势：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;接近真人同传的翻译准确率&lt;/strong&gt;&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;精准的语音理解能力保障了翻译准确度，在多人会议等复杂场景中英双向翻译准确率超 70%，单人演讲翻译准确率超 80%，接近真人专业同传水平。&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;&lt;strong&gt;极低延迟的 「边听边说」 能力&lt;/strong&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;采用全双工语音理解生成框架，翻译延迟可低至 2-3 秒，较传统机器同传系统降低超 60%，实现了真正的 「边听边说」 翻译。&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;&lt;strong&gt;零样本声音复刻，音色真实自然&lt;/strong&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;只需采样实时语音信号，便能提取声音特征，用说话人的音色特质实时 「说出」 外语，提升交流的沉浸感和亲和力。&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;&lt;strong&gt;智能平衡翻译质量、延迟和语音输出节奏&lt;/strong&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;可根据语音清晰度、流畅度、复杂程度，调整输出节奏，并适配不同语言特性。面对超长信息，依然能保证传译语音节奏的自然流畅。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;目前，Seed LiveInterpret 2.0 技术报告已公布，模型基于火山引擎对外开放。此外，Ola Friend 耳机也将在 8 月底接入 Seed LiveInterpret 2.0，成为首个支持该模型的智能硬件设备。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:8px; margin-right:8px; text-align:justify"&gt;&lt;span style="color:#000000"&gt;评测结果显示，在语音到文本的同传任务中，Seed LiveInterpret 2.0 中英互译平均翻译质量的人类评分达到 74.8（满分 100，评估译文准确率），较排名第二的基准系统（47.3 分）超出 58%。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:8px; margin-right:8px; text-align:justify"&gt;&lt;span style="color:#000000"&gt;在语音到语音中英同传任务中，仅 3 个测评的翻译系统支持该能力，其中 Seed LiveInterpret 2.0 中英互译平均翻译质量达到 66.3 分（满分 100，除评估译文准确率，还评估语音输出时延、语速、发音、流畅性等指标），远超其他基准系统，&lt;strong&gt;达到接近专业真人同传的水平&lt;/strong&gt;。同时，大部分基准系统也不支持声音复刻功能。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:8px; margin-right:8px; text-align:justify"&gt;&lt;span style="color:#000000"&gt;&lt;img height="367" src="https://oscimg.oschina.net/oscnet/up-5db0506a8b7711255ee86d2bb6986dc7f78.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:8px; margin-right:8px; text-align:justify"&gt;&lt;span style="color:#000000"&gt;&lt;img height="360" src="https://oscimg.oschina.net/oscnet/up-03b5d4e3cac39425b43d1c9044266bcc0e7.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:8px; margin-right:8px; text-align:justify"&gt;&lt;span style="color:#000000"&gt;在延迟表现上，Seed LiveInterpret 2.0 在语音到文本场景中，&lt;strong style="color:rgba(0, 0, 0, 0.9)"&gt;输出首字平均延迟仅 2.21 秒，在语音到语音场景中，输出延时仅 2.53 秒，做到了对翻译质量以及时延的均衡。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:8px; margin-right:8px; text-align:justify"&gt;&lt;span style="color:#000000"&gt;&lt;img height="255" src="https://oscimg.oschina.net/oscnet/up-86b7ef584ede587d9df7ad36aafec6d1ab2.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:8px; margin-right:8px; text-align:justify"&gt;&lt;span style="color:#000000"&gt;&lt;img height="260" src="https://oscimg.oschina.net/oscnet/up-8ec35eff6c30fc2e8e276f0f36159d3cf7d.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:8px; margin-right:8px; text-align:justify"&gt;&lt;span style="color:#000000"&gt;不过，字节方面也坦承尽管 Seed LiveInterpret 2.0 已初步展现出一定优势，&lt;strong style="color:rgba(0, 0, 0, 0.9)"&gt;其边界仍有拓展空间&lt;/strong&gt;。比如，在语言覆盖方面，目前模型主要支持中英互译，其他语种尚未较好支持。此外，其声音复刻的稳定性、语音表现力、情绪复刻能力、极复杂情况下的翻译准确性等仍有进步空间。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:8px; margin-right:8px; text-align:justify"&gt;&lt;span style="color:#000000"&gt;更多详情可&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fvjq_cwneALGoPf6RgxwuLQ" target="_blank"&gt;查看官方公告&lt;/a&gt;。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362097</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362097</guid>
      <pubDate>Thu, 17 Jul 2025 05:55:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>智能体时代，如何避免大厂垄断 AI ？</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;「过去十几年里，&lt;strong&gt;互联网的开放性正在逐步消失。&lt;/strong&gt;越来越多的服务、数据、用户，被锁定在几个大型平台的生态里。协议的边界被平台所取代，数据也变成了平台资产而不是网络资源。&lt;strong&gt;我们不希望智能体时代重复这一切。&lt;/strong&gt;」&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;近日，ANP 开源技术社区发起人常高伟在接受国际著名科学杂志《New Scientist》采访时指出了当前互联网的封闭性，并担心智能体时代将会重蹈覆辙。&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;目前来看，大部分 AI 协议都是由大型科技公司提出的，比如 Anthropic、Google 这些企业，他们推动了 MCP、A2A 等协议的发展，也让更多的人看到协议对智能体的价值。但这些协议的设计，很多时候是基于他们自己的产品路径和商业利益出发的。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;常高伟认为，这本身没有错——商业公司有自己的考量和节奏。「但问题是，如果整个智能体互联网的底层协议都由几家公司来主导，那我们可能会再次走上平台封闭化的老路。&lt;strong&gt;就像今天的社交平台、应用商店、广告系统，数据和权限越来越集中在少数大公司手里。&lt;/strong&gt;」&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;要打破这种封闭性困局，常高伟等人于 2024 年 4 月开源的 &lt;strong&gt;ANP（Agent Network Protocol）&lt;/strong&gt; 提供了新路径。与 Anthropic 主导的 MCP、Google 推动的 A2A 不同的是，&lt;span&gt;ANP 从一开始就关注智能体之间的身份认证问题。这样一来，任何两个智能体——不管是谁开发的，来自哪家公司，都能通过标准协议完成安全的双向身份认证。&lt;/span&gt;这一设计使 ANP 成为首个真正面向开放互联网的智能体协议。&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;更重要的是，ANP 由全球开发者社区共建&lt;/strong&gt;。其发起团队明确强调：这是一个&lt;strong&gt;不追求盈利的非商业化组织&lt;/strong&gt;，成员包含极客、学者与创业者。常高伟表示，AI 不应该被垄断，它的连接能力、协作能力，应该像空气和水一样，向所有人开放。ANP&amp;nbsp;通过完全开源和去中心化架构，让智能体间的协作回归以协议为中心的开放连接，打破平台封闭化的老路和数据孤岛，确保连接权回到每一个人手里，&lt;strong&gt;让互联网重新成为创造力的土壤，而不是流量的围墙。&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;为了在全球范围内推动智能体协议标准化的共识与合作，&lt;/span&gt;ANP&amp;nbsp;&lt;span&gt;开源技术社区牵头在 W3C 发起了 "AI Agent Protocol" 社区组（Community Group）。W3C 一直是互联网协议发展的重要推动者，从 HTTP 到 HTML，它见证并塑造了多个开放技术的诞生。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#2980b9"&gt;&lt;strong&gt;以下为《New Scientist》杂志采访常高伟全文：&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;New Scientist：Why are protocols important to enable agentic AI?（为什么协议对实现 Agentic AI 至关重要？）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;常高伟&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：想象一下，如果你让一个 AI 去使用 Excel 表格、点开网页、登录邮箱，才能获取信息，它要么得模仿人类的鼠标操作，要么得反复破解界面背后的逻辑。这种方式其实非常不自然——&lt;/span&gt;&lt;strong&gt;&lt;span&gt;AI 并不擅长使用为"人类"设计的软件，它更擅长的是直接处理数据&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;从这个角度看，我们其实应该反过来思考：让数据为 AI 所用，而不是让 AI 学着像人那样"用工具"。这就需要一种标准方式，把数据、身份、能力、安全都打包好，直接交给智能体使用。这种"标准方式"，就是&lt;/span&gt;&lt;strong&gt;&lt;span&gt;协议&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;。协议是承载数据，最好的容器。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;我认为这才是协议为什么对 AI 如此重要的最根本的原因。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;但协议的重要性不单单体现在让 AI 与数字世界交互，更重要的是，它会推动 Agentic web 的到来。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;Agentic Web&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;，我们可以把它简单理解为"&lt;/span&gt;&lt;strong&gt;&lt;span&gt;为 AI 而设计的下一代互联网&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;"。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;在今天的 Web 世界里，网页是给人看的，数据往往被封装在前端页面中，只有人类点击、滑动、输入后，背后的系统才会做出响应。这种设计模式是典型的"人机交互优先"。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;但 Agentic Web 的出发点不同：它是为 AI 与 AI 的交互而构建的。在 Agentic Web 中，智能体将成为第一公民——他们是互联网中最重要的参与者，智能体之间相互协作，帮助人类完成繁琐、复杂的任务。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;支撑这种协作的关键工具，正是智能体协议。协议将成为 Agentic Web 的基础设施，它不仅定义了身份、通信、能力调用等核心机制，还让来自不同平台、不同组织、不同个人的智能体能够自由连接与协作。无论一个智能体属于哪个公司或个人，只要遵循相同的协议，它就能融入这个新型网络。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;这将彻底改变现有互联网的格局。Agentic Web 有潜力打破今天由平台主导的数据孤岛，实现真正开放、互联、去中心化的网络结构，为下一代互联网打开新的可能性。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;New Scientist：Who has been developing these protocols so far?（到目前为止，这些协议都是由谁在开发的？）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;常高伟&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：目前全球有十几个智能体相关的协议项目，有些是由科技巨头主导，有些是由开源社区或小公司推动。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;第一个是 MCP（Model Context Protocol），这个是由 Anthropic 推动的。Anthropic 是 OpenAI 的主要对手之一，他们觉得大模型光靠预训练是不够的，还得"实时连上工具"，才能真正解决问题。MCP 就像是一个标准接口，让模型可以安全、统一地调用外部系统，比如搜索、数据库、插件等。现在包括微软、OpenAI、谷歌、亚马逊等都在支持这个协议。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;第二个是 A2A（Agent-to-Agent Protocol），这是 Google 推出来的，重点不是模型和工具的连接，而是"智能体和智能体之间怎么说话"。比如一个智能体说"我不会订机票"，另一个说"我来帮你"，A2A 定义了这背后的语言和流程。目前还在早期阶段，但被不少开发者看好，尤其适合企业内部多个 AI 系统之间的互联。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;第三个是 ANP（Agent Network Protocol），这是由我们开源社区发起的项目，也是目前全球最早关注"去中心化智能体通信"的协议，我们研究这个领域比 MCP 和 A2A 更早。我们希望构建一个安全、高效、开放的智能体互联网，在这个网络中，所有的智能体都将不受大型互联网平台的限制，相互之间都可以进行通信与协作。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;MCP 关注的是模型如何连接到工具和资源，A2A 解决的问题是智能体如何在企业内部进行连接与协作，ANP 解决的问题是智能体在互联网上如何进行连接与协作。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;除此之外，还有其他一些协议，比如 Cisco 旗下的 AGNTCY 社区主导的 Agent Connect Protocol，IBM Research 主导的 Agent Communication Protocol，以及一些研究机构的项目比如 agora protocol，他们都有不同的技术路线。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;我这里有几篇智能体协议相关的 paper： &lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;https://arxiv.org/abs/2504.16736 &lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;https://arxiv.org/pdf/2505.07176v1 &lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;https://arxiv.org/pdf/2505.02279v1&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;New Scientist：What are the issues with them and why do we need this version?（它们存在哪些问题？为什么我们需要这个版本？）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;常高伟&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：其实我们在 2024 年 4 月就启动了 ANP 这个开源项目，那个时候还没有 MCP，也没有 A2A。我们是最早一批真正从"智能体协作"角度出发，来思考协议应该怎么设计的团队。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;当时我们就有一个很强的直觉：协议会是智能体之间协作的关键基础设施。于是我们去研究了很多现有协议，包括 HTTP/HTML，发现它们本质上都是为"人-网页"交互设计的，不适用于"智能体-智能体"的通信。比如说：&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;两个智能体怎么发现彼此？&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span&gt;如何互相认证身份、交换数据？&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span&gt;通信过程中如何保证安全性和隐私？&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span&gt;现有的协议在这些方面几乎是空白的。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;带着这些问题，我们设计了 ANP。它从一开始就关注智能体之间的身份认证问题，我们希望任何两个智能体，不管是谁开发的，来自哪家公司，都能通过标准协议完成安全的双向身份认证。这一点，其实是我们和 MCP、A2A 最大的差异之一。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;我们常常用"email 模式"来比喻 ANP 的身份体系：只要你有一个"智能体地址"，你就能跟全世界的智能体建立联系。这跟 MCP、 A2A 那种比较中心化方式不太一样，我认为 MCP 和 A2A 其实并没有很好的解决智能体的身份问题，特别是智能体在互联网上的身份问题。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;在架构层面上，ANP 和 MCP 也有很大的区别。MCP 是典型的客户端-服务器架构（Client-Server），也就是说智能体要主动连接服务端，服务端是不能主动发起连接的。它更像一种"单向调用"。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;&lt;span&gt;而 ANP 是一个真正的点对点架构（&lt;/span&gt;&lt;span&gt;Peer-to-Peer&lt;/span&gt;&lt;span&gt;），任何两个智能体之间都可以对等地通信、交互。这种设计更符合未来智能体之间频繁互动、协同执行任务的需求。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;至于和 A2A 的差异，我们认为最重要的一点是：A2A 是基于任务传递的协作机制。一个智能体把一个"任务包"交给另一个智能体去执行。这种模式在企业内部还好，但放到开放的互联网环境中就会遇到隐私和权限的问题。比如说，我想订酒店，用 A2A 的方式，我可能要在任务中告诉对方智能体我喜欢什么、不喜欢什么，这种个人偏好数据一旦传出去，就存在泄露的风险。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;而 ANP 在设计之初就考虑了"智能体在互联网中协作"这一复杂现实场景，ANP 采用的是一种 Linked-data 的方案，可以将智能体对外公开信息编织成一个数据网络，一个智能体可以像爬虫一样将另外一个智能体的信息爬取下来，然后在本地进行分析与决策，从而避免用户的隐私泄漏。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;总体来说，&lt;/span&gt;&lt;strong&gt;&lt;span&gt;ANP 是一个非常有创造力、非常独特的尝试，它不是对现有协议的小修小补，而是从底层重新出发，真正为"智能体互联网"准备的协议。&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;New Scientist：How important is it that we have protocols developed outside of big tech companies?（由大科技公司之外的组织制定协议有多重要？）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;常高伟&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：这是一个非常关键的问题。我们现在看到的很多 AI 协议，确实是由大型科技公司提出的，比如 Anthropic、Google 这些企业，他们推动了 MCP、A2A 等协议的发展，也让更多的人看到协议对智能体的价值。但我们也需要看到，这些协议的设计，很多时候是基于他们自己的产品路径和商业利益出发的。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;这本身没有错——商业公司有自己的考量和节奏。但问题是，如果整个智能体互联网的底层协议都由几家公司来主导，那我们可能会再次走上"平台封闭化"的老路。就像今天的社交平台、应用商店、广告系统，数据和权限越来越集中在少数大公司手里。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;我们不希望"智能体互联网"成为另一个"数据孤岛联盟"。如果我们真的相信 AI 是一项改变人类社会的重要技术，那就更需要有一个开放、中立的社区来推动协议的设计，确保它的未来是属于每个人的，而不是某几家公司的。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;这也是我们发起 ANP 开源社区的初衷，我们有自己的理念，我们希望自己的理念能够实现。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;ANP 社区非常特别，它不是一个传统意义上的商业团队，它完全是一个不追求盈利的非商业化组织。我们来自各个方向——有极客、有创业者、有学者，大部分都是对未来充满热情的理想主义者。大家聚在一起，是因为共同相信：AI 不应该被垄断，它的连接能力、协作能力，应该像空气和水一样，向所有人开放。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;同时我们也意识到，光靠一个社区的努力是不够的，还需要在全球范围内推动标准化的共识与合作。这也是为什么我们牵头在 W3C 发起了 "AI Agent Protocol" 社区组（Community Group）。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;W3C 一直是互联网协议发展的重要推动者，从 HTTP 到 HTML，它见证并塑造了多个开放技术的诞生。它是一个中立、开放、面向全球的标准化组织，不属于任何一家公司，也不服务于某个商业利益集团。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;我们相信，在 W3C 这样的国际平台上推动 Agent 协议的讨论，有助于吸引全球更多开发者、研究者、公司和组织共同参与，真正形成一个开放、协作、可信的技术生态。这也与我们在 ANP 社区里的初心是一致的：协议不应该属于某家公司，它应该属于整个智能体社会。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;New Scientist：What do you hope the protocol will provide?（你希望这个协议能带来什么？）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;常高伟&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：这个问题其实也是我们做这个协议的初衷。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;我们一直坚信一个非常古老但至今依然重要的互联网信条：连接即权力（Connection is Power）。只要一个人能够自由地连接到工具、连接到他人、连接到信息，他就具备改变世界的能力。这就是互联网最初带给我们的力量——让一个普通人也能撬动整个系统。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;但我们也看到，过去十几年里，互联网的"开放性"正在逐步丧失。越来越多的服务、数据、用户，被锁定在几个大型平台的生态里。协议的边界被平台所取代，数据也变成了"平台资产"而不是"网络资源"。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;我们不希望智能体时代重复这一切。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;ANP 想要实现的，是让未来的智能体互联网，从以平台为中心的封闭生态，回归到"以协议为中心"的开放连接。它不依赖某个平台，不绑定某个技术栈，只要你遵循这个协议，无论你是谁、你在哪、你由谁开发，你的智能体都能被识别、被发现、被调用，真正融入这个网络。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;在这样的网络里，智能体不仅是信息的接收者，还是服务的提供者；不是被平台分发的"插件"，而是彼此对等的节点，可以自由协作、交易、共享能力。这意味着任何一个开发者，只要有想法和能力，就可以进入这个生态，而不必依赖大公司赋权。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;这也是我们特别在意"非封闭、非垄断"的原因。我们希望 AI 技术的红利能够真正普惠全球，而不是被少数平台控制。我们也相信，一个真正开放的智能体互联网，会比封闭平台更有活力，会诞生出更多天马行空的创意与合作。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;所以，如果你问我，我们希望 ANP 这个协议带来什么？&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;那就是：让智能体的连接权利回到每一个人手里，让互联网重新成为创造力的土壤，而不是流量的围墙。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;New Scientist：What's the next step? Will the W3C choose a "winner"? How long does this process usually take?（下一步是什么？W3C 会选出一个"胜出者"吗？这个过程通常需要多久？）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;常高伟&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：感谢这个很好的问题。如果您指的是我们在 W3C 的下一步工作，我需要坦诚地说，我们之前主要关注 W3C 相关的技术标准，但并没有深度参与到标准制定的具体流程中。因此，对于标准制定的周期以及一些未知问题的解决时间，我们目前还不能给出确切的判断。但我们希望能在 W3C 这个平台上全力推进标准的制定和行业共识的形成。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;关于"胜出者"这个话题，我想澄清一点：W3C 是一个致力于制定开放标准（Royalty-free）的组织，标准的制定是建立在整个行业共识基础上的。我们选择来到这里，正是看重这一点。我们的目标是做好一个智能体交互的协议标准，我们希望听取更多意见来打磨一个优秀的行业协议，并不希望也不打算与谁竞争。实际上，在我们创立社区组之初，W3C 智能体相关的工作组已经发出了联络邀请，希望共同协作，这也正是我们期望看到的。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;New Scientist：You are both proposing the protocol and serving as the group chair—doesn't that pose a conflict of interest? （你本人既提出协议，又担任小组主席，这是否存在利益冲突？）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;常高伟&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：这是一个很重要的问题。首先，我想强调 ANP 社区是一个开放中立、非盈利性的社区， ANP 协议，本身是完全开源的，我们非常愿意与其他协议项目共同探索，最终落地成为一个具有行业共识的标准协议。从我们开始探索协议设计之初，我们就希望能够听取最广泛的意见。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;担任小组主席的角色与我们 ANP 以及 W3C 的愿景是完全一致的。我们希望汇聚来自不同行业、不同国家的声音，共同推进一个适合全人类的、具有共识基础的标准。从这个角度看，不仅不存在利益冲突，更准确地说，我们实际上并没有什么商业利益考量——无论是 W3C 社区组还是 ANP 都是如此。如果非要说有什么"利益"的话，那就是我们希望实现 Agentic Web 这一技术愿景。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;W3C 有着"Web for All"的愿景，我们也愿意在这一愿景基础上推进 Agentic Web 的实现，所以这些目标之间并不冲突，而是相互促进的。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;New Scientist：What happens if the big tech companies ultimately decide to adopt their own protocols? （如果大型科技公司最终决定采用自己的协议，会发生什么？）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;常高伟&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：如果大型科技公司愿意采用我们的协议，那当然是非常好的结果。我们对自己的协议方案很有信心，相信它能够解决他们在智能体互联和协作方面遇到的关键问题。我们也非常愿意配合大公司来推进协议的实际落地应用。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;当然，我们更欢迎大公司能够参与到我们协议的制定和优化过程中来。我们是开源开放的，我们的最终目的是实现 Agentic Web 这一愿景，而不是推广某一份特定的协议或标准。如果通过开放合作能够产生更好的解决方案，我们完全支持这样的结果。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;归根结底，我们关注的是整个智能体生态的健康发展，而不是某个特定协议的"胜负"。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;New Scientist：Can we provide some examples of successful or failed standardization efforts led by the W3C in recent years? （我们能否提供一些近年来 W3C 推行标准时成功或失败的案例？）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;常高伟&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：正如我在第一个问题中提到的，这是我第一次深度参与 W3C 的标准制定工作，这个问题可能需要站在 W3C 工作人员的角度来回答，我很难给出权威的答案。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;但从我个人的理解来看：正如我们选择 W3C 的原因一样，这里是一个开放的标准组织。如果某个领域存在真实需求，自然会有相关的群体来制定或推进相应的标准。所以在我的概念里，这个过程不存在简单的"成功"或"失败"。如果一份标准被某个社群制定出来，那这份标准对他们来说就是有意义和价值的；如果一份标准后来被其他标准所替代，这说明技术的变革和迭代在发生，而这本身就是技术发展中一直在发生的自然过程。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;重要的是保持开放的心态，&lt;/span&gt;&lt;strong&gt;&lt;span&gt;让最好的技术方案在公开、透明的环境中得到充分讨论和验证。&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/3859945/blog/18685716</link>
      <guid isPermaLink="false">https://my.oschina.net/u/3859945/blog/18685716</guid>
      <pubDate>Thu, 17 Jul 2025 04:38:00 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>谷歌推出能分析古代文本的 AI 模型 Aeneas</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;谷歌 DeepMind 推出了新 AI 模型 Aeneas，旨在帮助历史学家更好地理解古代文本，其能够对公元前 7 世纪至公元 8 世纪的拉丁铭文进行分析。这是第一个专门为古代铭文提供上下文解读的人工智能工具。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="280" src="https://oscimg.oschina.net/oscnet/up-869ea2ab4dcdab6c7ca5844886c3d70e2f2.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;传统上，历史学家依赖自身的专业知识和资源来寻找文本之间的相似性，即所谓的 「平行文本」。而 Aeneas 则通过处理数以千计的拉丁铭文，将这一过程大大加速，能在几秒钟内提供相关的文本和上下文平行例证，从而帮助历史学家进行更深入的解读和研究。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;该模型不仅限于拉丁文，还可以扩展到其他古代语言、文字和材料，如纸莎草文和货币，进一步丰富了历史研究的可能性。Aeneas 的多模态输入能力，意味着它能够同时处理文本和图像信息，从而提高对铭文的地理来源的判断。这一工具的先进性体现在能够恢复长度不确定的文本缺口，和在对历史文本的恢复和预测方面设立了新的基准。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;为了训练 Aeneas，研究团队整合了来自多个历史数据库的数据，包括罗马铭文数据库、海德堡铭文数据库等，创建了一个包含超过 176000 条古罗马铭文的拉丁铭文数据集。通过这种方式，Aeneas 能够有效识别并对铭文进行分类，为历史学家的研究提供了强有力的支持。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在一项评估中，23 位历史学家参与了使用 Aeneas 进行铭文的恢复、鉴定和年代定位的研究。结果显示，当历史学家结合 Aeneas 提供的上下文信息与模型的预测时，取得了最佳的研究成果。许多参与者表示，Aeneas 加速了他们的工作流程并提高了对复杂铭文任务的信心。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362067</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362067</guid>
      <pubDate>Thu, 17 Jul 2025 03:24:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>2025 开放原子开源生态大会在北京开幕</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;7 月 23 日，&lt;strong&gt;以「开源赋能产业，生态共筑未来」为主题的 2025 开放原子开源生态大会在北京开幕&lt;/strong&gt;。大会开幕式上，北京国际开源社区「开源先锋企业」引领启航，「源创部落」正式启动建设；多个开源成果现场发布。工业和信息化部副部长熊继军、北京市人民政府副秘书长许心超出席大会并致辞。北京经济技术开发区工委副书记、管委会主任王磊出席大会开幕式并参与相关发布仪式。&lt;/p&gt; 
&lt;p&gt;&lt;img height="720" src="https://static.oschina.net/uploads/space/2025/0724/105745_8YjD_2720166.png" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;「2025 年首批开源先锋企业」授牌仪式。&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;北京国际开源社区经过两年建设取得积极进展，在开源基础设施建设、开发者服务、开源项目孵化运营方面持续突破，逐步构建起多维协同的开源生态。&lt;/strong&gt;一批优秀企业入驻，依托「开源+」模式驱动产业创新融合，为社区开源生态建设注入先锋力量。大会上，工业和信息化部信息技术发展司司长王彦青，北京经济技术开发区工委副书记、管委会主任王磊，北京市经济和信息化局副局长顾瑾栩，开放原子开源基金会理事长程晓明为「2025 年首批开源先锋企业」授牌。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0724/105836_QVwf_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;「源创部落」启动仪式。&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;现场，&lt;strong&gt;「源创部落」建设同步启动。&lt;/strong&gt;作为面向开源中小企业及初创团队的实体孵化平台，「源创部落」将提供「项目孵化、初创扶持、社区赋能、生态共建」四大能力，降低开源创新创业门槛，释放开源技术商业化潜力，推动北京国际开源社区向「实体空间+生态赋能」双轮驱动的 2.0 阶段升级，加速构建吸引开源企业「近悦远来」的「强磁场」。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0724/105948_g9xR_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;开源项目捐赠仪式。&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;开放原子开源基金会作为科技公益性服务机构，始终致力于打造以开发者为本的开源生态。&lt;/strong&gt;现场，开放原子开源基金会与 Mobius 大模型、OpenLoong、openDACS、IvorySQL、LLMOne、OpenIBC 增强型区块链平台、Codeya IDE 和 GeniusAI 算法研发平台完成捐赠签约，涵盖人工智能、具身智能、基础软件、区块链等多个关键技术领域，极大丰富和完善了开源生态，实现「工具-技术-场景」闭环。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0724/110007_9U1J_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;开源项目应用案例发布仪式。&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;从代码迭代到产业落地，从技术共享到生态共建，开源的力量正赋能千行百业。&lt;strong&gt;会上发布了 100 余家单位的 150 余个开源项目应用案例&lt;/strong&gt;，案例覆盖电力、通信、医疗、教育、金融、交通等 10 余个关系国计民生的关键行业，彰显了开源技术在促进产业升级、行业创新方面的强大动力。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0724/110036_jKd3_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;开放原子电鸿开源社区启动仪式。&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0724/110057_9LJz_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;开放原子旋武开源社区启动仪式。&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;开源的蓬勃发展，离不开产业界协同共建。会上举行了电鸿开源·旋武聚力——社区启动仪式。&lt;strong&gt;开放原子电鸿开源社区&lt;/strong&gt;作为基金会首个行业应用社区，聚焦全产业链协同与行业场景深度融合，聚力构建服务新型工业化、绿色化、智能化转型的核心工业底座。&lt;strong&gt;开放原子旋武开源社区&lt;/strong&gt;的核心使命是集合社区力量，推动 Rust 普及应用、培养汇聚专业人才、促进生态系统建设，进而强化数字基座安全性。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0724/110116_j7go_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;第三届开放原子大赛启动仪式。&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;开放原子大赛着眼于解决「真问题」，形成了多项具有应用价值的技术成果。&lt;strong&gt;现场第三届开放原子大赛正式启动，首批 12 个赛项同步发布。&lt;/strong&gt;大赛重点覆盖基础软件、工业软件、人工智能等领域，设置巅峰挑战赛、实战竞技赛、训练学习赛等多种类型，总奖金 1500 万元，致力于汇聚全球开源智慧，破解产业发展中的核心技术难题，加速创新成果转化与高水平开源人才培养。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0724/110142_tXmN_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;会上，&lt;strong&gt;多位专家围绕 2024 年度我国开源发展整体态势、近三年来国内外开源项目与开发者数据&lt;/strong&gt;，分别从开源许可证、代码托管平台、地方开源产业布局、重点技术领域、行业应用、开源安全、开源教育和开源学术、商业化等重点方向进行了解读。&lt;/p&gt; 
&lt;p&gt;华为终端 BG 首席执行官何刚，理想汽车 CTO 谢炎，深圳开鸿数字产业发展有限公司 CEO 王成录，vivo 副总裁、OS 产品副总裁、vivo AI 全球研究院院长周围，乐聚机器人董事长冷晓琨先后发表主题演讲。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;AI 与开源融合成为本届大会亮点之一，大会特别设置圆桌论坛环节。&lt;/strong&gt;麒麟软件首席科学家韩乃平主持，国家地方共建人形机器人创新中心首席科学家江磊，CSDN 创始人＆董事长、中国开源推进联盟副主席蒋涛，软通动力董事兼首席技术官刘会福，openKylin 生态委员会主任李震宁，开源中国董事长马越围绕「智能时代的开源开发探索与实践」主题展开深入探讨。&lt;/p&gt; 
&lt;p&gt;本次大会由开放原子开源基金会主办，来自工业和信息化部、相关省市工业和信息化主管部门、部分地方国资委、市县政府有关负责同志以及参与开源生态建设的央企、国企、科技龙头企业和中小企业代表；高校、研究机构代表；相关学会、协会、基金会等社会组织代表；开放原子开源基金会的理事、捐赠人、开源贡献人代表等千余人参加。大会将持续至 7 月 24 日。期间，&lt;strong&gt;共设置 26 场分论坛、多场交流会及开源生态交流区等形式，打通政、产、学、研、用、金等多方资源，为开源合作伙伴、项目设置充分的合作交流场景，协同各方形成合力为项目社区及生态发展赋能。&lt;/strong&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362063</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362063</guid>
      <pubDate>Thu, 17 Jul 2025 03:08:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>科研人员违规使用 AI 致泄密，国安部发布警示案例</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;国家安全部官方公众号&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fl3Kjg5L4SgI_rrvQg8NM-w" target="_blank"&gt;发文&lt;/a&gt;公布了一些涉及信息泄露的典型案例，特别是关于科研人员和公职人员在使用人工智能工具时的违规行为。提醒广大工作人员，务必在处理涉密信息时保持高度警惕，避免因一时疏忽而造成严重后果。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="334" src="https://oscimg.oschina.net/oscnet/up-3646e4ef5741e7aaae0c9a9adb3516a6676.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;案例中提到的一名叫小李的是某科研机构研究人员，在撰写一份研究报告时为了图方便，违规使用了一款 AI 应用软件。并擅自将核心数据和实验成果上传至该软件，结果导致该研究领域的涉密信息泄露。事后，小李受到了严厉的处理。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;为避免类似事件再次发生，国家安全部特别提醒公众，规范使用智能工具，防范技术风险。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;在使用生成式人工智能工具时，务必杜绝在互联网或非涉密环境中处理任何涉密信息。此外，使用的人工智能应用软件应从正规渠道下载，避免因使用山寨软件而导致信息安全风险。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362061</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362061</guid>
      <pubDate>Thu, 17 Jul 2025 03:04:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>vivo 正式开源基于 Rust 编写的蓝河操作系统内核</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;7 月 23 日，vivo AI 全球研究院院长周围在 2025 开放原子开源生态大会上宣布&lt;strong&gt;蓝河操作系统内核正式开源&lt;/strong&gt;。蓝河操作系统（BlueOS）是 vivo 自主研发的行业首个从内核到系统框架全栈使用 Rust 语言编写的操作系统。&lt;/p&gt; 
&lt;p&gt;&lt;img height="607" src="https://static.oschina.net/uploads/space/2025/0724/104535_5ou6_2720166.png" width="1280" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ul&gt; 
&lt;/ul&gt; 
&lt;p&gt;据介绍，由 Rust 语言编写的蓝河操作系统内核（以下简称「蓝河内核」），具备安全、轻量、通用的三大特性。&lt;/p&gt; 
&lt;p&gt;在安全方面，蓝河内核全栈使用 Rust 语言开发，也是行业首款适用于嵌入式平台和移动设备的开源 Rust 内核，基于编译期所有权系统，通过所有权、借用、生命周期的静态规则，编译期确保内存安全，而在运行时通过智能指针，灵活管理内存，无额外内存回收性能损耗，让内存安全从被动防御到主动掌控。&lt;/p&gt; 
&lt;p&gt;得益于对基础数据结构高性能低开销的设计，蓝河内核对硬件资源需求低，最小内核内存占用仅 13KB，能够以更低的成本满足各类终端产品的需求。&lt;/p&gt; 
&lt;p&gt;另外，蓝河内核兼容 RISC-V、ARM 等多芯片架构，可满足开发者在不同平台的业务需要，也支持兼容 POSIX 接口的标准库，拓展支持已有的生态，具有出色的通用性。&lt;/p&gt; 
&lt;ul&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img height="1656" src="https://static.oschina.net/uploads/space/2025/0724/104516_JDCC_2720166.png" width="2690" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;蓝河内核具备完整的系统调度、内存管理、文件系统、网络、和设备驱动五大内核能力。在系统调度上，蓝河内核支持主流的调度算法，包括基于时间片轮转调度和基于优先级队列的实时调度；在内存管理方面，将 Rust 语言内存安全核心特性和智能指针相结合保障内存安全，同时支持多种内存分配算法，可适用于不同场景，供开发者基于自己的业务场景灵活选择。&lt;/p&gt; 
&lt;p&gt;蓝河内核的文件系统则采用了经典的层次化结构设计，实现了对文件和 inode 等数据结构的抽象操作，支持快速适配不同的文件系统。&lt;/p&gt; 
&lt;p&gt;而对于网络，蓝河内核支持基础的 TCP/IP 协议栈，能够以阻塞模式和非阻塞模式调用，可支持接入多网卡设备，也基于 Rust Zero-Copy 零拷贝设计，消除数据传输过程的堆分配开销，支持了 socket api。&lt;/p&gt; 
&lt;p&gt;设备管理上，蓝河内核通过硬件抽象等一系列方式，提升了对于 CPU 架构和驱动的兼容能力，支持 Rust 语言开发驱动，也支持兼容已有的 C 语言内核的外设驱动。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;蓝河内核开源代码：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;AtomGit：https://atomgit.com/vivoblueos&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub：https://github.com/vivoblueos&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362058</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362058</guid>
      <pubDate>Thu, 17 Jul 2025 02:48:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>抖音集团基于 Flink 的亿级 RPS 实时计算优化实践</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-299b0ad831e8aa86ddb446bf5085c13099e.jpg" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;**摘要：**本文整理自抖音集团数据工程师陶王飞和羊艺超老师，在&amp;nbsp;Flink&amp;nbsp;Forward&amp;nbsp;Asia&amp;nbsp;2024&amp;nbsp;生产实践（一）专场中的分享主要内容主要分为以下四个部分：&lt;/p&gt; 
 &lt;p&gt;1、现状与痛点&lt;/p&gt; 
 &lt;p&gt;2、链路通用优化&lt;/p&gt; 
 &lt;p&gt;3、业务场景优化&lt;/p&gt; 
 &lt;p&gt;4、未来规划&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;span id="OSC_h2_1"&gt;&lt;/span&gt; 
&lt;h2&gt;01、现状与痛点&lt;/h2&gt; 
&lt;span id="OSC_h3_2"&gt;&lt;/span&gt; 
&lt;h3&gt;1.1&amp;nbsp;业务现状&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//31bfc2c668fca07ca815665dc86dfe11.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;抖音的主要业务场景为视频和直播，实时数据在其中有着广泛应用，如实时大屏、实时预警、对内实时分析（如大盘生态监控）、实时榜单以及为推荐提供实时特征数据等。&lt;/p&gt; 
&lt;p&gt;视频场景流量巨大，晚高峰整体流量达亿级 RPS；直播场景则状态数据量大，因为业务上直播间开关播时间无限制，导致存在许多超长周期存储聚合需求。&lt;/p&gt; 
&lt;span id="OSC_h3_3"&gt;&lt;/span&gt; 
&lt;h3&gt;1.2&amp;nbsp;问题挑战&lt;/h3&gt; 
&lt;p&gt;（1）实时数仓架构图&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//68bd7c819883b62cc8ca999547327819.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;数据源层包括客户端埋点、服务端日志以及业务库数据。数仓的分层使用 Flink 计算，依次为 ODS 层（数据源层）、DWD 层（进行维表关联与简单数据处理）、DWS 层（指标计算）和 APP 层（针对具体应用场景开发），最终将数据输出至下游存储。下游存储依据业务场景选择不同，ToC 场景多使用内部的 KV 存储引擎 Abase，分析型场景及对内产品、平台则使用 ClickHouse 或 Doris，以供下游业务使用。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;（2）问题挑战&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在开发过程中，主要面临着三个问题：&lt;/p&gt; 
&lt;p&gt;其一，由于数据量大且计算复杂，致使链路稳定性差，任务频繁失败；&lt;/p&gt; 
&lt;p&gt;其二，资源消耗巨大，整体计算资源已达 30 万 core；&lt;/p&gt; 
&lt;p&gt;其三，任务异常恢复缓慢，晚高峰时异常恢复时长 30+分钟。&lt;/p&gt; 
&lt;span id="OSC_h2_4"&gt;&lt;/span&gt; 
&lt;h2&gt;02、链路通用优化&lt;/h2&gt; 
&lt;span id="OSC_h3_5"&gt;&lt;/span&gt; 
&lt;h3&gt;2.1&amp;nbsp;通用优化方案&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//a5d8b340159cba4ee3ab177fb8d29116.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在 DWD 明细层，优化关联操作，核心维表在此层关联，而非核心维表及字段则在 DWD 扩展层关联。DWS 层和 APP 层计算直播及视频的天级累计指标。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;（1）分层建模优化&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在分层建模时，将相同实体、不同维度的数据合并为一张维表，以降低下游消费 RPS。在维度关联时，使用 Keyby 提升本地缓存命中率。对于 DWD 大作业，将其拆分成多个小任务灰度上线。在视频大流量场景下，采用宽表模型输出指标，即将所有数据置于一行，存储在一个 Map 中输出；直播场景则使用窄表 Anchor 模型，一条数据对应一个指标一行数据。这两种模型在新增指标时均可实现状态兼容。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;（2）作业性能优化&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在任务层面进行性能优化以降低资源消耗，具体在后文中进行介绍。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;（3）链路保障优化&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;对任务和队列进行分级，并构建全链路血缘来保障分级的准确性。对于高优任务，建立热备链路以及自动化容灾切换能力，提升链路大盘的容灾能力。&lt;/p&gt; 
&lt;span id="OSC_h3_6"&gt;&lt;/span&gt; 
&lt;h3&gt;2.2&amp;nbsp;技术手段优化&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//c28a4dcfac0a889caab651ef1a3670f3.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在抖音短视频业务中，流量呈现高度集中，top20 个的任务占据 40% 以上的计算资源，这些任务不仅成本高，还会降低稳定性，加大运维难度。&lt;/p&gt; 
&lt;p&gt;从头部任务开发分析，部分简单的 Pipeline 任务消耗大量计算资源，可以结合火焰图查找问题的根源。如上图中左下角的火焰图，Calc 算子消耗占比达 56%，这对于非计算型的任务是明显异常情况。经分析是 JIT 及时编译优化失效所致。&lt;/p&gt; 
&lt;p&gt;再结合右侧重新开启优化后的图分析，大任务资源消耗下降约 40%。此外，火焰图中 Calc 占比较大的情况常出现在大并发 Hash 场景，上游并发×下游并发的数据输出队列会导致任务 Shuffle 利用率低，资源消耗较大。我们前后发现了十几项优化项，并推广至其他业务，最终 top 20 任务资源消耗下降约 25%。&lt;/p&gt; 
&lt;span id="OSC_h2_7"&gt;&lt;/span&gt; 
&lt;h2&gt;03、业务场景优化&lt;/h2&gt; 
&lt;span id="OSC_h3_8"&gt;&lt;/span&gt; 
&lt;h3&gt;3.1&amp;nbsp;视频场景痛点优化&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//b679d7cd16af45c7c78f1261ae8bc6d9.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在 DWD 明细层，关联大量维表时面临巨大压力。大流量场景下整体流量达亿级规模，无法直接请求维表，即便开启维表 LRU cache 请求量也达千万级，这带来了成本和稳定性难题。在指标聚合计算时，大流量下解决重复数据问题挑战巨大。&lt;/p&gt; 
&lt;p&gt;（1）大流量维表关联优化&lt;/p&gt; 
&lt;p&gt;①场景分析&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//591030223c0f730869a04f404767f0cc.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;高 QPS 的维表访问导致 Abase 集群压力大，Flink 任务稳定性差，关联维表成为瓶颈。虽提升维表关联缓存命中率可降低外部请求 QPS，但目前缓存命中率已达 90% 以上，提升空间有限。且并非所有维表都超大且时效性要求高，如离线用户维表和百万级监控规则表都相对较小。数仓大量使用 Abase 这种 KV 存储支持大访问 QPS，但当超出其承受能力时，会带来不可控，因此需摆脱对 KV 引擎的依赖，引入新的维表存储方式。&lt;/p&gt; 
&lt;p&gt;②解决方案&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//00d80b09acbb9c9efed83450f3201c2f.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;整体思路是将外部组件访问转化为本地访问，最直接的方式就是将数据加载到内存中完成计算。基于此，我们通过开发 UDF 将 Hive 和 MySQL 中的数据加载到内存中关联，但其并不通用，每次查询 Hive 和 MySQL 语句时需要单独指定，且只能加载少量数据，于是，我们将 UDF 升级为 Flink Broadcast join 功能。&lt;/p&gt; 
&lt;p&gt;该功能设计分为三个模块，以 Hive 为例。分区发现模块通过 Broadcast 算子监测 Hive 分区，发现新分区时，即向下游下发 Watermark 和表元数据信息；数据构建模块的数据读取算子，可配置大并发用于读取 Hive 维表数据；数据分发模块可以将读取的数据分发到各个 TM 中，根据数据量不同有两种分发方式， 即 Broadcast 方式（将全量数据 copy 分发）或根据主键 Hash 分发（适用于数据量较大场景）。&lt;/p&gt; 
&lt;p&gt;在抖音内部场景，该功能支持了千亿级别的维表关联，主要适用于大流量场景下维表关联业务，对维表更新的感知在分钟级以上。功能上线后，它替代了部分 Abase 关联任务，减少约 400 万 QPS ，相关任务在追溯场景下无外部访问瓶颈。&lt;/p&gt; 
&lt;p&gt;（2）大流量幂等计算&lt;/p&gt; 
&lt;p&gt;①场景分析&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//bd261bf4c464cd22ed668087c1c2df7b.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在大流量聚合计算优化方面，由于数据量大，短视频的小时和天级指标从分钟中间层聚合，这会导致分钟聚合输出有重复数据、乱序数据甚至回撤数据。如果直接通过先取 max 等方式聚合，其计算成本高且会引入回撤流问题，导致原本递增的埋点指标下降。以分钟向小时聚合为例，小时任务需要每一分钟的最后一条数据。&lt;/p&gt; 
&lt;p&gt;②解决方案&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//f515a4786be013304ee91c7ff5a26886.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;针对以上问题，我们引入了 Bucket 的思路，即每一分钟维护一个 Bucket，保留最后一条数据，新数据到来时，将其对应的 Bucket 与原本的数据相较，仅下发正序递增数据。这样，从分钟级向小时级聚合时，即使分钟级流量达百万 RPS，最终也只有 60 个 Bucket。&lt;/p&gt; 
&lt;p&gt;基于此简化模型，如上图左下角展示的分钟数据输出，第一列是分钟值，即 Bucket key；第二列是时间位移，用于 Bucket 的时间比较；第三列是指标值。第一、二条数据均为 58 分钟，因此，其属于同一个 Bucket，数据也是正序到来的，因此，Bucket 记录为 30 秒；指标值为 100 的数据，第三条数据正常输出，第四条和第五条数据存在乱序，40 秒的数据先到，20 秒的数据后到，因此，Bucket 只记录 40 秒的数据，在 20 秒的数据进入后不再更新。这样，通过 Bucket 机制可有效处理重复下发、乱序和回撤数据，不影响小时及天指标聚合结果。&lt;/p&gt; 
&lt;p&gt;③性能优化&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//f0385e2fac946c801ad7ba46390c9b47.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;基于 Bucket 计算资源消耗仍较大，通过火焰图分析，发现 state 的序列化环节和 GC 环节占比较大，表明在状态和计算上仍有优化空间。我们从数据结构和业务两方面入手。&lt;/p&gt; 
&lt;p&gt;其一，优化 Bucket 结构，将多层嵌套结构改为两个数组，两个数组的长度等于 Bucket 的长度，无需要存储 Bucket key，按照数组的顺序取对应 Bucket。一个数组存储时间戳位移，将 long 格式的时间戳存储为 int 类型（减少存储占用），另一个数组通过字符串拼接指标值，将原本的 Map 中指标的 value 拼接成一行，节省 state 中的空间占用。&lt;/p&gt; 
&lt;p&gt;其二，进行 Bucket 时间压缩，从分钟向天级聚合最多 1440 个 Bucket，根据业务实际情况，将六个小时之前的 Bucket 压缩，Bucket 数量从 1440 个降至 378 个，降低约 70%。&lt;/p&gt; 
&lt;p&gt;完成这些优化后，整体资源消耗下降约 30%。&lt;/p&gt; 
&lt;span id="OSC_h3_9"&gt;&lt;/span&gt; 
&lt;h3&gt;3.2&amp;nbsp;直播场景痛点问题&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//8ff92760d26c1e1097ebc17a1402f5b0.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;直播场景的痛点问题主要分为计算大状态和回溯大流量两类。&lt;/p&gt; 
&lt;p&gt;首先是直播间场次聚合计算大状态。抖音直播中，直播间最长可开播 30 天，但目前 Flink 作业的 DWS 和 APP 层只计算开播后七天状态的数据，即 state TTL 为七天。其原因是目前单作业资源消耗大，最高已达 2000 核，状态可达 18T，稳定性不佳，无法简单横向扩展资源解决问题。而业务有查看直播间 30 天累计指标的诉求。&lt;/p&gt; 
&lt;p&gt;其次是冷启动和故障回溯大流量。在此场景下，回溯数据从小时到天级不等，DWS 作业向下游下发数据时会重复且大量下发，影响下游 MQ 及 Redis 等 QA 存储组件稳定性。此外，Flink 作业运行虽有资源投入，但回溯数据仍较慢，从业务视角看，存在数据恢复慢和性能指标上线周期长的问题。&lt;/p&gt; 
&lt;p&gt;针对这些问题，我们提出了相应的解决方案。&lt;/p&gt; 
&lt;p&gt;（1）大状态优化&lt;/p&gt; 
&lt;p&gt;①场景分析&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//56fdd1dd529ad5affae89677e9e98656.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;对于大状态优化场景，直播间开关播时间和时长不固定，最短不到分钟级，最长 30 天，平均在小时级别。分析 Flink 作业中不同开播时长的状态大小占比发现，state TTL 为七天时，开播时长一天的直播间状态大小占 98%，这部分多存储六天；大于一天小于七天的占 1%，也存在多存情况；大于八天的仅占 0.5‰，存在少存情况。该问题的核心是状态固定的 TTL 与直播间动态的 TTL 矛盾，导致 99% 的状态多存，0.5‰状态少存。&lt;/p&gt; 
&lt;p&gt;解决思路是对齐两者 TTL，实现直播间关播后删除状态。&lt;/p&gt; 
&lt;p&gt;②方案设计&lt;/p&gt; 
&lt;p&gt;最初设计了两种方案。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//a9521b265a30d6d8e8de8f9de9f4645d.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;第一种基于 Retract 机制删除状态。在 Flink 作业中，同时消费直播间流量数据和关播数据，将两条数据进行聚合。按直播间 ID 分组聚合计算，关播 source 收到数据后向下游聚合算子发送 delete 消息，下游的聚合算子在接受到该消息后删除状态。&lt;/p&gt; 
&lt;p&gt;但该方案存在问题，用户开发成本高，删除状态逻辑与 SQL 逻辑强耦合，导致改造作业时开发成本大；扩展性差，仅适用于 group key 为直播间 ID 的场景，而实际业务中 group key 可能包含更多内容，如用户画像或主播画像等，则不适用。&lt;/p&gt; 
&lt;p&gt;总结问题后，发现其核心是删除状态逻辑与 SQL 逻辑强耦合，进而设计了第二种方案作为 TTL CompactionFilter 方案的扩展，即自定义 RocksDB CompactionFilter 方案。两者执行时机相同，自定义 RocksDB CompactionFilter 方案支持通过 Java UDF 为指定状态设定 CompactionFilter 。两者的区别在于，TTL CompactionFilter 执行时解析状态中的时间戳判断状态是否删除，而自定义方案在 RocksDB 执行时，通过 JNI 将状态数据传给 Flink TM，解析直播间 ID 作为 CompactionFilter 入参，访问直播间 Abase 维表，判断是否关播，若关播，则 CompactionFilter 返回 true，删除状态。此方案实现了直播间关播后的状态删除，且与 SQL 逻辑完全解耦，解决了方案一中的问题。&lt;/p&gt; 
&lt;p&gt;③落地方案&lt;/p&gt; 
&lt;p&gt;与&amp;nbsp;Flink&amp;nbsp;架构组共建，实现了该方案的落地。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//4a72afb9ccd8582789e772b28472bfa9.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Flink 架构组在 RocksDB 层面支持 CompactionFilter 能力，在 SQL 层面支持用户为指定聚合算子指定 CompactionFilter。如代码示例所示，设置状态 TTL 为 30 天，通过 ADD RESOURCES 语句引入 CompactionFilter 的 jar 包，通过 SQL hint 指定聚合算子的 CompactionFilter，参数包括 path（路径） 和 filed（直播间 ID）。&lt;/p&gt; 
&lt;p&gt;实现过程中对性能问题进行了优化，如 CompactionFilter 查询性能优化，将实时访问 Abase 优化为批量加载关播直播间数据到本地，判断是否关播，避免 Compaction 执行过程中， CompactionFilter 访问外部组件查询阻塞，减少 CP 的时长；Cache 选择优化，将本地存储关播直播间的 cache 从内存优化到磁盘，降低 GC 时长；CompactionFilter 调用频次优化，设定 state 存储时长超过两天才调用 CompactionFilter，减少未关播直播间频繁调用导致的 CPU 浪费，同时在 RocksDB C++侧缓存，直播间开关播的结果（CompactionFilter 结果），利用 RocksDB 存储机制，将直播间 ID 放在 group by 语句最前面，顺序存储相同 ID 的状态数据，复用 CompactionFilter 调用结果，避免 JNI 调用带来的性能损耗。&lt;/p&gt; 
&lt;p&gt;通过该方案，业务上支持了直播间 30 天累计指标，技术上直播间场次作业状态平均下降 60%，CPU 资源使用下降 70%。&lt;/p&gt; 
&lt;p&gt;（2）大流量回溯优化&lt;/p&gt; 
&lt;p&gt;①场景分析&lt;/p&gt; 
&lt;p&gt;分析无&amp;nbsp;lag&amp;nbsp;场景和追 lag&amp;nbsp;场景下作业期望目标。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//23ffaab6834affe69f04a03720743f1d.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;无 lag 场景追求低延迟，数据实时产出，期望数据处理模式为流处理；追 lag 场景追求短时间内快速恢复，数据高吞吐，期望处理模式为批处理。&lt;/p&gt; 
&lt;p&gt;但当前 Flink 流处理作业仅能以流处理运行，设置 Minibatch 为 30 秒，在无 lag 场景可行，而在追 lag 场景则存在吞吐低、恢复慢的问题，与预期高吞吐目标不符。&lt;/p&gt; 
&lt;p&gt;为解决此问题，分析 Flink 流处理和批处理在引擎实现上的差异，在满足 Flink 流处理低延迟特性的同时，实现 Flink 批处理的高吞吐。流处理通过 Minibatch 机制保证低延迟，但其 RocksDB 随机访问和 Retract 机制限制了吞吐；批处理虽有高延迟，但通过 sort 排序处理且无 Retract 机制，吞吐较高。因此，我们提出在流作业中动态监测消费积压情况，判断作业对高吞吐或低延迟的倾向性，在当前算子引入 sort 排序算子和动态调整 Minibatch 大小的能力，实现流批执行模式的动态切换。&lt;/p&gt; 
&lt;p&gt;②方案设计&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//d1272c4b617b4e584d685b0d1961502a.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;该方案核心步骤包括积压检测、检测结果传递和动态启用 sorter 算子并调整 Minibatch 大小。Flink 作业运行时，Source 算子动态监测 lag size；当 lag size 超过指定值时，向下游算子发送数据时，标记 isBackLog 为 true，聚合算子接收数据后解析该字段，若为 true，则认为当前作业倾向于批处理，启用 sorter，将 Minibatch 的大小间隔调整为 CP 的间隔。&lt;/p&gt; 
&lt;p&gt;此方案实现了流作业执行过程中流处理和批处理模式的动态切换，且作业的 DAG 不变，状态完全兼容。&lt;/p&gt; 
&lt;p&gt;③落地方案&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//258cc68cab26724db1a0d2f863dcd020.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Flink 流批倾向性指标有三个，即自动检测积压状态时间间隔、触发切换批模式处理的平均 lag size 上限、触发切换流处理平均 lag size 下限。SQL 使用案例如上图左下角所示：首先，在运行参数中设置 backlog mode 开启，然后在 source 算子中指定以上三个参数，进而实现 Flink 作业流批融合的处理。&lt;/p&gt; 
&lt;p&gt;通过该方案，技术上，追 lag 场景回溯数据结果下发量减少，下游组件稳定性提升；业务上，批模式追溯速度比流模式提升一倍。&lt;/p&gt; 
&lt;span id="OSC_h2_10"&gt;&lt;/span&gt; 
&lt;h2&gt;04、未来规划&lt;/h2&gt; 
&lt;p&gt;在抖音一级 RPS 场景下，未来优化分为通用优化和个性化场景优化。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//ccf7f6a093cc54cd46625cbd98349751.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h3_11"&gt;&lt;/span&gt; 
&lt;h3&gt;4.1&amp;nbsp;通用优化&lt;/h3&gt; 
&lt;p&gt;作业重启时内存 localcache 失效导致的缓存穿透优化；使用 Paimon 维表能力减少对 Redis 等 K-V 存储的请求，使用 PaimonWithMQ 能力减少 MQ dump 派作业，节约资源；丰富 AutoScaling 的资源优化规则，获取更多收益。&lt;/p&gt; 
&lt;span id="OSC_h3_12"&gt;&lt;/span&gt; 
&lt;h3&gt;4.2&amp;nbsp;个性化场景优化&lt;/h3&gt; 
&lt;p&gt;解决多任务比值类指标分子分母更新快慢不一致导致的波动明显问题；实现 broadcast join 支持状态的增量加载，针对千亿级维表，拒绝全量加载，而是定时加载仅变化的增量数据，提升作业稳定性；进行内存优化。&lt;/p&gt; 
&lt;span id="OSC_h3_13"&gt;&lt;/span&gt; 
&lt;h3&gt;更多内容&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//6041760444b05a21431ac6efcf7680be.jpg" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;span id="OSC_h3_14"&gt;&lt;/span&gt; 
&lt;h3&gt;活动推荐&lt;/h3&gt; 
&lt;p&gt;阿里云基于 Apache Flink 构建的企业级产品-实时计算 Flink 版现开启活动： 新用户复制点击下方链接或者扫描二维码即可 0 元免费试用 Flink + Paimon &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ffree.aliyun.com%2F%3Futm_content%3Dg_1000395379%26productCode%3Dsc" rel="nofollow" target="_blank"&gt;实时计算 Flink 版&lt;/a&gt;（3000CU*小时，3 个月内） 了解活动详情：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ffree.aliyun.com%2F%3Futm_content%3Dg_1000395379%26productCode%3Dsc" rel="nofollow" target="_blank"&gt;https://free.aliyun.com/?utm_content=g_1000395379&amp;amp;productCode=sc&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//9dca849c94b7e32b3cff482a77660ce0.jpg" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/2828172/blog/18685623</link>
      <guid isPermaLink="false">https://my.oschina.net/u/2828172/blog/18685623</guid>
      <pubDate>Thu, 17 Jul 2025 02:34:00 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>白宫发布《美国 AI 行动计划》</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;白宫发布了名为&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.whitehouse.gov%2Farticles%2F2025%2F07%2Fwhite-house-unveils-americas-ai-action-plan%2F" target="_blank"&gt;《赢得 AI 竞赛：美国 AI 行动计划》（Winning the AI Race: America’s AI Action Plan）&lt;/a&gt;的战略文件，以保证美国毫无争议地成为全球 AI 霸主。&lt;/p&gt; 
&lt;p&gt;&lt;img height="595" src="https://static.oschina.net/uploads/space/2025/0724/103055_nOWT_2720166.png" width="521" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0724/103116_zfCg_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;该计划主要有三大支柱，加速 AI 创新、构建 AI 基础设施以及主导国际外交与安全，涵盖 90 多项具体行政命令。其中，废除限制 AI 创新监管条例，加速发电场、水资源、半导体芯片等基础设施建设，这对于像 OpenAI、微软、亚马逊、谷歌、Meta 等 AI 巨头来说非常有利。&lt;/p&gt; 
&lt;p&gt;白宫在 28 页的 AI 行动计划中特别要求，凡联邦政府采购的大语言模型必须「客观、不受自上而下意识形态影响」。此外，该计划还把中国列为主要竞争对手，希望在技术创新、模型开源、基础设施等方面领先。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362050</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362050</guid>
      <pubDate>Thu, 17 Jul 2025 02:31:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>蜻蜓 FM 开源 SmartXPlayer 音频播放组件</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;由蜻蜓 FM 研发的音频播放组件「SmartXPlayer」近日已正式开源并上线 OpenHarmony 三方库中心仓。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;根据&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FtWpwyDSCWemr7NCevxiiYw%3Fpoc_token%3DHHyUgWijpdpTgHnwJ0S3aKNInFpLFta7--q5a3AQ" target="_blank"&gt;介绍&lt;/a&gt;，作为一款专为鸿蒙多端场景打造的音频播放引擎，SmartXPlayer 基于鸿蒙系统分布式能力和多线程架构，提供高性能、易集成的音频播放能力支持，助力开发者高效构建更顺滑、更智能、更便捷的音频播放体验。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="273" src="https://oscimg.oschina.net/oscnet/up-86f4c94cb3161e141e4caf4a5965a5e4178.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;当前，随着音频内容和智能设备的普及，传统播放器在多端适配、分布式投播、主线程阻塞等方面存在开发难、效率低、体验差等痛点。在这一背景下，SmartXPlayer 应运而生，以组件化、跨线程、高扩展的技术路径，有效提升鸿蒙平台音频应用开发效率与终端播放体验。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;SmartXPlayer 基于蜻蜓 FM 实际业务场景研发打磨，在多项关键能力上具备优异表现：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;span style="color:#000000"&gt;跨线程播放架构，提升系统响应效率&lt;/span&gt;&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;SmartXPlayer 首创子线程播放技术，通过引入 ThreadWorker 机制，播放任务在子线程处理，主线程专注 UI 渲染与状态管理，将播放性能提升 50%，有效缓解主线程阻塞带来的卡顿、闪退等问题。&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;span style="color:#000000"&gt;支持分布式投播与后台播放，适配多端设备&lt;/span&gt;&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;SmartXPlayer 内置的 SXCastPlayer 实现了与本地播放器一致的标准播放接口，开发者无需为投播功能单独学习新接口。同时，它能实时监听设备连接状态变化，当检测到投播需求时，播放器会自动将内部的播放逻辑从本地播放器切换为 SXCastPlayer，从而实现「本地播放」到「跨设备投播」的无缝衔接。此外，它还具备后台播放与状态同步能力，实现鸿蒙「全场景互联」下的流畅音频体验。&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;span style="color:#000000"&gt;高度抽象 API，开发门槛低、接入效率高&lt;/span&gt;&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;SmartXPlayer 组件接口设计高度抽象，支持一行代码实现多端投播，仅需少量代码即可快速实现初始化与播放控制，开发效率大幅提升。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;据蜻蜓 FM 内部估算，在实际应用中，实现同样的音频播放效果，SmartXPlayer 相比传统方案能够将开发时长由 2 周缩短至 2-3 天，代码量减少 60%，维护成本降低 50%，用户体验显著提升。目前该方案已在蜻蜓 FM 鸿蒙版和蜻蜓电台元服务中集成使用, 整体表现优异，并计划在未来支持更多音频内容形态与播放场景的适配与扩展。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362049</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362049</guid>
      <pubDate>Thu, 17 Jul 2025 02:25:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>GitHub Spark 发布公测预览版，通过自然语言构建全栈应用</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;GitHub 宣布其新产品 GitHub Spark 已面向 Copilot Pro+订阅用户开启&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.blog%2Fchangelog%2F2025-07-23-github-spark-in-public-preview-for-copilot-pro-subscribers%2F" target="_blank"&gt;公测&lt;/a&gt;，该产品旨在让开发者通过自然语言在数分钟内完成从想法到部署的全栈智能应用程序的构建和发布，无需进行环境设置或配置。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0724/101944_bsPs_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Spark 的核心功能由 Claude Sonnet 4 驱动，允许用户通过描述想法来构建包含前端和后端的应用。该平台内置了数据存储、LLM 推理、托管、部署和 GitHub 身份验证等功能。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-d9febfe928d296469d6ce6186d282669ebe.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;开发者可以在应用中添加由 OpenAI、Meta、DeepSeek、xAI 等公司模型驱动的 AI 功能，而无需管理 API 密钥。应用可以通过一键点击进行部署，并自动创建一个包含 GitHub Actions 和 Dependabot 的 GitHub 仓库，确保所有内容保持同步。&lt;/p&gt; 
&lt;p&gt;开发者可以通过自然语言、可视化编辑控件或在集成 GitHub Copilot 代码补全的编辑器中进行编码来迭代他们的想法。&lt;/p&gt; 
&lt;p&gt;此外，用户可以直接从 Spark 中打开一个 codespace，以使用 Copilot agent 模式进行迭代，或将 issue 分配给 Copilot coding agent。Copilot Pro+订阅用户可直接访问 Spark，其使用会消耗 GitHub Copilot 计划中包含的 premium requests。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362046</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362046</guid>
      <pubDate>Thu, 17 Jul 2025 02:20:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Qwen3 系列模型迎来新第三方部署和价格特惠</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;阿里巴巴的 Qwen3 系列模型近期在多个平台获得部署并在官方平台开启了价格特惠。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0724/100552_pcPY_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Cerebras 宣布推出 Qwen3-235B 模型，实现了每秒 1500 个 token 的推理速度，目前可进行有限制的免费体验。&lt;/p&gt; 
&lt;p&gt;阿里云的通义灵码 IDE 已集成 Qwen3-Coder，并去掉了原有的 DeepSeek 模型。GMI inference cloud 也上线了 Qwen3 Coder 480B A35B Instruct FP8 版本，定价为输入$1.00/M Tokens，输出$2.00/M Tokens。&lt;/p&gt; 
&lt;p&gt;阿里云百炼平台宣布对 Qwen3-Coder-Plus 进行为期一个月的限时降价，并进一步对上下文缓存功能进行说明：「上下文缓存的命中概率并不是 100%，即使是上下文完全一致的请求，也存在无法命中的概率，命中概率依据系统判断而定。」&lt;/p&gt; 
&lt;table style="display:table; text-align:left"&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th style="border-color:rgba(36, 4, 4, 0.4); height:auto; text-align:left"&gt; &lt;p&gt;&lt;span&gt;Token 数量&lt;/span&gt;&lt;/p&gt; &lt;/th&gt; 
   &lt;th style="border-color:rgba(36, 4, 4, 0.4); height:auto; text-align:left"&gt; &lt;p&gt;&lt;span&gt;输入成本 (每千 Token)&lt;/span&gt;&lt;/p&gt; &lt;/th&gt; 
   &lt;th style="border-color:rgba(36, 4, 4, 0.4); height:auto; text-align:left"&gt; &lt;p&gt;&lt;span&gt;输出成本 (每千 Token)&lt;/span&gt;&lt;/p&gt; &lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td style="border-bottom-color:rgba(36, 4, 4, 0.4); border-bottom-left-radius:0px; border-bottom-right-radius:0px; border-bottom-style:solid; border-bottom-width:1px; border-left-color:rgba(36, 4, 4, 0.4); border-left-style:solid; border-left-width:1px; border-right-color:rgba(36, 4, 4, 0.4); border-right-style:solid; border-right-width:1px; border-top-color:rgba(36, 4, 4, 0.4); border-top-left-radius:0px; border-top-right-radius:0px; border-top-style:solid; border-top-width:1px; height:auto; text-align:left"&gt;&lt;strong style="color:#1f0c03"&gt;&lt;span&gt;&lt;span&gt;0-32K&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td style="border-bottom-color:rgba(36, 4, 4, 0.4); border-bottom-left-radius:0px; border-bottom-right-radius:0px; border-bottom-style:solid; border-bottom-width:1px; border-left-color:rgba(36, 4, 4, 0.4); border-left-style:solid; border-left-width:1px; border-right-color:rgba(36, 4, 4, 0.4); border-right-style:solid; border-right-width:1px; border-top-color:rgba(36, 4, 4, 0.4); border-top-left-radius:0px; border-top-right-radius:0px; border-top-style:solid; border-top-width:1px; height:auto; text-align:left"&gt; &lt;p&gt;&lt;span&gt;&lt;span&gt;0.004 元&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-bottom-color:rgba(36, 4, 4, 0.4); border-bottom-left-radius:0px; border-bottom-right-radius:0px; border-bottom-style:solid; border-bottom-width:1px; border-left-color:rgba(36, 4, 4, 0.4); border-left-style:solid; border-left-width:1px; border-right-color:rgba(36, 4, 4, 0.4); border-right-style:solid; border-right-width:1px; border-top-color:rgba(36, 4, 4, 0.4); border-top-left-radius:0px; border-top-right-radius:0px; border-top-style:solid; border-top-width:1px; height:auto; text-align:left"&gt; &lt;p&gt;&lt;span&gt;&lt;span&gt;0.016 元&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="border-bottom-color:rgba(36, 4, 4, 0.4); border-bottom-left-radius:0px; border-bottom-right-radius:0px; border-bottom-style:solid; border-bottom-width:1px; border-left-color:rgba(36, 4, 4, 0.4); border-left-style:solid; border-left-width:1px; border-right-color:rgba(36, 4, 4, 0.4); border-right-style:solid; border-right-width:1px; border-top-color:rgba(36, 4, 4, 0.4); border-top-left-radius:0px; border-top-right-radius:0px; border-top-style:solid; border-top-width:1px; height:auto; text-align:left"&gt;&lt;strong style="color:#1f0c03"&gt;&lt;span&gt;&lt;span&gt;32K-128K&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td style="border-bottom-color:rgba(36, 4, 4, 0.4); border-bottom-left-radius:0px; border-bottom-right-radius:0px; border-bottom-style:solid; border-bottom-width:1px; border-left-color:rgba(36, 4, 4, 0.4); border-left-style:solid; border-left-width:1px; border-right-color:rgba(36, 4, 4, 0.4); border-right-style:solid; border-right-width:1px; border-top-color:rgba(36, 4, 4, 0.4); border-top-left-radius:0px; border-top-right-radius:0px; border-top-style:solid; border-top-width:1px; height:auto; text-align:left"&gt; &lt;p&gt;&lt;span&gt;&lt;span&gt;0.0042 元 (原价 0.006 元的&amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;strong style="color:#1f0c03"&gt;&lt;span&gt;&lt;span&gt;7 折&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-bottom-color:rgba(36, 4, 4, 0.4); border-bottom-left-radius:0px; border-bottom-right-radius:0px; border-bottom-style:solid; border-bottom-width:1px; border-left-color:rgba(36, 4, 4, 0.4); border-left-style:solid; border-left-width:1px; border-right-color:rgba(36, 4, 4, 0.4); border-right-style:solid; border-right-width:1px; border-top-color:rgba(36, 4, 4, 0.4); border-top-left-radius:0px; border-top-right-radius:0px; border-top-style:solid; border-top-width:1px; height:auto; text-align:left"&gt; &lt;p&gt;&lt;span&gt;&lt;span&gt;0.0168 元 (原价 0.024 元的&amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;strong style="color:#1f0c03"&gt;&lt;span&gt;&lt;span&gt;7 折&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="border-bottom-color:rgba(36, 4, 4, 0.4); border-bottom-left-radius:0px; border-bottom-right-radius:0px; border-bottom-style:solid; border-bottom-width:1px; border-left-color:rgba(36, 4, 4, 0.4); border-left-style:solid; border-left-width:1px; border-right-color:rgba(36, 4, 4, 0.4); border-right-style:solid; border-right-width:1px; border-top-color:rgba(36, 4, 4, 0.4); border-top-left-radius:0px; border-top-right-radius:0px; border-top-style:solid; border-top-width:1px; height:auto; text-align:left"&gt;&lt;strong style="color:#1f0c03"&gt;&lt;span&gt;&lt;span&gt;128K-256K&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td style="border-bottom-color:rgba(36, 4, 4, 0.4); border-bottom-left-radius:0px; border-bottom-right-radius:0px; border-bottom-style:solid; border-bottom-width:1px; border-left-color:rgba(36, 4, 4, 0.4); border-left-style:solid; border-left-width:1px; border-right-color:rgba(36, 4, 4, 0.4); border-right-style:solid; border-right-width:1px; border-top-color:rgba(36, 4, 4, 0.4); border-top-left-radius:0px; border-top-right-radius:0px; border-top-style:solid; border-top-width:1px; height:auto; text-align:left"&gt; &lt;p&gt;&lt;span&gt;&lt;span&gt;0.005 元 (原价 0.01 元的&amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;strong style="color:#1f0c03"&gt;&lt;span&gt;&lt;span&gt;5 折&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-bottom-color:rgba(36, 4, 4, 0.4); border-bottom-left-radius:0px; border-bottom-right-radius:0px; border-bottom-style:solid; border-bottom-width:1px; border-left-color:rgba(36, 4, 4, 0.4); border-left-style:solid; border-left-width:1px; border-right-color:rgba(36, 4, 4, 0.4); border-right-style:solid; border-right-width:1px; border-top-color:rgba(36, 4, 4, 0.4); border-top-left-radius:0px; border-top-right-radius:0px; border-top-style:solid; border-top-width:1px; height:auto; text-align:left"&gt; &lt;p&gt;&lt;span&gt;&lt;span&gt;0.02 元 (原价 0.04 元的&amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;strong style="color:#1f0c03"&gt;&lt;span&gt;&lt;span&gt;5 折&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="border-bottom-color:rgba(36, 4, 4, 0.4); border-bottom-left-radius:0px; border-bottom-right-radius:0px; border-bottom-style:solid; border-bottom-width:1px; border-left-color:rgba(36, 4, 4, 0.4); border-left-style:solid; border-left-width:1px; border-right-color:rgba(36, 4, 4, 0.4); border-right-style:solid; border-right-width:1px; border-top-color:rgba(36, 4, 4, 0.4); border-top-left-radius:0px; border-top-right-radius:0px; border-top-style:solid; border-top-width:1px; height:auto; text-align:left"&gt;&lt;strong style="color:#1f0c03"&gt;&lt;span&gt;&lt;span&gt;256K-1M&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td style="border-bottom-color:rgba(36, 4, 4, 0.4); border-bottom-left-radius:0px; border-bottom-right-radius:0px; border-bottom-style:solid; border-bottom-width:1px; border-left-color:rgba(36, 4, 4, 0.4); border-left-style:solid; border-left-width:1px; border-right-color:rgba(36, 4, 4, 0.4); border-right-style:solid; border-right-width:1px; border-top-color:rgba(36, 4, 4, 0.4); border-top-left-radius:0px; border-top-right-radius:0px; border-top-style:solid; border-top-width:1px; height:auto; text-align:left"&gt; &lt;p&gt;&lt;span&gt;&lt;span&gt;0.01 元 (原价 0.02 元的&amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;strong style="color:#1f0c03"&gt;&lt;span&gt;&lt;span&gt;5 折&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-bottom-color:rgba(36, 4, 4, 0.4); border-bottom-left-radius:0px; border-bottom-right-radius:0px; border-bottom-style:solid; border-bottom-width:1px; border-left-color:rgba(36, 4, 4, 0.4); border-left-style:solid; border-left-width:1px; border-right-color:rgba(36, 4, 4, 0.4); border-right-style:solid; border-right-width:1px; border-top-color:rgba(36, 4, 4, 0.4); border-top-left-radius:0px; border-top-right-radius:0px; border-top-style:solid; border-top-width:1px; height:auto; text-align:left"&gt; &lt;p&gt;&lt;span&gt;&lt;span&gt;0.1 元 (原价 0.2 元的&amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;strong style="color:#1f0c03"&gt;&lt;span&gt;&lt;span&gt;5 折&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;详情：https://help.aliyun.com/zh/model-studio/qwen3-coder-plus-price-drop&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362044</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362044</guid>
      <pubDate>Thu, 17 Jul 2025 02:06:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>谷歌母公司发布 Q2 财报：全年资本支出飙升至 850 亿美元</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;div style="text-align:start"&gt; 
 &lt;p style="color:#222222; margin-left:0; margin-right:0"&gt;谷歌母公司 Alphabet 周三公布季度营收超出华尔街预期，得益于新推出的 AI 功能以及稳定的数字广告市场。公司还表示，将今年的资本支出计划从原本的大约 750 亿美元上调至约 850 亿美元。Alphabet A 股美股盘后下跌 2.8%，但随后回弹，一度跳涨 3.4%。&lt;/p&gt; 
&lt;/div&gt; 
&lt;div style="text-align:start"&gt; 
 &lt;p style="color:#222222; margin-left:0; margin-right:0"&gt;以下是 Alphabet 二季度财报要点：&lt;/p&gt; 
&lt;/div&gt; 
&lt;div style="text-align:start"&gt; 
 &lt;p style="color:#222222; margin-left:0; margin-right:0"&gt;&lt;strong&gt;主要财务数据：&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div style="text-align:start"&gt; 
 &lt;blockquote&gt; 
  &lt;p style="color:#222222; margin-left:0; margin-right:0"&gt;&lt;strong&gt;营收：&lt;/strong&gt;Alphabet 二季度营收 964.3 亿美元，同比增长 14%，高于分析师预期的 939.7 亿美元。&lt;/p&gt; 
  &lt;p style="color:#222222; margin-left:0; margin-right:0"&gt;&lt;strong&gt;调整后营收：&lt;/strong&gt;剔除合作伙伴分成后的第二季度销售额为 817 亿美元，高于分析师平均预期 796 亿美元。&lt;/p&gt; 
  &lt;p style="color:#222222; margin-left:0; margin-right:0"&gt;&lt;strong&gt;每股收益：&lt;/strong&gt;Alphabet 二季度每股收益 2.31 美元，同比增长 22%，高于分析师预期的 2.18 美元。&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/div&gt; 
&lt;div style="text-align:start"&gt; 
 &lt;p style="color:#222222; margin-left:0; margin-right:0"&gt;&lt;img height="425" src="https://oscimg.oschina.net/oscnet/up-184de4a234b4e0f59adc392a84a16afd3ea.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
 &lt;p style="color:#222222; margin-left:0; margin-right:0"&gt;&lt;strong&gt;分业务数据：&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div style="text-align:start"&gt; 
 &lt;blockquote&gt; 
  &lt;p style="color:#222222; margin-left:0; margin-right:0"&gt;&lt;strong&gt;谷歌云营收：&lt;/strong&gt;谷歌云营收第二季度为 136 亿美元，同比增长 32%，高于市场预期的 131 亿美元。&lt;/p&gt; 
  &lt;ul style="margin-left:0; margin-right:0"&gt; 
   &lt;li&gt;&lt;strong&gt;云计算利润：&lt;/strong&gt;谷歌云计算部门本季度运营利润为 28.3 亿美元，远超分析师预期的 22.5 亿美元。&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p style="color:#222222; margin-left:0; margin-right:0"&gt;&lt;strong&gt;广告总营收：&lt;/strong&gt;Alphabet 广告总营收 713 亿美元，同比增长 10.4%。&lt;/p&gt; 
  &lt;ul style="margin-left:0; margin-right:0"&gt; 
   &lt;li&gt;&lt;strong&gt;搜索业务营收：&lt;/strong&gt;搜索业务营收第二季度为 541 亿美元，同比增长 11.7%，高于分析师预期的 540 亿美元。&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;YouTube 广告收入：&lt;/strong&gt;YouTube 广告营收为 98 亿美元，略高于预期的 95 亿美元。&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/blockquote&gt; 
&lt;/div&gt; 
&lt;div style="text-align:start"&gt; 
 &lt;p style="color:#222222; margin-left:0; margin-right:0"&gt;Alphabet 股价自 4 月份发布上一份财报以来已上涨超过 18%。周三公布财报后，Alphabet A 股盘后交易中一度下跌超过 2.8%，但随后一度反弹 3.4%，涨幅后收窄至 2% 左右。&lt;/p&gt; 
&lt;/div&gt; 
&lt;div style="text-align:start"&gt; 
 &lt;h2 style="margin-left:0; margin-right:0"&gt;&lt;strong&gt;今年资本支出提高至 850 亿美元，预计 2026 年继续增加&lt;/strong&gt;&lt;/h2&gt; 
&lt;/div&gt; 
&lt;div style="text-align:start"&gt; 
 &lt;p style="color:#222222; margin-left:0; margin-right:0"&gt;&lt;strong&gt;谷歌母公司表示，其全年资本支出将提高 13%，达到 850 亿美元，而不是今年早些时候预测的 750 亿美元。相比之下，2024 年为 525 亿美元。&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div style="text-align:start"&gt; 
 &lt;p style="color:#222222; margin-left:0; margin-right:0"&gt;&lt;strong&gt;媒体分析，尽管 Alphabet 公司公布的营收好于预期并创下历史纪录，但由于 2025 年的资本支出将高于此前预测，这让公司在 AI 竞赛中的投资合理性面临更大压力。公司首席财务官 Anat Ashkenazi 在周三财报电话会上表示，公司预计 2026 年还将进一步增加资本支出。&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div style="text-align:start"&gt; 
 &lt;p style="color:#222222; margin-left:0; margin-right:0"&gt;谷歌首席执行官桑达尔·皮查伊（Sundar Pichai）和其他科技高管过去几年在 AI 开发上投入了数百亿美元，这是整个 AI 热潮的一部分。大部分资金都用于建设新的数据中心，以开发和运行 AI 模型。&lt;/p&gt; 
&lt;/div&gt; 
&lt;div style="text-align:start"&gt; 
 &lt;p style="color:#222222; margin-left:0; margin-right:0"&gt;皮查伊表示：&lt;/p&gt; 
&lt;/div&gt; 
&lt;div style="text-align:start"&gt; 
 &lt;blockquote&gt; 
  &lt;p style="color:#222222; margin-left:0; margin-right:0"&gt;「人工智能正在积极影响公司业务的每一个部分，推动整体强劲发展。我们正站在 AI 前沿，并以惊人的速度推进产品发布。AI 正在对业务的每一个环节产生积极影响，带来强劲的增长势头。」&lt;/p&gt; 
  &lt;p style="color:#222222; margin-left:0; margin-right:0"&gt;「搜索业务实现了两位数的收入增长，我们的新功能，比如 AI 概览（AI Overviews）和 AI 模式（AI Mode），表现良好。YouTube 和订阅服务继续保持强劲表现。云计算业务在收入、订单积压和盈利能力方面都取得强劲增长，其年化收入目前已超过 500 亿美元。」&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/div&gt; 
&lt;div style="text-align:start"&gt; 
 &lt;h2 style="margin-left:0; margin-right:0"&gt;巨额资本支出或将用于人才争夺战&lt;/h2&gt; 
&lt;/div&gt; 
&lt;div style="text-align:start"&gt; 
 &lt;p style="color:#222222; margin-left:0; margin-right:0"&gt;今年以来，美国科技巨头已经承诺为建设 AI 能力投资 3200 亿美元。面对竞争对手的压力，以及投资者对 AI 回报慢于预期的失望，科技巨头为其大规模 AI 支出进行了辩护，表示这些投资是推动业务增长和提升产品质量的必要条件。&lt;/p&gt; 
&lt;/div&gt; 
&lt;div style="text-align:start"&gt; 
 &lt;p style="color:#222222; margin-left:0; margin-right:0"&gt;&lt;strong&gt;在财报电话会上，在被问及是否在 AI 人才方面大手笔支出时，Ashkenazi 表示，Alphabet 会「确保我们为拥有业内最优秀和最聪明的人才进行适当投资」。&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div style="text-align:start"&gt; 
 &lt;p style="color:#222222; margin-left:0; margin-right:0"&gt;本月早些时候，谷歌在 AI 人才争夺战中引发关注，宣布将以 24 亿美元收购 AI 编程初创公司 Windsurf，该交易还包括技术授权，Windsurf 首席执行官 Varun Mohan 及其顶级研究团队将加入谷歌。&lt;/p&gt; 
&lt;/div&gt; 
&lt;div style="text-align:start"&gt; 
 &lt;p style="color:#222222; margin-left:0; margin-right:0"&gt;与此同时，竞争对手 Meta 正在疯狂进行挖角计划。Meta 此前宣布设立「超级智能实验室」部门并展开激进的人才招聘，给这场 AI 竞赛加码。据悉，Meta 为吸引顶尖 AI 研究人员，开出了超过 1 亿美元的薪资待遇。&lt;/p&gt; 
&lt;/div&gt; 
&lt;div style="text-align:start"&gt; 
 &lt;h2 style="margin-left:0; margin-right:0"&gt;搜索业务得到 AI 加持，继续强劲增长&lt;/h2&gt; 
&lt;/div&gt; 
&lt;div style="text-align:start"&gt; 
 &lt;p style="color:#222222; margin-left:0; margin-right:0"&gt;谷歌此次公布的财报是科技巨头们本轮财报季的第一份，微软、苹果、亚马逊和 Meta 将于下周陆续发布。媒体称，投资者目前高度关注这些大型科技公司不断膨胀的支出规模，因为它们正在竞相抢占 AI 赛道的领先地位。&lt;strong&gt;随着越来越多用户转向 ChatGPT 来获取网络信息，投资者一直在密切关注 Alphabet 是否出现疲软迹象。&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div style="text-align:start"&gt; 
 &lt;p style="color:#222222; margin-left:0; margin-right:0"&gt;同时，谷歌的情况具有一定特殊性，其云计算部门依靠数据中心向客户出售算力，是这波 AI 热潮的直接受益者；而公司其他业务则在加速将 AI 工具整合进搜索、YouTube 等热门产品中。虽然谷歌仍位居云计算市场第三，仅次于微软和亚马逊，但其在 AI 方面的优势已帮助其赢得了众多客户。该部门被广泛认为是 Alphabet 当前增长最强劲的动力来源，因为其核心搜索业务已日趋成熟。&lt;/p&gt; 
&lt;/div&gt; 
&lt;div style="text-align:start"&gt; 
 &lt;p style="color:#222222; margin-left:0; margin-right:0"&gt;&lt;strong&gt;第二季度，谷歌云业务营收为 136 亿美元，同比增长 32%，而第一季度增幅为 28%。Alphabet 本季度广告总收入为 713 亿美元，同比增长 10.4%。其中，Alphabet 在其核心搜索业务中实现了强劲增长，同比上涨超过 11%。&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div style="text-align:start"&gt; 
 &lt;p style="color:#222222; margin-left:0; margin-right:0"&gt;为了给 AI 投资腾出资源，谷歌多年来持续进行削减成本的努力。今年，公司多次在不同部门提供自愿买断方案以减少员工人数。&lt;/p&gt; 
&lt;/div&gt; 
&lt;div style="text-align:start"&gt; 
 &lt;p style="color:#222222; margin-left:0; margin-right:0"&gt;为抢占 AI 赛道，谷歌不断提升其 AI 模型与聊天机器人 Gemini 的能力，并将 AI 功能加入多个核心产品。今年 5 月，谷歌在美国推出了「AI 模式」，对传统搜索引擎进行了重大更新，该模式通过聊天式对话来回答搜索问题，减少了链接数量。&lt;/p&gt; 
&lt;/div&gt; 
&lt;div style="text-align:start"&gt; 
 &lt;h2 style="margin-left:0; margin-right:0"&gt;反垄断诉讼仍是未知之数&lt;/h2&gt; 
&lt;/div&gt; 
&lt;div style="text-align:start"&gt; 
 &lt;p style="color:#222222; margin-left:0; margin-right:0"&gt;不过，投资者也对谷歌搜索主导地位面临的反垄断诉讼结果表示担忧。一名负责该案的联邦法官预计将在下个月裁定是否对谷歌施加限制，包括在 AI 竞争方面设定某些边界。&lt;/p&gt; 
&lt;/div&gt; 
&lt;div style="text-align:start"&gt; 
 &lt;p style="color:#222222; margin-left:0; margin-right:0"&gt;这一裁决将是长达数月审判的结果，审判重点之一就是新兴 AI 参与者是否会侵蚀谷歌的搜索垄断地位。该案由美国司法部于 2020 年发起，诉求包括强制谷歌出售 Chrome 浏览器、禁止其向苹果支付「默认搜索引擎」费用，并要求谷歌与竞争对手共享部分数据。&lt;/p&gt; 
&lt;/div&gt; 
&lt;div style="text-align:start"&gt; 
 &lt;p style="color:#222222; margin-left:0; margin-right:0"&gt;谷歌方面则辩称，政府的要求过于极端，不仅会损害消费者利益，还将削弱美国的技术领先地位。The Futurum Group 首席执行官 Daniel Newman 认为，这种不确定性正在拖累股价。&lt;/p&gt; 
&lt;/div&gt; 
&lt;div style="text-align:start"&gt; 
 &lt;p style="color:#222222; margin-left:0; margin-right:0"&gt;「在人们担心人工智能颠覆之后，反垄断风险接踵而至，这造成了拖累。」他表示：&lt;/p&gt; 
&lt;/div&gt; 
&lt;div style="text-align:start"&gt; 
 &lt;blockquote&gt; 
  &lt;p style="color:#222222; margin-left:0; margin-right:0"&gt;「裁决结果可能是深度惩罚性的，只要我们不知道最终结果如何，这种不确定性的阴影就会持续很长一段时间。」&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/div&gt; 
&lt;div style="text-align:start"&gt; 
 &lt;p style="color:#222222; margin-left:0; margin-right:0"&gt;此次外，Ashkenazi 表示，公司总运营支出增长 20%，达到 261 亿美元。增长的最大原因是法律及其他相关费用，其中包括与一项和解相关的 14 亿美元支出。德州总检察长 Ken Paxton 于 5 月宣布，谷歌就 2022 年一项涉及数据隐私权的诉讼达成 13.7 亿美元的和解协议。&lt;/p&gt; 
&lt;/div&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362043</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362043</guid>
      <pubDate>Thu, 17 Jul 2025 02:01:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
  </channel>
</rss>
