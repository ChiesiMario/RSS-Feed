<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - news - 简体中文</title>
    <link>https://www.oschina.net/news/project</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news" rel="self" type="application/rss+xml"/>
    <description>已对该 RSS 进行格式化操作：中英字符之间插入空格、使用直角引号、标点符号修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-cn</language>
    <lastBuildDate>Thu, 11 Sep 2025 21:41:52 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>快手发布开源多模态大模型 Kwai Keye-VL-1.5</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;快手近日正式发布多模态大语言模型 Keye-VL-1.5-8B。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0911/195624_2HB4_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;https://huggingface.co/Kwai-Keye/Keye-VL-1_5-8B&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;据介绍，与之前的版本相比，Keye-VL-1.5 的综合性能实现显著提升，尤其在基础视觉理解能力方面，包括视觉元素识别、推理能力以及对时序信息的理—表现尤为突出。Keye-VL-1.5 在同等规模的模型中表现出色，甚至超越了一些闭源模型如 GPT-4o。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-99ae906bd3166733efda4ecc20d5895a63d.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Keye-VL-1.5 采用四阶段渐进式训练流水线，以系统化方式提升模型性能。在视觉编码器预训练阶段，使用 SigLIP-400M 权重初始化 ViT，并通过 SigLIP 对比损失持续预训练以适应内部数据分布。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-09d1b4cf72c51b253843c3541b0130725c4.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;第一阶段重点优化投影 MLP 层，实现跨模态特征的稳固对齐；第二阶段解冻全部参数进行端到端多任务预训练，显著增强基础视觉理解能力；第三阶段进行退火训练，利用高质量数据微调模型，弥补上一阶段中高质量样本接触不足的问题，同时将序列长度扩展至 128K、调整 RoPE 逆频率配置，并引入长视频、长文本和大尺度图像等长上下文数据。 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;最终，通过同质-异质融合技术对不同数据混合比例下的模型权重进行平均，减少固定数据比例带来的内在偏差，在保持多样化能力的同时提升模型的鲁棒性。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371648</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371648</guid>
      <pubDate>Wed, 10 Sep 2025 12:00:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>AI 编程公司 Replit 发布第三代自主编码 Agent</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Replit&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Freplit.com%2Fagent3" target="_blank"&gt;宣布&lt;/a&gt;推出第三代自主编码 Agent（Agent 3），官方称其自主性提升至前代的 10 倍，单次可连续运行 200 分钟，全程无需人工干预。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;自主性增强&lt;/strong&gt;：Agent 3 可以自主测试和修复代码，甚至在后台持续改进用户的应用，将用户从重复性工作中解放出来。它能够像人类一样在浏览器中 「点击」 和 「操作」，检查应用中的按钮、表单和 API，确保一切正常运行。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;持续运行能力&lt;/strong&gt;：该版本能够持续自主运行超过三小时，相比之前的版本有了很大的进步。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;提升开发效率&lt;/strong&gt;：Agent 3 能够根据用户需求生成高质量代码，并主动提供优化建议，从而提升开发效率。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0911/194603_LoIb_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;新系统通过自研测试框架在浏览器内自动点击按钮、填写表单、调用 API 并修复错误，其速度比主流 Computer Use 模型快 3 倍，成本则降低了 90%。&lt;/p&gt; 
&lt;p&gt;Agent 3 支持自然语言提示，用户可以用简单描述启动复杂项目，并在手机端通过 Live Monitoring 实时查看进度。其另一项突破是能够生成子 Agent 与自动化流程，成品可直接接入 Slack、Notion、邮件等平台，进一步扩展工作流。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371645/replit-agent3</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371645/replit-agent3</guid>
      <pubDate>Wed, 10 Sep 2025 11:48:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>谷歌推迟发布 Android 16 QPR1 的 AOSP 源代码，引发担忧</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.androidauthority.com%2Fandroid-16-qpr1-source-code-delay-3596650%2F"&gt;根据科技媒体 Android Authority 的报道&lt;/a&gt;，谷歌近期已向 Pixel 设备推送 Android 16 QPR1 更新，但迟迟未按惯例在 48 小时内同步开放 AOSP 源码，引发第三方 ROM 开发者担忧。&lt;/p&gt; 
&lt;p&gt;Android 16 QPR1（Quarterly Platform Release 1）是谷歌 Android 系统最近发布的一个更新，其中包含了 Material 3 Expressive 等新特性。&amp;nbsp;通常在发布类似更新后，谷歌会在 1-2 天内将对应的源代码上传至 AOSP，方便第三方定制 ROM 的开发人员同步或使用这些更新特性。&lt;/p&gt; 
&lt;p&gt;但截至目前，一周过去了，AOSP 上的源代码还没能找到。 谷歌对外表示，源代码会在 「接下来几周内」（「in the coming weeks」）发布，但未给出更具体的时间表也没有解释延迟原因。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-f069cf8e6cb319a979b8b5816b2b9f523c7.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;定制 ROM 社区（如 LineageOS 等）以及使用 AOSP 的开发者对此延迟表示担忧，因为他们依赖谷歌的及时源代码发布来更新他们的系统、添加新特性或修复兼容性问题。&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/news/341310/google-android-development-aosp" target="_blank"&gt;谷歌在年初已经将部分 Android 的开发流程 「完全私有化」&lt;/a&gt;（即不再在公共视线中进行部分开发）以简化流程。虽然该公司曾多次公开承诺 AOSP 不会取消，但这些动作加剧了对其未来战略走向的猜测。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371641/android-16-qpr1-source-code-delay</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371641/android-16-qpr1-source-code-delay</guid>
      <pubDate>Wed, 10 Sep 2025 11:31:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>宇树科技创始人王兴兴：AI 时代，小组织的爆发力会越来越强</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;9 月 11 日，宇树科技创始人兼 CEO 王兴兴出席了 2025 外滩大会，这也是宇树官宣 IPO 计划后王兴兴首次公开发声。&lt;/p&gt; 
&lt;p&gt;他认为&lt;span&gt;AI 时代的组织管理是一门新课题。王兴兴表示，宇树科技是一家以硬件为主要产品的公司，随着业务快速发展，人员规模更大之后，可能会带来协作效率的降低，需要花时间探索更高效的组织管理方式。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0911/192019_miGZ_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="font-family:-apple-system,BlinkMacSystemFont,&amp;quot;Apple Color Emoji&amp;quot;,&amp;quot;Segoe UI Emoji&amp;quot;,&amp;quot;Segoe UI Symbol&amp;quot;,&amp;quot;Segoe UI&amp;quot;,&amp;quot;PingFang SC&amp;quot;,&amp;quot;Hiragino Sans GB&amp;quot;,&amp;quot;Microsoft YaHei&amp;quot;,&amp;quot;Helvetica Neue&amp;quot;,Helvetica,Arial,sans-serif"&gt;尽管存在挑战，但王兴兴对未来依旧十分乐观，他认为，现在创新创业的门槛已经大幅降低，年轻创新者迎来了好时代。真正可以用 AI 工具去实现新创意，并且在 AI 时代，小组织的爆发力会越来越强。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;王兴兴还提到：「顶尖的 AI 人才肯定是缺的，我相信这是每个大公司共同的渴求。」&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371637</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371637</guid>
      <pubDate>Wed, 10 Sep 2025 11:21:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>AgentFly —— 基于记忆增强的在线强化学习框架</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                        
                                                                                    &lt;p&gt;AgentFly 是基于记忆增强的在线强化学习框架，通过记忆库存储经验轨迹并利用神经案例选择策略实现 LLM 代理的持续适应能力，无需对底层 LLM 参数进行微调。&lt;/p&gt;

&lt;p&gt;该方法将决策过程建模为记忆增强的马尔可夫决策过程（M-MDP），通过非参数或参数化记忆模块存储过往经验，并基于软 Q 学习优化案例检索策略。&lt;/p&gt;

&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-070739b87c472bf439392dc3bc00bb25ddc.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-3e6c7c875e8356c92b9a62c600cca2f9f9e.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

&lt;p&gt;实验表明，该方法通过记忆库的持续更新实现高效在线学习，在复杂工具调用和多轮推理任务中展现出显著优势，为构建具备持续学习能力的通用型 LLM 代理提供了新范式。&lt;/p&gt;

                                                                    &lt;/div&gt;
                                                                </description>
      <link>https://www.oschina.net/p/agentfly</link>
      <guid isPermaLink="false">https://www.oschina.net/p/agentfly</guid>
      <pubDate>Wed, 10 Sep 2025 11:18:00 GMT</pubDate>
    </item>
    <item>
      <title>小米 Kaldi 团队开源零样本语音合成模型模型 ZipVoice</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;近日，小米集团新一代 Kaldi 团队发布了基于 Flow Matching 架构的 ZipVoice 系列语音合成（TTS）模型——ZipVoice（零样本单说话人语音合成模型）与 ZipVoice-Dialog（零样本对话语音合成模型）。&lt;/p&gt; 
&lt;p&gt;作为 zipformer 在语音生成任务上的应用和探索，ZipVoice 解决了现有零样本语音合成模型的参数量大、合成速度慢的痛点，在轻量化建模和推理加速上取得了重要突破。ZipVoice-Dialog 则解决了现有对话语音合成模型在稳定性和推理速度上的瓶颈，实现了又快又稳又自然的语音对话合成。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-207702be1087c5d602185a495fa12acd620.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;ZipVoice 系列的模型文件、训练代码和推理代码以及 6.8k 小时的语音对话数据集 OpenDialog 已全部开源：https://github.com/k2-fsa/ZipVoice&lt;/p&gt; 
&lt;p&gt;Zipvoice 论文：https://arxiv.org/pdf/2506.13053&lt;/p&gt; 
&lt;p&gt;样例体验请访问：https://zipvoice.github.io&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371625</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371625</guid>
      <pubDate>Wed, 10 Sep 2025 10:56:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>AI 编程公司 Replit 融资 2.5 亿美元，估值达 30 亿美元</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;AI 编程公司 Replit &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Freplit.com%2Fnews%2Ffunding-announcement" target="_blank"&gt;宣布&lt;/a&gt;完成了一轮 2.5 亿美元融资，使其估值达到了约 30 亿美元，较 2023 年上一轮融资增长近三倍。在过去一年中，Replit 的年收入从 280 万美元飙升至 1.5 亿美元，目前。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;公告称，Replit 的用户数量已达到 4000 万。此次融资正值该公司年化收入在不到一年的时间里从 280 万美元增长至 1.5 亿美元之际（增幅超过 50 倍）。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="291" src="https://oscimg.oschina.net/oscnet/up-37388b8353eaedc1a8426cc5a02204bd9d6.png" width="600" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;此次融资由 Prysm Capital 主导，参与投资的还有 Amex Ventures 和谷歌的 AI Futures Fund。此外，Replit 的现有投资者，包括 Y Combinator、Craft Ventures、Andreessen Horowitz、Coatue Management 和 Paul Graham 等也参与了这一轮融资。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;与此同时，Replit 还宣布推出了 Agent 3，并声称是其迄今为止自主性最强的 agent。Agent 3 的自主性比之前的版本提高了十倍，能够测试和修复代码，并构建自定义代理和工作流，从而能够自动执行任何类型的复杂或重复性任务，而不仅仅是软件工程。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371618</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371618</guid>
      <pubDate>Wed, 10 Sep 2025 10:31:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>聚焦结构化注意力，探索提升多模态大模型文档问答性能</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;blockquote&gt; 
 &lt;p&gt;作者：vivo 互联网算法团队&lt;/p&gt; 
 &lt;p&gt;本文聚焦多模态大语言模型（MLLMs）在文档问答（DocQA）任务中的性能提升，提出无需改动模型架构或额外训练的结构化输入方法，通过保留文档层次结构与空间关系（如标题、表格、图像位置）优化理解能力。研究发现，传统无结构 OCR 输入导致注意力分散，性能下降，而 LaTeX 范式结构化输入显著提升表现。注意力分析揭示其诱导"结构化注意力"，减少无关区域干扰，聚焦语义核心。在 MMLongBench、PaperTab 等四个数据集上验证，该方法尤其在复杂图表任务中效果显著，为智能文档处理与自动问答提供高效的解决方案。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;本文提供配套演示代码，可下载体验：&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fvivo%2FStructureMatters" target="_blank"&gt;Github |&lt;/a&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fvivo%2FStructureMatters" target="_blank"&gt;StructureMatters&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;一、引言&lt;/h1&gt; 
&lt;p&gt;多模态大语言模型（Multimodal Large Language Models, MLLMs）蓬勃发展的今天，文档理解（Document Understanding）作为一项涉及文本、图表和图像的复杂任务，依然面临诸多挑战。如何高效整合多源信息、理解文档的层次结构，成为提升 MLLMs 性能的关键问题。研究发现了一种无需修改模型架构或额外训练的新方法：仅通过结构化输入提升 MLLMs 在文档问答（DocQA）任务中的表现，同时通过注意力分析实践探寻结构化输入带来性能提升的深层原因。&lt;/p&gt; 
&lt;h1&gt;二、文档理解的核心挑战&lt;/h1&gt; 
&lt;p&gt;文档理解要求模型同时处理文本、图表、图像等多模态信息，并准确回答问题。然而，现有方法多依赖于扩展上下文窗口或优化检索增强生成（RAG），忽略了一个关键问题：输入格式如何影响模型的理解能力？&lt;/p&gt; 
&lt;p&gt;研究发现，传统的无结构 OCR 文本输入在某些 case 下未提升模型性能，反而因注意力分散和结构丢失导致性能下降。例如，在 MMLongBench 数据集上，加入无结构 OCR 文本后，模型准确率从 0.389 下降至 0.370。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-12cc41cc30fcd9ed83f9ed46f2f8fc29448.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;当前主流多模态大模型已经具备处理多模态信息的能力，其中 Qwen2.5-VL-7B-Instruct，Phi-3.5-Vision-Instruct，SmolVLM-Instruct 等在多个多模态任务上达到了 SOTA，但在文档阅读任务中仍表现不佳。以往文档阅读模型通过训练得到专用模型来进行文档阅读理解，并基于文档回答问题，如 mPLUG-DocOwl，Textmonkey 等模型。但随着 RAG 的快速发展，像 ColBERT 和 ColPali 这样的 RAG 方法在分别检索文本或视觉信息方面已被证明有效，当前主流方法通常基于 RAG 检索证据页面，然后将证据信息直接输入多模态大模型中以便回答 DocQAs。但当问题需要整合来自两种模态的信息时，它们通常表现不佳。&lt;/p&gt; 
&lt;p&gt;随着通用大模型的发展和 AGI 概念的普及，如何直接利用通用多模态大模型达到目的，不额外进行训练成为研究热点。改变输入结构能否帮助多模态大模型进行高效推理为本文探讨的重点。本文致力于探寻通用多模态大模型在何种条件下能够具有更加高效的推理理解能力，能否具备在 trainning free 的条件下达到较高的多元素文档理解能力。&lt;/p&gt; 
&lt;h1&gt;三、创新方法：结构化输入与注意力分析&lt;/h1&gt; 
&lt;p&gt;为解决这一问题，提出了一种基于 LaTeX 范式的结构保留方法。该方法通过保留文档的层次结构和空间关系（如标题、表格、图像的位置），从而为模型提供更清晰的语义引导。&lt;/p&gt; 
&lt;p&gt;具体流程包括：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;结构化编码&lt;/strong&gt;：将 OCR 文本和图像输入 MLLMs，提示模型尽可能保留图表、表格和文本的结构，生成 LaTeX 格式的表示。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;联合输入&lt;/strong&gt;：将结构化文本与原始图像一同输入模型，指导其在回答问题时关注关键区域。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;注意力分析&lt;/strong&gt;：通过比较仅图像输入、图像加无结构文本、图像加结构化文本三种情况的注意力分布，发现结构化输入显著减少了注意力浪费，引导模型聚焦于语义相关的文本和图像区域。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;实验结果表明，该方法在多个文档理解基准数据集上显著提升了模型性能。例如，在 MMLongBench 上，QWEN2.5-VL-7B-INSTRUCT 的准确率从 0.389 提升至 0.435；在 PaperTab 数据集上，准确率提升高达 20%，得益于 LaTeX 格式对表格和图表的精准解析。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-51b199d7da4f2b8e482da26b791c77d6d76.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h1&gt;四、通过注意力机制进行深层原因探究&lt;/h1&gt; 
&lt;p&gt;进一步的，通过注意力分析揭示了结构化输入的内在机制。无结构文本输入导致模型注意力分布散乱，浪费在图像边缘或无关区域；而结构化文本添加了结构化约束，诱导模型形成"结构化注意力"模式，聚焦于文档的核心内容（如图表、文本块）。例如，在一个案例中，模型需根据图表回答"西德居民对美俄关系的看法比例"。无结构输入下，注意力分散在图像空白区域；结构化输入后，注意力集中于图表和相关文本，显著提高答案准确性。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-c1cf6c7f984dc7670fa8d60b87bd138c1bf.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;结构化输入帮助减少 MLLMs 对于图片边界 token 的关注度，提高了模型对于文章主体部分的注意力得分。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-4266ab22bf20fc556f2d3de92b379f90a11.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;具体实例分析，证明结构化输入的重要意义。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-8d93f9bca6600aaabfd3cab20c1a73d141c.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h1&gt;五、实验验证与数据支持&lt;/h1&gt; 
&lt;p&gt;在四个文档理解基准数据集（MMLongBench、LongDocUrl、PaperTab、FetaTab）上测试 4 种 MLLMs 模型（如 QWEN2-VL-7B-INSTRUCT、Phi-3.5-Vision-Instruct）。结果显示，结构化输入在所有数据集上均提升了模型性能，尤其在包含复杂图表的 PaperTab 数据集上效果显著。消融实验进一步证明，仅用结构化文本或仅用图像的性能均低于两者结合，验证了结构化输入与图像联合使用的必要性。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-41cf00ff446e8353ac63034f7b316b625ac.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-b172ced302ebcd0de4814ef3027cb3a7f41.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h1&gt;六、总结与展望&lt;/h1&gt; 
&lt;p&gt;实践研究揭示了输入格式对 MLLMs 文档理解能力的关键影响，提出了一种简单而高效的结构化输入方法。未来可进一步探索更先进的结构提取技术或设计注意力控制插件，以进一步释放 MLLMs 在文档理解中的潜力。该研究提供了一种无需重训模型即可提升性能的实用方案，适用于智能文档处理、自动问答等场景。在没有额外训练和架构修改的前提下，通过简单的结构化文本输入，可以提升现有多模态大模型在文档理解任务中的表现。此项研究可以帮助用户分析、工作解析等场景中更准确地提取信息，提升工作效率。同时，RAG（检索增强生成）系统也能结合结构化输入来降低信息检索中的噪声，从而更高效地利用检索到的证据页面，为未来文档处理与分析提供了新的实践路径。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/vivotech/blog/18691398</link>
      <guid isPermaLink="false">https://my.oschina.net/vivotech/blog/18691398</guid>
      <pubDate>Wed, 10 Sep 2025 10:08:00 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>支付宝推出国内首个 AI 付</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;支付宝宣布推出国内首个「AI 付」服务，面向 AI 时代为智能体提供支付服务，并率先在瑞幸咖啡的 AI 点单助手「Lucky AI」上线，用户可在瑞幸支付宝小程序或瑞幸咖啡 App，用说话的方式完成下单并支付。这也是行业首次打通智能体内的下单与支付全链路，实现无缝的 AI 服务体验。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;以点咖啡为例，此前用户通过 AI 智能体下单，仍需跳转至支付页面再予手动确认。现在，用户在瑞幸支付宝小程序或瑞幸 App 唤起「Lucky AI」，不仅可以说句话点咖啡，还可以直接说句「下单」，完成身份核验后即支付。整个过程用户无需离开 AI 对话界面，像日常和店员聊天一样简单自然。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="496" src="https://oscimg.oschina.net/oscnet/up-ed3ccbc5548251ac7a86f54e281b209e58a.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;蚂蚁集团数字支付事业群首席技术官朱林表示：支付宝始终致力于通过产品创新，解决支付中存在的安全信任和便捷两大课题，此次为智能体提供「AI 下单+支付」解决方案，旨在服务好用户、产业和时代的需求，做好激活 AI 产业生态的一把钥匙。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;朱林认为，伴随 AI 产业的爆发式增长和智能终端的普及，预计未来 5 年内更自然的新交互支付占比或超 50%，多样化的智能设备支付将增长 10 倍，而更智能的 AI 支付市场规模可达万亿级。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;支付宝正着力打造面向 AI 时代的支付服务，构筑融入 AI 交互服务的「支付新基建」。目前已推出包括:&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;国内第一个「支付 MCP Server」，让 AI 智能体可一键接入支付宝支付服务;&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;国内第一个「AI 打赏」服务，为在 AI 智能体内有收取赞赏、小费等需求的开发者提供便捷收款能力;&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;国内第一个「AI 订阅付费」功能，支持开发者在智能体中便捷接入，按服务次数或时长定价并收款;&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;全球第一个智能眼镜支付「看一下支付」服务，用户佩戴眼镜看商家收款码或收款设备即可支付;&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;国内第一个智能体支付服务「支付宝 AI 付」，用户说说话可点单又能支付。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371606</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371606</guid>
      <pubDate>Wed, 10 Sep 2025 09:41:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>微软再次提醒 Windows 将逐步淘汰 VBScript，分三阶段进行</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;近期微软再次提醒用户，VBScript 将被逐步淘汰，并分享了相关建议。&lt;/p&gt; 
&lt;p&gt;VBScript，即 Visual Basic Script，是微软在近三十年前开发的脚本语言，曾默认包含在 Windows 中，主要用于自动化任务，不过近年来，该语言也常被黑客利用。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-9c905bf5e0ab3ab7646889588b6b63a0c65.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;早在 2023 年 10 月，微软就&lt;a href="https://www.oschina.net/news/261322/microsoft-deprecated-vbscript-in-window" target="_blank"&gt;宣布&lt;/a&gt;将在未来的 Windows 版本中淘汰 VBScript，并在 2024 年 5 月发布了详细的时间表，如今微软又提供了更多细节。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-589957b28bdb1eaa90fcbc1849da2ea8216.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;VBScript 主要用于通过执行外部.vbs 脚本或引用正则表达式库来扩展 Office 应用程序的功能，不过，随着 VBScript 的淘汰，这些开发人员和用处将受到影响。&lt;/p&gt; 
&lt;p&gt;微软将淘汰 VBScript 分为三个阶段，第一阶段已经启动，可能会持续到 2026 年或 2027 年。在此期间，VBScript 将作为「按需功能」（FOD）默认启用，现有项目不会受到影响。&lt;/p&gt; 
&lt;p&gt;当第二阶段开始时，该 FOD 将被禁用，最后，在第三阶段，VBScript 将被完全移除，这将直接影响之前提到的用处。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;微软建议客户使用 Office 2508 版本中包含的 RegExp 类，并默认使用 Microsoft 365 订阅，这将使开发人员能够在 VBScript 中继续使用 RegExp。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;微软还建议客户通过 Microsoft 365 升级到最新版本的 Office，以利用新的 RegExp 实现，这将允许开发人员在 Visual Basic 编辑器（VBE）中使用该功能，而无需添加 vbscript.dll。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371602</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371602</guid>
      <pubDate>Wed, 10 Sep 2025 09:28:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Stability AI 发布专业音频生成模型 Stable Audio 2.5</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Stability AI 推出专业音频生成模型&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fstability.ai%2Fnews%2Fstability-ai-introduces-stable-audio-25-the-first-audio-model-built-for-enterprise-sound-production-at-scale" target="_blank"&gt;Stable Audio 2.5&lt;/a&gt;，借助 Adversarial Relativistic-Contrastive（ARC）后训练技术，实现复杂音乐结构的高效生成。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-e2eb31cef809fb7d09a49502cdb9a67f522.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在英伟达 H100 GPU 上，模型可在 2 秒内完成最长 3 分钟的音频创作，支持前奏、发展、尾声等多段落结构，并集成音频修复功能，允许用户上传现有音频进行续写。&lt;/p&gt; 
&lt;p&gt;该模型同步推出移动端轻量版 Stable Audio Open Small，可在手机端 7 秒内生成 11 秒立体声。为确保商用合规，Stable Audio 2.5 基于 licensed 数据集训练，并通过版权识别系统限制用户上传版权受限内容。&lt;/p&gt; 
&lt;p&gt;Stability AI 希望该技术能应用于广告、零售、品牌音效等多个领域，与 WPP 旗下的音效品牌代理机构 Amp 合作，为大型客户提供一致的音频识别服务。&lt;/p&gt; 
&lt;p&gt;Stability AI 的音频团队还可以根据公司的音效库调整模型，打造独特的音频标识。Stable Audio2.5 将通过 WPP Open 平台面向 WPP 的全球客户开放。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371600/stability-ai-introduces-stable-audio-2-5</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371600/stability-ai-introduces-stable-audio-2-5</guid>
      <pubDate>Wed, 10 Sep 2025 09:21:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>​字节 Seed 推出全新 AgentGym-RL 框架</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;字节跳动 Seed 研究团队推出了名为 AgentGym-RL 的新框架，专注于通过强化学习训练 LLM 代理，使其能够进行多轮互动决策。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;该框架具有模块化和解耦的架构，提供了极高的灵活性和扩展性。AgentGym-RL 覆盖了多种真实场景，能够支持主流的强化学习算法，帮助代理全面提升其决策能力。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="372" src="https://oscimg.oschina.net/oscnet/up-6607f56f1b2f55c4cb23f3482def33cf8f5.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;为了进一步优化训练效果，研究团队还提出了一种名为 ScalingInter-RL 的训练方法。该方法通过阶段性调整交互次数，帮助代理在早期专注于掌握基本技能，随后逐渐增加交互次数，以鼓励更多样化的问题解决策略。这种探索与利用的平衡设计，有助于代理在面对复杂任务时保持稳定的学习和决策能力。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;在实验过程中，研究者们采用了 Qwen2.5-3B 和 Qwen2.5-7B 作为基础模型，评估了 AgentGym-RL 和 ScalingInter-RL 在五个不同场景中的表现。结果显示，使用 AgentGym-RL 的代理在 27 个任务中，表现优于多个商业模型。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;研究团队计划将整个 AgentGym-RL 框架，包括代码和数据集，开源，以支持更多研究者开发智能代理。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;AgentGym-RL 框架涉及的多种场景包括网络导航、深度搜索、数字游戏、体感任务和科学实验等，代理在这些场景中需具备强大的决策能力和适应能力，才能完成复杂的任务。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371591</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371591</guid>
      <pubDate>Wed, 10 Sep 2025 08:40:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Salesforce 开源深度研究 Agent：SFR-DeepResearch (SFR-DR)</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Salesforce &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2FCaimingXiong%2Fstatus%2F1965440617334685886" target="_blank"&gt;发布&lt;/a&gt;了开源深度研究 Agent：SFR-DeepResearch（SFR-DR）。该模型基于 OpenAI 的小型开源权重模型，通过强化学习进行训练，具备推理、搜索与代码执行能力，可自主完成深度研究任务。&lt;/p&gt; 
&lt;p&gt;SFR-DR-20B 版本仅依靠网页搜索、浏览器和 Python 解释器，在纯文本的 Humanity's Last Exam 基准测试中取得了 28.7% 的成绩。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-1e558bc3cb2f0faa6a5717b108de0484740.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;SFR-DR 亮点特性如下&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;核心能力&lt;/strong&gt;：基于强化学习（RL）训练的自主研究代理，能够独立推理、搜索和编程，完成深度研究任务。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;性能表现&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;SFR-DR-20B&lt;/strong&gt; 在 Humanity's Last Exam（纯文本）上取得 &lt;strong&gt;28.7%&lt;/strong&gt; 的成绩&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;仅依赖网络搜索、网页浏览和 Python 解释器&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;超越 OpenAI o3 DeepResearch 和 Kimi Researcher&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;训练方式&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;端到端 RL，从优化推理能力的基础模型开始训练&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;保留推理能力的同时提升研究执行能力&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;自主性&lt;/strong&gt;：无需预定义多代理工作流，可自主规划、推理、提出方案并执行行动&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;论文地址：&lt;em&gt;https://arxiv.org/abs/2509.06283&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371589</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371589</guid>
      <pubDate>Wed, 10 Sep 2025 08:33:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>腾讯开源图检索增强生成框架 Youtu-GraphRAG</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;腾讯优图实验室开源了 Youtu-GraphRAG，这是一个全新的图检索增强生成框架，旨在通过大语言模型+RAG 模式，将知识组织成图谱，再交给大语言模型进行检索和推理，从而提高模型在处理复杂问答任务时的准确性和可追溯性。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;Youtu-GraphRAG 特别适用于企业知识库问答、科研文档解析、个人知识管理等知识密集型场景。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;Youtu-GraphRAG 通过三大创新实现了从图构建到索引、再到检索的垂直统一和认知闭环。首先，它采用了四层知识树结构，将知识拆解成属性、关系、关键词和社区四个层次，使得大模型在回答问题时能够沿着知识树定位信息，推理路径清晰可见。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;其次，社区检测升级不仅关注「谁和谁有关」，还结合语义理解「为什么它们有关」，生成简明摘要，帮助用户快速抓住问题本质。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;最后，智能迭代检索机制允许用户提出复杂问题时，将其拆解成多个子问题并行检索，并通过迭代反思机制对结果进行补充和修正，最终给出更完整、更可靠的回答。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="314" src="https://oscimg.oschina.net/oscnet/up-ab798039306f65590d7249203cb4394d0d9.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;Youtu-GraphRAG 在实践检验中表现出色。在六个&lt;span&gt;权威&lt;/span&gt;基准测试中，&lt;span&gt;最高&lt;/span&gt;可节省 90.71% 的 Token 成本，复杂推理任务的准确率&lt;span&gt;最高&lt;/span&gt;提升 16.62%。此外，该框架支持中英文双语，跨领域应用无需重构，具有很高的灵活性。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;使用 Youtu-GraphRAG 非常简单，只需四步即可上手。首先，通过命令行获取项目代码。其次，进行环境配置，包括获取远程调用模型的凭证 API key 并创建配置文件。然后，一键部署项目。最后，通过 curl 命令体验交互。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371566</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371566</guid>
      <pubDate>Wed, 10 Sep 2025 07:30:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>字节跳动发布开源多模态模型 Mini-o3</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;字节跳动发布开源多模态模型 Mini-o3，通过扩展推理模式和交互轮次提升视觉搜索性能，在复杂场景中实现显著突破。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0911/151240_xzZ9_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;https://mini-o3.github.io/&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;据介绍，Mini-o3 是一个完全开源的多模态模型，专为「边看边想」的视觉搜索任务设计。它通过强化学习将工具调用次数扩展到数十轮，在 VisualProbe、V* Bench、HR-Bench、MME-Realworld 等基准上取得了 7B 量级的最佳成绩。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-a111377d25b371aba3ccf675445d406ca6b.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-71b6b52f2c739864eda55692e83e9bbe7ce.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;项目公开了训练代码、模型权重以及包含 4,500 条数据的 Visual Probe 数据集，允许研究者在非商业许可下复现 OpenAI o3 风格的深度推理行为。&lt;/p&gt; 
&lt;p&gt;Mini-o3 支持深度优先搜索、试错等多样化推理模式，测试时交互轮次可扩展至 32 轮以上，准确率随轮次增加显著提升（如 VisualProbe-Hard 任务准确率从 35.1% 提升至 48.0%）。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;核心创新&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;挑战性数据集构建&lt;/strong&gt;：推出 VisualProbe 数据集，包含高分辨率图像、小目标和密集干扰物场景，强制模型进行多轮探索。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;迭代数据收集&lt;/strong&gt;：通过冷启动数据生成多样化推理轨迹，覆盖回溯、假设验证等策略，解决预训练模型缺乏多轮交互能力的问题。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Over-Turn Masking 策略&lt;/strong&gt;：在强化学习中避免对超轮次响应的惩罚，支持模型深度探索，训练时轮次上限设为 6 轮，测试时可扩展至 32 轮以上。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;应用案例&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-32f28f4c28db5eb7a911b5d6fe714e2b11b.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371562</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371562</guid>
      <pubDate>Wed, 10 Sep 2025 07:18:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>🔥🔥AI 能打造盲人的第三只眼？</title>
      <description/>
      <link>https://www.oschina.net/ai-creation/details/2194</link>
      <guid isPermaLink="false">https://www.oschina.net/ai-creation/details/2194</guid>
      <pubDate>Wed, 10 Sep 2025 07:03:00 GMT</pubDate>
    </item>
    <item>
      <title>🔥🔥智能植物收割？能割韭菜不？</title>
      <description/>
      <link>https://www.oschina.net/ai-creation/details/2195</link>
      <guid isPermaLink="false">https://www.oschina.net/ai-creation/details/2195</guid>
      <pubDate>Wed, 10 Sep 2025 07:03:00 GMT</pubDate>
    </item>
    <item>
      <title>Visual Studio 2026 Insiders 预览版发布：深度集成 AI、界面设计焕然一新</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Visual Studio 2026 Insiders 首个预览版&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdevblogs.microsoft.com%2Fvisualstudio%2Fvisual-studio-2026-insiders-is-here%2F" target="_blank"&gt;已发布&lt;/a&gt;，带来了更快的启动速度、更智能的 AI Agent 和更现代的 UI。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-e324bcd11fdf3e7e28980bdbf8987580a75.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;这是自 2021 年以来 Visual Studio 的首次重大版本更新。本次更新主打更深层的人工智能集成及全新界面设计，为广大开发者带来了全新体验。&lt;/p&gt; 
&lt;p&gt;新版 Visual Studio 采用微软 Fluent Design 设计体系，并推出数款新主题（包括 「Mango Paradise」「Juicy Plum」 等），同时启用了新 Logo，并将 Visual Studio Preview 更名为 Visual Studio Insiders。虽然界面焕新，核心依然运行在传统 .NET Framework 上，许多新功能更侧重于提升 AI 辅助开发体验。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0911/144937_XQRx_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;AI 增强成为亮点。Copilot 可获取更多上下文，拥有 「Profiler Copilot Agent」 等新功能，能基准测试代码、查找并自动实现优化建议。「自适应粘贴」 让 Copilot 能根据已有代码自动调整粘贴内容，「URL 上下文」 功能支持通过聊天窗口指定网页规则，让 AI 辅助更智能。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0911/145432_IwpX_2720166.gif" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;开发者还可自由选择接入的 AI 大模型及 API 密钥，目前支持 Anthropic、Google 及 OpenAI 等主流厂商，增强了模型选择的灵活性。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-5b5a94b7cf0eda7accb37905ed368768183.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;新版设置系统采用可编辑 JSON 配置文件，并支持高级过滤和批量同步，可与项目一同纳入版本控制。原仅企业版独有的代码覆盖率功能（用於单元测试覆盖分析）现向社区及专业版用户开放。&lt;/p&gt; 
&lt;p&gt;对于扩展兼容，Visual Studio 2026 承诺与 2022 版扩展完全兼容，便于用户无痛升级。而 NDepend 开发团队等第三方工具表示，虽然主进程依旧依赖旧版.NET Framework，但新版已为扩展和子进程引入 .NET Core，多数代码运行环境得到一定现代化。&lt;/p&gt; 
&lt;p&gt;此外，VS 2026 新增一键工具，支持将.NET Framework 应用迁移至.NET 10（预计随新版同步发布），并保持传统 Windows 平台的高适配性。&lt;/p&gt; 
&lt;p&gt;用户社区评论普遍对 AI 功能感兴趣，也有呼吁微软更多关注性能优化与资源效率，以及期盼 Linux 版 Visual Studio 的声音。微软方面称，过去一年团队共修复了 4489 项问题和 290 项新需求，大部分已纳入新版本。&lt;/p&gt; 
&lt;p&gt;目前尚无 Visual Studio 2026 的正式发布日期，预计将与.NET 10 一同于 11 月发布。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371552/visual-studio-2026-insiders</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371552/visual-studio-2026-insiders</guid>
      <pubDate>Wed, 10 Sep 2025 06:50:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>9 月线下活动汇总</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;span id="OSC_h4_1"&gt;&lt;/span&gt; 
&lt;h4&gt;&lt;strong&gt;9 月 13 日，广州，Solar 开发者咖啡&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;👨‍💻熟悉的开发者咖啡活动回归，这次搬到了广州！这也是 Solar 第一次在广州举办社区活动，尝试在大湾区布局更多站点。期待开发者们的加入！&lt;/p&gt; 
&lt;p&gt;🗓️时间：9 月 13 日，周六 14:30-18:00&lt;br&gt; 📝现在报名 https://luma.com/xhki98ic&lt;/p&gt; 
&lt;p&gt;分享主题：&lt;/p&gt; 
&lt;p&gt;- Solana 历史、POH 机制、开发差异；&lt;br&gt; - Solana 现在的开源生态、技术、项目；&lt;br&gt; - 基于 Blinks 开发所用的技术、需求分析、商业模式和收获；&lt;br&gt; - Solar.zens.one, Solar 的项目导航&lt;br&gt; - Solana DeFi 生态现状和创新&lt;/p&gt; 
&lt;span id="OSC_h4_2"&gt;&lt;/span&gt; 
&lt;h4&gt;9 月 13–14 日，杭州，RustChinaConf 2025 x Rust Global China&lt;/h4&gt; 
&lt;p&gt;今年正值 Rust 诞生 10 周年 🎉&lt;br&gt; 👉 讲师阵容全面升级，海外讲师议题比例高达 45%！&lt;br&gt; 👉 Rust Foundation 团队将首次现场参与交流！&lt;br&gt; 👉 国内大厂创业公司纷纷加入，了解 Rust 语言的最近进展&lt;br&gt; 👉 现场设有 Rust 十周年庆典与特别大礼包&lt;/p&gt; 
&lt;p&gt;🔎 查看完整议程： &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Frustcc.cn%2F2025conf%2Fschedule.html" target="_blank"&gt;https://rustcc.cn/2025conf/schedule.html&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;🎟️ 立即购票： &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhangzhou2025.gosim.org%2Ftickets%2F" target="_blank"&gt;https://hangzhou2025.gosim.org/tickets/&lt;/a&gt;&lt;/p&gt; 
&lt;span id="OSC_h4_3"&gt;&lt;/span&gt; 
&lt;h4&gt;9 月 14 日，成都，聚焦 Kiro：体验下一代 Agentic IDE&lt;/h4&gt; 
&lt;p&gt;生成式 AI 正在重构开发流程，AI Agent 成为人机交互新核心！亚马逊云科技成都 User Group 技术沙龙邀您共同探索最前沿的 Agentic 实践。&lt;/p&gt; 
&lt;p&gt;🔍 聚焦 Kiro：体验下一代 Agentic IDE，基于 MCP 协议重塑开发环境&lt;br&gt; 🧠 依托 Amazon Bedrock：构建与部署高效 AI Agent（如 Strands Agents）&lt;br&gt; 🛠 实战指引：掌握亚马逊云科技 Builder Cards 中文版，加速生成式 AI 应用开发&lt;/p&gt; 
&lt;p&gt;立即报名，携手迈进智能体驱动开发的新时代！&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F1mqqg_M34zzVig1QZl0KCg" target="_blank"&gt;https://mp.weixin.qq.com/s/1mqqg_M34zzVig1QZl0KCg&lt;/a&gt;&lt;/p&gt; 
&lt;span id="OSC_h4_4"&gt;&lt;/span&gt; 
&lt;h4&gt;9 月 20 日，成都，Gitee Talk | 模力方舟 AI 应用开发沙龙&lt;/h4&gt; 
&lt;p&gt;📅 9 月 20 日（下周六）13:30-17:00&lt;br&gt; 📍 成都春熙路 voco 酒店（3 号线春熙路 E1 口出直达）&lt;/p&gt; 
&lt;p&gt;✨ 聚焦 AI 开发全链路：从模型调用、开源实践、3D 交互到硬件选型，5 大实战议题，带你突破技术边界！&lt;br&gt; 🙌 现场和大咖交流互动、结识技术同频人，开源老传统披萨畅吃，抽模力方舟千元代金券和更多惊喜好礼～&lt;/p&gt; 
&lt;p&gt;🚀 立即报名，锁定席位：&lt;a href="https://www.oschina.net/event/8598033"&gt;https://www.oschina.net/event/8598033&lt;/a&gt;&lt;/p&gt; 
&lt;span id="OSC_h4_5"&gt;&lt;/span&gt; 
&lt;h4&gt;9 月 20 日，合肥，架构师的 AI 进化论&lt;/h4&gt; 
&lt;p&gt;🚀「架构师的 AI 进化论——从架构升级到行业应用」腾讯云架构师技术沙龙 · 合肥&lt;/p&gt; 
&lt;p&gt;🤔AI 时代，架构师是被替代？被重构？还是——主动进化？&lt;/p&gt; 
&lt;p&gt;🌟下周六（9 月 20 日）14:00，相约合肥！与多位一线专家面对面，全程硬核干货，深入 AI 架构实战经验与前沿思考，千万不要错过！&lt;/p&gt; 
&lt;p&gt;🎁 现场还有鹅厂限定周边、公仔、定制好礼抽奖送不停，戳链接免费报名👇&lt;br&gt; &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmc.tencent.com%2FD91JjaK9" target="_blank"&gt;https://mc.tencent.com/D91JjaK9&lt;/a&gt;&lt;/p&gt; 
&lt;span id="OSC_h4_6"&gt;&lt;/span&gt; 
&lt;h4&gt;9 月 20 日，青岛，奇妙 AI 之旅&lt;/h4&gt; 
&lt;p&gt;免费参与，0 门槛入场！&lt;br&gt; 这是一个真正能带走方案、解决实际问题的 AI 实战工作坊！💡&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;聚焦真问题，挖掘问题痛点；&lt;/li&gt; 
 &lt;li&gt;一起头脑风暴，打破思维限制；&lt;/li&gt; 
 &lt;li&gt;快速搭建方案原型，让想法能实实在在被看到；&lt;/li&gt; 
 &lt;li&gt;多方面验证，直到拿出可落地的方案；&lt;/li&gt; 
 &lt;li&gt;现场路演，让 AI 创新方案被看到。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;👉🏼立即报名：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.huodongxing.com%2Fevent%2F9823342171500%3Fcoupon%3D88ab88" target="_blank"&gt;https://www.huodongxing.com/event/9823342171500?coupon=88ab88&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;全程边玩边掌握 AI 技能，更有 3000 元奖金🧧等你来拿～&lt;/p&gt; 
&lt;p&gt;外地朋友别犹豫！机票 / 车票报销名额等你来抽！&lt;/p&gt; 
&lt;span id="OSC_h4_7"&gt;&lt;/span&gt; 
&lt;h4&gt;9 月 20 日，上海，PyCon China 2025&lt;/h4&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;1 个主会场 + 4 个分会场 + 开源松 + Vibe Coding 黑客松 + 社区开源集市展&lt;/span&gt;&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;汇集国内外 Python 领域专家与实力开发者，立足 AI 新纪元，&lt;/span&gt;&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;为大家呈现一场丰富、精彩、趣味，且充满活力的 PyCon 盛会。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;开创性地带来首届 PyCon China Vibe Coding 黑客松大赛&lt;/span&gt;。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;活动详情：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FDfU9DaqCMT5nDgHufzzaNw" target="_blank"&gt;https://mp.weixin.qq.com/s/DfU9DaqCMT5nDgHufzzaNw&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/3859945/blog/18691462</link>
      <guid isPermaLink="false">https://my.oschina.net/u/3859945/blog/18691462</guid>
      <pubDate>Wed, 10 Sep 2025 06:37:00 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>网信办副主任王京涛：促进软件生态、开源代码等自主生态构建</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;中央网信办副主任、国家网信办副主任王京涛在《新安全》杂志上撰文表示，今年是「十四五」规划收官之年、「十五五」规划谋篇布局之年，也是进一步全面深化改革的重要一年。&lt;/p&gt; 
&lt;p&gt;网络安全工作机遇与挑战并存，要以更大力度、更实举措加快推进国家网络安全体系和能力现代化，推动我国网络安全事业发展迈上新台阶。进一步建立安全可信的供应链安全管理体系。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0911/143425_K3L6_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;https://mp.weixin.qq.com/s/b-rPE4C9CD9-5IEJ7odAXA&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;围绕集成电路、基础软件、人工智能、量子信息等重点领域，加强产业链协同创新，加快推动关键核心技术研发突破。充分发挥我国超大规模市场优势，促进软件生态、开源代码等自主生态构建，推动人工智能等技术在产业应用上取得更大突破。充分发挥网络安全审查、云计算服务安全评估、新技术新应用安全评估等制度机制作用，防范网络安全风险，提高供应链安全水平。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371539</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371539</guid>
      <pubDate>Wed, 10 Sep 2025 06:35:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
  </channel>
</rss>
