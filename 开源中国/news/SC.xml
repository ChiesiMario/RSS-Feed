<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - news - 简体中文</title>
    <link>https://www.oschina.net/news/project</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news" rel="self" type="application/rss+xml"/>
    <description>已对该 RSS 进行格式化操作：中英字符之间插入空格、使用直角引号、标点符号修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-cn</language>
    <lastBuildDate>Sun, 29 Jun 2025 12:46:09 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>关键领域软件工厂的安全中枢：Gitee Scan 全面升级供应链检测能力</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;blockquote&gt; 
 &lt;p&gt;作者：Gitee DevSecOps 团队，李颖萍，李文博&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;随着软件供应链安全体系在互联网、金融等领域逐步成熟，关键领域正加速迈向以 MLOps、软件工厂为核心的新型研发生态。在这一过程中，面对代码安全、依赖合规、系统可信等多重挑战，传统人工审查模式已难以满足国家级高安全性要求。&lt;/p&gt; 
&lt;p&gt;Gitee Scan&amp;nbsp;&lt;strong&gt;集成 SAST（静态应用安全测试）、DAST（动态应用安全测试）、SBOM（软件物料清单）等关键技术&lt;/strong&gt;，面向关键领域企业提供自动化安全检测能力，为软件供应链构建「可视、可控、可追溯」的防线。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;关键领域软件安全合规的挑战&lt;/h2&gt; 
&lt;p&gt;关键领域软件系统需满足比通用商业软件更为严苛的安全标准，挑战主要包括：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;高级威胁对抗能力不足：如供应链攻击、零日漏洞利用、侧信道攻击等，要求具备防篡改、防逆向的设计能力。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;依赖来源可信难保障：缺乏有效 SBOM 工具与流程，组件溯源难、更新慢。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;研发环境隔离性强：普遍采用物理内网与封闭开发工具链，难以对接现代 DevSecOps 流程。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;安全环节后置：采用瀑布式模型，无法实现安全「左移」，风险发现延迟。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;开发工具不兼容：现有工具难以集成自动化 CI/CD 流程。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;生命周期极长：系统生命周期长达 10～30 年，对漏洞管理与文档留存要求极高。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;法规合规负担重：需同时满足 GJB5000A、GJB8114、ISO 27001、NIST SP800 等多套标准体系。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;过程留痕难：人工文档负担重，难以满足 DevSecOps 强调的「基础设施即代码」理念。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;Gitee Scan：构建关键领域软件工厂的质量中枢&lt;/h2&gt; 
&lt;p&gt;作为 Gitee DevSecOps 平台中质量保障的核心组件，Gitee Scan 担纲软件工厂中的质量车间角色，贯穿从代码提交到漏洞修复的全过程，构建覆盖 SAST、DAST、SBOM 的安全扫描闭环。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0627/192102_QtGg_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;核心技术能力&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Gitee Scan 采用自主研发的 BCA 扫描引擎&lt;/strong&gt;，源自独创代码执行链分析技术（已申请专利），结合 AST 静态分析、控制流/数据流建模与指纹匹配算法，实现高精度漏洞识别。&lt;/p&gt; 
&lt;p&gt;目前，已上线的有 BCA–Kotlin、BCA-Java、BCA-OC、BCA-Cobol、BCA-SQL、BCA-C/C++，覆盖 CWE、OWASP Top 10、GJB8114 等主流规则体系。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0627/192119_G2qO_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;高性能架构设计&lt;/h3&gt; 
&lt;p&gt;支持分布式部署与高并发扫描，结合权限隔离与多租户机制，实现平台级弹性伸缩与私有部署能力。&lt;/p&gt; 
&lt;h3&gt;安全闭环能力&lt;/h3&gt; 
&lt;p&gt;与 Gitee Team 集成，提供扫描-跟踪-整改-审计全流程联动，支持问题归属与整改责任划分，留痕可追溯，满足关键领域内审要求。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;多维扫描能力构建安全基础设施&lt;/h2&gt; 
&lt;h3&gt;SAST（静态应用安全测试）&lt;/h3&gt; 
&lt;p&gt;基于自研 BCA 引擎，解析代码结构与执行路径，识别高危漏洞（如注入类、缓冲区溢出、硬编码凭据等），并对超长函数、圈复杂度、重复代码等可维护性指标进行分析。误报率控制在 10% 以下，处于行业领先水平。&lt;/p&gt; 
&lt;h3&gt;SBOM 分析能力&lt;/h3&gt; 
&lt;p&gt;自动生成软件物料清单（SBOM），支持对开源组件、第三方依赖、内部模块的全量溯源，标注许可证信息与风险等级，助力组件可信评估。&lt;/p&gt; 
&lt;h3&gt;DAST（动态应用安全测试）&lt;/h3&gt; 
&lt;p&gt;结合模拟输入与接口探测，自动发现服务层漏洞，提升运行时风险发现覆盖面。与 SAST 形成静动结合检测体系。&lt;/p&gt; 
&lt;h2&gt;未来展望：AI 驱动的智能安全助手&lt;/h2&gt; 
&lt;p&gt;同时，Gitee Scan 正在探索 AI 技术在安全能力中的深度融合，推进「自动判断 + 智能修复」能力落地：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;误报识别辅助：结合大模型能力分析扫描结果上下文，自动判断漏洞是否为误报，减轻研发人员人工判断负担。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0627/192200_BOHD_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;个性化修复建议：基于代码上下文与历史缺陷库，生成高质量修复建议，提升研发效率与合规响应速度。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0627/192208_A5pp_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2 style="margin-left:0; margin-right:0; text-align:left"&gt;Gitee DevSecOps 的现代化研发生态&lt;/h2&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;Gitee DevSecOps 是一站式国产化研发与交付平台，集成了代码托管（Code）、项目协作（Team）、持续集成（CI）、持续部署（CD）、代码安全（Scan）、数据洞察（Insight）等多项能力，致力于打造具备全生命周期管控能力的现代软件工厂。&lt;/p&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0523/174619_MpFL_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;&lt;em&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgitee.cn%2Ffactory" target="_blank"&gt;https://gitee.cn/factory&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;平台设计充分考虑关键领域行业对安全性、可控性、合规性的极高要求，具备以下核心特征：&lt;/p&gt; 
&lt;ul style="list-style-type:disc; margin-left:0; margin-right:0"&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;国产化适配与私有化部署能力：全面兼容国产操作系统与基础设施，支持灵活部署于内网环境，保障数据主权；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;全流程 DevSecOps 管控体系：代码从提交、审核、构建、扫描、部署到发布全流程可视、可追溯、安全可控；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;模块化产品结构：各能力模块（如 Code、Team、Repo、Pipe、Scan、Insight 等）可灵活组合、渐进集成，适配多样化团队与流程要求；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;深度可观测与度量体系：内置研发效能度量与数据洞察引擎，支撑管理者宏观掌控项目态势与交付健康度。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0516/162046_MD15_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;在多个国家级重大项目与关键领域单位落地实践中，Gitee DevSecOps 已成为构建「自主、可控、高效、安全」的软件工程体系的重要基石。&lt;/p&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-825957ffbed1798ea7b6a37079fd6c99d18.gif" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357644</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357644</guid>
      <pubDate>Fri, 09 May 2025 11:22:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>中国首个海洋领域开源大模发布</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;中国首个海洋领域开源大模型 OceanGPT(沧渊) 于日前在浙江杭州发布。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;该大模型由海洋精准感知技术全国重点实验室 (浙江大学) 牵头研发，具备基础的海洋专业知识问答，以及声呐图像、海洋观测图等海洋特色多模态数据的自然语言解读能力。其采用的领域知识增强「慢思考」推理机制，相较现有通用大模型能有效降低幻觉式错误。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="334" src="https://oscimg.oschina.net/oscnet/up-7aaa94e1643eaae4cb8270cd536a75a4b94.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;此外，模型还适用于海洋机器人操控与水下具身智能等关键场景。沧渊研发团队负责人、浙江大学计算机学院教授陈华钧介绍称，「依托大模型的代码自动生成能力，只需输入一句自然语言指令，OceanGPT 即可生成对应的机器人操控代码，实现下发、部署和任务执行。未来，我们希望即使是非专业人士，也能通过语音指令驱动水下机器人完成复杂任务。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;其表示，OceanGPT 还集成了 MCP 大模型协议，旨在实现大模型驱动的多机器人协同协作。后续 OceanGPT 还可直接部署于海洋机器人上的端侧大模型，借助端侧大模型的推理能力进一步提升海洋装备的自主作业能力和作业效率。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;据悉，目前 OceanGPT 已在浙江大学海鹰系列水下机器人平台上完成技术验证，实测表明该模型将原本依赖人工编写的机器人代码编写效率从「小时级」提升至「秒级」。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357639</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357639</guid>
      <pubDate>Fri, 09 May 2025 10:40:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Embabel Agent —— Spring 之父出品的 JVM 的 Agent 框架</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                        
                                                                                    &lt;p&gt;&lt;span style="background-color:#ffffff; color:#1f2328"&gt;Embabel (Em-BAY-bel) 是一个在 JVM 上编写 agentic flows&lt;/span&gt;&amp;nbsp;&lt;span style="background-color:#ffffff; color:#1f2328"&gt;的框架，它将 LLM 触发的交互与代码和领域模型无缝融合。支持智能路径查找，以达到目标。它使用 Kotlin 编写，但提供了 Java 的自然使用模型。它出自 Spring 的创始人之手。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style="background-color:#ffffff; color:#1f2328"&gt;&lt;img alt="" height="304" src="https://static.oschina.net/uploads/space/2025/0627/155828_hK0o_4252687.jpg" width="200" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Actions：&lt;/strong&gt;Steps an agent takes&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Goals：&lt;/strong&gt;agent 试图实现的目标&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Conditions：&lt;/strong&gt;执行操作或确定目标是否已达成之前需要评估的条件。每次执行操作后都会重新评估条件。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Domain model：&lt;/strong&gt;支撑流程并告知动作、目标和条件的对象。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Plan&lt;/strong&gt;&lt;strong&gt;：&lt;/strong&gt;为实现目标而采取的一系列行动。计划由系统而非程序员动态制定。系统会在每次行动完成后重新制定计划，使其能够适应新的信息并观察前一次行动的效果。这实际上是一个&lt;a href="https://en.wikipedia.org/wiki/OODA_loop"&gt;OODA 循环&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p style="text-align:start"&gt;&lt;span&gt;&lt;span&gt;&lt;span style="color:#1f2328"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style="background-color:#ffffff"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;这些概念巩固了与其他 agent 框架的区别：&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;复杂的规划。&lt;/strong&gt;通过引入真正的规划步骤，使用非 LLM AI 算法，超越有限状态机或嵌套顺序执行。这使得系统能够通过以新的顺序组合已知步骤来执行未编程的任务，并做出有关并行化和其他运行时行为的决策。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;卓越的可扩展性和重用性&lt;/strong&gt;：由于动态规划，添加更多的域对象、动作、目标和条件可以扩展系统的功能，&lt;em&gt;而无需编辑 FSM 定义&lt;/em&gt;或现有代码。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;强类型和面向对象的优势&lt;/strong&gt;：操作、目标和条件由领域模型（可包含行为）定义。所有内容均为强类型，提示符和手动编写的代码可以清晰交互。告别 &lt;span style="background-color:#ffffff; color:#1f2328"&gt;magic maps&lt;/span&gt;。享受全面的重构支持。&lt;/li&gt;
&lt;/ul&gt;

&lt;p style="text-align:start"&gt;&lt;span&gt;&lt;span&gt;&lt;span style="color:#1f2328"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style="background-color:#ffffff"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;其他好处：&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;平台抽象&lt;/strong&gt;：编程模型和平台内部之间的明确分离允许在本地运行，同时可能在生产中提供更高的 QoS，而无需更改应用程序代码。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;专为 LLM 混合设计&lt;/strong&gt;：轻松构建混合 LLM 的应用程序，确保提供最具成本效益且功能强大的解决方案。这使得系统能够利用不同模型的优势来执行不同的任务。特别是，它有助于使用本地模型执行点任务。这对于成本和隐私至关重要。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;基于 Spring 和 JVM 构建，&lt;/strong&gt;可轻松访问现有企业功能和能力。例如：
&lt;ul&gt;
&lt;li&gt;Spring 可以注入和管理代理，包括使用 Spring AOP 来装饰功能。&lt;/li&gt;
&lt;li&gt;提供强大的持久性和事务管理解决方案。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;专为可测试性&lt;/strong&gt;而设计。单元测试和代理端到端测试都非常简单。&lt;/li&gt;
&lt;/ul&gt;

                                                                    &lt;/div&gt;
                                                                </description>
      <link>https://www.oschina.net/p/embabel-agent</link>
      <guid isPermaLink="false">https://www.oschina.net/p/embabel-agent</guid>
      <pubDate>Fri, 09 May 2025 09:24:00 GMT</pubDate>
    </item>
    <item>
      <title>Black Forest 开源 FLUX.1 Kontex 模型，使用文本即可实现一键 PS</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;知名开源平台 Black Forest 开源了文生图模型 FLUX.1-Kontext 的开发者版本，该模型让用户通过自然语言就能实现一键 P 图。&lt;/p&gt; 
&lt;p&gt;Black Forest 公布的测试数据显示，FLUX.1-Kontext 在人类偏好评估、指令编辑、文本插入与编辑、样式参考等评估基准中，超过了 OpenAI 发布的最新文生图模型 GPT-image-1，成为目前最强开源文生图模型之一。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1070" src="https://static.oschina.net/uploads/space/2025/0627/164257_fGbo_2720166.png" width="1280" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;FLUX.1 Kontext 专注于&lt;strong&gt;图像编辑任务&lt;/strong&gt;：包括迭代编辑、角色保持、局部与全局精细控制。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;可以非常准确地「重绘」图片中的局部或全图，比如：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;把帽子加到人物头上&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;改变背景风景&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;把原图中的狗换成猫，人物保持原样&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;多次修改也不会「跑偏」或者失真&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;跟很多流行工具（如 ComfyUI）无缝结合，方便使用&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0627/164220_OOmD_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;模型下载：https://huggingface.co/black-forest-labs/FLUX.1-Kontext-dev&lt;/p&gt; 
&lt;p&gt;技术报告：https://arxiv.org/abs/2506.15742&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357607/flux-1-kontext-dev</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357607/flux-1-kontext-dev</guid>
      <pubDate>Fri, 09 May 2025 08:44:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>腾讯开源轻量级混元-A13B 模型</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;腾讯&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FmWfUrWz7bc7f9RhnOltQOA" target="_blank"&gt;宣布&lt;/a&gt;开源混元大模型家族的新成员——混元-A13B 模型。该模型采用基于专家混合（MoE）架构，总参数规模达 800 亿，激活参数为 130 亿。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;公告称，该模型在保持顶尖开源模型效果的同时，大幅降低了推理延迟与计算开销。对个人开发者和中小企业来说，极端条件下仅需 1 张中低端 GPU 卡即可部署。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在性能表现上，混元-A13B 模型在数学、科学和逻辑推理任务中展现出领先效果。例如，在数学推理测试中，模型能够准确完成小数比较并展现分步解析能力。对于时下热门的智能体（Agent）应用，模型可调用工具，高效生成出行攻略、数据文件分析等复杂指令响应。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="330" src="https://oscimg.oschina.net/oscnet/up-6dda73013829ccd4efe52c7929dd54e70c9.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;根据介绍，预训练中，混元-A13B 模型使用了 20 万亿高质量网络词元语料库，提升了模型推理能力的上限；完善了 MoE 架构的 Scaling Law&amp;nbsp;（即规模定律）理论体系，为 MoE 架构设计提供了可量化的工程化指导，提升了模型预训练效果。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;用户可以按需选择思考模式，&lt;strong&gt;快思考模式&lt;/strong&gt;提供简洁、高效的输出，适合追求速度和最小计算开销的简单任务；&lt;strong&gt;慢思考模式&lt;/strong&gt;涉及更深、更全面的推理步骤。这优化了计算资源分配，兼顾效率和准确性。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;此外，混元还开源了两个新数据集。其中，ArtifactsBench 主要用于代码评估，构建了一个包含 1825 个任务的新基准；C3-Bench 针对 Agent 场景模型评估，设计了 1024 条测试数据，以发现模型能力的不足。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357604</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357604</guid>
      <pubDate>Fri, 09 May 2025 08:18:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>开源 | MeiGen-MultiTalk：基於单张照片实现多人互动演绎</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;blockquote&gt; 
 &lt;p&gt;近日，美团推出了音频驱动的多人对话视频生成框架 MultiTalk，并在 GitHub 上开源，首创 L-RoPE 绑定技术，通过标签旋转位置编码精准解决多音频流与人物错位难题。该框架创新性地采用局部参数训练+多任务学习策略，在保留复杂动作指令跟随能力的同时，实现自适应动态人物定位。只需输入多人音频流、参考图像和文本提示，即可生成口型精准同步、肢体自然的交互视频，可支持影视制作、直播电商等场景的工具升级。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-1f49ae4f4fb275deb2b5f21c7d98b147cc3.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;如果给你一张图片，再给你一段语音，怎么能让它们完美融合在一起，让图片中人物自然说话和做动作，甚至多人之间还能互动起来呢？近日，美团视觉智能团队在 GitHub 上开源了一款产品&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmeigen-ai.github.io%2Fmulti-talk%2F" target="_blank"&gt;MeiGen-MultiTalk&lt;/a&gt;，它就非常巧妙地解决了这个问题。先上视频，看一下它实力如何：&lt;/p&gt; 
&lt;p&gt;1.输入图像+对话语音&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//7ad2d3b67c25b4c879c9baeab11830b8.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;2.使用 MultiTalk 生成视频&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fuser-attachments%2Fassets%2F94ce6060-4591-4179-833b-b0e7da3bd31c" target="_blank"&gt;点击查看视频&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;注：图像和音频均由 AI 生成。&lt;/p&gt; 
&lt;p&gt;还有下面这部《Smile》短片中所有镜头，也都是由 MeiGen-MultiTalk 合成的，是不是很惊艳？&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fuser-attachments%2Fassets%2Fb9a7ef25-6068-4296-ab41-1aa94e9b9545" target="_blank"&gt;点击查看视频&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;注：每个镜头首帧图像和音频来源《Smile》- Morgan Wallen&lt;/p&gt; 
&lt;p&gt;不仅仅是这种风格，还有很多其他很多类型的融合，让小猫说话，给动画片配音，甚至还让双人对唱飚高音，它也表现的相当不错。感兴趣的同学，可移步到&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmeigen-ai.github.io%2Fmulti-talk%2F" target="_blank"&gt;项目主页&lt;/a&gt;进行查看。或者查看美团技术团队微信公众号的推文：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FEJHbR0ShT53BhMTUceY9pg" target="_blank"&gt;开源 | MeiGen-MultiTalk：基於单张照片实现多人互动演绎&lt;/a&gt;。展示完毕，接下来就是最重要的部分，上链接！&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;项目主页&lt;/strong&gt; ：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmeigen-ai.github.io%2Fmulti-talk%2F" target="_blank"&gt;https://meigen-ai.github.io/multi-talk/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;开源代码&lt;/strong&gt; ：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FMeiGen-AI%2FMultiTalk" target="_blank"&gt;https://github.com/MeiGen-AI/MultiTalk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;技术报告&lt;/strong&gt; ：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2505.22647" target="_blank"&gt;https://arxiv.org/abs/2505.22647&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;一、引言：超越"会说话的头"------AI 人像视频的下一个前沿&lt;/h2&gt; 
&lt;p&gt;当前，人工智能在视觉内容生成领域取得了令人瞩目的进展，尤其是在音频驱动的人像视频方面。无论是"会说话的头"还是"会说话的身体"技术，都已能够从音频信号生成与面部动作高度同步、视觉质量令人满意的视频。这些技术在模拟单人讲话方面表现出色，例如在虚拟主播或数字替身等应用中展现出逼真的效果。&lt;/p&gt; 
&lt;p&gt;然而，现有方法在处理更复杂的场景时，其局限性也日益凸显，面对多人对话视频生成时面临三大挑战：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;多音频流输入适配&lt;/strong&gt;：如何区分并绑定不同人物的音频信号？&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;动态人物定位&lt;/strong&gt;：当人物在画面中移动时，如何精准定位其运动区域？&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;指令遵循能力&lt;/strong&gt;：如何让生成的视频严格遵循文本描述的复杂动作（如大幅肢体动作）？&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;这些挑战促使研究人员思考，AI 人像视频的下一个前沿究竟在哪里。从最初仅关注面部表情的"会说话的头"，到能够模拟全身动作的"会说话的身体"，再到如今 MultiTalk 所提出的"多人物对话视频生成"，这清晰地揭示了 AI 人像视频领域从关注局部细节到全身动作，再到模拟复杂社会互动的演进趋势。这种演进不仅仅是技术能力的简单提升，更体现了对真实世界复杂性模拟需求的增长，以及 AI 在内容创作中扮演更高级角色的潜力。用户对 AI 生成内容的"真实感"和"复杂性"要求越来越高，简单的"动起来"已不足够，现在需要 AI 能够"自然地互动"并"理解和执行复杂指令"。&lt;/p&gt; 
&lt;h2&gt;二、MultiTalk 的框架图：如何实现 AI 对话视频生成&lt;/h2&gt; 
&lt;p&gt;MultiTalk 实现音频驱动的多人物对话视频生成的技术框架，如下图 2 所示：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//1692b1d06eb9109b7420450994f1fcf1.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;2.1 基础模型结构：DiT 与 3D VAE&lt;/h3&gt; 
&lt;p&gt;MultiTalk 以 DiT（Diffusion-in-Transformer）为基础的视频扩散模型作为其核心骨架。DiT 模型因其在图像和视频生成方面的卓越性能而备受关注，它用 Transformer 结构替代了传统的 U-Net，能够更好地捕捉长距离依赖关系。&lt;/p&gt; 
&lt;p&gt;为了高效处理视频数据，MultiTalk 集成了 3D 变分自编码器（VAE）。3D VAE 能够对视频数据在空间和时间维度上进行压缩，将高维原始视频数据编码成更紧凑的潜在表示。这种压缩大大降低了后续扩散模型的计算负担，同时保留了关键的视觉信息。&lt;/p&gt; 
&lt;p&gt;首先，使用文本编码器，将用户输入的文本提示（例如"一个男人和女人正在舞台上唱歌"）转化为文本条件嵌入，指导视频内容的生成。其次，通过 CLIP 图像编码器提取的全局上下文信息也被注入到 DiT 模型中。这些图像上下文与文本条件通过解耦的交叉注意力机制协同作用，为生成视频提供视觉和语义指导，确保生成内容与参考图像和文本提示保持一致。&lt;/p&gt; 
&lt;h3&gt;2.2 让 AI"说话"：单人音频集成&lt;/h3&gt; 
&lt;p&gt;基础的图像到视频（I2V）扩散模型通常不原生支持音频输入。为了让模型能够"说话"，MultiTalk 在每个 DiT 块的文本交叉注意力层之后，添加了新的层，这些层包含层归一化和音频交叉注意力机制，专门用于处理和整合音频条件。&lt;/p&gt; 
&lt;p&gt;在音频嵌入的提取与上下文整合方面，MultiTalk 采用了 Wav2Vec，这是一种广泛使用的音频特征提取器，能够将音频波形转换为高维的音频嵌入。在音频驱动的人体视频中，当前时刻的动作不仅受当前音频帧影响，也受前后音频帧的影响。因此，MultiTalk 遵循现有方法，将与当前帧相邻的音频嵌入进行拼接（通过上下文长度 k 参数控制），形成更具时间上下文信息的音频嵌入，以更好地捕捉语音的动态变化。&lt;/p&gt; 
&lt;p&gt;一个重要的挑战是，由于 3D VAE 对视频数据进行了时间压缩，视频潜在空间的帧长度通常比原始音频嵌入的帧长度短，这使得两者之间无法直接进行帧对帧的交叉注意力计算。为了解决这种时序长度不匹配的问题，MultiTalk 使用了一个音频适配器。该适配器通过一系列操作对音频嵌入进行压缩和对齐：首先将输入音频嵌入分割为初始帧和后续帧；然后对后续帧进行下采样；接着分别通过多个 MLP 层编码初始帧和下采样后的后续帧；将编码后的特征拼接起来；最后，再次通过 MLP 层对拼接后的特征进行编码，从而获得与视频潜在空间帧长度匹配的压缩音频条件。音频适配器解决了视频和音频数据固有的时间粒度不匹配问题，确保了信息流的顺畅，使得不同模态的数据能够高效地在同一框架内进行交互。&lt;/p&gt; 
&lt;h3&gt;2.3 核心挑战：当多重声音让 AI"困惑"&lt;/h3&gt; 
&lt;p&gt;与单人视频相比，多人物对话视频生成带来了多重复杂性，这些是现有方法无法解决的。首先，对话场景中，音频信号来自多个人物，模型需要能够同时、独立地处理这些不同的音频流，这是"多流音频输入处理"的挑战。其次，也是最核心的挑战之一，是"音频与人物的精确绑定"。必须确保视频中的每个人物只由其对应的音频流驱动，以防止唇形同步错误地出现在所有人物身上，导致不自然的"齐声说话"现象，这在真实对话中是极不自然的。最后，生成视频中的人物是动态的，他们的位置和姿态会随着对话和动作而变化。这要求模型具备一种"自适应方法"，能够精确追踪每个人物在视频帧中的运动区域，以便将音频准确地映射到正确的视觉区域。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//8563dcaa19a0ce3dd0f9b554af583220.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在探索多流音频注入方案时，MultiTalk 尝试了多种直觉性的方法，如上图 3 所示。但多数都未能有效解决音频与人物的绑定问题，这凸显了问题本身的复杂性，并非简单的拼接或分割就能解决。最初的尝试包括：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;直接拼接多流音频嵌入&lt;/strong&gt;： 将多流音频的嵌入直接拼接起来，然后与视频潜在空间进行交叉注意力计算。然而，这种方法未能将拼接后的多流音频与视频中对应的特定人物区域绑定，导致混乱的同步。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;分别计算后相加&lt;/strong&gt;： 分别计算每个音频流与视频潜在空间的交叉注意力结果，然后将这些结果相加。然而，这种方法同样未能解决绑定问题，模型无法区分哪个音频应该驱动哪个人物。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;分割视频潜在空间（左右区域）&lt;/strong&gt;： 考虑到视频中人物通常位于左右两侧，MultiTalk 尝试将视频潜在空间简单地分割成左右两部分，并让每个部分与对应的音频流计算注意力。虽然这种方法在一定程度上成功绑定了多流音频到不同人物，但其泛化能力极其有限。它仅适用于人物动作范围很小的视频；一旦人物出现大范围移动或交叉，这种简单的空间分割就会导致音频绑定失败。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;这些传统方法失败的根本原因在于它们缺乏自适应的对动态主体进行定位的能力。直接拼接、简单相加或基于固定空间位置的分割，无法让模型理解哪个音频流应该对应视频中哪个动态变化的人物。缺乏这种深层的"人物感知"和"语义绑定"机制，导致了"错误绑定"------所有人都同步说话，这在对话场景中是极不自然的，严重影响了生成视频的真实感和可用性。&lt;/p&gt; 
&lt;h3&gt;2.4 让 AI"交谈"：L-ROPE 实现无缝多人物绑定&lt;/h3&gt; 
&lt;p&gt;为了解决这个问题，MultiTalk 提出了 L-ROPE。在应用 L-ROPE 进行音频绑定之前，MultiTalk 首先需要解决一个基础问题：如何在视频中动态地识别并追踪每个人物的位置。给定包含多个人物的参考图像，模型首先识别出每个人物的掩码区域以及背景掩码。在 DiT 模型中，视频的第一帧通常作为参考图像。MultiTalk 利用"参考图像到视频的自注意力图"。如图 4a），通过计算视频潜在空间中每个 Token 与参考图像中每个人物掩码的平均相似度，模型能够得到一个相似度矩阵。利用这个相似度矩阵，模型可以自适应地确定视频中每个潜在 Token 属于哪个人物或背景，从而实现了对每个人物的动态定位和追踪。&lt;/p&gt; 
&lt;p&gt;Label Rotary Position Embedding （L-ROPE）是 MultiTalk 的核心创新，它基于 ROPE（Rotary Position Embedding）的思想。ROPE 是一种在大型语言模型（LLMs）和视频扩散模型中广泛使用的相对位置编码技术，以其在捕捉 Token 间关系和处理时空信息方面的卓越能力而闻名。L-ROPE 的创新之处在于，它将"类别标签"融入到位置编码中，从而在 DiT 块的音频交叉注意力层中，实现了多流音频与多个人物的精准绑定。&lt;/p&gt; 
&lt;p&gt;在标签分配策略上，视频潜在空间包多个类别，比如多个人物和背景的区域。MultiTalk 为每个人物分配了一个特定的数值范围作为标签（例如，第一个人物的视觉标签范围是{0-4}，第二个人物是{20-24}）。视频潜在空间中每个 Token 的最终标签，是根据其与对应人物掩码的相似度，通过归一化函数在这个范围内计算得出的。背景区域则被赋予一个静态标签，以确保它不与任何音频流关联，避免背景元素被音频驱动。对于多流音频嵌入，MultiTalk 首先将它们拼接起来，然后为每个音频流分配一个静态的、唯一的标签。为了与视频中的人物绑定，这些音频标签被精心选择，与对应人物的视觉标签范围"接近"或"匹配"（例如，第一个音频流标签为 2，第二个音频流标签为 22）。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//368e33e0bdb9af15aa2b5d480b2b1de5.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;L-ROPE 的特点在于它将"类别信息"（哪个像素属于哪个人物类或背景类）巧妙地融入了"位置编码"中。传统的 ROPE 处理的是纯粹的时空位置信息，而 L-ROPE 则更进一步，将"类别"信息编码进去。它使得模型能够区分场景中的不同个体。在音频交叉注意力机制中，Q（来自视频潜在空间）和 K（来自多流音频嵌入）都经过 L-ROPE 处理。通过这种带有语义标签的旋转，当视频潜在空间中某个区域（例如，对应人物 1 的区域）的标签与音频 1 的标签"匹配"时，它们之间的注意力权重就会被有效激活，从而强制模型将音频 1 的驱动作用集中到人物 1 身上，解决了不正确的绑定问题，如图 4c)。这种策略能够有效激活音频交叉注意力图中的特定区域，从而确保音频与对应人物的唇形和动作精确同步。&lt;/p&gt; 
&lt;p&gt;为了验证 L-ROPE 的有效性，论文进行了一项消融研究，重点关注标签范围的选择。实验结果（如下表 3 所示）表明，即使为不同人物选择不同的标签范围，所产生的性能指标接近。这说明 L-ROPE 对具体的标签范围变化不敏感。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//3180443fcdf3b17a4c8b2edea7406776.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;2.5 训练策略&lt;/h3&gt; 
&lt;p&gt;MultiTalk 框架采用了多项训练策略，这些策略共同确保了模型在多人物场景下的高性能、精确的音频同步以及指令遵循能力。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;1. 两阶段训练：循序渐进的技能提升&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;MultiTalk 的训练过程被划分为两个阶段，旨在逐步增强模型的音频处理和唇形同步能力。第一阶段的主要目标是开发模型对单人视频的强大能力，此阶段模型使用单人说话视频数据集进行训练。在模型掌握了单人视频能力之后，进入第二阶段。第二阶段使用专门收集的包含双流音频的训练数据，以促进模型学习多人物视频和交互。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2. 部分参数训练：精准调优，避免退化&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;这是 MultiTalk 训练中的一个关键策略。在整个训练过程中，研究者仅更新音频交叉注意力层和音频适配器中的网络参数，而冻结了所有其他基础模型的网络参数。论文发现表明，在计算资源和数据量有限的情况下，如果进行全参数训练，会导致模型指令遵循能力的显著下降（特别是对于复杂的动作和人物交互），甚至可能引起生成视频中手部和物体变形等视觉伪影。相反，通过仅训练与音频输入直接相关的特定层，MultiTalk 能够很好地保留基础模型原有的强大指令遵循能力，并避免了上述视觉退化问题。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;3. 多任务训练：丰富场景理解，强化指令遵循&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;MultiTalk 采用了多任务混合训练范式，将模型训练任务分为音频+图像到视频（AI2V）训练和图像到视频（I2V）训练。尽管任务不同，但它们共享相同的网络参数。在 AI2V 任务中，模型同时使用参考图像和音频作为条件输入，专注于学习音频驱动的唇形同步和动作生成。在 I2V 任务中，音频条件被移除（通过将音频嵌入置零）。I2V 任务使用的训练数据是独特的，主要包含大量多事件视频。这些视频涵盖了人物、物体和场景之间复杂的交互，例如人物拿起杯子、与环境互动等。这种多事件数据集对于确保模型能够准确理解和执行文本提示中描述的复杂动作和交互至关重要。论文指出，如果仅使用说话的头和身体数据进行 AI2V 训练，网络的指令遵循能力会显著削弱。然而，通过将 I2V 训练纳入多任务范式，模型能够有效地保留其强大的指令遵循能力，从而生成更符合用户意图的视频，如下图 5 所示。这种策略体现了泛化与鲁棒性，即通过多任务训练，在保持特定任务能力的同时，增强模型的通用理解和指令遵循能力。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//77370c360f865826c0a5aeb1f35060db.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;2.6 长视频生成&lt;/h3&gt; 
&lt;p&gt;尽管 MultiTalk 模型能够生成高质量的短视频（例如 3-5 秒），但这对于实际应用场景（如制作电影片段、直播内容）来说远远不够，因为这些场景通常需要持续更长的视频。为了突破单次生成长度的限制，MultiTalk 引入了一种基于自回归（Autoregressive）的方法来生成长视频。将之前生成视频的末尾部分作为条件，来生成新的视频片段，从而实现时间上的连续性和扩展。&lt;/p&gt; 
&lt;p&gt;在具体的实现机制上，传统的图像到视频（I2V）模型通常只使用视频的第一帧作为生成后续帧的条件。MultiTalk 在此基础上进行了关键改进。在生成新的视频片段时，它不再仅仅依赖第一帧，而是将先前已生成视频的最后 5 帧作为额外的条件输入到当前的推理步骤中。这使得模型能够"记住"并延续之前的动作和场景状态。这些作为条件的 5 帧视频，首先会通过 3D VAE 进行压缩，将其转化为更紧凑的 2 帧潜在噪声表示。随后，为了匹配 DiT 模型的输入格式，新的视频帧（除了从历史信息得来的 2 帧潜在噪声）会用零填充。这些填充的帧、来自历史信息的潜在噪声以及一个视频掩码被拼接在一起，形成完整的输入。最终，这个包含历史上下文信息的输入被送入 DiT 模型进行推理，生成新的视频片段。下面视频展示了生成结果的流畅性。&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;输入图像+对话语音&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//d68a049c134922b777999b4d82e04f34.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;使用 MultiTalk 生成视频&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fuser-attachments%2Fassets%2Fec77e7e3-f03b-41ac-810b-49fd8d70968a" target="_blank"&gt;点击查看视频&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;注：图像和音频源于《破产姐妹》。&lt;/p&gt; 
&lt;h2&gt;三、MultiTalk 实战：性能表现&lt;/h2&gt; 
&lt;p&gt;MultiTalk 的性能通过广泛的实验进行了验证，包括与现有最先进方法的定量和定性比较，充分展示了其在多人物对话视频生成方面的能力。&lt;/p&gt; 
&lt;p&gt;在数据集与评估指标方面，MultiTalk 的训练数据集在第一阶段使用了约 2K 小时的单人说话视频，用于学习基础的音频驱动视频能力；第二阶段则使用了 100 小时的双人对话视频，用于专门训练多人物交互和绑定。MultiTalk 在三类不同的测试数据集上进行了评估：说话的头数据集（HDTF 和 CelebV-HQ ）、说话的身体数据集（EMTDT ）以及双人说话身体数据集（MTHM）。评估采用了行业内通用的多维度指标：FID (Frechet Inception Distance) 和 FVD (Fréchet Video Distance) 用于评估生成数据质量；E-FID (Expression-FID) 用于评估生成视频中面部表情的表现力；Sync-C 和 Sync-D 用于精确测量生成视频中唇部动作与音频的同步程度。&lt;/p&gt; 
&lt;p&gt;在定量评估中，MultiTalk 在说话的头和说话的身体生成任务上，与 AniPortrait、VExpress、EchoMimic、Hallo3、Sonic、Fantasy Talking 等多个最先进的方法进行了对比。结果显示，MultiTalk 在大多数指标上超越了这些方法，尤其在唇形同步（Sync-C, Sync-D）和视频质量（FID, FVD）方面表现出卓越性能。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//3e0ee05f747d8947c63d6637507dc3ee.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;此外，我们还专门探讨了多流音频训练是否会导致单人视频性能下降的问题（具体可以参考&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2505.22647" target="_blank"&gt;论文&lt;/a&gt;）。实验结果（表 1 和表 2 中"MultiTalk-single"与"MultiTalk-multiple"的对比）显示，MultiTalk 的多人视频模型在单人数据集上表现与单人视频模型相当。这表明，MultiTalk 在引入多人物处理能力时，并未牺牲原有的单人视频性能，实现了能力的无损叠加。&lt;/p&gt; 
&lt;p&gt;在定性评估中，MultiTalk 取得了不错的效果，如下图 6 所示。其显著优势之一是强大的指令遵循能力。当提供复杂的文本提示（例如"一个男人合上笔记本电脑并放在桌上"、"一个女人戴着耳机坐在桌旁，然后她拿起耳机"）时，MultiTalk 能够成功生成精确响应这些指令的视频，而其他同类方法则难以做到，往往出现动作不符或物体变形。MultiTalk 生成的视频中，视觉伪影（如手部或物体扭曲）显著减少，整体视觉质量更高，画面更自然真实。作为首个专门针对多人物生成任务设计的方法，MultiTalk 在处理复杂的交互场景时表现出色。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//f3b1a346abe9f61b3fb9bcce7710c261.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;与简单的"视频拼接"方法（即将左右人物视频分别生成再拼接）相比（如下图 7 所示），MultiTalk 能够有效处理人物间的互动，避免了拼接方法中常见的左右片段不一致性问题，使得多人物对话和互动更加流畅自然。论文还通过可视化自注意力图，直观地展示了 MultiTalk 能够自适应地识别视频中特定人物的定位，这进一步证明了 L-ROPE 方法在实现精确音频绑定方面的有效性。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//e2edf1964be1420e0dcd5b658a2c58cb.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h2&gt;四、总结与展望&lt;/h2&gt; 
&lt;p&gt;MultiTalk 提出一种音频驱动多人物对话视频生成方案，其核心突破在于其创新的 L-ROPE 方法，它通过结合自适应人物定位和带有类别信息的标签编码，有效解决了多流音频的注入和人物绑定这一难题。此外，其精心设计的部分参数训练和多任务训练策略，确保了模型在有限资源下依然能够保持强大的指令遵循能力和高质量的视觉输出。&lt;/p&gt; 
&lt;p&gt;MultiTalk 的诞生，预示着其在多角色电影制作、虚拟直播、游戏开发、教育内容创作等领域具有广阔的应用前景。我们深信，未来它将极大地降低多角色视频的制作门槛，使个性化、交互式内容创作变得更加高效和便捷。尽管仍存在真实音频与合成音频的性能差距等局限，但 MultiTalk 为未来的研究指明了方向。我们期待 MultiTalk 及其后续研究能够进一步推动 AI 在模拟和创造复杂人机交互方面的能力，使数字世界中的人物更加栩栩如生。&lt;/p&gt; 
&lt;p&gt;现在，MultiTalk 已经在 GitHub 上&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FMeiGen-AI%2FMultiTalk" target="_blank"&gt;开源&lt;/a&gt;，欢迎更多的同学加入我们，一起共建。&lt;/p&gt; 
&lt;h2&gt;五、关于美团视觉智能部&lt;/h2&gt; 
&lt;p&gt;美团视觉智能部围绕丰富的本地生活电商场景，建设从基础通用到细分领域的视觉技术能力，包括：视觉生成大模型、多模交互虚拟人，助力营销创意生产和商家低成本直播；文档、商品、安全多模态大模型，助力商家开店经营、平台商品治理和违规账号治理；人脸识别、文字识别、细粒度图像分析、高性能检测分割、街景理解，成为公司基础设施能力。曾开源行业最大规模&lt;a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2F123.57.42.89%2FFoodProject.html" target="_blank"&gt;食品图像数据集 Food2K&lt;/a&gt;被全球各地区上百家机构使用，&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fmeituan%2FYOLOv6" target="_blank"&gt;目标检测框架 YOLOV6&lt;/a&gt;荣登 2023 年度世界开源贡献榜，获得 10+项国际竞赛冠军，上百项发明专利，60+篇顶会顶刊论文。曾与国内多家知名科研机构合作，多次获得省部级科技进步奖项。&lt;/p&gt; 
&lt;p&gt;| 关注「美团技术团队」微信公众号，在公众号菜单栏对话框回复【2024 年货】、【2023 年货】、【2022 年货】、【2021 年货】、【2020 年货】、【2019 年货】、【2018 年货】、【2017 年货】等关键词，可查看美团技术团队历年技术文章合集。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//2b7e833c2041e319a96878b7c8c571f0.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;| 本文系美团技术团队出品，著作权归属美团。欢迎出于分享和交流等非商业目的转载或使用本文内容，敬请注明 "内容转载自美团技术团队"。本文未经许可，不得进行商业性转载或者使用。任何商用行为，请发送邮件至 &lt;a href="https://www.oschina.net/action/GoToLink?url=mailto%3Atech%40meituan.com" target="_blank"&gt;tech@meituan.com&lt;/a&gt; 申请授权。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/meituantech/blog/18638557</link>
      <guid isPermaLink="false">https://my.oschina.net/meituantech/blog/18638557</guid>
      <pubDate>Fri, 09 May 2025 07:30:00 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>Rust 1.88.0 发布：引入裸函数 (Naked Functions)、改进链式 let 语句</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Rust 1.88.0 已正式&lt;u&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.rust-lang.org%2F2025%2F06%2F26%2FRust-1.88.0%2F" target="_blank"&gt;发布&lt;/a&gt;&lt;/u&gt;。&lt;/p&gt; 
&lt;p&gt;Rust 1.88 引入了支持编写没有编译器生成和前导代码的裸函数，以允许对该函数生成的汇编代码拥有完全控制。裸函数使用 &lt;code&gt;_#\[unsafe(naked)\]_&lt;/code&gt; 属性进行标记。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-2fe189cbfe1916654a90d164d7b95825701.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Rust 1.88 还添加了在 if 和 while 语句中使用 "&amp;amp;&amp;amp;" 链式连接 let 语句的支持。&lt;/p&gt; 
&lt;p&gt;在 Cargo 方面，Rust 1.88 为 Cargo 带来了自动缓存清理功能，通过在主目录内运行垃圾回收来清理缓存。Cargo 目前设置为在三个月内未被访问的从网络下载的文件将被删除。&lt;/p&gt; 
&lt;p&gt;Rust 1.88 还为 cfg 条件谓词语言引入了布尔字面量的支持，并稳定了一些 API，同时将 i686-pc-windows-gnu 目标降级为二级架构。&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.rust-lang.org%2F2025%2F06%2F26%2FRust-1.88.0%2F" target="_blank"&gt;详情查看发布公告&lt;/a&gt;。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357590/rust-1-88-0</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357590/rust-1-88-0</guid>
      <pubDate>Fri, 09 May 2025 07:24:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>没人喜欢写 README？Gitee：现在你不用写了</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;README 是开源项目的第一张脸，它决定着项目的专业度，也直接影响用户能否快速上手。但你我都知道，写代码有意思，写文档太难了。尤其在项目初期，README 不是空白，就是随便应付两句草草了事。&lt;/p&gt; 
&lt;p&gt;现在，Gitee 帮你解决这个老大难问题——&lt;strong&gt;全新上线的「AI README 生成与优化」，能一键生成结构清晰、内容专业的项目文档&lt;/strong&gt;，还能自动优化现有 README，让你的项目门面焕然一新！&lt;/p&gt; 
&lt;h2&gt;核心亮点&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;自动识别项目类型&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;深度分析依赖、配置文件与代码结构，精准提取关键信息。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;社区最佳实践模板&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;内置项目简介、快速开始、使用指南、贡献规范、许可证等完整模块。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;一键生成 &amp;amp; 智能优化&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;适配新旧仓库：没有 README 时快速生成，有 README 时自动补全、重排结构、提升可读性。&lt;/p&gt; 
&lt;h2&gt;上手指南&lt;/h2&gt; 
&lt;p&gt;该功能已向所有 Gitee 社区版&amp;amp;企业版开源仓库开放，新老项目均可直接体验。&lt;/p&gt; 
&lt;h3&gt;进入仓库主页&lt;/h3&gt; 
&lt;p&gt;Gitee 会自动检测是否存在 README。若缺失，将显示「AI 生成 README」按钮。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0627/150448_MUT2_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;启动 AI 生成&lt;/h3&gt; 
&lt;p&gt;点击「AI 生成 README」按钮后，马建仓 AI 助手将：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;🔍 分析仓库内容&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;📊 摘取关键信息&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;📝 生成专业 README&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;📤 将变更以轻量级 PR 提交&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0627/150509_02hr_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;审核并合并 PR&lt;/h3&gt; 
&lt;p&gt;检查自动生成的内容，确认无误后合并即可。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0627/150522_Aq2Q_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0627/150531_yTAj_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;完成展示&lt;/h3&gt; 
&lt;p&gt;合并后，新 README 会立即呈现在仓库首页。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0627/150541_gohB_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;如何优化现有 README？&lt;/h3&gt; 
&lt;p&gt;对于已有 README 文件的项目，也可使用 AI 优化功能：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;补全缺失板块（贡献指南、许可证等）&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;重排段落结构、统一格式&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;强化示例代码与项目特色描述&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0627/150552_701l_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h2&gt;智能开发，就在 Gitee&lt;/h2&gt; 
&lt;p&gt;AI README 生成与优化功能，是 Gitee 在智能开发领域迈出的又一步。它回应了开发者在文档撰写上的真实痛点，&lt;strong&gt;将重复性工作交由 AI 自动完成&lt;/strong&gt;，让开发者能够专注于更有价值的创新；同时通过标准化模板与智能润色机制，&lt;strong&gt;帮助项目迅速建立专业形象，提升传播效果&lt;/strong&gt;；清晰、完善的文档结构，也&lt;strong&gt;为协作与社区建设提供了坚实基础&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;未来，Gitee 将持续拓展智能工具链能力，打造更开放、更高效的开发环境，让 AI 真正融入开发全流程，服务每一位开发者与每一个项目。&lt;/p&gt; 
&lt;p&gt;欢迎体验：&lt;a href="https://gitee.com/"&gt;https://gitee.com/&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357583</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357583</guid>
      <pubDate>Fri, 09 May 2025 07:06:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>谷歌发布 Gemma 3n，专为移动设备打造的全新 AI 模型</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Google&lt;u&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdevelopers.googleblog.com%2Fen%2Fintroducing-gemma-3n-developer-guide%2F" target="_blank"&gt;宣布&lt;/a&gt;&lt;/u&gt;推出 Gemma 3n，这是其下一代的开放 AI 模型，与我们之前看到的相比有了显著的提升。继上个月在 Google I/O 大会上进行预览后，完整版现已发布，可直接在移动硬件上运行。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-3a9a3ee99131ac4d39d56d44480a911816f.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;与 Gemini 的不同之处在于，Gemma 是为开发者下载和修改而设计的，而 Gemini 是 Google 的封闭式专有模型。&lt;/p&gt; 
&lt;p&gt;该模型现在可以原生处理图像、音频和视频等输入并生成文本，这比仅仅基于文本的模型有了很大的飞跃。它甚至可以在内存仅为 2GB 的硬件上运行，并且据称在编码和推理等任务上表现更佳。以下是 Google 列出的所有改进：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;多模式设计： Gemma 3n 原生支持图像、音频、视频和文本输入和文本输出。&lt;/li&gt; 
 &lt;li&gt;专为设备端优化： Gemma 3n 型号以效率为设计重点，提供两种基于有效参数的尺寸：E2B 和 E4B。虽然它们的原始参数数量分别为 5B 和 8B，但架构创新使其运行内存占用与传统的 2B 和 4B 型号相当，仅需 2GB (E2B) 和 3GB (E4B) 内存即可运行。&lt;/li&gt; 
 &lt;li&gt;突破性的架构： Gemma 3n 的核心是新颖的组件，例如用于计算灵活性的 MatFormer 架构、用于提高内存效率的每层嵌入 (PLE) 以及针对设备用例优化的新型音频和基于 MobileNet-v5 的视觉编码器。&lt;/li&gt; 
 &lt;li&gt;增强质量： Gemma 3n 在多语言（支持 140 种文本语言和 35 种语言的多模式理解）、数学、编码和推理方面实现了质量改进。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Gemma 3n 高效的核心是 Google 称之为 MatFormer 的新架构。Google 用俄罗斯套娃的比喻来描述它&lt;strong&gt;：一个较大的模型里面包含一个较小的、功能齐全的版本&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;这使得单个模型能够以不同的规模运行不同的任务。至于基准测试，更大的 E4B 模型是第一个在 10B 参数下突破 LMArena 1300 分的模型。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-429797bc92f4d938d5e5f8d3ab6c9e9af2b.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;该模型的音频功能现在支持设备上的语音转文本和翻译，并使用能够精细处理语音的编码器。视觉方面则由名为 MobileNet-V5 的全新编码器提供支持，该编码器比其前代产品速度更快、效率更高。它能够在 Google Pixel 设备上以高达 60FPS 的速度处理视频。&lt;/p&gt; 
&lt;p&gt;如果您有兴趣，可以立即开始使用，因为这些模型可以通过 Hugging Face 和 Kaggle 等熟悉的平台获得，您甚至可以直接在 Google AI Studio 中对它们进行试验：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Faistudio.google.com%2Fprompts%2Fnew_chat%3Fmodel%3Dgemma-3n-e4b-it" target="_blank"&gt;https://aistudio.google.com/prompts/new_chat?model=gemma-3n-e4b-it&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;u&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdevelopers.googleblog.com%2Fen%2Fintroducing-gemma-3n-developer-guide%2F" target="_blank"&gt;更多详情请参阅官方公告帖&lt;/a&gt;&lt;/u&gt;。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357582/google-gemma-3n</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357582/google-gemma-3n</guid>
      <pubDate>Fri, 09 May 2025 06:54:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Jakarta EE 11 发布，增强企业 Java 开发人员生产力和性能</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="margin-left:0; margin-right:0; text-align:start"&gt;&lt;span style="color:#4c4d4e"&gt;Eclipse 基金会&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblogs.eclipse.org%2Fpost%2Ftatjana-obradovic%2Fjakarta-ee-11-empowering-enterprise-java-developers-enhanced-productivity" target="_blank"&gt;宣布&lt;/a&gt;发布&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fjakarta.ee%2Frelease%2F11%2F" target="_blank"&gt;Jakarta EE 11&lt;/a&gt;平台，该平台基于之前的核心配置文件 (2024 年 12 月) 和 Web 配置文件 (2025 年 3 月) 版本构建。&amp;nbsp;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:start"&gt;&lt;span style="color:#4c4d4e"&gt;此版本标志着企业级 Java 在简化开发、提升开发人员生产力和整体性能方面的进步。主要亮点包括：现代化的测试兼容性工具包 (TCK)、全新 Jakarta Data 规范的引入、对现有规范的重大更新以及对最新 Java LTS 版本的支持，使开发人员能够充分利用 Java 21 中的增强功能，包括虚拟线程。&lt;/span&gt;&lt;/p&gt; 
&lt;h4 style="text-align:start"&gt;&lt;strong&gt;&lt;span style="color:#272726"&gt;Jakarta EE 11 的主要功能&lt;/span&gt;&lt;/strong&gt;&lt;/h4&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:start"&gt;&lt;span style="color:#4c4d4e"&gt;&lt;strong&gt;自 Jakarta EE 10 发布以来，企业级 Java 复兴进程持续加速。Jakarta EE 11 在此基础上进一步提升性能，并在 Jakarta Data 规范&lt;/strong&gt;的引入下，进一步提升了开发人员的工作效率&amp;nbsp;。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:start"&gt;&lt;span style="color:#4c4d4e"&gt;Jakarta Data 在简化企业应用程序持久化逻辑方面迈出了重要一步。主要功能包括：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;BasicRepository&amp;nbsp;&lt;/strong&gt;：基础存储库接口，为基本数据操作提供开箱即用的支持，减少样板和设置时间。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CrudRepository&amp;nbsp;&lt;/strong&gt;：在 BasicRepository 的基础上提供完整的创建、读取、更新和删除 (CRUD) 功能，实现干净、直观的数据库交互。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Pagination&lt;/strong&gt;：包括对基于偏移量和基于游标的分页支持，为开发人员提供灵活的工具来有效地管理大型数据集。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Query Language&lt;/strong&gt;：引入一种简洁、专用的查询语言，可直接简化 Jakarta 数据存储库中的方法级查询定义。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4 style="text-align:start"&gt;&lt;strong&gt;&lt;span style="color:#272726"&gt;TCK 的现代化&lt;/span&gt;&lt;/strong&gt;&lt;/h4&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:start"&gt;&lt;span style="color:#4c4d4e"&gt;此次发布的一大亮点是对 Jakarta EE 平台测试兼容性套件 (TCK) 的现代化升级和增强。通过整合 JUnit 5 和 Maven 等现代测试工具，这项举措提升了可维护性和灵活性，使 TCK 能够更轻松地与平台一同发展。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:start"&gt;&lt;span style="color:#4c4d4e"&gt;这些更新简化了兼容性测试，降低了贡献新测试的门槛，有助于推动 Jakarta EE 生态系统未来的创新。因此，贡献者可以更轻松地上手，而有意参与的新供应商或开发者也将发现加入和贡献的便捷性显著提升。有意参与的人士可以通过&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fprojects.eclipse.org%2Fprojects%2Fee4j.jakartaee-tck" target="_blank"&gt;&amp;nbsp;Jakarta EE TCK 项目页面&lt;/a&gt;了解更多信息并积极参与。&amp;nbsp;&lt;/span&gt;&lt;/p&gt; 
&lt;h4 style="text-align:start"&gt;&lt;strong&gt;&lt;span style="color:#272726"&gt;精简规范&lt;/span&gt;&lt;/strong&gt;&lt;/h4&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:start"&gt;&lt;span style="color:#4c4d4e"&gt;一项显著的变化是&amp;nbsp;&lt;strong&gt;移除了已弃用的托管 Bean 规范&lt;/strong&gt;，该规范早已被更灵活、更强大的替代方案所取代。此次清理有助于降低遗留系统的复杂性，并明确了未来推荐的编程模型。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:start"&gt;&lt;span style="color:#4c4d4e"&gt;为此，该平台高度重视&amp;nbsp;&lt;strong&gt;上下文和依赖注入 (CDI)&amp;nbsp;&lt;/strong&gt;作为核心编程模型。Jakarta EE 11 进一步增强了 CDI，使其成为托管 Bean 的标准替代方案。该版本还通过&amp;nbsp;&lt;strong&gt;支持各种规范（例如 Jakarta Bean Validation）的 Java Records 功能，进一步强化了其对现代 Java 特性的承诺&lt;/strong&gt;。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:start"&gt;&lt;span style="color:#4c4d4e"&gt;最后，Jakarta EE 11 与 Java 平台的方向保持一致，&amp;nbsp;&lt;strong&gt;删除了对 Java SE SecurityManager 的所有引用&lt;/strong&gt;，遵循了 JEP 411 中概述的弃用规定。&lt;/span&gt;&lt;/p&gt; 
&lt;h4 style="text-align:start"&gt;&lt;strong&gt;&lt;span style="color:#272726"&gt;Leveraging Java 21 Enhancements&lt;/span&gt;&lt;/strong&gt;&lt;/h4&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:start"&gt;&lt;span style="color:#4c4d4e"&gt;Jakarta EE 11 支持 Java 17 或更高版本，并为 Java 21 用户提供了独特的增强功能。其中最显著的功能之一是更新的并发规范，使开发人员能够利用 Java 21 中的虚拟线程。这可以高效处理并发任务，而无需承担传统线程管理的开销，从而显著提升性能。&lt;/span&gt;&lt;/p&gt; 
&lt;h4 style="margin-left:0px; margin-right:0px; text-align:start"&gt;&lt;strong&gt;&lt;span style="color:#4c4d4e"&gt;接下来&lt;/span&gt;&lt;/strong&gt;&lt;/h4&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:start"&gt;&lt;span style="color:#4c4d4e"&gt;Jakarta EE 12 的开发工作已在进行中，计划于 2026 年发布。下一版本旨在将平台的 API 源代码级别提升至 Java SE 21，并瞄准 Java SE 25 的运行时支持。社区正在积极推进多项规范的增强，包括可能引入 Jakarta Query 和 Jakarta MVC 等功能，并持续改进 Jakarta NoSQL。Jakarta EE 秉承既定的两年发布周期，继续优先考虑长期规划和持续创新。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357577/jakarta-ee-11</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357577/jakarta-ee-11</guid>
      <pubDate>Fri, 09 May 2025 06:41:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>我国自主研发首套航空运输大模型发布</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;我国自主研发的首套航空运输大模型天牧」低空大模型日前在南京发布，该大模型可以作为空中交通指挥专家，同时具备智能问答、辅助决策等核心能力，其研发在低空智能管理领域创下多项技术首发成果，实现了多项关键技术的突破。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;「天牧」低空大模型属于低空飞行智慧大脑「天行」中枢的系列产品，该系列产品可以作为空中交通指挥专家，解决低空飞行中的空情监控、资源调度等问题。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;中国电科首席专家丁一波介绍称，「我们最主要的功能可以归纳为管、协、服三大能力。管理就是对航空器的登记注册和飞行管理工作。我们的协作主要是在相应的飞行活动过程中着力解决有效协同问题。我们的服务主要是提供情报的服务、气象的服务和我们各种飞行中的数据服务。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="367" src="https://oscimg.oschina.net/oscnet/up-be2d6109d171ae5f30cc313a4110597193e.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在「天牧」低空大模型的加持下，「天行」系统通过对超千万条低空运行规则的学习训练，在高算力集群支撑下实现了对复杂场景自主查询效率提升 50% 的技术跨越，首次实现了「AI 驱动」的低空管服模式。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;除了「天行」及「天牧」低空大模型，此次大会上还发布了聚焦低空安全的「天衞」系统以及聚焦飞行器产品研发的「天工」等系列产品。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357561</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357561</guid>
      <pubDate>Fri, 09 May 2025 05:42:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>美团在 AI 投入超百亿，由于 GPU 价格昂贵</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F-Cj0BqaRie7Jk8WNA7QRjA" target="_blank"&gt;根据《科创板日报》的报道&lt;/a&gt;，美团核心本地商业 CEO 王莆中 6 月 26 日透露，美团在 AI 上的投入非常大，每年的投入超过百亿元。&lt;/p&gt; 
&lt;p&gt;王莆中特别指出，由于图形处理器（GPU）等硬件设施价格昂贵，AI 技术的研发成本居高不下。然而，他认为这种投入是必要的，因为只有建立起坚实的 AI 基础设施，并不断推进大模型的研究开发，才能让过去十几年间积累的宝贵数据资源焕发出新的活力，从而更好地服务于用户。&lt;/p&gt; 
&lt;p&gt;根据最新的财务报告，美团在 2024 年度的研发总支出达到了 211 亿元。王莆中预测，到 2030 年，中国服务零售行业的线上化率将从当前的 9% 提升至 25%，市场规模将达到 7 万亿元人民币。届时，预计将有 300 个品牌能够开设至少一千家店铺，成为行业的领军者。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;相关阅读&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/news/354675/meituan-nocode" target="news"&gt;美团发布 AI&amp;nbsp;Coding Agent 工具「NoCode」&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/news/354604" target="news"&gt;美团王兴详解 AI 布局：No Code 平台免费开放，1680 个应用已上线&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/news/352148" target="news"&gt;美团：过去一季度内 52% 代码由 AI 生成&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357551</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357551</guid>
      <pubDate>Fri, 09 May 2025 04:03:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Ubuntu 开发商 Canonical 去年营收近 3 亿美元</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;十年前， Ubuntu Linux 背后的公司 Canonical 的营收约为 8100 万美元（2014 年），员工人数约为 337 人。当时，他们的 Linux 桌面业务仍在 OEM/ODM 预装系统、企业桌面环境以及利润丰厚的服务器/云领域站稳脚跟。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Canonical 最近提交了 2024 年年度报告，目前其营收已接近 3 亿美元，员工人数超过 1100 人&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;几天前，Canonical 向其总部所在地英国公司注册局提交了截至 2024 年 12 月 31 日的年度报告。这份报告为公众提供了一些关于这家 Ubuntu Linux 背后公司的健康状况和整体增长情况的有趣见解。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="image.webp" src="https://static.oschina.net/uploads/img/202506/27112815_bbMf.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Canonical 2024 年年终财报显示，公司营收为 2.92 亿美元，较 2023 年的 2.51 亿美元大幅增长。而前一年，也就是 2022 年，他们的营收为 2.05 亿美元。他们的毛利率也从前一年的 80% 提升至 83%。2024 年，&lt;/p&gt; 
&lt;p&gt;Canonical 招聘了 100 多名新员工，公司平均员工人数从 1034 人增长至 1175 人。2022 年，他们的平均员工人数为 858 人。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="image.webp" src="https://static.oschina.net/uploads/img/202506/27112816_FIDc.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在其 2.91 亿美元的营收中，他们报告的毛利润为 2.58 亿美元，营业利润为 1550 万美元。1550 万美元的营业利润高于 2023 年的 1120 万美元，比 Canonical 早期的情况要好得多，那时他们通常每年都处于亏损状态，并且依靠 Ubuntu 创始人 Mark Shuttleworth 的资金维持运营。随着 Ubuntu Linux 的发展，Canonical 已经稳固地站稳了脚跟，到目前为止，这种情况已经持续了很多年。&lt;/p&gt; 
&lt;p&gt;已经有一段时间没有听到任何关于 Canonical 可能进行 IPO 的传闻/谈论了……上一次是在 2022 年，据此前报道他们计划在 2023 年进行 IPO，但后来就没了下文。&lt;/p&gt; 
&lt;p&gt;无论如何，至少他们的财务状况依然强劲，并且继续朝着正确的方向前进，为明年 Ubuntu 26.04 LTS 的重大发布大步迈进。 感兴趣的朋友可以通过&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ffind-and-update.company-information.service.gov.uk%2Fcompany%2F06870835%2Ffiling-history%2FMzQ2OTM2OTYwNWFkaXF6a2N4%2Fdocument%3Fformat%3Dpdf%26download%3D0" target="_blank"&gt;英国公司注册处 (UK Companies House)&lt;/a&gt;获取完整的 Canonical 2024 年年度报告。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357540/canonical-2024-annual-report</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357540/canonical-2024-annual-report</guid>
      <pubDate>Fri, 09 May 2025 03:28:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>DeepSeek R2 推迟发布：因 H20 算力短缺、以及梁文锋对其性能尚不满意</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.theinformation.com%2Farticles%2Fdeepseeks-progress-stalled-u-s-export-controls" target="_blank"&gt;根据《The Information》的报道&lt;/a&gt;，DeepSeek 工程师在过去几个月一直致力于完善 R2 模型，但梁文锋对 R2 现在的性能还不满意，工程师团队仍在全力优化和打磨，发布时间待定。梁文峰要求模型达到更出色的结果才批准发布。&lt;/p&gt; 
&lt;p&gt;此外，由于美国出口管制导致中国市场英伟达服务器芯片（H20）短缺，R2 的大规模普及可能面临困难。&lt;/p&gt; 
&lt;p&gt;目前，大多数使用 DeepSeek R1 模型的中国云客户仍依赖 H20 芯片。报道指出，如果 DeepSeek 即将推出的 R2 模型其性能超过目前市面上的开放替代模型，预计使用量将激增，&lt;strong&gt;超出中国云平台的处理能力&lt;/strong&gt;。因为他们需要先进的英伟达芯片来运行 AI 模型。&lt;/p&gt; 
&lt;p&gt;DeepSeek 已向部分中国云公司提供了 R2 的技术规范，以指导其托管和分发模型的计划，但尚未公布具体的发布日期。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;相关阅读&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/news/352701/deepseek-r1-0528-release-notes" target="news"&gt;DeepSeek-R1-0528 更新：思考更深，推理更强&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/news/352460/deepseek-r1-0528" target="news"&gt;DeepSeek R1 模型完成小版本试升级，逻辑理解能力提升&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357535/deepseeks-progress-stalled-u-s-export-controls</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357535/deepseeks-progress-stalled-u-s-export-controls</guid>
      <pubDate>Fri, 09 May 2025 03:16:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Nacos 3.0 架构全景解读，AI 时代服务注册中心的演进</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;p&gt;作者：杨翊（席翁），柳遵飞（翼严），罗鑫（子葵）&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Nacos&lt;/strong&gt; &lt;code&gt;/nɑ:kəʊs/&lt;/code&gt;是 Dynamic &lt;strong&gt;Na&lt;/strong&gt; ming and &lt;strong&gt;Co&lt;/strong&gt; nfiguration &lt;strong&gt;S&lt;/strong&gt; ervice 的首字母简称，随着 Nacos 3.0 的发布，定位由&lt;code&gt;"更易于构建云原生应用的动态服务发现、配置管理和服务管理平台"&lt;/code&gt;升级至&lt;code&gt;" 一个易于构建 AI Agent 应用的动态服务发现、配置管理和 AI 智能体管理平台"&lt;/code&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-334a4cbf68666f5998460020dbedad24d97.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Nacos 从 2018 年 7 月宣布开源以来，已经走过了第六个年头，在这六年里，备受广大开源用户欢迎，收获许多社区大奖。Nacos 在社区共同的建设下不断成长，逐步开始帮助用户解决实际问题，助力企业数字化转型，目前已经广泛使用在国内的公司中，根据微服务领域调查问卷，Nacos 在注册配置中心领域已经成为&lt;strong&gt;国内首选&lt;/strong&gt; ，占有 &lt;strong&gt;50%+国内市场&lt;/strong&gt; 份额，被&lt;strong&gt;各行各业的头部企业&lt;/strong&gt;广泛使用。在此期间，Nacos 的部署包下载量突破 300w 次，官网每年访问用户数超过 90w 人，被国内各主流云厂商托管服务。&lt;/p&gt; 
&lt;p&gt;随着 AI 时代到来以及 Nacos 3.0 版本的正式发布，Nacos 未来的演进目标以及架构也会随之升级。本文会对比 Nacos 3.0 与 Nacos 2.0 的架构异同，对 Nacos 3.0 的主要功能原理进行介绍。&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Nacos 2.0 架构回顾&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr&gt; 
&lt;p&gt;Nacos 2.0 的架构主要聚焦对&lt;code&gt;性能&lt;/code&gt;和&lt;code&gt;可扩展性&lt;/code&gt;进行优化和提升。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-92aef5263493adf4644595a964f278a1c2f.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;对于&lt;code&gt;性能&lt;/code&gt;升级，Nacos 2.0 通过将通信模型从 HTTP 升级至 gRPC，从短连接模型升级到长连接模型，使得 Nacos 的通信吞吐量中极大提升；同时配合数据存储和数据结构模型的升级，进一步减少核心操作所涉及的步骤和链路，最终实现性能的 &lt;strong&gt;10 倍提升&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;关于&lt;code&gt;可扩展性&lt;/code&gt;升级，Nacos 2.0 通过将一些具有个性化需求的通用能力进行抽象，进行插件化改造的方式，允许 Nacos 用户和运维人员能够开发自定义插件，适配个人或企业的个性化需求。&lt;/p&gt; 
&lt;p&gt;虽然 Nacos 2.0 在&lt;code&gt;性能&lt;/code&gt;和&lt;code&gt;可扩展性&lt;/code&gt;实现了一些突破，但仍然还存在一些挑战。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-37c34e83fbdfb2c9872dfe9451498e525a9.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;其中一个主要的挑战就是 Nacos 的安全风险。比如：Nacos 2.0 中所有的 HTTP API 均使用 8848 端口， 这其中及包含了 1.X 客户端使用的 API，也包含了运维人员以及控制枱的 API。不同类型的 API， 对于权限的需求其实是不同的，对于网络访问的连通性要求也是不同的。使用单端口并且使用唯一的鉴权开关，导致了网络的访问控制，以及鉴权控制都不是很灵活。许多用户为了方便使用，将此端口暴露在办公网甚至公网环境，同时未开启鉴权，这就造成了安全风险。&lt;/p&gt; 
&lt;p&gt;另一个问题就是默认命名空间的使用，Nacos 最初的版本中定义了命名空间作为数据资源的强隔离属性，不同命名空间之间的服务和配置不能互相发现和获取；但在最初版本中因为历史原因，注册中心和配置中心对于默认命名空间的处理方式有一定的不统一，这导致了许多用户在使用默认命名空间时经常配置错误或者出现疑惑；并且在 Nacos 2.0 提供各种插件能力之后，许多插件实现时需要额外工作进行适配，严重阻碍了插件的开发以及插件的稳定性。&lt;/p&gt; 
&lt;p&gt;随着 AI 时代来临，AI Agent 应用的部署形态在之前云原生可弹性可伸缩的基础上，要求更加轻量，更加弹性，例如 FC 场景；在这种要求下，我们需要考虑 Nacos 之前的服务发现和配置管理的能力是否还能承载 AI Agent 的应用的部署。同时，随着越来越多的 AI Agent 的应用贯穿业务全线，Nacos 能否帮助更好地管理 AI Agent 的应用，是 Nacos 在当前的挑战，同时也是新的机遇。&lt;/p&gt; 
&lt;p&gt;为了应对这些挑战以及机遇，Nacos 3.0 架构也做了对应的升级。目标是在 AI 时代成为更安全的 Registry。设计理念也由之前的&lt;code&gt;一个更易于构建云原生应用的动态服务发现、配置管理和服务管理平台&lt;/code&gt;升级为&lt;code&gt;一个易于构建 AI Agent 应用的动态服务发现、配置管理和 AI 智能体管理平台&lt;/code&gt;。&lt;/p&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Nacos 3.0 架构&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr&gt; 
&lt;h3&gt;2.1 Nacos 3.0 整体架构解析&lt;/h3&gt; 
&lt;p&gt;Nacos 3.0 升级后的整体架构仍然以一致性协议、通信模块、其他模块等通用功能模块为基座，承载出注册中心、配置中心、AI Registry、协议增强等功能；同时通过各类多语言 SDK，桥接各个生态组件。架构的左右两侧，分别是 Nacos 的插件以及 Nacos 的一些拓展组件，它们一起构成了 Nacos 3.0 的整体架构。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-36bacdaa811ec267912daebb99f3bff0018.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;我们来重点关注 Nacos 3.0 的新增能力，即图中绿色和棕色的部分。&lt;/p&gt; 
&lt;p&gt;这其中既包括对原本注册配置中心的增强功能，即模糊订阅，也包括了对 AI 相关能力的实现和规划，如 MCP 和管理，MCP Router，动态 Prompt 及 A2A 协议支持；同时也通过支持 xDS 协议及 Nacos Controller 继续加强和探索 Mesh 生态。&lt;/p&gt; 
&lt;h3&gt;2.2 Nacos 3.0 AI Registry 架构&lt;/h3&gt; 
&lt;p&gt;了解完 Nacos 3.0 的整体架构，接下来我们来看 Nacos AI 中心（AI Registry）的架构设计。作为 Nacos 3.0 规划中最重要的能力，Nacos AI Registry 的架构被分为 3 个层次，分别是&lt;code&gt;模型层、``工具层&lt;/code&gt;和&lt;code&gt;应用层&lt;/code&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-84c758dce0cbc64de279f716cb03fd3cd8d.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在&lt;code&gt;模型层&lt;/code&gt;中，主要通过对 AI 模型中一些常用的动态参数，比如 Prompt、学习率、联网参数等进行管理，采取复用在云原生应用中配置动态管理和分发能力的方式，帮助 AI 智能体在模型层进行快速调整及试错。&lt;/p&gt; 
&lt;p&gt;模型层之上是&lt;code&gt;工具层&lt;/code&gt;，工具层主要帮助 LLM 模型和提供数据的 MCP 工具之间进行自动的发现、注册以及检索等能力，复用在云原生应用中服务的动态注册、管理、发现的能力，帮助 AI 智能体应用快速及便捷地发现 MCP 工具，同时快速过滤无关工具，减少 Token 损耗。&lt;/p&gt; 
&lt;p&gt;最顶层是 Agent 的&lt;code&gt;应用层&lt;/code&gt;，实现 AI 应用与 AI 应用之间的发现与协作。目前规划是通过支持 A2A 等社区标准协议，同时配合 Spring AI Alibaba 等 AIAgent 应用框架，帮助 AIAgent 应用便捷的自动注册自身 AI 应用，同时发现其他 AI 应用，并能够像云原生应用一样，进行任务的分发以及结论的构成。&lt;/p&gt; 
&lt;p&gt;如果从功能视角出发，Nacos AI Registry 又可被分为针对大模型 LLM 的&lt;code&gt;模型动态配置调优，&lt;/code&gt;针对 AI 应用平台的&lt;code&gt;应用开发管理&lt;/code&gt;以及针对 AI Agent 应用的&lt;code&gt;运行时能力增强&lt;/code&gt;。Nacos 希望通过不同的功能点，帮助 AI 应用像微服务云原生应用一样，能动态的调整 Prompt，学习率等参数，无需重新发布，从而帮助 AI 应用简化开发，调试过程中的繁琐操作，提高 AI 应用的开发和运行效率。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-dd07e5e90a95088b6e84dc992e28fb225fe.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-d4273e4083d0c6f838fde1393f16872b30f.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;2.3 Nacos 3.0 安全架构&lt;/h3&gt; 
&lt;p&gt;Nacos 2.0 中面临的一个主要的风险就是 Nacos 所有的 HTTP OpenAPI 均通过统一的端口进行暴露，同时使用了统一的鉴权开关，这使得使用者必须在便捷性和安全性中作出取舍，导致在许多部署的环境中可能存在安全风险。&lt;/p&gt; 
&lt;p&gt;Nacos 3.0 为了解决这个问题，从 Nacos 的部署架构上作出演进，&lt;code&gt;独立控制枱部署&lt;/code&gt;，&lt;code&gt;拆分鉴权开关&lt;/code&gt;，&lt;code&gt;分类 API &lt;/code&gt;并&lt;code&gt;默认开启控制枱及管控类 API 的鉴权。&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-3fc2b32b2e404eafbf929142ad4585d3c3b.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;同时配合&lt;code&gt;配置加密插件，``TLS 传输&lt;/code&gt;，来实现 Nacos 3.0 的零信任安全架构。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-f835e58de73cbab938bc6e5d6ed9849472a.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;除了针对 Nacos 自身的安全零信任架构外，Nacos 3.0 还将与 Druid，Spring AI Alibaba/Spring Cloud Alibaba 等开源社区，及 KMS 等安全云产品合作，提供面向应用侧的数据源运行期动态轮转方案。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-f6a73d965abc4e2b27e2a584508f18ea32d.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在这套解决方案中，数据源的凭据始终由 KMS 等凭据托管平台和系统保存，全程无人工传递和配置的过程。用户可以设置定期进行凭据的自动轮转，或在怀疑密钥泄漏时手动触发凭据轮转；触发后会通过 Nacos 动态无损的将新的加密凭据通知到 Druid 或 Spring AI Alibaba/Spring Cloud Alibaba，进行凭据的动态刷新和无损替换。这种方式极大降低了凭据泄漏的可能性，同时显著提高了安全性及出现安全风险时的收敛恢复速度。&lt;/p&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Nacos MCP Registry&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr&gt; 
&lt;h3&gt;3.1 Nacos MCP Registry 架构&lt;/h3&gt; 
&lt;p&gt;Nacos 3.0 最主要的能力升级就是作为 MCP Registry，支持了 MCP 服务的管理能力。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-88c91519a2722fe1b60a505a553c7428a19.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Nacos MCP Registry 支持三类 MCP 服务的注册方式：&lt;/p&gt; 
&lt;p&gt;第一类是将存量 HTTP 或 RPC 的服务，通过声明自动转化为 MCP 服务，配合 Higress 的协议转换能力，实现 0 代码改造成 MCP 服务协议，如何将存量 API 转化为 MCP 服务，详情可参见文档【1】。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-3f1b620f54932854273dacb77d9d5d1d201.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;第二类就是新构建的 MCP 服务注册， 配合 Spring AI 等 AI Agent 应用框架和 Nacos-MCP 的 sdk，能够做到像微服务一样自动注册到 Nacos 中进行统一的管理和维护，如何通过 Spring AI 或 Nacos-MCP 的 sdk 进行 MCP 服务的自动注册与发现，请参见文档【2】。&lt;/p&gt; 
&lt;p&gt;第三类就是已经构建好的或其他供应商提供的 MCP 服务，可以导入到 Nacos 中，进行其描述、工具列表、工具 Schema 等内容的动态修改和维护，让调试 MCP 服务变得更加简单。&lt;/p&gt; 
&lt;h3&gt;3.2 Nacos MCP Router&lt;/h3&gt; 
&lt;p&gt;Nacos 3.0 支持用户通过 3 种方式发布 MCP 服务，并对 MCP 服务的元数据和版本进行管理，但如果最终不能将这些元数据和版本信息进行实际的使用，这些信息就没有意义。&lt;/p&gt; 
&lt;p&gt;因此 Nacos 3.0 提供 Nacos MCP Router 帮助终端使用者无需实际感知 MCP 服务列表，即可自动发现和使用需要的 MCP 服务。&lt;/p&gt; 
&lt;p&gt;Nacos MCP Router 提供两种工作模式，&lt;code&gt;动态路由&lt;/code&gt;和&lt;code&gt;动态代理&lt;/code&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-054053f13b5e4c1b1712330bffd28c95d5d.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;动态路由模式将会根据 LLM 所提供的关键字信息，对注册在 Nacos 中的 MCP 服务进行相关性过滤和筛选，选择出与关键字相关的 MCP 服务进行实际的使用，从而减少对 LLM 上下文的消耗，实现&lt;code&gt;路由 &lt;/code&gt;MCP 服务的能力。&lt;/p&gt; 
&lt;p&gt;而代理模式能够进行 MCP 协议的转换，将 &lt;code&gt;stdio &lt;/code&gt;和 &lt;code&gt;sse &lt;/code&gt;类型的 MCP 服务，代理成 &lt;code&gt;streamable &lt;/code&gt;类型的 MCP 服务。代理模式下的 Nacos MCP Router 不根据关键字进行筛选，仅是将注册在 Nacos 中的 &lt;code&gt;stdio &lt;/code&gt;和 &lt;code&gt;sse &lt;/code&gt;类型的 MCP 服务，转化成 &lt;code&gt;streamable &lt;/code&gt;类型，同时应用用户在 Nacos 上修改和编辑的 Tool 描述信息，将转化后的 MCP 服务列表，返回给 LLM 供其使用。&lt;/p&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;Nacos 3.0 RoadMap&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr&gt; 
&lt;p&gt;Nacos 3.0 的目标是成为&lt;code&gt;全面拥抱 AI 时代的服务、配置、AI Registry 平台&lt;/code&gt;，因此 Nacos3.0 的 RoadMap 将会逐步实现 AI Registry 的能力，从当前的 MCP 管理，拓展到 Prompt 管理，Agent 的自动注册发现，再到 LLM 模型的参数管理和托管；同时进一步加强注册配置中心的能力和更多相关领域协议的支持（如 DNS，Mesh）。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-445ba3a6100745072fa880c31873dc11d3d.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Nacos 也希望有更多的社区贡献者加入进 Nacos 社区，帮助 Nacos 更快更好的完善和实现 Nacos3.0。&lt;/p&gt; 
&lt;ol start="5"&gt; 
 &lt;li&gt;欢迎加入 Nacos 社区&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr&gt; 
&lt;p&gt;Nacos 致力于帮助您发现、配置和管理微服务。Nacos 提供了一组简单易用的特性集，帮助您快速实现动态服务发现、服务配置、服务元数据及 AI 管理。&lt;/p&gt; 
&lt;p&gt;Nacos 帮助用户更敏捷和容易地构建、交付和管理云原生 AI 应用的平台。 Nacos 是构建以"服务"为中心的现代应用架构 (例如微服务范式、云原生范式、AI 原生范式) 的服务基础设施。&lt;/p&gt; 
&lt;p&gt;Nacos 3.0 还有很多待完成的功能及大量待探索和开发的领域，欢迎大家扫码加入 Nacos 社区群及 Nacos MCP 社区讨论群，参与 Nacos 社区的贡献和讨论，在 Nacos 社区一起搭把手，让你的代码和能力有机会能在各行各业领域内进行释放能量，期待认识你和你一起共建 Nacos 社区；&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;"Nacos 相信一切都是服务，每个服务节点被构想为一个星球，每个服务都是一个星系；Nacos 致力于帮助这些服务建立连接赋予智能，助力每个有面向星辰的梦想能够透过云层，飞在云上，更好的链接整片星空。"&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Nacos 官网：&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnacos.io%2F" target="_blank"&gt;https://nacos.io/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Nacos 仓库地址：&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Falibaba%2Fnacos" target="_blank"&gt;https://github.com/alibaba/nacos&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;"Nacos 社区群 5"群的钉钉群号：&lt;/p&gt; 
&lt;p&gt;120960003144&lt;/p&gt; 
&lt;p&gt;"Nacos MCP 社区讨论群"群的钉钉群号：&lt;/p&gt; 
&lt;p&gt;97760026913&lt;/p&gt; 
&lt;ol start="6"&gt; 
 &lt;li&gt;更多了解 Nacos 3.0&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr&gt; 
&lt;p&gt;6 月 6 日，Nacos 在上海举办了开源开发者沙龙 MeetUp 活动，此次是 Nacos 社区成员今年首次线下分享最新的能力和实践，并邀请了 Spring AI Alibaba 和 Higress 一起分享一站式的开源解决方案。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-fecce2cf0706af578321cb7f3935150e704.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;有需要 MeetUp 的 PPT 或希望回看 MeetUp 活动视频的同学，欢迎加入本文末尾的群中获取。&lt;/p&gt; 
&lt;p&gt;同时如果对 Nacos 3.0 的架构，运行原理，最佳实践等内容感兴趣的同学，欢迎阅读 Nacos 3.0 更多相关文章：&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzUzNzYxNjAzMg%3D%3D%26mid%3D2247574941%26idx%3D1%26sn%3D18ea079839f6c0e5563f0e0b3bd2e80d%26scene%3D21%23wechat_redirect" target="_blank"&gt;《0 代码改造实现应用运行时数据库密码无损轮转》&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzUzNzYxNjAzMg%3D%3D%26mid%3D2247574770%26idx%3D1%26sn%3Df975121e36eb751a7659a66f42cbac04%26scene%3D21%23wechat_redirect" target="_blank"&gt;《Nacos MCP Router 新版发布：支持 Docker 远程部署，MCP 的多协议 stido、SSE、Streamable 互相转换》&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzUzNzYxNjAzMg%3D%3D%26mid%3D2247574635%26idx%3D1%26sn%3D383a5c81ec298585f660b687a5dd4b12%26scene%3D21%23wechat_redirect" target="_blank"&gt;《企业生产环境中，实现 MCP 服务的统一管理和智能路由的实践》&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU4NzU0MDIzOQ%3D%3D%26mid%3D2247519792%26idx%3D1%26sn%3D4c1c6491fb2d1f32f8370057be15e6ba%26scene%3D21%23wechat_redirect" target="_blank"&gt;《Nacos 3.0 正式发布：MCP Registry、安全零信任、链接更多生态》&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;【1】存量 API 转换 MCP 手册&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnacos.io%2Fdocs%2Flatest%2Fmanual%2Fuser%2Fai%2Fapi-to-mcp%2F%3Fspm%3D5238cd80.2ef5001f.0.0.3f613b7ciRMNL5" target="_blank"&gt;https://nacos.io/docs/latest/manual/user/ai/api-to-mcp/?spm=5238cd80.2ef5001f.0.0.3f613b7ciRMNL5&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;【2】MCP Server 自动注册手册&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnacos.io%2Fdocs%2Flatest%2Fmanual%2Fuser%2Fai%2Fmcp-auto-register%2F%3Fspm%3D5238cd80.2ef5001f.0.0.3f613b7ciRMNL5" target="_blank"&gt;https://nacos.io/docs/latest/manual/user/ai/mcp-auto-register/?spm=5238cd80.2ef5001f.0.0.3f613b7ciRMNL5&lt;/a&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/3874284/blog/18638349</link>
      <guid isPermaLink="false">https://my.oschina.net/u/3874284/blog/18638349</guid>
      <pubDate>Fri, 09 May 2025 03:14:00 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>雷军：一些人说小米自研芯片产品卖不动，瞎扯</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;在小米人车家全生态发布会后，雷军进行了分享表示，做玄戒 O1 的时候，小米完全没有想到 O1 做的这么好。「我真的没想到，所以整个 O1 的芯片总量定的就不够。规划是做了 4 款产品」。&lt;/p&gt; 
&lt;p&gt;&lt;img height="305" src="https://oscimg.oschina.net/oscnet/up-b6b4bb5a4c2a474139aff036b5980fbc097.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;雷军还特别强调，「特别感谢朱丹领军的整个芯片团队为小米做出的巨大贡献，我自己用的也是玄戒的手机，现在体验特别好」。&lt;/p&gt; 
&lt;p&gt;雷军透露，第二代玄戒芯片会考虑在车上应用。「第一代主要是验证技术，技术好到我无法相信」。&lt;/p&gt; 
&lt;p&gt;「我们这几款手机和平板备货都很少，我也看到一些说我们卖不动，瞎扯」，他说，「主要是三四年前，你能知道芯片做的有这么好？肯定不知道。我们下一步肯定会芯片上车，我们全部自研了四合一的域控制器，我们就是为了掌握这个技术，为将来小米芯片上车做好准备」。（新浪科技）&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357527</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357527</guid>
      <pubDate>Fri, 09 May 2025 02:52:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>字节跳动旗下开源多模态智能体 Agent TARS 发布 Beta 版本</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;字节跳动正式&lt;u&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fagent-tars.com%2Fzh%2Fblog%2F2025-06-25-introducing-agent-tars-beta.html" target="_blank"&gt;发布&lt;/a&gt;&lt;/u&gt;了&amp;nbsp;Agent TARS&amp;nbsp;的 Beta 版本，将其定位为一个开源的多模态&amp;nbsp;AI Agent&amp;nbsp;工具，旨在提供与各种现实世界工具的无缝集成能力，并实现「随时随地可用」。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-de6be480af81939d00d5c997035bf46b81b.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Agent TARS&amp;nbsp;的早期预览版已于三月份开源。此次 Beta 版本的发布得益于&amp;nbsp;Seed&amp;nbsp;多模态模型（如&amp;nbsp;UI-TARS 1.5&amp;nbsp;和&amp;nbsp;Doubao 1.5 VL）的逐步增强，并解决了早期架构中&amp;nbsp;Agent UI&amp;nbsp;未解耦等挑战。&lt;/p&gt; 
&lt;p&gt;新版本引入了全新的多层分层架构，其核心构建在一个基于事件流驱动的&amp;nbsp;Agent Kernel&amp;nbsp;上。Agent TARS&amp;nbsp;的设计原则包括：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;上下文工程 (Context Engineering)&lt;/strong&gt;：为构建长时间运行的 Agent，新版本对 Agent Loop 的内存进行了动态优化，针对不同模态内容采用不同的滑动窗口，并面向上下文窗口进行优化和计算，以解决上下文溢出问题。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP (多模态通信协议) 的洞察&lt;/strong&gt;：Agent TARS 团队认为 MCP 应作为标准化的工具分发协议，而非自由拓展工具的方式，并计划建立 MCP 工具的基准测试，以评估其模型兼容性、上下文压缩率和性能等。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;上下文压缩 (Context Compress)&lt;/strong&gt;：正在推动多级内存设计（L0 永久记忆、L1 会话记忆、L2 循环记忆、L3 临时记忆），并结合选择性上下文和 LLM/SLM 摘要等策略进行压缩。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;可观测与可评估 (Observable and Evaluable)&lt;/strong&gt;：引入了 Snapshot 框架，能够在运行时将 Agent 依赖的环境保存为快照，用于回放和调试，已驱动持续集成和测试。同时，通过 Agent 与 UI 分层的新架构，实现了 Headless 运行模式，使得自动化评测成为可能，并参考 OpenAI 的 simple-evals 实现了 browsecomp 评测方案。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;易于构建应用 (Easy to build applications)&lt;/strong&gt;：引入了 Agent Event Stream，将 Agent 的状态、工具调用细节、最终回复和环境信息等以流式方式输出，使得用户可以轻松构建自定义 Agent UI。Agent TARS Server 与 Web UI 的架构设计也因此变得简洁。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-35f79b54f76d476c905a32d464b7cf6e45a.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;同时，Agent TARS 关注并学习了 AG-UI 协议，并说明了 Agent Event Stream 在构建上下文方面的差异化优势。&lt;/p&gt; 
&lt;p&gt;新版本的主要特性包括：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Agent TARS CLI&lt;/strong&gt;：取代了&amp;nbsp;&lt;code&gt;Electron&lt;/code&gt;&amp;nbsp;应用，具有更容易更新迭代和显著减小的安装体积优势。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;混合浏览器 GUI Agent&lt;/strong&gt;：结合了基于&amp;nbsp;&lt;code&gt;DOM&lt;/code&gt;&amp;nbsp;分析的&amp;nbsp;&lt;code&gt;Browser Use&lt;/code&gt;&amp;nbsp;和基于&amp;nbsp;&lt;code&gt;UI-TARS&lt;/code&gt;/&lt;code&gt;Doubao 1.5 VL&lt;/code&gt;&amp;nbsp;的视觉控制方案，操作逻辑更接近人类理解屏幕的方式。提供了&amp;nbsp;&lt;code&gt;dom&lt;/code&gt;、&lt;code&gt;visual-grounding&lt;/code&gt;&amp;nbsp;和&amp;nbsp;&lt;code&gt;hybrid&lt;/code&gt;&amp;nbsp;三种操作方案。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;更好的跨模型兼容性&lt;/strong&gt;：完全重写了&amp;nbsp;&lt;code&gt;Model Provider&lt;/code&gt;&amp;nbsp;层，现在支持&amp;nbsp;&lt;code&gt;Volcengine (Seed1.5-VL)&lt;/code&gt;、&lt;code&gt;Anthropic (Claude-3.7-Sonnet)&lt;/code&gt;&amp;nbsp;和&amp;nbsp;&lt;code&gt;OpenAI (GPT-4o)&lt;/code&gt;&amp;nbsp;等模型提供商。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;原生流式传输 (Native Streaming)&lt;/strong&gt;：整个架构构建在流式传输之上，显著提升了复杂任务的交互体验。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Web UI&lt;/strong&gt;：得益于&amp;nbsp;&lt;code&gt;Agent Event Stream&lt;/code&gt;，&lt;code&gt;Web UI&lt;/code&gt;&amp;nbsp;可以完全独立开发，支持&amp;nbsp;&lt;code&gt;GUI Grounding Transition&lt;/code&gt;（实时鼠标追踪）、&lt;code&gt;Replay&lt;/code&gt;&amp;nbsp;保存与分享，并实现了通用的多模态内容渲染器。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Agent TARS 团队还展示了多项内部开发者示例，包括 UI 复刻、先写游戏再玩游戏，以及图文并茂的报告生成等，这些都得益于模型能力和上下文工程的提升。Agent TARS 仍在快速发展中，未来将推出带有动态规划推理的版本，并致力于成为随时随地可用的 Agent 开发者工具。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357518/agent-tars-beta</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357518/agent-tars-beta</guid>
      <pubDate>Fri, 09 May 2025 02:22:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>阿里巴巴 2025 财年收入 9963 亿元</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;6 月 26 日晚，阿里巴巴集团发布 2025 财年年报显示，2025 财年阿里巴巴集团收入达 9963.47 亿元，净利润同比增长 77% 至 1259.76 亿元，展现出强劲的盈利能力。在 AI 需求的推动下，阿里云财年收入突破双位数增长，AI 相关产品收入连续七个季度实现三位数同比增长。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在 AI 领域，过去一年阿里发布并开源多款模型，覆盖全尺寸、全模态、多场景。4 月最新发布的阿里通义 Qwen3（简称「千问 3」）大模型，开源仅一个月全球累计下载量突破 1250 万。截至 4 月底，阿里通义已开源 200 余款模型，全球下载量超过 3 亿次，千问系列衍生模型数量超 10 万个，成为全球最大的开源模型家族。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;阿里云加速 AI 产品国际化，截至 2025 年 3 月 31 日，为全球 34 个地区提供云计算服务。以通义大模型为底座，淘宝天猫、1688、阿里国际站、夸克、钉钉、高德、飞猪、闲鱼等阿里多业务 AI 升级加速。其中，阿里 AI 旗舰应用夸克用户规模同比迅速增长，截至 2025 财年末，月活跃用户数已突破 2 亿；2025 年 3 月，钉钉的平均付费周活跃用户数达 4200 万，目前钉钉是国内最大的效率办公类 App。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在致股东信中，阿里巴巴表示，「阿里的基因里没有守成，只有创造。今天的阿里巴巴，正在以创业者的姿态，开启面向 AI 时代的全新征程。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="373" src="https://oscimg.oschina.net/oscnet/up-074b55c1ba3ac1d7195b638a6c1bada394d.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;此外，阿里巴巴合伙人名单相比 2024 财年年报披露时发生变化，总数从 26 人减少至 17 人，戴珊、方永新、彭蕾、宋洁、孙利军、武衞、俞永福、张勇、朱顺炎等 9 人退出合伙人之列。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357517</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357517</guid>
      <pubDate>Fri, 09 May 2025 02:19:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>快手开源多模态大模型 Kwai Keye-VL</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;快手宣布并开源其最新自研的多模态大语言模型 Kwai Keye-VL。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;根据介绍，Kwai Keye-VL 以 Qwen3-8B 语言模型为基础，引入了基于开源 SigLIP 初始化的 VisionEncoder，能够深度融合并处理文本、图像、视频等多模态信息，凭借其创新的自适应交互机制与动态推理能力，旨在为用户提供更智能、全面的多模态交互体验。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;Kwai Keye-VL 支持动态分辨率输入，按原始比例将图像切分为 14x14 &amp;nbsp;patch 序列，由一个 MLP 层将视觉 Token 进行映射与合并。模型采用 3D RoPE （旋转位置编码）统一处理文本、图像和视频，并通过位置编码与时间戳对齐，精准捕捉视频时序变化。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="366" src="https://oscimg.oschina.net/oscnet/up-f9bb9b208e03575669510048f8ff6cabc1e.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="138" src="https://oscimg.oschina.net/oscnet/up-a5530c104b198c517ed650e3e67740584c5.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在视觉理解与逻辑推理能力方面，Kwai Keye-VL 的综合感知能力媲美同规模顶尖模型，并在复杂推理任务中展现出显著优势。尤其是逻辑推理方面，Kwai Keye-VL 在最新的 2025 年高考全国数学卷中取得了 140 分的成绩。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="314" src="https://oscimg.oschina.net/oscnet/up-d0c30a1de0375792399c2797d5e075fce36.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;为突破公开数据集的数据污染、语言覆盖局限及任务单一性等问题，快手构建了内部评测集 KC-MMBench。结果显示：该模型在 VideoMME 等权威公开 Benchmark 中以 67.4 分超越 Qwen2.5-VL-7B（62.7）与 InternVL-3-8B（65.5）；在内部短视频场景评测中优势进一步扩大，综合得分领先 SOTA 模型超 10%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="296" src="https://oscimg.oschina.net/oscnet/up-d5f95b60cd23280d98fa13ffda02bd537e7.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;更多详情可&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F2JRGYhB_VDPecXMjp3gZsQ" target="_blank"&gt;查看官方公告&lt;/a&gt;。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357515</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357515</guid>
      <pubDate>Fri, 09 May 2025 02:12:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>VTJ.PRO 的 AI+低代码设计器和渲染器技术架构和实现原理</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="margin-left:0; margin-right:0"&gt;本文介绍支持 VTJ.PRO 低代码体验的可视化设计环境 （&lt;code&gt;@vtj/designer&lt;/code&gt;） 和运行时渲染系统 （&lt;code&gt;@vtj/renderer&lt;/code&gt;）。Designer 提供了交互式可视化编辑环境，用户可以在其中拖放和配置组件，而 Renderer 在设计时预览和运行时环境中执行生成的 DSL 架构。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;有关这些系统运行的核心引擎和数据模型的信息，请参阅&lt;strong&gt;引擎、提供程序和服务层&lt;/strong&gt;&lt;span&gt; &lt;/span&gt;。有关提供实际小组件的 UI 组件库的详细信息，请参阅&lt;span&gt; &lt;/span&gt;&lt;strong&gt;UI 组件库&lt;/strong&gt;&lt;span&gt; &lt;/span&gt;。&lt;/p&gt; 
&lt;h2&gt;Designer 架构&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fvtj.pro%2Fwiki%2Fpackage%2Fdesigner-and-renderer.html%23designer-%25E6%259E%25B6%25E6%259E%2584" target="_blank"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Designer 系统通过以&lt;span&gt; &lt;/span&gt;&lt;code&gt;Designer&lt;/code&gt;&lt;span&gt; &lt;/span&gt;类及其与仿真环境的集成为中心的复杂事件驱动架构提供交互式可视化编辑功能。&lt;/p&gt; 
&lt;h3&gt;Core Designer 类&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fvtj.pro%2Fwiki%2Fpackage%2Fdesigner-and-renderer.html%23core-designer-%25E7%25B1%25BB" target="_blank"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&lt;code&gt;Designer&lt;/code&gt;&lt;span&gt; &lt;/span&gt;类充当所有设计时交互的中心编排器，管理鼠标事件、拖放作、元素选择和视觉反馈系统。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//05010de879d43b96f68f8cac262bee37.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Designer 通过对具有特殊属性 （&lt;code&gt;__vtj__&lt;/code&gt;、&lt;code&gt;__context__&lt;/code&gt;） 的 DOM 元素进行检测来运行，这些属性使其能够在可视元素及其相应的数据模型之间进行映射。&lt;/p&gt; 
&lt;h3&gt;事件处理系统&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fvtj.pro%2Fwiki%2Fpackage%2Fdesigner-and-renderer.html%23%25E4%25BA%258B%25E4%25BB%25B6%25E5%25A4%2584%25E7%2590%2586%25E7%25B3%25BB%25E7%25BB%259F" target="_blank"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Designer 实现了一个全面的事件处理系统，该系统可以捕获用户交互并将其转换为设计作：&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//ef1a4b256c3eed48a11a91f75601b83e.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;视觉反馈组件&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fvtj.pro%2Fwiki%2Fpackage%2Fdesigner-and-renderer.html%23%25E8%25A7%2586%25E8%25A7%2589%25E5%258F%258D%25E9%25A6%2588%25E7%25BB%2584%25E4%25BB%25B6" target="_blank"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Designer 系统通过突出显示交互式元素的叠加组件提供实时视觉反馈：&lt;/p&gt; 
&lt;h2&gt;渲染器架构&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fvtj.pro%2Fwiki%2Fpackage%2Fdesigner-and-renderer.html%23%25E6%25B8%25B2%25E6%259F%2593%25E5%2599%25A8%25E6%259E%25B6%25E6%259E%2584" target="_blank"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Renderer 系统为 VTJ DSL 模式提供运行时执行环境，支持多种执行模式和上下文。&lt;/p&gt; 
&lt;h3&gt;上下文系统&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fvtj.pro%2Fwiki%2Fpackage%2Fdesigner-and-renderer.html%23%25E4%25B8%258A%25E4%25B8%258B%25E6%2596%2587%25E7%25B3%25BB%25E7%25BB%259F" target="_blank"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&lt;code&gt;Context&lt;/code&gt;&lt;span&gt; &lt;/span&gt;类充当运行时执行环境，为组件实例提供对状态、props、生命周期方法和实用程序函数的访问：&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//b053bdd59830ab949e97dfc731235450.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Context 系统支持三种不同的执行模式：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;设计模式&lt;/strong&gt;&lt;span&gt; &lt;/span&gt;：用于设计时交互的 Instruments 元素&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;运行模式&lt;/strong&gt;&lt;span&gt; &lt;/span&gt;：提供生产执行环境&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;VNode 模式&lt;/strong&gt;&lt;span&gt; &lt;/span&gt;：支持无 refs 的虚拟节点渲染&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;多模式渲染&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fvtj.pro%2Fwiki%2Fpackage%2Fdesigner-and-renderer.html%23%25E5%25A4%259A%25E6%25A8%25A1%25E5%25BC%258F%25E6%25B8%25B2%25E6%259F%2593" target="_blank"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Renderer 支持通过 Context 系统在设计时和运行时模式之间无缝切换：&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//c9a5959e3c493abbfe9aebba842450b0.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h2&gt;设计时集成&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fvtj.pro%2Fwiki%2Fpackage%2Fdesigner-and-renderer.html%23%25E8%25AE%25BE%25E8%25AE%25A1%25E6%2597%25B6%25E9%259B%2586%25E6%2588%2590" target="_blank"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Designer 和 Renderer 系统通过基于 iframe 的共享模拟环境进行集成，该环境支持实时预览和交互。&lt;/p&gt; 
&lt;h3&gt;模拟器架构&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fvtj.pro%2Fwiki%2Fpackage%2Fdesigner-and-renderer.html%23%25E6%25A8%25A1%25E6%258B%259F%25E5%2599%25A8%25E6%259E%25B6%25E6%259E%2584" target="_blank"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//e059163c63246e9ea099ceea511c2597.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;元件检测&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fvtj.pro%2Fwiki%2Fpackage%2Fdesigner-and-renderer.html%23%25E5%2585%2583%25E4%25BB%25B6%25E6%25A3%2580%25E6%25B5%258B" target="_blank"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;在设计模式下，Renderer 使用特殊属性检测 DOM 元素，使 Designer 能够跟踪和作它们：&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//35e8660da19e5361de71bddb62f84af2.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h2&gt;视口和响应式设计&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fvtj.pro%2Fwiki%2Fpackage%2Fdesigner-and-renderer.html%23%25E8%25A7%2586%25E5%258F%25A3%25E5%2592%258C%25E5%2593%258D%25E5%25BA%2594%25E5%25BC%258F%25E8%25AE%25BE%25E8%25AE%25A1" target="_blank"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Designer 提供了一个复杂的视口系统，该系统支持多种设备模式和自定义大小调整，以便进行响应式设计测试。&lt;/p&gt; 
&lt;h3&gt;视口模式&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fvtj.pro%2Fwiki%2Fpackage%2Fdesigner-and-renderer.html%23%25E8%25A7%2586%25E5%258F%25A3%25E6%25A8%25A1%25E5%25BC%258F" target="_blank"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//6ee43b64521f23c8c941a1168cd5a29b.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h2&gt;运行时性能优化&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fvtj.pro%2Fwiki%2Fpackage%2Fdesigner-and-renderer.html%23%25E8%25BF%2590%25E8%25A1%258C%25E6%2597%25B6%25E6%2580%25A7%25E8%2583%25BD%25E4%25BC%2598%25E5%258C%2596" target="_blank"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Renderer 系统包括针对运行时性能的多项优化，用于区分设计时插桩和生产执行。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//fd21715b2388c67e35deed2e7443e1b6.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Context 系统会自动管理 Vue 实例生命周期、引用跟踪和清理，以确保在不同执行模式下的最佳性能，同时保持设计时插桩所需的灵活性。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357511</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357511</guid>
      <pubDate>Fri, 09 May 2025 01:41:00 GMT</pubDate>
      <author>来源: 投稿</author>
    </item>
  </channel>
</rss>
