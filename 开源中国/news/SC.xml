<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - news - 简体中文</title>
    <link>https://www.oschina.net/news/project</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news" rel="self" type="application/rss+xml"/>
    <description>已对该 RSS 进行格式化操作：中英字符之间插入空格、使用直角引号、标点符号修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-cn</language>
    <lastBuildDate>Mon, 21 Jul 2025 07:47:58 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>黄仁勋佩服 DeepSeek 惊人创新力，称「中国创新步伐不可能被阻挡」</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;据央视新闻报道，美国英伟达公司创始人兼首席执行官黄仁勋在接受总枱《面对面》栏目采访时力赞了 DeepSeek，并表示 AI 是一个极其复杂的系统，中国的创新能力很惊人。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-66c55b590ff8ad01e0596b421907bdcd52f.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0721/153417_BUFz_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;黄仁勋称，中国创新的步伐是不可能被阻挡的，相信英伟达能作出重要贡献。AI 是一个极其复杂的系统，就像多层蛋糕一样复杂，其芯片只是底层，上面还有系统、网络技术、AI 基础设施、软件、AI 算法，以及最上层的应用服务，整个系统异常复杂。一方面 AI 的发展，需要这个系统每一层的创新，但如果某一层进展不够快，工程师们足够聪明，他们可以通过上下层的创新来弥补，从而推动整个系统前进。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-fc65db9d265cf9efd9514df5236e2511215.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;黄仁勋还表达了对中国创新能力的乐观和信心，并直言不得不佩服深度求索这家公司的创新能力，他们研发的 R1 模型是真正的创新，它重新设计了 AI 模型的很多运行方式，让它们能充分发挥 H20 架构的优势，这种做法非常有创意。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361459</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361459</guid>
      <pubDate>Mon, 21 Jul 2025 07:32:34 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>京东回应一日连投三家机器人企业：高度重视具身智能等技术</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;今日上午，机器人行业迎来资本涌动新态势——千寻智能宣布完成 6 亿元 PreA+轮融资，逐际动力（LimX Dynamics）披露新一轮战略融资，众擎机器人同步官宣 A1 轮融资，三家企业融资均由京东集团领投。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="280" src="https://oscimg.oschina.net/oscnet/up-dc849627f54aa7791bbf8a55bf44b10afa0.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;针对密集投资动作，京东集团相关负责人回应称，公司正持续加大对具身智能、大模型等前沿技术赛道的战略投入。未来将围绕供应链场景需求，通过内部技术研发与外部生态投资双轮驱动，构建覆盖"仓储-运输-配送"全链条的智能技术体系。数据显示，京东物流已累计投入自动化技术研发资金超 200 亿元，持有相关专利超 4000 件。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;根据京东集团技术委员会发布的《智能供应链技术白皮书》，到 2025 年，京东将建成全球首个全链路无人化供应链网络，其中机器人集群协同作业、多模态人机交互等关键技术将实现突破。此次融资潮或预示着，以仓储机器人为起点的供应链智能化革命，正从单点技术创新迈向生态体系重构的新阶段。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361455</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361455</guid>
      <pubDate>Mon, 21 Jul 2025 07:11:34 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>英伟达 CUDA 将支持 RISC-V 架构</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;英伟达的驱动程序和 CUDA 软件堆栈一直以来主要支持 x86_64 和 AArch64 系统，但过去也支持 IBM Power。在 RISC-V 中国峰会上，英伟达副总裁 Frans Sijstermans 宣布 CUDA 将支持 RISC-V。&lt;/p&gt; 
&lt;p&gt;RISC-V International &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Frisc_v%2Fstatus%2F1946251939823370697" target="_blank"&gt;在 X 上转播了这一消息&lt;/a&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-b3e65923bbaa9649fa004ec673e037d59d8.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;鉴于 RISC-V 在数据中心领域日益增长的兴趣，这并不令人意外。英伟达 Linux 驱动程序堆栈也相当内敛且易于移植，正如过去在 POWER 架构下以及 x86/x86_64/AArch64 架构下所展现的那样。更别提他们过去甚至还通过共享代码库驱动程序支持 Solaris 上的 Itanium 和 SPARC。&lt;/p&gt; 
&lt;p&gt;移植工作包括 CUDA Toolkit（如 NVCC、GDB、工具链等）和驱动程序（如 CUDA KMD 和 UMD），以及适配 CUDA 核心库，以支持 AI、数据分析、EDA 加速等多个领域 。&lt;/p&gt; 
&lt;p&gt;尽管 RISC-V CPU 目前板卡选择有限且性能尚待提升（如基于 SiFive P550 和阿里巴巴玄铁 C920 的开发），英伟达正与合作伙伴共同推动 CUDA 在 RISC-V 架构上的成熟运行，未来标准 CUDA 版本将支持符合服务器平台规范和 Linux 操作系统的 RISC-V 架构 CPU。&lt;/p&gt; 
&lt;p&gt;最大对手 AMD 方面，其上游开源内核计算驱动程序 AMDKFD 已经可以在 RISC-V 上构建，ROCm 用户空间组件也可以在 RISC-V 上构建。最近，我们甚至看到 AMDKFD / ROCm 也可以在 LoongArch 处理器上运行。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361454</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361454</guid>
      <pubDate>Mon, 21 Jul 2025 07:04:34 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>哔哩哔哩第三方开源客户端 PiliPala 收到侵权告知函，宣布停更</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;7 月 18 日，哔哩哔哩（B 站）第三方开源客户端 PiliPala（&lt;em&gt;https://github.com/guozhigq/pilipala&lt;/em&gt;）的开发者 guozhigq 因收到 B 站发出的侵权告知函，宣布即日起停止更新该项目。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-3fc4211d3c345025c4e1b647c4f4b784c55.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;侵权告知函指出，PiliPala 未经许可破解 B 站平台接口，将数据聚合到自身应用中，使用户无需通过 B 站平台即可浏览相关内容，对 B 站起到了实质性替代作用，减少了 B 站网络流量，构成不正当竞争。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-e5c6612e88564ad34f3d44ee0015979d296.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;B 站要求开发者立即停止运营 PiliPala，下架任何平台（包括 GitHub）上的项目，并停止破解 B 站接口的行为 。尽管律师函仅为警告而非起诉，但开发者因法律风险决定停止开发和维护。&lt;/p&gt; 
&lt;p&gt;目前，PiliPala 最后一次更新为今年 4 月发布的 v1.0.27.0402 版，项目已在各平台下架。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361450</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361450</guid>
      <pubDate>Mon, 21 Jul 2025 06:48:34 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>GPU 维修，一个百亿市场是如何形成的？</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;div&gt; 
 &lt;p&gt;2020 年，全球人工智能（AI）迎来热潮，大模型技术席卷全球。 作为全球最大的互联网和科技应用市场之一，中国对 AI 算力的需求早已快速增长，阿里、腾讯、字节跳动等科技巨头和众多 AI 初创公司，争相购入英伟达（NVIDIA）的高端 GPU，组建庞大算力集群，投入大模型研发竞赛。&lt;/p&gt; 
 &lt;p&gt;凭借专为 AI 计算设计的 GPU，尤其是数据中心级的 A100 芯片，英伟达在中国市场赚得盆满钵满，其高端 GPU 供不应求，价格水涨船高。&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;本来，这是一个双赢的局面：英伟达提供铲子，中国公司挖掘 &lt;/strong&gt;&lt;strong&gt;AI&lt;/strong&gt;&lt;strong&gt; 金矿。&lt;/strong&gt;然而，中美科技竞争的暗流早已涌动，尤其是在人工智能和半导体等攸关国家安全和科技主导权的领域更是斗争激烈。美国接连挥下的芯片禁售令，不仅斩断了获取新铲子的渠道，反而倒逼出一个规模或达百亿的 GPU 维修产业填补着官方退场后的空白。&lt;/p&gt; 
 &lt;span id="OSC_h4_1"&gt;&lt;/span&gt; 
 &lt;h4&gt;&lt;span style="color:#2980b9"&gt;&lt;strong&gt;三次禁售令与囤货抢购&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt; 
 &lt;p&gt;2022 年 8 月，美国政府向英伟达等公司发出通知，限制其高端 AI 芯片对中国的出口。10 月，美国商务部工业和安全局（BIS）正式更新《出口管理条例》。&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;美国的目标明确：通过限制中国获取顶级 &lt;/strong&gt;&lt;strong&gt;AI&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;&lt;strong&gt;算力&lt;/strong&gt;&lt;strong&gt;，减缓其在尖端 AI 领域（尤其军事应用）的进展。这些新规，像一道无形的铁幕，开始落下。&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;新规的核心条款之一，就是对英伟达的旗舰产品——A100 和 H100 芯片及其相关技术（如组成大型服务器的 HGX 模组）实施严格的出口管制。任何公司——包括英伟达及其合作伙伴如戴尔、惠普、超微，向中国大陆及中国香港、澳门出口这些芯片前，都必须获得美国政府颁发的特别许可证。因为这种许可证极难获得或根本不会发放，实质上就是禁止销往中国。&lt;/p&gt; 
 &lt;p style="text-align:left"&gt;&lt;strong&gt;英伟达瞬间陷入两难。&lt;/strong&gt;为了保住部分中国市场，2022 年底，英伟达迅速行动，开发了针对中国市场的特供版芯片——A800 和 H800。也可以叫阉割版芯片：这些芯片在关键性能指标，即芯片间数据传输速率上进行了人为限制，使其性能刚好低于美国出口管制的「红线」。&lt;/p&gt; 
 &lt;p&gt;A800/H800 虽然性能打折，但仍是当时中国公司能合法获得的最强算力选项之一。&lt;strong&gt;尤其是在 &lt;/strong&gt;&lt;strong&gt;ChatGPT&lt;/strong&gt;&lt;strong&gt; 引发的&lt;/strong&gt;&lt;strong&gt;大模型&lt;/strong&gt;&lt;strong&gt;热潮下， 中国 &lt;/strong&gt;&lt;strong&gt;AI&lt;/strong&gt;&lt;strong&gt; 市场进一步大爆发。A800/H800 被大量采购，暂时缓解了部分算力紧迫的局面。&lt;/strong&gt;&lt;/p&gt; 
 &lt;p style="text-align:left"&gt;然而，美国很快掐断了这一后路，在 2023 年 10 月进一步升级了管制规则，直接将英伟达的「特供版」 A800 和 H800 也纳入了禁售范围！中国公司通过合法渠道获取先进 AI 芯片的最后一条主要路径也被切断。&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;但市场对&lt;/strong&gt;&lt;strong&gt;算力&lt;/strong&gt;&lt;strong&gt;的强劲需求并未缓解，反而在断供威胁下愈发焦灼&lt;/strong&gt;。因为国内 AI 巨头们的大模型竞赛此刻正如火如荼，对顶尖算力的需求是刚性且刻不容缓的。&lt;/p&gt; 
 &lt;p&gt;在国产替代尚无法完全扛起大梁、产能爬坡仍需时间的现实下，即使是性能被阉割的英伟达芯片，也是支撑研发与商业化的硬通货。禁令阴影下，恐慌性囤货潮瞬间引爆——客户争相抢购最后的库存，只为在窗口期彻底关闭前，囤积尽可能多的算力弹药。&lt;/p&gt; 
 &lt;p style="text-align:left"&gt;既要挽救中国市场，又要遵守美国出口限制，英伟达无法向中国出售高端 AI 芯片（如 H100、A100），因此针对中国市场推出符合管制规则的特供版芯片（如 H20、L20 PCIe、L2 PCIe），通过大幅削弱互联带宽和算力以满足美方要求。&lt;/p&gt; 
 &lt;p style="text-align:left"&gt;其中，H20 是旗舰计算卡 H100 的替代品，虽然都是基于英伟达的 Hopper 架构，但 H20 的 GPU 核心数量减少 41%，性能降低 28%。但通过优化互联带宽与软件性能，H20 仍成为国内大模型训练的重要选择。&lt;/p&gt; 
 &lt;p style="text-align:left"&gt;L20 和 L2 是基于 RTX 4090 级消费卡（Ada Lovelace 架构）的「降级版」，主要面向 AI 推理场景。H20 芯片上市后，由于国内客户担忧后续断供，集中抢购囤货。&lt;/p&gt; 
 &lt;p&gt;研究机构 Omdia 根据英伟达财报预估，2024 年，国内仅字节跳动和腾讯就分别订购了约 23 万片英伟达的芯片，仅次于微软。&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;而国内市场对断供的担忧再一次得到了验证：2025 年 &lt;/strong&gt;&lt;strong&gt;4 月 16 日，美政府已经禁止英伟达向中国出口 H20 芯片。&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;英伟达 CEO 黄仁勋说得很直白，对华限制「非常痛苦」，「我们将失去一个规模巨大的增长市场」。&lt;/p&gt; 
 &lt;p&gt;英伟达 2025 财年报告显示，它在中国大陆（含中国香港地区）收入 171 亿美元，同比增长 66%，相当于每天入账 3.3 亿人民币。&lt;/p&gt; 
 &lt;span id="OSC_h4_2"&gt;&lt;/span&gt; 
 &lt;h4&gt;&lt;span style="color:#2980b9"&gt;&lt;strong&gt;禁售之后，官方售后也失效&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;屡次禁售带来两个直接后果：&lt;/strong&gt; 第一，中国公司再也买不到新的英伟达高端 AI 芯片（A100/H100/H800/H20 等）。第二，更重要的是，连那些已经在中国数据中心里运行的、价值数百万一台的 A100/H100 服务器，也失去了官方的售后保障。&lt;/p&gt; 
 &lt;p&gt;「原厂」或 OEM 官方维修路径理论上存在，但实际上极其困难，原路返回就是最大的障碍。这些设备很多是通过非官方渠道（例如转口贸易、灰色市场）进入中国的。将它们运回原厂（通常在美国或中国台湾等地区）进行维修，需要面临极其复杂的出口管制合规审查，几乎不可能获得许可。&lt;/p&gt; 
 &lt;p&gt;就算设备有正规来源并能完成极其繁琐的合规手续进行返修，整个流程（物流、合规审查、排队、维修、再进口）耗时很长，短则 3 月，长则半年。&lt;/p&gt; 
 &lt;p&gt;所以，对于受限的英伟达高端数据中心 GPU/HGX 模组，在中国获得有效、及时、可靠的「官方」售后服务基本不存在，即使有，需付出代价也会高昂到无法接受。&lt;/p&gt; 
 &lt;p&gt;在禁售令生效前，大量的 A100/H100 及其系统已被采购并部署在各种数据中心（尤其是大模型训练集群）。这对于租赁或自用的算力服务商来说，设备宕机意味着巨大的收入损失，半年收益可能为 0。&lt;/p&gt; 
 &lt;span id="OSC_h4_3"&gt;&lt;/span&gt; 
 &lt;h4&gt;&lt;span style="color:#2980b9"&gt;&lt;strong&gt;GPU&amp;nbsp;&lt;/strong&gt;&lt;strong&gt;维修，一笔百亿元的产业&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;巨大的需求和官方服务的真空，催生了一个庞大的第三方&lt;/strong&gt;&lt;strong&gt;GPU&lt;/strong&gt;&lt;strong&gt;维修产业。&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;捷智算是一家位于深圳的 GPU 维修企业，其销售总监李玉侠表示，从客户下单到维修完成，通常只需要 7 至 15 天。维修一张高端数据中心 GPU 的费用通常在数千到数万元人民币不等，这取决于损坏程度、是否需要更换核心等高价值部件。&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;有人根据&lt;/strong&gt;&lt;strong&gt;保有量&lt;/strong&gt;&lt;strong&gt;以及故障率预测，认为这可能是一笔百亿元的产业。&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;尽管国内 A100/H100 的保有量是核心机密，但普遍认为在数百万张级别。&lt;/p&gt; 
 &lt;p&gt;据业内人士预估，从 2023 年到至今，H100、H800、H200、H20 等 GPU 是智算中心建设的主力采购型号，NVLink 整机形态（机头+HGX 模组）产品的出货量巨大，保守估计国内存量约为 400 万片。其中仅 H20 在最近一年多时间内，出货量就高达 200 万片。考虑其他 OEM 整机和更早型号，如 V100 等仍有价值，总量庞大。&lt;/p&gt; 
 &lt;p style="text-align:left"&gt;据公开报告显示，GPU 服务器的年故障率因使用强度、散热条件和维护水平而异，一般在 1%-5%。&lt;/p&gt; 
 &lt;p style="text-align:left"&gt;而英伟达的 H 系列因其高性能设计，在 AI 训练等密集计算任务中故障率还会更高。据 Meta 公开的数据显示，H100 GPU 集群在训练 Llama 3 模型的极端负载下，单块 GPU 在高强度使用下的年度故障率约为 9%，三年累计故障率可能达到 27%。有业内人士预估，如果维修一块 H100 GPU 收费 2 万，每年 10 万卡的维修需求，就有约 20 亿的市场空间。&lt;/p&gt; 
 &lt;p&gt;这么算下来，几百万张卡的维修市场，说是百亿元的产业并不为过。&lt;/p&gt; 
 &lt;p&gt;不过，这百亿元产业，很大一部分都要落到深圳的兜里了。深圳是国内乃至全球重要的高端 GPU 第三方维修中心。&lt;/p&gt; 
 &lt;p&gt;这都要归功于深圳华强北打下的基础。华强北是全球闻名的电子元器件集散地和电子产品维修/翻新中心，拥有极其完备的电子产业链。海量的技术工人——特别是芯片级维修工程师，以及强大的元器件供应链，包括拆机件、翻新件、兼容件，虽然部分来源可能存疑。&lt;/p&gt; 
 &lt;p&gt;长期维修手机、主板、显卡等精密电子设备，积累了丰富的 BGA 焊接、芯片植球、电路板飞线、故障诊断等高难度维修技术。这些技术可以直接迁移到 GPU 维修上。&lt;/p&gt; 
 &lt;p&gt;当然了，第三方维修并非没有后顾之忧。由于维修所需的高端 GPU 核心（裸 Die）等关键部件，官方渠道不可能提供。维修点主要依赖拆解报废卡、从其他故障卡回收、或者通过非正规渠道（可能涉及走私或侵犯知识产权）获取。这是产业最大的灰色地带和法律风险。&lt;/p&gt; 
 &lt;p style="text-align:left"&gt;即使是顶尖技术团队操刀，经过维修的 GPU 的稳定性、寿命、性能可能与原厂有差距。维修本身也可能导致设备失去官方保修（尽管在禁售后这已无意义）。&lt;/p&gt; 
 &lt;p&gt;美国层层加码的芯片禁售令，不仅卡住了中国获取新 AI 芯片的脖子，还让中国公司之前买到的数百万张高端 GPU 失去了官方维修。不过，正是这个‘修不了’的大麻烦，直接催生了一个年规模可能达百亿人民币的第三方 GPU 维修产业，而深圳成了这个产业的核心。&lt;/p&gt; 
 &lt;p&gt;据悉，英伟达又获准向中国出口 H20 芯片。这来来回回的禁售与放开之间，GPU 维修间的压测机在昼夜不停地工作，很多维修点的订单已经排到了半个月后。&lt;/p&gt; 
 &lt;hr&gt; 
 &lt;p&gt;参考链接：&lt;/p&gt; 
 &lt;p&gt;1、GPU Lifetimes on Titan Supercomputer：Survival Analysis and Reliability&lt;/p&gt; 
 &lt;p&gt;https://christian-engelmann.de/publications/ostrouchov20gpu.pdf&lt;/p&gt; 
 &lt;p&gt;2、Datacenter GPU service life can be surprisingly short — only one to three years is expected according to unnamed Google architect&lt;/p&gt; 
 &lt;p&gt;https://www.tomshardware.com/pc-components/gpus/datacenter-gpu-service-life-can-be-surprisingly-short-only-one-to-three-years-is-expected-according-to-unnamed-google-architect&lt;/p&gt; 
 &lt;p&gt;3、Compared to the H100, how does the performance of NVIDIA's AI chips specially designed for China, fare?&lt;/p&gt; 
 &lt;p&gt;https://longportapp.com/en/news/102150690&lt;/p&gt; 
 &lt;p&gt;4、一文了解 H 系列机型质保、故障、维修哪些事&lt;/p&gt; 
 &lt;p&gt;https://mp.weixin.qq.com/s/jq6B-HZHEKW3hcopO3YQEQ&lt;/p&gt; 
 &lt;p&gt;5、H 系列 GPU 维修的生意火了！&lt;/p&gt; 
 &lt;p&gt;https://mp.weixin.qq.com/s/jLpwOrDv5SDzFnzQFYOu2Q&lt;/p&gt; 
 &lt;p&gt;6、黄仁勋回应争议，英伟达在中美博弈中找到微妙平衡&lt;/p&gt; 
 &lt;p&gt;https://finance.sina.com.cn/stock/relnews/us/2025-07-16/doc-inffsnhq2777954.shtml&lt;/p&gt; 
 &lt;p&gt;7、Chinese Firms Including ByteDance, Alibaba Place $16 Bn NVIDIA GPU Orders: Reports&lt;/p&gt; 
 &lt;p&gt;https://analyticsindiamag.com/ai-news-updates/chinese-firms-including-bytedance-alibaba-place-16-bn-nvidia-gpu-orders-reports/&lt;/p&gt; 
 &lt;p&gt;8、H20 芯片重返中国市场&lt;/p&gt; 
 &lt;p&gt;https://finance.sina.cn/tech/2025-07-18/detail-inffvhce4759535.d.html?fromtech=1&amp;amp;vt=4&lt;/p&gt; 
&lt;/div&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/3859945/blog/18685123</link>
      <guid isPermaLink="false">https://my.oschina.net/u/3859945/blog/18685123</guid>
      <pubDate>Fri, 18 Jul 2025 11:48:00 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>SwiftFormat —— 格式化 Swift 代码</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                            &lt;p style="text-align:start"&gt;&lt;span&gt;&lt;span&gt;&lt;span style="color:#24292f"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style="background-color:#ffffff"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;SwiftFormat 是一个代码库和命令行工具，用于在 macOS 或 Linux 上重新格式化 Swift 代码。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p style="text-align:start"&gt;&lt;span&gt;&lt;span&gt;&lt;span style="color:#24292f"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style="background-color:#ffffff"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;SwiftFormat 除了调整空格之外，它还可以插入或删除隐式&lt;code&gt;self&lt;/code&gt;、删除多余的括号，并纠正许多其他与标准 Swift 习语的偏差。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p style="text-align:start"&gt;SwiftFormat 的配置分为&amp;nbsp;&lt;strong style="color:#24292f"&gt;rules&amp;nbsp;&lt;/strong&gt;和&amp;nbsp;&lt;strong style="color:#24292f"&gt;options&lt;/strong&gt;。&lt;span style="background-color:#ffffff; color:#24292f"&gt;Rules&lt;/span&gt;&amp;nbsp;是 SwiftFormat 库中的函数，用于将更改应用于代码。&lt;span style="background-color:#ffffff; color:#24292f"&gt;Options&lt;/span&gt;&amp;nbsp;是控制 rules 行为的设置。&lt;/p&gt;

&lt;p&gt;SwiftFormat 包含超过 50 条 rules，并且一直在添加新 rules。可以在&amp;nbsp;&lt;a href="https://github.com/nicklockwood/SwiftFormat/blob/master/Rules.md"&gt;Rules.md 中&lt;/a&gt;找到最新列表以及有关如何使用它们的文档。&lt;/p&gt;

&lt;p&gt;SwiftFormat 主要被设计为一个格式化程序而不是 linter，即它旨在修复你的代码，而不是告诉你代码出了什么问题。但是，有时在不希望实际改变代码的情况下，验证代码是否已被格式化会很有用。&lt;/p&gt;

&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;span style="background-color:#ffffff"&gt;目前，SwiftFormat 适用于 macOS 10.13 (High Sierra) 及更高版本，也适用于 Ubuntu Linux。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

                                                                    &lt;/div&gt;
                                                                </description>
      <link>https://www.oschina.net/p/swiftformat</link>
      <guid isPermaLink="false">https://www.oschina.net/p/swiftformat</guid>
      <pubDate>Fri, 18 Jul 2025 10:10:00 GMT</pubDate>
    </item>
    <item>
      <title>Gitee AI MCP Server 上线：在 Cursor 里玩 AI 生图 + 语音</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;模力方舟现已上线&amp;nbsp;&lt;strong&gt;Gitee AI MCP Server&lt;/strong&gt;，为 AI 助手和多模态应用提供统一的上下文协议（Model Context Protocol, MCP）接入能力，目前已支持文本生成图片与语音两项功能。&lt;/p&gt; 
&lt;p&gt;MCP 是干什么的相信大家已经很熟悉了，那么&lt;strong&gt;话不多说，先看效果&lt;/strong&gt;：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0718/174720_hnOw_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;这张图展示了使用 Gitee AI MCP Server 在 Cursor 客户端中完成&lt;strong&gt;「AI 生成项目 Logo 并自动加入项目目录」&lt;/strong&gt;的完整流程：&lt;/p&gt; 
&lt;p&gt;1️⃣&amp;nbsp;&lt;strong&gt;调用 MCP 服务生成图片&lt;/strong&gt;：AI 根据用户输入，通过 text-to-image 接口生成图像，并返回公网 URL；&lt;/p&gt; 
&lt;p&gt;2️⃣&amp;nbsp;&lt;strong&gt;自动下载图片并保存&lt;/strong&gt;：通过 wget 命令将图片保存至项目本地目录（assets/logo.png）；&lt;/p&gt; 
&lt;p&gt;3️⃣&amp;nbsp;&lt;strong&gt;智能插入引用代码&lt;/strong&gt;：AI 自动将图片路径添加至 index.html 和 README.md，分别用于页面展示与项目文档；&lt;/p&gt; 
&lt;p&gt;4️⃣&amp;nbsp;&lt;strong&gt;图片成功渲染&lt;/strong&gt;：最终图像正确加载，展示在页面中，形成清晰的视觉输出。&lt;/p&gt; 
&lt;p&gt;这正是 Gitee AI MCP Server 在实际开发流程中「即插即用」的真实写照：图像生成、文件管理、代码修改，用 AI 一气呵成。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;除了 Cursor 外，Gitee AI MCP Server 还支持 Claude Code 和 Cherry Studio 等支持 MCP 协议的 AI 工具。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;对于还不熟悉 MCP 的同学，接下来就听听马建仓的详细介绍：&lt;/p&gt; 
&lt;h4&gt;什么是 Gitee AI MCP Server？&lt;/h4&gt; 
&lt;p&gt;Gitee AI MCP Server 是一项专为模力方舟设计的模型上下文协议服务，支持通过 MCP 协议接入多媒体模型，包括图像生成和语音合成工具。它可集成到 Cursor、Claude Desktop 等 AI 工具中，&lt;strong&gt;帮助 AI 助手「看得见、讲得出」&lt;/strong&gt;。&lt;/p&gt; 
&lt;h4&gt;两大核心能力：文本生成图片与语音&lt;/h4&gt; 
&lt;p&gt;🖼&amp;nbsp;&lt;strong&gt;文本生成图片&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;支持模型：如&amp;nbsp;&lt;code&gt;stable-diffusion-3.5-large-turbo&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;可设定图像尺寸、参考图像（URL 或 Base64）&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;输出格式灵活（Base64、URL 链接）&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;支持用户 ID 追踪与内容定向&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;🔊&amp;nbsp;&lt;strong&gt;文本生成语音&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;支持模型：如&amp;nbsp;&lt;code&gt;whisper-large-v3-turbo&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;输出格式支持 MP3、WAV，支持二进制流或临时 URL&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;URL 链接有效期为 1 小时，适合集成即时内容播报场景&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;快速接入指南&lt;/h4&gt; 
&lt;p&gt;1.登录模力方舟获取访问令牌（Access Token）：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0718/174835_XPrS_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;2.在支持 MCP 的客户端中配置：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;{
&amp;nbsp;&amp;nbsp;"mcpServers": {
&amp;nbsp; &amp;nbsp;&amp;nbsp;"gitee-ai": {
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"url":&amp;nbsp;"https://ai.gitee.com/mcp/sse",
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"headers": {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"Authorization":&amp;nbsp;"Bearer &amp;lt;your_access_token&amp;gt;"
&amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; }
}

&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;以 Cursor 为例：&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;1️⃣ 进入设置-添加新的 MCP Server&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0718/174851_3Vqd_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;2️⃣ 填写配置文件+访问令牌&lt;strong&gt;并保存&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0718/174912_yS8Z_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;3️⃣ 显示为绿色即加载成功，快去试试吧！&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0718/174934_QqDI_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;配置完成后，你就可以在日常使用 AI 助手时，直接调用图像和语音生成功能。例如在写技术文档时一键生成配图、给项目介绍加上语音旁白，甚至批量产出社媒图文组合内容。&lt;/p&gt; 
&lt;p&gt;不管你是在 Cursor 或 Claude Desktop 中使用 AI 助手，还是在自己的项目中集成多模态能力，Gitee AI MCP Server 都能为你提供你快速接入图像与语音能力。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;查看文档：&lt;/strong&gt;&lt;a href="https://ai.gitee.com/docs/best-practice/mcp" target="_blank"&gt;https://ai.gitee.com/docs/best-practice/mcp&lt;/a&gt;，了解更多有关 Gitee AI MCP Server 的详细信息。&lt;/p&gt; 
&lt;/blockquote&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361064</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361064</guid>
      <pubDate>Fri, 18 Jul 2025 09:50:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>OpenNJet v3.3.1 发布，云原生应用引擎</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;OpenNJet v3.3.1 已经发布，云原生应用引擎。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;新功能&lt;/strong&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;ul&gt; 
 &lt;li&gt;深层清理 Lua 代码&lt;/li&gt; 
 &lt;li&gt;http ssl 模块新增一个指令 ssl_certificate_management 用于证书续签功能配置&lt;/li&gt; 
 &lt;li&gt;新增 helper 程序 cert-manager，支持 ACME 协议，实现自动证书管理&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;功能优化&lt;/strong&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;ul&gt; 
 &lt;li&gt;http/stream vs 不允许重名&lt;/li&gt; 
 &lt;li&gt;Http 动态添加 ssl VS 时，检查是否配置证书。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Bug fix&lt;/strong&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;ul&gt; 
 &lt;li&gt;修复 njt_stream_ftp_proxy_module 头文件中宏定义错误&lt;/li&gt; 
 &lt;li&gt;修复 upstream api 添加 ip 时，指定 service 字段，提示为：route 字段。&lt;/li&gt; 
 &lt;li&gt;修复 upstream api 解析 fail_timeout，slow_start 字段，错误格式，被解析为 -1 的 bug。&lt;/li&gt; 
 &lt;li&gt;修复编译时 openapi_parser_module 没有被编译的错误。&lt;/li&gt; 
 &lt;li&gt;修复动态 location proxy_set_header 动态新变量不生效的 bug&lt;/li&gt; 
 &lt;li&gt;Http 添加动态 upstream，hash 算法无法获取新变量值的 bug&lt;/li&gt; 
 &lt;li&gt;修复了 api gateway 通过 Open API 3.0 格式文档导入 API 接口时，报 internal error 错误&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;遗留已知问题&lt;/strong&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;ul&gt; 
 &lt;li&gt;tcp 流量劫持基于 iptables nat 表 prerouting 添加规则，只针对外部访问（非本机 ip）的数据做流量劫持&lt;/li&gt; 
 &lt;li&gt;HA/MA 配置同步关于命令式 api 动态删除 location 消息同步存在问题&lt;/li&gt; 
 &lt;li&gt;应用加速功能，删除 location 后本地缓存文件没有立马同步清理&lt;/li&gt; 
 &lt;li&gt;配置沙箱进程，在 NJet 可执行文件热升级时，无法在旧的沙箱进程退出后，再启动新的配置沙箱&lt;/li&gt; 
 &lt;li&gt;动态 VS，只能在存在的 listen 上添加 VS ，该功能不能创建 listen&lt;/li&gt; 
 &lt;li&gt;动态 VS，不支持 zone、 location、ssl_ocsp、ssl_stapling、quic 指令&lt;/li&gt; 
 &lt;li&gt;动态 VS，不支持动态创建，新的 error_log、access_log 文件&lt;/li&gt; 
 &lt;li&gt;动态 location，不支持 zone 指令&lt;/li&gt; 
 &lt;li&gt;动态 SSL 证书添加时必须有初始证书，通过动态 VS 添加的 ssl server，如果未配置初始证书，通过动态 ssl 接口添加证书会添加失败&lt;/li&gt; 
 &lt;li&gt;显示 location 的 api，配置 if 和 limit_except 时，存在显示异常&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;rpm 包安装&lt;/strong&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;下面是 centos 系统使用步骤（其他系统需要从下载链接选择对应合适的安装包）&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;下载 njet-3.3.1-1.el7.x86_64.rpm 安装包&lt;/li&gt; 
 &lt;li&gt;上传文件到待安装主机&lt;/li&gt; 
 &lt;li&gt;sudo yum localinstall ./njet-3.3.1-1.el7.x86_64.rpm&lt;/li&gt; 
 &lt;li&gt;systemctl start njet&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;otel 安装包为 opentelemetry 模块的安装包（如果需要该功能，从下载连接选择对应的安装包安装即可） captcha 安装包是动态验证码模块的安装包（如果需要该功能，从下载连接选择对应的安装包安装即可） OpenNJet ubuntu deb 安装包基于 18.04 版本编译，可在 ubuntu20、ubuntu22 上安装使用&lt;/p&gt; 
&lt;p&gt;已安装 njet 的 ubuntu , 升级 njet 包： sudo systemctl stop njet sudo apt-get update sudo apt install --only-upgrade njet&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;docker 方式运行&lt;/strong&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;可使用如下命令启动运行： docker run -d --rm --privileged tmlake/njet:3.3.1 详细运行方法参考： &lt;a href="https://gitee.com/link?target=https%3A%2F%2Fnjet.org.cn%2Fcases%2Fnjet-docker%2F" target="_blank"&gt;https://njet.org.cn/cases/njet-docker/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;下载列表安装包说明：&lt;/strong&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;njet: OpenNJet 安装包&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;.deb 适用 ubuntu 系统&lt;/li&gt; 
   &lt;li&gt;el7.x86_64.rpm 适用 centos 系统&lt;/li&gt; 
   &lt;li&gt;.ky10.x86_64.rpm 适用 kylin 麒麟系统&lt;/li&gt; 
   &lt;li&gt;.an8.loongarch64.rpm 适用龙芯系统&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;njet_python_wsgi: 支持 python 脚本及 wsgi 应用的模块，安装包&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;.deb 适用 ubuntu 系统&lt;/li&gt; 
   &lt;li&gt;el7.x86_64.rpm 适用 centos 系统&lt;/li&gt; 
   &lt;li&gt;.ky10.x86_64.rpm 适用 kylin 麒麟系统&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;njet_otel: telemetry 模块（链路追踪）安装包&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;.deb 适用 ubuntu 系统&lt;/li&gt; 
   &lt;li&gt;el7.x86_64.rpm 适用 centos 系统&lt;/li&gt; 
   &lt;li&gt;.ky10.x86_64.rpm 适用 kylin 麒麟系统&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;njet_captcha: 动态验证码认证模块，可以针对 location 设定访问频率，当某个 location 被恶意的频繁访问时，可以根据配置， 对访问频率超过一定次数的 client_ip,动态的弹出认证页面，手动输入验证码，解除限制&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;.deb 适用 ubuntu 系统&lt;/li&gt; 
   &lt;li&gt;el7.x86_64.rpm 适用 centos 系统&lt;/li&gt; 
   &lt;li&gt;.ky10.x86_64.rpm 适用 kylin 麒麟系统&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;portal_1.0.2_noarch.npk: Portal 应用管理模块 (系统无关)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ssh_remote_mod.so: Portal 需要的 Lua 额外模块 (系统无关)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;cert-manager: 支持 ACME 协议，实现自动证书管理&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;cert-manager-arm64 适用 arm64&lt;/li&gt; 
   &lt;li&gt;cert-manager 适用 x86-64&lt;/li&gt; 
   &lt;li&gt;cert-manager-loongarch64 适用 LoongArch&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;registry-sync: 从配置中心同步服务的实例信息，并自动更新到 NJet 的 Upstream 中， 目前只支持 etcd&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;registry-sync-arm64 适用 arm64&lt;/li&gt; 
   &lt;li&gt;registry-sync 适用 x86-64&lt;/li&gt; 
   &lt;li&gt;registry-sync-loongarch64 适用 LoongArch&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;详情查看：&lt;a href="https://gitee.com/njet-rd/njet/releases/v3.3.1"&gt;https://gitee.com/njet-rd/njet/releases/v3.3.1&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361061</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361061</guid>
      <pubDate>Fri, 18 Jul 2025 09:39:00 GMT</pubDate>
      <author>来源: 资讯</author>
    </item>
    <item>
      <title>英特尔将终止开发 Clear Linux</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;英特尔将终止开发其优化性能的 Clear Linux 发行版。近日，英特尔发布了一份声明，&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcommunity.clearlinux.org%2Ft%2Fall-good-things-come-to-an-end-shutting-down-clear-linux-os%2F10716"&gt;宣布&lt;/a&gt;Clear Linux 即将停更：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;经过多年的创新和社区协作，我们将终止对 Clear Linux OS 的支持。英特尔将立即停止为 Clear Linux OS 提供安全补丁、更新或维护，Clear Linux OS GitHub 存储库将以只读模式存档。因此，如果您目前正在使用 Clear Linux OS，我们强烈建议您尽快规划迁移到其他积极维护的 Linux 发行版，以确保持续的安全性和稳定性。&lt;/p&gt; 
 &lt;p&gt;请放心，英特尔将继续大力投资 Linux 生态系统，积极支持和贡献各种开源项目和 Linux 发行版，以支持和优化英特尔硬件。&lt;/p&gt; 
 &lt;p&gt;衷心感谢过去十年来为 Clear Linux OS 的打造做出贡献的每一位开发者、用户和贡献者。你们的反馈和贡献非常宝贵。&lt;/p&gt; 
 &lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0721/142106_LhpF_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;近年来，坊间多次传闻 Clear Linux 可能面临被砍掉的风险，原因是英特尔削减成本，并试图强调主流 Linux 发行版的开箱即用性能，并将其更多工作成果也推向上游。不过今天，这件事终于得到了官方的确认。&lt;/p&gt; 
&lt;p&gt;Clear Linux 为 Linux 系统提供了许多出色的开箱即用性能优化，并展示了多年来通过配置文件引导优化 / 链接时优化、各种内核调整和其他创新技术对打包系统带来的改进。至少像 CachyOS 这样的系统已经采用了其中一些优化。&lt;/p&gt; 
&lt;p&gt;英特尔工程师也在与其他主流 Linux 发行版合作，以提升其 Linux 发行版的性能。但这些细节以及他们是否会加大这方面的努力仍不清楚。&lt;/p&gt; 
&lt;p&gt;本周，一位非常杰出的 Linux 工程师离开了英特尔，由于另一位工程师的离职，上游 Linux 驱动程序现在已经进入无人维护的状态，而作为英特尔最新重组的一部分，其他几位从事开源 / Linux 工作的英特尔软件工程师也纷纷离职。&lt;/p&gt; 
&lt;p&gt;过去十年，Clear Linux 操作系统展现了其在 x86_64 硬件上开箱即用的性能潜力，不仅在英特尔平台上，甚至在 AMD x86_64 上也表现出了极其出色的性能。但随着英特尔的成本削减和裁员行动，Clear Linux 即将终止开发。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361443</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361443</guid>
      <pubDate>Thu, 17 Jul 2025 06:23:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>马斯克宣布将推出儿童版 AI 应用「Baby Grok」</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;埃隆・马斯克通过社交平台 X 宣布，他的人工智能公司 xAI 将推出一款专为儿童设计的全新应用&lt;/span&gt;&lt;span style="color:#000000"&gt; 「Baby Grok」。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="293" src="https://oscimg.oschina.net/oscnet/up-283f6a6c5ebd5db431335f51b4ae7e651ae.jpg" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;马斯克并未详细说明 「Baby Grok」 的具体功能，但他强调该应用将提供 「友好型内容」，旨在保护儿童在网络世界中的安全。Baby Grok&amp;nbsp;可能包含以下内容：&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:start"&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;span style="color:#000000"&gt;界面简单，方便孩子使用。&lt;/span&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;span style="color:#000000"&gt;强大的内容过滤器可以阻止任何不安全的内容。&lt;/span&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;span style="color:#000000"&gt;家长工具用于检查和管理孩子与机器人的对话。&lt;/span&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;span style="color:#000000"&gt;针对不同年龄段设计的学习工具和互动故事。&lt;/span&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;xAI 公司在过去几个月内的快速发展备受关注。早在今年 7 月，该公司刚刚推出了新一代聊天机器人 Grok4，时间距离前一版本的发布仅数月。然而，Grok4 也曾因在社交平台上发布的一些言论而引发舆论争议。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;「Baby Grok」 的推出标志着 xAI 公司在发展方向上的新尝试。此前，该公司一直专注于通用人工智能模型的开发，现在则开始关注儿童这一特殊群体。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361435</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361435</guid>
      <pubDate>Thu, 17 Jul 2025 06:09:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>人形机器人公司加快融资及上市步伐</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#141414; text-align:justify"&gt;&lt;span style="color:#000000"&gt;中国招标投标公共平台 7 月 18 日信息显示，优必选中标觅亿（上海）汽车科技有限公司 9051.15 万元设备采购项目。据悉，这是优必选目前中标金额最大的采购订单。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#141414; text-align:justify"&gt;&lt;span style="color:#000000"&gt;与此同时，7 月 7 日，星动纪元宣布完成近 5 亿元 A 轮融资；7 月 8 日，云深处宣布完成近 5 亿元新一轮融资，它石智航完成 1.22 亿美元天使+轮融资，小雨智造完成约 1 亿元 A++轮融资；7 月 9 日，星海图宣布接连完成 A4 轮及 A5 轮战略融资，两轮合计融资金额超过 1 亿美元；7 月 15 日，智元机器人透露于近日获得正大集团旗下正大机器人的战略投资……仅 7 月以来，人形机器人赛道便发生多起投融资事件。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#141414; text-align:justify"&gt;&lt;span style="color:#000000"&gt;资本持续涌入，不少头部人形机器人公司获得大额订单，产业何时能够实现整体商业化爆发，值得关注。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#141414; text-align:justify"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;资本热切涌入&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#141414; text-align:justify"&gt;&lt;span style="color:#000000"&gt;据 IT 桔子数据，今年上半年，国内人形机器人领域共发生 77 起投融资事件，包括了宇树科技、智元机器人、银河通用、千寻智能、逐际动力、众擎机器人等一众知名公司，这超过了去年全年的 67 起。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#141414; text-align:justify"&gt;&lt;span style="color:#000000"&gt;其中最引人瞩目的是 6 月份完成 C 轮融资的宇树科技。这笔 2024 年底启动交割的融资，吸引了中国移动旗下基金、腾讯、锦秋基金、阿里、蚂蚁、吉利资本等一众知名机构的入局，成为投资人最后的上车机会。仅一个月后，宇树科技便宣布启动上市辅导。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#141414; text-align:justify"&gt;&lt;span style="color:#000000"&gt;至此，2016 年创立的宇树科技已完成 9 轮融资，在红杉中国、腾讯、阿里、蚂蚁、美团龙珠、深创投、中关村科学城、上海科创基金、经纬创投、祥峰投资、源码资本、顺为资本、北京机器人产业投资基金等助推下，其上市前的估值达到 120 亿元。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#141414; text-align:justify"&gt;&lt;span style="color:#000000"&gt;智元机器人同样受到热切追捧。智元机器人于近日获得了正大集团旗下正大机器人的战略投资。正大机器人将助力智元机器人在生命科技、新零售、新消费、康养服务等垂直领域进行全场景业务的探索开发。此前，智元机器人已获得腾讯、京东、比亚迪、上汽、北汽、TCL 等多家产业资本投资。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#141414; text-align:justify"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;多重因素催化&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#141414; text-align:justify"&gt;&lt;span style="color:#000000"&gt;资本市场对人形机器人的热情，离不开各类标志性事件的发生与政策的持续加持。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#141414; text-align:justify"&gt;&lt;span style="color:#000000"&gt;今年 1 月 6 日，智元机器人量产的第 1000 台通用具身机器人下线；1 月 17 日，乐聚机器人举办第 100 台全尺寸人形机器人交付仪式；2 月，宇树科技产品亮相春晚后，在其京东官方旗舰店上架了两款人形机器人产品，首批产品很快售罄……种种数字让人形机器人逐渐从一个概念变成了可以预见的产业。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#141414; text-align:justify"&gt;&lt;span style="color:#000000"&gt;一些传统制造领域巨头的下场，也增添了资本市场对人形机器人的信心。例如，广汽集团称公司第三代具身智能人形机器人 GoMate，是行业首创可变轮足构型具身智能机器人，计划 2025 年实现自研零部件批量生产；2026 年实现整机小批量生产，并逐步扩展至大规模量产。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#141414; text-align:justify"&gt;&lt;span style="color:#000000"&gt;引爆一级市场投融资热情，少不了二级市场的火热行情。7 月 8 日，科创板公司上纬新材宣布，智元机器人旗下的持股平台上海智元恒岳科技合伙企业（有限合伙）及其一致行动人上海致远新创科技设备合伙企业（有限合伙）将通过「协议转让+主动要约」方式收购公司控制权。7 月 9 日复牌以来，上纬新材已连续多个交易日「20cm」涨停。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#141414; text-align:justify"&gt;&lt;span style="color:#000000"&gt;不少人形机器人公司上市进程也显著提速。除了宣布启动上市辅导的宇树科技，智元机器人在入主上市公司之余将继续按照既定目标冲击港股 IPO。此外，极智嘉 7 月 9 日登陆港交所，乐动机器人、仙工智能、卧安机器人、云迹科技等也拟在今年赴港上市。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#141414; text-align:justify"&gt;&lt;span style="color:#000000"&gt;从政策方面来看，2023 年，工业和信息化部印发的《人形机器人创新发展指导意见》提出，到 2025 年，人形机器人创新体系初步建立；到 2027 年，人形机器人技术创新能力显著提升，形成安全可靠的产业链供应链体系，构建具有国际竞争力的产业生态，综合实力达到世界先进水平。北京、上海、深圳等多地也于今年纷纷出台对具身智能的相应支持政策。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#141414; text-align:justify"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;处在商业化爆发前夕&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#141414; text-align:justify"&gt;&lt;span style="color:#000000"&gt;判断这场资本盛宴能持续多久的，则是人形机器人的商业化前景。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#141414; text-align:justify"&gt;&lt;span style="color:#000000"&gt;「我们预判人形机器人产业正处于商业化爆发前夜。」夏厦精密总经理夏挺对中国证券报记者表示，「首先，头部厂商如特斯拉、比亚迪等加速布局，推动核心零部件需求激增。其次，人口老龄化加速，以及制造业‘危险、肮脏、枯燥’岗位的劳动力缺口，将催生人形机器人的规模化需求。再次，技术突破与成本下降的拐点正在临近，技术突破推动人形机器人成本从 10 万美元向 2 万美元下探，商业化门槛将显著降低。」&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#141414; text-align:justify"&gt;&lt;span style="color:#000000"&gt;产业爆发的信号已然显现。优必选自今年 3 月人形机器人天工行者发布后在手订单已达百台，刚刚又中标近亿元订单；智元机器人和宇树科技日前中标 1.24 亿元人形机器人订单；乐聚机器人预计全年人形机器人总交付量将进入千台级别；松延动力总订单规模已超 2500 台，总合同额超过 1 亿元……&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#141414; text-align:justify"&gt;&lt;span style="color:#000000"&gt;在人形机器人产业快速发展的同时，也要冷静看待、耐心等待。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#141414; text-align:justify"&gt;&lt;span style="color:#000000"&gt;7 月 15 日，在国新办举行的「新征程上的奋斗者」中外记者见面会上，宇树科技创始人王兴兴表示，目前行业处在相对早期阶段，大家可以多给一些耐心。未来 3 到 5 年，人形机器人应用会越来越快，当下已经有一些应用场景，国内外不少公司人形机器人出货量都有明显增长，服务业、家用、工业场景、危险场景救援救灾场景都有推进，但「大规模应用、大规模推广，可能还需要一些时间。」&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#141414; text-align:justify"&gt;&lt;span style="color:#000000"&gt;高工机器人产业研究所所长卢瀚宸也对中国证券报记者表示，「人形机器人核心技术与场景落地瓶颈还有待突破，行业尚处于发展初期阶段，没有成功的模板可以参照，从技术到产品，再到落地应用，均需要经历摸索的过程，主要的思路是通过强化攻关、开放场景、补强企业生态，加速人形机器人从展示向应用跃迁。」&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361412</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361412</guid>
      <pubDate>Thu, 17 Jul 2025 03:54:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Debian 13.0 将于 8 月 9 日发布</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Debian 团队&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flists.debian.org%2Fdebian-devel-announce%2F2025%2F07%2Fmsg00003.html" target="_blank"&gt;宣布&lt;/a&gt; Debian 13.0 "Trixie"计划于 8 月 9 日发布，7 月 27 日完全冻结。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0721/115000_ldRv_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Debian Trixie 代表了为期两年的开发历程，内核采用 Linux 6.12 LTS，包含桌面环境 GNOME 48 、GCC 14.2 编译器、Python 3.13 等大量软件更新。Debian Trixie 将首次正式支持 64 位 RISC-V 架构。&lt;/p&gt; 
&lt;p&gt;10 年前，Debian Linux 通过 RISCV64 移植版为 RISC-V 架构提供支持，而现在 Debian 13.0 将作为官方支持 RISC-V 的正式版，RV64GC 是 Debian RISC-V 的当前目标，使用基于 UEFI 启动作为默认启动方式。软件包方面目前已经有超过 17000 个源 Debian 软件包正在使用 Debian 13 Trixie 为 RISC-V 架构提供构建。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361407/debian-13-risc-v-ready</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361407/debian-13-risc-v-ready</guid>
      <pubDate>Thu, 17 Jul 2025 03:51:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>AI 编程智能体 Gemini CLI 发布 v0.1.13</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;谷歌更新了其 Gemini 命令行工具 (CLI) 至 v0.1.13 版本，并发布了该工具的 v1 版本路线图。&lt;/p&gt; 
&lt;p&gt;此次更新带来了一系列&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fgoogle-gemini%2Fgemini-cli%2Fdiscussions%2F4516" target="_blank"&gt;功能增强和体验优化&lt;/a&gt;。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;取消操作提示：在取消操作时显示被修改的内容。&lt;/li&gt; 
 &lt;li&gt;确认快捷键：使用数字键作为确认快捷方式。&lt;/li&gt; 
 &lt;li&gt;认知循环缓解：引入对认知循环的检测与缓解机制。&lt;/li&gt; 
 &lt;li&gt;MCP 服务器管理：允许用户指定允许或排除特定的 MCP 服务器。&lt;/li&gt; 
 &lt;li&gt;默认身份验证：为频繁切换的用户设置默认身份验证类型。&lt;/li&gt; 
 &lt;li&gt;隐藏启动横幅：提供隐藏启动横幅的选项。&lt;/li&gt; 
 &lt;li&gt;终端换行支持：通过 \ 字符为不支持 Shift+Enter 的终端提供换行支持。&lt;/li&gt; 
 &lt;li&gt;代理支持：新增 --proxy 标志以通过代理路由所有请求。&lt;/li&gt; 
 &lt;li&gt;MCP 调试：输出在 ctrl+o 窗格中查看 MCP 工具调试输出。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;下载地址：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fgoogle-gemini%2Fgemini-cli%2Freleases%2Ftag%2Fv0.1.13" target="_blank"&gt;https://github.com/google-gemini/gemini-cli/releases/tag/v0.1.13&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361404</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361404</guid>
      <pubDate>Thu, 17 Jul 2025 03:39:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Mistral AI 寻求 10 亿美元融资</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;来自彭博社的消息称，法国知名的大模型开源平台 Mistral 正在与阿布扎比的 MGX 基金以及法国的一些贷款机构进行洽谈，计划筹集一轮高达 10 亿美元的融资。这一举措表明 Mistral 正朝着快速扩张的方向迈进，进一步巩固其在人工智能领域的地位。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="235" src="https://oscimg.oschina.net/oscnet/up-c8384010133323aaece7fe7cfae2f077d66.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;MistralAI 成立于 2023 年 4 月，由三位曾在 Meta AI 和 Google DeepMind 任职的研究员 ——Arthur Mensch、Guillaume Lample 和 Timothée Lacroix 共同创立。自成立以来，MistralAI 一直专注于开发高性能的人工智能大模型，并于 2023 年 9 月发布了其首个模型 Mistral7B。该模型在多项基准测试中表现优异，超越了竞争对手 Llama213B，显示出其强大的技术实力。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;MistralAI 自成立以来，融资步伐迅速。2023 年 6 月，公司成功募得约 1.05 亿欧元;而在 10 月，又进一步筹集到 3.85 亿欧元;到 12 月，其估值已突破 20 亿欧元。进入 2024 年后，Mistral 继续迎来融资热潮，最新一轮融资金额达到 6 亿欧元，估值飙升至约 58 亿欧元。这些资金将为公司的持续发展和技术创新提供强有力的支持。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;在产品方面，MistralAI 不仅专注於单一的大型模型，还积极开发基于混合专家架构的多个模型，如 Mixtral8x7B 和 Pixtral Large。同时，针对边缘设备的需求，Mistral 也推出了小型模型 MiniStral 系列。其旗舰模型 Mistral Large 支持多语言交流，包括法语、英语、德语、西班牙语和意大利语等多种语言，展示了其在全球市场的潜力。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361399</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361399</guid>
      <pubDate>Thu, 17 Jul 2025 03:17:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>告别提示词工程，「上下文工程」才是 AI Agent 的核心竞争力</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;编者按：&lt;/strong&gt; 什么样的技能才能真正决定 AI 智能体的成败？是更复杂的算法，还是更精妙的提示词？我们今天为大家带来的文章，作者的观点是：构建强大 AI 智能体的关键已从"提示词工程"转向"上下文工程"。&lt;/p&gt; 
 &lt;p&gt;文章从"上下文"的广义定义出发，详细拆解了影响 AI 决策的七大关键要素，包括系统指令、用户输入、历史对话、长期记忆、外部检索信息、可用工具及输出结构。通过对比"廉价演示项目"与"神奇智能体"的案例，作者生动展现了上下文质量如何决定 AI 的表现 ------ 真正的差距不在于模型本身，而在于是否提供了恰当的上下文支持。作者进一步提出，上下文工程是一套动态流程，需跨领域协作，以结构化的方式整合业务需求与技术实现，确保 LLM 在正确的时间获得正确的信息与工具。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;作者 | Philipp Schmid&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;编译 | 岳扬&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;上下文工程（Context Engineering）是一个在人工智能领域逐渐走红的新术语。行业内讨论的焦点正从"提示词工程"（prompt engineering）转向一个更广泛、更强大的概念：上下文工程（Context Engineering）。托比·卢克（Tobi Lutke）[1]将其描述为"&lt;strong&gt;为任务提供完整的上下文背景，使大语言模型能够合理解决问题的一门艺术&lt;/strong&gt;"，他说得很到位。&lt;/p&gt; 
&lt;p&gt;随着 Agents 的兴起，将哪些信息输入"有限的工作记忆（limited working memory）"中变得越来越重要。我们观察到，决定一个 Agent 成败的关键因素，通常就在于你提供给它的上下文质量。&lt;strong&gt;大多数 Agent 的失败早已不是模型本身的问题，而恰恰是上下文供给的失败。&lt;/strong&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;01 什么是上下文（Context）？&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;要理解上下文工程（Context Engineering），我们首先必须扩展对"上下文"的定义。它不仅指你发送给 LLM 的单一提示词（prompt）。应该将其视为模型在生成响应前所看到的一切信息。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-e95332f2e32fa52708edde20871b576d0d0.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;指令 / 系统提示词（Instructions / System Prompt）&lt;/strong&gt; ： 用于定义模型在对话期间行为的初始指令集，可以/应该包含示例、规则等。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;用户提示词（User Prompt）&lt;/strong&gt; ： 来自用户的即时任务或问题。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;状态 / 历史（短期记忆）[State / History (short-term Memory]&lt;/strong&gt; ： 当前的对话内容，包括导致此刻结果的"用户与模型的历史回复"。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;长期记忆（Long-Term Memory）&lt;/strong&gt; ： 在之前的多次对话中收集的持久性知识库，包含学习到的用户偏好、过往对话摘要、或被明确告知需要记忆以备后续使用的信息。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;检索信息（RAG）[Retrieved Information (RAG)]&lt;/strong&gt; ： 外部的、最新的知识，来自文档、数据库或 API 的相关信息，用于回答特定问题。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;可用工具（Available Tools）&lt;/strong&gt; ： 所有可调用函数或内置工具的标准化描述（如输入参数、输出格式、功能说明）（例如 check_inventory, send_email）。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;结构化输出（Structured Output）&lt;/strong&gt; ： 对模型响应格式的定义，例如一个 JSON 对象。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;&lt;strong&gt;02 为什么重要？从「廉价的演示项目」到「神奇的智能体产品」&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;构建真正高效的 AI 智能体的秘诀，与你编写代码的复杂程度关系不大，而与你提供上下文的质量息息相关。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;构建智能体，与你编写的代码或使用的框架关系不大。&lt;/strong&gt; 一个廉价的演示项目和"神奇的智能体"之间的区别，就在于你所提供上下文的质量。假设让一个 AI 助手根据一封简单的邮件来安排会议：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Hey, just checking if you're around for a quick sync tomorrow.&lt;/p&gt; 
 &lt;p&gt;嘿，想问一问明天方不方便，我们快速碰个头？&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;"廉价的智能体演示项目"的上下文质量很差。它只看到用户的请求，其他什么都看不到。它的代码可能功能完善，它会调用 LLM 并获得响应，但输出的内容却毫无帮助，且充满机械感：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Thank you for your message. Tomorrow works for me. May I ask what time you had in mind?&lt;/p&gt; 
 &lt;p&gt;感谢来信！明天我有空。你想约在几点？&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;"神奇的智能体"则由丰富的上下文驱动。其代码的主要任务并非琢磨如何回应，而是收集 LLM 所需的信息，以便更好地响应用户需求。在调用 LLM 之前，你可以扩展上下文，使其包含：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;你的日历信息（显示你日程已满）。&lt;/li&gt; 
 &lt;li&gt;你与此人的过往邮件（用于确定合适的非正式语气）。&lt;/li&gt; 
 &lt;li&gt;你的联系人列表（用于识别 ta 为关键的合作伙伴）。&lt;/li&gt; 
 &lt;li&gt;send_invite 或 send_email 工具。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;然后便能生成回应：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Hey Jim! Tomorrow's packed on my end, back-to-back all day. Thursday AM free if that works for you? Sent an invite, lmk if it works.&lt;/p&gt; 
 &lt;p&gt;嗨 Jim！明天我这边日程全排满了，从早到晚连轴转。周四上午有空，你看行不？邀请已发，确认下是否合适~&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;这种神奇的效果并非源于更聪明的模型或更精巧的算法，而在于为正确的任务提供了恰当的上下文。这就是为什么上下文工程（Context Engineering）非常重要。&lt;strong&gt;智能体的失败并非仅仅是模型的失败，本质上是上下文的缺失。&lt;/strong&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;03 从提示词工程到上下文工程&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;什么是上下文工程？如果说"提示词工程（prompt engineering）"侧重于在单个文本字符串中精心设计一套完美的指令，那么上下文工程（context engineering）的范畴则宽广得多。简而言之：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;上下文工程是一门设计和构建动态系统的学科，它能以正确的格式、在正确的时间提供正确的信息与工具，赋予 LLM 完成任务所需的一切资源。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;上下文工程是&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;一套流程，而非某些字符串&lt;/strong&gt;：上下文不仅是一个静态的提示词模板。它是主 LLM 调用前系统运行所产生的输出。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;动态构建的&lt;/strong&gt;：随任务即时生成，适配用户当下的需求。对某个请求，其上下文可能是日历数据，对另一请求，上下文则可能是邮件记录或网页搜索结果。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;在正确的时间提供正确的信息和工具&lt;/strong&gt;：其核心职责是确保模型不遗漏关键细节（Garbage In, Garbage Out）。这意味着只有在必需且有帮助时才提供知识（信息）与能力（工具）。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;注重呈现格式&lt;/strong&gt;：如何呈现信息很重要。简明扼要的摘要胜过原始数据的堆砌，清晰的工具架构胜过模糊的指令。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;&lt;strong&gt;04 Summary&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;构建强大且可靠的 AI 智能体，已经不再需要寻找神奇的提示词或更新模型版本。其核心在于上下文工程，即以正确的格式、在正确的时间提供正确的信息与工具。这是一项跨领域协作的挑战，需要理解业务场景、定义预期输出，并结构化组织所有必要的信息，使 LLM 能够真正"完成任务"。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;05 致谢&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;本综述的完成得益于深度研究（deep research）与人工校验（manual research），并从以下优质资源中汲取了灵感与信息：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Tobi Lutke tweet[1]&lt;/li&gt; 
 &lt;li&gt;Karpathy tweet[2]&lt;/li&gt; 
 &lt;li&gt;The rise of "context engineering"[3]&lt;/li&gt; 
 &lt;li&gt;Own your context window[4]&lt;/li&gt; 
 &lt;li&gt;Context Engineering by Simon Willison[5]&lt;/li&gt; 
 &lt;li&gt;Context Engineering for Agents[6]&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;END&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;本期互动内容 🍻&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;❓&lt;strong&gt;你是否有过动态构建上下文的经验？能否分享一个你认为特别成功的案例？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;文中链接&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;[1]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Ftobi%2Fstatus%2F1935533422589399127" target="_blank"&gt;https://x.com/tobi/status/1935533422589399127&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[2]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Fkarpathy%2Fstatus%2F1937902205765607626" target="_blank"&gt;https://x.com/karpathy/status/1937902205765607626&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[3]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.langchain.com%2Fthe-rise-of-context-engineering%2F" target="_blank"&gt;https://blog.langchain.com/the-rise-of-context-engineering/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[4]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fhumanlayer%2F12-factor-agents%2Fblob%2Fmain%2Fcontent%2Ffactor-03-own-your-context-window.md" target="_blank"&gt;https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-03-own-your-context-window.md&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[5]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fsimonwillison.net%2F2025%2FJun%2F27%2Fcontext-engineering%2F" target="_blank"&gt;https://simonwillison.net/2025/Jun/27/context-engineering/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[6]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Frlancemartin.github.io%2F2025%2F06%2F23%2Fcontext_engineering%2F" target="_blank"&gt;https://rlancemartin.github.io/2025/06/23/context_engineering/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;strong&gt;本文经原作者授权，由&lt;/strong&gt; &lt;strong&gt;Baihai IDP&lt;/strong&gt; &lt;strong&gt;编译。如需转载译文，请联系获取授权。&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;原文链接：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.philschmid.de%2Fcontext-engineering" target="_blank"&gt;https://www.philschmid.de/context-engineering&lt;/a&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/IDP/blog/18685020</link>
      <guid isPermaLink="false">https://my.oschina.net/IDP/blog/18685020</guid>
      <pubDate>Thu, 17 Jul 2025 03:05:00 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>Sam Altman 透露 GPT-5 即将发布</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;在宣布其模型获得国际数学奥林匹克竞赛金牌的同时，OpenAI&amp;nbsp;CEO&amp;nbsp;Sam Altman&amp;nbsp;和研究科学家&amp;nbsp;Alexander Wei&amp;nbsp;透露，GPT-5&amp;nbsp;即将发布。然而，他们均明确设定了市场预期：即将发布的&amp;nbsp;GPT-5&amp;nbsp;并非在 IMO 竞赛中获奖的模型。&lt;/p&gt; 
&lt;p&gt;&lt;img height="687" src="https://static.oschina.net/uploads/space/2025/0721/104124_33Ko_2720166.png" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Altman&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Fsama%2Fstatus%2F1946569252296929727" target="_blank"&gt;&amp;nbsp;强调&lt;/a&gt;，获得金牌的 IMO 模型是一个实验性的研究成果，整合了未来将用于其他模型的新研究技术，而即将面世的&amp;nbsp;GPT-5&amp;nbsp;不会具备同等级别的数学能力。&lt;/p&gt; 
&lt;p&gt;他表示，用户会喜欢&amp;nbsp;GPT-5，但具有 IMO 金牌级能力的模型在未来数月内不会发布。&lt;/p&gt; 
&lt;p&gt;与此同时，社区发现在一个公开的基准测试&amp;nbsp;GitHub&amp;nbsp;仓库中出现了一个名为&amp;nbsp;gpt-5-reasoning-alpha-2025-07-13&amp;nbsp;的模型标识符，进一步引发了关于新模型的讨论。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361390</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361390</guid>
      <pubDate>Thu, 17 Jul 2025 02:42:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Altman：2025 年底 OpenAI 将上线超 100 万块 GPU</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;OpenAI &lt;/span&gt;&lt;span style="color:#000000"&gt;CEO 萨姆・奥尔特曼（Sam Altman）近日在社交媒体上宣布，该公司计划在 2025 年底前上线超过 100 万块 GPU。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="418" src="https://static.oschina.net/uploads/space/2025/0721/110350_Gexx_4252687.jpg" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;据悉，OpenAI 的战略主要围绕三个核心领域展开：Stargate（星际之门）项目、芯片供应链重构以及能源挑战。Stargate 是 OpenAI 新成立的公司，目标是为 AI 基础设施建设注入巨额资金。未来四年，该项目预计将投资高达 5000 亿美元（约合 3.59 万亿元人民币），旨在在美国打造一座全新的 AI 基础设施。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;Stargate 项目的首期工程设立在得克萨斯州的阿比林市，占地 1000 英亩，计划建造全球最大的 AI 训练集群。OpenAI 与软银、甲骨文等多家知名企业建立了紧密的合作关系。软银 CEO 孙正义将担任 Stargate 董事长，负责整体财务规划，而 OpenAI 则负责日常运营。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;除了 Stargate 项目外，OpenAI 还将与 Arm、微软和英伟达等巨头合作，进一步推动 AI 技术的发展与应用。这一系列的举措显示了 OpenAI 在全球 AI 基础设施竞赛中的强烈竞争意识和技术雄心。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;值得一提的是，随着 GPU 需求的激增，OpenAI 的计划将可能引发市场的剧烈反响。AI 行业的竞争正日益白热化，OpenAI 的 「百倍扩容」 愿景不仅是自身发展的重要里程碑，也将深刻影响整个行业的格局与未来。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361387</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361387</guid>
      <pubDate>Thu, 17 Jul 2025 02:31:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Manus 创始人覆盘构建 AI Agent 的「上下文工程」实践</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;在爆火仅四个月后，Manus AI 突然几乎全面撤出中国市场，不仅清空全部社交账号内容，而且国行版本的 Manus 也疑似暂停推进。&lt;/p&gt; 
&lt;p&gt;中国通用 AI Agent（智能体）创业公司 Manus 将总部迁至新加坡，并百万年薪招聘 AI 工程师，对被裁员工给予 N+3 或者 2N 赔偿&lt;/p&gt; 
&lt;p&gt;早在上个月，Manus 联合创始人张涛便曾宣布，公司已将全球总部迁至新加坡，并在东京和加州设有办公室。尽管官方未正面回应，只称是「基于经营效率的调整」，但出海所引发裁员等一连串争议问题，也让外界普遍猜测其是否正在「跑路」。&lt;/p&gt; 
&lt;p&gt;风波之中，Manus 联合创始人季逸超近日&lt;u&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmanus.im%2Fblog%2FContext-Engineering-for-AI-Agents-Lessons-from-Building-Manus" target="_blank"&gt;发布了一篇技术博客&lt;/a&gt;&lt;/u&gt;，试图将外界关注点重新拉回产品技术本身。&lt;/p&gt; 
&lt;p&gt;经过四次重构和数百万真实交互，他在文中坦诚地总结了团队在构建 Manus 过程中积累的经验教训。内容既有实操干货，也不乏反思，对业内同行与普通用户来说，都不失为一份值得一读的参考材料。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1140" src="https://static.oschina.net/uploads/space/2025/0721/102230_H9TJ_2720166.png" width="1280" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;省流版：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;1. 押注上下文，不再训练模型&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;与其耗时训练，不如围绕大模型构造「记忆」和流程。上下文工程让你在几小时而不是几周内发布产品更新。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2. KV-Cache 命中率至关重要&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;输入越稳定，缓存命中率越高，成本和延迟越低。三条实战建议：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;避免提示中使用时间戳；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;只追加上下文，避免修改历史记录；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;手动标记缓存断点，保障前缀一致性。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;3. 工具不要动态添加，而是用「遮蔽」法控制选择&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;动态修改工具列表会让缓存失效、模型混乱。Manus 使用「遮蔽 token logits」的方法，让模型「看不见」不应调用的工具。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;4. 用文件系统承载持久上下文&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;大模型上下文再长也会被打满。Manus 让模型把长期记忆写入虚拟文件系统，按需读写，实现「外部记忆」，规避信息丢失。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;5. 重写 ToDo 清单，是操控注意力的重要方法&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;模型容易「中途忘记目标」。Manus 会不断用自然语言更新并重述 &lt;a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Ftodo.md" target="_blank"&gt;todo.md&lt;/a&gt; 文件，把全局目标拉回注意力焦点，防止任务跑偏。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;6. 错误不是要掩盖，而是要保留&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;失败是构建 Agent 过程中的一部分。保留错误日志（如失败的操作、堆栈信息），能帮助模型更新内部信念，减少重复错误。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;7. 少样本提示不是灵丹妙药，要防「同质化陷阱」&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;模型会盲目模仿上下文中的行为模式。Manus 通过引入结构化变化（如不同措辞或顺序），避免模型在长任务中陷入复制粘贴式幻觉。&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;在 Manus 项目的最初阶段，我和我的团队面临一个关键决策：我们是应该使用开源基础模型训练一个端到端的智能体模型，还是基于前沿模型的上下文学习能力构建一个智能体？&lt;/p&gt; 
&lt;p&gt;在我的 NLP 生涯的第一个十年里，我们没有这种选择的奢侈。在遥远的 BERT 时代（是的，已经过去七年了），模型必须先进行微调——和评估——才能迁移到新任务。这个过程通常每次迭代需要数周时间，尽管与今天的 LLM 相比，这些模型非常小。对于快速发展的应用，特别是在产品市场匹配 (PMF) 之前，这种缓慢的反馈循环是一个致命缺陷。这是我上一个创业公司的惨痛教训，当时我从头开始训练模型用于开放信息提取和语义搜索。&lt;/p&gt; 
&lt;p&gt;然后 GPT-3 和 Flan-T5 出现了，我的内部模型一夜之间变得无关紧要。具有讽刺意味的是，这些相同的模型标志着上下文学习的开始——以及一条全新的前进道路。 这个来之不易的教训使选择变得明确：&lt;strong&gt;Manus 将押注于上下文工程&lt;/strong&gt;。这使我们能够在几小时而非几周内交付改进，并使我们的产品与底层模型保持正交：如果模型进步是上涨的潮水，我们希望 Manus 成为那条船，而不是固定在海床上的柱子。&lt;/p&gt; 
&lt;p&gt;尽管如此，上下文工程证明绝非易事。这是一门实验科学——我们已经重建了我们的代理框架四次，每次都是在发现了更好的塑造上下文的方式之后。我们亲切地将这种手动架构搜索、提示调整和经验猜测的过程称为**"&lt;strong&gt;&lt;strong&gt;随机&lt;/strong&gt;&lt;/strong&gt;研究生&lt;strong&gt;&lt;strong&gt;下降&lt;/strong&gt;&lt;/strong&gt;"**。这并不优雅，但它有效。&lt;/p&gt; 
&lt;p&gt;这篇文章分享了我们通过自己的"SGD"所达到的局部最优解。如果你正在构建自己的 AI 代理，我希望这些原则能帮助你更快地收敛。&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;围绕 KV 缓存进行设计&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;如果我必须选择一个指标，我认为&amp;nbsp;KV-cache 命中率是生产阶段 AI 代理最重要的单一指标。它直接影响延迟和成本。为了理解原因，让我们看看典型代理是如何运作的：&lt;/p&gt; 
&lt;p&gt;在接收用户输入后，代理通过一系列工具使用链来完成任务。在每次迭代中，模型根据当前上下文从预定义的动作空间中选择一个动作。然后在环境中执行该动作（例如，Manus 的虚拟机沙盒）以产生观察结果。动作和观察结果被附加到上下文中，形成下一次迭代的输入。这个循环持续进行，直到任务完成。&lt;/p&gt; 
&lt;p&gt;正如你所想象的，随着每一步的推进，上下文不断增长，而输出——通常是结构化的函数调用——保持相对简短。这使得代理（agents）相比聊天机器人的预填充和解码比例高度倾斜。例如在 Manus 中，平均输入与输出的 token 比例约为 100:1。&lt;/p&gt; 
&lt;p&gt;幸运的是，具有相同前缀的上下文可以利用 KV 缓存，这大大减少了首个 token 的生成时间 (TTFT) 和推理成本——无论你是使用自托管模型还是调用推理 API。我们说的不是小幅度的节省：例如使用 Claude Sonnet 时，缓存的输入 token 成本为 0.30 美元/百万 token，而未缓存的成本为 3 美元/百万 token——相差 10 倍。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0721/102411_9ZeU_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;从上下文工程的角度，提高 KV 缓存命中率涉及几个关键实践：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;1.保持你的提示前缀稳定&lt;/strong&gt;。由于 LLM 的自回归特性，即使是单个标记的差异也会使该标记之后的缓存失效。一个常见的错误是在系统提示的开头包含时间戳——尤其是精确到秒的时间戳。虽然这让模型能告诉你当前时间，但也会降低你的缓存命中率。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2.使你的上下文只追加&lt;/strong&gt;。避免修改之前的操作或观察。确保你的序列化是确定性的。许多编程语言和库在序列化 JSON 对象时不保证键顺序的稳定性，这可能会悄无声息地破坏缓存。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;3.在需要时明确标记缓存断点&lt;/strong&gt;。某些模型提供商或推理框架不支持自动增量前缀缓存，而是需要在上下文中手动插入缓存断点。在分配这些断点时，要考虑潜在的缓存过期问题，并至少确保断点包含系统提示的结尾。&lt;/p&gt; 
&lt;p&gt;此外，如果你正在使用像 vLLM 这样的框架自托管模型，请确保启用了前缀/提示缓存，并且你正在使用会话 ID 等技术在分布式工作节点之间一致地路由请求。&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;遮蔽，而非移除&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;随着代理能力的增强，其行动空间自然变得更加复杂——简单来说，工具数量爆炸式增长。最近流行的 MCP 只会火上浇油。如果你允许用户自定义工具，相信我：总会有人将数百个神秘工具插入到你精心策划的行动空间中。结果，模型更可能选择错误的行动或采取低效的路径。简而言之，你武装过度的代理变得更加愚蠢。&lt;/p&gt; 
&lt;p&gt;一个自然的反应是设计一个动态行动空间——可能是使用类似于 RAG 的方法按需加载工具。我们在 Manus 中也尝试过这种方法。但我们的实验表明了一个明确的规则：除非绝对必要，&lt;strong&gt;避免在迭代过程中动态添加或移除工具&lt;/strong&gt;。这主要有两个原因：&lt;/p&gt; 
&lt;p&gt;1.在大多数 LLM 中，工具定义在序列化后位于上下文的前部，通常在系统提示之前或之后。因此任何更改都会使后续所有动作和观察的 KV 缓存失效。&lt;/p&gt; 
&lt;p&gt;2.当先前的动作和观察仍然引用当前上下文中不再定义的工具时，模型会感到困惑。如果没有约束解码，&lt;strong&gt;这通常会导致模式违规或幻觉动作&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;为了解决这个问题并仍然改进动作选择，Manus 使用上下文感知的状态机来管理工具可用性。它不是移除工具，而是在解码过程中掩蔽 token 的 logits，以基于当前上下文阻止（或强制）选择某些动作。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0721/102429_jnnQ_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在实践中，大多数模型提供商和推理框架支持某种形式的&lt;strong&gt;响应预填充&lt;/strong&gt;，这允许你在不修改工具定义的情况下约束动作空间。函数调用通常有三种模式（我们将使用 NousResearch 的&amp;nbsp;Hermes 格式&amp;nbsp;作为示例）：&lt;/p&gt; 
&lt;p&gt;•自动&amp;nbsp;– 模型可以选择调用或不调用函数。通过仅预填充回复前缀实现：&lt;/p&gt; 
&lt;p&gt;&amp;lt;|im_start|&amp;gt;assistant&lt;/p&gt; 
&lt;p&gt;•必需&amp;nbsp;– 模型必须调用函数，但选择不受约束。通过预填充到工具调用令牌实现：&lt;/p&gt; 
&lt;p&gt;&amp;lt;|im_start|&amp;gt;assistant&amp;lt;tool_call&amp;gt;&lt;/p&gt; 
&lt;p&gt;•指定&amp;nbsp;– 模型必须从特定子集中调用函数。通过预填充到函数名称的开头实现：&amp;lt;|im_start|&amp;gt;assistant&amp;lt;tool_call&amp;gt;{"name": "browser_&lt;/p&gt; 
&lt;p&gt;通过这种方式，我们通过直接掩码 token 的 logits 来约束动作选择。例如，当用户提供新输入时，Manus 必须立即回复而不是执行动作。我们还有意设计了具有一致前缀的动作名称——例如，所有与浏览器相关的工具都以 browser_开头，命令行工具以 shell_开头。这使我们能够轻松确保代理在给定状态下只从特定工具组中进行选择而无需使用有状态的 logits 处理器。&lt;/p&gt; 
&lt;p&gt;这些设计有助于确保 Manus 代理循环保持稳定——即使在模型驱动的架构下。&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;使用文件系统作为上下文&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;现代前沿 LLM 现在提供 128K 令牌或更多的上下文窗口。但在真实世界的代理场景中，这通常不够，有时甚至是一种负担。有三个常见的痛点：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;1.观察结果可能非常庞大&lt;/strong&gt;，尤其是当代理与网页或 PDF 等非结构化数据交互时。很容易超出上下文限制。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2.模型性能往往会下降&lt;/strong&gt;，超过一定的上下文长度后，即使技术上支持该窗口大小。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;3.长输入成本高昂&lt;/strong&gt;，即使使用前缀缓存。你仍然需要为传输和预填充每个 token 付费。&lt;/p&gt; 
&lt;p&gt;为了解决这个问题，许多代理系统实现了上下文截断或压缩策略。但过度激进的压缩不可避免地导致信息丢失。这个问题是根本性的：代理本质上必须根据所有先前状态预测下一个动作——而你无法可靠地预测哪个观察结果可能在十步之后变得至关重要。从逻辑角度看，任何不可逆的压缩都带有风险。&lt;/p&gt; 
&lt;p&gt;这就是为什么我们在 Manus 中将文件系统视为终极上下文：大小不受限制，天然持久化，并且代理可以直接操作。模型学会按需写入和读取文件——不仅将文件系统用作存储，还用作结构化的外部记忆。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0721/102445_cdBw_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;我们的压缩策略始终设计为&lt;strong&gt;可恢复&lt;/strong&gt;的。例如，只要保留 URL，网页内容就可以从上下文中移除；如果沙盒中仍然保留文档路径，则可以省略文档内容。这使得 Manus 能够缩短上下文长度，而不会永久丢失信息。&lt;/p&gt; 
&lt;p&gt;在开发这个功能时，我发现自己在想象状态空间模型 (State Space Model, SSM) 在智能体环境中有效工作需要什么条件。与 Transformer 不同，SSM 缺乏完整的注意力机制，并且在处理长距离的后向依赖关系时表现不佳。但如果它们能够掌握基于文件的记忆——将长期状态外部化而不是保存在上下文中——那么它们的速度和效率可能会开启一类新型智能体。基于 SSM 的智能体可能是神经图灵机真正的继任者。&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;通过复述操控注意力&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;如果你使用过 Manus，你可能注意到一个有趣的现象：在处理复杂任务时，它倾向于创建一个 todo.md 文件——并在任务进行过程中逐步更新它，勾选已完成的项目。&lt;/p&gt; 
&lt;p&gt;这不仅仅是可爱的行为——这是一种&lt;strong&gt;操控注意力&lt;/strong&gt;的刻意机制。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0721/102503_Sccz_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Manus 中的一个典型任务平均需要大约 50 次工具调用。这是一个很长的循环——由于 Manus 依赖 LLM 进行决策，它很容易偏离主题或忘记早期目标，尤其是在长上下文或复杂任务中。&lt;/p&gt; 
&lt;p&gt;通过不断重写待办事项列表，Manus 将其目标复述到上下文的末尾。这将全局计划推入模型的近期注意力范围内，避免了"丢失在中间"的问题，并减少了目标不一致。实际上，它使用自然语言来使自己的注意力偏向任务目标——而不需要特殊的架构变更。&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;保留错误的内容&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;代理会犯错。这不是 bug——这是现实。语言模型会产生幻觉，环境会返回错误，外部工具会出现异常行为，意外的边缘情况随时都会出现。在多步骤任务中，失败不是例外；它是循环的一部分。&lt;/p&gt; 
&lt;p&gt;然而，一个常见的冲动是隐藏这些错误：清理痕迹，重试操作，或重置模型的状态并将其留给神奇的"温度"。这感觉更安全，更受控制。但这是有代价的：&lt;strong&gt;擦除失败会移除证据&lt;/strong&gt;。没有证据，模型就无法适应。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0721/102515_Ejog_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;根据我们的经验，改善代理行为最有效的方法之一出奇地简单：&lt;strong&gt;将错误的尝试保留在上下文中&lt;/strong&gt;。当模型看到一个失败的行动——以及由此产生的观察结果或堆栈跟踪——它会隐式地更新其内部信念。这会改变其先验，降低重复相同错误的可能性。 事实上，我们认为错误恢复是真正代理行为的最明显指标之一。然而，在大多数学术工作和公共基准测试中，这一点仍然代表性不足，它们通常关注理想条件下的任务成功。&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;不要被少样本示例所困&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;少样本提示是提高 LLM 输出的常用技术。但在代理系统中，它可能会以微妙的方式适得其反。&lt;/p&gt; 
&lt;p&gt;语言模型是优秀的模仿者；它们模仿上下文中的行为模式。如果你的上下文充满了类似的过去行动-观察对，模型将倾向于遵循该模式，即使这不再是最优的。&lt;/p&gt; 
&lt;p&gt;这在涉及重复决策或行动的任务中可能很危险。例如，当使用 Manus 帮助审查 20 份简历时，代理通常会陷入一种节奏——仅仅因为这是它在上下文中看到的，就重复类似的行动。这导致偏离、过度泛化，或有时产生幻觉。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0721/102529_Xy1G_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;解决方法是增加多样性。Manus 在行动和观察中引入少量的结构化变化——不同的序列化模板、替代性措辞、顺序或格式上的微小噪音。这种受控的随机性有助于打破模式并调整模型的注意力。&lt;/p&gt; 
&lt;p&gt;换句话说，&lt;strong&gt;不要让自己陷入少样本学习的窠臼&lt;/strong&gt;。你的上下文越单一，你的智能体就变得越脆弱。&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;结论&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;上下文工程仍然是一门新兴的科学——但对于智能体系统来说，它已经是必不可少的。模型可能变得更强大、更快速、更经济，但再多的原始能力也无法替代对记忆、环境和反馈的需求。你如何塑造上下文最终决定了你的智能体的行为方式：它运行的速度、恢复的效果以及扩展的范围。&lt;/p&gt; 
&lt;p&gt;在 Manus，我们通过反复的重写、死胡同以及面向数百万用户的实际测试学到了这些经验。我们在这里分享的内容并非放之四海而皆准的真理——但这些是对我们有效的模式。如果它们能帮助你避免哪怕一次痛苦的迭代，那么这篇文章就达到了它的目的。&lt;/p&gt; 
&lt;p&gt;智能体的未来将一次构建一个上下文。好好设计它们吧。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361386</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361386</guid>
      <pubDate>Thu, 17 Jul 2025 02:25:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>马斯克确认 Grok 未来将支持构建自定义 AI 伴侣</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;马斯克已确认，xAI 旗下 AI 聊天机器人 Grok 未来支持用户构建自定义 AI 伴侣，用户将能够创建拥有定制声音、外观和个性的数字伴侣（「每一个都会是独一无二的」）。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-3b409a325fcce4b0a222058e37fd50141e5.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;目前，Grok 已推出基于 Grok 4 大模型的「伴侣」（Companions）功能，首批上线了动漫风格角色 Ani（哥特风「AI 女友」）和卡通小熊猫 Bad Rudy，支持动态语音互动及角色外观自定义，用户可通过设置启用该功能。目前这项服务仅向每月支付 30 美元的 SuperGrok 订阅服务用户开放。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-5effcad31d7075014c476e238de5b315b9b.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;马斯克表示，这项功能仍处于「软启动」阶段，未来几天将简化启用流程，并逐步推出更多角色（如即将上线的男性角色 Chad 和 Valentine）以满足不同用户需求 。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361384</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361384</guid>
      <pubDate>Thu, 17 Jul 2025 02:17:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Meta 迎来顶尖人才：40% 曾在 OpenAI 任职，薪资高达 1 亿美元</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;Meta 近日正在积极扩展其人工智能团队，成立了名为 「&lt;span&gt;超级&lt;/span&gt;智能实验室」（Superintelligence Labs）的新部门，旨在推动基础模型的开发。据内部消息人士透露，该实验室目前已成功招募 44 名&lt;span&gt;顶尖&lt;/span&gt;人才，令人瞩目的是，约一半的员工来自中国，而 40% 的员工曾在 OpenAI 工作过。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;Meta 首席执行官马克・扎克伯格以其豪爽的投资风格而闻名，曾经投入 460 亿美元用于元宇宙项目，但由于未达到预期效果，现在他将重心转向人工智能领域。Meta 希望通过大规模的招聘行动来占领 AI 市场的制高点。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;近期，Meta 不仅成功从 OpenAI 和苹果等知名企业挖角，还在招募苹果基础模型负责人时开出了高达 2 亿美元的签约奖金。尽管如此，Meta 也并非每次都能达到这样的薪资水平，一些新员工的签约奖金并未达到 1 亿美元。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="383" src="https://oscimg.oschina.net/oscnet/up-f4808a66f8475012aeed9fa2aba51e71449.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在新招募的员工中，75% 拥有博士学位，70% 为研究人员，背景非常多元化。除了 50% 来自中国外，还有 40% 来自 OpenAI，20% 来自谷歌的 DeepMind，15% 来自 Scale 公司。这一人才结构的多样性将为 Meta 的 AI 研发注入新的活力。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;值得注意的是，这些新入职员工大多还未在 Meta 工作满一个月，年薪可能在 1000 万至 1 亿美元之间，具体数额尚未得到官方确认。这一招聘动态显示了 Meta 在 AI 领域的雄心，也意味着行业人才竞争的加剧。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361382</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361382</guid>
      <pubDate>Thu, 17 Jul 2025 02:05:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
  </channel>
</rss>
