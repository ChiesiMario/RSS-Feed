<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - news - 简体中文</title>
    <link>https://www.oschina.net/news/project</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news" rel="self" type="application/rss+xml"/>
    <description>已对该 RSS 进行格式化操作：中英字符之间插入空格、使用直角引号、标点符号修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-cn</language>
    <lastBuildDate>Fri, 04 Jul 2025 07:46:11 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>法国 AI 研究机构开源 Kyutai TTS，低延迟流式文本转语音技术</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;法国 AI 研究机构 Kyutai Labs 宣布开源其最新文本转语音（TTS）技术——Kyutai TTS，这是一个实时的语音生成解决方案。Kyutai TTS 以低延迟与高保真声音为亮点，支持文本流式传输，无需完整文本即可开始生成音频，特别适合实时交互场景。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1856" src="https://static.oschina.net/uploads/space/2025/0704/145146_pipR_2720166.png" width="3360" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fkyutai.org%2Fnext%2Ftts" target="_blank"&gt;https://kyutai.org/next/tts&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Kyutai TTS 在性能上表现卓越。使用单块 NVIDIA L40S GPU，该模型可同时处理 32 个请求，延迟仅为 350 毫秒。此外，系统不仅生成高质量音频，还能输出单词的精确时间戳，方便实时字幕生成或交互式应用，如 Unmute 平台的中断处理功能。&lt;/p&gt; 
&lt;p&gt;在语言支持与质量评估方面，Kyutai TTS 目前支持英语和法语，单词错误率（WER）分别为 2.82 和 3.29，展现出高准确度。说话者相似度达到 77.1%(英语) 和 78.7%(法语)，确保语音自然且接近原始样本。模型还能处理长篇文章，突破传统 TTS 的 30 秒限制，适合新闻、书籍等长篇内容生成。&lt;/p&gt; 
&lt;p&gt;Kyutai TTS 采用延迟流建模（DSM）架构，结合 Rust 服务器实现高效批处理，已在 GitHub 和 Hugging Face 开放源码与模型权重，助力全球开发者推动语音技术创新。&lt;/p&gt; 
&lt;p&gt;开源地址：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fkyutai-labs%2Fdelayed-streams-modeling" target="_blank"&gt;https://github.com/kyutai-labs/delayed-streams-modeling&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358755</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358755</guid>
      <pubDate>Fri, 04 Jul 2025 06:53:07 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>宇树科技或于科创板 IPO</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;据《每日经济新闻》从宇树科技相关投资方获悉，宇树科技后续有计划于科创板 IPO（首次公开募股）。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="264" src="https://oscimg.oschina.net/oscnet/up-b580657b265e86eea613b405eac902d810a.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;2025 年 5 月 29 日，宇树科技发布通知称，因公司发展需要，杭州宇树科技有限公司即日起名称变更为「杭州宇树科技股份有限公司」。彼时，有媒体报道称，宇树科技这一举动可视同完成股改。至于为何变更名称，外界认为或许是为了 IPO 铺路。而宇树科技曾回应，「这是公司运营方面的常规变更」。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;6 月份消息，宇树科技已完成了始于去年 9 月的 C 轮融资交割，由中国移动旗下基金、腾讯、锦秋、阿里、蚂蚁、吉利资本共同领投，绝大部分老股东跟投。融资金额接近 7 亿元人民币，投后估值超 120 亿元。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;宇树科技创始人兼 CEO 王兴兴还在夏季达沃斯论坛上透露，公司年度营收已超 10 亿元人民币。此前也有宇树科技投资人透露，自 2020 年起，宇树科技已连续 5 年实现盈利。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;span style="color:#000000"&gt;相关阅读：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li style="text-align:start"&gt;&lt;a href="https://www.oschina.net/news/352690" target="_blank"&gt;宇树科技回应更名 「股份有限公司」&lt;/a&gt;&lt;/li&gt; 
 &lt;li style="text-align:start"&gt;&lt;a href="https://www.oschina.net/news/356437" target="_blank"&gt;宇树科技确认：近期已完成 C 轮融资交割&lt;/a&gt;&lt;/li&gt; 
 &lt;li style="text-align:start"&gt; &lt;p style="margin-left:0px; margin-right:0px; text-align:start"&gt;&lt;a href="https://www.oschina.net/news/357324" target="_blank"&gt;宇树科技王兴兴：公司目前年度营收超过十亿元&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358747</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358747</guid>
      <pubDate>Sun, 11 May 2025 06:22:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>印度程序员用一份假简历骗了 5 家硅谷 AI 公司</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;AI 时代「最强」打工王者出现了。近期，一名叫 Soham Parekh 的印度程序员被曝通过一份假简历，在多家硅谷 AI 初创公司进行远程兼职，最多时同时打了五份工，领取多份工资，引发广泛关注。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-45b140ce1925eb09a92af71a5196c086da0.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Playground AI 的创始人 Sohail Doshi 最早发文爆料，称这名印度工程师靠着一份几乎 90% 造假的简历，同时在 3-5 家初创公司上班。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-4a368826a86d01bca7fba881f66b1122a59.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Sohail Doshi 表示，入职第一周就发现对方不对劲，于是立马开除，临走前还苦口婆心地劝对方别再骗人了，没想到这位印度工程师非但没收手，还越战越勇，继续多线作战。&lt;/p&gt; 
&lt;p&gt;后续，风投机构 YC 总裁 Garry Tan 也亲自下场发文表示，如果没有 YC 社区，这个人可能还在继续操作，甚至永远不会被发现。&lt;/p&gt; 
&lt;p&gt;据 Garry Tan 的说法，&lt;strong&gt;这位印度工程师专挑 YC 支持的创业公司下手，同时在至少三家由 YC 支持的创业公司打卡上班，最多甚至五家，并且每家公司都以为他是全职&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-627ac6534e25a6f42898e1b76863c6317df.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;据悉，Soham Parekh 虽然在简历和面试这两方面堪称完美，但在正式工作（或者工作试用期）真正开始的时候， 他会找一个又一个的借口，解释为什么缺席会议，或者为什么工作被推迟，亦或者他每天都会找个借口请假半天，比如说要见律师。后面，这些借口越来越荒谬，直到所有人开始意识到他明显在撒谎。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358729</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358729</guid>
      <pubDate>Sun, 11 May 2025 04:10:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>号码生成系统的创新实践：游戏周周乐幸运码设计</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                                        &lt;p&gt;作者： vivo 互联网服务器团队- Zhang Jing&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;本文以游戏周周乐的幸运码为切入点，针对其生成过程中涉及的随机性、唯一性及高并发等特点，设计了一种基于号段+子码的创新架构。该方案不仅在生成速度上表现突出，还显著提升了存储效率，同时降低了扩容成本，为类似的号码生成系统提供了设计上的新思路和启发。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;文章太长？1 分钟看图抓住核心观点👇&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-d0f19634e0555861f34c6d1a965332e5035.gif" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h1&gt;一、业务背景&lt;/h1&gt; 
&lt;p&gt;用户可通过完成相关任务获取周周乐幸运码，幸运码的投放规则如下：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;基础投放量&lt;/strong&gt;：每期 100 万注唯一无重复的 6 位数字幸运码&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;动态扩容机制&lt;/strong&gt;：参与人数超额时，可实时追加 100 万注&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;二、幸运码特性&lt;/h1&gt; 
&lt;p&gt;根据背景介绍，我们可以知道幸运码需要支持如下特性：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;随机性&lt;/strong&gt;，发给每个用户的幸运码都是随机的，同时每个用户领取的多个幸运码也是随机的。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;唯一性&lt;/strong&gt;，每一组的幸运码中，各幸运码都是唯一的。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;范围性&lt;/strong&gt;，幸运码严格限定在 000000 到 999999 区间内。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;高并发&lt;/strong&gt;，幸运码的生成和发放需要支持高并发，能够至少达到 300QPS。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;可追加&lt;/strong&gt;，在当期活动非常火爆时，需要可临时追加一组幸运码库存。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;三、方案选型&lt;/h1&gt; 
&lt;p&gt;鉴于幸运码需严格限定在 6 位数字范围内（000000-999999），传统雪花算法因生成超长 ID（64 位二进制）且依赖时间戳递增特性，难以直接适配。以下将对比三种方案：实时随机生成模式、预生成库存模式及号段+子码模式，并会根据生成速度、存储效率、扩容成本三个核心指标进行系统性评估，最终选择出最优解决方案。&lt;/p&gt; 
&lt;h2&gt;3.1 方案一：实时随机生成模式&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;实现逻辑：&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;生成随机数&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;再查询数据库是否有该随机数&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;若没有则入库，完成幸运码发放&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;若有再重新执行第一步&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;缺陷分析：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;碰撞概率随库存消耗不断上升&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;数据库 IO 压力随并发量线性增长&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;不满足高并发场景性能要求&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;3.2 方案二：预生成库存模式&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;实现逻辑：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;采用预生成幸运码方式：离线生成 100 万个幸运码，随机打散，写入数据库，每个幸运码对应一个从 1 自增的序列号，并使用 Redis 记录幸运码序列号索引，初始值为 1。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;发放步骤&lt;/strong&gt;如下：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;从 Redis 查询幸运码序列号索引&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;使用该索引查询幸运码，并完成幸运码发放&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;递增 Redis 的序列号索引，确保序列号索引关联的幸运码是下一个可发放的幸运码&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;缺陷分析：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;存储空间浪费&lt;/strong&gt;：未发放号码占用存储&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;扩容效率低下&lt;/strong&gt;：追加库存需重新预生成&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;3.3 方案三：号段+子码模式&lt;/h2&gt; 
&lt;p&gt;采用号段+子码机制：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;号段管理&lt;/strong&gt;：将 10^6 号码划分为 1000 个号段（号段值：0-999）&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;子码管理&lt;/strong&gt;：每个号段维护 1000 个可用子码（子码值：0-999）&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;生成规则&lt;/strong&gt;：幸运码=随机号段*1000+随机子码（示例：129358=129*1000+358）&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-45d325576bad2645b81b131cf8277e2e471.jpg" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h2&gt;3.4 方案对比&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-079c02eb66e15c161b0b30001341fb6548d.jpg" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;综上，我们综合幸运码生成速度、存储效率、扩容成本等指标，最终采用了号段+子码模式来生成幸运码。&lt;/p&gt; 
&lt;h1&gt;四、关键技术实现&lt;/h1&gt; 
&lt;h2&gt;4.1 号段分层机制&lt;/h2&gt; 
&lt;p&gt;将 100 万注幸运码划分为 1000 个号段（每段 1000 注），每个号段由两部分组成：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;号段 ID&lt;/strong&gt;：号段 ID 为唯一且不重复的整数，范围介于 0 到 999 之间。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;子码串&lt;/strong&gt;：1000 位字符串，采用"01"标记使用状态，0 表示未使用，1 表示已使用，初始全 0。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;幸运码生成公式：&lt;/p&gt; 
&lt;p&gt;幸运码 = 号段 ID * 1000 + 子码位置&lt;/p&gt; 
&lt;p&gt;该设计既保留了生成幸运码的随机性（号段 ID 随机+子码随机），又通过子码的类比特位存储方式提升了存储效率。&lt;/p&gt; 
&lt;h2&gt;4.2 分布式并发控制&lt;/h2&gt; 
&lt;h3&gt;4.2.1 多级缓存策略&lt;/h3&gt; 
&lt;p&gt;Redis 存储可用号段集合，若号段的子码使用完，该号段会从 Redis 集合中剔除，同时本地缓存也会预加载可用号段，确保发码时能更高效地获取候选号段。&lt;/p&gt; 
&lt;h3&gt;4.2.2 高效锁抢占策略&lt;/h3&gt; 
&lt;p&gt;系统为每个号段分配了分布式锁，当执行发放幸运码时，会从本地缓存随机获取 15 个候选号段。然后在遍历获取号段时，将等待锁的超时时间设置成 30ms，确保号段被占用的情况下能够快速遍历到下一个号段（根据实际场景统计，等待锁的情况很少发生，一般最多遍历到第二个号段即可成功抢占）。一旦成功获得号段的分布式锁后，便可进一步随机获取该号段下的可用子码。&lt;/p&gt; 
&lt;h3&gt;4.2.3 动态库存策略&lt;/h3&gt; 
&lt;p&gt;要追加库存，只需再创建一组幸运码号段，并写入 Redis，后续发放时获取该组的可用号段生成幸运码即可。从性能和存储空间上远优于预生成方式。&lt;/p&gt; 
&lt;h2&gt;4.3 幸运码发放&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;发放步骤&lt;/strong&gt;：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;随机获取至多 15 个可用号段&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;遍历号段&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;抢占号段的分布式锁&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;若号段的分布式锁抢占成功，则随机获取号段中可用的子码，再根据号段和子码生成幸运码&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;若号段的分布式锁抢占失败，则遍历下一个号段，并重复上述步骤&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-18b4f57ed57a5e892f4084f7f66b15e177f.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h1&gt;五、总结&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;（1）双重随机保障&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;一级随机：号段选择随机（0-999）&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;二级随机：子码选择随机（0-999）&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;通过号段随机和子码随机方式确保生成的幸运码完全随机&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;（2）数据唯一性&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;通过号段唯一和号段内的子码唯一确保生成的幸运码全局唯一&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;（3）弹性扩展能力&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;扩容耗时仅需秒级别&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;存储空间相比预生成方案节省 80%&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;（4）高性能发放&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;通过多重缓存及高效号段抢占策略大幅提升幸运码生成效率&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;实测 QPS&amp;gt;300，平均响应时间&amp;lt;15ms&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;本设计方案通过创新的号段+子码管理机制，在保证号码随机性和唯一性的同时，实现了高并发场景下的稳定服务能力，为类似号码生成系统的设计提供了可复用的架构范式。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/vivotech/blog/18683260</link>
      <guid isPermaLink="false">https://my.oschina.net/vivotech/blog/18683260</guid>
      <pubDate>Sun, 11 May 2025 03:44:00 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>​前 OpenAI 研究员揭露：签约 Meta 并未获 1 亿美元奖金</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;近期，一位前 OpenAI 研究员的言论引发了广泛关注。他表示，尽管 Meta 公司在挖角 OpenAI 的科研人才时声称提供高达 1 亿美元的签约奖金，但他和他的同事并未收到这笔奖金。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="386" src="https://oscimg.oschina.net/oscnet/up-48f34a74f8c7850e98ec9150ffd25671085.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;这位研究员名叫卢卡斯・贝耶尔（Lucas Beyer），他和同事亚历山大・科列斯尼科夫 (Alexander Kolesnikov)、翟晓华 (Xiaohua Zhai) 在去年 11 月加入 OpenAI，并共同设立了该公司在苏黎世的办公室。根据《商业内幕》的报道，这三位前 Google DeepMind 的研究科学家近期选择离开 OpenAI，加入了 Meta。贝耶尔在社交媒体上公开表示，他和同事并没有获得 Meta 所宣传的 1 亿美元签约奖金，这一消息引发了网友们的热议。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在贝耶尔的社交媒体帖子下，许多人对此表示同情，认为他在职业生涯中作出了重要的选择，但同时也有网友认为他选择了离开 OpenAI 并获得了相应的经济补偿，是他应得的结果。与此同时，Meta 也在加速发展，并向数据标注公司 Scale AI 投资了 150 亿美元。预计该公司的创始人兼首席执行官 Alexandr Wang 将辞职加入 Meta，与贝耶尔等人一起合作研发&lt;span&gt;超级&lt;/span&gt;智能。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;对此，OpenAI 的现任研究人员可能会感到一丝宽慰，因为尽管 Meta 多次试图挖走他们，但目前尚未有员工接受这份工作。OpenAI 首席执行官 Sam Altman 在一次节目中提到，Meta 正在积极招聘，希望从其他 AI 公司中挖走优秀人才，但他认为这种方式并不会创造出良好的企业文化。他同时也表达了对 Meta 在创新能力上的一些保留态度。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;至今，Meta 尚未对此事作出任何正式回应，而贝耶尔的声明则给人们带来了更多的思考。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358715</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358715</guid>
      <pubDate>Sun, 11 May 2025 03:16:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Ollama v0.9.5 发布：支持跨平台网络共享、性能优化升级</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;开源本地大模型运行工具 Ollama 发布了 v0.9.5 版本，带来了跨平台网络共享功能、灵活的模型目录管理以及 macOS 端的原生化改进和性能优化，极大提升了用户体验和应用价值。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-0612ff6688c843662d35acf99da9484e2db.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;功能改进&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;跨平台网络共享功能&lt;/strong&gt; ：首次引入 「Expose Ollama on the network」，允许用户将运行在一台设备上的 Ollama 实例，通过 LAN 局域网甚至互联网暴露给其他设备使用。比如在性能强大的 Mac、PC 或 Linux 服务器上运行 Ollama，让配置较弱的笔记本、平板甚至智能手机远程调用模型服务，还可实现团队内部模型资源共享，提升协同开发效率。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;灵活的模型目录管理&lt;/strong&gt; ：用户可将模型存储至外部硬盘或其他位置，打破此前版本模型存储路径固定的限制，保障磁盘空间与管理便捷性。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;macOS 端原生化改进和性能优化&lt;/strong&gt; ：实现原生应用转型，区别于以往基于跨平台框架打包的版本，原生 app 启动流程经优化后启动时间显著缩短，仅包含必需的系统组件使安装体积变小，且支持 macOS 原生通知、快捷键、深色模式等功能无缝结合。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;已解决的问题&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;macOS 命令行工具未安装问题&lt;/strong&gt; ：解决了 macOS 启动过程中 CLI 未被顺利安装的问题，确保开发者及高级用户通过终端调用时的连贯体验。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ollama-darwin.tgz 文件数字签名问题&lt;/strong&gt; ：修复了某些文件未进行苹果开发者认证签名，避免 macOS 的安全警告与功能限制。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;增加社区集成支持&lt;/strong&gt; ：引入 NativeMind 作为社区集成组件，拓展生态功能性。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;下载地址：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Follama%2Follama%2Freleases%2Ftag%2Fv0.9.5" target="_blank"&gt;https://github.com/ollama/ollama/releases/tag/v0.9.5&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358712/ollama-0-9-5</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358712/ollama-0-9-5</guid>
      <pubDate>Sun, 11 May 2025 02:55:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>LiblibAI 正式推出 Lovart 国内版本「星流 Agent」</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;7 月 3 日，LiblibAI 正式&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FMRJOdgJZaYL2Ocn8fnc5HQ"&gt;推出&lt;/a&gt;Lovart 国内版本「星流 Agent」，定位为一款面向中文创作者的智能设计拍档。产品延续 Lovart 海外版本的核心能力，支持自然语言生成整套设计物料，包含主图、海报、社媒封面、视频动画及 3D 模型等。系统已接入十余个主流大模型，支持图像、视频、声音、3D 的一站式生成与导出。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-b48e5a2b164ddb85b9eab30d0fb019dfa69.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;据介绍，星流 Agent 的核心能力包括全流程自动化设计，用户输入简单需求后，Agent 能自动拆解任务、确定风格并生成包括主图、延展图、社交媒体封面在内的全套视觉物料。它还引入了「无边画布」和智能协作编辑功能，支持在画布内进行多轮对话式改图、修图、换图和调整构图。&lt;/p&gt; 
&lt;p&gt;此外，星流 Agent 背后接入了 F.1、Kling、Qwen 等十多个顶尖模型，能自动调用最合适的模型组合，一站式生成图像、视频、声音、3D 等多种模态内容，并支持多种格式导出。&lt;/p&gt; 
&lt;p&gt;目前星流 Agent 已在 PC 端及移动端同步上线。&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.xingliu.art%2F%C2%A0"&gt;https://www.xingliu.art/&amp;nbsp;&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358708</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358708</guid>
      <pubDate>Sun, 11 May 2025 02:45:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>AI 代码编辑器 Cursor 发布 1.2：新增 Agent Planning、Memories 正式 GA</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;AI 代码编辑器 Cursor 发布了 1.2 版本，带来了多项功能增强。&lt;/p&gt; 
&lt;p&gt;新版本引入了 Agent Planning 功能，通过结构化的待办事项列表（To-do lists）帮助 Agent 更好地规划和执行长时程任务。用户现在可以为 Agent 排队发送后续指令，无需等待当前任务完成。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0704/103205_n4EJ_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;此外，"Memories"功能正式 GA，改进了记忆生成质量和 UI。代码库搜索功能通过新的嵌入模型和优化的提示词变得更加准确，同时新增了对 PR、issue、commit 和分支的语义搜索和上下文提取能力。Tab 代码补全速度提升了约 100 毫秒，Agent 也具备了解决合并冲突的能力。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0704/103244_iVp5_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;同时，Cursor 的 CEO 在官方论坛发文，澄清了 6 月 16 日对 Pro 套餐的调整。新方案从请求次数限制改为算力限制，Pro 用户每月可获得至少等值 20 美元 API 价格的模型推理额度，并取消了 Agent 的工具调用次数限制。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0704/103432_ObdB_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;相关阅读：&lt;a href="https://www.oschina.net/news/355977" target="news"&gt;Cursor 推出月费 200 美元的 Ultra 计划，Pro 计划将更新为「不限量但有速率限制」的模式&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;官方文档中还提到了新的 Pro+（60 美元/月，3 倍额度）和 Ultra（200 美元/月，20 倍额度）套餐。尽管官方做出了澄清，但部分用户仍在论坛表示新定价模型透明度不足。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;相关来源&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcursor.com%2Fcn%2Fchangelog%2F1-2" target="_blank"&gt;https://cursor.com/cn/changelog/1-2&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fforum.cursor.com%2Ft%2Fclarifying-june-16-pro-changes%2F111740" target="_blank"&gt;https://forum.cursor.com/t/clarifying-june-16-pro-changes/111740&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358702/cursor-1-2</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358702/cursor-1-2</guid>
      <pubDate>Sun, 11 May 2025 02:33:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>昆仑万维开源第二代奖励模型 Skywork-Reward-V2 系列</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;昆仑万维&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fcjv5V_PSZSYObiD9b8VwHA" target="_blank"&gt;宣布&lt;/a&gt;继续开源第二代奖励模型（Reward Model）Skywork-Reward-V2 系列，共包含 8 个基于不同基座模型和不同大小的奖励模型，参数规模从 6 亿到 80 亿不等，其在七大主流奖励模型评测榜单中全面夺魁。在 2024 年 9 月，昆仑万维曾首次开源了 Skywork-Reward 系列模型及相关数据集。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;公告称，在打造这一新一代奖励模型的过程中，昆仑万维方面构建了一个包含总共 4000 万对偏好对比的混合数据集 Skywork-SynPref-40M。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;为实现大规模、高效的数据筛选与过滤，特别设计了人机协同的两阶段流程，将人工标注的高质量与模型的规模化处理能力相结合。在这一流程中，人类提供经过严格验证的高质量标注，大型语言模型（LLMs）则根据人工指导进行自动整理和扩充。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;基于上述优质的混合偏好数据开发了 Skywork-Reward-V2 系列，其展现了广泛的适用性，在多个能力维度上表现出色，包括对人类偏好的通用对齐、客观正确性、安全性、风格偏差的抵抗能力，以及 best-of-N 扩展能力。经实验验证，该系列模型在七个主流奖励模型评测基准上均获得最佳表现。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="166" src="https://oscimg.oschina.net/oscnet/up-aba612d774aa51bd830ebc7d8a86ace93bd.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;相比上一代 Skywork-Reward，昆仑万维全新发布的 Skywork-Reward-V2 系列提供了基于 Qwen3 和 LLaMA3 系列模型训练的 8 个奖励模型，参数规模覆盖从 6 亿至 80 亿。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="283" src="https://oscimg.oschina.net/oscnet/up-f62f4cf74e27a339b3b803de36d10cac23e.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;即便基于最小模型 Skywork-Reward-V2-Qwen3-0.6B，其整体性能已几乎达到上一代最强模型 Skywork-Reward-Gemma-2-27B-v0.2 的平均水平。更进一步，Skywork-Reward-V2-Qwen3-1.7B 在平均性能上已超越当前开源奖励模型的 SOTA——INF-ORM-Llama3.1-70B。而最大规模的 Skywork-Reward-V2-Llama-3.1-8B。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="261" src="https://oscimg.oschina.net/oscnet/up-3f53bbb3373c196b2c3a4cab3dd05e6faf8.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;此外，Skywork-Reward-V2 在多项高级能力评估中均取得领先成绩：包括 Best-of-N(BoN) 任务、偏见抵抗能力测试（RM-Bench）、复杂指令理解及真实性判断（RewardBench v2），展现了出色的泛化能力与实用性。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="116" src="https://oscimg.oschina.net/oscnet/up-2a48408860f058ccd51f588fdb4c87c35ae.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="323" src="https://oscimg.oschina.net/oscnet/up-85c3640ee15c11793833144e683ec2270db.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;Skywork-Reward-V2 系列模型专注于对偏好数据规模扩展的研究，昆仑万维方面表示，其团队也将研究辐射面陆续转向其他尚未被充分探索的领域，例如替代训练技术与建模目标。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;更多详情可&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fcjv5V_PSZSYObiD9b8VwHA" target="_blank"&gt;查看官方公告&lt;/a&gt;。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358700</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358700</guid>
      <pubDate>Sun, 11 May 2025 02:23:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>全球 AI 人才榜单首次曝光，华人撑起半边天</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;7 月 3 日，2025 全球数字经济大会上，一份重磅榜单面向全球首次揭晓。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;该榜单基于近十年近 10 万篇文献深度分析，列出了全球人工智能领域的 Top100 人才&lt;/strong&gt;。其中，华人依旧拿下了主力席位。&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FsDvBk1WCxUrJqMOu2Q9p6g" target="_blank"&gt;凤凰网科技从中提炼了出了一份较为瞩目的人员名单&lt;/a&gt;，这些人现多数就职于国内外企业，仍在人工智能前沿探索领域活跃，包括：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;何恺明&lt;/strong&gt;，曾任职于 AI 殿堂美国麻省理工学院（MIT），作为 ResNet 之父，是深度学习革命的关键推手，其提出的残差学习（Residual Learning）概念，一举攻破了困扰神经网络多年的「梯度消失」难题，让深达千层的网络训练成为可能。其论文引用数以骇人的数十万次傲视群雄（公开数据约 40 万 +），被誉为「CV 界的诺奖级工作」。6 月 26 日，何恺明刚刚官宣入职 GoogleDeepMind，担任杰出科学家，同时还保留了 MIT 终身副教授身份。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/194046_NzCP_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;张祥雨&lt;/strong&gt;，曾是旷视研究院的顶梁柱，现已入职阶跃星辰任首席科学家； 2016 年，以 ResNet（作为核心贡献者之一）问鼎 CVPR 最佳论文，震动世界！随后在 ImageNet、COCO 等视觉「奥林匹克」赛场多次屠榜。其与团队开创的 ResNet、ShuffleNet 系列影响颇深，Google Scholar 引用逾 40,000 次，是无数手机、摄像头、自动驾驶系统的「核心引擎」。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/194104_my7v_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;任少卿&lt;/strong&gt;，蔚来汽车自动驾驶的灵魂人物，计算机视觉与自动驾驶融合领域的顶尖专家，曾发表多篇极具影响力的 CV 顶会论文。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/194115_XBIO_2720166.png" referrerpolicy="no-referrer"&gt;&lt;img alt="" src="https://img.ithome.com/newsuploadfiles/2025/7/9ccdcd47-cf38-495f-a15c-11c7758e41da.png?x-bce-process=image/format,f_avif" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;田奇&lt;/strong&gt;，华为在人工智能领域的重要人物，也是华为计算产品线（升腾 AI 处理器等）和 MindSpore 框架背后的核心操盘手。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/194127_1K5b_2720166.png" referrerpolicy="no-referrer"&gt;&lt;img alt="" src="https://img.ithome.com/newsuploadfiles/2025/7/06d18e54-babe-4198-a0f2-f0ed8e8fc7d2.png?x-bce-process=image/format,f_avif" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;王云鹤&lt;/strong&gt;，华为诺亚方舟实验室的研究员。专注于 AI 基础模型、神经网络架构搜索（NAS）、模型轻量化等前沿方向，是华为 AI「顶天」（前沿）又「立地」（落地）战略的重要实践者。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/194139_3XP0_2720166.png" referrerpolicy="no-referrer"&gt;&lt;img alt="" src="https://img.ithome.com/newsuploadfiles/2025/7/3e5805f0-1b78-44bb-ba08-0ba2b5e6c899.png?x-bce-process=image/format,f_avif" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;谢凌曦&lt;/strong&gt;，华为天才少年，在计算机视觉，特别是视觉大模型、自监督学习、对抗鲁棒性等领域有系列开创性工作。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/194149_Jxf5_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://img.ithome.com/newsuploadfiles/2025/7/85b5a95b-03b3-4936-96f3-fecdbf8a954a.png?x-bce-process=image/format,f_avif" referrerpolicy="no-referrer"&gt;&lt;strong&gt;王晓刚&lt;/strong&gt;，商汤科技联合创始人、核心技术奠基人之一。在商汤，他主导了核心视觉算法框架的构建。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/194158_juQG_2720166.png" referrerpolicy="no-referrer"&gt;&lt;img alt="" src="https://img.ithome.com/newsuploadfiles/2025/7/b2340c80-9b01-4125-9c86-035e2353db25.png?x-bce-process=image/format,f_avif" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;石建萍&lt;/strong&gt;，商汤科技自动驾驶研发团队的领军女将。带领团队在智能驾驶视觉感知、多传感器融合、高精定位等方面取得突破性进展。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/194208_Tj3r_2720166.png" referrerpolicy="no-referrer"&gt;&lt;img alt="" src="https://img.ithome.com/newsuploadfiles/2025/7/b0c91622-96b2-4f33-be72-40b2b4802389.png?x-bce-process=image/format,f_avif" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;闫俊杰&lt;/strong&gt;，MiniMax 创始人，国内最早一批成立的大模型公司核心人物。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/194219_yNU3_2720166.png" referrerpolicy="no-referrer"&gt;&lt;img alt="" src="https://img.ithome.com/newsuploadfiles/2025/7/6b21b373-3805-435f-8f02-df120995bb9a.png?x-bce-process=image/format,f_avif" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;曹越&lt;/strong&gt;，前微软杰出工程师，现 Sand.AI 创始人兼 CEO。专注打造下一代 AI 智能体（AI Agent）平台。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/194228_wqU6_2720166.png" referrerpolicy="no-referrer"&gt;&lt;img alt="" src="https://img.ithome.com/newsuploadfiles/2025/7/f1b580d1-bd41-490b-9080-e4bdc8effc1e.png?x-bce-process=image/format,f_avif" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;陶大程&lt;/strong&gt;，新加坡南洋理工大学（NTU）协理副校长、澳大利亚桂冠教授，视觉大牛，IEEE / AAAS / ACM 三料 Fellow，曾任京东探索研究院院长（2023 年卸任）。研究横跨计算机视觉、机器学习、统计学习、可信 AI（鲁棒性、可解释性、公平性）等核心领域。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/194240_wWsg_2720166.png" referrerpolicy="no-referrer"&gt;&lt;img alt="" src="https://img.ithome.com/newsuploadfiles/2025/7/cb439f78-abc0-4ebb-a823-9121a7ada5a5.png?x-bce-process=image/format,f_avif" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;刘子纬&lt;/strong&gt;，新加坡南洋理工大学（NTU）新锐实力派教授，学术明星。在视觉-语言理解（VLP）、多模态大模型、信息检索方向成果斐然。其工作多次发表于 CVPR, ICCV, NeurIPS 等顶会，并常居排行榜前列，是推动多模态预训练模型发展与应用的重要力量。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/194307_cOI1_2720166.png" referrerpolicy="no-referrer"&gt;&lt;img alt="" src="https://img.ithome.com/newsuploadfiles/2025/7/e10b6eaa-b533-4846-ac3d-1688c5e7ca50.png?x-bce-process=image/format,f_avif" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;贾佳亚&lt;/strong&gt;，香港中文大学计算机科学与工程系教授，思谋科技创始人。著名计算机视觉学者，专注于底层视觉重建、图像增强、图像分割、医学图像分析等方向。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/194322_DDlL_2720166.png" referrerpolicy="no-referrer"&gt;&lt;img alt="" src="https://img.ithome.com/newsuploadfiles/2025/7/f17121d8-c2e7-4ff0-9203-e1743fc8674f.png?x-bce-process=image/format,f_avif" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;杨明玄&lt;/strong&gt;，美国加州大学默塞德分校（UC Merced）计算机科学教授,Google DeepMind 研究员。深耕机器学习和数据挖掘领域，特别是在图神经网络（GNN）、推荐系统、社交网络分析等方面有突出建树。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/194332_aIW0_2720166.png" referrerpolicy="no-referrer"&gt;&lt;img alt="" src="https://img.ithome.com/newsuploadfiles/2025/7/5c0ee9b8-ae73-4c39-b4ad-9ba80f95187f.png?x-bce-process=image/format,f_avif" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;刘威&lt;/strong&gt;，前腾讯混元大模型技术负责人之一，腾讯杰出科学家，2025 年初离职创业。2024 年领导开源混元文生图模型、3D 生成模型「Hunyuan3D-1.0」，推动腾讯内部 700 + 业务接入 AI 能力（如微信输入法、腾讯会议）。发表顶会论文 100 + 篇，总引用 3600 + 次，获 CVPR 青年研究者奖、SIGIR 最佳论文荣誉奖。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/194344_6WyQ_2720166.png" referrerpolicy="no-referrer"&gt;&lt;img alt="" src="https://img.ithome.com/newsuploadfiles/2025/7/44dd9265-ca1d-4db3-8e44-70ee47565a3a.png?x-bce-process=image/format,f_avif" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;颜水成&lt;/strong&gt;，新加坡国立大学终身教授，培养 50 + 名博士，建立亚太最大计算机视觉实验室，历任 360 首席科学家、依图 CTO、Sea 集团 AI Lab 主任；2023 年加入昆仑万维任 2050 全球研究院院长，2024 年底卸任。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/194359_j5g6_2720166.png" referrerpolicy="no-referrer"&gt;&lt;img alt="" src="https://img.ithome.com/newsuploadfiles/2025/7/496764e9-1b16-47dd-a8e1-3349844dd95d.png?x-bce-process=image/format,f_avif" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;据东壁科技数据创始人、深圳大学特聘教授吴登生介绍，该报告基于东壁全球科技文献数据平台（dbdata.com），对全球 AI 研究生态进行系统分析，数据集涵盖近 20 万名来自 175 个国家和地区、3847 个机构的学者，时间跨度为 2015 至 2024 年。&lt;/p&gt; 
&lt;p&gt;值得一提的是，何恺明、刘子纬、王晓刚、陶大程均师承中国人工智能先驱、商汤科技创始人汤晓鸥，张祥雨曾师从何恺明。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358641</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358641</guid>
      <pubDate>Sat, 10 May 2025 11:45:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Bytebase 3.8.0 - 显著优化 schema 同步 / 回滚兼容性</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                                        &lt;h1&gt;🔔 重大变更&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;显著优化多数据库（MySQL/PostgreSQL/TiDB/SQL Server/Oracle）的 schema 同步/回滚兼容性，支持绝大多数常见数据库对象。 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.bytebase.com%2Fchange-database%2Fsynchronize-schema%23supported-objects" target="_blank"&gt;文档地址&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-21a7f23efb479d2c33408917a613e8bb91b.png" alt="file" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;下线工单订阅功能。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;将 SQL 审核中心更名为变更计划。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;🎄 改进&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;新增查询结果行数限制功能。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-cad54dad1deaf4e15ad8f12933c09c1609c.png" alt="file" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;新增多域名配置支持。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-4380358be992a487045a8f39061a6ba967e.png" alt="file" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;简化默认值输入：不再区分表达式和值，统一通过文本输入框填写。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;在审批流程处展示全部审批人，且可悬停显示细节。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-5c8191a2cf70d8536b5ec930d9fca7b36e2.png" alt="file" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h1&gt;🐞 Bug 修复&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;修复了 ClickHouse 查询中的 JSON 数据类型显示问题。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;在 Terraform SSL 配置中新增 &lt;code&gt;use_ssl&lt;/code&gt; 字段。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;📕 安装及升级&lt;/h1&gt; 
&lt;p&gt;新安装 &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.bytebase.com%2Fget-started%2Fself-host" target="_blank"&gt;https://docs.bytebase.com/get-started/self-host&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;升级 &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.bytebase.com%2Fget-started%2Fupgrade" target="_blank"&gt;https://docs.bytebase.com/get-started/upgrade&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;strong&gt;升级前请备份元数据库，升级后无法回退版本。&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;💡 更多资讯，请关注 Bytebase 公号：Bytebase&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/6148470/blog/18683355</link>
      <guid isPermaLink="false">https://my.oschina.net/u/6148470/blog/18683355</guid>
      <pubDate>Sat, 10 May 2025 10:24:00 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>Perplexity 上线月费 200 美元的「Max」订阅服务</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Perplexity 公司推出了&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.perplexity.ai%2Fhub%2Fblog%2Fintroducing-perplexity-max" target="_blank"&gt; Perplexity Max &lt;/a&gt;订阅服务，月费为 200 美元（约合 1433 元人民币），可以享受诸多权益。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-1abe17d787b691e146a089eccf4834806a9.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;购买 Perplexity Max 订阅计划的用户，可以无限制访问电子表格和报告生成工具 Labs，并支持提前体验 Comet 浏览器在内的诸多新功能。&lt;/p&gt; 
&lt;p&gt;该公司表示，Perplexity Max 是为以下用户设计的：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;需要无限访问全面分析工具的专业人士&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;需要大量研究能力的内容创作者和作家&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;进行竞争情报和市场研究的商业策略师&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;从事复杂、多方面项目的学术研究人员&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Max 用户还支持在 Perplexity 服务中，调用 OpenAI 的 o3-pro 和 Claude Opus 4 等先进 AI 模型。随着 Max 的推出，Perplexity 成为最新一家推出超高端订阅服务层的 AI 提供商。&lt;/p&gt; 
&lt;p&gt;OpenAI 是首家推出每月 200 美元的 ChatGPT Pro 订阅服务的公司，但近几个月来，Google、Anthropic 和 Cursor 也纷纷效仿。&lt;/p&gt; 
&lt;p&gt;Perplexity 目前提供多种订阅计划。除了每月 200 美元的 Max 计划外，还提供每月 20 美元的消费者 Pro 计划，以及每人每月 40 美元的企业 Pro 计划。该公司表示，最终也将为企业客户推出超高端的 Max 计划。&lt;/p&gt; 
&lt;p&gt;Perplexity 在 2024 年主要依靠每月 20 美元的 Pro 计划订阅，实现了大约 3400 万美元的收入，但据 The Information 看到的财务数据，公司仍烧掉了约 6500 万美元现金。&lt;/p&gt; 
&lt;p&gt;据报道，Perplexity 的现金消耗主要来自对云服务器的重金投入以及购买 OpenAI 和 Anthropic 的 AI 模型访问权。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;相关文档：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.perplexity.ai%2Fhelp-center%2Fen%2Farticles%2F11680686-perplexity-max" target="_blank"&gt;https://www.perplexity.ai/help-center/en/articles/11680686-perplexity-max&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358621/perplexity-max</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358621/perplexity-max</guid>
      <pubDate>Sat, 10 May 2025 09:14:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>X 平台（原 Twitter）上线 AI 笔记功能</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;X 平台（原 Twitter）&lt;u&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2FCommunityNotes%2Fstatus%2F1940132205486915917" target="_blank"&gt;宣布&lt;/a&gt;&lt;/u&gt;正式启动 AI Note Writer 的 API 试点计划，允许全球开发者创建 AI 机器人来撰写社区内容。未来，这些由 AI 撰写的笔记如果被一定数量的用户认为有帮助，就可以在平台上公开展示。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-b9141239b889f5cfe9e01d79314f39bdc80.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;X 表示，该计划旨在加速社区内容的产出效率，同时通过社区反馈机制，不断优化 AI 的准确性、公正性与实用性。AI 笔记将遵循与人工内容相同的审核准则，并会在介面中明确标注。&lt;/p&gt; 
&lt;p&gt;首批 AI Note Writer 将在本月内开放上线，且将逐步开放。&lt;/p&gt; 
&lt;p&gt;详情查看文档：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcommunitynotes.x.com%2Fguide%2Fen%2Fapi%2Foverview" target="_blank"&gt;https://communitynotes.x.com/guide/en/api/overview&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358606/ai-note-writer-api</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358606/ai-note-writer-api</guid>
      <pubDate>Sat, 10 May 2025 08:35:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>毕马威：医疗大模型中国发布数量占全球七成</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;毕马威中国发布《首届健康科技 50》报告指出，医疗大模型目前主要分为五类，包括大型语言模型 (LLM)、语言条件多智能体大型、多模态大模型、图学习大模型、视觉语言大模型。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在全球范围内，据不完全统计，在已发布的医疗大模型中，按模型类别来看，大语言模型数量最多，占比近 65%；按分布海内外发表数量来看，中国发布数量占比超 70%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="265" src="https://oscimg.oschina.net/oscnet/up-0f5af0008879ca0ee50480307bd0d62db20.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;此外报告指出，2024 年，中国医疗科技市场规模突破百亿，达到 102.5 亿元，同比增长 75.3%。2025-2027 年，中国医疗科技的增幅预计会有所放缓，但行业总市场规模仍呈现稳健增长趋势。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;2023 年中国医疗机器人市场规模达到约 108 亿元，近五年年均复合增长率高达 25.74%。中国智能医疗器械市场增长迅猛，预计 2025 年将达 242.3 亿元，2026 年-2027 年总体有望保持较高速增长。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;更多详情可&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fassets.kpmg.com%2Fcontent%2Fdam%2Fkpmg%2Fcn%2Fpdf%2Fzh%2F2025%2F07%2Fkpmg-china-healthcare-health-tech-50.pdf" target="_blank"&gt;查看完整报告&lt;/a&gt;。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358603</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358603</guid>
      <pubDate>Sat, 10 May 2025 08:21:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>IT 技术人员被解雇，怒改公司所有密码，获刑 7 个月</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;据报道，在英国西约克郡，一位被解雇 IT 技术人员因心怀怨恨，对雇主公司发动了一场数字攻击，最终被判处 7 个月零 14 天的监禁。&lt;/p&gt; 
&lt;p&gt;根据警方的公告，2022 年 7 月，Mohammed Umar Taj 在被公司暂停工作后的数小时内，便开始实施恶意的 「数字暴行」。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-d7c99b29132ed474e2163c68143d842ddfa.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;他非法侵入公司系统，擅自更改登录凭证，还破坏了公司的多因素身份验证系统，致使公司日常运营受到严重干扰，造成至少 20 万美元的损失。&lt;/p&gt; 
&lt;p&gt;公司称 Taj 的报复行为不仅给公司带来了经济损失，还损害了公司的声誉，给公司带来了声誉损害和业务损失。&lt;/p&gt; 
&lt;p&gt;由于网络攻击的连锁反应，不仅约克郡的员工工作受阻，英国、德国以及巴林的客户也受到了不同程度的影响。&lt;/p&gt; 
&lt;p&gt;上周，31 岁的 Taj 在约克郡的利兹刑事法庭出庭受审，他在之前的听证会上承认了 「未经授权对计算机进行操作并妨碍计算机访问」 的罪名，最终被判处 7 个月零 14 天的监禁。&lt;/p&gt; 
&lt;p&gt;警方还在报告中提醒所有企业：「保护好你们的网络，这不仅能防止数据丢失和代价高昂的网络攻击，还能维护与客户和利益相关者的信任关系。」&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;以下是该事件的具体介绍：&lt;/p&gt; 
&lt;h3&gt;案件经过&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Taj 在被公司停职后，公司未及时撤销其账号权限，他便利用这一漏洞，非法侵入系统，篡改了公司的系统权限，把公司员工以及海外客户都踢出系统之外，导致公司运营陷入瘫痪。&lt;/li&gt; 
 &lt;li&gt;他在实施攻击后，还曾 「自豪地谈论」 自己的攻击过程，警方在后续调查中找到了他作案的操作记录和通话内容。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;法院判决&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Taj 最终在利兹皇家法院承认自己未经授权干扰公司计算机系统的正常运行，于 2025 年 6 月 26 日在利兹刑事法庭被判处七个月零 14 天的监禁。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;事件影响&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;直接经济损失&lt;/strong&gt; ：该公司遭受了至少 20 万英镑的损失，约合人民币近 200 万元。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;业务中断&lt;/strong&gt; ：系统的瘫痪直接导致公司业务无法正常开展，无论是内部的日常运作还是与海外客户的合作都受到了严重影响，公司的声誉也受到了损害。&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358595</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358595</guid>
      <pubDate>Sat, 10 May 2025 08:10:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Cloudflare 推出「按次抓取付费」计划，发起「内容独立日」倡议</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Cloudflare 宣布推出一项名为 「Pay per crawl」 的新功能，并联合多家内容平台发起 「内容独立日」 倡议，旨在改变 AI 公司无偿抓取网络内容进行模型训练的现状。该计划允许网站所有者向 AI 爬虫收取内容访问费用，为内容创作者提供除完全开放或完全封锁之外的第三种选择。Cloudflare 将担任该计划的记录商家（Merchant of Record）。&lt;/p&gt; 
&lt;p&gt;该功能基于 HTTP 状态码 402 (Payment Required) 实现。当 AI 爬虫请求受保护内容时，若未携带支付意图，将收到 402 响应及定价信息。网站所有者可以为自己的域名设定一个统一的、按次请求的单价，并对不同的 AI 爬虫设置三种策略：允许免费访问、按价收费或完全阻止。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-5f8ad61f543b26ca4f0d391b8c340b1f6a3.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;技术上，该系统通过 「Web Bot Auth」 机制，使用 Ed25519 密钥对和 HTTP 消息签名来验证爬虫身份，防止欺骗。爬虫可通过在请求头中加入 crawler-max-price（愿意支付的最高价格）或在收到 402 响应后加入 crawler-exact-price（同意支付的确切价格）来表明支付意图。交易成功后，响应头中会包含 crawler-charged 字段。&lt;/p&gt; 
&lt;p&gt;过去 30 年，谷歌和内容创作者之间形成了一种默契：谷歌用创作者的内容吸引用户搜索，再把用户送回原网站，让创作者赚取广告费或订阅收入。但随着 AI 工具兴起，用户越来越多地直接从 AI 获得答案，原创内容的网站流量暴跌，创作者的收益严重受损。&lt;/p&gt; 
&lt;p&gt;为此，Cloudflare 联合众多内容平台，在 2025 年 7 月 1 日宣布了 「内容独立日」，明确要求 AI 公司不能再免费抓取内容，必须为内容创作者支付合理的报酬。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-b62a9cf03f1853620715f17a648ca213898.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Cloudflare 表示，此举旨在为内容创作者提供对其数字资产的程序化控制，确保他们能从自己的工作中获得补偿，从而维持一个健康、多样化的互联网内容生态。未来，该系统有望演变为一个更复杂的代理（Agent）经济市场，AI 代理可以根据预算，以编程方式协商并购买所需的数据访问权限。&lt;/p&gt; 
&lt;p&gt;他们希望通过这样的行动，让创作者重新获得应有的价值和尊重，同时推动 AI 和原创内容之间形成一种新的、公平的生态模式。&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;更多详情：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.cloudflare.com%2Fcontrol-content-use-for-ai-training%2F"&gt;https://blog.cloudflare.com/control-content-use-for-ai-training/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.cloudflare.com%2Fzh-cn%2Fcontent-independence-day-no-ai-crawl-without-compensation%2F"&gt;https://blog.cloudflare.com/zh-cn/content-independence-day-no-ai-crawl-without-compensation/&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358592/content-independence-day-no-ai-crawl</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358592/content-independence-day-no-ai-crawl</guid>
      <pubDate>Sat, 10 May 2025 07:56:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>腾讯提醒开发者可将微信小程序迁移至 QQ 客户端</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;腾讯 QQ 小程序开发者平台发文，提醒 QQ 客户端将全面接入微信小程序，&lt;strong&gt;开发者可以将微信小程序迁移至 QQ 以取代原有的旧版 QQ 小程序&lt;/strong&gt;，开发者当前已上线的旧版 QQ 小程序仍可正常使用和更新，不过官方称「强烈建议尽早迁移，以便获得更完整的接口支持，同时享受 QQ + 微信的双端流量红利」。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-e508f98702091d79c43aa333caf718d83a9.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;迁移前，QQ 小程序引擎实际上对微信小程序的大多数接口进行了兼容，原本只需要在微信小程序原有代码基础上做一些简单判断（主要是登录方面）就可以分别提交两个平台。&lt;/p&gt; 
&lt;p&gt;而在迁移后，QQ 端运行的就是微信小程序，体验比 QQ 小程序会好一些。开发者需要通过 QQ 提供的插件（qq-wxmini-plugin）区分运行环境、处理登录逻辑。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;来源：https://m.ithome.com/html/864991.htm&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;相关阅读：&lt;a href="https://www.oschina.net/news/346915" target="news"&gt;手机版 QQ 支持微信小程序&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358586</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358586</guid>
      <pubDate>Sat, 10 May 2025 07:38:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>多模态才是智能应用爆发的关键？</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                                        &lt;p&gt;此前，快手发布 2025 年一季度财报时，一个数字引发关注：成立仅两年的 AI 业务线「可灵 AI」单季度贡献营收 1.5 亿元，同比增长 320%。而可灵 AI 正是一个多模态应用的典型产品，涉及到语言、视频、音频等交互。&lt;/p&gt; 
&lt;p&gt;前不久，在 OSCHINA 和小度教育技术负责人丁小晶的&lt;a href="https://my.oschina.net/u/4489239/blog/18426743" rel="nofollow"&gt;对话&lt;/a&gt;中。丁小晶表示，多模态技术非常重要，甚至可以说，没有多模态技术效果的快速提升，教育行业不可能如此迅猛发展。比如 AI 作业批改和 AI 讲题答疑方向的应用，完全靠纯文本大模型是无法满足需求的，非常依赖对大模型的图片理解能力。还比如超拟人 AI 老师，语音情感大模型就起来非常关键的作用。&lt;/p&gt; 
&lt;p&gt;百度最新发布的发布文心快码 Comate AI IDE 产品，其中也提到了多模态能力的增强，比如支持 Figma 设计稿一键转换为高可用代码，能实现图层的精准还原。百度工程效能部前端研发经理杨经纬告诉开源中国，无论是从自然语言、图片还是设计稿生成代码，最终都是为了能更加接近人类工程的意图，因为人类去描述自己想要实现的想法的方式与形态是多种多样的，也就对应了研发过程中的多模态形式。&lt;/p&gt; 
&lt;p style="text-align:center"&gt;&lt;img height="210" src="https://oscimg.oschina.net/oscnet/up-db06f16dbd4e854566d762bff8c3dfe1e5f.png" width="800" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;人类从不会只用一种感官认知世界。人工智能也势必不能仅有一种交互途径。&lt;/p&gt; 
&lt;p&gt;我们闻到咖啡香气的瞬间，脑海里会立刻浮现深褐色液体与白瓷杯的画面；听到「猫」这个词时，脑海中自动补全毛茸茸的触感和呼噜声。这种多模态信息融合，正是人类智能的底层逻辑。而单一模态交换的 AI 模型的信息处理能力有限，例如文本生成模型难以理解图像语义，无法根据文字生成图像，视频生成工具则无法同步解析声音与画面逻辑。这种时候，就需要多模态模型或是能力的配合。&lt;/p&gt; 
&lt;span id="OSC_h2_1"&gt;&lt;/span&gt; 
&lt;h2&gt;多模态，比文本慢一步&lt;/h2&gt; 
&lt;p&gt;智源研究院院长王仲远不久前公开&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.msn.cn%2Fzh-cn%2F%25E6%258A%2580%25E6%259C%25AF%2F%25E6%258A%2580%25E6%259C%25AF%25E5%2585%25AC%25E5%258F%25B8%2F%25E8%2581%259A%25E7%2584%25A6%25E5%25A4%259A%25E6%25A8%25A1%25E6%2580%2581-chatgpt%25E6%2597%25B6%25E5%2588%25BB%25E6%259C%25AA%25E5%2588%25B0-2025%25E5%25A4%25A7%25E6%25A8%25A1%25E5%259E%258B-%25E5%258F%2598%25E6%2585%25A2-%25E4%25BA%2586%25E5%2590%2597%2Far-AA1GjaHk%3Focid%3DBingNewsSerp" rel="nofollow" target="_blank"&gt;指出&lt;/a&gt;，当前多模态大模型的学习路径，尤其是多模态理解模型，通常是先将语言模型训练到很强的程度，再学习其他模态信息。在这个过程中，模型的能力可能会出现下降。&lt;/p&gt; 
&lt;p&gt;比单一模态更难的是，多模态模型还需解决一个核心问题：如何将图像、文本、音频等异构数据在语义层面对齐并融合。&lt;/p&gt; 
&lt;p&gt;文本、图像、声音等模态的数据结构天然异构——文本是离散符号序列，图像是连续像素矩阵，音频是时间序列信号。比如要让模型理解「猫」的文本描述与猫的图片、叫声之间的关联，需构建跨模态的共享语义空间。&lt;/p&gt; 
&lt;p&gt;早期，有研究尝试通过数据级拼接，将图像像素和文本特征直接拼接，实现跨模态融合，但由于图像和文本的时空特性差异较大，导致特征对齐困难，最终效果不佳。直到对比学习和注意力机制的出现，才实现跨模态语义映射。比如 OpenAI 2021 年推出的一种基于对比学习只的多模态预训练模型 CLIP，它通过大规模的图像和文本数据进行训练，使得模型能够理解图像内容和相关文本之间的语义关系。CLIP 的核心贡献在于它打破了传统的固定类别标签范式，通过对比学习的方式，将图像和文本映射到同一个向量空间中，从而实现跨模态的检索和分类。但是 CLIP 模型的训练数据规模庞大，据 OpenAI 披露，其使用了约 4 亿图像-文本对进行训练，训练成本高达数千 GPU 日，远超 GPT-3 等纯文本模型。&lt;/p&gt; 
&lt;p style="text-align:center"&gt;&lt;img height="304" src="https://oscimg.oschina.net/oscnet/up-4ad6b286433edebde043654fd53af191e30.png" width="800" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="text-align:center"&gt;&lt;span style="color:#8f959e"&gt;&lt;em&gt;CLIP 模型方法概述 &lt;/em&gt;&lt;/span&gt;&lt;span style="color:#8f959e"&gt;&lt;u&gt;&lt;em&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2103.00020" rel="nofollow" target="_blank"&gt;Learning Transferable Visual Models From Natural Language Supervision&lt;/a&gt;&lt;/em&gt;&lt;/u&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;多模态融合需处理高维数据，如 4K 视频的像素量是文本的百万倍，传统 Transformer 的二次方计算复杂度成为致命短板。对此，业界也有一些解决方式，比如此前 Mamba 架构通过状态空间模型 SSM 将计算复杂度降至线性，2025 年扩展动态融合模块&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F2985554863" rel="nofollow" target="_blank"&gt; FusionMamba&lt;/a&gt;，在其中实现多模态特征高效交互，推理速度提升 3 倍。&lt;/p&gt; 
&lt;p&gt;不仅如此，相较于文本的资料库和数据集，高质量多模态数据集也更加稀缺，收集难度更大。比如医疗影像、工业质检的报告中的缺陷描述等，就需专家级别的标注人员。&lt;/p&gt; 
&lt;span id="OSC_h2_2"&gt;&lt;/span&gt; 
&lt;h2&gt;落地需求更多&lt;/h2&gt; 
&lt;p&gt;虽然技术上还有诸多难点，但是多模态能力正在逐步提升，并且带来非常可观的价值和效果。&lt;/p&gt; 
&lt;p&gt;比如，从图片或者是 Figma 设计稿直接生成代码可以帮助许多开发者或是产品经理完成一些开发工作。这项能力此前在一些低代码或是辅助编程工具中也存在，但往往是通过 Figma DSL 进行设计稿解析，通过节点虚拟化技术实现像素级还原，其不足在于不一定适配当前项目，比如转了一套 Vue 框架的代码，就无法在 React 框架项目中使用。&lt;/p&gt; 
&lt;p&gt;杨经纬介绍，此次文心快码 Comate AI IDE 的发布以及相关功能更新后，通过大模型能力增强了 Figma to Code 和当前项目的融合度。首先在 IDE 里进行操作，天然就可以理解用户当前环境和本地优势，而 IDE 内智能体 Zulu 的接入，会更深入到本地项目中了解当前的框架、能力、代码风格等，再结合 Image to Code 的能力，可以实现较高的还原度，并且适配当前的项目。&lt;/p&gt; 
&lt;p&gt;而根据一些公开信息显示，可灵 AI 的多模态技术，支持通过图片、文字、声音甚至手绘轨迹等输入生成视频。在上半年的 2.0 模型的迭代中，可灵 AI 也发布了 AI 视频生成的全新交互理念 Multi-modal Visual Language（MVL），让用户能够结合图像参考、视频片段等多模态信息，将脑海中包含身份、外观、风格、场景、动作、表情、运镜在内的多维度复杂创意，直接高效地传达给 AI。MVL 由 TXT（Pure Text，语义骨架）和 MMW（Multi-modal-document as a Word，多模态描述子）组成，能从视频生成设定的基础方向以及精细控制这两个层面。此外，其技术也结合了类 Sora 的 DiT 结构和 Flow 扩散模型，提升在物理模拟和细节上的表现。&lt;/p&gt; 
&lt;p&gt;基于这些技术特征。商业化层面，截至今年 6 月，可灵 AI 已为超过 1 万家企业客户提供 API 服务，覆盖广告营销、影视动画等领域，企业客户续费率较高。&lt;/p&gt; 
&lt;p&gt;此外，一些传统行业或场景也在结合多模态能力，实现与 AI 的加速融合。比如迪瑞医疗近期采用的多模态 AI 大模型算法技术为临床诊断带来了重要的技术革新，结合多种检测结果和患者的多维信息，如尿常规、血常规、生化和化学发光免疫，以及患者的个人背景、临床表现、现病史与既往病史等，进行全面分析。&lt;/p&gt; 
&lt;p&gt;这种跨学科的信息整合使得诊断提示更加精准，对于减少漏诊、误诊的概率具有显著的作用，并进一步提升了医疗诊疗的整体效率。大洋彼岸，斯坦福医学院的科研团队研发出了一种名为&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzkxMTM0OTQzNQ%3D%3D%26mid%3D2247486194%26idx%3D1%26sn%3D5ac605d67ca7019b3b2e524d65b0f88e%26chksm%3Dc0eed67e545679711993370e69032cc62e9d4fc0c6ff3e283c43854fd93355eae8a07b4fcb02%23rd" rel="nofollow" target="_blank"&gt; MUSK 的 AI 模型&lt;/a&gt;，将视觉数据，如病理图像和文本数据的病历和临床记录相结合，为癌症治疗带来了新的可能。MUSK 模型不仅提高了预测癌症患者预后和治疗反应的准确性，而且通过分析数千个数据点，更准确地确定了哪些疗法对个体患者最有效。&lt;/p&gt; 
&lt;p style="text-align:center"&gt;&lt;img height="285" src="https://oscimg.oschina.net/oscnet/up-196e0ee8b1058ba8ee70698e626a846fe72.png" width="800" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="text-align:center"&gt;&lt;span style="background-color:#f2f3f5"&gt;&lt;em&gt;视觉问答测试，图片来源于网络&lt;/em&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;在金融领域。江苏银行通过本地化部署微调 DeepSeek-VL2 多模态模型、轻量 DeepSeek-R1 推理模型，分别运用于智能合同质检和自动化估值对账场景中，通过对海量金融数据的挖掘与分析，重塑金融服务模式，实现金融语义理解准确率与业务效率双突破。具体而言，DeepSeek-VL2 多模态模型采用了最新的 Transformer 架构，结合多层次的特征融合机制，有效提升了金融合同、账单等复杂文本与图像信息的理解能力。模型在智能合同质检场景中表现出色，准确率较传统方法提升了 15% 以上，显著降低了人工审核成本。同时，轻量化的 DeepSeek-R1 推理模型则在自动化估值与对账场景中展现出极佳的实时响应能力，推理速度提升了 30%，为金融业务流程的自动化提供了坚实支撑。&lt;/p&gt; 
&lt;span id="OSC_h2_3"&gt;&lt;/span&gt; 
&lt;h2&gt;新的基础设施&lt;/h2&gt; 
&lt;p&gt;应用边界在不断拓宽的同时，多模态模型的能力也在成长。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;而随着应用场景的深化，模型架构也在同步进化，从基础感知迈向复杂推理成为必然趋势。OpenAI 在 2025 年 4 月发布了多模态模型 O3 和 O4-mini，实现了「用图像思考」的突破性能力。这些模型不仅能够识别图像内容，还能将图像信息整合进推理思维链，支持多步推理和因果分析，比如够处理模糊、倒置或复杂的图像输入，并给出合理的推理结果。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;其背后的关键技术包括分层注意力机制，将图像分解为局部细节、全局关系和时序逻辑三层结构，从而提升对图像内容的理解能力；动态工具链调用，在推理过程中，模型可以自主选择 Python 分析、知识图谱检索、图像生成等工具辅助决策，以及安全约束模块，通过对抗训练减少模型的幻觉输出。&lt;/p&gt; 
&lt;p&gt;就在本月，中国科学院自动化研究所等单位的科研人员&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fkw.beijing.gov.cn%2Fxwdt%2Fkcyx%2Fxwdtkjqy%2F202506%2Ft20250611_4111006.html" rel="nofollow" target="_blank"&gt;首次证实&lt;/a&gt;，多模态大语言模型在训练过程中自己学会了「理解」事物，而且这种理解方式和人类非常像。&lt;/p&gt; 
&lt;p&gt;科研人员借鉴人脑认知的原理，设计了一个巧妙的实验：让大模型和人类玩「找不同」游戏。实验人员会给出三个物品概念（选自 1854 种常见物品），要求选出最不搭的那个。通过分析高达 470 万次的判断数据，科研人员绘制出了大模型的「思维导图」——「概念地图」。通过实验证实多模态大模型具备类人「概念理解」能力。研究团队设计「找不同」游戏，基于 470 万次判断数据绘制大模型「概念地图」，提炼 66 个理解维度（如物体功能、文化意义），发现其与人脑神经活动高度一致，证明多模态模型比纯文本模型更接近人类思维模式。&lt;/p&gt; 
&lt;p&gt;据谷歌云在 2024 年年底发布的&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnews.qq.com%2Frain%2Fa%2F20241219A07AW200" rel="nofollow" target="_blank"&gt;《2025 年人工智能商业趋势报告》&lt;/a&gt;，预测到 2025 年，多模态 AI 将成为企业采用 AI 的主要驱动力。这种技术通过整合图像、视频、音频和文本等多种数据源，使 AI 能够以前所未有的准确性从更广泛的上下文源中学习，提供更精确、定制化的输出，创造自然直观的体验。报告预计，全球多模态 AI 市场规模将在 2025 年达到 24 亿美元，到 2037 年底达到 989 亿美元。&lt;/p&gt; 
&lt;p&gt;2025 进度已经过半，我们也能看到市面上许多多模态技术和产品的进展，而这场变革的终极图景，或许正是让 AI 真正成为理解世界、服务人类的「多模态智能伙伴」。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/4489239/blog/18679654</link>
      <guid isPermaLink="false">https://my.oschina.net/u/4489239/blog/18679654</guid>
      <pubDate>Sat, 10 May 2025 07:31:00 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>Windows 11 记事本正式支持 Markdown</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;5 月底，预览体验计划中的 Windows 11 记事本迎来史诗级更新：&lt;a href="https://www.oschina.net/news/353253/windows-notepad-markdown"&gt;支持 Markdown 格式&lt;/a&gt;。现在普通用户也可以使用这个版本了，只需要在应用商店中更新记事本即可使用。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-af88d0da29ab2cf4246999e5b5025bf874c.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;目前支持：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;粗体&lt;/li&gt; 
 &lt;li&gt;斜体&lt;/li&gt; 
 &lt;li&gt;链接&lt;/li&gt; 
 &lt;li&gt;序号&lt;/li&gt; 
 &lt;li&gt;标题（H1～H5）&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;如下面的屏幕截图所示，您可以点击新的「H1」图标，然后选择您喜欢的标题：标题、副标题、章节，甚至是小节。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-387e4d297dcd5ac663e49554b55e2f51091.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;接下来，我们可以看到项目符号和数字列表按钮，以及用于加粗或应用斜体的选项，但最吸引我注意的是超链接支持。现在，您可以使用 Ctrl + K 键盘快捷键（该快捷键在 Word 中也使用）插入带有锚文本的链接，并在默认浏览器中打开。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-79d07dc4e8307d47c79e780de309830f227.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;此外，您可以点击屏幕底部的「格式化视图」按钮切换到 Markdown 语法（原始）视图。&lt;/p&gt; 
&lt;p&gt;与「格式」视图不同，语法视图允许您将井号转换为标题、使用星号强调、用反引号包裹代码等等。语法视图类似于在后端使用 Markdown 编辑器，但它不会更改输出。这取决于您在记事本中使用 Markdown 的方式。&lt;/p&gt; 
&lt;p&gt;在我们的测试中，Windows 最新版本观察到记事本默认启用 Markdown，但您有两个选择。您可以单击清理格式按钮，返回原始记事本体验，而无需禁用 Markdown 支持。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-78b526954a85441998f9ce8f508e77c65f8.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;或者，您可以打开「设置」，向下滚动一点，找到一个名为「格式化」的新选项。关闭「格式化」后，记事本的经典体验将恢复。您将不再看到格式化栏，Windows 也不会提示您使用它。&lt;/p&gt; 
&lt;p&gt;测试中，我们还注意到微软在记事本中实现了非常轻量级的 Markdown 功能，它不会让您的电脑运行速度变慢。&lt;/p&gt; 
&lt;p&gt;有些人可能会认为，给记事本添加太多功能违背了它作为纯文本编辑器的初衷。这种观点很有道理，但只要 Markdown 之类的功能是可选的，我并不介意。如果我需要它们，可以在「设置」中打开；如果不需要，也可以再次关闭。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;相关阅读：&lt;a href="https://www.oschina.net/news/353253/windows-notepad-markdown" target="news"&gt;Windows 记事本支持 Markdown&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358574/windows-11-notepad-markdown</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358574/windows-11-notepad-markdown</guid>
      <pubDate>Sat, 10 May 2025 07:09:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>抖音内容技术团队开源 ContentV：有限算力下高效训练视频生成模型的新路径</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                                        &lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//adfb650ff16d8feb12d600710037b8a4.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;div class="ckeditor-html5-video" style="text-align:center"&gt; 
  &lt;video controls="controls" controlslist="nodownload" src="https://www.bilibili.com/video/BV1jC3azYEaS/?spm_id_from=333.1387.upload.video_card.click&amp;amp;vd_source=c09f0713b2507369924e94f4fec6c133"&gt;
    &amp;nbsp; 
  &lt;/video&gt; 
 &lt;/div&gt; 
 &lt;div class="ckeditor-html5-video" style="text-align:center"&gt; 
  &lt;video controls="controls" controlslist="nodownload" src="https://www.bilibili.com/video/BV1jC3azYEuW/?spm_id_from=333.1387.upload.video_card.click&amp;amp;vd_source=c09f0713b2507369924e94f4fec6c133"&gt;
    &amp;nbsp; 
  &lt;/video&gt; 
 &lt;/div&gt; 抖音内容技术团队开源了 ContentV，一种面向视频生成任务的高效训练方案。该方案在多项技术优化的基础上，使用 256 块显卡，在约 4 周内完成了一个 8B 参数模型的训练。尽管资源有限，ContentV 在多个评估维度上取得了与现有主流方案相近的生成效果。该工作探索了在有限算力条件下训练视频生成模型的可行路径。目前，推理代码与模型权重已对外开放。 
&lt;/div&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;项目主页&lt;/strong&gt;：https://contentv.github.io&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;技术报告&lt;/strong&gt;：https://arxiv.org/abs/2506.05343&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;代码仓库&lt;/strong&gt;：https://github.com/bytedance/ContentV&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;模型权重&lt;/strong&gt;：https://huggingface.co/ByteDance/ContentV-8B&lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="OSC_h2_1"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;核心亮点&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;极简设计&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;CogVideoX、HunyuanVideo 和 Wan2.1 等一系列优秀的开源工作表明，视频生成的关键并不在于架构上的特殊设计，而在于如何高效利用有限的数据资源，并有效对齐人类偏好。&lt;/p&gt; 
&lt;p&gt;为验证 ContentV 方案的通用性，本次开源的版本在扩散模型部分采用了经典的文生图模型 Stable Diffusion 3.5 Large。为了适配视频模态，模型在结构上仅做了以下两项必要调整：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;将原始图像 VAE 替换为 Wan2.1 中使用的 3D-VAE；&lt;/li&gt; 
 &lt;li&gt;将 2D 位置编码升级为 3D 版本。在具体编码方式上，团队对比了传统的绝对位置编码与主流的旋转位置编码。评估结果显示，两者在客观指标和主观感受上差异较小，因此保留了计算更高效的绝对位置编码方案。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//a970cfacc9f335795a1cb051ba654811.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="text-align:center"&gt;ContentV 模型结构‌&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;多阶段渐进训练策略&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;上述的最小化结构改动，在解锁了视频生成能力的同时，也最大限度地保留了原模型的图像生成能力。实验证明，在新的 VAE 和位置编码的适配阶段，沿用 Flow Matching 的训练方式，仅需 1000 步左右的微调，就能基本还原模型的图片生成能力，大幅节省图片预训练阶段的训练成本。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//da7aab81f63561b7c0d8679eceba1022.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="text-align:center"&gt;VAE 适配过程‌&lt;/p&gt; 
&lt;p&gt;在视频生成的预训练阶段，为加速收敛实现高效训练，研究团队设计了一套从「低清短片」到「高清长片」的多阶段渐进式训练流程，逐步引导模型学习时间维度与空间维度上的动态表征，从而提升视频的连续性、动态表现力和画面细节。&lt;/p&gt; 
&lt;p&gt;此外，实验证明，在推理阶段引入非线性采样步长机制（Flow Shift）能够显著提升视频的整体生成质量。通过多组对比实验，团队最终确定了最优的采样策略，进一步优化了生成效果。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;轻量级 RLHF 强化训练&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//25bf5c0471707e1d93343f8649c7f46a.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//aaf7366e824c8a20dc80a43af6d4e872.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="text-align:center"&gt;RLHF 显著提升画面质感‌&lt;/p&gt; 
&lt;p&gt;在后训练阶段，除了使用高质量数据集进行微调外，通过 RLHF 或 DPO 等对齐人类偏好的监督训练，也能显著提升视频生成质量。然而，这类方法通常依赖大量人工标注，用于训练奖励模型或直接监督扩散模型。同时，相较于图像，视频的序列长度显著增加了 RLHF 和 DPO 的训练资源需求。&lt;/p&gt; 
&lt;p&gt;为此，ContentV 研究团队提出了一种轻量级的 RLHF 训练方案，旨在不依赖人工标注的前提下，低成本提升视频质量：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;利用开源的图像奖励模型对生成视频的单帧进行监督。相较于视频场景，目前图像奖励模型的训练数据更易获取，且在实际效果中表现更佳。实验证明，由于 MM DiT 采用全局注意力机制，仅优化单帧即可带动整体视频质量的提升；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;将监督范围限制在生成视频的前 1 秒，相较于对完整视频进行监督，可大幅减少训练资源的消耗，同时获得相近的质量提升效果。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;采用上述策略后，在无需人工标注的情况下，仅使用少量训练资源，便可显著提升画面质量。RLHF 微调后，模型在视觉质量（VQ）指标上的表现大幅提升，评估胜率高达 89.38%。&lt;/p&gt; 
&lt;span id="OSC_h2_2"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;效果对比&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;在 VBench 这一主流视频生成评测基准上，ContentV（8B）取得了 85.14 的综合得分，表现优于多个现有的商业闭源模型，包括 Sora、Kling 1.6 和 Gen-3 等。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//2e5d2e9c6fb2a651ef95db56aa008420.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="text-align:center"&gt;VBench 榜单 (按照 Overall 分数降序排列)‌&lt;/p&gt; 
&lt;p&gt;为更贴近真实用户偏好，研究团队围绕感知质量、指令跟随、物理一致性和视觉效果四个维度开展了人类偏好评估。结果显示，ContentV 在整体表现上与 CogVideoX-5B、HunyuanVideo-13B 和 Wan2.1-14B 等主流开源模型相比具有一定优势。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//547c06ccbfa684f90c21d1432e0c6786.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="text-align:center"&gt;人类偏好评估指标‌&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/6210722/blog/18683305</link>
      <guid isPermaLink="false">https://my.oschina.net/u/6210722/blog/18683305</guid>
      <pubDate>Sat, 10 May 2025 06:53:00 GMT</pubDate>
      <author>原创</author>
    </item>
  </channel>
</rss>
