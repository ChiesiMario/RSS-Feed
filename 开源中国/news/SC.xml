<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>开源中国-最新资讯</title>
        <link>https://www.oschina.net/news/project</link>
        <atom:link href="http://8.134.148.166:30044/oschina/news" rel="self" type="application/rss+xml"></atom:link>
        <description>开源中国-最新资讯 - Powered by RSSHub</description>
        <generator>RSSHub</generator>
        <webMaster>contact@rsshub.app (RSSHub)</webMaster>
        <language>en</language>
        <lastBuildDate>Mon, 17 Feb 2025 07:36:58 GMT</lastBuildDate>
        <ttl>5</ttl>
        <item>
            <title>最新尸检报告认定 OpenAI「吹哨人」死因为自杀</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;2024 年 11 月 26 日，前 OpenAI 员工 Suchir Balaji 在旧金山的公寓中被发现死亡，年仅 26 岁。时至今日，旧金山法医部门在最新公布的尸检报告裁定 Balaji 的死因为开枪自杀，驳斥了 Balaji 家人有关他杀的怀疑。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;344&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-1a05ba4a3131262dbb151b1411f9a3d8cd9.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;资料显示，Balaji 是一名印度裔美国人，曾在加州大学伯克利分校学习并获得了计算机科学学士学位。大学期间，他于 2019 年在 Scale AI 实习，并于 2021 年毕业后加入 OpenAI，参与过 WebGPT 的研发，后来又加入 GPT-4 的预训练团队，o1 的推理团队以及 ChatGPT 的后训练团队。2024 年 8 月，他因对公司的商业行为感到失望后离职，并公开表达了自己的担忧：「如果你相信我所相信的，你就必须离开公司」。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;10 月份，Balaji 因指控 OpenAI 非法使用受版权保护的材料来训练其 AI 模型而广受关注。《纽约时报》后来将他列为该报对 OpenAI 的诉讼中「拥有独特和相关文件」的关键人物。彼时，OpenAI 正在被众多著名作家和新闻出版商起诉侵犯版权。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;离开 OpenAI 后，Balaji 表示自己一直在从事「个人项目」。据他母亲说，他计划创建一个以机器学习和神经科学为中心的非营利组织。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334332/death-of-openai-suchir-balaji</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334332/death-of-openai-suchir-balaji</guid>
            <pubDate>Mon, 17 Feb 2025 07:13:04 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>Android 16 第二个 Beta 版本发布</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Android 16 第二个 Beta 版本现已发布，增加了对专业相机体验、图形效果的新支持，扩展了性能框架，并继续改进与隐私、安全和后台任务相关的功能。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Android 16 增强了对专业相机用户的支持，允许混合自动曝光以及精确的色温和色调调整。使用新的 Intent 操作拍摄动态照片比以往任何时候都更容易，并且继续改进 UltraHDR 图像，支持 HEIC 编码和 ISO 21496-1 草案标准中的新参数。&lt;/span&gt;&lt;/p&gt; 
&lt;pre style=&quot;margin-left:0; margin-right:0; text-align:left !important&quot;&gt;&lt;strong style=&quot;color:#008000&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;fun&lt;/span&gt;&lt;/strong&gt;&lt;span style=&quot;color:#000000&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color:#0000ff&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;setISOPriority&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;()&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;{&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;
   &lt;/span&gt;&lt;em&gt;&lt;span style=&quot;color:#006600&quot;&gt;// ...&lt;/span&gt;&lt;/em&gt;&lt;span style=&quot;color:#000000&quot;&gt;

    &lt;/span&gt;&lt;strong style=&quot;color:#008000&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;val&lt;/span&gt;&lt;/strong&gt;&lt;span style=&quot;color:#000000&quot;&gt; availablePriorityModes &lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt; mStaticInfo&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;characteristics&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;.&lt;/span&gt;&lt;strong style=&quot;color:#008000&quot;&gt;&lt;span style=&quot;color:#000088&quot;&gt;get&lt;/span&gt;&lt;/strong&gt;&lt;span style=&quot;color:#666600&quot;&gt;(&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;
        &lt;/span&gt;&lt;span style=&quot;color:#660066&quot;&gt;CameraCharacteristics&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;CONTROL_AE_AVAILABLE_PRIORITY_MODES
    &lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;)&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;
    &lt;/span&gt;&lt;em&gt;&lt;span style=&quot;color:#006600&quot;&gt;// ...&lt;/span&gt;&lt;/em&gt;&lt;span style=&quot;color:#000000&quot;&gt;
    
    &lt;/span&gt;&lt;em&gt;&lt;span style=&quot;color:#006600&quot;&gt;// Turn on AE mode to set priority mode&lt;/span&gt;&lt;/em&gt;&lt;span style=&quot;color:#000000&quot;&gt;
    reqBuilder&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;[&lt;/span&gt;&lt;span style=&quot;color:#660066&quot;&gt;CaptureRequest&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;CONTROL_AE_MODE&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;]&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color:#660066&quot;&gt;CameraMetadata&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;CONTROL_AE_MODE_ON
    reqBuilder&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;[&lt;/span&gt;&lt;span style=&quot;color:#660066&quot;&gt;CaptureRequest&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;CONTROL_AE_PRIORITY_MODE&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;]&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color:#660066&quot;&gt;CameraMetadata&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;CONTROL_AE_PRIORITY_MODE_SENSOR_SENSITIVITY_PRIORITY
    reqBuilder&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;[&lt;/span&gt;&lt;span style=&quot;color:#660066&quot;&gt;CaptureRequest&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;SENSOR_SENSITIVITY&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;]&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt; TEST_SENSITIVITY_VALUE
    &lt;/span&gt;&lt;strong style=&quot;color:#008000&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;val&lt;/span&gt;&lt;/strong&gt;&lt;span style=&quot;color:#000000&quot;&gt; request&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;:&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color:#660066&quot;&gt;CaptureRequest&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt; reqBuilder&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;build&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;()&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;

    &lt;/span&gt;&lt;em&gt;&lt;span style=&quot;color:#006600&quot;&gt;// ...&lt;/span&gt;&lt;/em&gt;&lt;span style=&quot;color:#000000&quot;&gt;

&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;}&lt;/span&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;385&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-b23fc5ea06a5a11a06d173580a172064ded.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Android 16 还将添加 RuntimeColorFilter 和 RuntimeXfermode，允许用户在绘制调用中添加图形效果，例如阈值、棕褐色和色相饱和度。&lt;/span&gt;&lt;/p&gt; 
&lt;pre style=&quot;margin-left:0; margin-right:0; text-align:left !important&quot;&gt;&lt;strong style=&quot;color:#008000&quot;&gt;&lt;span style=&quot;color:#000088&quot;&gt;private&lt;/span&gt;&lt;/strong&gt;&lt;span style=&quot;color:#000000&quot;&gt; &lt;/span&gt;&lt;strong style=&quot;color:#008000&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;val&lt;/span&gt;&lt;/strong&gt;&lt;span style=&quot;color:#000000&quot;&gt; thresholdEffectString &lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color:#ba2121&quot;&gt;&lt;span style=&quot;color:#008800&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;color:#008800&quot;&gt;
    uniform half threshold;
    half4 &lt;/span&gt;&lt;span style=&quot;color:#0000ff&quot;&gt;&lt;span style=&quot;color:#008800&quot;&gt;main&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;color:#008800&quot;&gt;(half4 c) {
        half luminosity = dot(c.rgb, half3(&lt;/span&gt;&lt;span style=&quot;color:#666666&quot;&gt;&lt;span style=&quot;color:#008800&quot;&gt;0.2126&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;color:#008800&quot;&gt;, &lt;/span&gt;&lt;span style=&quot;color:#666666&quot;&gt;&lt;span style=&quot;color:#008800&quot;&gt;0.7152&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;color:#008800&quot;&gt;, &lt;/span&gt;&lt;span style=&quot;color:#666666&quot;&gt;&lt;span style=&quot;color:#008800&quot;&gt;0.0722&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;color:#008800&quot;&gt;));
        half bw = step(threshold, luminosity);
        &lt;/span&gt;&lt;strong style=&quot;color:#008000&quot;&gt;&lt;span style=&quot;color:#008800&quot;&gt;return&lt;/span&gt;&lt;/strong&gt;&lt;span style=&quot;color:#008800&quot;&gt; bw.xxx1 * c.a;
    }&lt;/span&gt;&lt;span style=&quot;color:#ba2121&quot;&gt;&lt;span style=&quot;color:#008800&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;

&lt;/span&gt;&lt;strong style=&quot;color:#008000&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;fun&lt;/span&gt;&lt;/strong&gt;&lt;span style=&quot;color:#000000&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color:#0000ff&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;setCustomColorFilter&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;(&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;paint&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;:&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color:#660066&quot;&gt;Paint&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;)&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;{&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;
   &lt;/span&gt;&lt;strong style=&quot;color:#008000&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;val&lt;/span&gt;&lt;/strong&gt;&lt;span style=&quot;color:#000000&quot;&gt; filter &lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt; &lt;/span&gt;&lt;span style=&quot;color:#660066&quot;&gt;RuntimeColorFilter&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;(&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;thresholdEffectString&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;)&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;
   filter&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;setFloatUniform&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;(&lt;/span&gt;&lt;span style=&quot;color:#666666&quot;&gt;&lt;span style=&quot;color:#006666&quot;&gt;0.5&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;)&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;
   paint&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;.&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;colorFilter &lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt; filter
&lt;/span&gt;&lt;span style=&quot;color:#666600&quot;&gt;}&lt;/span&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;此外，新版本中的一些变化还包括&amp;nbsp;R.attr#windowOptOutEdgeToEdgeEnforcement 将被弃用并禁用、Health and fitness permissions&amp;nbsp;的新 API targets、针对意图重定向攻击的默认安全强化等等。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Android 16 计划于 2025 年第二季度发布，这将是今年发布的唯一包含功能变更的 Android 版本。预计第四季度将发布另一个包含新开发者 API、优化和错误修复的版本。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;133&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-26a04b0db45f01253cbf0b8249a6864b748.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;117&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-5ac252c1d5405e7554865078867a3e4708b.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;更多详情可&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fandroid-developers.googleblog.com%2F2025%2F02%2Fsecond-beta-android16.html&quot; target=&quot;_blank&quot;&gt;查看官方公告&lt;/a&gt;。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334329/second-beta-android16</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334329/second-beta-android16</guid>
            <pubDate>Mon, 17 Feb 2025 06:54:33 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>Asahi Linux 创始人宣布辞去项目负责人职务</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;上周，Hector Martin 辞去了 Linux 内核 Apple Silicon 代码的上游维护工作。当时他仍然计划为 Asahi Linux 项目的下游内核做出贡献，但就在前两天，&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmarcan.st%2F2025%2F02%2Fresigning-as-asahi-linux-project-lead%2F&quot; target=&quot;_blank&quot;&gt;他出人意料地决定辞去 Asahi Linux 项目负责人的职位&lt;/a&gt;。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/143821_rzmV_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Asahi Linux 项目创始人 Hector Martin 在博客宣布，他将辞去项目负责人的职务。Martin 说道，随着时间的推移，参与项目变得越来越没有乐趣，并注意到了关于 Asahi Linux 在 Apple Silicon 上缺乏 Apple M3/M4 支持以及其他缺失功能（如 Thunderbolt 和 USB-C 显示器）的用户投诉。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;由于围绕 Apple 芯片硬件上 Asahi Linux 的用户期望感到沮丧，并且最近还与 Linux 内核中 Rust 代码的上游挫折/争论/挑战以及其他因素相关，Hector Martin 决定辞职&lt;/strong&gt;。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;「我立即辞去 Asahi Linux 项目负责人的职务。Asahi Linux&amp;nbsp;项目将继续进行，我正在与团队的其他成员一起处理职责和行政凭证的移交。我的个人 Patreon 将暂停，那些曾向我个人捐赠的用户建议转移到 Asahi Linux OpenCollective（GitHub Sponsors 不允许我单方面暂停付款，但我的赞助者将被告知这一变化，以便他们可以手动取消赞助）。&lt;/p&gt; 
 &lt;p&gt;我想感谢整个 Asahi Linux 团队，没有你们，我独自一人根本无法取得任何进展。我还对我的所有 Patreon 和 GitHub 赞助者表示最深切的感激，是你们让这个项目从一开始就成为一个可行的现实。」&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Martin 在博客中也表达了对 Linus 的失望：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Rust for Linux 作为一个上游 Linux 项目所遇到的问题已经有详细的记录，我就不在此赘述了。我只想说，我认为 Linus 在处理将 Rust 整合到 Linux 中的问题上是其作为领导者的一大败笔。&lt;strong&gt;这样一个大型项目需要得到主要利益相关者的大力支持才能生存下去，而他的做法似乎只是静观其变&lt;/strong&gt;。&lt;/p&gt; 
 &lt;p&gt;与此同时，在他下游的多个子系统维护者却竭力阻挠或妨碍项目的进行，发出令人无法接受的辱骂，并普遍打击士气。几个月前，一位主要的 Rust for Linux 维护者已经辞职。&lt;/p&gt; 
 &lt;p&gt;当苹果发布 M1 时，Linus Torvalds 希望它能运行 Linux，但并不抱太大希望。我们实现了这一愿望，Linux 5.19 从运行 Asahi Linux 的 M2 MacBook Air 上发布。我曾希望他的热情能转化为对我们社区的支持，并帮助我们解决上游问题。&lt;/p&gt; 
 &lt;p&gt;遗憾的是，这一切都没有实现。2023 年 11 月，我向他发出邀请，与他讨论内核贡献和维护方面的挑战，看看我们能提供什么帮助。他从未回复。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Asahi Linux 博客也&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fasahilinux.org%2F2025%2F02%2Fpassing-the-torch%2F&quot; target=&quot;_blank&quot;&gt;已确认了 Hector 的辞职&lt;/a&gt;，而剩余的开发者计划继续推动 Linux 在 Apple Silicon 硬件上的发展。&lt;/p&gt; 
&lt;p&gt;当前 Asahi Linux 成员包括 Alyssa Rosenzweig、chaos_princess、Davide Cavalca、Neal Gompa、James Calligeros、Janne Grunau 和 Sven Peter。剩余的开发者表示他们仍将专注于将代码提交到 Linux 内核。预计 Apple M3 和 M4 硬件支持将在他们更多的代码被提交到上游以及持续集成取得进展之后才会实现。&lt;/p&gt; 
&lt;p&gt;对于今年的 Apple M1/M2 硬件，他们希望实现 DP Alt Mode、Vulkan 驱动程序中的稀疏图像以及内置麦克风支持。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334317/marcan-resigning-as-asahi-linux-project-lead</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334317/marcan-resigning-as-asahi-linux-project-lead</guid>
            <pubDate>Mon, 17 Feb 2025 06:44:04 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>百度测试社区 APP 「次遇」</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;百度近期正在测试一款名为「次遇」的 App。据悉，这是一款基于兴趣的原创社区 App，产品会在近日上线。 另据企查查显示，百度关联公司正在申请注册相关商标。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-0e92cc7029db9ff18aad093ab328066e5d5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;整个社区的覆盖人群主要以动漫用户、OC 兴趣用户、游戏用户和追星用户为主，性别上，以年轻女用户为主，预计占比 90%。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-15d3184f96d57ee30a3e73f05e989f68ea3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;为了让产品在发展初期能够吸引更多用户，以及保证平台的内容质量，「次遇」邀请了不少高质量的原创作者。一位入驻次遇的二次元画师表示，百度正在从 B 站、LOFTER、抖音、微博和小红书等平台，邀请二次元创作者加入，入驻门槛除了作品质量的要求外，粉丝数也需要几万以上。&lt;/p&gt; 
&lt;p&gt;与此同时，平台还为创作者提供流量扶持、创作激励和个人 IP 孵化。具体实施上，将会提供千万级流量扶持，主要依靠百度系 App 矩阵，包括百度 App、百度地图、贴吧、百度输入法和百度文库。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333783</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333783</guid>
            <pubDate>Fri, 14 Feb 2025 11:44:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>OpenAI 官方发布推理类模型的最佳实践</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;OpenAI 官方博客发布了推理类模型的最佳实践，指导大家如何更好的使用 o1、o3 这类推理模型，当然也可以应用在 deepseek r1 上。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-264a714cd2f2a9ba42f99650890334024a0.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;这里摘录一下比较重要的原则：&lt;/p&gt; 
&lt;h4&gt;⭐&lt;strong&gt;什么时候适合用推理模型？&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;1. 处理模糊任务&lt;/strong&gt;&lt;br&gt; 推理模型特别擅长利用有限的信息或不同的信息片段，并通过简单的提示理解用户的意图，并处理指令中的任何空白。 事实上，推理模型通常会在做出不成熟的猜测或试图填补信息空白之前提出澄清问题。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2. 大海捞针&lt;/strong&gt;&lt;br&gt; 当您传递大量非结构化信息时，推理模型非常擅长理解并仅提取最相关的信息来回答问题。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;3.在大型数据集中寻找关系和细微差别&lt;/strong&gt;&lt;br&gt; 我们发现推理模型特别擅长对具有数百页密集、非结构化信息的复杂文档进行推理——例如法律合同、财务报表和保险索赔。 这些模型特别擅长在文档之间建立联系，并根据数据中未言明的真相做出决策。&lt;br&gt; 推理模型还擅长对细致的政策和规则进行推理，并将它们应用于手头的任务，以得出合理的结论。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;4. 多步骤智能体规划&lt;/strong&gt;&lt;br&gt; 推理模型对于智能体规划和战略制定至关重要。 当推理模型用作「规划者」时，我们已经看到了成功，它可以为问题生成详细的多步骤解决方案，然后根据高智能还是低延迟最重要来选择和分配正确的 GPT 模型（「执行者」）用于每个步骤。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;5.视觉推理 （o1、QvQ 等视觉推理模型专享功能）&lt;/strong&gt;&lt;/p&gt; 
&lt;h4&gt;⭐&lt;strong&gt;怎么有效地用推理模型？&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;1. &lt;strong&gt;保持提示简单直接&lt;/strong&gt;： 这些模型擅长理解和响应简洁、清晰的指令。&lt;br&gt; 2. &lt;strong&gt;避免思维链提示&lt;/strong&gt;： 由于这些模型在内部执行推理，因此提示它们「逐步思考」或「解释你的推理过程」是不必要的。&lt;br&gt; 3. &lt;strong&gt;使用分隔符以提高清晰度&lt;/strong&gt;： 使用分隔符（如 Markdown、XML 标签和章节标题）来清楚地指示输入的不同部分，这有助于模型正确地解释各个部分。&lt;br&gt; 4. &lt;strong&gt;首先尝试零样本&lt;/strong&gt;，如果需要再尝试少样本： 推理模型通常不需要少样本示例（few-shot examples）就能产生好的结果，所以首先尝试编写没有示例的提示。 如果你对期望的输出有更复杂的要求，在提示中包含一些输入和期望输出的示例可能会有所帮助。但要确保示例与你的提示指令非常一致，因为两者之间的差异可能会导致不良结果。&lt;br&gt; 5. &lt;strong&gt;提供具体的指导方针&lt;/strong&gt;： 如果你想明确地限制模型的响应（例如「提出一个预算低于 500 美元的解决方案」），请在提示中明确地列出这些约束条件。&lt;br&gt; 6. &lt;strong&gt;非常明确地说明你的最终目标&lt;/strong&gt;： 在你的指令中，尝试为成功的响应提供非常具体的参数，并鼓励模型持续推理和迭代，直到符合你的成功标准。&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;原文：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fplatform.openai.com%2Fdocs%2Fguides%2Freasoning-best-practices&quot; target=&quot;_blank&quot;&gt;https://platform.openai.com/docs/guides/reasoning-best-practices&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333767/reasoning-best-practices-by-openai</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333767/reasoning-best-practices-by-openai</guid>
            <pubDate>Fri, 14 Feb 2025 09:36:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>DeepSeek 等大模型私有化服务器快速上升，近九成在裸奔</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;经济参考网报道称，随着 DeepSeek 大模型的迅速流行，越来越多的公司和个人选择将该开源大模型私有化部署。奇安信资产测绘鹰图平台监测发现，8971 个 Ollama 大模型服务器中，有 6449 个活跃服务器，其中 88.9% 都「裸奔」在互联网上，导致任何人不需要任何认证即可随意调用、在未经授权的情况下访问这些服务，有可能导致数据泄露和服务中断，甚至可以发送指令删除所部署的 DeepSeek、Qwen 等大模型文件。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;公开信息显示，运行 DeepSeek R1 大模型的服务器正在快速上升，上述 8971 个服务器中有 5669 个在中国。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;308&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-621c404ca9e7daa19db425427e2fa23b30b.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;span style=&quot;color:#000000&quot;&gt;奇安信资产测绘鹰图平台显示大概有 8971 个 IP 运行了 Ollama&lt;/span&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;为了应对这些问题，专家建议，所有部署 DeepSeek 服务的企业和个人应立即采取有效的安全防护措施。具体措施如下：&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color:#000000&quot;&gt;尽快修改配置&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;建议立即修改 Ollama 配置，加入身份认证手段。同时及时修改防火墙、WAF、入侵检测等相关安全配置，例如制定 IP 白名单限制访问，确保只有授权人员能够访问模型服务。定期检查和关闭不必要的端口、限制计算资源的使用、加强监控等措施也是提高安全性的关键。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color:#000000&quot;&gt;确保数据传输加密&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;在大模型运行中，需要对所有传输的数据进行加密，避免在遭遇攻击及数据窃取时泄露敏感信息。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color:#000000&quot;&gt;部署专业安全产品&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;通过部署奇安信大模型衞士等产品，可以有效的抵御针对应用服务的传统网络攻击，尤其对大模型应用特有的越狱、提示词注入等攻击进行全面有效的防护；部署奇安信 API 安全衞士等产品，对大模型应用的 API 接口访问做好全面监测与防护。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;此外，个人用户更应警惕不知名厂商提供的 DeepSeek 大模型服务，一些不良厂商使用被盗资源对外售卖，骗取钱财的同时，还可实时监控用户提交的所有数据，可造成隐私泄露。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;随着大模型技术的不断发展，安全问题将变得愈发复杂。行业专家呼吁，使用 DeepSeek 及类似大模型的用户应尽快采取预防措施，确保技术的安全部署和稳定运行。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333761</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333761</guid>
            <pubDate>Fri, 14 Feb 2025 09:18:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>Ubuntu 24.04.2 LTS 延期至下周发布</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p style=&quot;color:#000000; text-align:start&quot;&gt;Ubuntu 24.04.2 LTS 及其衍生版本原定于本周四发布，但一个临时的技术问题导致此次发布&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flists.ubuntu.com%2Farchives%2Fubuntu-release%2F2025-February%2F006310.html&quot; target=&quot;_blank&quot;&gt;被推迟&lt;/a&gt;。&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0214/163047_Nn1o_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;由于某些 Ubuntu 24.04.2 LTS 版本在制作时&lt;strong&gt;没有包含硬件启用「HWE」内&lt;/strong&gt;核，Ubuntu 24.04.2 LTS 的发布被推迟了一周，以便有时间重新制作。&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;Ubuntu 24.04.2 的重要意义在于它是 Ubuntu 24.04 LTS 系列中第一个采用 HWE 内核的版本，而 HWE 内核是 Ubuntu 24.10 的向后移植内核和其他组件。由于 Ubuntu 24.04.2 LTS 具有 Linux 6.11 内核选项和其他硬件驱动程序升级，它比一年前 Ubuntu 24.04 推出时提供了更好的硬件支持。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333744/ubuntu-24-04-2-lts-delayed</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333744/ubuntu-24-04-2-lts-delayed</guid>
            <pubDate>Fri, 14 Feb 2025 08:32:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>中国信通院：正式启动 DeepSeek 国产化适配测评工作</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;中国信息通信研究院（简称「中国信通院」）宣布正式启动 DeepSeek 国产化适配测评工作，旨在为 DeepSeek 系列模型在多硬件多场景下的适配部署提供参考。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;一是评价模型在包括硬件芯片、计算设备、智算集群等软硬件系统中的适配效果；二是反映模型在软硬件系统适配过程中软件栈及工具的适配易用性及开发部署成本。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;本次测评将依托由中国信通院人工智能软硬件协同创新与适配验证中心（亦庄）、人工智能关键技术和应用评测工业和信息化部重点实验室联合推进的 AISHPerf（Performance Benchmarks of Artificial Intelligence Software and Hardware,以下简称 AISHPerf）人工智能软硬件基准体系及测试工具，面向包括芯片、服务器、集群、开发框架及工具链、智算设施及平台等在内的人工智能软硬件产品及系统开展。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;测试将主要围绕表 1 所示的 DeepSeek 不同模态、不同尺寸的系列模型，面向推理、微调、训练过程，低成本使用测试工具 AISHPerf，从适配成本、功能完备性、优化效果、性能指标等多方面开展测试评估。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;img height=&quot;172&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-f31d6d384d3e96b7b27f04655df7e6d0a52.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.qq.com%2Fform%2Fpage%2FDVnh1bVFndmt1bnFM%3Fu%3Df50706e2c08c43acb447aeaa0ec26470%23%2Ffill&quot; target=&quot;_blank&quot;&gt;报名表&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333734</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333734</guid>
            <pubDate>Fri, 14 Feb 2025 08:07:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>GNOME 官网全新改版</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;GNOME 全新官网已&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.gnome.org%2F&quot; target=&quot;_blank&quot;&gt;上线&lt;/a&gt;&lt;/u&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;screenshot&quot; src=&quot;https://static.oschina.net/uploads/img/202502/17142818_JmM4.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;新的设计看起来既时尚又现代，它简化了头部设计、空间更宽敞，色彩更鲜艳，还有简单而有效的动画，等等，比之前（相对单调）的旧版本更能传达 GNOME 充满活力、以用户为中心的理念。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://static.oschina.net/uploads/img/202502/17142819_RWzj.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;文档方面，GNOME 开发文档和设计指南现在各自拥有专门的章节，并附上了相关链接，还有一个部分展示了支持 GNOME 的组织列表（包括 Canonical），以强调 GNOME 在更广泛的 Linux 生态中扮演的关键角色。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://static.oschina.net/uploads/img/202502/17142820_bW25.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;详情访问 GNOME 官网：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.gnome.org%2F&quot; target=&quot;_blank&quot;&gt;https://www.gnome.org/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334308/gnome-website-revamp-goes-live</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334308/gnome-website-revamp-goes-live</guid>
            <pubDate>Sat, 08 Feb 2025 06:28:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>「天工」成为全球首例登百级台阶的人形机器人</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;国地共建具身智能机器人创新中心宣布，在户外真实地形测试中，「天工」机器人连续攀爬多级阶梯，成功登上北京通州区海子墙公园最高点，成为全球首例可在室外连续攀爬多级阶梯的人形机器人。&lt;/span&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;国创中心持续提升具身小脑能力，实现了基于视觉的感知行走，可实现无磕碰、不踩棱、不踏空地跨越连续多级楼梯和 35 厘米大高差台阶，奔跑时速提高至 12km/h，并且能在雪地进行高速奔跑，同时具备更强的抗干扰能力，大外力冲击下仍可保持平衡。应对复杂地形的移动能力提升，将成为人形机器人走出实验室，在真实环境执行任务，甚至在山地、雪地救援、废墟等极端环境下作业的基础，为具身智能机器人规模化应用夯实技术底座。&lt;/span&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;282&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-fb4868df102f12e166908215d6ce18410f2.gif&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;此外，升级后的「天工」能够轻松应对超 10KG 重物落下所造成的高达 45Ns 冲量，相当于一名职业拳击手以 450 N 的力，重击对手的一瞬间打出的力道，即使在光滑的雪地上从各个方向突然出现的各类干扰等，「天工」均能保持稳定平衡不发生摔倒，达到业内领先水平。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;通过具身小脑所带来的全身控制能力升级，「天工」面对复杂环境的移动能力再次大幅提升，首次真正发挥出双足结构为人形机器人带来的多地形通用性优势，在实现全地形场景技术闭环的同时，更为行业确立了复杂环境移动能力的全新标杆。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;未来，该技术也将纳入国创中心所打造的开源开放生态汇总，通过技术共享降低行业创新门槛将加速具身智能机器人在千行百业的规模化落地，为具身智能产业化开辟更具想象力的落地路径。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color:#000000&quot;&gt;相关阅读：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/news/289801&quot; target=&quot;_blank&quot;&gt;北京人形机器人创新中心发布全球首个纯电驱拟人奔跑的全尺寸人形机器人 「天工」&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334302</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334302</guid>
            <pubDate>Sat, 08 Feb 2025 06:00:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>ReasonFlux：通过分层模板缩放提升 LLM 推理</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;大型语言模型（LLMs）已经展现出了卓越的问题解决能力，然而，复杂的推理任务——例如竞技级别的数学问题或复杂的代码生成——仍然具有挑战性。这些任务需要精确地穿越庞大的解空间，并进行细致的逐步思考。现有的方法虽然在提高准确性方面有所改进，但往往面临着高计算成本、僵化的搜索策略以及难以跨不同问题进行泛化的难题。&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.marktechpost.com%2F2025%2F02%2F15%2Freasonflux-elevating-llm-reasoning-with-hierarchical-template-scaling%2F&quot; target=&quot;_blank&quot;&gt;https://www.marktechpost.com/2025/02/15/reasonflux-elevating-llm-reasoning-with-hierarchical-template-scaling/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;在这篇论文中，研究人员介绍了一个新的框架，&lt;strong&gt;ReasonFlux&lt;/strong&gt;，它通过重新构想 LLMs 如何使用分层、模板引导的策略来规划和执行推理步骤，从而解决了这些局限性。 最近用于增强大型语言模型推理的方法分为两大类：&lt;em&gt;深思熟虑的搜索_和_奖励引导的方法&lt;/em&gt;。像思维树（ToT）这样的技术使 LLM 能够探索多个推理路径，而蒙特卡洛树搜索（MCTS）则将问题分解为步骤，这些步骤由过程奖励模型（PRM）引导。&lt;/p&gt; 
&lt;p&gt;尽管这些方法有效，但由于采样过多和手动搜索设计，它们的可扩展性较差。例如，MCTS 需要遍历成千上万的潜在步骤，这使得它在实际应用中计算成本过高。与此同时，像思维缓冲（BoT）这样的检索增强生成 RAG 方法利用存储的问题解决模板，但在适应性地整合多个模板方面存在困难，这限制了它们在复杂场景中的效用。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;1066&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0217/135909_MMin_3820517.png&quot; width=&quot;1750&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;ReasonFlux 引入了一个结构化的框架，该框架结合了精选的高层次思维模板库与分层强化学习（HRL），以动态规划和优化推理路径。它不是优化单个步骤，而是专注于配置最优的 &lt;em&gt;模板轨迹&lt;/em&gt;——从结构化知识库中检索出的抽象问题解决策略序列。这种方法简化了搜索空间，并使高效适应子问题成为可能。该框架由三个主要组件组成：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;结构化模板库&lt;/strong&gt;：研究团队构建了一个包含 500 个思维模板的库，每个模板封装了一种问题解决策略（例如，「三角代换优化积分」）。模板包含元数据——名称、标签、描述和应用步骤——以实现高效的检索。例如，一个标记为「有理函数优化」的模板可能会指导大型语言模型（LLM）应用特定的代数替换。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;分层强化学习&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;基于结构的微调&lt;/strong&gt;：将基本 LLM（例如，Qwen2.5-32B）微调以将模板元数据与其功能描述关联起来，确保它理解何时以及如何应用每个模板。&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;模板轨迹优化&lt;/strong&gt;：利用偏好学习，该模型学会根据效果对模板序列进行排序。对于给定的问题，会采样多个轨迹，并根据它们在类似问题上的成功率来确定奖励。这训练模型优先考虑高奖励序列，从而提高其规划能力。&lt;/p&gt; &lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;自适应推理缩放&lt;/strong&gt;：在推理过程中，ReasonFlux 充当「导航员」，分析问题以检索相关模板，并根据中间结果动态调整轨迹。例如，如果一个涉及「多项式因式分解」的步骤产生了意外的约束，系统可能会转向「约束传播」模板。这种规划和执行之间的迭代互动反映了人类的解决问题方式，其中部分解决方案会指导后续步骤。&lt;/p&gt; &lt;img height=&quot;376&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0217/135928_eZy6_3820517.png&quot; width=&quot;1686&quot; referrerpolicy=&quot;no-referrer&quot;&gt; &lt;p&gt;&amp;nbsp;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;ReasonFlux 在 MATH、AIME 和 OlympiadBench 等竞争级基准测试中进行了评估，超越了前沿模型（GPT-4o、Claude）以及专业开源模型（DeepSeek-V3、Mathstral）。关键结果包括：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;MATH 准确率达到 91.2%，超过 OpenAI 的 o1-preview 6.7%。&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;AIME 2024 准确率为 56.7%，超出 DeepSeek-V3 45%，与 o1-mini 相当。&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;OlympiadBench 准确率为 63.3%，比先前方法提高了 14%。&lt;/strong&gt;此外，结构化模板库展示了强大的泛化能力：当应用于不同的问题时，它将小型模型（例如，7B 参数）的能力提升至能够通过直接推理超越大型模型。此外，ReasonFlux 实现了更好的探索-利用平衡，在复杂任务上比 MCTS 和 Best-of-N 需要少 40% 的计算步骤（见图 5）。 总结来说，ReasonFlux 重新定义了 LLMs 处理复杂推理的方式，通过将高级策略与逐步执行解耦。其分层模板系统减少了计算开销，同时提高了准确性和适应性，解决了现有方法中的关键差距。通过利用结构化知识和动态规划，该框架为高效、可扩展的推理设定了新的标准——证明即使是小型、有良好指导的模型也能与最大的前沿系统相媲美。这一创新为在资源受限的环境中部署高级推理开辟了道路，从教育到自动化代码生成。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334301/reasonflux-llm</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334301/reasonflux-llm</guid>
            <pubDate>Sat, 08 Feb 2025 06:00:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>微软开源「专业领域知识-推理能力 RAG」 —— PIKE-RAG</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;近年来，大语言模型（LLM）凭借强大的文本生成能力在各个领域引起了广泛关注。它们不仅能写文章、翻译语言，还能完成创作任务。但当遇到需要专业领域知识支持的工业级问题时，比如半导体设计、制药研发或法律条文解读，这些模型往往力不从心。这不仅因为训练数据中缺少足够的专业信息，还因为单靠「生成」能力，难以构建严谨的逻辑推理和多层次的信息整合。&lt;/p&gt; 
&lt;h2&gt;为什么传统方法会遇到瓶颈？&lt;/h2&gt; 
&lt;p&gt;目前，为了解决这一问题，业界提出了「检索增强生成」（Retrieval-Augmented Generation，简称 RAG）的思路。其核心理念是在生成答案之前，先从一个庞大的外部知识库中检索出相关信息，再将这些信息融入生成的上下文中，从而使回答更准确、更有事实依据。&lt;/p&gt; 
&lt;p&gt;然而，传统 RAG 方法存在以下几个问题：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;知识来源复杂&lt;/strong&gt;：现实中的数据不仅仅是纯文本，还包括表格、图表、图片等多种格式。单一的文本检索难以捕捉这些多样信息。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;专业领域知识不足&lt;/strong&gt;：工业应用中的专业知识具有特定术语和逻辑，普通模型难以准确提取和理解，从而导致回答不够严谨。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;「一刀切」的策略&lt;/strong&gt;：不同类型的问题（如简单事实问答与需要多步推理的复杂问题）要求不同的处理策略，而传统方法往往采用统一流程，无法兼顾所有需求。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;PIKE-RAG 的创新之处&lt;/h2&gt; 
&lt;p&gt;为了解决上述不足，微软亚洲研究院提出了 PIKE-RAG —— 一种专注于「知识」和「推理」增强的生成框架。PIKE-RAG 不仅帮助模型检索相关知识，更注重如何理解、拆解和合理组织这些信息，从而构建出严谨的推理链。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;788&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0217/121127_pF8y_3820517.png&quot; width=&quot;2072&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;792&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0217/121200_n64m_3820517.png&quot; width=&quot;2058&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;810&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0217/121040_68c7_3820517.png&quot; width=&quot;2088&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;下面我们来看看它的核心设计：&lt;/p&gt; 
&lt;h3&gt;1. 分级任务设计&lt;/h3&gt; 
&lt;p&gt;论文将问题大致分为四类：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;事实型问题&lt;/strong&gt;：例如「这款 LED 产品的额定电流是多少？」&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;链式推理问题&lt;/strong&gt;：需要跨多个信息点进行关联，比如比较多个产品的性能。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;预测型问题&lt;/strong&gt;：例如「未来 5 年半导体技术可能有哪些突破？」&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;创造型问题&lt;/strong&gt;：要求模型发挥创造力，提出新见解。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;这种分类使得系统能根据问题的难度和性质，采用针对性的处理策略，从而「量体裁衣」地提升答案的准确性和逻辑性。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;1246&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0217/120535_p84F_3820517.png&quot; width=&quot;1124&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;2. 知识「原子化」与任务分解&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;知识原子化&lt;/strong&gt;：面对复杂问题，系统会将长文档或复杂数据拆分成最基本的信息单元（知识原子）。这种拆分类似于把大问题拆成小问题，每个小单元便于独立检索和理解。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;知识感知的任务分解&lt;/strong&gt;：系统根据问题需求，动态分解任务，并利用已提取的知识原子构建逻辑推理链。这样一来，即使是多步推理的问题，系统也能循序渐进地「拼凑」出最终答案。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;任务分解器训练&lt;/strong&gt;：为实现高效分解，系统还引入了可训练的任务分解模块，通过大量领域数据学习如何将问题正确拆解并合理组合各个知识点。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;3. 分阶段系统构建&lt;/h3&gt; 
&lt;p&gt;PIKE-RAG 采用了分阶段的开发策略，逐步提升系统的处理能力：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;初级阶段&lt;/strong&gt;：专注于构建一个多模态知识库。系统会从文本、表格、图像等多种格式中抽取信息，并利用解析算法将它们统一组织成一个结构化、关联紧密的知识网络。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;中级阶段&lt;/strong&gt;：在事实型问题上引入多粒度检索技术，结合增强型文本切分和自动标记机制，确保能精确提取出关键信息。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;高级阶段&lt;/strong&gt;：逐步引入链式推理模块、知识原子化处理和任务分解器，使系统不仅能够检索信息，更能在多跳推理、预测和创造性回答等复杂任务中表现优异。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;实现原理：如何让系统「知晓」与「推理」&lt;/h2&gt; 
&lt;p&gt;在 PIKE-RAG 系统中，设计者采用了层次化、分阶段的实现策略，确保系统能逐步提升对复杂问题的处理能力。下面详细介绍各个主要环节的实现原理：&lt;/p&gt; 
&lt;h3&gt;1. 知识库构建（Level-0）&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;文件解析&lt;/strong&gt;：系统首先从各种格式的数据中抽取信息，将非结构化数据（如扫描文档、表格、图片中的文字）经过专门算法转换为统一的文本数据。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;知识组织&lt;/strong&gt;：解析后的信息被组织成一个多层次的异构图，各类数据节点（例如产品技术规格、图表、说明文字等）通过超链接、引用关系等方式互相连接，形成结构化的知识库，便于后续的高效检索和利用。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;2. 专门模块针对不同问题&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;事实型问题模块（Level-1）&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;增强型切分与自动标记&lt;/strong&gt;：长文档被切分成更小的信息块，并自动为每个信息块打上标签，以便在检索时更精确地匹配查询内容。&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;多粒度检索&lt;/strong&gt;：系统在检索时不仅搜索全文，还能在不同层级和粒度上查找相关信息，提高检索的准确性。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;链式推理问题模块（Level-2）&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;知识原子化&lt;/strong&gt;：将大块复杂知识拆解成最小的基本单元，使得每个单元都能独立检索并参与推理。&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;任务分解&lt;/strong&gt;：针对复杂问题，系统动态分解成多个子任务，每个子任务依次解决后再组合成最终答案。&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;训练可调的任务分解器&lt;/strong&gt;：通过大量领域数据训练，系统学会如何针对不同专业问题设计合适的分解策略和推理流程。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;预测型与创造型问题模块（Level-3 &amp;amp; Level-4）&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;在高级阶段，系统不仅能处理已知信息，还能在已有数据基础上推演预测未来趋势或提出创造性观点，从而满足更高层次的应用需求。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img height=&quot;684&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0217/120551_g5tH_3820517.png&quot; width=&quot;828&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;3. 分阶段开发策略&lt;/h3&gt; 
&lt;p&gt;整个系统从构建基础知识库开始，逐步引入不同层次的检索与推理模块。每个阶段的开发都以解决特定问题为目标：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;初级阶段&lt;/strong&gt;确保系统在简单事实问答上表现出色；&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;中级阶段&lt;/strong&gt;引入多跳推理和任务分解，处理更复杂的问题；&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;高级阶段&lt;/strong&gt;则针对预测和创造性任务进行优化，使系统具备更强的灵活性和适应性。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;实验效果与应用前景&lt;/h2&gt; 
&lt;p&gt;通过大量实验验证，PIKE-RAG 在开放领域和法律领域的问答任务中均展现了卓越的性能。得益于知识原子化、任务分解以及多粒度检索技术，该系统在处理多步推理和复杂查询时表现尤为出色。这不仅为工业级问答系统的发展提供了新思路，也为未来在更多复杂场景中的应用奠定了基础。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2FPIKE-RAG&quot; target=&quot;_blank&quot;&gt;https://github.com/microsoft/PIKE-RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fpike-rag.azurewebsites.net%2F&quot; target=&quot;_blank&quot;&gt;https://pike-rag.azurewebsites.net&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334286/microsoft-pike-rag</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334286/microsoft-pike-rag</guid>
            <pubDate>Sat, 08 Feb 2025 04:12:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>OBS Studio 批评 Fedora 的 Flatpak 打包，称其是恶意分支</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;开源屏幕录制和直播应用 OBS Studio 近日向 Fedora 提出了批评，指出它对该应用程序的 Flatpak 打包存在问题，并威胁说如果不加以解决，将采取法律行动。&lt;/p&gt; 
&lt;p&gt;三周前 OBS Studio 团队就提交了&amp;nbsp;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgitlab.com%2Ffedora%2Fsigs%2Fflatpak%2Ffedora-flatpaks%2F-%2Fissues%2F39%23note_2344970813&quot; target=&quot;_blank&quot;&gt;Fedora Flatpak SIG 工单&lt;/a&gt;&amp;nbsp;—— 关于 Fedora 提供「损坏」的 OBS Studio Flatpak 被视为官方软件包：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;「Fedora Flatpaks 应用商店提供的非官方 OBS Studio Flatpak 似乎打包不佳且已损坏，导致用户向上游投诉，因为他们认为这是 OBS Studio 的官方软件包。这种情况在 OBS Studio 之外也存在多个例子，许多用户对 Fedora Flatpaks 被强制推广，缺少或没有明确的选项退出感到不满。&lt;/p&gt; 
 &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgitlab.gnome.org%2FGNOME%2Fgnome-software%2F-%2Fissues%2F2754&quot; target=&quot;_blank&quot;&gt;https://gitlab.gnome.org/GNOME/gnome-software/-/issues/2754&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fpagure.io%2Ffedora-workstation%2Fissue%2F463&quot; target=&quot;_blank&quot;&gt;https://pagure.io/fedora-workstation/issue/463&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;我们希望请求将该软件包移除，或者明确指出它是一个第三方软件包。&lt;strong&gt;确保下游软件包正常工作不应是上游的责任，尤其是当它们覆盖官方软件包时&lt;/strong&gt;。&lt;/p&gt; 
 &lt;p&gt;我还想了解为什么有人认为将一个运行得非常完美的 Flatpak 版本破坏后，以更高的优先级发布到我们的官方构建中是一个好主意。我们在官方 Flatpak 上投入了大量的努力，以确保它们在 Flathub 上发布时能尽可能地正常运行。」&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;但然后在过去的一天里，Fedora 不但没有删除，似乎还和 OBS Studio 团队对骂起来，这让后者非常不爽，因此认定 Fedora Flatpak 上的 OBS Studio 是个恶意分支，并威胁采取法律行动：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;由于目前很明显 Fedora 对此没有兴趣进行理性讨论，并决定诉诸于人身攻击，我们现在将 Fedora Flatpaks 分发的 OBS Studio 视为恶意分支。&lt;/p&gt; 
 &lt;p&gt;这是一个正式请求，要求从您的分发中移除我们所有的品牌标识，包括但不限于我们的名称、我们的标志、属于 OBS 项目的任何附加知识产权。&lt;/p&gt; 
 &lt;p&gt;如果不遵守，可能会导致采取进一步的法律行动。我们期望在接下来的 7 个工作日内收到回复（截至 2025 年 2 月 21 日星期五）。&lt;/p&gt; 
&lt;/blockquote&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334282/obs-studio-poor-fedora-flatpak</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334282/obs-studio-poor-fedora-flatpak</guid>
            <pubDate>Sat, 08 Feb 2025 03:54:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>2024 年 Rust 社区调查报告</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;Rust 调查团队很高兴与大家分享我们关于 Rust 编程语言的 2024 年调查结果，该调查于 2024 年 12 月 5 日至 2024 年 12 月 23 日进行。与往年一样，2024 年的 Rust 状态调查旨在收集 Rust 用户以及更广泛地关注 Rust 未来的所有人的见解和反馈。&lt;/p&gt; 
&lt;p&gt;这份调查的第九版揭示了来自全球 Rust 语言社区的全新见解和学习机会，以下我们将进行总结。除了这篇博客文章外，&lt;strong&gt;我们还&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fraw.githubusercontent.com%2Frust-lang%2Fsurveys%2Fmain%2Fsurveys%2F2024-annual-survey%2Freport%2Fannual-survey-2024-report.pdf&quot; target=&quot;_blank&quot;&gt;准备了一份报告&lt;/a&gt;&lt;/u&gt;&lt;/strong&gt;，其中包含了调查中所有问题的汇总结果图表。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;我们对每一位在过去一年中抽出时间表达对 Rust 看法和体验的社区成员表示最诚挚的感谢。您的参与将帮助我们使 Rust 对每个人来说都变得更好。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;下文包含了大量数据，所以请坐稳，享受阅读！&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;参与&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111550_gaCL_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;如上图所示，2024 年，我们收到的调查查看次数比上一年少。这可能是由于调查仅进行了两周，而上一年调查进行了近一个月。然而，完成率也有所下降，这似乎表明调查可能有点太长了。我们将考虑这一点，为下一次调查的版本进行调整。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;社区&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Rust 状态调查不仅为我们提供了关于世界各地有多少 Rust 用户在使用和体验该语言的宝贵见解，而且还让我们了解了我们全球社区的结构。这些信息让我们了解到语言的使用情况以及随着时间的推移，我们可能需要解决的接入差距。我们希望这些数据和我们的相关分析能进一步促进关于我们如何继续优先考虑 Rust 社区的全球接入和包容性的重要讨论。&lt;/p&gt; 
&lt;p&gt;与往年一样，我们询问了受访者他们居住在哪个国家。排名前十的国家依次是：美国（22%）、德国（14%）、英国（6%）、法国（6%）、中国（5%）、加拿大（3%）、荷兰（3%）、俄罗斯（3%）、澳大利亚（2%）和瑞典（2%）。我们很高兴看到 Rust 受到世界各地用户的喜爱！您可以在下面的图表中尝试找到您的国家：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111604_Xkme_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;我们也询问了受访者是否认为自己属于一个边缘化社区的一员。在回答者中，74.5% 选择了「否」，15.5% 选择了「是」，10% 选择不愿意透露。&lt;/p&gt; 
&lt;p&gt;我们询问了选择「是」的群体，他们将自己识别为哪些特定群体的成员。将自己视为技术领域中被代表性不足或边缘化群体成员的大多数人将自己识别为女同性恋、男同性恋、双性恋或其他非异性恋。其次是神经多样性群体，占比 46%，其次是跨性别群体，占比 35%。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111617_hrzo_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;每年，我们必须承认 Rust 社区和开源整体在多样性、公平性和包容性（DEI）方面的差距。我们相信，Rust 基金会在推进 Rust 社区聚会全球访问和在每个周期向多元化的维护者群体分配补助金方面正在开展出色的工作，您可以在这里了解更多信息。即便如此，全球包容性和访问性只是 DEI 的一个要素，调查工作组将继续在这个领域推动进步。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Rust 使用情况&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;自认是 Rust 用户的人数与去年相当，大约为 92%。这个高比例并不令人惊讶，因为我们主要针对现有的 Rust 开发者进行这项调查。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111627_7qhc_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;同样地，像去年一样，大约 31% 的未将自己标识为 Rust 用户的人士将难度感知作为不使用 Rust 的主要原因。不使用 Rust 的最常见原因是受访者们还没有机会尝试它。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111639_1Cns_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;在参与 2024 年调查的前 Rust 用户中，36% 的人士将不可控因素列为他们不再使用 Rust 的原因，这比去年下降了 10 个百分点。&lt;/p&gt; 
&lt;p&gt;今年，我们还询问受访者如果有机会，他们是否会考虑再次使用 Rust，结果发现很大一部分受访者（63%）会这么做。这真是令人欣慰！&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111652_RnJ5_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;标记为 N/A 的封闭答案在调查的前一个版本中并未出现。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;不再使用 Rust 的人告诉我们，这主要是因为他们实际上并不需要它（或他们公司的目标发生了变化），或者因为它不是这项工作的合适工具。少数人报告称，他们被这种语言或其生态系统整体所压倒，或者认为转向或引入 Rust 在人力成本上过于昂贵。&lt;/p&gt; 
&lt;p&gt;在 2024 年使用 Rust 的人中，有 53% 的人是每天（或几乎每天）使用它——比上一年增加了 4 个百分点。我们可以观察到，在过去的几年中，Rust 的使用频率呈上升趋势，这表明 Rust 在工作场所的使用越来越多。这一点也由下文「Rust at Work」部分中提到的其他答案所证实。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111737_L7g6_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Rust 的专业技能在我们的受访者中也持续增长！20% 的受访者能够编写（仅）简单的 Rust 程序（相比 2023 年下降了 3 个百分点），而 53% 的人认为自己使用 Rust 是高效的——这一比例在 2023 年为 47%。虽然这项调查只是衡量 Rust 整体技能变化的一个工具，但这些数字令人鼓舞，因为它们代表了每年回归调查的许多 Rustaceans 的知识增长。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111747_VI2v_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;不出所料，最受欢迎的 Rust 版本是最新稳定版，无论是最新版本还是与用户的 Linux 发行版一起提供的版本。几乎三分之一的用户也使用最新的夜间版本，由于各种原因（见下文）。然而，似乎 beta 工具链的使用并不多，这有点遗憾。我们希望鼓励 Rust 用户更多地使用 beta 工具链（例如在 CI 环境中），以帮助测试即将稳定化的 Rust 版本。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111759_RPoz_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;人们使用夜间工具链主要是为了获取特定的不稳定语言功能。也有几位用户提到，他们对夜间版本的 rustfmt 更满意，或者他们使用夜间编译器是因为编译速度更快。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111809_jJfZ_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;学习 Rust&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;要使用 Rust，程序员首先必须学习它，所以我们总是对他们是怎样学习的很感兴趣。根据调查结果，似乎大多数用户通过 Rust 文档以及《Rust 编程语言》这本书来学习，这本书长期以来一直是新 Rustaceans 最喜欢的学习资源。许多人似乎也通过阅读 Rust crate 的源代码来学习。事实上，成千上万 Rust crate 的文档和源代码都可在 docs.rs 和 GitHub 上找到，这使得学习变得更加容易。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111822_KHvR_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;关于属于「其他」类别的回答，它们可以归纳为三个类别：使用 LLM（大型语言模型）助手（如 Copilot、ChatGPT、Claude 等）、阅读官方 Rust 论坛（Discord、URLO）或在贡献 Rust 项目时接受指导的人。我们想向那些使我们的空间对新来者友好和欢迎的人表示衷心的感谢，因为这是一项重要的工作，而且它是有回报的。有趣的是，相当数量的人通过「做中学」来学习，并使用 rustc 错误信息和 clippy 作为指南，这是 Rust 诊断质量的良好指标。&lt;/p&gt; 
&lt;p&gt;至于正规教育，似乎 Rust 尚未渗透到大学课程中，因为这是一个通常发展缓慢的领域。只有极少数受访者（大约 3%）曾上过大学的 Rust 课程或使用过大学学习材料。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111833_l4Zn_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;编程环境&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;关于 Rustaceans 使用的操作系统，Linux 是最受欢迎的选择，而且它似乎每年都在变得越来越受欢迎。其次是 macOS 和 Windows，它们的使用份额非常相似。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111844_giEi_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-9695c9f7f5975eb79647c3db5c61467af25.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
 &lt;p&gt;顺便提一下，如您在词云中看到的，还有一些用户更喜欢 Arch。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Rust 程序员使用他们的 Rust 程序针对一系列的平台。我们发现针对嵌入式和移动平台的目标用户有所增加，但除此之外，平台分布与去年大致相同。由于 WebAssembly 目标相当多样化，我们这次将其分为两个单独的类别。根据结果，很明显，在使用 WebAssembly 时，它主要是在浏览器（23%）的上下文中，而不是其他用例（7%）。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111901_JLWt_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;当然，我们不能忘记许多程序员最喜爱的主题：他们使用哪个 IDE（开发环境）。尽管 Visual Studio Code 仍然是最受欢迎的选择，但今年的市场份额下降了 5 个百分点。另一方面，Zed 编辑器似乎最近获得了相当大的关注度。选择「其他」的少数人正在使用各种各样的不同工具：从 CursorAI 到经典如 Kate 或 Notepad++。特别提一下使用「ed」的 3 个人，这真是一项了不起的成就。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111912_hDw1_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-e87b497c85dbd1dced8a457b81fc3d05e46.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
 &lt;p&gt;您还可以查看词云，它总结了对此问题的开放性回答（「其他」类别），以了解其他哪些编辑器也受欢迎。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Rust 在工作中的使用&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;我们很高兴看到越来越多的人在工作时使用 Rust 进行大部分编码，从去年的 34% 上升到 38%。在过去几年中，这一指标呈现出明显的上升趋势。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111924_MVQm_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Rust 在公司中的使用似乎也在增加，因为 45% 的受访者表示他们的组织在 Rust 上的使用并非微不足道，这比 2023 年增加了 7 个百分点。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111934_YhGk_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;再次强调，我们调查受访者雇主投资 Rust 的首要原因是可以构建相对正确且无 bug 的软件。其次受欢迎的原因是 Rust 的性能特性。21% 在工作中使用 Rust 的受访者这么做是因为他们已经熟悉它，因此它是他们的默认选择，比 2023 年增加了 5 个百分点。这似乎表明，Rust 正成为越来越多公司选择的基础语言之一。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111945_PfzP_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;与上一年相似，很大比例的受访者（82%）报告说 Rust 帮助他们的公司实现了目标。总的来说，似乎程序员和公司对他们在 Rust 上的使用感到非常满意，这真是太好了！&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/111956_tivA_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;在技术领域，情况与前一年相当相似。Rust 似乎特别受欢迎，用于创建服务器后端、Web 和网络服务以及云计算技术。它似乎也在嵌入式用例方面获得了更多的关注。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/112012_w7WV_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;您可以向右滚动图表以查看更多领域。请注意，在 2023 年的调查中，汽车领域并未作为封闭答案提供（它只是通过开放式答案输入的），这或许可以解释为什么会有如此大的跳跃。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;看到专业 Rust 使用的持续增长以及许多用户对其性能、控制、安全性、安全性、愉悦性等方面的信心，这令人兴奋！&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;挑战&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;正如往常一样，State of Rust 调查的主要目标之一是揭示过去一年 Rustaceans 心中的挑战、担忧和优先事项。&lt;/p&gt; 
&lt;p&gt;我们询问了用户关于限制他们生产力的 Rust 方面。不出所料，缓慢的编译速度位列榜首，这似乎一直是 Rust 用户的永久性担忧。一如既往，有努力正在进行中以提高编译器的速度，例如启用并行前端或默认切换到更快的链接器。我们邀请您测试这些改进，并告诉我们如果您遇到任何问题。&lt;/p&gt; 
&lt;p&gt;其他挑战包括对 Rust 调试的支持不佳以及 Rust 编译器工件的高磁盘使用量。另一方面，大多数 Rust 用户似乎对它的运行时性能、编译器的正确性和稳定性以及 Rust 的文档都非常满意。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/112026_HY9X_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;关于 Rust 用户希望稳定（或实现）的具体不稳定（或缺失）功能，最希望的是异步闭包和 if/let while 链。嗯，好消息是！异步闭包将在 Rust 的下一个版本（1.85）中稳定，而 if/let while 链有望在 Edition 2024 发布后不久跟进很快之后，这次发布也将发生在 Rust 1.85 中。&lt;/p&gt; 
&lt;p&gt;其他备受渴望的功能包括生成器（同步和异步）以及更强大的泛型常量表达式。您可以关注 Rust 项目目标以跟踪这些（以及其他）功能的进展。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/112038_zfN3_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;在对此问题的公开回答中，人们真的很有帮助，并尽力描述限制他们生产力的最显著问题。我们看到了关于异步编程（永恒的宠儿）的挑战，错误的可调试性（人们普遍喜欢，但并不适合每个人）或 Rust 工具缓慢或资源密集（rust-analyzer 和 rustfmt）的提及。一些用户还希望有更好的 IDE 故事和与其他语言的改进互操作性。&lt;/p&gt; 
&lt;p&gt;今年，我们还增加了一个关于 Rust 进化速度的新问题。虽然大多数人似乎对现状感到满意，但回答此问题的人中有超过四分之一的人希望 Rust 能够更快地稳定和/或添加新功能，只有 7% 的受访者希望 Rust 放慢或完全停止添加新功能。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/112052_mhgg_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;有趣的是，当我们询问受访者关于他们对 Rust 未来发展的主要担忧时，其中一个最常提到的答案是担心 Rust 会变得过于复杂。这似乎与上一个问题的答案形成了对比。也许 Rust 用户仍然认为 Rust 的复杂性是可控的，但他们担心有一天它可能会变得过于复杂。&lt;/p&gt; 
&lt;p&gt;我们很高兴地看到，对 Rust 项目治理和 Rust 基金会支持不足的担忧在 2023 年下降了约 6 个百分点。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/112103_2fAx_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;展望未来&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;每年，Rust 状态调查的结果都有助于揭示 Rust 项目和生态系统中许多需要改进的领域，以及对我们社区运作良好的方面。&lt;/p&gt; 
&lt;p&gt;如果您对 Rust 年度调查有任何建议，请告诉我们！&lt;/p&gt; 
&lt;p&gt;我们非常感谢参与 2024 年 Rust 状态调查并帮助其创建的人们。虽然开发和维护一种编程语言总是伴随着挑战，但今年我们很高兴看到高水平的调查参与和坦率的反馈，这将真正帮助我们让 Rust 更好地服务于每个人。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334273/2024-state-of-rust-survey-results</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334273/2024-state-of-rust-survey-results</guid>
            <pubDate>Sat, 08 Feb 2025 03:27:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>RWKV 首届全球开发者大会定档 2 月 21 日，研讨 RWKV-7 架构与未来趋势</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;新一代大模型架构 RWKV 将于 &lt;strong&gt;2025 年 2 月 22 日&lt;/strong&gt;在&lt;strong&gt;上海&lt;/strong&gt;举办首届主题为 &lt;strong&gt;《RWKV-7 架构与未来趋势》&lt;/strong&gt; 的开发者大会，大会将深入探讨 RWKV-7 的独家技术亮点、应用场景以及未来趋势，展示 RWKV 在推动全球 AI 发展中的前瞻性与领导力。&lt;/p&gt; 
&lt;p&gt;RWKV-7 架构采用动态状态演化（dynamic state evolution）机制，超越了传统的 attention/linear attention 范式，拥有强大的上下文学习（in-context learning）能力和持续学习能力。RWKV-7 模型在推理过程中就能不断自动根据新的数据进行自我优化和改进（test-time training），从而显著提升了模型的理解力和处理能力。例如 RWKV-7 2.9B 模型的英文和多语言能力（英文评测 71.1%，多语言评测 62.3%），均显著超越所有同尺寸模型，包括 Llama 3.2 3B（英文评测 68.7%，多语言评测 57.3%）、Qwen2.5 3B（英文评测 68.6%，多语言评测 57.0%）等知名优秀开源模型。且 RWKV-7 2.9B 只训练了 3T tokens，另两者训练了接近 20T tokens。更大规模的 RWKV-7 也在训练中。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-0dde119f43dfd830ac5ecc616e6a70e1783.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;此次大会将汇聚来自全球的技术专家、顶尖大学教授、行业领袖与创业者，预计超过 3000 名开发者和 AI 技术爱好者将参与其中。大会将设有多个&lt;strong&gt;分享和互动环节&lt;/strong&gt;，为参与者提供一个宝贵的交流与合作平台，帮助全球开发者共同探索 AI 的未来发展方向。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;RWKV 开发者论坛演讲嘉宾及议程：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-a9aff24ab7e5cc3158b5d66b43b45612a83.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;RWKV 开发者大会 | 大会信息：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;时间：2025 年 2 月 22 日 14:00&lt;/li&gt; 
 &lt;li&gt;地点：上海漕河泾现代服务园大厦 A6 号楼&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;RWKV 开发者大会 | 报名二维码：&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;200&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-1646ff713189f4ad7d2790a64066d4124a6.jpg&quot; width=&quot;200&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;未来，RWKV 将继续通过持续创新和生态建设，致力于为全球开发者提供强大的技术支持与资源，推动 AI 技术的普及与应用，敬请期待！&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334263</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334263</guid>
            <pubDate>Sat, 08 Feb 2025 02:51:00 GMT</pubDate>
            <author>来源: 投稿</author>
        </item>
        <item>
            <title>x-easypdf v3.3.0 发布，拥有 AI 加持的 pdf 框架</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;x-easypdf v3.3.0 发布，拥有 AI 加持的 pdf 框架&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;319&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-58a18b3315bcc283bc5eab13ddfa051a4dd.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;x-easypdf 是一个 java 语言简化处理 pdf 的框架，包含 fop 模块与 pdfbox 模块，fop 模块以创建功能为主，基于 xsl-fo 模板生成 pdf 文档，以数据源的方式进行模板渲染；pdfbox 模块以编辑功能为主，对标准的 pdfbox 进行扩展，添加了成吨的功能。&lt;/p&gt; 
&lt;p&gt;本次更新内容如下：&lt;/p&gt; 
&lt;p&gt;新特性：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;【pdfbox】新增 jpeg2000 格式图像支持&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增大模型解析文档的支持&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增开源中国（gitee）AI 解析器&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增智谱（glm）AI 解析器&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增腾讯（hunyuan）AI 解析器&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增阿里（qwen）AI 解析器&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增深度求索（deepseek）AI 解析器&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增字节跳动（doubao）AI 解析器&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增昆仑万维（tiangong）AI 解析器&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增月之暗面（kimi）AI 解析器&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增讯飞（spark）AI 解析器&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增线性化支持&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增 office 文件转换 pdf 的支持（依赖 office 服务）&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增 word 转换器&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增 excel 转换器&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增 ppt 转换器&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增 html 转换器&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增 rtf 转换器&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增附件处理器&lt;/li&gt; 
 &lt;li&gt;【pdfbox】新增加载 awt 字体的支持&lt;/li&gt; 
 &lt;li&gt;【fop】新增条形码无白边配置&lt;/li&gt; 
 &lt;li&gt;【fop】新增设置条形码缓存的方法&lt;/li&gt; 
 &lt;li&gt;【fop】新增权限配置&lt;/li&gt; 
 &lt;li&gt;【fop】新增从资源路径加载 awt 字体的支持&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;原有变更：&lt;/p&gt; 
&lt;p&gt;maven 座标变更，原 &lt;code&gt;groupId 「org.dromara.x-easypdf」&lt;/code&gt; 变更为 &lt;code&gt;org.dromara&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;问题修复：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;【pdfbox 模块】修复表格组件单元格添加多组件换行错误问题&lt;/li&gt; 
 &lt;li&gt;【pdfbox 模块】修复表格重叠问题&lt;/li&gt; 
 &lt;li&gt;【pdfbox 模块】修复空文本错误问题&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334262/x-easypdf-3-3-0-released</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334262/x-easypdf-3-3-0-released</guid>
            <pubDate>Sat, 08 Feb 2025 02:50:00 GMT</pubDate>
            <author>来源: 投稿</author>
        </item>
        <item>
            <title>Zadig：首个深度集成 DeepSeek 的 DevOps 平台</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                    
                                                                                                                                                    &lt;p&gt;&lt;img height=&quot;383&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-2230cda1042253217386ec9e74ab4b9bf7b.png&quot; width=&quot;898&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_1&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;引言：当工程效能遭遇数据迷雾&lt;/h1&gt; 
&lt;p&gt;在微服务与云原生架构普及的今天，DevOps 团队正面临双重挑战：日均千次的流水线执行产生 TB 级数据，却难以转化为有效洞见；K8s 生产环境复杂度指数级增长，人工巡检如同大海捞针。Zadig 与 DeepSeek 的深度协同，首次将 AGI 技术注入 DevOps 全生命周期，推出「&lt;strong&gt;AI 效能分析&lt;/strong&gt;」与「&lt;strong&gt;AI 环境巡检&lt;/strong&gt;」两大核心能力，实现从经验驱动到智能决策的范式转移。现已面向社区用户全面开放，开源力量再进化！&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_2&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;AI 效能诊断：让数据说话，精准定位效能瓶颈&lt;/h1&gt; 
&lt;p&gt;传统工程效能分析往往依赖人工统计与经验判断，效率低且易受主观因素影响，而 Zadig 沉淀了研发过程的构建、部署、测试等大量效能数据，基于 DeepSeek 的 AI 能力，通过智能分析数据，为团队提供客观、可操作的改进建议。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;核心能力：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;智能数据分析&lt;/strong&gt;：通过自然语言交互（Prompt 方式），AI 可快速分析流水线、构建、测试等环节的效能数据，识别瓶颈问题。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img height=&quot;1530&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-74e2b55272658d6609a7f07b0434738960b.png&quot; width=&quot;2942&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;问题精准定位&lt;/strong&gt;：无论是构建耗时过长、测试通过率低，还是资源利用率不足，AI 都能清晰指出问题所在。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img height=&quot;1486&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-888422976ca2585f16a7cfc4a352681b7ad.png&quot; width=&quot;2948&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;科学改进建议&lt;/strong&gt;：基于分析结果，AI 提供具体的优化建议，例如并行测试策略、资源分配调整等，帮助团队快速提升效能。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img height=&quot;1486&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-d89c13f6e249b6d1517a5d28bd169d63f7d.png&quot; width=&quot;2948&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;场景价值：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;无需手动分析海量数据，AI 自动生成效能报告，节省大量时间。&lt;/li&gt; 
 &lt;li&gt;通过数据驱动的优化建议，团队可快速落地改进措施，提升交付效率。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id=&quot;OSC_h1_3&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;AI 环境巡检：全天候守护，让环境问题无所遁形&lt;/h1&gt; 
&lt;p&gt;面对复杂的 Kubernetes 生产环境，传统人工巡检耗时费力，且难以覆盖潜在风险。Zadig 的 AI 环境巡检功能，通过定时巡检与智能告警，确保环境稳定性。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;核心能力：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;定时自动巡检&lt;/strong&gt;：AI 定期对 Kubernetes 环境进行全方位检查，覆盖资源状态、服务健康度等关键指标。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img height=&quot;2170&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-6351737e713118924456d4aa5a02c16d82b.png&quot; width=&quot;3410&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;智能问题识别&lt;/strong&gt;：自动识别常见环境问题，如 Pod 异常、资源不足、配置错误等，并给出相应的解决方案。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img height=&quot;1530&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-f291d5d35f8a63fbb52c7f1a73cd20d7696.png&quot; width=&quot;2942&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;即时告警推送&lt;/strong&gt;：巡检结果通过 IM 工具（如飞书、钉钉、企业微信等）实时通知相关责任人，确保问题第一时间被处理。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img height=&quot;1666&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-c8ddc2f1472da2e9cebbc2efb6f9a52cdef.png&quot; width=&quot;2234&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;场景价值：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;无需手动巡检，AI 自动完成环境健康检查，大幅降低人力成本。&lt;/li&gt; 
 &lt;li&gt;通过即时告警，团队可快速响应环境问题，避免小问题演变为大故障。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id=&quot;OSC_h1_4&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;结语&lt;/h1&gt; 
&lt;p&gt;Zadig 通过集成 DeepSeek 的 AI 能力，将智能技术深度融入 DevOps 流程，为研发运维团队带来了前所未有的效能提升和环境稳定性保障。未来，随着 AI 技术的不断发展，Zadig 将继续探索更多创新应用场景，助力企业实现数字化转型，提升核心竞争力。&lt;/p&gt; 
&lt;p&gt;Zadig 免费基础版已全面支持 AI 能力，0 成本解锁智能 DevOps！&lt;/p&gt; 
&lt;p style=&quot;color:#ff4c88; margin-left:0; margin-right:0; text-align:center&quot;&gt;&lt;strong&gt;即日起，Zadig 新版发布&lt;br&gt; 扫码咨询抢先体验&lt;/strong&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#191b1f; margin-left:0; margin-right:0; text-align:center&quot;&gt;&lt;img height=&quot;943&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-0c7876673ed701ed97107bb53b607d661dd.png&quot; width=&quot;1797&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:center&quot;&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fkoderover%2Fzadig&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;Zadig 在 Github&lt;/a&gt;&amp;nbsp;/&amp;nbsp;&lt;a href=&quot;https://gitee.com/koderover/zadig&quot; rel=&quot;nofollow&quot;&gt;Zadig 在 Gitee&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;strong&gt;&lt;span&gt;推荐阅读：&lt;/span&gt;&lt;/strong&gt; 
 &lt;/div&gt; 
 &lt;div&gt; 
  &lt;p style=&quot;color:#002a64; margin-left:0; margin-right:0&quot;&gt;&lt;a href=&quot;https://my.oschina.net/koderover/blog/11210095&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;Zadig 官网博客正式发布，技术干货实践管饱&lt;/a&gt;&amp;nbsp;/&amp;nbsp;&lt;a href=&quot;https://my.oschina.net/koderover/blog/16492101&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;流水线早已 out 了？你需要更高效能的工作流&lt;/a&gt;&amp;nbsp;/&lt;span style=&quot;background-color:#ffffff; color:#002a64&quot;&gt;&amp;nbsp;&lt;/span&gt;&lt;a href=&quot;https://my.oschina.net/koderover/blog/10316143&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;Jenkins 迁移 Zadig，新项目实施上线效率提升 6 倍&lt;/a&gt;&amp;nbsp;/&amp;nbsp;&lt;a href=&quot;https://my.oschina.net/koderover/blog/16507771&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;🚀 重大更新！Zadig V3.2.0 重塑工作流体验，强势推出迭代管理&lt;/a&gt;&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/div&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/koderover/blog/17622087</link>
            <guid isPermaLink="false">https://my.oschina.net/koderover/blog/17622087</guid>
            <pubDate>Sat, 08 Feb 2025 02:44:00 GMT</pubDate>
            <author>原创</author>
        </item>
        <item>
            <title>月之暗面因 DeepSeek 调整工作重心，内部人士：强化学习或许会是个方向</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;据媒体报道，月之暗面内部已经将「持续拿到 SOTA 结果」确定为当下最重要的工作目标。&lt;/p&gt; 
&lt;p&gt;2025 年，月之暗面围绕模型能力的关键方向除了继续强化多模态部分外，还会继续强化长文本推理能力。报道分析称，DeepSeek 爆火后，DeepSeek 与月之暗面存在的路线差异，让外界面临重新审视月之暗面技术模式、用户增长模式的情况。&lt;/p&gt; 
&lt;p&gt;而今，DeepSeek 采用区别与月之暗面的路线，也取得了现阶段更为出色的效果。业内人士认为，月之暗面如果想守住生态位，「需要做一些改变或者尝试，比如开源，比如调整引流策略等。」&lt;/p&gt; 
&lt;p&gt;不过目前，月之暗面尚未明确是否「接入」DeepSeek，对于接下来是否「开源」，公司也未置评媒体问询。&lt;/p&gt; 
&lt;p&gt;对于月之暗面是否会因 DeepSeek 而调整工作重心一事，向月之暗面方面求证，截止发稿公司暂无回应。不过有内部人士透露称，&lt;strong&gt;「RL（强化学习）大概率会是一个（工作重点）方向」&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;去年 11 月份月之暗面宣布推出&lt;a href=&quot;https://www.oschina.net/news/320859&quot; target=&quot;_blank&quot;&gt;新一代数学推理模型 k0-math &lt;/a&gt;之际，Kimi 探索版便通过运用强化学习技术创新了搜索体验，在意图增强、信源分析和链式思考三大推理能力上实现突破。彼时，月之暗面 Kimi 创始人杨植麟便对强化学习这一技术路线带来的模型能力提升给予了高度评价。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-f3ef9f71486f2898a14d0b17103cbd4308a.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;而在近日 OpenAI 发布关于推理模型在竞技编程中应用的研究论文报告《Competitive Programming with Large Reasoning Models》中，论文也特别提到，「中国的 DeepSeek-R1 和 Kimi k1.5 通过独立研究显示，利用思维链学习（COT）方法，可显著提升模型在数学解题与编程挑战中的综合表现。其中 k1.5 便是 DeepSeek 和 Kimi 在 1 月 20 日同时发布的新型推理模型。」&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334255</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334255</guid>
            <pubDate>Sat, 08 Feb 2025 02:28:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>微信搜索接入 DeepSeek，正在灰度测试中</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;2 月 15 日，部分微信用户发现，微信搜索已经上线「AI 搜索」功能，并接入 DeepSeek-R1 提供的「深度思考」服务。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2 月 16 日，记者从腾讯集团确认，微信搜一搜在调用混元大模型丰富 AI 搜索的同时，正式灰度测试接入 DeepSeek&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/101747_uyUN_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;腾讯方面表示，部分测试用户可在微信对话框顶部搜索入口，看到「AI 搜索」字样，点击进入后，可免费使用 DeepSeek-R1 满血版模型，获得更多元化的搜索体验。若未显示该入口，说明此次灰度测试暂未覆盖到该用户账号，可耐心等待后续开放。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/101729_EaEQ_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;有用户表示，通过在微信 AI 搜索「如何在微信上使用 DeepSeek 的 R1 模型」问题得到的答案是，该功能正灰度测试中，仅部分用户可见，微信版本需更新至最新版本。若暂未获得测试方案，微信团队正逐步扩大测试范围，建议定期检查更新及搜索功能变化。&lt;/p&gt; 
&lt;p&gt;从功能附带的开源与鸣谢声明能看出，&lt;strong&gt;微信中内置的 DeepSeek R1 基于开源版本构建而来，但其中并未明确提及其使用的模型体积，是否是 671B 的「满血」R1 版本&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0217/101712_mjq7_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;据证券时报今日消息，对于一些相关细节，腾讯方面还作了进一步说明：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;1、AI 搜索的数据源包含公众号吗？&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;微信 AI 搜索接入的 DeepSeek 支持联网搜索（用户无需手动选择），基于公众号等丰富的微信生态内容，以及全网优质内容，能为用户提供更全面的高质量回答。&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;2、AI 搜索已经全量吗？&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;目前该能力还在灰度测试中，将根据用户体验和反馈持续优化。&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;3、微信的搜索场景为什么要接入大模型？&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;大模型可以提升搜索的智能化和精准度，如更好地理解用户的搜索意图，分析和处理复杂的查询内容等。&lt;/p&gt; 
 &lt;p&gt;结合用户需求，腾讯在搜索场景中接入了包括混元、DeepSeek 在内的大模型，进一步丰富用户的搜索体验。&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;4、AI 搜索会用我微信内的朋友圈、聊天等个人信息吗？&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;AI 搜索仅整合公众号及互联网其他公开信息，不会使用用户的个人信息和相关隐私信息。&lt;/p&gt; 
&lt;/blockquote&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334252</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334252</guid>
            <pubDate>Sat, 08 Feb 2025 02:18:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>百度搜索宣布将全面接入 DeepSeek</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;百度搜索发文宣布，&lt;span&gt;&lt;span&gt;&lt;span&gt;为丰富更多元化的搜索体验，&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;百度搜索将全面接入&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;DeepSeek&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;和文心大模型最新的深度搜索功能。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;搜索用户可免费使用 DeepSeek 和文心大模型深度搜索功能，文心智能体平台的开发者也将能随时调用 DeepSeek 模型创建并调优智能体。&lt;/p&gt; 
&lt;p&gt;根据介绍，文心大模型深度搜索功能于 2 月 13 日上线，具备更强大的思考规划和工具调用能力，可为用户提供专家级内容回复，并处理多场景任务，实现多模态输入与输出。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;161&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-f7a3cff12b27c30d05def83a2c2f4477122.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334250</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334250</guid>
            <pubDate>Sat, 08 Feb 2025 02:03:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
    </channel>
</rss>