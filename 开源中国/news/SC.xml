<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>开源中国-最新资讯</title>
        <link>https://www.oschina.net/news/project</link>
        <atom:link href="http://8.134.148.166:30044/oschina/news" rel="self" type="application/rss+xml"></atom:link>
        <description>开源中国-最新资讯 - Powered by RSSHub</description>
        <generator>RSSHub</generator>
        <webMaster>contact@rsshub.app (RSSHub)</webMaster>
        <language>en</language>
        <lastBuildDate>Wed, 19 Feb 2025 21:36:38 GMT</lastBuildDate>
        <ttl>5</ttl>
        <item>
            <title>Svelte 5 不是 JavaScript</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;blockquote&gt; 
 &lt;p&gt;本文翻译自：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhodlbod.npub.pro%2Fpost%2F1739830562159%2F&quot; target=&quot;_blank&quot;&gt;https://hodlbod.npub.pro/post/1739830562159/&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;在过去几周里，我一直忙于处理将一个 Web 应用程序升级到 Svelte 5 所带来的后果。抛开对框架更新换代和迁移烦恼的抱怨，我在迁移过程中遇到了一些有趣的问题。到目前为止，我没有看到很多人报告过相同的问题，所以我觉得自己阐述这些问题可能会有所帮助。&lt;/p&gt; 
&lt;p&gt;我会尽量不在这篇帖子中抱怨太多，因为我很感激多年来享受的 Svelte 3/4。但我想我不会再选择 Svelte 来开发任何新的项目了。我希望我在这里的一些反思对其他人也会有所帮助。&lt;/p&gt; 
&lt;p&gt;如果您对我在这里提到的问题的复现感兴趣，可以在以下链接找到。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fsveltejs%2Fsvelte%2Fissues%2F15327&quot; target=&quot;_blank&quot;&gt;无法将状态保存到 indexeddb&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fsveltejs%2Fsvelte%2Fissues%2F15327&quot; target=&quot;_blank&quot;&gt;组件卸载导致闭包中的变量未定义&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;对速度的需求&lt;/h2&gt; 
&lt;p&gt;首先，让我简要地认可一下 Svelte 团队所努力的方向。看起来版本 5 的大部分重大变化都是围绕&lt;strong&gt;「深层响应性」(deep reactivity)&lt;/strong&gt;构建的，这允许更细粒度的响应性，从而带来更好的性能。这当然很好，Svelte 团队在性能与开发体验（DX）的协调方面一直表现出色。&lt;/p&gt; 
&lt;p&gt;在 Svelte 的早期版本中，实现这一目标的主要方式是通过 Svelte 编译器。涉及许多辅助技术来提高性能，但拥有一个框架编译步骤给了 Svelte 团队很大的灵活性，可以在幕后重新排列事物，而无需让开发者学习新概念。这就是 Svelte 最初如此独特的原因。&lt;/p&gt; 
&lt;p&gt;同时，这也导致了一个比以往更加晦涩难懂的系统框架，使得开发者调试更复杂的问题变得更加困难。更糟糕的是，编译器存在缺陷，导致了一些只能通过「盲猜」重构问题组件才能修复的错误。我个人至少遇到过五六次这样的情况，这也是我最终转向 Svelte 5 的原因。&lt;/p&gt; 
&lt;p&gt;尽管如此，我始终认为这是为了速度和生产力而可以接受的权衡。当然，有时我不得不删除我的项目，并将其迁移到一个新的仓库，但这个框架确实是一个使用起来的乐趣。&lt;/p&gt; 
&lt;p&gt;Svelte 5 更是加大了这种权衡的力度——这是有意义的，因为这正是该框架与众不同的地方。这次的不同之处在于，抽象/性能的权衡并没有停留在编译器领域，而是以两种重要的方式侵入了运行时：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;使用 proxies 来支持 deep reactivit&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;隐式组件生命周期状态。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;这两个改动不仅提升了性能，还让开发者的 API 看起来更加整洁。为什么不喜欢呢？&lt;/p&gt; 
&lt;p&gt;不幸的是，这两个特性都是&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.joelonsoftware.com%2F2002%2F11%2F11%2Fthe-law-of-leaky-abstractions%2F&quot; target=&quot;_blank&quot;&gt;抽象泄漏&lt;/a&gt;的典型例子，最终是开发变得更加复杂，而不是更简单。&lt;/p&gt; 
&lt;h2&gt;Proxies 不是 Objects&lt;/h2&gt; 
&lt;p&gt;使用 proxies 似乎让 Svelte 团队能够在不要求开发者做额外工作的前提下，从框架中榨取更多性能。&lt;/p&gt; 
&lt;p&gt;在 React 等框架中，通过多个组件层级传递状态而不引发不必要的重新渲染，是一项臭名昭著的困难任务。 Svelte 的编译器避免了与虚拟 DOM 比较解决方案相关的一些陷阱，但显然仍有足够的性能提升，足以证明引入 proxies 的合理性。&lt;/p&gt; 
&lt;p&gt;Svelte 团队似乎也 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fsvelte.dev%2Fblog%2Frunes&quot; target=&quot;_blank&quot;&gt;认为&lt;/a&gt; 他们的引入代表了开发者体验的改进：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;我们……可以最大化兼顾效率和人体工学。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;问题是：Svelte 5 &lt;em&gt;看起来&lt;/em&gt; 更简单，但实际上引入了 &lt;em&gt;更多&lt;/em&gt; 的抽象。&lt;/p&gt; 
&lt;p&gt;使用 proxies 来监控数组方法很有吸引力，因为它允许开发者忘记确保状态是响应性的所有古怪启发式方法，只需向数组中 &lt;code&gt;push&lt;/code&gt; 即可。我无法计算我在 Svelte 4 中写了多少次 &lt;code&gt;value = value&lt;/code&gt; 来触发响应性。 在 Svelte 4 中，开发者必须了解 Svelte 编译器的工作原理。编译器作为一个有缺陷的抽象，迫使用户知道赋值是用来表示响应性的方式。在 Svelte 5 中，开发者可以「忘记」编译器！&lt;/p&gt; 
&lt;p&gt;但实际上，他们不能。所有新抽象的引入实际上只是引入了更多复杂的启发式方法，开发者必须将它们记在心里，以便让编译器按照他们的意愿工作。&lt;/p&gt; 
&lt;p&gt;事实上，这就是为什么在使用 Svelte 多年后，我发现自己在越来越多地使用 Svelte stores，而响应性声明则越来越少。原因在于 Svelte stores 就是 JavaScript。在 store 上调用&lt;code&gt;update&lt;/code&gt;很简单，而且能够用&lt;code&gt;$&lt;/code&gt;来引用它们只是个额外的便利——无需记住，如果编译器出错，它就会提醒我。&lt;/p&gt; 
&lt;p&gt;proxies 引入了与响应性声明类似的问题，那就是它们看起来像一件事，但在边缘上却表现得像另一件事。 当我开始使用 Svelte 5 时，一切运行得都很顺利——直到我尝试将 proxies 保存到 indexeddb（&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fsveltejs%2Fsvelte%2Fissues%2F15327&quot; target=&quot;_blank&quot;&gt;GitHub 上的 issue&lt;/a&gt;），那时我遇到了 &lt;code&gt;DataCloneError&lt;/code&gt;。更糟糕的是，没有通过 &lt;code&gt;try/catch&lt;/code&gt; 结构化克隆来可靠地判断某个对象是否是 &lt;code&gt;Proxy&lt;/code&gt;，这是一个性能密集型操作。&lt;/p&gt; 
&lt;p&gt;这迫使开发者记住哪些是 proxies，哪些不是，每次将 proxies 传递给一个不期望或不知道它们的上下文时，都要调用 &lt;code&gt;$state.snapshot&lt;/code&gt;。这抵消了他们最初给予我们的所有美好抽象。&lt;/p&gt; 
&lt;h2&gt;组件不是函数&lt;/h2&gt; 
&lt;p&gt;虚拟 DOM 在 2013 年之所以能够流行起来，是因为它能够将应用程序建模为一系列组合函数，每个函数接收数据并输出 HTML。Svelte 保留了这种范式，使用编译器来规避虚拟 DOM 的低效和生命周期方法的复杂性。&lt;/p&gt; 
&lt;p&gt;在 Svelte 5 中，组件生命周期又回来了，采用了 react-hooks 风格。 在 React 中，hooks 是一种抽象，它允许开发者避免编写与组件生命周期方法相关的所有状态代码。现代 React 教程普遍推荐使用 hooks，这些 hooks 依赖于框架在不可见的方式下同步状态与渲染树。&lt;/p&gt; 
&lt;p&gt;虽然这确实会导致代码更简洁，但也要求开发者谨慎行事，以避免破坏围绕 hooks 的假设。只需尝试在&lt;code&gt;setTimeout&lt;/code&gt;中访问状态，你就会明白我的意思。&lt;/p&gt; 
&lt;p&gt;Svelte 4 有几个类似的陷阱——例如，与组件的 DOM 元素交互的异步代码必须跟踪组件是否已卸载。这和你在依赖生命周期方法的旧 React 组件中看到的那种模式非常相似。&lt;/p&gt; 
&lt;p&gt;在我看来，Svelte 5 通过添加与组件生命周期相关的隐式状态来协调状态变化和效果，似乎是走上了 React 16 的道路。 例如，以下是 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fsvelte.dev%2Fdocs%2Fsvelte%2F%24effect&quot; target=&quot;_blank&quot;&gt;$effect&lt;/a&gt; 文档的摘录：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;您可以将 $effect 放置在任何位置，而不仅仅是组件的最顶层，只要它在组件初始化期间（或父级效果激活时）被调用。然后它将与组件（或父级效果）的生命周期相关联，因此当组件卸载（或父级效果被销毁）时，它将自动销毁。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;这非常复杂！为了有效地使用 $effect...（抱歉），开发者必须理解状态变化是如何被追踪的。组件生命周期 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fsvelte.dev%2Fdocs%2Fsvelte%2Flifecycle-hooks&quot; target=&quot;_blank&quot;&gt;文档&lt;/a&gt; 声称：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;在 Svelte 5 中，组件的生命周期只包含两个部分：其创建和其销毁。介于两者之间的一切——当某些状态更新时——与组件整体无关；只有需要响应状态变化的那些部分才会收到通知。这是因为底层最小的变化单位实际上不是组件，而是组件在初始化时设置的（渲染）效果。因此，并没有「更新前」/「更新后」钩子这样的东西。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;然而，它接着介绍了与 &lt;code&gt;$effect.pre&lt;/code&gt; 结合的「tick」概念。本节解释说，「&lt;code&gt;tick&lt;/code&gt; 返回一个 promise，它在任何挂起的州变化被应用后解决，或者在下一个微任务中如果没有挂起的变化时解决。」&lt;/p&gt; 
&lt;p&gt;我确信有一些心理模型可以证明这一点，但我不认为当必须紧接着关于状态变化的补充说明时，声称组件的生命周期仅由挂载/卸载组成真的很有帮助。 这个地方真正让我感到困扰，也是这篇博客帖子的动机所在，那就是当状态与组件的生命周期耦合在一起时，即使这个状态被传递给一个对 Svelte 一无所知的函数。&lt;/p&gt; 
&lt;p&gt;在我的应用程序中，我通过在存储中保存我想要渲染的组件及其属性来管理模态对话框，并在应用程序的&lt;code&gt;layout.svelte&lt;/code&gt;中渲染它。这个存储也与浏览器历史同步，以便使用后退按钮关闭它们。有时，向这些模态之一传递一个回调是有用的，将调用者特定的功能绑定到子组件上：&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-plaintext&quot;&gt;const {value} = $props()
const callback = () =&amp;gt; console.log(value)
const openModal = () =&amp;gt; pushModal(MyModal, {callback})
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img height=&quot;1&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0219/174804_lFSI_2720166.gif&quot; width=&quot;1&quot; referrerpolicy=&quot;no-referrer&quot;&gt;这是 JavaScript 中的一个基本模式。传递回调只是你做的事情之一。&lt;/p&gt; 
&lt;p&gt;不幸的是，如果上述代码位于模态对话框本身中，调用组件会在回调被调用之前被卸载。在 Svelte 4 中，这运行得很好，但在 Svelte 5 中，当组件卸载时，&lt;code&gt;value&lt;/code&gt;会被更新为&lt;code&gt;    &lt;/code&gt;。&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fsveltejs%2Fsvelte%2Fissues%2F15325&quot; target=&quot;_blank&quot;&gt;这里有一个最小化复制的例子&lt;/a&gt;。&lt;/p&gt; 
&lt;p&gt;这只是一个例子，但对我来说，很明显，任何被生命周期比其组件长的回调函数封闭的属性，在我想要使用它时都会是&lt;code&gt;    &lt;/code&gt;——没有任何重新赋值存在于词法作用域中。&lt;/p&gt; 
&lt;p&gt;这根本不是 JavaScript 的工作方式。我认为 Svelte 之所以这样做，&lt;strong&gt;是因为它试图重新发明垃圾回收&lt;/strong&gt;。因为&lt;code&gt;value&lt;/code&gt;是组件的属性，它显然需要在组件生命周期的末尾被清理。我确信这背后有很好的工程原因，但这确实令人惊讶。&lt;/p&gt; 
&lt;h2&gt;结论&lt;/h2&gt; 
&lt;p&gt;简单的事情很美好，但正如 Rich Hickey 所说，&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.infoq.com%2Fpresentations%2FSimple-Made-Easy%2F&quot; target=&quot;_blank&quot;&gt;简单的事情并不总是简单的&lt;/a&gt;。而且像 Joel Spolsky 一样，我不喜欢感到意外。Svelte 一直充满了魔法，但在我看来，随着最新版本的发布，重复咒语的认知成本终于超过了它赋予的力量。&lt;/p&gt; 
&lt;p&gt;在这篇文章中，我的目的并不是贬低 Svelte 团队。我知道很多人喜欢 Svelte 5（以及 react hooks）。我试图表达的观点是，在为用户做事和赋予用户自主权之间有一个权衡。&lt;strong&gt;好的软件是建立在理解之上，而不是聪明之上&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;我也认为，随着 AI 辅助编码越来越受欢迎，记住这一点非常重要。不要选择让你与工作疏远的工具。选择那些利用你已经积累的智慧，并帮助你深化对这门学科理解的工具。 感谢 Rich Harris 及其团队多年来愉快的开发经历。我希望（如果你看到这段话的话），其中的不准确之处不至于影响作为用户反馈的价值。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334755</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334755</guid>
            <pubDate>Fri, 07 Feb 2025 10:00:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>工人日报：越来越多科创企业选择开源影响几何</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;blockquote&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;《工人日报》（2025 年 02 月 18 日 07 版）记者，杨冉冉&lt;/span&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;2025 年伊始，中国本土科创企业的代表 DeepSeek，成为一匹 AI 创新黑马，其推出的开源通用人工智能模型 DeepSeek-V3 和 R1 系列，以低成本、高性能震动全球科技界。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;同样引发高度关注的还有宇树科技，16 个身着花袄、手持彩绢的宇树 H1 人形机器人站上春晚舞台，以一场灵动欢快的「扭秧歌」表演惊艳世界。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;值得关注的是，这两家科创企业得到全球科技巨头认可的背后，有一个关键核心要素——那就是都选择了开源。DeepSeek 将 R1 训练技术全部公开，通过开源为全球开发者提供了一个创新与应用的开放平台。宇树科技则在 2024 年宣布开源强化学习代码库，吸引全球超万名开发者参与创新。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;什么是开源？开源可以看作源代码可开放共享的开发模式，具有大众协同、开放共享、持续创新的特性。其源起于软件领域，并发展延伸到开源硬件、开源设计，还有由贡献者、用户和爱好者组成的开源社区。在智能化转型的浪潮下，开源在云计算、大数据、区块链、人工智能、生物工程、脑科学、智能驾驶、机器人、工业软件等新赛道不断深化。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;公开数据显示，目前，全球 97% 的软件开发者和 99% 的企业使用开源软件，70% 以上的新立项软件项目采用开源模式。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;为什么越来越多的企业参与到开源的研发模式中，核心原因还是开源能够降低总体生产成本、提升产品创新速度、构建合作伙伴生态，推动产业发展。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;开源可以降低创新门槛，让更多贡献者「站在巨人的肩膀上」，参与前沿领域的创新；还能集合产业智慧，产业链上下游都可以进入，构建良性生态系统；同时可以促进人才聚集，不断产生创新的思维火花。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;以 DeepSeek 为例，目前华为云、阿里云等云厂商、国产芯片企业以及智能硬件、汽车、金融等上下游产业链企业都在积极接入 DeepSeek 模型，希望借助其能力来升级自身服务。业界评价，「通过开源，DeepSeek 推动了全球技术的协同创新，加速了 AI 技术的普及。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;除了技术层面的选择，科创企业选择开源更是为了适应市场竞争环境。在智能手机市场，相比苹果，安卓就是凭借开源战略实现了市场份额的领先。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;总之，开源作为信息技术发展的重要协作方式和生态构建的形式，已经成为科技创新的重要引擎。这一轮中国科创企业崭露头角，正是开源的受益者。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;目前，我国已成为全球开源生态的重要贡献力量，参与国际开源社区协作的开发者数量排名全球第二。企业「拥抱」开源趋势明显，本土发起的开源项目不断涌现、使用开源技术的企业占比近 90%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;不能忽视的是，开源与闭源之间向来是共存的。闭源体系在商业化闭环、服务质量和数据安全可控性上展现出独特的优势。开源体系也一直面临商业模式的质疑。有业内人士评估：「如果 DeepSeek 能持续保持开源第一的地位，其经济价值可能突破十万亿元人民币，并通过金融杠杆进一步放大。」DeepSeek 让人们看到了开源商业模式的价值和影响力，看到撬动产业格局的可能性。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;开源，正为企业带来翻天覆地的变革，也是加快发展新质生产力的有效措施，如何深入挖掘开源的巨大潜力，实现技术革新与商业利益的双重丰收，开辟从科技创新向新质生产力转化的共赢局面，科创企业还有很长的路要走。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334754</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334754</guid>
            <pubDate>Fri, 07 Feb 2025 09:58:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>阿里 AI To C 业务开放数百个招聘岗位</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;《科创板日报》记者多方获悉，阿里 AI To C 业务近期开启大规模人员招聘，开放招聘岗位达到数百个，集中在 AI 大模型相关的产品、技术研发岗位。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;据悉，此次招聘岗位数量数百个，其中 AI 技术、产品研发岗位占比达到 90%，主要分布在 AI 产品和 AI 技术研发方向，将重点投入到文本、多模态大模型、AI Agent 等前沿技术与应用的相关工作中。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;2 月初，全球顶尖人工智能科学家、前 Salesforce 集团副总裁许主洪出任阿里集团副总裁，负责 AI To C 业务的多模态基础模型及 Agents 相关基础研究与应用解决方案。据内部人士透露，许主洪目前正在筹备规模超百人的顶级 AI 大模型研究团队，推动前沿科研成果向实际应用解决方案的转化。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334751</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334751</guid>
            <pubDate>Fri, 07 Feb 2025 09:19:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>蚂蚁下场自研具身智能机器人</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;近日有招聘平台信息显示，蚂蚁集团开放招聘具身智能人形机器人系统和应用等岗位。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;《科创板日报》记者从知情人士处获悉，相关招聘主体为上海蚂蚁灵波科技有限公司，该公司于 2024 年底注册成立，注册资本 1 亿元。「蚂蚁确实在做具身智能。」上述人士表示。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;611&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-af588e63ca4b7228c8a7f06a89489f1b1eb.png&quot; width=&quot;300&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334747</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334747</guid>
            <pubDate>Fri, 07 Feb 2025 08:52:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>云风宣布最新开源项目 Soluna：基于 Lua 的 2D 游戏框架</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;知名游戏开发者云风&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Fcloudwu%2Fstatus%2F1891688746216484976&quot; target=&quot;_blank&quot;&gt;宣布&lt;/a&gt;创建新的开源项目 Soluna，这是一个基于 Lua 的游戏框架，支持使用多线程制作 2D 游戏。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-1b8941a90590dd71f627b51d8eb00c0405d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;云风介绍了&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fcloudwu%2Fsoluna%2Fdiscussions%2F1&quot; target=&quot;_blank&quot;&gt;创建 Soluna 的动机&lt;/a&gt;：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;我的游戏项目不希望牵扯太多图形技术（分散精力），主要是实践 gameplay 方面的想法。策略向游戏目前用 2d 就够了，之前的 ANT 以 3d 为主，如果用来做 2d 游戏，多了许多不必要的复杂性。&lt;/p&gt; 
 &lt;p&gt;另外，新项目不需要主打手机平台，不必围绕手机设备开发，所以 ANT 核心之一的 vfs 可以极大简化。直接使用本地文件系统能加快工作流。2d 游戏的美术资产制作简单（仅仅图片和非常有限数量的 shader ），也不需要复杂的资源编译过程。&lt;/p&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#e67e22&quot;&gt;&lt;strong&gt;我已经半年没有写复杂的程序，而写程序对我是一件非常有乐趣的事，开一个新坑会更有意思&lt;/strong&gt;&lt;/span&gt;。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;相关阅读&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/news/291324&quot; target=&quot;news&quot;&gt;云风从阿里离职，未来计划制作 Windows 平台的独立游戏&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/news/276078/cloudwu-ant-engine-open-source&quot; target=&quot;news&quot;&gt;云风宣布开源基于 Lua 的自研游戏引擎 Ant Engine&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334730</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334730</guid>
            <pubDate>Fri, 07 Feb 2025 07:40:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>百度 Q4 业绩会实录：DeepSeek 让我们明白要将最优秀的模型开源</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;百度昨天&lt;a href=&quot;https://www.oschina.net/news/334611&quot;&gt;发布&lt;/a&gt;了截至 12 月 31 日的 2024 年第四季度及全年财报。第四季度，营收为 341 亿元，同比下滑 2%。属于百度的净利润为 52 亿元。不按美国通用会计准则，归属于百度的净利润为 67 亿元。整个 2024 年，总营收为 1331 亿元，同比下滑 1%。归属于百度的净利润为 238 亿元。不按美国通用会计准则，归属于百度的净利润为 270 亿元。&lt;/p&gt; 
&lt;p&gt;财报发布后，百度董事长兼 CEO 李彦宏，移动生态事业群总裁罗戎，智能云事业群总裁沈抖，代理 CFO 何俊杰等高管出席随后召开的财报电话会议，解读财报要点并回答分析师提问。&lt;/p&gt; 
&lt;p&gt;以下是分析是问答环节主要内容：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;花旗银行分析师 Alicia Yap&lt;/strong&gt;：DeepSeek 最近备受关注，百度也宣布即将开源文心大模型 4.5 系列，并且将免费提供使用。请问管理层，此举背后的战略考量是什么？另外，我们如何看待当前基础模型领域的竞争格局？百度计划如何在这个不断演变的市场中保持领先地位？&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;李彦宏&lt;/strong&gt;：生成式人工智能和基础模型市场仍处于初期阶段，但发展速度极快，DeepSeek 的成功案例肯定会加快基础模型的应用速度。随着基础模型变得更容易获取且成本降低，我们正进入一个真正的变革阶段，我们会看到新的人工智能应用和使用案例在数量上呈爆发式增长，这将为所有人带来巨大的机遇，并拓展人工智能的边界，增加更多可能性。&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;从 DeepSeek 我们学到一点，那就是将最为优秀的模型开源供所有人使用，将可以极大地推动其应用，因为大家出于好奇自然会想去尝试开源模型，进而推动其更广泛的应用。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;文心大模型 4.5 早期版本将是我们有史以来最出色的模型，我们希望用户和客户试用起来能比以往更容易，更轻松。我们决定将 4.5 早期系列进行开源，也源于对自身技术领先地位的坚定信心，这种信心源自我们数十年在研发方面的持续投入、不断的技术创新，以及我们作为全球为数不多具备全栈人工智能能力公司之一的独特地位。&lt;/p&gt; 
&lt;p&gt;文心一言已经展现出强大的市场吸引力，日 API 调用量在短短一年内从 5000 万激增至 16.5 亿。通过开源，我们相信更多开发者和用户将认识到文心一言的真正价值，推动其更广泛应用，并扩大其在更多场景中的影响力。&lt;/p&gt; 
&lt;p&gt;同样，将文心一言机器人免费提供使用，能让更多用户在同等条件下将我们的基础模型与其他模型进行比较，尤其是其他模型收费而我们不收费的情况。我们上次对文心大模型进行重大升级是在 2023 年 10 月，距今已有很长时间，所以，未来几个月大家敬请期待文心大模型的 4.5 版本。&lt;/p&gt; 
&lt;p&gt;话说回来，我也想强调一个关键的重点。无论开源还是闭源，基础模型只有在能够大规模有效解决现实世界问题时才真正有价值，我们致力于以应用为导向，持续迭代文心大模型。秉持这种理念，自推出以来，我们一边利用文心大模型升级内部产品，一边服务企业客户。&lt;/p&gt; 
&lt;p&gt;借助文心大模型成功改造了百度面向消费者的产品，如百度搜索和百度文库；此外，通过千帆平台，我们正在提升企业客户的模型和应用开发体验。文心一言在指令遵循、先进的检索增强生成技术（该技术将幻觉问题降至最低）等方面的行业领先能力，使其在各种场景中得到广泛应用。千帆平台的综合工具链让我们的客户能够针对自身应用场景定制任何模型。展望未来，我们将瞄准性能提升和成本削减的潜力，加快文心大模型的迭代，继续在对现实世界影响潜力最大的领域对其进行开发。我们对人工智能发展的新篇章感到兴奋，期待看到人工智能技术带来更多具有开创性且对社会有持久价值的应用。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;高盛分析师 Lincoln Kong&lt;/strong&gt;：我有一个关于公司搜索业务的问题。在经历了最近几个季度的搜索改版后，我们应该如何看待未来的调整或变化？目前生成式人工智能结果在搜索结果中的占比是多少，我们的目标占比又是多少？另外，随着人工智能整合的不断深入，鉴于像 Deepseek 和豆包这样的人工智能聊天机器人也具备搜索功能，能否分享更多关于用户指标方面的信息？您如何看待百度搜索与这些人工智能聊天机器人之间的竞争态势？&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;罗戎&lt;/strong&gt;：目前，约 22% 的搜索结果页面包含人工智能生成的内容，但幕后还有更多的工作在进行，我们正在从根本上对搜索服务进行了变革，使其比以往任何时候都更强大、更高效。就像李彦宏在事先准备好的发言中提到的，借助文心一言，我们将搜索范围从文本和链接扩展到提供多样化的内容形式，包括短视频、直播、智能助理笔记和商品展示。这些不同的形式可以动态组合，创造出个性化的搜索体验。在持续改革搜索产品的过程中，我们也在开发能实现更深度个性化的功能，以适应每位用户的习惯和偏好。&lt;/p&gt; 
&lt;p&gt;我们的努力带来了更好的用户使用效果，包括在每月使用百度进行搜索的活跃用户中，83% 的用户会与生成式人工智能进行内容互动。更令我们倍受鼓舞的是，12 月百度应用上每位用户的搜索查询量同比增长了 2%。我们专注于不断提升用户体验，考虑到用户参与度日益积极，我们也会扩大人工智能的作用。&lt;/p&gt; 
&lt;p&gt;在此基础上，我可以同大家分享一个文心一言智能助理如何在刚刚过去的春节假期为我们的广告客户创造价值的案例。春节期间，许多中小企业放假一周，但某些行业的客户需求却达到高峰。我们的文心一言智能助理有效地弥补了这一缺口。例如，一家助听器公司因为家庭团聚，子女关心年迈父母的健康需求，咨询量有所增加，但该公司的客服团队因放假不在岗。通过文心一言智能助理，该公司有效地识别并筛选出高度相关的销售线索，使其即便在假期人员减少的情况下，也能高效跟进并无缝服务客户。&lt;/p&gt; 
&lt;p&gt;关于你提到的人工智能聊天机器人与搜索的问题，我们认为，由基础模型驱动的人工智能革命仍处于非常早期的阶段。无论是像 Deepseek 这样原生的人工智能工具，还是我们基于人工智能的产品，这些努力都代表了探索人工智能潜力的不同方式。作为拥有数亿用户的中国搜索市场领导者，我们通过采用最优秀的创新成果并融入真正创新的人工智能功能，保持灵活性和全面的市场视野。百度将继续引领人工智能变革，为我们庞大的用户群体提升搜索体验。&lt;/p&gt; 
&lt;p&gt;此外，搜索本质上深深扎根于语言和文本理解，这与大语言模型的能力完美契合，使我们能够在人工智能赋能的搜索变革中占据领先地位。我们相信，搜索正在演变成一个集成平台，它超越了人工智能驱动的探索阶段，不仅能提供智能答案，还能引导用户完成整个流程，从寻找答案、进行深入分析、完成任务，最终提供全面的服务和解决方案。&lt;/p&gt; 
&lt;p&gt;虽然人工智能驱动的探索是人工智能应用开发的一个重要但仍处于早期发展的阶段，目前尚未出现具有决定性影响力的应用程序，而对我们来说，关键是要保持这种快速且坚定的发展态势。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;摩根大通分析师 Alex Yao&lt;/strong&gt;：能否请管理层谈谈 2025 年第一季度和 2025 年全年核心广告业务的增长前景？支撑这些预测的宏观假设是什么？我们看到业务在 2024 年第四季度触底并开始复苏。最后一个问题是，生成式人工智能搜索的潜在盈利机会有哪些，预计何时能实现盈利？&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;罗戎&lt;/strong&gt;：过去几个月，我们看到各类支持政策相继出台，比如货币宽松政策、财政政策以及贸易政策。我们相信这些举措最终会推动经济增长，但它们需要时间才能产生效果。鉴于百度的广告业务与中小企业高度相关，而中小企业对宏观经济状况尤为敏感，再加上竞争环境持续严峻，尽管短期内可能面临压力，我们还会继续利用基础模型对搜索进行变革，这些工作正在稳步推进。正如我们在事先准备的发言稿中提到的，相信这种人工智能变革将持续提升用户体验，并创造新的可能性。&lt;/p&gt; 
&lt;p&gt;本季度，我们进一步深化了搜索在人工智能方向的变革，用户指标也出现了令人鼓舞的改善。我们认为这将推动收入增长，并在长期内开启新的盈利机会。此外，我们尚未大规模实现人工智能生成搜索结果的盈利，目前这类结果约占总查询量的 22%，而一旦我们的人工智能驱动搜索功能得到充分优化，我们将凭借更高质量的用户产品推进盈利进程。基于这些因素，我们看到了未来增长的机会，判断我们的广告业务正在触底。我们预计业务未来将逐步改善，今年上半年的表现会好于第四季度，下半年相比上半年会有进一步提升。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;摩根士丹利分析师 Gary Yu&lt;/strong&gt;：我的问题是关于人工智能云服务的。鉴于公司人工智能云业务增长强劲，我们能否期待该业务能够在 2025 年继续保持良好的发展态势？在收入和盈利能力方面，其主要驱动因素是什么？另外，管理层能否分享一下对 2025 年人工智能云市场的展望？我们应如何看待企业云服务需求？人工智能又给这一领域带来了哪些新增机会？&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;沈抖&lt;/strong&gt;：正如李彦宏前面提到的，我们人工智能云业务的收入在 2024 年第四季度同比增长加速至 26% ，推动 2024 年全年收入增长 17%。值得注意的是，2024 年与生成式人工智能相关的收入同比增长近两倍，这一增长得益于对文心一言和人工智能基础设施的需求不断上升，以及市场对百度在技术方面的领导力高度认可。因此，我们成功吸引了多样化的客户群体，并建立了强大的潜在业务渠道。&lt;/p&gt; 
&lt;p&gt;在人工智能基础设施方面，我们已与多个行业建立合作关系，涵盖互联网、汽车、智能设备、制造业、能源、金融、公用事业以及众多人工智能生成内容初创企业。我们的客户群体持续健康增长，在大中型客户中均取得显著进展，这表明我们在中国云市场的份额正在不断扩大。&lt;/p&gt; 
&lt;p&gt;通过千帆大模型平台，我们为市场提供具有行业领先性价比的全面基础模型。我们推出了从旗舰版到轻量版的文心大模型系列，旨在满足各种不同的需求。除了文心大模型，我们还提供广泛的优质第三方基础模型，包括 DeepSeek V3 和 R1 模型。得益于百度的全栈人工智能能力和端到端优化，我们能够确保平台上托管的任何模型都具备最佳性能和稳定性，同时保持极具竞争力的价格。此外，我们还提供一整套用于微调模型和构建原生人工智能应用程序的工具，客户能够轻松制定满足自身特定需求的解决方案。&lt;/p&gt; 
&lt;p&gt;关于人工智能云市场展望的问题，我们认为未来将迅速增长。一方面，在最近的春节假期期间，大语言模型成为了广泛讨论的话题，这不仅进一步提高了公众对基础模型的认知度，还促使更多人深入思考如何利用这些模型来提升自身业务。另一方面，我们相信基础模型的性能将不断提升，而成本会稳步下降，这无疑将进一步降低使用这些模型的门槛。因此，我们认为更多企业会将基础模型整合到从研发到生产的业务运营各个环节，从而推动 API 调用量的显著增长。&lt;/p&gt; 
&lt;p&gt;我们也预计百度人工智能云服务的支出将会增加，因为过去我们已经观察到一个明显的趋势：通过千帆大模型平台的 API 调用而使用基础模型的客户，也倾向于增加在我们人工智能云服务上的支出。在利润方面，由于我们专注于符合战略重点的高价值机会，推动了稳健的可持续增长，我们 2024 年第四季度非通用会计准则下的运营利润率持续同比扩大。展望 2025 年，我们有信心人工智能云业务收入增长将保持强劲势头，同时继续产生正的运营利润。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;瑞银证券分析师 Wei Xiong&lt;/strong&gt;：我想问一下利润率趋势的问题。鉴于核心广告业务近期面临的压力以及云业务收入占比的不断增加，我们应该如何看待 2025 年第一季度和全年的核心利润率水平？在运营效率方面还有进一步优化的空间吗？另外，公司全年有哪些投资计划？&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;何俊杰&lt;/strong&gt;：尽管面临短期压力，但对于人工智能领域的战略投资将产生更为可持续的影响方面，我们仍然非常乐观。我们始终专注于提升业务运营各方面的韧性，并且在各项业务中都看到了令人鼓舞的进展。&lt;/p&gt; 
&lt;p&gt;首先，对于我们的在线营销业务，预计受到人工智能生成搜索结果和技术变现计划的推动，以及百度在捕捉增长机遇方面的准备和市场宏观环境的改善，我们的广告收入将逐步提升。其次，我们的人工智能云业务已经展现出强劲的增长，随着市场份额的不断扩大，以及我们在自主研发独特全层级人工智能架构和全栈人工智能能力方面的竞争优势，利润率得到持续改善，因此，我们有信心在未来保持这一强劲势头。第三，对于我们的智能驾驶业务，尤其是萝卜快跑，我们将继续致力于通过提高运营效率和改善单位经济效益来缩小亏损，我们也在探索创新的运营模式，包括轻资产模式。&lt;/p&gt; 
&lt;p&gt;关于你问到 2025 年的投资与优化情况，我们将保持对高增长机遇的最优资源配置，同时与我们的长期战略保持一致。我们的投资将继续聚焦于那些既拥有巨大未来机遇，又具备强大投资回报率潜力的项目，包括进一步提升盈利能⼒、深化搜索业务的人工智能转型、增强我们的人工智能云产品，以及拓展我们的自动驾驶项目。在推进这些项目的过程中，我们将继续致力于提高运营效率，促进各业务集团之间的协同效应，以最大限度地发挥这些战略投资的影响力。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;美银美林分析师 Miranda Zhuang&lt;/strong&gt;：我有一个关于萝卜快跑业务的问题。管理层能否介绍一下 2025 年该业务的进展情况，包括车队规模目标、能够贡献哪些独特的经济效益，以及业务的价值主张是什么？管理层认为行业目前处于什么阶段？我们是否正在接近一个转折点？鉴于国内自动驾驶出租车市场上的竞争对手正在与汽车制造商和打车平台合作，采用生态系统战略，管理层对于接下来该行业的竞争态势有何看法？请问百度的竞争和规模化策略是什么？&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;李彦宏&lt;/strong&gt;：我们在自动驾驶技术领域已经投入了十多年，通过萝卜快跑，我们将宏伟愿景变为了现实，确立了公司在自动驾驶技术领域的全球领先地位。中国的自动驾驶市场环境是非常具有挑战性的，由于中国人口众多、路况多样、交通场景动态变化以及城市布局繁复，其交通状况是非常复杂的，而在中国乘坐自动驾驶出租车的价格约为美国的七分之一。因此，我们在中国的成功运营展示了百度卓越的技术和运营能力，包括我们所设计的全新第六代量产无人车 RT6，是世界上有史以来最具成本效益的自动驾驶出租车。凭借我们的这些优势，这一商业模式得到成功验证，并为进一步规模化发展和全球扩张奠定了坚实基础。&lt;/p&gt; 
&lt;p&gt;在第四季度，萝卜快跑在全国范围内提供了约 110 万次出行服务，同比增长 36%。到 1 月份，向公众提供的累计出行服务次数已超过 900 万次。此外，正如我之前提到的，我们在中国实现了 100% 完全无人驾驶运营，这意味着车辆上不再配备安全员，这是一个新的行业标杆，巩固了我们在该领域的领先地位。正如我之前提到的，去年 11 月，自动驾驶业务获得批准在香港开始开放道路测试，这是非常重要的一步，因为香港是我们首个右舵驾驶和靠左行驶的交通市场。这表明我们有能力使自动驾驶技术适应不同的交通系统，为拓展到其他具有类似驾驶设置的市场打开了大门。&lt;/p&gt; 
&lt;p&gt;今年对我们的业务拓展至关重要，随着业务的推进，预计车队规模和出行服务量的增长速度将超过以往任何时候。同时，我们也在积极寻找合作机会，我们已经确定了各种潜在合作伙伴，包括出行服务提供商、当地出租车公司、第三方车队运营商以及其他潜在合作伙伴，这种轻资产模式将使我们能够高效扩大规模，同时保持灵活性。通过与不同类型的合作伙伴合作，我们旨在加强市场地位，让更多人体验到自动驾驶服务。从更广阔的市场来看，正如我在上次财报电话会议中提到的，由于市场仍处于起步阶段，竞争实际上有助于加速市场发展，并营造更有利于创新的监管环境。在这个不断增长的市场中，我们的使命始终明确，那就是为更多用户提供更安全、便捷和舒适的出行体验。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;杰富瑞分析师 Thomas Chong&lt;/strong&gt;：展望 2025 年，百度业务投资的战略重点会是什么？具体而言，资源将如何在搜索、自动驾驶、云服务和基础模型之间分配？此外，管理层对 2025 年全年的资本支出以及股东回报有何展望？&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;何俊杰&lt;/strong&gt;：在评估 2025 年的资本配置选项时，我们的方法基于以下战略考量。我们将继续把提升人工智能能力作为长期战略重点进行投资。在此基础上，我们将进一步深化全产品线的人工智能转型，尤其是搜索业务。在人工智能云业务方面，我们旨在推动企业客户采用我们的人工智能基础设施、飞桨（PaddlePaddle）深度学习平台以及文心一言，满足他们对人工智能产品和解决方案以及人工智能云服务日益增长的需求。在智能驾驶方面，我们专注于扩大国内业务规模，探索创新运营模式，并拓展国际业务。&lt;/p&gt; 
&lt;p&gt;在确保严格的投资回报率管理和有效控制资本支出损失的同时，有两个关键优先事项驱动我们的决策，那就是强化我们的技术领先地位，以及加速人工智能产品和人工智能云服务在不同行业的应用。至于你问到的股东回报，自 2024 年初以来，我们的股票回购金额已超过 10 亿美元，这显著高于 2023 年全年的回购总额。对于截至 2025 年 12 月，规模为 50 亿美元的股票回购计划，我们总计已回购 17 亿美元。展望未来，作为持续回报股东长期信任的举措之一，我们计划加快股票回购计划的推进。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334727</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334727</guid>
            <pubDate>Fri, 07 Feb 2025 07:16:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>宇树科技申请春晚机器人图形商标</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;天眼查资料显示，2 月 5 日，杭州宇树科技有限公司申请注册一枚「春晚机器人」样式图形商标，国际分类为厨房洁具，当前商标状态为等待实质审查。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;img height=&quot;269&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-bc4fda9d3052fe48a476a7f163206880819.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;杭州宇树科技有限公司成立于 2016 年 8 月，法定代表人为王兴兴，注册资本约 259 万人民币，并已于 2024 年完成了 C 轮，交易金额数亿人民币。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;经营范围包括智能机器人的研发、智能机器人销售、工业机器人制造、工业机器人销售等，由王兴兴、汉海信息技术（上海）有限公司、宁波红杉科盛股权投资合伙企业（有限合伙）等共同持股。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;目前，宇树科技有四足机器狗和通用人形机器人两大系列产品。在创业早期，宇树科技以四足机器狗起家，第一款产品为 XDog。随后，Laikago、AlienGo、A1、Go1、B1 等一系列机器狗产品相继研发问世，推出市场。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334724</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334724</guid>
            <pubDate>Fri, 07 Feb 2025 07:14:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>Wasmer 6.0 Alpha 1 发布</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;Wasmer 6.0 首个 Alpha 已发布。此版本主要变化是&lt;strong&gt;支持同时启用多种异构后端&lt;/strong&gt;。例如，在同一个二进制发布版本中可以启用 llvm、v8 和 wamr 后端。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-5214fb7c60b85030a206656f1d74f66fe46.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Wasmer 6.0 Alpha 1 还通过了 LLVM 为 linux-x64、linux-aarch64 和 macOS 增加了异常处理提案的初步支持，这种 WASM 异常处理依赖于 LLVM 后端，此外还包含 WASIX 进程创建改进以及各种其他修复/增强。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;Wasmer 是支持 WASI 和 Emscripten 的通用 WebAssembly 运行时，提供基于 WebAssembly 的超轻量级容器，专注于支持在任何平台上运行 WASM 代码：从桌面端到云端、以及 IoT 设备，并且能嵌入在任何编程语言中。&lt;/p&gt; 
 &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2023/0627/173716_02s8_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
 &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;Wasmer 凭借其多样化的支持和专注于从通用桌面应用程序到 「便携式 ML/AI 应用程序」 的领域，目前仍然是领先的 WASM 运行时之一。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;下载 Wasmer 6.0 Alpha 1 及更多详细信息，请访问&amp;nbsp;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fwasmerio%2Fwasmer%2Freleases%2Ftag%2Fv6.0.0-alpha.1&quot; target=&quot;_blank&quot;&gt;GitHub&lt;/a&gt;。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334721/wasmer-6-0-alpha-1</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334721/wasmer-6-0-alpha-1</guid>
            <pubDate>Fri, 07 Feb 2025 07:02:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>Omdia：2029 年电信 IT 人工智能软件市场达 50 亿美元</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;研究机构 Omdia 最新发布了一份《电信 IT 人工智能市场预测》报告，展示了 Omdia 对于在跟踪的电信 IT 软件的五个主要产品类别中 AI 所占价值的估算。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Omdia 观点：&lt;/span&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;我们估计，2023 年，电信 IT 软件中 AI 的价值为 18 亿美元，而且我们预测，到 2024 年底，其价值会达到 23 亿美元。从 2024 年至 2029 年，整体价值会以 17% 的复合年增长率（CAGR）增长，达到 50 亿美元。&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img height=&quot;233&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-175f0c5940f60ad73ea3821a680982bf8f1.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;预测性 AI 将占整个预测期中大部分价值，于 2029 年达到总价值的 76%。在预测期期间，预测性 AI 将以 13% 的 CAGR 增长。然而，生成式 AI 会增长得更快，CAGR 为 37%，到 2029 年占电信 IT 软件中 AI 价值的 24%。&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;本预测的前几年将由自动化和相关用例驱动，相比于预计到预测期末会更普遍的推荐和预测用例，这些用例相对价值更低。消费者互动和网络管理将占大部分的增长，因为 AI 已经在这些细分市场获得关注，也正在推动运营效率提升，CSP 可以在此基础上再接再厉。在没有 AI 益处的情况下，分析已经能够完成很多任务，所以其增长会稍慢一点。目前 AI 对于创收和服务管理的作用更加受限，并且我们预测，在预测期期间这些细分市场的增长最低。&lt;/span&gt;&lt;/p&gt; 
&lt;/blockquote&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334718</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334718</guid>
            <pubDate>Fri, 07 Feb 2025 06:53:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>2024 年中国在开源人工智能模型领域的崛起和变革</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;最近，开源中国 OSCHINA、Gitee 与 Gitee AI&lt;a href=&quot;https://www.oschina.net/news/330623/china-open-source-2024-annual-report&quot;&gt;联合发布了《2024 中国开源开发者报告》&lt;/a&gt;。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;报告地址：&lt;a href=&quot;https://talk.gitee.com/report/china-open-source-2024-annual-report.pdf?fr=news0120&quot;&gt;2024 中国开源开发者报告.pdf&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;报告聚焦 AI 大模型领域，对过去一年的技术演进动态、技术趋势、以及开源开发者生态数据进行多方位的总结和梳理。&lt;/p&gt; 
&lt;p&gt;在第二章《TOP 101-2024 大模型观点》中，Hugging Face 工程师 &lt;strong&gt;Tiezhen&lt;/strong&gt;、Hugging Face 中文社区项目经理 &lt;strong&gt;Adina &lt;/strong&gt;以及 Hugging Face Fellow &lt;strong&gt;Lu Cheng&lt;/strong&gt;，从崛起与变革两个维度，探讨中国开源模型在 2024 年取得的重大成就和未来展望：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;2024 年中国在开源人工智能模型领域从 「追随者」 到 「引领者」 转变，体现技术实力且反映人工智能生态系统快速完善。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2024 年中国学术界和产业界推进自主研发，在技术创新和模型能力上飞跃，多款自主研发模型在国内外评测表现卓越。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Qwen 系列因多尺寸选项、多语言支持及友好授权功能获高度评价；DeepSeek 引入 MLA 技术实现性能成本突破；智谱 CogVideoX 系列成全球首批开源文生视频模型之一。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;中国开源模型从质疑中崛起获广泛认可，其成功得益于政府支持与行业巨额投入，中国人工智能生态体系迅速完善，未来可能在全球占更核心地位。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;随着开源模型影响力提高，中国开源社区活跃度提升，企业、研究机构、个体开发者积极参与，如 Qwen 系列被广泛集成促进交流协作，智源研究院等机构建立协作机制贡献基础工作和资源。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2024 年中国开源社区涌现高质量自发研究成果，如 MAP 团队的 Map Neo、InstantX 团队的 InstantID，为中国模型赢得国际认可。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;中国在推动人工智能技术发展同时建立完善透明治理机制，如《人工智能示范法 2.0（专家建议稿）》《生成式人工智能服务管理暂行办法》，为开源模型发展提供稳定政策环境并确保技术应用符合社会价值导向。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;端上模型兴起，中国 AI 社区推出多款移动友好型模型，如 Qwen2 - 1.5B 等，虽有挑战但代表 AI 技术隐私保护和成本优化未来方向。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;中国开源社区在逻辑推理领域推出创新项目，如 Macro - o1、QwQ 等，通过开源策略分享研究细节，推动小模型推理能力提升与行业技术进步。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;中国开源模型发展从 「百模大战」 迈向多元化和深度细分，发布大量高质量开源模型，涵盖多模态理解与生成等多个领域，模型竞争转向应用场景细化。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0219/135245_kWl7_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;完整全文：&lt;a href=&quot;https://my.oschina.net/u/3859945/blog/17503717&quot; target=&quot;_blank&quot;&gt;https://my.oschina.net/u/3859945/blog/17503717&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334705</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334705</guid>
            <pubDate>Fri, 07 Feb 2025 05:54:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>Meta 将在 4 月底举办首届 AI 开发者大会 LlamaCon</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;Meta&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.llama.com%2Fevents%2Fllamacon%2Fsignup%2F&quot; target=&quot;_blank&quot;&gt;宣布&lt;/a&gt;将在今年 4 月 29 日举行首届 LlamaCon——专门面向生成式人工智能的开发者大会。大会的名字源于 Meta 的开源 AI 模型 Llama 系列。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-26ace593ef0174eac7b8b3e6bcbe581ade6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Meta&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fai.meta.com%2Fblog%2Ffuture-of-ai-built-with-llama%2F&quot; target=&quot;_blank&quot;&gt;表示&lt;/a&gt;，随着开源 Llama 模型和工具集合的空前增长和发展势头强劲，公司决定于 4 月 29 日举行首届专门面向 AI 领域的开发者大会 LlamaCon。他们将&lt;span style=&quot;background-color:rgba(255, 255, 255, 0.65); color:#151631&quot;&gt;在大会上分享开源 AI 发展的最新动态，来帮助开发者构建「令人惊叹的应用和产品」。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;background-color:rgba(255, 255, 255, 0.65); color:#151631&quot;&gt;在接下来的几周里，Meta 也将公布更多与 LlamaCon 有关的信息。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;在 1 月底的财报说明会上，扎克伯格曾表示，Llama 4 的目标是引领（市场），具备原生的多模态能力。他当时也暗示 Llama 4 最早也要到今年二季度才会发布，所以 4 月 29 日会是一个较为合适的日期。&lt;/p&gt; 
&lt;p&gt;留出两个月的时间，也给 Meta 公司带来了不确定性。目前 OpenAI 已经确认会在近期发布 GPT-4.5、Anthropic 也被爆料将在「数周内」发布混合 AI 模型 Claude 4。&lt;/p&gt; 
&lt;p&gt;最令 Meta 忌惮的是，DeepSeek 是否会在未来两个月里搞出更多的「大新闻」。&lt;/p&gt; 
&lt;p&gt;据报道，在 DeepSeek 发布 R1 模型之后，Meta 迅速组建了四个「战情室」，核心忧虑是 DeepSeek 的最新大模型可能会比 Llama AI 的下一代版本更强。&lt;/p&gt; 
&lt;p&gt;知情员工透露，四个「战情室」中有两个负责研究 DeepSeek 如何降低训练和运行 AI 模型的成本，并将心得用于训练 Llama。另外两个团队，负责尝试找出 DeepSeek 用于训练的数据，以及如何将中国 AI 的先进训练方法用于重构 Meta 自己的产品。&lt;/p&gt; 
&lt;p&gt;在分析师电话会议上，扎克伯格曾表示：「他们（DeepSeek）做了一些新颖的事情，我认为我们仍在消化中。他们取得的一些进展，我们希望在我们的系统中应用，这就是开源世界运作的本质。」&lt;/p&gt; 
&lt;p&gt;与此同时，扎克伯格也已经宣布，今年将在人工智能相关的项目上投资 600-650 亿美元，包括建设一个超大型数据中心和更多的人才招聘。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334689/meta-llamacon-2025</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334689/meta-llamacon-2025</guid>
            <pubDate>Fri, 07 Feb 2025 04:06:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>OpenAI 前 CTO 官宣新创业 AI 公司，团队成员多来自 OpenAI</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;OpenAI 前 CTO Mira Murati 今天凌晨&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Fmiramurati%2Fstatus%2F1891918876029616494&quot; target=&quot;_blank&quot;&gt;宣布&lt;/a&gt;创立了新的 AI 公司 Thinking Machines Lab（思维机器实验室）。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-d07fc6d37ca4e1534888a3ca6098802c5d2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fthinkingmachines.ai%2F&quot; target=&quot;_blank&quot;&gt;官网写道&lt;/a&gt;，公司将专注于构建人工智能（AI）模型和产品，以支持更多、跨工作领域的「人类-AI 协作」，「虽然当前的系统擅长编程和数学，但我们正在构建能够适应人类所有专业知识并实现更广泛应用的人工智能。」 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;他们还强调这会是一家重视研究开放的公司，&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Fthinkymachines%2Fstatus%2F1891919141151572094&quot; target=&quot;_blank&quot;&gt;其推文中承诺&lt;/a&gt;&lt;/u&gt;：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;我们致力于通过论文发表和代码发布来开放科学，同时会重点关注应用于不同领域的人机协作。我们的方法包括共同设计研究和产品，以便从实际部署和快速迭代中学习。这项工作需要三个核心基础：&lt;strong&gt;SOTA 的模型智能、高质量的基础设施和先进的多模态能力&lt;/strong&gt;。我们致力于构建处于能力领先的模型来兑现这一承诺。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;该公司官方网站对这三核心基础进行了展开说明：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;模型智能是基石。除了强调人机协作和定制之外，模型智能也至关重要，我们正为科学和编程等领域构建前沿能力模型。最终，最先进的模型将解锁最具变革性的应用和优势，例如实现新颖的科学发现和工程突破。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;基础设施质量是重中之重。研究生产力至关重要，在很大程度上取决于基础设施的可靠性、效率和易用性。我们的目标是长期正确地构建事物，以最大限度地提高生产力和安全性，而不是走捷径。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;先进的多模态能力。我们认为多模态对于实现更自然、更高效的通信、保存更多信息、更好地捕捉意图以及支持与现实环境的更深入集成至关重要。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;官网贴出了公司 29 人团队名单，其中超过 20 人有在 OpenAI 供职的经验。其中较为知名的有 OpenAI 联合创始人约翰·舒尔曼（John Schulman），他正担任新公司的首席科学家。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334684</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334684</guid>
            <pubDate>Fri, 07 Feb 2025 03:35:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>没有所谓的 1875 纪元，美国 150 多岁老人领社保福利不是 COBOL 语言的锅</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;近期，一位美国政府官员曾宣称：「我们这里有些人看起来都已经 150 岁了」，并指出这些人正在领取社会保障福利。由此，有人开始流传这样一种说法：社会保障局（SSA）在存储日期时使用了一个 1875 年的纪元，把那些未知出生年份的记录存为 0，从而默认显示为 1875 年。&lt;/p&gt; 
&lt;p&gt;这种观点的起源可以追溯到某个帖子，帖子中有人调侃道：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;「看起来埃隆那群天才程序员根本就不懂 COBOL 的工作原理。社会保障系统正是运行在 COBOL 上，而 COBOL 并没有专门的日期或时间类型。于是日期就以数字形式存储，按照 ISO 8601 标准计算，纪元定在了 150 年前（1875 年）——也就是米制标准的开始。结果如果不知道某个日期，就会存储成 0，而在 COBOL 中这就会默认解析为 1875 年，也就是 150 年前。」&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img height=&quot;1668&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0219/112214_BPqK_3820517.png&quot; width=&quot;1198&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;然而，笔者对此并不认同，主要基于以下几点理由：&lt;/p&gt; 
&lt;h2&gt;数据库中存在 1875 年前的出生年份&lt;/h2&gt; 
&lt;p&gt;2007 年，社会保障局曾发布过一份数据集，该数据集包含了在 2007 年 1 月之前发放的社会保障号码持有者的收入记录（约占全部数据的 1%）。在这份数据集中，他们明确说明：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;移除了出生年份早于 1870 年的 5,935 条记录&lt;/li&gt; 
 &lt;li&gt;移除了出生年份等于 2007 的 1,096 条记录&lt;/li&gt; 
 &lt;li&gt;以及少数缺失出生年份的记录&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;这表明，SSA 的数据库中确实保存了 1875 年前（甚至 1869 年及更早）的出生年份数据，并非将未知年份一律默认为 1875。&lt;/p&gt; 
&lt;h2&gt;数据中没有 1875 年出生人数激增的异常&lt;/h2&gt; 
&lt;p&gt;如果系统将所有未知出生年份的记录默认转换为 1875 年，那么在统计数据中，1875 年的出生人数应该会异常增多。但实际上，从公开数据来看，并不存在这样一个「高峰」。（注：该数据集只是 1% 的样本，若存在默认值问题，趋势应当更加明显。）&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;698&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0219/112303_XwHt_3820517.png&quot; width=&quot;1174&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;社会保障局并未使用 ISO 8601 标准存储日期&lt;/h2&gt; 
&lt;p&gt;负责跟踪社会保障福利支付的主记录（Master Beneficiary Record, MBR）建立于 1962 年，这远早于 ISO 8601 标准于 1988 年的发布。即便是其前身 ISO 2016 标准也在 1976 年发布，并且并没有任何依据指向 1875 年。实际上，有研究论文基于 SSA 数据指出，SSA 对生日等信息的存储采用的是固定宽度格式，而不是 ISO 8601 标准的日期字符串格式。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;The data abstracted from the MBR consisted of a 26-character record for each deceased individual. The four data items on each record were… the month and year of death&lt;/span&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ISO 8601 标准本身并不涉及纪元概念&lt;/h2&gt; 
&lt;p&gt;ISO 8601 仅仅是一种用于表示日期和时间的字符串格式，其本质并不是基于数字计算时间流逝，因此根本无需设定一个「纪元」。虽然 ISO 8601:2004 版曾固定引用 1875 年 5 月 20 日——即《米制公约》在巴黎签署的那一天——作为参考日期，但这一引用在 ISO 8601-1:2019 版中已被移除。换句话说，这个日期仅用于定义格里高利历，并非作为一个时间计数的起点。&lt;/p&gt; 
&lt;h2&gt;没有任何证据显示 1875 年被用作时间计算的起点&lt;/h2&gt; 
&lt;p&gt;经过查找，笔者没有发现任何系统或标准会将 1875 年作为时间纪元。尤其在 COBOL 语言中，也没有这样的约定或实践。所有迹象都表明，所谓的「1875 纪元」只是个误解。&lt;/p&gt; 
&lt;p&gt;总的来说，从 SSA 的数据实践、存储方式以及国际标准的角度来看，都没有任何证据支持「1875 纪元」这一说法。该观点看似有趣，但实际上缺乏坚实的依据。&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;原文：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fiter.ca%2Fpost%2F1875-epoch%2F&quot; target=&quot;_blank&quot;&gt;https://iter.ca/post/1875-epoch&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334682/1875-epoch-cobol-150-american</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334682/1875-epoch-cobol-150-american</guid>
            <pubDate>Fri, 07 Feb 2025 03:26:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>没有处理过遗留项目，别自称资深工程师</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;大家都不喜欢维护遗留项目，我也不例外。命运总爱跟人开玩笑，最近一个遗留项目正好落到了我手上。虽然在这个项目上工作的经历并没有减少我对遗留系统的厌恶，反而让我对当下所采用的流程与实践有了更深刻的认识。&lt;/p&gt; 
&lt;p&gt;我为自己所在的团队感到自豪，因为我们遵循了许多业界最佳实践：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;编写简洁且易维护的代码，并配以自动化测试&lt;/li&gt; 
 &lt;li&gt;积极参与代码合并请求和任务评审&lt;/li&gt; 
 &lt;li&gt;合并到主分支后，当天就能将应用推向生产环境&lt;/li&gt; 
 &lt;li&gt;高度采用敏捷开发模式&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;当然，一切并非尽善尽美。合并请求中偶尔会出现一些无关痛痒的建议和讨论；运维团队有时也会搞砸一些事情（至少在我们开发人员看来是这样）；而产品负责人也时不时催促我们加快推出某些「简单」的新功能……总的来说，情况还算不错。&lt;/p&gt; 
&lt;h2&gt;穿越回 Ant 时代&lt;/h2&gt; 
&lt;p&gt;由于团队表现出色，公司决定将我们的开发效率借调给另一个由其他部门负责的产品。令我们略感失望的是，这个项目不仅使用的是较老版本的 Java，其代码风格也与我们的习惯大相径庭。&lt;/p&gt; 
&lt;p&gt;任务要求我们添加几个简单的监控指标，比如应用是否正常运行、运行时长、数据处理是否足够迅速等。由于项目正处于维护模式，已经有段时间没有添加新功能了。按理说，添加这些指标对我们来说应该是小菜一碟。&lt;/p&gt; 
&lt;p&gt;然而，当我们卷起袖子开始工作时，首先发现这个项目竟然使用了一种非常古老的构建方式——Ant 构建文件。那是一种庞大的 XML 文件，详细描述了如何构建整个项目：从编译、测试到打包，每个环节都必须显式配置，包括源码路径、目标路径以及资源位置。过去，这种做法在许多编程语言中都很常见：写好一个构建文件，复制到每个新项目中，再不断调整直至适应新项目需求。&lt;/p&gt; 
&lt;p&gt;例如，一个简单的「Hello World」项目的 Ant 构建文件可能如下所示：&lt;/p&gt; 
&lt;pre&gt;&amp;lt;project&amp;gt;

&amp;nbsp; &amp;nbsp; &amp;lt;target name=&quot;clean&quot;&amp;gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;lt;delete dir=&quot;build&quot;/&amp;gt;
&amp;nbsp; &amp;nbsp; &amp;lt;/target&amp;gt;

&amp;nbsp; &amp;nbsp; &amp;lt;target name=&quot;compile&quot;&amp;gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;lt;mkdir dir=&quot;build/classes&quot;/&amp;gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;lt;javac srcdir=&quot;src&quot; destdir=&quot;build/classes&quot;/&amp;gt;
&amp;nbsp; &amp;nbsp; &amp;lt;/target&amp;gt;

&amp;nbsp; &amp;nbsp; &amp;lt;target name=&quot;jar&quot;&amp;gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;lt;mkdir dir=&quot;build/jar&quot;/&amp;gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;lt;jar destfile=&quot;build/jar/HelloWorld.jar&quot; basedir=&quot;build/classes&quot;&amp;gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;lt;manifest&amp;gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;lt;attribute name=&quot;Main-Class&quot; value=&quot;oata.HelloWorld&quot;/&amp;gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;lt;/manifest&amp;gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;lt;/jar&amp;gt;
&amp;nbsp; &amp;nbsp; &amp;lt;/target&amp;gt;

&amp;nbsp; &amp;nbsp; &amp;lt;target name=&quot;run&quot;&amp;gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;lt;java jar=&quot;build/jar/HelloWorld.jar&quot; fork=&quot;true&quot;/&amp;gt;
&amp;nbsp; &amp;nbsp; &amp;lt;/target&amp;gt;

&amp;lt;/project&amp;gt;
&lt;/pre&gt; 
&lt;p&gt;难道就没有更便捷的方式吗？正因如此，「约定优于配置」的理念应运而生。这一理念主张：开发者只需关心那些偏离约定的特殊情况，而现代构建工具正是基于这一思想，通过提供可覆盖的默认配置，免去了重复配置的麻烦。正因为如此，大多数 Java 源码统一存放在 &lt;code&gt;src/main/java&lt;/code&gt; 目录下，而编译后的文件则放在 &lt;code&gt;target&lt;/code&gt; 目录中，避免了繁琐的重复设置。&lt;/p&gt; 
&lt;p&gt;这一发现启发了我们：或许同样的原则也能应用到我们当前项目中的应用配置上。面对一个庞大且大部分数值雷同（如应用端口）的配置文件，采用默认值机制无疑能让配置文件变得更精简。&lt;/p&gt; 
&lt;h2&gt;被我们视为理所当然的事情&lt;/h2&gt; 
&lt;p&gt;回到遗留项目，我们顺利构建并打包了应用！那枯燥的部分终于过去，可以安心开始编码了。但问题随之而来：如何将我们负责监控的指标组件嵌入到这套老旧的代码库中？在我们的常规开发框架中，这一切通常是自动处理的，因此我们一度认为这毫不费事。&lt;/p&gt; 
&lt;p&gt;但实际上，将指标组件「注入」到遗留代码的各个角落，最佳方案是什么呢？初看起来，单例模式似乎是最简单的选择；不过，开发社区普遍认为单例是一种反模式。为什么呢？毕竟，我们钟爱的某某框架不也依赖单例吗？如果不是，那它到底采用了什么机制？依赖注入究竟是什么？其底层又是如何运作的？&lt;/p&gt; 
&lt;p&gt;这一连串问题促使我们重新审视那些一直视为理所当然的基本概念。虽然在这种情况下使用单例并非最糟，因为代码大部分缺乏单元测试，但要让我们心安理得，代码必须经得起推敲。经过尝试，我们最终采用了一种不同的方案，写出了既简洁又清晰的代码——既没有依赖单例，也未引入多余的抽象层。&lt;/p&gt; 
&lt;h2&gt;开发者角色的局限性&lt;/h2&gt; 
&lt;p&gt;项目的最后一步是部署，只有部署成功后才能进行测试。但问题来了——这一次，我们既不负责部署，也不负责测试。部署工作由运维团队完成，而测试则交由专门的测试团队。为什么开发者就不能全程掌控，从开发到上线，而要先提交工单，再等待其他团队的配合呢？&lt;/p&gt; 
&lt;p&gt;首先，由于代码测试覆盖率不足，手动测试不可避免；其次，公司的基础设施也不允许我们自行部署应用。&lt;/p&gt; 
&lt;p&gt;这一系列经历使我们开始思考职责分离的原因，以及现行模式为何更为合理。事实证明，这个项目的任务交付时间和迭代周期远远超过平时（通常几天就能交付的工作，此次竟拖延了数周），这无疑验证了分工合作的必要性。&lt;/p&gt; 
&lt;h2&gt;通过旧实践理解现代方法&lt;/h2&gt; 
&lt;p&gt;到了月底，我们的监控指标终于在生产环境中顺利运行。虽然我对遗留项目的看法依旧——我仍然讨厌它们，也不奢望你会因此改变看法——但这段经历给了我们宝贵的启示。&lt;/p&gt; 
&lt;p&gt;我们无法选择所分配到的项目，但我们可以调整对待遗留系统的态度。与其心存无奈，不如把它当作一个提问、学习与成长的机会。通过了解过去的做法及其局限，我们不仅掌握了当下的最佳实践，更获得了背后历史的宝贵经验。&lt;/p&gt; 
&lt;p&gt;一旦你积累了这种深厚的知识，其他开发者自然会认可并信赖你的专业能力。如果你希望成为这样的人，就得勇于钻研那些看似繁琐的遗留项目。&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;原文：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.infobip.com%2Fdevelopers%2Fblog%2Fseniors-working-on-a-legacy-project&quot; target=&quot;_blank&quot;&gt;https://www.infobip.com/developers/blog/seniors-working-on-a-legacy-project&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;作者：Alen Kosanovic&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334678/seniors-working-on-a-legacy-project</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334678/seniors-working-on-a-legacy-project</guid>
            <pubDate>Fri, 07 Feb 2025 03:09:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>Grok 3 是否意味着大力出奇迹的大模型法则仍然成立？</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;blockquote&gt; 
 &lt;p&gt;本文转载自：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F24609799526&quot; target=&quot;_blank&quot;&gt;https://zhuanlan.zhihu.com/p/24609799526&lt;/a&gt;&lt;br&gt; 作者：张俊林（中科院软件所，博士）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;媒体风向变化太快，让人目不暇接。早上还在夸 Deepseek 成本低，性价比高，预训练 Scaling Law 死了，不需要太多机器和 GPU 卡，性价比优先，英伟达休矣；中午 Grok 3 一出来，说是用了 10 万张英伟达 H100 卡，效果力压 OpenAIo3 mini 和 Deepseek R1，就转向说 Scaling law 还成立，还需要大量的卡，英伟达股价有救了，还是要大力出奇迹……&lt;/p&gt; 
&lt;p&gt;这两个观点明显对立，有一真必有一假，那事实的真相到底是啥呢？我们来推一推。&lt;/p&gt; 
&lt;h2&gt;一、预训练阶段的 Scaling Law 是否仍然成立&lt;/h2&gt; 
&lt;p&gt;-预训练阶段的 Scaling Law 成立吗？当然是成立的，所谓「Scaling Law 撞墙」，大家普遍遇到的问题是数据不够了，没有大量新数据，导致预训练阶段的 Scaling Law 走势趋缓，注意是趋缓但不是停顿，预训练阶段的 Scaling Law 并没到天花板。按照 Chinchilla Scaling Law 推断，即使没有新数据，也并不意味着模型效果提不上去了，很简单，只要增加基座模型尺寸，效果仍然会提高，只是从付出的算力和获得的效果提升来说很不合算，性价比过低，这是为何大家转到 RL Scaling Law 和 Test Time Scaling Law 的原因，是因为付出同样的算力，在后面两个阶段大模型智商提升更明显，就是性价比高。&lt;/p&gt; 
&lt;p&gt;-&lt;strong&gt;目前可以提高模型效果的 Scaling 方法，按照性价比由高到低排序的话: Test time Scaling Law&amp;gt; RL Scaling Law&amp;gt;预训练阶段 Scaling Law(数据不够了，只能推大模型尺寸)&lt;/strong&gt;，有性价比高的 Scaling，当然优先做这种，性价比低的 Scaling，只有在没有性价比更高的情况下才会采用。这跟购物一个道理，有性价比高的当然不会去买性价比低的商品。&lt;/p&gt; 
&lt;p&gt;-&lt;strong&gt;如果哪天 RL Scaling Law 和 Test Time Scaling Law 到了天花板，又没有找到新的性价比更合算的 Scaling law，也不是说模型效果就提不上去了，大家仍然可以回归预训练阶段的 Scaling Law，没有新数据也没关系，推大模型尺寸规模就可以，效果仍然会上升&lt;/strong&gt;。但这基本是最后的选择，没办法的办法，只要有性价比高的方法就不会走这条路。&lt;/p&gt; 
&lt;p&gt;-有人问了：那按照你的意思，囤那么多 GPU 算力，其实对训最好的模型也没啥用？要是按照上面的理论，那确实是没有太大必要，比如 Deepseek 2000 卡也可以作出最好的模型不是。但是卡多有个好处，就是能压缩实验新想法和训练大模型基座的时间周期。比如你总得探索一些不同的算法、参数或数据配比的模型进行各种实验，你有 10 个新想法，如果只有 2000 张卡，可能得跑 5 天才能得出结论，要是有几万张卡，可能 1 天就能得出结论，所以卡多对于探索效率是有极大帮助的。卡多创新多，这点肯定成立。&lt;/p&gt; 
&lt;h2&gt;二、Grok 3 基座模型（对标 Deepseek V3，非 R1 这种逻辑推理模型）&lt;/h2&gt; 
&lt;p&gt;-为何 Grok 3 作为通用基座模型，它的评测指标只有数学、科学和代码数据集？没有通用能力比如最常用的 MMLU 指标的对比，这是不太规范的对比模式。推断可能 Grok 3 的通用能力相对 OpenAI 和 Deepseek 的模型没有大幅提升，所以不拿出来比？&lt;/p&gt; 
&lt;p&gt;-&lt;strong&gt;如果想要提升基座模型的数学、科学和代码能力，无论从方法还是从成本角度来讲，难度并不大，目前比较标准的做法是类似 Deepseek V3 从 Deepseek R1 蒸馏数学、代码等逻辑题的长 COT 数据，即深度思考过程数据，就是说把深度思考长 COT 数据引入基座的 Post-Training 阶段、甚至前置到预训练阶段（所谓大模型「左脚（Deepseek 基座）踩右脚（Deepseek R1）自我飞升」的模式），这样就能大幅提升基座模型在数学和代码方面相关的能力，也就是 Grok3 宣传具备的「有思维链推理和自我纠错机制」，评测指标看着会比较好看，而且蒸馏的数据总量也不会太大（几百 B 级别应该够了），成本很低，对算力要求不高。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;-OpenAI 很快会发布的非逻辑推理模型 GPT 4.5，大概也应是类似的思路，从 o3 模型蒸馏 COT 数据，用深度思考数据来提升 GPT 4.5 基座模型的智商，大模型「左脚踩右脚自我飞升」大法，这会是之后基座模型提升能力的主要手段。&lt;/p&gt; 
&lt;p&gt;-Grok 3 的算力消耗是 Grok 2 的 10 倍，如果遵照 Chinchilla Scaling Law，最佳做法是 Grok 3 的训练数据量比 Grok 2 增加 3 倍，模型大小同时比 Grok 2 增加 3 倍（但是目前的趋势是减小模型大小，增大数据量[就是说「小模型大数据」的模式]，尽管这样不满足训练最优原则，但因为模型尺寸小了，所以这种模型更适合在线推理服务，降低服务成本）。&lt;/p&gt; 
&lt;p&gt;-如果像发布会宣称的，Grok 3 耗费算力是 Grok 2 的 10 倍消息为真的话，那有两种可能。一种是数据量增长极大，这样只能是增加了大量多模态数据，比如数据量从 10T 增长到 30T（目前文本模型使用的数据量，最多到 18T 到 20T 之间，基本到顶，再多没有了，要大幅增加只能加多模态数据，但是增加多模态数据对提升大模型智商帮助不大，所以这个增量按理说不应该太大），如果这样推算，Grok3 的模型规模增长 3 倍左右；第二种可能是训练数据量比 20T 增加的不多，如果这样可以推出 Grok3 模型尺寸比 Grok 2 要大很多，至少 4 到 5 倍起步（若新增数据不多，那只能靠增加模型尺寸来消耗新增算力）。不论是哪种可能，Grok 3 的模型大小肯定比 Grok 2 大了很多，而 Grok 2 模型本身可能就不小（Grok 2 发布网页评测效果超过 Llama 3.1405B，所以无论数据还是模型大小，都不会太小，要是 Dense 模型， 70B 是最小的估计了），&lt;strong&gt;所以 Grok 3 的尺寸规模很可能不是一般的大&lt;/strong&gt;（感觉在 200B 到 500B 之间）。&lt;/p&gt; 
&lt;p&gt;-很明显，Grok 3 仍然在采取推大基座模型尺寸的「传统」做法，也就是上面「Scaling Law」部分分析的预训练阶段增大模型尺寸的方法来提升基座模型能力，上面分析过，这种做法是性价比很低的。比较时髦的做法是把训练重心放在 RL Scaling 方面，性价比会高太多。但是为啥他要做这种赔本买卖呢？在后面会给出一个可能的解释。&lt;/p&gt; 
&lt;h2&gt;三、Grok 3 逻辑推理版本 (深度思考版本，对标 Deepseek R1)&lt;/h2&gt; 
&lt;p&gt;-Grok 3 的深度思考版本，不说体验，单从评测指标看，达到或者超过了 o3 mini，确实是目前效果最好的，或者说最好的之一没有什么问题。&lt;/p&gt; 
&lt;p&gt;-说回上面提到的问题，为啥明知靠推大预训练阶段模型尺寸规模性价比低，Grok 3 还要用这种模式呢？很可能内在的原因在于（推断无证据）：&lt;strong&gt;Post-Training 阶段采取 RL Scaling，其效果可能跟基座模型的大小是有正相关关系的，就是说，同样的 RL 阶段的算力消耗，如果基座模型尺寸更大，则 RL 阶段的 Scaling 效果越好。&lt;/strong&gt;只有这样，才有在预训练阶段尽量把模型规模推大的必要性。而我们可以假设，Grok 3 之所以采取这种过于耗费算力，看着性价比不高的方式，是希望通过加大基座，把深度思考版本的能力明显提起来。&lt;/p&gt; 
&lt;p&gt;-貌似 Deepseek R1 效果很好又开源，获得一片好评，但大家想要实际用起来，会发现基座太大，部署难度和消耗资源太高，对下游应用不太友好。那为啥 Deepseek 非得推这种对下游应用来说明显过大的模型呢？（小点的蒸馏模型看着指标很好，但是实际应用效果貌似差不少），是否也是因为基座模型如果不够大，深度思考模型效果就没那么好的原因？&lt;/p&gt; 
&lt;p&gt;-如果上述假设成立，那意味着：&lt;strong&gt;三个 Scaling Law(Pre-train、RL 、Test Time)，从提高大模型智商的性价比来说，由高到低是：Test Time &amp;gt; RL &amp;gt; Pre-Train，这个是之前的结论。但如果上述假设成立，说明 Test Time Scaling 的天花板最低，它的天花板依赖于 RL 阶段的 Scaling 能力，而 RL 阶段 Scaling 天花板次低，它的天花板依赖于预训练阶段 Pre-Train 的 Scaling？&lt;/strong&gt;如果这样，如果有一天当 RL 和 Test Time 天花板到顶，意味着我们可以再启动一轮，去推大基座模型的模型尺寸，RL 阶段 Scaling 的天花板随之升高，然后可以再去 Scale RL 和 Test Time，就进一步得到智商更高的大模型。如果这成立，那意味着 AGI 的解决方案已经完整了？其实不需要新的 Scaling Law 存在就够？&lt;/p&gt; 
&lt;p&gt;-上述推论，是在一个前提成立的条件下的推出来的，这个前提是：Grok 3 耗费这么大算力推大模型规模，这是个深思熟虑或小规模实验的结果，而不是仅仅受到之前老观念（预训练阶段算力越高效果越好）影响下的决策。&lt;/p&gt; 
&lt;p&gt;如果这个前提不成立，则上述推论不成立。总之，一切责任在马斯克，Over。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334674</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334674</guid>
            <pubDate>Fri, 07 Feb 2025 03:02:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>DeepSeek 提出新的注意力机制：原生稀疏注意力 (NSA)，创始人亲自提交论文</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;2 月 18 日，DeepSeek 官方发文公布了一篇新的论文，&lt;strong&gt;论文提出了一种新的注意力机制「NSA」&lt;/strong&gt;。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-adf24ce9b3e5ac8760a1ec13688871f61ab.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
 &lt;p&gt;论文地址：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2502.11089v1&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/pdf/2502.11089v1&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;据 DeepSeek 介绍，&lt;strong&gt;&lt;span&gt;「原生稀疏注意力 (Native Sparse Attention, NSA) 」&lt;/span&gt;&lt;/strong&gt;是一个用于超快长上下文训练和推断的本地可训练的稀疏注意力机制，并且还具有与硬件对齐的特点。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;论文摘要：&lt;/p&gt; 
 &lt;p&gt;长文本建模对下一代语言模型来说至关重要，但标准注意力机制的高计算成本带来了显著的计算挑战。稀疏注意力为提高效率同时保持模型能力提供了一个有前景的方向。我们提出了 NSA（原生稀疏注意力），这是一个将算法创新与硬件对齐优化相结合的、原生可训练的稀疏注意力机制，用于实现高效的长文本建模。&lt;/p&gt; 
 &lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-d43b8ad2a0cb2e2f280f11b7d2d96a63fba.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;NSA 核心组件包括：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;动态分层稀疏策略&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;粗粒度 token 压缩&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;细粒度 token 选择&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;研究通过对现实世界语言语料库的综合实验来评估 NSA。其中作者评估了 NSA 在通用语言评估、长上下文评估和链式推理评估中的表现。实验结果表明，NSA 实现了与 Full Attention 基线相当或更优的性能，同时优于现有的稀疏注意力方法。&lt;/p&gt; 
&lt;p&gt;此外，与 Full Attention 相比，NSA 在解码、前向和后向阶段提供了明显的加速，且加速比随着序列长度的增加而增加。这些结果验证了分层稀疏注意力设计有效地平衡了模型能力和计算效率。&lt;/p&gt; 
&lt;p&gt;另外，有网友发现，arXiv 上 NSA 这篇论文的提交记录显示，它于 2 月 16 日提交，提交者正是梁文锋本人，他也是这篇论文的合著者。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-521d0ee94e504b1caa033cbf984b6fde938.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334671/deepseek-nsa</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334671/deepseek-nsa</guid>
            <pubDate>Fri, 07 Feb 2025 02:46:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>Linus Torvalds 将不顾维护者反对合并 Rust 内核代码</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;在 &lt;a href=&quot;https://www.oschina.net/news/334317/marcan-resigning-as-asahi-linux-project-lead&quot;&gt;Asahi Linux 创始人宣布辞去项目负责人职务&lt;/a&gt;之后，围绕 Linux 内核中 Rust 代码的争议还在继续。 &lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;DMA 映射助手及内核其他多个领域的维护者 Christoph Hellwig 一直对 Linux 内核中的 Rust 代码及其长期可维护性持批评态度，他在最新发布的一封邮件列表帖子中指出， Linus Torvalds 私下提到将推翻维护者对内核中 Rust 代码的否决权。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;409&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-84071f688bda1ac854a0ee922963f204016.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;考虑到最近几天的讨论，我决定发布此页面，其中包含我们的理解：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Frust-for-linux.com%2Frust-kernel-policy&quot; target=&quot;_blank&quot;&gt;https://rust-for-linux.com/rust-kernel-policy&lt;/a&gt;……&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Linus 私下表示，他绝对会不顾维护者的反对合并 Rust 代码。因此，从现在开始，作为 Linux 开发者或维护者，无论你是否愿意，都必须处理 Rust。&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;这里的 Rust 代码不仅仅是指 Rust 代码——这些绑定看起来一点也不像地道的 Rust 代码，它们是一种完全不同的存在，试图弥合巨大的语义鸿沟。而且它们在某些地方并没有做到这一点，因为它们现在被塞进了每个小子系统和库中。&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;因此，这些绑定会像癌症一样蔓延到各处，并迅速从一个允许并追求全局改进的软件项目，转向日益增加的隔离化。这将使 Linux 变成一个用多种语言编写的项目，而没有明确的指南说明在何处使用何种语言。即使在绑定之外，由于内核数据结构（如无处不在的链表）的侵入性和自引用特性，许多代码也不会是非常地道的 Rust。我们是否既对不起那些试图将现有代码库带入更安全空间的人，也对不起那些用 Rust 进行系统编程的人？&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;我曾经在类似的代码库上工作过，它们是我最糟糕的噩梦，因为由于原因 X，不断有部分代码从语言 A 重写为语言 B，然后又由于原因 Z 重写回去。而这还没有算上 Linux 维护者之间常见的‘创造性’内斗。&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;我想了解这个 Rust ‘实验’的目标是什么：如果我们想解决现有的内存安全问题，我们需要针对现有代码进行修复，并找到改进的方法。最近在这方面做了很多工作，我们还需要更多。但这也表明，核心维护者对诸如检查整数溢出或编译器强制同步（如 clang hread sanitizer)）等琐碎事情感到厌烦。我们如何弥合内核中一部分甚至不接受相对简单的安全改进规则，而另一部分却强制执行更严格规则之间的差距？&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;如果我们只是想使编写驱动程序更容易，那么引入一种新语言只会增加更多工作，并加重已经超负荷工作的核心基础设施维护者的负担。&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;因此，我认为这份政策文件没有太大用处。目前的规则是，Linus 可以强迫你做任何他想要的事情，我认为他需要非常清楚地阐明这一点，包括对贡献者的期望。&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;就我个人而言，我可以很好地处理 Rust 本身，我很乐意将内核带入一个更安全的内存世界，但处理一个不受控制的多语言代码库肯定会让我把业余时间花在其他事情上。我听到其他一些人嘀咕类似的话，但并不是每个人都像我这样直言不讳。&lt;/span&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;详情可&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flore.kernel.org%2Frust-for-linux%2FZ7SwcnUzjZYfuJ4-%40infradead.org%2F&quot; target=&quot;_blank&quot;&gt;查看邮件列表&lt;/a&gt;。&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334666/linus-torvalds-rust-code</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334666/linus-torvalds-rust-code</guid>
            <pubDate>Fri, 07 Feb 2025 02:34:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>中软国际被曝不协商直接降薪，有人直降 35%，引发员工抗议</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fweibo.com%2F7884923627%2FPeQTeBEot&quot; target=&quot;_blank&quot;&gt;有网友爆料&lt;/a&gt;，华为外包大厂中软国际员工在公司楼下聚集维权，抗议公司未提前协商突然降薪操作，疑似降薪幅度 10%-35%，甚至还有传出 0 元工资的极端情况，引发员工强烈不满。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0219/102639_R1Xq_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0219/102923_eWur_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;据悉，目前 IT 中心产品部确认降薪，其他受影响部门暂不清楚。有员工反映，公司在降薪的同时还在大力拓展新业务，高层薪资福利却丝毫未减，这让员工心里很不平衡。&lt;/p&gt; 
&lt;p&gt;据透露，中软国际提供了三种选择：一是按照薪资比例折合 13 个月+的薪资，多出来的月份按照绩效年终发放；第二种方案为直接降薪 18%；或者选择在 3 月底主动离职。三种方案任选其一，且没有书面形式，只是口头通知。更让员工难以接受的是，此次降薪并未明确说明原因，且强制执行。&lt;/p&gt; 
&lt;p&gt;面对员工的质疑，截至目前，该外包大厂派了一个所谓的办事员前来收集员工的诉求，并承诺将与高层协商解决。「他们只是来听我们说话，却没有给出任何实质性的答复。」一位参与沟通的员工表示，「感觉就像是走过场，根本没有解决问题的诚意。」&lt;/p&gt; 
&lt;p&gt;值得注意的是，据员工爆料，中软国际的邮箱让员工可以收到邮件，但是员工发送邮件出去，发件箱和发件记录是空的，「这是怕员工留痕特意设置的。」该员工表示。更有网友爆料，甲方开价不低，到手的钱却层层缩水，之前就有裁员 2.2 万人的先例，现在又不协商直接降薪。&lt;/p&gt; 
&lt;p&gt;资料显示，中国软件外包市场规模已突破 4454 亿元，年增速超 10%，而该公司作为在岸外包龙头，客户包括了如华为等互联网大厂。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334665</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334665</guid>
            <pubDate>Fri, 07 Feb 2025 02:29:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>李彦宏：文心大模型 4.5 系列将开源，是最强大的文心大模</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;在百度 2024 年 Q4 及全年财报电话会上，百度创始人、董事长兼首席执行官李彦宏透露，文心大模型 4.5 将开源，4.5 将是百度有史以来最强大的大模型，「希望客户和用户能比之前更方便地体验这款模型」。&lt;/p&gt; 
&lt;p&gt;他表示，开源 4.5 系列的决策源自于对技术领先地位的坚定信心，开源将进一步促进文心大模型的广泛应用，并在更多场景中扩大其影响力，「但我想强调的是，无论开源闭源，基础模型只有在大规模解决现实问题时，才具备真实价值」。未来，百度将加速推动文心大模型的性能升级与成本降低。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;400&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-3f76b9accbca6162153bf21f08d99a59ecc.jpg&quot; width=&quot;300&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;此外，李彦宏还在业绩会上表示，2025 年是萝卜快跑重要的扩张之年。他透露，百度将寻求与移动服务运营商、出租车公司及第三方车队运营方等合作，以加速业务扩展。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334659</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334659</guid>
            <pubDate>Fri, 07 Feb 2025 02:05:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>百度 2024 年总营收 1331 亿元，对 AI 投资充满信心</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;百度刚刚发布了 2024 年 Q4 及全年财报：全年总营收 1331 亿元，归属百度核心净利润达 234 亿元，同比增长 21%。&lt;/p&gt; 
&lt;p&gt;百度联合创始人兼 CEO 李彦宏表示：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;2024 年是百度从以互联网为中心向以 AI 为先转型的关键一年。我们的全栈 AI 能力得到了市场的广泛认可，从而推动了人工智能云的发展势头。在移动生态系统方面，我们坚定不移地推进 AI 转型，使搜索更接近原生 AI 能力，从而提供更好的用户体验。&lt;/p&gt; 
 &lt;p&gt;Apollo Go 经过多年的投入也充分验证了其商业模式，为全球扩张和可扩展的轻资产战略铺平了道路。随着我们的战略远见逐渐得到验证，我们预计我们的 AI 投资将在 2025 年取得更显著的成果。&lt;/p&gt; 
 &lt;p&gt;&lt;img height=&quot;1440&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0218/184432_hRLQ_2720166.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;受 AI 驱动，百度智能云呈高速增长，四季度收入同比增长达 26%。近期，百度智能云成功点亮昆仑芯三代万卡集群，未来还将进一步点亮三万卡集群。&lt;/p&gt; 
&lt;p&gt;12 月，文心大模型日均调用量达 16.5 亿次，一年增长 33 倍。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;1440&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0218/184634_17th_2720166.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334611</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334611</guid>
            <pubDate>Thu, 06 Feb 2025 10:47:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
    </channel>
</rss>