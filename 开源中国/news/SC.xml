<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - news - 简体中文</title>
    <link>https://www.oschina.net/news/project</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news" rel="self" type="application/rss+xml"/>
    <description>已对该 RSS 进行格式化操作：中英字符之间插入空格、使用直角引号、标点符号修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-cn</language>
    <lastBuildDate>Wed, 13 Aug 2025 02:43:22 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>阿里通义升级 Qwen Chat 的 Deep Research （深入研究）功能</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;阿里通义 Qwen 团队宣布对 Qwen Chat 的 Deep Research 能力进行了升级。此次升级旨在提供更智能、更具洞察力的研究报告。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1200" src="https://static.oschina.net/uploads/space/2025/0813/102724_zT9i_2720166.png" width="900" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;新版本通过更深入的搜索来获取更丰富的研究发现，并提高了信息准确性以减少幻觉现象。技术上，新版本支持模块化工具和并行执行。&lt;/p&gt; 
&lt;p&gt;此外，一个重要的新增功能是多模态输入支持，允许用户上传文件和图像进行研究。用户可通过官方链接体验此项新功能。&lt;/p&gt; 
&lt;p&gt;https://chat.qwen.ai/?inputFeature=deep_research&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365895</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365895</guid>
      <pubDate>Wed, 13 Aug 2025 02:31:40 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>腾讯混元发布 52B 参数多模态理解模型 Large-Vision</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;腾讯混元团队近日&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FgjZygQA9mRm-fYLWa1YqMA" target="_blank"&gt;发布&lt;/a&gt;了全新的多模态理解模型——混元 Large-Vision，该模型采用腾讯混元擅长的 MoE（专家混合）架构，激活参数达到 52B 规模，在性能与效率之间实现了良好平衡。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="314" src="https://oscimg.oschina.net/oscnet/up-c4400b13dcd221f1bca8f7c1cfe0d3ae958.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="221" src="https://oscimg.oschina.net/oscnet/up-e4ccecb8e928aa25ac259f743c1c80510d1.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;混元 Large-Vision 的核心亮点在于其强大的多模态输入支持能力。该模型不仅支持任意分辨率的图像处理，还能处理视频和 3D 空间输入，为用户提供了全方位的视觉理解体验。这一技术突破意味着用户可以直接输入各种格式和尺寸的视觉内容，无需进行复杂的预处理操作。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;MoE 架构通过动态激活部分专家网络来处理不同类型的输入，既保证了模型的强大性能，又避免了全参数激活带来的计算资源浪费。52B 的激活参数规模在当前多模态模型中处于先进水平，能够处理复杂的视觉理解任务。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;该模型还重点提升了多语言场景理解能力，这对于全球化应用具有重要意义。在处理包含多种语言文字的图像或视频时，混元 Large-Vision 能够准确识别和理解不同语言环境下的视觉内容，为跨语言的多模态应用提供了技术基础。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;混元 Large-Vision 支持任意分辨率图像输入的特性尤其值得关注。传统的视觉模型往往需要将输入图像调整到固定尺寸，这可能导致信息丢失或画质下降。而混元 Large-Vision 能够直接处理原始分辨率的图像，保持了视觉信息的完整性，这对于需要精细视觉分析的应用场景具有重要价值。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;3D 空间输入支持则进一步扩展了模型的应用范围，为虚拟现实、增强现实、3D 建模等领域的 AI 应用提供了强有力的技术支撑。结合视频处理能力，该模型有望在智能监控、视频分析、内容创作等多个行业发挥重要作用。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365893</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365893</guid>
      <pubDate>Wed, 13 Aug 2025 02:27:40 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>昆仑万维开源 Skywork UniPic 2.0</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"&gt;昆仑万维&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FCN8NaVnxsqxFe6WF_eo2iA" target="_blank"&gt;宣布&lt;/a&gt;正式开源 Skywork UniPic 2.0 模型——面向统一多模态建模的高效训练和推理框架，围绕生成和编辑模块轻量化、连接多模态理解模型进行联合训练，构建了理解、生图、编辑一体化的核心能力，旨在实现「高效、高质、统一」的多模态生成模型。&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"&gt;目前，Skywork UniPic 2.0 及其系列模型已全面开源，涵盖模型权重、推理代码、强化策略等。&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"&gt;Skywork UniPic 2.0 由三个核心模块组成：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;生图编辑（下图中）：&lt;/strong&gt;基于 SD3.5-Medium 架构将原本只支持文本输入的模型改进成也接受文本图像同时输入，然后通过高质量图像生成和编辑数据的训练将原本生图能力扩展成生图、编辑双能力。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;统一模型能力（下图左侧与中间）：&lt;/strong&gt;通过冻结生图编辑模块，多模态模型（Qwen2.5-VL-7B），Pre-Train 连接器来构建出理解生成编辑一体化能力，再通过连接器和生图编辑模块一起联合微调，实现最终的一体化理解、生图、编辑模型。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;生图编辑后训练（下图右）：&lt;/strong&gt;为提升生图编辑整体性能，设计了基于 Flow-GRPO 的渐进式双任务强化策略，实现了生成与编辑任务在不互相干扰下的协同优化，在预训练的基础上进一步提升了模型性能。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img height="203" src="https://oscimg.oschina.net/oscnet/up-67f45853462faa0c1bce2a535fd0a260701.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"&gt;Skywork UniPic 2.0 的核心优势包括有：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;生成模块轻量高效，性能拉满&amp;nbsp;&lt;/strong&gt;生成模块基于 2B 参数的 SD3.5-Medium 架构训练，生图和编辑指标超越生成模块具有 7B 参数的 bagel，4B 参数的 OmniGen2，12B 参数的 UniWorld-V1 和 Flux-kontext 模型。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;引入强化学习，效果显著&amp;nbsp;&lt;/strong&gt;基于 Flow-GRPO 首创渐进式双任务强化策略，有效提升模型对复杂指令的理解能力与图像生成和编辑的一致性，两大任务协同优化、互不干扰。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;一体化灵活切换，拓展能力强&amp;nbsp;&lt;/strong&gt;将生图编辑的 Kontext 模型与多模态模型端到端整合，微调轻量连接器，即可快速构建统一理解-生成-编辑模型，并且生图和编辑的性能进一步提升。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;更多详情可&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FCN8NaVnxsqxFe6WF_eo2iA" target="_blank"&gt;查看官方公告&lt;/a&gt;。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365887</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365887</guid>
      <pubDate>Wed, 13 Aug 2025 02:07:40 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Fedora 43 获准支持 Hare 编程语言，默认启用硬链接</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Fedora 工程与指导委员会 (FESCo) 本周批准了即将发布的 Fedora Linux 43 版本的多项新增功能。其中包括获准发布 Hare 软件包，Hare 是一种新的系统编程语言，旨在简化、稳定和健壮。&lt;/p&gt; 
&lt;p&gt;Hare 本身仍在开发中，但 FESCo 现已批准将 Hare 工具链打包并发布到 Fedora 43 的仓库中。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0812/191447_Wl7H_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;FESCo 还批准在 Fedora 43 中发布即将发布的 PHP 8.4 版本，这并不令人意外。FESCo 还批准弃用 YASM，转而使用 NASM。YASM 汇编器目前无人维护，而 NASM 的情况也好多了。&lt;/p&gt; 
&lt;p&gt;作为英特尔 oneAPI 线程构建版本 (TBB) 的最新更新，Threaded Building Blocks 2022.2 也已获批准发布。FESCo 本周还批准了默认对 Fedora RPM 软件包中，相同的 /usr 文件进行硬链接的提案。&lt;/p&gt; 
&lt;p&gt;有关 Fedora 43 版本中这些新批准更改的更多详细信息，&lt;u&gt;&lt;em&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flists.fedoraproject.org%2Farchives%2Flist%2Fdevel%40lists.fedoraproject.org%2Fthread%2FMVPWNTBSZUUJINZX6PZQGTYE2BA7NFKL%2F" target="_blank"&gt;请通过此 FESCo 邮件列表帖子获取&lt;/a&gt;&lt;/em&gt;&lt;/u&gt;，该版本将于今年晚些时候发布。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365798</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365798</guid>
      <pubDate>Mon, 11 Aug 2025 11:15:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>One Million Screenshots：收集了超过 100 万张网站截图的网站</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;「One Million Screenshots」 是一个专门收集网站截图的网站，声称截图了超过 100 万个热门 Web 主页。此外还提供了搜索相似网站的功能，以及查看网站截图的历史变化。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1856" src="https://static.oschina.net/uploads/space/2025/0812/185333_PgpB_2720166.png" width="3360" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;https://onemillionscreenshots.com/&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;没想到 OSCHINA 也荣幸出镜了：&lt;em&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fonemillionscreenshots.com%2Foschina.net%2Fscreenshot" target="_blank"&gt;https://onemillionscreenshots.com/oschina.net/screenshot&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="6306" src="https://static.oschina.net/uploads/space/2025/0812/185758_Ydik_2720166.png" width="1604" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;下面是关于该网站的常见问题：&lt;/p&gt; 
&lt;p&gt;&lt;img height="2386" src="https://static.oschina.net/uploads/space/2025/0812/185221_4Hkz_2720166.png" width="1514" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365794</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365794</guid>
      <pubDate>Mon, 11 Aug 2025 11:00:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>人工智能正在降低知识的价值，大学应该重新考虑所教授的内容？</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;生成式人工智能，尤其是大型语言模型（LLM）的兴起，正以前所未有的速度改变知识获取的格局。奥克兰大学商学院教授帕特里克·多德在《对话》(The Conversation) 上撰文指出，随着 AI 以低成本、高效率的方式提供知识，大学作为传统知识来源的价值正在受到挑战。他认为，大学必须重新审视其核心功能，以适应这个由 AI 驱动的新时代。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;多德教授分析，大学长期以来奉行「知识稀缺」的原则，通过提供独家课程和学位证书来证明学生获取知识的能力。然而，AI 技术的进步已大大降低了获取专业知识的门槛，LLM 不仅能检索事实，还能进行解释、翻译和总结，使得曾经「稀缺」的知识价值大打折扣。这种变化已经在劳动力市场显现，自 ChatGPT 问世以来，英国入门级职位空缺减少了约三分之一，美国部分州甚至取消了公共部门职位的学位要求。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;然而，多德强调，并非所有知识都同等贬值。虽然基础知识的价值下降，但&lt;strong&gt;隐性知识&lt;/strong&gt;，如团队协作、伦理判断、创造力以及解决复杂问题的能力，仍是 AI 无法取代的稀缺资源。他指出，未来教育的重点应从传授信息转向培养这些关键的人类技能。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;为应对这一挑战，多德教授为大学提出了四项转型建议：&lt;/span&gt;&lt;/p&gt; 
&lt;ol style="margin-left:0; margin-right:0"&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;评估转型&lt;/strong&gt;：将课堂评估重点从单纯的知识记忆转向&lt;strong&gt;判断和综合能力&lt;/strong&gt;的考察。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;体验式学习&lt;/strong&gt;：投入资源开发导师指导项目、模拟现实场景，并利用 AI 作为工具进行伦理决策研究。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;技能微证书&lt;/strong&gt;：创建针对协作、自主学习和伦理判断等关键能力的微证书。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;深化产学研合作&lt;/strong&gt;：大学提供专业知识，企业提供真实案例，学生则专注于验证和完善想法，共同培养适应未来市场的复合型人才。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;多德总结道，如果大学想要在未来立于不败之地，就必须从一个单纯的&lt;strong&gt;信息来源&lt;/strong&gt;转变为一个&lt;strong&gt;判断力中心&lt;/strong&gt;，教会学生如何与 AI 协同思考，而非与之竞争。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365792</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365792</guid>
      <pubDate>Mon, 11 Aug 2025 10:40:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Syncthing 2.0.0 正式发布，连续文件同步工具</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;Syncthing&amp;nbsp;是一个免费开源的工具，它能在你的各个网络计算机间同步文件 / 文件夹，它的同步数据是直接从一个系统中直接传输到另一个系统的，并且它是安全且私密的。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#1f2328"&gt;Syncthing 全新 2.0 系列的首发版本已正式推出，一些更新亮点如下：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;数据库后端从 LevelDB 切换到 SQLite。首次启动时需要迁移，对于大型系统来说，迁移过程可能会比较耗时。新数据库更易于理解和维护，且希望其稳定性更高。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;日志格式已更改为使用结构化日志条目（一条消息加上多个键值对）。此外，现在可以按包控制日志级别，并在 INFO 和 ERROR 之间新增了 WARNING 日志级别（此前该级别被称为 WARNING...）。INFO 级别的日志内容更加详细，会显示 Syncthing 执行的同步操作。新增命令行参数&lt;code&gt;--log-level&lt;/code&gt;可设置所有包的默认日志级别，&lt;code&gt;STTRACE&lt;/code&gt;环境变量和 GUI&amp;nbsp;也已更新以支持按包设置日志级别。-&lt;code&gt;--verbose&lt;/code&gt;和 &lt;code&gt;--logflags&lt;/code&gt;命令行选项已被移除，若指定将被忽略。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;已删除的项目不再永久保存在数据库中，而是在六个月后被清楚。如果你的用例要求删除操作在六个月以上后生效，建议将&lt;code&gt;--db-delete-retention-interval&lt;/code&gt;命令行选项或相应的环境变量设置为零，或选择更长的时间间隔。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;现代化的命令行选项解析。旧的 single-dash long 选项不再支持，例如，&lt;code&gt;-home&lt;/code&gt;必须改为&lt;code&gt;--home&lt;/code&gt;。部分选项已重命名，其他选项则变为子命令。所有服务选项现在也可作为环境变量接受。详情可参阅&amp;nbsp;&lt;code&gt;syncthing --help&lt;/code&gt;和&lt;code&gt;syncthing serve --help&lt;/code&gt;。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;不再支持对 shifted data 的滚动 hash 检测，因为这实际上毫无帮助。相反，没有它，扫描和同步会更快、更高效。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;首次启动时不再创建「default folder」。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;v2 设备之间现在默认使用多个连接。新的默认值是使用三个连接：一个用于索引元数据，两个用于数据交换。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;遗憾的是，由于与 SQLite 交叉编译相关的复杂性，以下平台目前无法在 syncthing.net 和 GitHub 上下载预构建的二进制文件：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;dragonfly/amd64&lt;/li&gt; 
   &lt;li&gt;illumos/amd64 and solaris/amd64&lt;/li&gt; 
   &lt;li&gt;linux/ppc64&lt;/li&gt; 
   &lt;li&gt;netbsd/*&lt;/li&gt; 
   &lt;li&gt;openbsd/386 and openbsd/arm&lt;/li&gt; 
   &lt;li&gt;windows/arm&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;涉及已删除文件的 conflict 解决处理方式已更改。现在，删除操作可以作为 conflict 解决的最终结果，从而导致已删除文件被移动到 conflict copy。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="text-align:start"&gt;&lt;span style="color:#1f2328"&gt;本次更新还提供以下版本：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;APT repository:&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fapt.syncthing.net%2F" target="_blank"&gt;https://apt.syncthing.net/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Docker image:&amp;nbsp;&lt;code&gt;docker.io/syncthing/syncthing:2.0.0&lt;/code&gt;&amp;nbsp;or&amp;nbsp;&lt;code&gt;ghcr.io/syncthing/syncthing:2.0.0&lt;/code&gt;(&lt;code&gt;{docker,ghcr}.io/syncthing/syncthing:2&lt;/code&gt;&amp;nbsp;to follow just the major version)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;更多详情可查看：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fsyncthing%2Fsyncthing%2Freleases%2Ftag%2Fv2.0.0" target="_blank"&gt;https://github.com/syncthing/syncthing/releases/tag/v2.0.0&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365789/syncthing-2-0-0-released</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365789/syncthing-2-0-0-released</guid>
      <pubDate>Mon, 11 Aug 2025 10:10:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>开源编程字体「Hack」创始人 Christopher Simpkins 去世</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Christopher Eric Simpkins 是知名开源编程字体「Hack」创始人，他于 2025 年 6 月 20 日在新罕布什尔州汉诺威突然&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftypo.social%2F%40Hilary%2F114845913381245488" target="_blank"&gt;去世&lt;/a&gt;，享年 51 岁。&lt;/p&gt; 
&lt;p&gt;&lt;img height="904" src="https://static.oschina.net/uploads/space/2025/0812/175131_lS6n_2720166.png" width="1150" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Christopher Simpkins 讣告页面&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.legacy.com%2Fus%2Fobituaries%2Fvnews%2Fname%2Fchristopher-simpkins-obituary%3Fid%3D58856786" target="_blank"&gt;显示&lt;/a&gt;，他&lt;span&gt;在佐治亚理工学院取得计算机博士学位后先在美军服役，退役又完成医学训练成为一名器官移植外科医生&lt;/span&gt;。他医术精湛、待人温和，被誉为「温柔的巨人」，拯救了许多生命并屡获教学奖。&lt;/p&gt; 
&lt;p&gt;后来他转向科技领域，&lt;span&gt;加入 Google Fonts 团队任&lt;/span&gt;高级用户体验项目经理&lt;span&gt;，&lt;/span&gt;专注字体开发&lt;span&gt;，发起 Codeface 项目为开发者整理并推荐高可读性的编程字体，持续推动开源字体生态。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0812/175331_L0CQ_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;2015 年，&lt;/span&gt;Christopher Simpkins 创造了&lt;span&gt;开源 Hack 字体，这款基于 DejaVu Sans Mono 重新调校的等宽字体迅速成为程序员最喜爱的编辑器字体之一。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;谷歌近期发布的开源字体&lt;/span&gt;&lt;a href="https://www.oschina.net/news/363609/googlesans-code" target="_blank"&gt;&amp;nbsp;Google Sans Code &lt;/a&gt;正是由&amp;nbsp;Christopher Simpkins 负责主导。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1710" src="https://static.oschina.net/uploads/space/2025/0812/180516_SXlX_2720166.png" width="1686" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365788</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365788</guid>
      <pubDate>Mon, 11 Aug 2025 10:07:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>英伟达推出 Cosmos 与 Nemotron 模型，推动物理 AI 与智能体发展</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblogs.nvidia.cn%2Fblog%2Fnvidia-opens-portals-to-world-of-robotics-with-new-omniverse-libraries-cosmos-physical-ai-models-and-ai-computing-infrastructure%2F" target="_blank"&gt;据英伟达官方消息&lt;/a&gt;，英伟达在技术领域再推新进展。其推出的 NVIDIA Cosmos 平台，整合前沿生成式世界基础模型（WFM）、先进分词器、护栏以及高效数据处理和管理工作流，旨在加速物理 AI 开发。该平台的世界基础模型经 2000 万小时真实世界数据训练，能预测和生成虚拟环境未来状态，助力开发者构建新一代机器人和自动驾驶汽车。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0812/174129_nIuV_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;同时，英伟达宣布推出 Nemotron 模型系列。Llama Nemotron 基于热门开源模型 Llama 构建，经剪枝和训练，在指令遵循等方面表现出色，能为 AI 智能体开发提供优化基础模组。Cosmos Nemotron 视觉语言模型（VLM）则可助力开发者构建智能体，使其能分析图像和视频并做出响应。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-9ddc7846bd0a958a9b0a9772dcf6c6a4e47.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;目前，已有众多物理 AI 领域的领先者，如机器人公司，以及自动驾驶汽车开发商等开始与 Cosmos 协作，加速模型开发进程。开发者可在 NVIDIA API 目录预览相关模型，并从 NGC 目录和 Hugging Face 下载模型系列与微调框架。&lt;/p&gt; 
&lt;p&gt;https://docs.nvidia.com/cosmos/&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365780</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365780</guid>
      <pubDate>Mon, 11 Aug 2025 09:41:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Altman：计划在未来 5 个月内将算力集群扩容一倍</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;OpenAI CEO 萨姆・奥尔特曼（SamAltman）在社交平台发文上表示，鉴于 GPT-5 带来的需求激增，该公司计划在未来几个月的算力优先分配如下：&lt;/span&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;首先确保当前付费版 ChatGPT 用户的总可用量比 GPT-5 推出前更多。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;其次优先满足 API 需求，直至达到当前分配的产能和已对客户做出的承诺。（粗略估计，以现有产能可在当前基础上再支持约 30% 的新 API 增长。）&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;提升 ChatGPT 免费版的服务质量。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;再优先满足新的 API 需求。计划在未来 5 个月内将算力集群扩容一倍，因此这一情况有望改善。&lt;/span&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="450" src="https://oscimg.oschina.net/oscnet/up-546ae50cc300f2af8894d1b266b905551e4.png" width="300" referrerpolicy="no-referrer"&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365777</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365777</guid>
      <pubDate>Mon, 11 Aug 2025 09:39:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>OpenAI 发布面向 GPT-5 的 Prompt 指南</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;OpenAI 官方写的 GPT-5 prompt 指南来了，看看官方是怎么让 GPT-5 表现更好的。该指南融汇贯通后，还可用于其他 AI 大模型。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0812/172857_F753_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;1、 明确角色和目标 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;开头就让 AI 模型知道它是谁、要做什么，比如：&lt;/p&gt; 
&lt;p&gt;你是资深前端工程师，请帮我在现有 React 项目里实现...&lt;/p&gt; 
&lt;p&gt;2、 设定工作方式 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;用分步指令，让模型按既定节奏走，而非一次性输出：&lt;/p&gt; 
&lt;p&gt;- 先分析需求和不确定点&lt;br&gt; - 再给执行计划 &amp;nbsp; &amp;nbsp;&lt;br&gt; - 按计划分步完成&lt;br&gt; - 每步结束时总结进度&lt;/p&gt; 
&lt;p&gt;3、 控制主动性 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;想要它多动脑，就加：在不确定时自行推断并执行，完成后再告知用户。 &amp;nbsp;&lt;br&gt; 想让它少跑偏，就加：仅按已知信息执行，不额外探索。&lt;/p&gt; 
&lt;p&gt;4、 给出完成标准 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;告诉模型何时算任务完成，比如： &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;当所有代码改动已在/app/theme 目录生效，并通过现有测试时，结束任务。&lt;/p&gt; 
&lt;p&gt;5、 嵌入风格与规范 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;在提示里放工程或写作规范，让它自动匹配你的需求： &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;变量用驼峰命名，CSS 类名用 BEM 规范，注释保持英文简短描述。&lt;/p&gt; 
&lt;p&gt;6、 善用示例 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;给它 1-2 个高质量示例，让它照着学，比空口说效果好得多。&lt;/p&gt; 
&lt;p&gt;7、 善用「工具前言」&lt;/p&gt; 
&lt;p&gt;工具前言可以写：先复述目标，再列计划，执行时简短说明当前步骤，最后单独总结成果。&lt;/p&gt; 
&lt;p&gt;8、 清除歧义 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;检查提示里是否有前后矛盾或模糊指令，否则 GPT-5 会花大量精力试图自圆其说，反而降低效率。&lt;/p&gt; 
&lt;p&gt;记住一个公式：角色+目标+步骤+完成标准+风格+示例，如此 GPT-5 才会既有创造力又不跑偏。&lt;/p&gt; 
&lt;p&gt;这本指南还涵盖了 API 参数具体怎么调，感兴趣的开发者可以看看。&lt;/p&gt; 
&lt;p&gt;地址：cookbook.openai.com/examples/gpt-5/gpt-5_prompting_guide&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365773/openai-gpt-5-prompting-guide</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365773/openai-gpt-5-prompting-guide</guid>
      <pubDate>Mon, 11 Aug 2025 09:29:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>字节跳动推出视频字幕无痕擦除方案，基于 DiT 大模型打造</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;字节跳动技术团队&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FKsl_lF8KNwM0vRtsjzWaBA" target="_blank"&gt;宣布&lt;/a&gt;推出一项创新技术，基于 DiT 大模型与字体级分割的视频字幕无痕擦除方案，旨在助力短剧等视频内容的全球化传播。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在全球化内容制作中，原始视频的中文字幕对于海外观众而言不仅是无效信息，还严重影响观看体验。传统的字幕添加或马赛克、GAN（生成对抗网络）等字幕擦除方案，往往导致画面杂乱、模糊或帧间闪烁，无法彻底解决这一问题。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;火山引擎视频点播推出的这一方案，通过两大核心技术突破和强大的工程能力，重新定义了字幕擦除标准，实现了全片真实自然的「无痕擦除」，并支持多字幕框、指定时间段的精准擦除。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="262" src="https://oscimg.oschina.net/oscnet/up-e6a7ee75485165b360e216e57f4f3f2e85f.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;该方案的核心在于两个技术突破：一是 DiT 视频字幕擦除模型，二是字体级分割模型。DiT 模型通过强鲁棒性预训练基底、摆脱辅助先验依赖、两阶段训练策略提升鲁棒性与修复精细度，实现了像素级无痕修复。字体级分割模型则通过精准定位目标区域，实现了从「粗放擦除」到「像素级修复」的转变，有效避免了传统块填充导致的背景模糊或纹理重复问题。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="143" src="https://oscimg.oschina.net/oscnet/up-dc217440e603a0e87eec97675dbaaf606fc.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;火山引擎多媒体实验室联合工程团队构建了兼顾精度与效率的技术体系，经过超万集视频数据集验证，擦除任务成功率达到 100%。创新的视频分镜技术结合服务器集群分布式计算，显著提升了视频处理效率。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;此外，该方案还支持多语言内容流转，突破了中英文限制，支持多个小语种字幕擦除，为全球内容流转提供了双向通道。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365771</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365771</guid>
      <pubDate>Mon, 11 Aug 2025 09:23:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Spring AI 1.0.1 发布</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Spring AI 1.0.1 现已&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fspring.io%2Fblog%2F2025%2F08%2F08%2Fspring-ai-1" target="_blank"&gt;发布&lt;/a&gt;，此版本包括&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fspring-projects%2Fspring-ai%2Freleases%2Ftag%2Fv1.0.1" target="_blank"&gt;150 多项变化，&lt;/a&gt;重点关注稳定性、增强功能和文档改进。&lt;/p&gt; 
&lt;h4&gt;&lt;strong&gt;展望未来：Spring AI 1.1 及未来&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;1.0.1 版本专注于稳定性和错误修复，而 Spring AI 团队正在为 1.1 版本开发新功能。&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fclaude.ai%2Fpublic%2Fartifacts%2Fe211dc9e-249d-425d-abd6-9425b8a2bf16" target="_blank"&gt;2025 年路线图&lt;/a&gt;提供了关键日期，并展示团队基于全新 Spring Boot 4 基础的 Spring AI 2.0 的规划重点。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Spring AI 1.1 的当前重点领域&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;1.1 版本将专注于一系列高影响力的增强功能和有针对性的基础工作，并明确关注在代码冻结之前能够切实完成的工作。&lt;/p&gt; 
&lt;hr&gt; 
&lt;h4&gt;&lt;strong&gt;1. Model Context Protocol (MCP)&amp;nbsp;支持&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;与最新的 MCP Java SDK 版本深度集成，使 Spring AI 与最新的协议和传输功能保持一致：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;多协议版本协商&lt;/strong&gt;（2024-11-05 和 2025-03-26）。&lt;/li&gt; 
 &lt;li&gt;通过新的传输定制器&lt;strong&gt;实现 OAuth2 安全的 MCP 服务器连接。&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;可流式传输的 HTTP 和 WebMVC/HttpServlet 服务器传输，用于反应式和 servlet 部署。&lt;/li&gt; 
 &lt;li&gt;使用 JSON Schema 强制执行的&lt;strong&gt;结构化输出验证。&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;分页、保持活动 ping、URI 模板支持&lt;/strong&gt;更丰富的资源交互。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;改进的错误处理、日志记录和初始化流程&lt;/strong&gt;。&lt;/li&gt; 
 &lt;li&gt;迁移到&amp;nbsp;&lt;strong style="color:#363636"&gt;builder-based APIs&lt;/strong&gt;&lt;span style="background-color:#ffffff; color:#000000"&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;for tools and transport providers&lt;/span&gt;。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;h4&gt;&lt;strong&gt;2. Core Responses API Enhancements&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;扩展 Responses API 以缩小功能差距、改善 provider parity 并引入最新的 SDK 功能：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;及时缓存&lt;/strong&gt;以减少延迟和成本。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;「Thinking」模型支持&lt;/strong&gt;增强推理能力。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;消息批处理&lt;/strong&gt;以实现更高的吞吐量。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;跨提供商的本机 JSON 模式&lt;/strong&gt;和更强大的结构化输出处理。&lt;/li&gt; 
 &lt;li&gt;在保持统一 API 的同时，为&lt;strong&gt;提供商特定的扩展&lt;/strong&gt;提供 Hook。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Google Vertex AI SDK 更新&lt;/strong&gt;– 升级到最新 SDK 以： 
  &lt;ul&gt; 
   &lt;li&gt;解锁新发布的 endpoints（包括非聊天 API）。&lt;/li&gt; 
   &lt;li&gt;确保与增强的 Responses API 功能兼容。&lt;/li&gt; 
   &lt;li&gt;带来安全修复和长期支持。&lt;/li&gt; 
   &lt;li&gt;刷新并扩展 Vertex AI 集成测试。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;h4&gt;&lt;strong&gt;3. Chat Memory&amp;nbsp;改进&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;改进 Spring AI 在生产环境中的内存管理：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;内存压缩&lt;/strong&gt;来管理 token 预算。&lt;/li&gt; 
 &lt;li&gt;可配置长期对话的保留策略。&lt;/li&gt; 
 &lt;li&gt;改进了自定义内存存储的集成点。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;h4&gt;&lt;strong&gt;4. 可观察性和多客户端配置&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;简化的可观察性设置&lt;/strong&gt;，包括更容易与 Langfuse 等工具集成。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;多客户端配置改进&lt;/strong&gt;，简化了在同一应用程序中与多个提供商的工作流程。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;h4&gt;&lt;strong&gt;5. Net new areas&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;这些大多是全新的实现，如果时间紧迫，可能超出 1.1 版本范围，但早期准备工作可能已开始：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Azure OpenAI&lt;/strong&gt;&amp;nbsp;– 新的 SDK 支持。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;向量存储改进&lt;/strong&gt;，包括混合搜索。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reranking&amp;nbsp;–&amp;nbsp;&lt;/strong&gt;对&amp;nbsp;re-ranker&amp;nbsp;模型提供一流的支持。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enterprise guardrails&amp;nbsp;–&amp;nbsp;&lt;/strong&gt;安全性和合规性功能。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;h4&gt;&lt;strong&gt;6. 可能进入孵化阶段的项目&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;MemGPT-style chat memory&amp;nbsp;实现&lt;/strong&gt;。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AgentClient&amp;nbsp;&lt;/strong&gt;用于通过 Spring AI 运行自主 CLI 代理（例如 Claude Code）。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;p&gt;公告表示，项目团队将继续调整优先事项，以努力实现&amp;nbsp;9 月 23 日 1.1 版的 code freeze，同时平衡近期交付成果与 Spring AI 2.0 的战略基础。&lt;/p&gt; 
&lt;p&gt;更多详情可&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fspring.io%2Fblog%2F2025%2F08%2F08%2Fspring-ai-1" target="_blank"&gt;查看官方公告&lt;/a&gt;。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365766/spring-ai-1-0-1-released</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365766/spring-ai-1-0-1-released</guid>
      <pubDate>Mon, 11 Aug 2025 09:09:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>谷歌向发现 Chrome 高危漏洞的安全研究员奖励 25 万美元</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span&gt;谷歌近日依据漏洞奖励计划（VRP）向一名安全研究员&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fissues.chromium.org%2Fissues%2F412578726" target="_blank"&gt;发放 25 万美元（约合 179.8 万元人民币）奖金&lt;/a&gt;，奖励其发现 Chrome 浏览器高危漏洞。 &lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0812/170559_YQ7o_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;该研究员于 4 月 23 日报告了一个「沙盒逃逸」漏洞，编号为 CVE-2025-4609，存在于 Chrome 内核的 IPCZ 通信系统中。攻击者可通过诱导用户访问恶意网站，利用该漏洞突破浏览器沙箱限制，实现远程代码执行。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;尽管研究员最初将其标记为「中等危害」，但谷歌评估其严重性为 S0/S1 级，并列为 P1 优先级修复。 谷歌已于 5 月发布的 Chrome 更新中修复该漏洞，并在 8 月 12 日公开披露细节。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;根据谷歌「漏洞猎人（&lt;/span&gt;Google Bug Hunters&lt;span&gt;）」计划，提交包含 RCE 演示的高质量非沙盒进程逃逸或内存损坏漏洞报告，可获 2.5 万至 25 万美元奖励，此次为顶格奖励。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365764</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365764</guid>
      <pubDate>Mon, 11 Aug 2025 09:06:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Actual - 个人理财工具</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                        
                                                                                    &lt;p&gt;&lt;span style="background-color:#ffffff; color:#1f2328"&gt;Actual 是一款本地优先的个人理财工具。它 100% 免费开源，使用 NodeJS 编写，并具备同步功能，方便用户在不同设备之间轻松迁移更改。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img height="271" src="https://static.oschina.net/uploads/space/2025/0806/154746_TajJ_4252687.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

                                                                    &lt;/div&gt;
                                                                </description>
      <link>https://www.oschina.net/p/actual</link>
      <guid isPermaLink="false">https://www.oschina.net/p/actual</guid>
      <pubDate>Mon, 11 Aug 2025 08:53:00 GMT</pubDate>
    </item>
    <item>
      <title>360 智脑推出 Light-IF 系列模型</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;360 智脑团队&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FnwyQDZxYGFFA5pTmkxn3JQ" target="_blank"&gt;宣布&lt;/a&gt;推出全新的 Light-IF 系列模型，这一创新框架旨在显著提升大型语言模型（LLM）在复杂指令遵循方面的能力。随着人工智能技术的不断进步，尽管 LLM 在数学、编程等领域已经展现出了卓越的推理能力，但在遵循复杂指令方面仍存在不足。为了解决这一问题，360 智脑团队提出了以预览-自检式推理和信息熵控制为核心的 Light-IF 框架。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;Light-IF 框架通过五个关键环节来提升模型性能:难度感知指令生成、Zero-RL 强化学习、推理模式提取与过滤、熵保持监督冷启动、熵自适应正则强化学习。这一框架的提出，旨在破解当前推理模型中存在的「懒惰推理」现象，即模型在思考阶段仅复述指令而不主动检查约束是否被满足，导致指令执行不准确的问题。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;img height="316" src="https://oscimg.oschina.net/oscnet/up-30ae24d430fc7fd7a393ecaf7c48fbadefc.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;在实验中，Light-IF 系列模型在 SuperCLUE、IFEval、CFBench 及 IFBench 四个中文和跨语言指令遵循基准上均取得了显著提升。特别是 32B 版本的 Light-IF-32B，其在 SuperClue 得分达到了 0.575，比下一个最佳模型高出 13.9 个百分点。此外，参数规模仅为 1.7B 的 Light-IF-1.7B 在 SuperClue 和 IFEval 上的表现甚至超过了 Qwen3-235B-A22B 等体量更大的模型。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;360 智脑团队表示，Light-IF 系列模型的推出，不仅为开源社区提供了一套可复现的完整路线和配套的开源代码，而且全系模型将陆续开放，供社区使用、对比与复现。同时，训练中使用的冷启动数据集也将同步开放。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;此外，360 与 SuperCLUE 联合推出的中文精确指令遵循测评基准 SuperCLUE-CPIFOpen 也将在 Github 上开放，便于研究者评测模型的中文精确指令遵循能力。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365748</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365748</guid>
      <pubDate>Mon, 11 Aug 2025 08:31:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>MiniMax 发布全球首个可交易 Agent Remix Marketplace</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;MiniMax 稀宇科技宣布推出全球首个 Agent Remix Marketplace，并启动了一项奖金高达 15 万美金的全球挑战赛。这一创新平台旨在将个人的想法转化为商业价值，让每个人都能成为「个体 GDP 创造者」。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;Agent Remix Marketplace 是一个允许用户一键提效的工具，用户可以通过点击「Remix」对已发布的成熟作品进行再创作，无需从零开始，从而将效率提升 10 倍。此外，用户还可以通过发布自己的 Agent 作品至 Gallery 并允许他人 Remix，每次作品被 Remix 都能获得 100 积分的收益。这不仅是一个创作和分享的平台，也是一个涨粉和建立个人品牌的利器。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="285" src="https://oscimg.oschina.net/oscnet/up-d064cf16f7166c2be8c20d9ed0ee1bc940e.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;MiniMax 强调，这一平台是「Agent 全民经济」的颠覆性突破，具有四大独家优势。用户可以轻松地 Remix 各种模板，如香氛蜡烛电商模板，快速开启自己的电商创业。此外，用户还可以定制任何行业或主题的 Daily Newsletter，甚至将 Netflix 和 Bilibili、LinkedIn 和 Tinder 等不同平台的功能进行 Remix，创造出全新的用户体验。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在技术层面，MiniMax 在研发过程中注重 Agent 的可靠性，包括上下文压缩总结、API 信息脱敏引擎以及多 Agent 任务路由等技术，以确保用户数据的安全和隐私。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;全球挑战赛面向所有人开放，鼓励参与者用自己的想法挑战 15 万美金的奖池。挑战赛分为原创和 Remix 双赛道，无论是原创作品还是基于已发布作品的二创，都有机会获奖。参与者无需代码能力，即可参与这一全球智能普惠的活动。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;体验地址：&lt;/span&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fminimax-agent-hackathon.space.minimax.io%2F" target="_blank"&gt;https://minimax-agent-hackathon.space.minimax.io/&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365744</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365744</guid>
      <pubDate>Mon, 11 Aug 2025 08:20:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>华为发布 AI 推理创新技术 UCM：可实现高吞吐、低时延推理体验，计划 9 月开源</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbaijiahao.baidu.com%2Fs%3Fid%3D1840229710674780713%26wfr%3Dspider%26for%3Dpc" target="_blank"&gt;根据报道&lt;/a&gt;，华为正式发布了 AI 推理创新技术 UCM（推理记忆数据管理器）。&lt;/p&gt; 
&lt;p&gt;华为推出的 UCM（推理记忆数据管理器）是一款以 KV Cache 为中心的推理加速套件，融合多类型缓存加速算法工具，通过分级管理推理过程中产生的 KV Cache 记忆数据，扩大推理上下文窗口，实现高吞吐、低时延的推理体验，，降低每 Token 推理成本。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0812/160552_0ocB_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;此外，华为计划于 2025 年 9 月正式开源 UCM，届时将在魔擎社区首发，后续逐步贡献给业界主流推理引擎社区，并共享给业内所有 Share Everything (共享架构) 存储厂商和生态伙伴。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365742</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365742</guid>
      <pubDate>Mon, 11 Aug 2025 08:06:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Claude 新增聊天记录记忆功能</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Anthropic 为其 Claude 聊天机器人推出备受期待的&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Fclaudeai%2Fstatus%2F1954982275453686216" target="_blank"&gt;「记忆」功能&lt;/a&gt;，用户可让机器人检索并参考过往对话内容。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0812/155847_bdCE_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;该功能支持网页、桌面及移动端，能区分不同项目和工作区。用户只需在 「个人资料」 的 「设置」 中开启 「搜索和查看聊天记录」，即可使用。&lt;/p&gt; 
&lt;p&gt;目前，Claude 的 Max、Team 和 Enterprise 订阅层级已率先上线，其他套餐将在近期开放。与 ChatGPT 的持续记忆不同，Claude 的记忆功能为被动触发模式，仅在用户明确要求时才检索过往对话，且不会构建用户画像。&lt;/p&gt; 
&lt;p&gt;作为 AI 领域的头部企业，Anthropic 与 OpenAI 竞争激烈，双方在语音模式、上下文窗口、订阅服务等方面不断角力。此次记忆功能的推出，旨在提升用户黏性和使用时长。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365741</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365741</guid>
      <pubDate>Mon, 11 Aug 2025 07:59:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>微软为 Excel 加入 AI 公式讲解，内联解释直达单元格</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span&gt;微软&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftechcommunity.microsoft.com%2Fblog%2Fexcelblog%2Fexplain-formulas-with-copilot%25E2%2580%2594now-on-the-grid%2F4424028" target="_blank"&gt;宣布&lt;/a&gt;，其电子表格工具 Excel 迎来一项重要更新：由 Copilot 驱动的&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;strong&gt;「解释此公式」（Explain Formula）&lt;/strong&gt;&lt;span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;功能正式上线，旨在帮助用户快速理解复杂公式，显著提升数据处理效率。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;该功能的&lt;span&gt;最大&lt;/span&gt;亮点在于操作简便。用户无需单独打开聊天面板，只需点击包含有效公式的单元格，并在旁边的 Copilot 图标中选择「解释此公式」，即可在单元格内直接获得内联解释。这些解释基于当前工作表的上下文生成，比传统网络搜索更精准、更贴合实际工作场景。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="225" src="https://oscimg.oschina.net/oscnet/up-68c25bd98edf5ade9b15bb52dea74ff751f.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;微软表示，Copilot 能够分解并逐步讲解各种复杂程度的公式，帮助用户快速掌握其逻辑。默认情况下，解释会以内联形式显示;若 Copilot 聊天面板已开启，内容将优先在面板中呈现。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;目前，该功能正分阶段向 Windows 版和网页版 Excel 用户推送。微软鼓励用户在每次使用后通过点赞或点踩反馈，协助优化 AI 解释效果。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365740</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365740</guid>
      <pubDate>Mon, 11 Aug 2025 07:54:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
  </channel>
</rss>
