<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - news - 简体中文</title>
    <link>https://www.oschina.net/news/project</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news" rel="self" type="application/rss+xml"/>
    <description>已对该 RSS 进行格式化操作：中英字符之间插入空格、使用直角引号、标点符号修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-cn</language>
    <lastBuildDate>Wed, 23 Jul 2025 07:48:02 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>AI 将在五年内成为企业生存的 「基本技能」</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;美国科技亿万富翁马克・库班最近在接受采访时表示，人工智能（AI）将在未来五年内成为每个职场人士必备的 「基本技能」，就像电子邮件和 Excel 软件一样普及。他强调，企业主如何有效利用人工智能将直接影响他们在未来几年的竞争力和成功。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="340" src="https://oscimg.oschina.net/oscnet/up-6045b74ea4f19a7984f7137d2b1c96ff8a2.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;库班提到，在接下来的十年内，人工智能助手将极大改变工作模式。他预测，未来会有越来越多的人选择自己创业，借助人工智能的帮助，单个创始人将能够承担起一个完整团队的工作。这一变化不仅能够提升个人的工作效率，还能够为更多人打开创业的大门。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;他指出，企业主在利用人工智能的过程中，必须快速做出决策并行动。他表示:「如果你没有利用人工智能来更快地决策，你就会落后于竞争对手。」 库班将人工智能视为企业主可以投资的 「团队成员」，它能够协助企业主完成多个角色的工作，包括运营副总裁、销售代表、数据分析师及法律顾问，而这些角色的工作费用将大幅降低。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;不过，库班也提醒企业家们，尽管 AI 具有强大的能力，他们在使用人工智能时仍需谨慎。他建议将人工智能视作 「最聪明的实习生」，不仅要提出正确的问题，还要认真审核其提供的答案。他指出，许多企业家在当前环境中面临的&lt;span&gt;最大&lt;/span&gt;挑战是 「恐惧和资金」。然而，人工智能代理的出现，将帮助那些由于招聘成本而无法进入某一行业的创业者，打破这一壁垒。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;库班总结道，人工智能不仅是工具，更是企业家们的强大杠杆。成功的企业家将懂得如何有效利用这一技术，确保在竞争中立于不败之地。他强调:「最终，人工智能是一个乘数，善用它，但不要被它操控。」&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361931</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361931</guid>
      <pubDate>Wed, 23 Jul 2025 07:40:49 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>亚马逊云科技上海 AI 研究院解散</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;AWS 亚马逊云科技上海 AI 研究院的首席应用科学家王敏捷发朋友圈称，「刚收到通知，AWS 亚马逊云科技上海 AI 研究院（也是 AWS 最后一个海外研究院）正式解散。」&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-0bf820d1cb694fdf814a9c083a90582b673.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;王敏捷感慨道，「近 6 年带队时光，赶上了外企研究院的黄金周期，更得益于张峥老师的细心指导，有幸成为 AWS 亚太地区最年轻的首席应用科学家。」&lt;/p&gt; 
&lt;p&gt;今日，亚马逊云科技就上海 AI 研究院解散一事&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbaijiahao.baidu.com%2Fs%3Fid%3D1838398566569114998%26wfr%3Dspider%26for%3Dpc"&gt;回应新浪科技称&lt;/a&gt;：「经过对公司组织、发展重点及未来战略方向的深入评估，我们决定对亚马逊云科技部分团队进行人员精简。」&lt;/p&gt; 
&lt;p&gt;亚马逊云科技表示，「做出这些决定是非常艰难的，我们将全力支持员工顺利过渡，我们做出这些必要的决定，是为了持续投资、优化资源，为客户带来更多的创新」。&lt;/p&gt; 
&lt;p&gt;今年以来，已有多家科技巨头对其在华研发业务进行了调整。&lt;/p&gt; 
&lt;p&gt;2025 年 3 月，IBM 关闭了在中国运营了 32 年的研发部门，另外，有媒体报道微软也在逐步关闭其上海 AI 实验室，并将数百名专家搬迁至美国、澳大利亚、爱尔兰等地。今年 6 月，花旗集团宣布，作为在全球持续推进的简化工作的一部分，花旗将精简位于中国上海和大连的全球技术解决中心，减少约 3500 名技术人员。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361930</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361930</guid>
      <pubDate>Wed, 23 Jul 2025 07:37:49 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Fedora 考虑放弃 DVD 光盘启动支持</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Fedora 项目正在向其用户和开发者社区征求反馈，探讨是否可能更新其发布标准，使其不再阻止光盘启动问题（DVD 映像），以及是否继续将基于 Intel 的 Mac 的双启动问题视为发布阻碍。&lt;/p&gt; 
&lt;p&gt;提出的&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdiscussion.fedoraproject.org%2Ft%2Fproposal-drop-optical-media-boot-release-criterion%2F160524" target="_blank"&gt;第一项建议&lt;/a&gt;是，是否放弃 Fedora 发布标准中关于光盘启动支持的内容。这涉及 Fedora 发行版安装程序映像，这些映像必须在写入光盘（DVD）后才能启动。Fedora 已于 2020 年正式停止测试光盘支持，目前他们正在考虑使其不再阻止任何已报告的问题：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;自 2020 年以来，质量团队不再被要求测试光学介质，但发现的任何问题仍然可能成为阻碍。我们不太喜欢这个解决方案（阻止我们未测试的内容），但测试光学启动实在太耗时，而且已经很小众了。五年后的今天，我们认为是时候放弃整个标准了。光学启动的重要性早已不复存在，我们认为是时候将其从名为发布标准的「关键功能列表」中移除了。这不会为我们节省太多时间（我不记得过去几年出现过任何引人注目的光学启动问题），但它可以稍微简化我们的测试矩阵（使其更易于阅读），解决发布阻止状态和质量覆盖范围之间的矛盾，并且如果发现问题，它可以让我们丢弃仍然准备好的 DVD 驱动器和介质（但可能已经无法使用了）。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.phoronix.com%2Fnews%2F%3D%2522https%3A%2F%2Fdiscussion.fedoraproject.org%2Ft%2Fproposal-drop-intel-based-macbook-dual-boot-release-criterion%2F160525%2522" target="_blank"&gt;第二项提案&lt;/a&gt;是关于取消基于 Intel 的 Apple MacBook 双启动发布标准。当前的发布标准指出，Fedora 安装程序必须能够与基于 Intel 的 Mac 电脑上现有的 macOS 安装一起双启动/安装到可用空间中。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;「这个标准正在随着时间的推移而被淘汰，因为苹果多年前就停止生产基于英特尔的 Mac（取而代之的是「Apple Silicon」 M* 处理器，目前 Fedora 并不支持这种处理器）。最后几款可以合理使用 Fedora 的 MacBook 是 2017 年款，它仍然包含 T1 安全芯片（较新的型号有 T2 芯片，并且它们的内部键盘和触摸板不适用于 Fedora 内核）。2017 年款的系统更新支持将于今年结束，老款已经过时。这意味着 2017 年之前款的用户很可能已经切换到 Fedora，如果他们愿意的话，2017 年款的用户可能会在今年这样做，而且未来不会有这样的用户，因为他们的硬件得不到 Fedora 的良好支持。」&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;这主要涉及 2017 年及更早的 Apple Mac，因为配备 T2 安全芯片的较新 Apple Mac 目前在 Fedora Linux 上运行不佳。这不会影响 Apple Silicon Mac 的任何变更，只会影响 Intel x86_64 系统。&lt;/p&gt; 
&lt;p&gt;由于 Fedora 质量团队目前对剩余的 Intel Mac 的访问权限较少，并且能够投入较少时间来修复这些老旧系统的任何问题，因此现在的愿望是不要将任何 Apple Mac 双启动问题视为发布阻碍。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361929</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361929</guid>
      <pubDate>Wed, 23 Jul 2025 07:31:49 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>周鸿祎：最近都采购华为芯片，英伟达 H20 性价比不高</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;7 月 23 日，360 集团创始人周鸿祎&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.yicai.com%2Fnews%2F102737141.html"&gt;对第一财经记者表示&lt;/a&gt;，360 的芯片采购正转向国产芯片，最近采购的都是华为的产品，未涉及英伟达 H20 芯片。&lt;/p&gt; 
&lt;p&gt;他坦言，尽管国产芯片与英伟达产品存在差距，但必须坚持使用，因为只有通过大量应用，才能推动国产芯片持续改进。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0723/152238_twYV_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;H20 是英伟达针对中国市场推出的 H100 简化版，采用 Hopper 架构和 CoWoS 封装技术，但性能差距明显。周鸿祎分析，H20 更适合 AI 推理任务，而推理对芯片要求较低，无需高速互联和集群部署。在此领域，国产芯片性价比更高，使得定位中间的 H20 显得尴尬。当前美国芯片出口限制背景下，华为等企业加大研发，其升腾系列芯片性能逐步提升，部分领域已超越 H20。360 的选择既获性价比优势，也助力国产芯片产业迭代，实现双赢。&lt;/p&gt; 
&lt;p&gt;此前，360 已采购约 1000 片华为升腾 910B AI 芯片，并合作将 AI 框架移植到该芯片上。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361928</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361928</guid>
      <pubDate>Wed, 23 Jul 2025 07:24:49 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>深度剖析 TDMQ RabbitMQ 版经典队列底层存储机制</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;h2&gt;导语&lt;/h2&gt; 
&lt;p&gt;RabbitMQ 作为开源消息队列的标杆产品，凭借灵活的路由机制与高可用设计，支撑着海量业务场景的消息流转。而经典队列（Classic Queue） 作为 RabbitMQ 最基础、应用最广泛的队列类型，其底层存储机制直接决定了消息处理的性能边界与可用性上限。&lt;/p&gt; 
&lt;p&gt;理解经典队列的存储架构，不仅是掌握 RabbitMQ 核心原理的关键，更为生产环境的运维优化提供了理论支撑。本文将从文件目录结构、存储格式定义、读写流程到运维实践策略，全面解析经典队列的底层存储实现逻辑，帮助读者深入理解其在消息生命周期管理中的核心作用。&lt;/p&gt; 
&lt;h2&gt;经典队列介绍&lt;/h2&gt; 
&lt;p&gt;RabbitMQ 作为一款历史悠久的开源消息队列，被广泛应用于各个领域。在 RabbitMQ 中，用户使用虚拟主机（Vhost）隔离资源，交换机负责路由消息，队列则是消息存储的最小单元。&lt;/p&gt; 
&lt;p&gt;用户通过客户端与 RabbitMQ 的服务端建立连接后，基于通道（Channel）实现消息的高效交互：生产者经过通道将消息发送至交换机，由交换机按绑定规则路由至目标队列；消费者则通过通道从队列中拉取消息，完成业务逻辑处理。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-356020c87a4782a64efc9913f5c4615dfea.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在这一过程中，队列作为消息生命周期的核心载体，衍生出三种差异化实现：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;经典队列（Classic Queue）&lt;/strong&gt;：采用轻量级索引与共享存储架构，在单机性能与存储效率间取得平衡，适用于高吞吐非强一致性场景；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;仲裁队列（Quorum Queue）&lt;/strong&gt;：基于 Raft 协议实现多副本强一致性，保障关键业务数据不丢失，适用于金融交易、订单管理等关键业务；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;流队列（Stream Queue）&lt;/strong&gt;：以日志结构存储消息流，支持回溯消费与持久化流处理，适用于实时数据分析场景。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;经典队列作为使用频率最高的队列，了解它的存储机制对于理解其可用性和性能至关重要，接下来将从存储架构、文件格式、读写流程等维度，深入解析经典队列的底层实现逻辑。&lt;/p&gt; 
&lt;h2&gt;存储架构解析&lt;/h2&gt; 
&lt;h3&gt;目录结构&lt;/h3&gt; 
&lt;p&gt;RabbitMQ 通过虚拟主机（Vhost）实现资源隔离，每个 Vhost 有独立的物理存储目录，其典型结构如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;vhost_name/
├── msg_store_persistent/      # 共享存储目录，存储大消息
│   ├── 0.rdq                  # 共享存储文件
│   └── 1.rdq                  # 支持文件滚动
└── queues/                    # 队列专属存储目录
    └── queue_name/            # 单个队列目录
        ├── queue_name.qi      # 队列索引文件
        └── queue_name.qs      # 队列存储文件
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;msg_store_* 是共享存储目录，顾名思义是这个 Vhost 下所有队列共享的存储。由于 Exchange 可能会将同一条消息路由到不同的队列，而将同一条消息存储多次会增加磁盘空间占用，因此经典队列会将大小超过某个阈值的消息存储在共享存储下，通过引用计数来管理这部分消息。&lt;/p&gt; 
&lt;p&gt;每个队列在 queues 目录下都有属于自己的目录，队列目录下主要有两类文件：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;队列存储&lt;/strong&gt;：名称为 *.qs 的文件，负责存储这个队列中消息大小小于这个阈值的消息。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;队列索引&lt;/strong&gt;：名称为 *.qi 的文件，负责存储消息元数据和消息所在位置。队列索引存储了消息的偏移或唯一标识，通过它们可以定位到消息在队列存储或共享存储中的位置，索引文件中的 Entry 和存储文件中的 Entry 因此在逻辑上构成了一对一的映射关系。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-c21a1914865fc33ac57c80f3056a8a6522b.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;队列索引&lt;/h3&gt; 
&lt;p&gt;队列索引文件由一个 Header 和若干 Entry 组成，Entry 的数量由 classic_queue_index_v2_segment_entry_count 这一参数控制，默认为 4096。Entry 有两种类型：Publish Entry 和 Ack Entry。&lt;/p&gt; 
&lt;p&gt;生产者将消息成功发送到队列后会产生一个 Publish Entry，队列将这条消息投递给消费者并且得到消费者确认后会使用 Ack Entry 覆盖原来的 Publish Entry，代表这条消息可以被删除。&lt;/p&gt; 
&lt;p&gt;Publish Entry 存储了这条消息的元数据，包括 MsgId、SeqId、存储位置、消息属性和是否持久化的标识。&lt;/p&gt; 
&lt;p&gt;MsgId 是 RabbitMQ 为每条消息随机生成的 GUID，用来确定消息在共享存储的位置。&lt;/p&gt; 
&lt;p&gt;SeqId 是这条消息在队列中的序号，用来决定消息在队列索引和队列存储中的位置。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-0d7a0133f530ca3140ec9f5eae8f0132f65.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;队列存储&lt;/h3&gt; 
&lt;p&gt;队列存储文件和索引文件是一对一的关系，当队列删除它的索引文件时，也会删除对应的存储文件。队列存储文件的结构与索引文件类似，也是由 Header 和 Entry 构成。Header 和 Entry 的具体组成如下所示。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-f1cb3f97618cb1b9d332ea7825e2572cb48.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;共享存储&lt;/h3&gt; 
&lt;p&gt;ETS 是 Erlang 内置的单机 KV 存储，共享存储使用 ETS 维护了两个组件：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Index&lt;/strong&gt;：是 MsgId 到消息位置的映射。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;FileSummary&lt;/strong&gt;：文件到文件统计信息的映射。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;经典队列在读取消息时通过索引文件中的 Publish Entry 获取到 MsgId 后还需要从 Index 中获取消息的具体位置，包括这条消息所在的文件、偏移以及它的引用计数。相同 MsgId 的多条消息只会被写入一次，删除消息时，它的引用计数会被减一。文件统计信息中记录了文件中有效数据的数量，这在整理文件时会被用到。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-0c96ca072a09a3c72ec0b3a9950eed78807.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;共享存储文件的大小由参数 msg_store_file_size_limit 控制，默认为 16MB。每个文件由若干个 Entry 组成，每个 Entry 的具体组成如下所示。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-c58a84b548a6cf8d3a9adc321ae22d8fe7c.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h2&gt;核心工作流程&lt;/h2&gt; 
&lt;h3&gt;消息写入&lt;/h3&gt; 
&lt;p&gt;RabbitMQ 根据消息大小决定将消息写入到哪个存储。如果消息大小大于或等于某个值（由参数 queue_index_embed_msgs_below 控制，默认为 4KB），RabbitMQ 会将其存于共享存储中，否则会存于队列存储中。&lt;/p&gt; 
&lt;p&gt;将消息写入存储时会直接写到内部缓冲区：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;队列存储内部的缓冲区大小由参数 classic_queue_store_v2_max_cache_size 控制，默认为 512KB。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;共享存储内部的缓冲区大小则固定为 1MB。将消息写入到共享存储时除了需要写入到缓冲区外，还需要更新它内部的 Index 和 FileSummary 组件。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;缓冲区大小超过限制后会 Flush 其中的数据，值得注意的是，Flush 时不会调用 Fsync，而是调用 Write 将数据写入到操作系统的 Page Cache 上。这种方式通过牺牲数据安全性以获得更低的延迟，如果需要更强的数据安全性应使用仲裁队列。&lt;/p&gt; 
&lt;p&gt;存储写入完成后需要在队列索引文件中写入 Publish Entry，此时消息被认为成功写入了。之后还要更新内存中的消息缓存，以加速消息读取。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-06d103e2eaab27dd60507d35936ea839341.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h2&gt;消息读取&lt;/h2&gt; 
&lt;p&gt;经典队列在内存中维护了专门的缓存来提升读取性能，底层存储会根据队列的消费速率批量读取不超过 2048 条消息到缓存中。读取消息时会先检查缓存中是否有这条消息，如果有则直接返回，否则会先将消息批量读取到缓存。&lt;/p&gt; 
&lt;p&gt;将消息从磁盘批量读取到内存中需要先到队列索引中读取元数据，然后分别到队列存储和共享存储中读取消息体，并将它们组装到一起。即便缓存中有消息，但是实际的消息体仍然可能不在缓存中，因为过大（&amp;gt;12KB）过少（&amp;lt;10 条）的消息的消息体并不会被读到缓存里，需要在投递消息时逐条去磁盘中读取消息体。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-62aec5d5efbcb40873a77832b210429310c.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h2&gt;文件整理&lt;/h2&gt; 
&lt;p&gt;共享存储会定时整理有效数据占比低于一半的文件以回收空间。整个过程分为三步：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;将文件末尾的有效数据拷贝到文件前面的无效数据处。&lt;/li&gt; 
 &lt;li&gt;更新 Index 组件。&lt;/li&gt; 
 &lt;li&gt;在没有进程读取文件后截断文件。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;RabbitMQ 会将文件中的无效数据置 0，称为空洞（blank holes）。在文件整理时，RabbitMQ 从最后一条有效消息开始查看其是否能填补前面的空洞，如果可以就将其拷贝到前面，如果它比前面的任何一个空洞都大，那么这一次的文件整理将无法释放任何空间，这是为了防止意外覆盖被移动过的消息。Index 组件中存储了消息的位置，拷贝完成后需要更新对应消息的位置。在没有进程读取文件后就可以截断这个文件以节省磁盘空间。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-92a86e83dda9f187412ce495b8d272dad17.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h2&gt;运维实践&lt;/h2&gt; 
&lt;h3&gt;发送确认&lt;/h3&gt; 
&lt;p&gt;为了提高消息发送的可靠性，我们推荐用户打开发送确认（Confirm）。RabbitMQ 会在将消息从缓冲区 Flush 到磁盘后向客户端发送 Confirm，此时生产者可以认为这条消息已经被成功发送到队列。&lt;/p&gt; 
&lt;h3&gt;消费确认&lt;/h3&gt; 
&lt;p&gt;为了提高消息消费的可靠性，我们推荐用户打开手动确认（Manual Ack）。RabbitMQ 在收到 Ack 后会写入 Entry 到队列索引中，只有在索引文件中的所有 Publish Entry 全部被 Ack 后，才会删除该文件。如果消费者在发送 Ack 前宕机了，RabbitMQ 会重复投递这条消息，确保消息能真正被消费掉。未被客户端 Ack 的消息会堆积在内存中，如果数量过多则可能触发内存水位限制，甚至导致服务端 OOM。因此在用户打开手动确认后，我们建议用户设置一次最多能预取（prefetch count）的消息数量，避免大量消息堆积在客户端和服务端内存中。&lt;/p&gt; 
&lt;h3&gt;保证队列尽可能短&lt;/h3&gt; 
&lt;p&gt;保持生产和消费速率一致可以减少消息堆积。RabbitMQ 会在发现索引文件中的消息全部被消费后删除索引文件和对应的存储文件，这样可以减少磁盘空间占用。队列的堆积数量少意味着多数读取都可以从缓存中直接读取到消息体，从而提升读取性能。&lt;/p&gt; 
&lt;h2&gt;总结&lt;/h2&gt; 
&lt;p&gt;本文全面探讨了 RabbitMQ 经典队列的底层存储机制，包括其整体架构、实现原理及运维实践。经典队列的底层存储由队列索引和消息存储两大模块构成，其中消息存储又细分为共享存储和队列存储，通过精心设计的文件结构和内存管理策略，实现了高效的消息读写与存储管理。文章详细解析了队列索引、消息存储（包括共享存储和队列存储）的文件结构，介绍了消息读取与写入的流程，以及文件整理的逻辑。在运维实践方面，强调了发送确认、消费确认与保持队列尽可能短的重要性，并给出了相应的配置建议。希望通过本文的介绍，可以帮助大家深入理解 RabbitMQ 经典队列的底层存储机制，为实际应用中的性能优化与故障排查提供有力支持。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/4587289/blog/18684804</link>
      <guid isPermaLink="false">https://my.oschina.net/u/4587289/blog/18684804</guid>
      <pubDate>Wed, 23 Jul 2025 07:21:49 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>智元首款四足机器人 D1 ULTRA 已在官网上架</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;7 月 23 日，智元机器人官网已上架其首款四足机器人&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.zhiyuan-robot.com%2Fproducts%2FD1Ultra" target="_blank"&gt;D1 ULTRA&lt;/a&gt;，定位为「行业级小型四足机器人」，隶属于智元灵犀系列，最高奔跑速度达 3.7 米/秒，最大上下斜坡角度≥30 度，可向前或向上跳跃离地面高度达 35 厘米，支持最高 16 厘米楼梯连续攀爬，适用于特种应用、安防巡检、科研教育等场景。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0723/150843_zowq_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;具体参数如下：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0723/150938_ID8h_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;目前，智元尚未公布 D1 ULTRA 的售价。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361922</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361922</guid>
      <pubDate>Wed, 23 Jul 2025 07:10:49 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>昔日合作伙伴反目成仇，微软 AI 主管挖角谷歌 20 名核心员工</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;硅谷 AI 人才市场近日掀起了一阵风暴，微软消费级 AI 战略负责人穆斯塔法・苏莱曼（Mustafa Suleyman）开始大规模从其曾创办的 DeepMind 团队挖角，已经成功引入超过 20 名核心员工。作为曾经的合作伙伴，苏莱曼如今却将目光投向了他的老东家，直接影响到谷歌的 AI 研发力量。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="325" src="https://oscimg.oschina.net/oscnet/up-f52907ca2cc81233a194ab25b2e460ef981.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;在这次人才争夺战中，前谷歌 Gemini 聊天机器人工程负责人阿马尔・苏布拉马尼亚（Amar Subramanya）也确认将加入微软，担任人工智能副总裁。他在个人社交平台上表示，微软的文化让他感到耳目一新，团队的氛围既谦逊又雄心勃勃。除了苏布拉马尼亚，微软还成功说服了其他几位 DeepMind 的核心员工加入，显示出其对人才的强烈需求。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;根据消息人士透露，微软在过去六个月内已从 DeepMind 吸纳了至少 24 名员工。此时，正值各大科技公司加大力度从竞争对手处挖角顶尖&amp;nbsp;AI 研究员和工程师，行业薪酬水平也因而迅速上升。对此，OpenAI 首席执行官萨姆・奥特曼（Sam Altman）曾批评其他公司用高额签约奖金吸引人才，认为这助长了 「唯利是图」 的风气。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;随着苏莱曼的挖角行动，微软不仅获得了宝贵的专业技术，还可能干扰到谷歌的研发进程。苏莱曼于今年 3 月加入微软，之前曾因其创办的初创公司 Inflection 被微软收购而走上现在的职位。他的战略意图显而易见，通过快速组建一支高效团队，加速微软在关键 AI 领域的布局。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;与此同时，谷歌也因内部人事变动进行重组，以应对日益激烈的竞争。谷歌数据显示，ChatGPT 的月活跃用户数已经达到 6 亿，而其自家 Gemini 仅为 4 亿。更有消息称，谷歌也从微软挖走了部分研究人员，表明两家公司在人才争夺上的白热化程度。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;值得注意的是，科技巨头们为顶尖&amp;nbsp;AI 人才开出的高薪，反映出行业的巨大矛盾。尽管如此，很多普通员工却面临着裁员的压力。微软最近裁减了 9100 个职位，企图通过精简运营将资源重分配到 AI 转型上。这种趋势在整个科技行业中普遍存在，形成了一个新的高薪专家阶层，同时挤压了大批普通员工的生存空间。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361921</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361921</guid>
      <pubDate>Wed, 23 Jul 2025 07:06:49 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>英伟达发布 OpenReasoning-Nemotron 系列推理模型，专注于数学、科学和代码</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;NVIDIA 发布了 OpenReasoning-Nemotron，这是由四个精简的推理模型组成的集合，参数分别为 15 亿、70 亿、140 亿和 320 亿，均源自拥有 671 亿参数的 DeepSeek R1 0528。通过将庞大的「老师」模型压缩成四个基于 Qwen-2.5 的「学生」模型，NVIDIA 使得即使在标准游戏设备上也能进行高级推理实验，而无需担心高昂的 GPU 费用和云使用量。&lt;/p&gt; 
&lt;p&gt;这些模型在数学、科学和代码等多个推理基准测试中，均在其各自的规模级别上达到了业界领先水平。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0723/145755_fzqJ_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
 &lt;p&gt;https://huggingface.co/blog/nvidia/openreasoning-nemotron&lt;br&gt; https://nvidia.github.io/NeMo-Skills/releases/openreasoning/&lt;br&gt; https://huggingface.co/collections/nvidia/openreasoning-nemotron-687730dae0170059860f1f01&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;关键不在于复杂的技巧，而在于原始数据。NVIDIA 使用 NeMo Skills 流水线生成了 500 万个数学、科学和代码解决方案，然后通过纯粹的监督学习对每个解决方案进行微调。目前，320 亿参数的模型在 AIME24 上获得了 89.2 分，在 HMMT 二月竞赛中获得了 73.8 分，而即使是 15 亿参数的版本也取得了 55.5 分和 31.5 分的稳定成绩。&lt;/p&gt; 
&lt;p&gt;NVIDIA 设想将这些模型打造为强大的研究工具包。所有四个检查点均可在 Hugging Face 上下载，为探索强化学习驱动的推理或针对特定任务定制模型奠定坚实的基础。使用 GenSelect 模式（每个问题进行多次迭代），可以生成多个并行生成并选出最佳答案，从而使 32B 模型的性能达到卓越水平，在多个数学和编码基准测试中堪比甚至超越 OpenAI 的 o3-high 性能。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-ed39ae4f4d16b16bc9163d29c2687756b43.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;由于 NVIDIA 仅使用监督微调训练这些模型，而未使用强化学习，因此社区拥有清晰、先进的未来强化学习实验起点。对于游戏玩家和家庭爱好者来说，如果您拥有更强大的游戏 GPU，我们将获得一个完全本地化的模型，该模型可以非常接近最先进的水平。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361915/nvidia-openreasoning-nemotron</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361915/nvidia-openreasoning-nemotron</guid>
      <pubDate>Wed, 23 Jul 2025 07:01:49 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>jemalloc 作者自述：开发已陷入停滞</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;jemalloc 内存分配器最初于 2004 年初构思，并且现在已公开使用了大约 20 年。由于开源软件许可的性质，jemalloc 将无限期地保持公开可用。但积极的上游开发已结束。本文简要描述了 jemalloc 的发展阶段，每个阶段都有成功/失败的亮点，随后是一些回顾性的评论。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;阶段 0：Lyken&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;2004 年，我在科学计算的背景下开始开发 Lyken 编程语言。Lyken 最终成为了死胡同，但其手动内存分配器在 2005 年 5 月已经功能完整。（本应利用其功能的垃圾收集器从未完成。）2005 年 9 月，我开始将分配器集成到 FreeBSD 中，并在 2006 年 3 月，为了使用线程特定数据和 dlsym(3) 实现的薄封装，我从 Lyken 中移除了分配器。&lt;/p&gt; 
&lt;p&gt;在投入了这么多精力之后，为什么又要从 Lyken 中移除内存分配器呢？一旦将分配器集成到 FreeBSD 后，就明显发现系统分配器的唯一缺失功能是跟踪分配量的机制，以便触发线程垃圾收集。而这可以通过使用线程特定数据和 dlsym(3) 的薄封装来实现。有趣的是，多年后，jemalloc 甚至添加了 Lyken 需要的统计收集功能。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;阶段 1：FreeBSD&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;2005 年，多处理器计算机的转变正在进行中。FreeBSD 拥有 Poul-Henning Kamp 的出色 phkmalloc 内存分配器，但该分配器没有并行线程执行的机制。Lyken 的分配器似乎是一个明显的可扩展性改进，经过朋友和同事的鼓励，我将它快速集成到了众所周知的 jemalloc 中。但等等！在集成后不久，就发现 jemalloc 在某些负载下有严重的碎片问题，尤其是由 KDE 应用程序引起的。正当我以为已经差不多完成时，这个现实世界的失败质疑了 jemalloc 的可行性。&lt;/p&gt; 
&lt;p&gt;简而言之，碎片问题是由于使用了统一的范围分配方法（即没有大小类区分）。我从 Doug Lea 的 dlmalloc 获得了一些基本灵感，但没有那些复杂的、经过实战测试的启发式方法来避免许多最严重的碎片问题。随后进行了大量的研究和实验。当 jemalloc 成为 FreeBSD 发布的一部分时，其布局算法完全改变了，采用了大小分组的区域，如 2006 年 BSDCan 的 jemalloc 论文所述。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;阶段 1.5：Firefox&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;2007 年 11 月，Mozilla Firefox 3 即将发布，高碎片化问题仍未解决，尤其是在微软 Windows 上。于是开始了与 Mozilla 合作的一年内存分配工作。将 jemalloc 移植到 Linux 几乎微不足道，但 Windows 却不一样。当时 jemalloc 的源代码在 FreeBSD 的 libc 库中，所以我们基本上对 jemalloc 进行分叉，并添加了可移植性代码，将任何对 FreeBSD 相关的内容向上游提交。整个实现仍然在一个文件中，这减少了分叉维护时的摩擦，但该阶段的实现复杂性显然超过了合理范围。&lt;/p&gt; 
&lt;p&gt;多年后，Mozilla 开发者为了摆脱他们的分叉，对上游的 jemalloc 做出了重大贡献。不幸的是，Mozilla 的基准测试一直显示，分叉版本比上游版本表现得更好。我不确定这是因为对局部最优的过度拟合还是性能回归的真正迹象，但这是我对 jemalloc 最大的失望之一。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;阶段 2：Facebook&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;2009 年我加入 Facebook 时，我惊讶地发现，阻碍 jemalloc 在 Facebook 基础设施中普遍使用的主要障碍是仪器设备。关键的内部服务陷入了尴尬的境地，它们依赖 jemalloc 来控制内存碎片，但工程师们需要使用 tcmalloc 和 gperftools 中的 pprof 堆分析工具来调试内存泄漏。pprof 兼容的堆分析功能成为了 jemalloc 1.0.0 发布的主要亮点。&lt;/p&gt; 
&lt;p&gt;jemalloc 开发迁移到 GitHub，并在接下来的几年里随着问题和机遇的出现而继续进行。其他开发者也开始做出重要的功能贡献。3.0.0 版本引入了广泛的测试基础设施以及 Valgrind 支持。4.x 版本系列引入了基于衰减的清除功能和 JSON 格式的遥测。5.x 系列从「块」转向「范围」，为更好地与 2 MiB 大页进行交互做好准备。&lt;/p&gt; 
&lt;p&gt;较为有争议的是，我在 5.0.0 版本中移除了 Valgrind 支持，因为这是一项重大的维护复杂性问题（在微妙的地方有很多分支），而且在 Facebook 内部未被使用；其他工具如 pprof 和 MemorySanitizer 占据主导地位。我几乎没有收到关于 Valgrind 支持的反馈，因此推断它未被使用。但回顾起来，这似乎并非如此。特别是，Rust 语言直接将 jemalloc 整合到编译的程序中，而我认为 Rust 开发者和 Valgrind 开发者之间有某种重叠。人们感到愤怒。jemalloc 可能比自然发展进程更快地被踢出了 Rust 二进制文件。&lt;/p&gt; 
&lt;p&gt;Facebook 的内部遥测非常壮观，拥有各种服务的性能数据，这对内存分配器的开发是非常宝贵的。我不认为在过去的十年中，最快内存分配器之一（tcmalloc 和 jemalloc）受益于如此数据是巧合。即使是像快速路径优化这样的「简单」事情，当有汇总的 Linux perf 数据可用时，也更容易正确执行。像碎片管理这样的困难事情仍然困难，但如果数千种不同的工作流表现良好，没有异常的回归，那么更改可能是安全的。jemalloc 在 Facebook 基础设施中受益匪浅，从性能、弹性到一致的行为。此外，jemalloc 自身的集成统计数据报告功能正是应对这种普遍遥测环境而直接诞生的，这在实现上花费的精力不多，但对 jemalloc 开发和非 Facebook 应用程序的调优/调试普遍带来了巨大的好处。&lt;/p&gt; 
&lt;p&gt;在我在 Facebook 的最后一年，我被鼓励组建一个小的 jemalloc 团队，以便我们可以解决一些原本令人生畏的重要任务。除了重大的性能改进，我们还获得了持续集成测试和全面的遥测功能。当我 2017 年离开 Facebook 时，jemalloc 团队继续出色地进行开发和维护工作多年，几乎完全不涉及我的参与，由我尊敬的同事王歧领导，并且从提交历史来看，也有许多其他人的出色贡献。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;阶段 3：Meta&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在 Facebook 更名为 Meta 的时期，jemalloc 的开发趋势明显发生了变化。Facebook 基础设施工程减少了核心科技的投资，而是强调投资回报率。这在 jemalloc 的提交历史中显而易见。特别是，有原则的巨型页面分配（HPA）的种子早在 2016 年就已经埋下！HPA 工作持续了几年，然后在不断的调整中放缓，并逐渐停滞，因为没有进行必要的重构来保持代码库的健康。这个特征路线最近崩溃了。对我而言，有些伤心，但我已经多年未密切参与，因此情感上的冲击被减弱了。但鉴于 Meta 内部近期的变化，我们现在没有人能推动长期的 jemalloc 开发，注重通用性。&lt;/p&gt; 
&lt;p&gt;我不打算深入讨论这些纷争，但也许值得一提的是，尽管涉及的大多数人都出于善意，但最终 jemalloc 在 Facebook/Meta 手中走向了令人遗憾的结局。企业文化会随着外部和内部压力而变化。而人们经常发现自己处于无法解决的困境中，主要的选择是 1）在极端压力下做出糟糕的决定，2）在极端压力下服从，或者 3）被绕开。作为个人，我们有时有足够的影响力来减缓组织的退化，甚至可能在局部复兴，但我们无法阻止不可避免的事情。&lt;/p&gt; 
&lt;p&gt;我仍然非常感激我的前同事在 jemalloc 上的所有优秀工作，以及 Facebook/Meta 对它投入了如此多资源、如此长的时间。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;阶段 4：停滞&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;现在怎么办？就我而言，「上游」的 jemalloc 开发已经结束。Meta 的需求早已与外部使用的需求不一致，他们最好做自己的事情。如果我重新参与，第一步至少需要数百小时的重构以偿还累积的技术债务。而我对之后可能带来的东西并不足够兴奋以支付如此高昂的前期成本。也许其他人会创建可行的分支，无论是从 dev 分支还是从 5.3.0 版本（已经三年了！）。&lt;/p&gt; 
&lt;p&gt;在上述部分中，我提到了几个特定阶段的失败，但还有一些一般性的失败让我感到意外，尽管我的职业一直专注于开源开发。&lt;/p&gt; 
&lt;p&gt;如前所述，移除 Valgrind 引起了某些负面情绪。但问题的根本在于对其他使用和需求缺乏意识。如果我早知道它对任何人来说都重要，我可能会与其他人一起保留 Valgrind 支持。另一个例子是，我完全不知道 jemalloc 作为 Android 内存分配器的使用，可能有两年时间。而且，多年后，直到事后才知道它已被取代。&lt;br&gt; 即使 jemalloc 完全公开在外部（没有在 Facebook 内部封存），该项目从未吸引到其他组织的主要贡献者。Mike Hommey 推动 Firefox 迁移到上游 jemalloc 的努力是一个差一点的成功。其他人试图迁移到基于 CMake 的构建系统也多次失败，从未完成。我从与 Darwin 硬碰硬的经验中知道，内部封存的开源项目无法繁荣（HHVM 是一个重复的例子），但 jemalloc 作为一个独立项目，需要的不只是开放开发。&lt;/p&gt; 
&lt;p&gt;对我而言，jemalloc 是一项奇特的分心，因为我长期以来一直是垃圾收集的坚定支持者，而不是手动内存管理。我个人很高兴再次投入到垃圾收集系统中，但 jemalloc 是一个令我非常满足的项目。感谢所有让这个项目变得如此有价值的人，包括合作者、支持者和用户。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361911</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361911</guid>
      <pubDate>Wed, 23 Jul 2025 06:51:49 GMT</pubDate>
      <author>来源: 资讯</author>
    </item>
    <item>
      <title>Pika Labs 发布首款纯 AI 社交视频应用</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Pika&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Fpika_labs%2Fstatus%2F1947427650555023410" target="_blank"&gt;宣布&lt;/a&gt;了其开发的首款完全基于 AI 的社交视频应用，并已开放早期访问，用户可通过下载&amp;nbsp;iOS&amp;nbsp;应用加入等待名单。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;img height="1764" src="https://static.oschina.net/uploads/space/2025/0723/144812_Lkpt_2720166.png" width="1984" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;https://apps.apple.com/gb/app/pika-social-ai-video/id6744712684&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;span&gt;Pika &lt;/span&gt;建立在一个「高度表现力的人类视频模型」之上，&lt;span&gt;主打 AI 生成自拍视频，用户仅需一张自拍即可快速生成风格各异的视频，如一键音画同步（如生成唱歌、说唱、Vlog 等视频），更换发色、服装、环境等外观，对他人视频进行混剪，以及 AI 自动生成 Talking Video 脚本等等。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;经过几周的内测后，Pika&amp;nbsp;现已开放早期访问。用户可以下载其&amp;nbsp;iOS&amp;nbsp;应用加入等待名单，或通过邀请码直接获得访问权限。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361912</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361912</guid>
      <pubDate>Wed, 23 Jul 2025 06:51:49 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>快手联合上交开源统一多模态生成理解模型 Orthus</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;在今年的 ICML 上，快手联合上海交通大学提出了一种支持图文交错生成的统一模型——Orthus，目前已开源。该模型基于自回归 Transformer 架构，能够从文生图、图到文等不同任务学习有价值信号。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;根据介绍，仅使用极少的计算资源，Orthus 便在多个图像理解指标上超越了现有混合理解生成模型 Chameleon 和 Show-o，并在文生图生成的 GenEval 指标上优于专用扩散模型 SDXL。此外，Orthus 还展现出强大的图文交错数据建模能力，在图像编辑和网页生成任务中展现出巨大潜力。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;Orthus 具有以下核心特性：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;自回归 Transformer 主干；&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;处理离散的文本 token 和连续的图像 feature；&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;基于线性层定义的 language head 和 diffusion MLP 定义的 image head 来分别生成文和图；&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;足够计算高效。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;如下图所示，项目团队利用上述两个 heads，将图片细节的扩散建模从 Transformer 主干中解耦。该设计使得主干网络能够专注于刻画文本与图像特征表示之间的关联，而将图像细节信号的恢复任务交由更专业的 diffusion head 完成。这样解耦既缓解了图像离散化表示带来的信息损失，又避免了端到端扩散建模与自回归机制之间的分歧。本质上，Orthus 可以看作何恺明在图像生成领域的工作&lt;/span&gt;&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2406.11838" target="_blank"&gt;MAR&lt;/a&gt;&amp;nbsp;&lt;span style="color:#000000"&gt;向多模态领域上的拓展。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="261" src="https://oscimg.oschina.net/oscnet/up-3e5c3c4f44771f1655d2c0e7ac740d7f9c5.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;具体实现上，Orthus 由以下组件构成：一个文本分词器、一个视觉自编码器、两个特定模态的嵌入模块、一个 Transformer 主干网络和两个特定模态的输出头。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;给定文本和图像，Orthus 会将离散的文本 token（由文本分词器生成）和连续的图像特征（由视觉自编码器提供）嵌入到统一的表示空间中。在该空间内，自回归 Transformer 主干负责建模模态内部（如文本-文本）及跨模态（文本-图像）之间的相互依赖关系。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在主干之上，Orthus 使用两个模态特定的头部来分别生成文本和图像：一个是常规的语言建模线性头，用于预测离散的文本 token；另一个是扩散 MLP 头，用来生成连续的图像特征。在推理阶段，Orthus 根据特殊标记的指示，自回归地预测下一个文本 token 或图像 feature。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;span style="background-color:#ffffff"&gt;实验结果表明，得益于 Orthus 对图像的连续表示及扩散建模方法的优势，Orthus 相较在同样的数据设定下微调的 Chameleon 表现更佳。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="234" src="https://oscimg.oschina.net/oscnet/up-5842806691a566273a48e8c29c24d728245.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="130" src="https://oscimg.oschina.net/oscnet/up-9c510895543b1e84c96343f3a21dd47efc5.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="129" src="https://oscimg.oschina.net/oscnet/up-ead8be4b204a40ca0cd61e1e7e5bdc6c02c.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;更多详情可&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Ftr70YNC6xjXAuvTZO4-0Gw" target="_blank"&gt;查看官方公告&lt;/a&gt;。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361910</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361910</guid>
      <pubDate>Wed, 23 Jul 2025 06:50:49 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>商汤科技将成立独立的具身智能公司</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;u&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.leiphone.com%2Fcategory%2Fai%2FXfVFHlE91fPINqJk.html" target="_blank"&gt;根据雷峰网 AI 科技评论的独家报道&lt;/a&gt;&lt;/u&gt;，商汤科技将成立独立的具身智能公司，核心班底已经初步搭建起来，包括王晓刚、陶大程等视觉技术大咖，目前正在业内「招兵买马」。&lt;/p&gt; 
&lt;p&gt;报道称，商汤科技此前在 2025 技术交流日上展示了基于大装置 SenseCore 2.0 训练的具身智能成果——AI 超市「双机协作」采购场景。活动现场，商汤也同傅利叶、松应科技两家机器人公司达成了战略合作。&lt;/p&gt; 
&lt;p&gt;据商汤大装置事业群业务人员透露，从 2024 年开始到今年，具身智能机器人领域的增量客户明显增多；为机器人本体企业训练提供模型能力的生态合作成为商汤重要定位，其全流程 AI 研发体系能通过端到端一站式平台支持千机并行仿真训练，为具身智能提供从开发到验证的闭环支持。&lt;/p&gt; 
&lt;p&gt;此外，商汤还领投了众擎机器人的天使轮系列，今年还在继续参投其 Pre-A 轮。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361907</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361907</guid>
      <pubDate>Thu, 17 Jul 2025 06:41:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>FreeBSD 15.0 计划提供 KDE 桌面安装选项</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;致力于增强 FreeBSD 笔记本电脑支持的团队希望在 FreeBSD 15 的安装程序中增加一个安装选项，以便轻松提供基于 KDE Plasma 的桌面环境。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-1bc26da69e09b20573cb7a36008f563bb2a.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;目前，FreeBSD 台式机/笔记本电脑用户需要进行基础安装，然后使用其控制枱启动 FreeBSD 安装程序，最后通过包管理系统安装所需的桌面。&lt;/p&gt; 
&lt;p&gt;FreeBSD 15.0 计划于今年晚些时候发布，其希望将桌面选项集成到操作系统的文本安装程序中，以提供基于 KDE Plasma 的桌面。&lt;/p&gt; 
&lt;p&gt;选择图形桌面选项时，KDE Plasma 6 将与 SDDM 显示管理器一起安装，以便在新的 FreeBSD 安装中提供远胜于现有桌面环境的出色桌面体验。不过，这不如项目终止前 PC-BSD 和 TrueOS 等系统提供的体验那么好。&lt;/p&gt; 
&lt;p&gt;FreeBSD 笔记本电脑团队在其每月状态更新中&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FFreeBSDFoundation%2Fproj-laptop%2Fblob%2Fmain%2Fmonthly-updates%2F2025-06.md%23kde-desktop-installer-option" target="_blank"&gt;指出：&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;对于 FreeBSD 15.0，我们的目标是扩展 FreeBSD 安装程序，使其能够提供基于 KDE 的精简桌面作为安装选项。初始概念是采用低交互的安装流程，安装完成后，用户将直接进入 KDE 图形登录屏幕。我们&lt;/p&gt; 
 &lt;p&gt;目前正在评估所需的 pkg 依赖项，以便自动选择合适的显卡驱动程序。&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0723/143553_lbDx_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;原型设计希望为 FreeBSD 15.0 提供一个「最低限度」的桌面环境，供那些希望增强 FreeBSD 桌面/笔记本电脑用户安装过程的用户使用。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361905</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361905</guid>
      <pubDate>Thu, 17 Jul 2025 06:36:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>OpenAI 为「深度研究」报告增加 docx 文件导出功能</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;OpenAI &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2FOpenAI%2Fstatus%2F1947756508579828097" target="_blank"&gt;宣布&lt;/a&gt;，用户现在可以直接将 ChatGPT 生成的深度研究报告导出为 .docx 文件，该功能已在网页版上线，并即将登陆移动应用。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1736" src="https://static.oschina.net/uploads/space/2025/0723/142623_3QYN_2720166.png" width="1290" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;深度研究（Deep Research）是 OpenAI 于 2025 年 2 月推出的一款 AI 研究代理，基于 o1 推理模型开发，专为处理复杂研究任务设计。该工具能够自主浏览网络，分析数百个在线来源然后进行信息整合，并在 5 至 30 分钟内生成详尽的研究报告，报告内容包含精准引用的网页或 PDF 段落。与传统的手动研究相比，这节省了大量时间。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361903</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361903</guid>
      <pubDate>Thu, 17 Jul 2025 06:32:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Claude 移动端将支持「记忆」功能</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Anthropic 正在为 Claude 的移动端 APP 准备一次重大升级，将此前仅限于网页版的功能引入其 iOS 应用。其中最引人注目的新增功能是未公开的&lt;strong&gt;跨聊天搜索和记忆（memory &amp;amp; recall）能力&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-42837ff4193bca74fce930bea993efe51cc.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;与当前的移动版本不同，这项功能将使 Claude 能够跨会话保留信息并引用之前的对话，这与 ChatGPT 及其他竞争对手中已有的「记忆」实现方式类似。虽然网页版的 Claude 目前也不支持此功能，但其在 iOS 的代码暗示着未来可能进行跨平台推广。&lt;/p&gt; 
&lt;p&gt;这项功能对于依赖长期上下文或重复性任务的用户，例如研究人员、专业人士和学生。通过记住相关细节，Claude 有可能演变为更可靠的助手，尤其适用于上下文依赖度高的工作流。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361902</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361902</guid>
      <pubDate>Thu, 17 Jul 2025 06:23:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>OpenAI 与 Oracle 签下 300 亿美元数据中心大单</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;根据《华尔街日报》的报道，OpenAI 与甲骨文（Oracle）达成了一项价值每年 300 亿美元的数据中心服务协议。OpenAI 的首席执行官山姆・奥特曼在社交媒体上确认了这一合同的细节，但没有透露具体的金额。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;这项交易最初在 6 月 30 日由甲骨文在一份证券交易委员会的文件中披露，虽然当时并没有透露客户名称和具体服务内容，但这一消息使甲骨文的股票飙升，创下历史新高，创始人兼首席技术官拉里・埃里森一度成为全球第二富豪。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="274" src="https://oscimg.oschina.net/oscnet/up-9080e8a3ccb262f7bb5138b2eaaaa5b874f.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在这项交易达成后，外界纷纷猜测，究竟是什么公司需要如此庞大的数据中心服务。根据甲骨文的财报，2025 财年该公司总共向所有客户销售的云服务总额为 245 亿美元。OpenAI 随后解释，这项与甲骨文的交易涉及 4.5 千兆瓦的电力容量，这是双方在 1 月宣布的价值 5000 亿美元的数据中心建设项目 「星际之门」（Stargate）的一部分。值得注意的是，300 亿美元的合同并不包括日本软银 (SoftBank)。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;《华尔街日报》指出，4.5 千兆瓦的电力容量相当于两个胡佛水坝的输出，足够为约 400 万家庭提供电力。然而，这并不是甲骨文的简单胜利，OpenAI 与甲骨文仍需共同建设这一庞大的数据中心，这将是一项耗资巨大且能源消耗极大的工程。建设地点位于德克萨斯州的阿比林（Abilene）。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;与此同时，甲骨文在过去的财年中支出 212 亿美元用于资本支出，预计今年将再花费 250 亿美元。由此可见，在短短两年内，甲骨文在数据中心上的支出接近 500 亿美元，这一数字并未包含土地购买的费用。需要指出的是，这笔资金也用于支持甲骨文现有客户的需求，而不仅仅是为了满足 OpenAI 的要求。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;有趣的是，奥特曼最近透露，OpenAI 的年度经常性收入已达到 100 亿美元，较去年约 55 亿美元的水平有了显著增长。这一与甲骨文的合同，已是目前 OpenAI 年收入的三倍多，而不包括公司其他开支和现有数据中心的承诺。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361900</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361900</guid>
      <pubDate>Thu, 17 Jul 2025 06:11:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>​OpenAI 赢得商标诉讼，阻止竞争对手使用 Open AI</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;OpenAI 在与一家名为 Open AI（两者之间有空格）的公司之间的商标争议中获得了胜利。法院裁定，Open AI 在申请商标注册时存在误导性行为，意图混淆消费者，造成与 ChatGPT 的制造商之间的虚假关联。根据裁定，Open AI 需要停止使用其名称和互联网域名 open.ai。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;img height="282" src="https://oscimg.oschina.net/oscnet/up-cbe3bb3b907ab6fd11df1f9a6e137fb2a94.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;Open AI 公司在 OpenAI 推出产品之前，就已经提前获得了 open.ai 的域名。此次裁决不仅仅是对 Open AI 名称的限制，还意味着它将失去该域名的使用权。法庭认为，OpenAI 在其发展过程中 「漂移」 到了 Open AI 的市场领域，并在品牌形象上造成了一定的重叠。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;这一判决对于 OpenAI 来说无疑是个利好消息，进一步巩固了其在人工智能领域的市场地位。OpenAI 表示，他们将继续保护自己的商标和品牌，确保消费者能够清晰地辨别不同公司的产品。OpenAI 的法律团队强调，商标的独特性对于公司和消费者都至关重要，而任何模糊或误导性的名称都会对市场造成混乱。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;此次事件也提醒其他科技公司在品牌命名和商标注册方面需更加谨慎，确保不会侵犯他人的商标权益。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361878</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361878</guid>
      <pubDate>Thu, 17 Jul 2025 03:20:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>2025 年中 CNCF 前 30 个开源项目回顾</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;CNCF 首席技术官 Chris Aniszczyk 近日&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.cncf.io%2Fblog%2F2025%2F07%2F18%2Fa-mid-year-2025-look-at-cncf-linux-foundation-and-the-top-30-open-source-projects%2F" target="_blank"&gt;发布&lt;/a&gt;了一篇名为「2025 年中展望 CNCF、Linux 基金会和排名前 30 的开源项目」的文章，并分析得出以下一些要点内容：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;Kubernetes 仍然是拥有最多贡献者的项目，体现了其持续成熟和广泛采用。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;OpenTelemetry 贡献者数量持续增长，依然是 CNCF 中速度第二快的项目，已成为观测（o11y）领域的「Kubernetes」。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;Backstage 解决了开发者体验的痛点，成为全球最受欢迎的开源内部开发者门户（IdP）。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;GitOps 依然是云原生生态的重要组成部分，Argo 和 Flux 等项目培育了庞大的社区。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;Crossplane 过去一年贡献者增长超过 20%，显示了开源控制平面和多云管理的需求。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;Kubeflow 进入了 CNCF 前 30 项目名单，凸显了 CNCF 项目在支持大规模 AI 基础设施中的作用。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img alt="" height="329" src="https://oscimg.oschina.net/oscnet/up-05b106cedab37d27080c40226eadd31ed42.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;em&gt;注：使用了气泡图展示三个维度的数据：提交数、贡献者数和评论/拉取请求数，采用对数-对数座标图以覆盖大规模数据。&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;em&gt;气泡面积与贡献者数量成正比&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;Y 轴代表拉取请求和问题的总数&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;X 轴代表提交数&lt;/em&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="text-align:start"&gt;&lt;span style="color:#000000"&gt;所有&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fcncf%2Fvelocity%23current-reports" target="_blank"&gt;&lt;strong&gt;当前&lt;/strong&gt;&lt;/a&gt;和&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fcncf%2Fvelocity%23past-reports" target="_blank"&gt;&lt;strong&gt;过去的&lt;/strong&gt;&lt;/a&gt;报告都可以在 GitHub 上找到。所有用于生成此数据的脚本均位于&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fcncf%2Fvelocity" target="_blank"&gt;&lt;strong&gt;https://github.com/cncf/velocity&lt;/strong&gt;&lt;/a&gt;（遵循 Apache 2.0&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.cncf.io%2Fblog%2F2017%2F02%2F01%2Fcncf-recommends-aslv2%2F" target="_blank"&gt;&lt;strong&gt;许可&lt;/strong&gt;&lt;/a&gt;）。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361874/a-mid-year-2025-cncf-top-30-open-source-projects</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361874/a-mid-year-2025-cncf-top-30-open-source-projects</guid>
      <pubDate>Thu, 17 Jul 2025 03:06:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>上半年 186700000000GB，你贡献了多少？</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;「2025 年上半年通信业经济运行情况」指出，上半年通信业运行基本平稳。电信业务量收保持增长，新型基础设施建设有序推进，5G、千兆、物联网等用户规模持续扩大，移动互联网接入流量保持较快增势。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="547" src="https://oscimg.oschina.net/oscnet/up-118c4e7cc08604443114ab8fa4a1e7943b2.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;span style="color:#000000"&gt;一、总体运行情况&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;电信业务收入保持正增长，电信业务总量增长较快。上半年，电信业务收入累计完成 9055 亿元，同比增长 1%。按照上年不变价计算的电信业务总量同比增长 9.3%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="262" src="https://oscimg.oschina.net/oscnet/up-65798e186319e4cfe28d924c89b48d324da.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="527" src="https://oscimg.oschina.net/oscnet/up-8782dd8f8b8f02e43d35a04a2f912918629.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;span style="color:#000000"&gt;二、电信用户发展情况&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;固定宽带接入用户规模稳步增长，千兆用户数持续扩大。截至 6 月末，三家基础电信企业的固定互联网宽带接入用户总数达 6.84 亿户，比上年末净增 1426 万户。其中，100Mbps 及以上接入速率的固定互联网宽带接入用户达 6.51 亿户，占总用户数的 95.1%；1000Mbps 及以上接入速率的固定互联网宽带接入用户达 2.26 亿户，比上年末净增 1915 万户，占总用户数的 33%，占比较上年末提升 2.1 个百分点。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;移动电话用户稳中有增，5G 用户快速发展。截至 6 月末，三家基础电信企业及中国广电的移动电话用户总数达 18.1 亿户，比上年末净增 1993 万户。其中，5G 移动电话用户达 11.18 亿户，比上年末净增 1.04 亿户，占移动电话用户的 61.8%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="262" src="https://oscimg.oschina.net/oscnet/up-d3bd68efb3ed58918357446e42df654c010.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;移动物联网终端用户增长较快，互联网电视（IPTV、OTT）用户稳步增加。截至 6 月末，三家基础电信企业发展移动物联网终端用户 28.31 亿户，比上年末净增 1.75 亿户。互联网电视（IPTV、OTT）用户数达 4.11 亿户，比上年末净增 344 万户。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;span style="color:#000000"&gt;三、电信业务使用情况&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;移动互联网流量延续较快增势，6 月 DOU 值维持高水平区间。上半年，移动互联网累计流量达 1867 亿 GB，同比增长 16.4%。截至 6 月末，移动互联网用户数达 15.92 亿户，比上年末净增 2141 万户。6 月当月户均移动互联网接入流量（DOU）达到 20.75GB/户·月，同比增长 14.4%，比上年底高 1.05GB/户·月。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;电话通话量持续下滑，移动短信业务量快速增长。上半年，移动电话去话通话时长完成 1 万亿分钟，同比下降 5.6%；固定电话主叫通话时长完成 351 亿分钟，同比下降 7.6%。上半年，全国移动短信业务量同比增长 22.3%；移动短信业务收入同比增长 2.2%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;span style="color:#000000"&gt;四、通信能力情况&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;光缆线路总长度稳步增加。截至 6 月末，全国光缆线路总长度达到 7377 万公里，同比增长 9.9%。其中接入网光缆、本地网中继光缆和长途光缆线路所占比重分别为 59.9%、38.5% 和 1.6%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;千兆光纤宽带网络建设稳步推进。截至 6 月末，全国互联网宽带接入端口数量达 12.34 亿个，比上年末净增 3244 万个。其中，光纤接入（FTTH/O）端口达到 11.93 亿个，比上年末净增 3264 万个，占互联网宽带接入端口的 96.6%。截至 6 月末，具备千兆网络服务能力的 10G PON 端口数达 3022 万个，比上年末净增 201.9 万个。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;5G 网络建设持续推进。截至 6 月末，5G 基站总数达 454.9 万个，比上年末净增 29.8 万个，占移动基站总数的 35.7%，占比较一季度提高 1.3 个百分点。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;span style="color:#000000"&gt;五、地区发展情况&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;各地区千兆和 5G 用户渗透率持续提升。截至 6 月末，东、中、西部和东北地区 1000Mbps 及以上固定宽带接入用户渗透率分别为 33.2%、33.3%、34% 和 26.4%，较上年末分别提升 2 个、2.2 个、2.3 个和 2.8 个百分点；5G 移动电话用户渗透率分别为 61.5%、62.6%、61.5%、61.9%，较上年末分别提升 4.8 个、5.4 个、5.1 个和 5.7 个百分点。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;各地区移动互联网接入流量增速保持两位数水平。上半年，东、中、西部和东北地区移动互联网接入流量分别达到 780.4 亿 GB、441.8 亿 GB、534.3 亿 GB 和 110.3 亿 GB，同比增长 15.7%、16.7%、15.4% 和 25.8%。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361866</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361866</guid>
      <pubDate>Thu, 17 Jul 2025 02:54:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>诚邀您参与 2025 网民网络安全感满意度调查活动</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#3498db"&gt;&lt;strong&gt;「2025 网民网络安全感满意度调查活动」&lt;/strong&gt;&lt;/span&gt;定于 7 月 22 日 0 时至 31 日 24 时开展样本采集工作，全国同步开通问卷答题通道，面向广大网民广泛征集意见。&lt;/p&gt; 
&lt;p&gt;诚邀各位参与问卷填写，动动手指，说说感受：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0723/104413_st2O_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;公众网民：&lt;u&gt;&lt;em&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmyd.iscn.org.cn%2F%23%2Fs%2F8N7eFYUI%3FsourceId%3D706730" target="_blank"&gt;https://myd.iscn.org.cn/#/s/8N7eFYUI?sourceId=706730&lt;/a&gt;&lt;/em&gt;&lt;/u&gt;&lt;br&gt; 从业人员：&lt;em&gt;&lt;u&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmyd.iscn.org.cn%2F%23%2Fs%2Fz4PrSkac%3FsourceId%3D706730" target="_blank"&gt;https://myd.iscn.org.cn/#/s/z4PrSkac?sourceId=706730&lt;/a&gt;&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;网民网络安全感满意度调查活动以「网络安全为人民，网络安全靠人民」为主题，每年举办一届，是一个全国性、公益性网络安全社会调查活动。&lt;/p&gt; 
&lt;p&gt;活动采取线上问卷调查方式开展，网民群众可在每年的样本采集期间通过参与答卷，反映上网用网的感受、评价、建议及意见，是广大网民表达诉求、意见、建议的绿色通道。&lt;/p&gt; 
&lt;p&gt;为保障网民权益，让网民放心答题，本调查以不记名方式进行，问卷不涉及个人隐私信息。所采集的数据经汇总后仅用于相关课题研究。&lt;/p&gt; 
&lt;p&gt;与亿万网民同心而行，与国家网络安全同向而进。2025 年度调查活动将继续秉承「以人为本」的理念，携手社会各界力量，进一步「听民声，集民意，解民忧，畅民心」，助力提升网民群众的获得感、幸福感、安全感，全心服务于国家网络安全和信息化事业高质量发展，必将交上一份满意的答卷。&lt;/p&gt; 
&lt;p&gt;据了解，调查活动自 2018 年首次开展以来，影响力逐年提升，得到了广大网民支持和参与，得到了网络安全主管部门和教育、密码、人社等部门的关心和指导，在全国各地参与单位、团队的共同努力下，活动已连续成功举办 7 届，截至 2024 年已累计采集有效样本量突破 1300 万份，收集网民意见近 100 万条，发布全国总报告、专题报告、区域报告、行业分析报告等系列调查报告 1200 多份。调查数据被党政有关部门、研究机构广泛引用和权威发布，为我国网络安全研究、互联网综合治理提供了有力支撑。&lt;/p&gt; 
&lt;p&gt;2025 年是国家「十四五」规划收官之年,也是「十五五」规划谋篇布局之年，做好 2025 年网民网络安全满意度调查工作，对有效提升我国数字化高质量发展具有重要意义。据了解，2025 年度的调查工作主要围绕 2025 年网民的新诉求和我国网络社会发展新阶段、新动向、新问题、新挑战等进行问卷设计，除主问卷的共性题外，还设置了 10 个公众网民版专题问卷、3 个网络从业人员版专题问卷，以「更贴近民心、更深入一线」的方式开展样本采集，以更高要求、更高质量完成数据分析和各类报告的撰写工作。&lt;/p&gt; 
&lt;p&gt;「智能社会发展与治理挑战」专题为 2025 年度热点专题，旨在深入了解人工智能时代下，新技术给社会治理和民众生活带来的影响与挑战。同时受中国互联网协会委托，新增「中国网民权益保护」专题调查，以深入了解网民权益保护现状，推动完善相关政策与服务。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/361864</link>
      <guid isPermaLink="false">https://www.oschina.net/news/361864</guid>
      <pubDate>Thu, 17 Jul 2025 02:50:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
  </channel>
</rss>
