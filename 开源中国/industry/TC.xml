<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - industry - 繁體中文（台灣）</title>
    <link>https://www.oschina.net/news/industry</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news/industry" rel="self" type="application/rss+xml"/>
    <description>已對該 RSS 進行格式化操作：中英字符之間插入空格、使用直角引號、標點符號修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-tw</language>
    <lastBuildDate>Tue, 26 Aug 2025 07:41:10 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>鴻蒙新手福音！每日 300 分鐘免費時長+海量鴻蒙真機，輕鬆上手雲測雲調​</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="margin-left:0; margin-right:0"&gt;開發者們是否常因真機設備不足、測試流程繁瑣及硬件成本高昂而受阻？HUAWEI AppGallery Connect 雲測試、雲調試能力，通過免設備投入、低操作門檻及海量鴻蒙真機資源，讓鴻蒙應用測試變得簡單又高效。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;核心能力亮點：&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;海量鴻蒙真機在線選：平台配備了多種型號的鴻蒙真機，覆蓋主流/熱門機型，滿足多樣化測試場景需求，滿足開發者在各種場景下的測試需求，無需自己購買設備。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;每天 300 分鐘免費使用時長：每天提供 300 分鐘的免費使用時間，足夠支撐新手嚐鮮、輕量級項目測試或多次驗證，0 成本起步測試，立省真機購買投入！&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;上手快且操作簡單：平台界面簡潔，操作流程直觀，新手無需複雜學習，按照操作指引很快就能上手使用，專注於應用測試本身。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;新手常見問題解答：&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Q1：應用馬上要上線了，自己的手機不是鴻蒙系統，有什麼測試渠道嗎？&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;A1：通過雲測試+雲調試申請很便捷。登錄 AppGallery Connect 平台後，在設備列表中選擇你需要的鴻蒙真機型號，點擊申請即可，無需繁瑣的審批流程，還能享受每日 300 分鐘免費時長。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&lt;img alt="鴻蒙新手福音！每日 300 分鐘免費時長+海量鴻蒙真機，輕鬆上手雲測雲調" src="https://oscimg.oschina.net/oscnet//f7983403a41ded04543d099c78760d95.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Q2：每日免費的 300 分鐘時長，是隻能用一台測試機嗎？&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;A2：不是的。每日都會發放 300 分鐘使用時長，可以在平台上切換不同的鴻蒙真機進行測試，只要每日累計使用時間不超過 300 分鐘，都可以免費使用。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Q3：測試過程中，能像操作自己的手機一樣操控測試機嗎？&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;A3：可以。遠程操控體驗和操作自己的手機類似，可以在測試機上安裝應用、點擊操作、輸入內容等，真實還原應用的使用場景。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Q4：除了基礎的功能測試，能測試應用的性能嗎？&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;A4：可以。雲測試可全面檢測應用兼容性、性能、穩定性、功耗及 UX 等關鍵指標，幫助你瞭解應用在真機上的性能表現，便於進行優化。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Q5：在雲調試時，能實時查看代碼運行情況並修改嗎？&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;A5：可以。雲調試支持實時查看代碼運行狀態，真實運行環境精準復現用戶場景，斷點、日誌即時獲取，可對代碼進行修改並重新調試，快速定位並解決問題。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Q6：測試完成後，能保存測試過程中的數據或截圖嗎？&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;A6：可以。平台支持保存測試過程中的截圖、日誌等數據，方便你後續查看和分析，更好地排查應用存在的問題。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Q7：如果每日 300 分鐘免費時長用完了，還想繼續使用怎麼辦？&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;A7：每日的免費時長用完後，可以等待次日免費時長刷新或在平台上選擇付費套餐繼續使用，套餐價格靈活，能滿足不同開發者的需求，成本遠低於購置真機，按需付費毫無壓力！。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&lt;img alt="鴻蒙新手福音！每日 300 分鐘免費時長+海量鴻蒙真機，輕鬆上手雲測雲調" height="260" src="https://oscimg.oschina.net/oscnet//8afb5d635df5346fdba209f87279676e.png" width="800" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;如果你是鴻蒙應用開發新手，想要輕鬆解決真機測試難題，不妨試試雲測試+雲調試能力。每日贈 300 分鐘免費時長！輕量測試 0 成本起步，極簡操作，高效輸出報告。成本低、易上手，點此立即試用 &amp;gt;&amp;gt;&lt;/p&gt; 
&lt;p style="color:#2b2b2b; margin-left:0; margin-right:0; text-align:start"&gt;AppGallery Connect 致力於為應用的創意、開發、分發、運營、經營各環節提供一站式服務，構建全場景智慧化的應用生態體驗。為給你帶來更好服務，請掃描下方二維碼或者點擊此處免費諮詢。&lt;/p&gt; 
&lt;p style="color:#2b2b2b; margin-left:0; margin-right:0; text-align:start"&gt;&lt;img alt="鴻蒙新手福音！每日 300 分鐘免費時長+海量鴻蒙真機，輕鬆上手雲測雲調" height="120" src="https://oscimg.oschina.net/oscnet//1602863077126f569fd497524b470ed6.png" width="120" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2b2b2b; margin-left:0; margin-right:0; text-align:start"&gt;如有任何疑問，請發送郵件至 agconnect@huawei.com 諮詢，感謝你對 HUAWEI AppGallery Connect 的支持！&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368551</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368551</guid>
      <pubDate>Tue, 26 Aug 2025 07:21:07 GMT</pubDate>
      <author>作者: 開源科技</author>
    </item>
    <item>
      <title>得物新商品審核鏈路建設分享</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;span id="OSC_h1_1"&gt;&lt;/span&gt; 
&lt;h1&gt;一、 前言&lt;/h1&gt; 
&lt;p&gt;得物近年來發展迅猛，平台商品類目覆蓋越來越廣，商品量級越來越大。而以往得物的上新動作更多依賴於傳統方式，效率較低，無法滿足現有的上新訴求。那麼如何能實現更加快速的上新、更加高效的上新，就成為了一個至關重要的命題。&lt;/p&gt; 
&lt;p&gt;近兩年 AI 大模型技術的發展，使得發佈和審核逐漸向 AI 驅動的方式轉變成為可能。因此，我們可以探索利用算法能力和大模型能力，結合業務自身規則，構建更加全面和精準的規則審核點，以實現更高效的工作流程，最終達到我們的目標。&lt;/p&gt; 
&lt;p&gt;本文圍繞 AI 審核，介紹機審鏈路建設思想、規則審核點實現快速接入等核心邏輯。&lt;/p&gt; 
&lt;span id="OSC_h1_2"&gt;&lt;/span&gt; 
&lt;h1&gt;二、如何實現高效審核&lt;/h1&gt; 
&lt;p&gt;對於高效審核的理解，主要可以拆解成「高質量」、「高效率」。目前對於「高質量」的動作包括，基於不同的類目建設對應的機審規則、機審能力，再通過人工抽查、問題 Case 分析的方式，優化算法能力，逐步推進「高質量」的效果。&lt;/p&gt; 
&lt;p&gt;而「高效率」，核心又可以分成業務高效與技術高效。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;業務高效&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;逐步通過機器審核能力優化審核流程，以解決資源不足導致上新審核時出現進展阻礙的問題。&lt;/li&gt; 
 &lt;li&gt;通過建設機審配置業務，產品、業務可以直觀的維護類目-機審規則-白名單配置，從而高效的調整機審策略。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;技術高效&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;通過建設動態配置能力，實現快速接入新的機審規則、調整機審規則等，無需代碼發佈，即配即生效。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Q2 在搭建了動態配置能力之後，算法相關的機審規則接入效率提升了 70% 左右。&lt;/p&gt; 
&lt;span id="OSC_h1_3"&gt;&lt;/span&gt; 
&lt;h1&gt;三、動態配置實現思路&lt;/h1&gt; 
&lt;p&gt;建設新版機審鏈路前的調研中，我們對於老機審鏈路的規則以及新機審規則進行了分析，發現算法類機審規則佔比超過 70% 以上，而算法類的機審規則接入的流程比較固化，核心分成三步：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;與算法同學溝通定義好接口協議&lt;/li&gt; 
 &lt;li&gt;基於商品信息構建請求參數，通過 HTTP 請求算法提供的 URL，從而獲取到算法結果。&lt;/li&gt; 
 &lt;li&gt;解析算法返回的結果，與自身商品信息結合，輸出最終的機審結果。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;而算法協議所需要的信息通常都可以從商品中獲取到，因此通過引入「反射機制」、「HTTP 泛化調用」、「規則引擎」等能力，實現算法規則通過 JSON 配置即可實現算法接入。&lt;/p&gt; 
&lt;span id="OSC_h1_4"&gt;&lt;/span&gt; 
&lt;h1&gt;四、商品審核方式演進介紹&lt;/h1&gt; 
&lt;p&gt;&lt;img alt="" src="https://mp.toutiao.com/mp/agw/article_material/open_image/get?code=YmRjMjQ4ZmI2YTM2MmZkMWE4ZDI0NzI4MzhmMDA0ZGUsMTc1NjE4OTk2Nzc1Ng==" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;商品審核方式的演進&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;人審&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;依賴商管、運營，對商品上架各字段是否符合得物上新標準進行人工覈查。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;機審&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;對於部分明確的業務規則，比如白底圖、圖片清晰度、是否重複品、是否同質品等，機審做前置校驗並輸出機審結果，輔助人工審核，降低審核成本，提升審核效率。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;AI 審核&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;通過豐富算法能力、強化 AI 大模型能力、雷達技術等，建設越來越多的商品審核點，並推動召回率、準確率的提升，達標的審核點可通過自動駁回、自動修改等 action 接管商品審核，降低人工審核的佔比，降低人工成本。&lt;/p&gt; 
&lt;span id="OSC_h1_5"&gt;&lt;/span&gt; 
&lt;h1&gt;五、現狀問題分析&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;產品層面&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;機審能力不足，部分字段沒覆蓋，部分規則不合理：&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;機審字段覆蓋度待提升&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;機審規則採納率不足&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;部分機審規則不合理&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;缺少產品配置化能力，配置黑盒化，需求迭代費力度較高：&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;規則配置黑盒&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;規則執行結果缺乏 trace 和透傳&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;調整規則依賴開發和發佈&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;缺少規則執行數據埋點&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;技術層面&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;系統可擴展性不足，研發效率低：&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;業務鏈路（AI 發品、審核、預檢等）不支持配置化和複用&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;規則節點不支持配置化和複用&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="OSC_h1_6"&gt;&lt;/span&gt; 
&lt;h1&gt;六、流程介紹&lt;/h1&gt; 
&lt;p&gt;搭建機審配置後台，可以通過配置應用場景+業務身份+商品維度配置來確定所需執行的全量規則，規則可複用。&lt;/p&gt; 
&lt;p&gt;其中應用場景代表業務場景，如商品上新審核、商家發品預檢、AI 發品預檢等；業務身份則表示不同業務場景下不同方式，如常規渠道商品上新的業務場景下，AI 發佈、常規商品上新（商家後台、交易後台等）、FSPU 同款發佈品等。&lt;/p&gt; 
&lt;p&gt;當商品變更，通過 Binlog 日誌觸發機審，根據當前的應用場景+業務身份+商品信息，構建對應的機審執行鏈（ProcessChain）完成機審執行，不同的機審規則不通過支持不同的 action，如自動修正、自動駁回、自動通過等。&lt;/p&gt; 
&lt;p&gt;鏈路執行流程圖如下：&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://mp.toutiao.com/mp/agw/article_material/open_image/get?code=NjYyNDM4M2JmNzY2Njg5ZDFjYmI4MDQwZTkzZTkwODMsMTc1NjE4OTk2Nzc1Ng==" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h1_7"&gt;&lt;/span&gt; 
&lt;h1&gt;七、詳細設計&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;整體架構圖&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://mp.toutiao.com/mp/agw/article_material/open_image/get?code=MjZiYmQyYWJhMWFjYTJjNzU3NzM3Zjg1NGQ4OTMyNWMsMTc1NjE4OTk2Nzc1Ng==" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;業務實體&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ER 圖&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://mp.toutiao.com/mp/agw/article_material/open_image/get?code=ODQyN2IzNjk1ZjU3NDJiYmY4MGYxNzQwNTNiODBlMmEsMTc1NjE4OTk2Nzc1Ng==" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;含義解釋&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;※ 業務場景&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;觸發機審的應用場景，如新品發佈、商家新品預檢等。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;※ 業務身份&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;對於某個應用場景，進一步區分業務場景，如新品發佈的場景下，又有 AI 發品、常規發品、FSPU 同款發品等。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;※ 業務規則&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;各行業線對於商品的審核規則，如校驗圖片是否是白底圖、結構化標題中的類目需與商品類目一致、發售日期不能超過 60 天等。同一個業務規則可以因為業務線不同，配置不同的機審規則。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;※ 規則組&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;對規則的分類，通常是商品字段模塊的名稱，&lt;strong&gt;一個規則組下可以有多個業務規則，如商品輪播圖作為規則組，可以有校驗圖片是否白底圖、校驗圖片是否清晰、校驗模特姿勢是否合規&lt;/strong&gt;等。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;※ 機審規則&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;對商品某個商品字段模塊的識別並給出審核結果，&lt;strong&gt;數據依賴機審能力以及 spu 本身&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;※ 機審能力&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;商品信息（一個或多個商品字段模塊）的審核數據獲取，&lt;strong&gt;通常需要調用外部接口&lt;/strong&gt;，用於機審規則審核識別。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;※ 業務&amp;amp;機審規則關聯關係&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;描述業務規則和機審規則的關聯關係，同一個業務規則可以根據不同業務線，給予不同的機審規則，如輪播圖校驗正背面，部分業務線要求校驗全量輪播圖，部分業務線只需要校驗輪播圖首圖/規格首圖。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;機審執行流程框架&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;流程框架&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;通過責任鏈、策略模式等設計模式實現流程框架。&lt;/p&gt; 
&lt;p&gt;觸發機審後會根據當前的業務場景、業務身份、商品信息等，獲取到對應的業務身份執行鏈（不同業務身份綁定不同的執行節點，最終構建出來一個執行鏈）並啓動機審流程執行。&lt;/p&gt; 
&lt;p&gt;由於機審規則中存在數據獲取 rt 較長的情況，如部分依賴大模型的算法能力、雷達獲取三方數據等，我們通過異步回調的方式解決這種場景，也因此衍生出了「異步結果更新機審觸發」。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;※ 完整機審觸發&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;完整機審觸發是指商品變更後，通過 Binlog 日誌校驗當前商品是否滿足觸發機審，命中的機審規則中如果依賴異步回調的能力，則會生成 pendingId，並記錄對應的機審結果為「pending」（其他規則不受該 pending 結果的影響），並監聽對應的 topic。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://mp.toutiao.com/mp/agw/article_material/open_image/get?code=ZGM5YTNjMzUyMzA5YjQzMDczZDgzNzc0ODQyOWNiMzIsMTc1NjE4OTk2Nzc1Ng==" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;※ 異步結果更新機審觸發&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;部分 pending 規則產出結果後發送消息到機審場景，通過 pendingId 以及對應的商品信息確認業務身份，獲取異步結果更新責任鏈（&lt;strong&gt;與完整機審的責任鏈不同&lt;/strong&gt;）再次執行機審執行責任鏈。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://mp.toutiao.com/mp/agw/article_material/open_image/get?code=ODdlOWEyYTc1MTA2MGU2YzM4YTJmNTRlOWFmOGFlNzIsMTc1NjE4OTk2Nzc1Ng==" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;動態配置能力建設&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;調研&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;新機審鏈路建設不僅要支持機審規則複用，支持不同業務身份配置接入，還要&lt;strong&gt;支持新機審規則快速接入&lt;/strong&gt;，降低開發投入的同時，還能快速響應業務的訴求。&lt;/p&gt; 
&lt;p&gt;經過分析，機審規則絕大部分下游為算法鏈路，並且算法的接入方式較為固化，即「構建請求參數」 -&amp;gt; 「發起請求」 -&amp;gt; 「結果解析」，並且數據模型通常較為簡單。因此技術調研之後，通過&lt;strong&gt;HTTP 泛化調用&lt;/strong&gt;實現&lt;strong&gt;構建請求參數&lt;/strong&gt;、&lt;strong&gt;發起請求&lt;/strong&gt;，利用**規則引擎（規則表達式）**實現結果解析。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;規則引擎技術選型&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;調研市面上的幾種常用規則引擎，基於歷史使用經驗、上手難度、文檔閲讀難度、性能等方面綜合考慮，最終決定選用&lt;strong&gt;QLExpress&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://mp.toutiao.com/mp/agw/article_material/open_image/get?code=N2Y4ZDNlOWE1ZWNkODE4OGExZmRlZmU2ODIxNDVmODUsMTc1NjE4OTk2Nzc1Ng==" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;**&amp;nbsp;HTTP 泛化調用能力建設**&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;※ 實現邏輯&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;定義 MachineAuditAbilityEnum 統一的動態配置枚舉，並基於 MachineAuditAbilityProcess 實現其實現類。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;統一入參為 Map 結構，通過反射機制、動態 Function 等方式，實現商品信息映射成算法請求參數；&lt;strong&gt;另外為了提升反射的效率，利用預編譯緩存的方式，將字段轉成 MethodHandle&lt;/strong&gt;，後續對同一個字段做反射時，可直接獲取對應的 MethodHandle，提升效率。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;/**
&amp;nbsp;* 緩存類字段的 MethodHandle（Key: Class+FieldName, Value: MethodHandle）
&amp;nbsp; */
private&amp;nbsp;static&amp;nbsp;final&amp;nbsp;Map&amp;lt;String,&amp;nbsp;MethodHandle&amp;gt;&amp;nbsp;FIELD_HANDLE_CACHE&amp;nbsp;=&amp;nbsp;new&amp;nbsp;ConcurrentHashMap&amp;lt;&amp;gt;();


/**
&amp;nbsp;* 根據配置從對象中提取字段值到 Map
&amp;nbsp;*&amp;nbsp;@return&amp;nbsp;提取後的 Map
&amp;nbsp;*/
public&amp;nbsp;Map&amp;lt;String,&amp;nbsp;Object&amp;gt;&amp;nbsp;fieldValueMapping(AutoMachineAlgoRequestConfig requestConfig,&amp;nbsp;Object&amp;nbsp;spuResDTO) {
&amp;nbsp; &amp;nbsp;&amp;nbsp;AutoMachineAlgoRequestConfig.RequestMappingConfig&amp;nbsp;requestMappingConfig = requestConfig.getRequestMappingConfig();
&amp;nbsp; &amp;nbsp;&amp;nbsp;Map&amp;lt;String,&amp;nbsp;Object&amp;gt; targetMap =&amp;nbsp;Maps.newHashMap();
&amp;nbsp; &amp;nbsp;&amp;nbsp;//1.簡單映射關係，直接將 obj 裏的信息映射到 resultMap 當中


&amp;nbsp; &amp;nbsp;&amp;nbsp;//2.遍歷複雜映射關係，value 是基礎類型
&amp;nbsp; &amp;nbsp;&amp;nbsp;//3.遍歷複雜映射關係，value 是對象


&amp;nbsp;&amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;return&amp;nbsp;targetMap;
}


/**
&amp;nbsp;* &amp;nbsp;預編譯 FieldMapping
&amp;nbsp; */
private&amp;nbsp;List&amp;lt;AutoMachineAlgoRequestConfig.FieldMapping&amp;gt;&amp;nbsp;compileConfig(List&amp;lt;AutoMachineAlgoRequestConfig.FieldMapping&amp;gt; fieldMappingList,&amp;nbsp;Object&amp;nbsp;obj) {
&amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;List&amp;lt;AutoMachineAlgoRequestConfig.FieldMapping&amp;gt; mappings =&amp;nbsp;new&amp;nbsp;ArrayList&amp;lt;&amp;gt;(fieldMappingList.size());
&amp;nbsp; &amp;nbsp;&amp;nbsp;//緩存反射 mapping
&amp;nbsp; &amp;nbsp;&amp;nbsp;return&amp;nbsp;mappings;
}


private&amp;nbsp;Object&amp;nbsp;getFieldValue(Object&amp;nbsp;request,&amp;nbsp;String&amp;nbsp;fieldName) throws&amp;nbsp;Throwable&amp;nbsp;{
&amp;nbsp; &amp;nbsp;&amp;nbsp;String&amp;nbsp;cacheKey = request.getClass().getName() +&amp;nbsp;"#"&amp;nbsp;+ fieldName;
&amp;nbsp; &amp;nbsp;&amp;nbsp;MethodHandle&amp;nbsp;handle =&amp;nbsp;FIELD_HANDLE_CACHE.get(cacheKey);
&amp;nbsp; &amp;nbsp;&amp;nbsp;return&amp;nbsp;handle !=&amp;nbsp;null&amp;nbsp;? handle.invoke(request) :&amp;nbsp;null;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;基於實現@FeignClient 註解，實現 HTTP 調用的執行器，其中@FeignClient 中的 URL 表示域名，autoMachineAuditAlgo 方法中的 path 表示具體的 URL，requestBody 是請求體，另外還包含 headers，不同算法需要不同 headers 也可動態配置。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;返回結果均為 String，而後解析成 Map&amp;lt;String,Object&amp;gt;用於規則解析。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;@FeignClient(
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; name =&amp;nbsp;"xxx",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; url =&amp;nbsp;"${}"
)
public&amp;nbsp;interface&amp;nbsp;GenericAlgoFeignClient&amp;nbsp;{


&amp;nbsp; &amp;nbsp;&amp;nbsp;@PostMapping(value =&amp;nbsp;"/{path}")
&amp;nbsp; &amp;nbsp;&amp;nbsp;String&amp;nbsp;autoMachineAuditAlgo(
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;@PathVariable("path")&amp;nbsp;String&amp;nbsp;path,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;@RequestBody&amp;nbsp;Object&amp;nbsp;requestBody,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;@RequestHeader&amp;nbsp;Map&amp;lt;String,&amp;nbsp;String&amp;gt; headers
&amp;nbsp; &amp;nbsp; );
&amp;nbsp; &amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;@GetMapping("/{path}")
&amp;nbsp; &amp;nbsp;&amp;nbsp;String&amp;nbsp;autoMachineAuditAlgoGet(
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;@PathVariable("path")&amp;nbsp;String&amp;nbsp;path,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;@RequestParam&amp;nbsp;Map&amp;lt;String,&amp;nbsp;Object&amp;gt; queryParams,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;@RequestHeader&amp;nbsp;Map&amp;lt;String,&amp;nbsp;String&amp;gt; headers
&amp;nbsp; &amp;nbsp; );


}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;動態配置 JSON。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;{
&amp;nbsp; &amp;nbsp;&amp;nbsp;"url":&amp;nbsp;"/ai-check/demo1",
&amp;nbsp; &amp;nbsp;&amp;nbsp;"requestMappingConfig":&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"fieldMappingList":&amp;nbsp;[
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"sourceFieldName":&amp;nbsp;"categoryId",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"targetKey":&amp;nbsp;"categoryId"
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;},
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"sourceFieldName":&amp;nbsp;"brandId",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"targetKey":&amp;nbsp;"brandId"
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;}
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;],
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"perItemMapping":&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"mappingFunctionCode":&amp;nbsp;"firstAndFirstGroundPic",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"fieldMappingList":&amp;nbsp;[
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"sourceFieldName":&amp;nbsp;"imgId",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"targetKey":&amp;nbsp;"imgId"
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;},
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"sourceFieldName":&amp;nbsp;"imgUrl",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"targetKey":&amp;nbsp;"imgUrl"
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;}
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;]
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;}
&amp;nbsp; &amp;nbsp;&amp;nbsp;}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;機審規則動態解析建設&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;※ 實現邏輯&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;定義 MachineAuditRuleEnum 統一的動態配置枚舉，並基於 MachineAuditRuleProcess 實現其統一實現類。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;搭建 QLExpress 規則引擎，&lt;strong&gt;為了提升 QLExpress 規則引擎的效率，同樣引入了緩存機制&lt;/strong&gt;，在機審規則配置表達式時，則觸發 loadRuleFromJson，將表達式轉換成規則引擎並注入到緩存當中，真正機審流程執行時會直接從緩存裏獲取規則引擎並執行，效率上有很大提升。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;// 規則引擎實例緩存
private&amp;nbsp;static&amp;nbsp;final&amp;nbsp;Map&amp;lt;String,&amp;nbsp;ExpressRunner&amp;gt; runnerCache =&amp;nbsp;new&amp;nbsp;ConcurrentHashMap&amp;lt;&amp;gt;();


// 規則配置緩存
private&amp;nbsp;static&amp;nbsp;final&amp;nbsp;Map&amp;lt;String,&amp;nbsp;GenericEngineRule&amp;gt; ruleConfigCache =&amp;nbsp;new&amp;nbsp;ConcurrentHashMap&amp;lt;&amp;gt;();


// 規則版本信息
private&amp;nbsp;static&amp;nbsp;final&amp;nbsp;Map&amp;lt;String,&amp;nbsp;Integer&amp;gt; ruleVersionCache =&amp;nbsp;new&amp;nbsp;ConcurrentHashMap&amp;lt;&amp;gt;();


/**
&amp;nbsp;* 加載 JSON 規則配置
&amp;nbsp;*&amp;nbsp;@param&amp;nbsp;jsonConfig 規則 JSON 配置
&amp;nbsp;*/
public&amp;nbsp;GenericEngineRule&amp;nbsp;loadRuleFromJson(String&amp;nbsp;ruleCode,&amp;nbsp;String&amp;nbsp;jsonConfig) {


&amp;nbsp; &amp;nbsp;&amp;nbsp;//如果緩存裏已經有並且是最新版本，則直接返回
&amp;nbsp; &amp;nbsp;&amp;nbsp;if(machineAuditCache.isSameRuleConfigVersion(ruleCode) &amp;amp;&amp;amp; machineAuditCache.getRuleConfigCache(ruleCode) !=&amp;nbsp;null) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;return&amp;nbsp;machineAuditCache.getRuleConfigCache(ruleCode);
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp;&amp;nbsp;// 如果是可緩存的規則，預加載


&amp;nbsp;&amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;return&amp;nbsp;rule;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;機審規則執行時，通過配置中的規則名稱，獲取對應的規則引擎進行執行。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;/**
&amp;nbsp;* 根據規則名稱執行規則
&amp;nbsp;*&amp;nbsp;@param&amp;nbsp;ruleCode 規則名稱
&amp;nbsp;*&amp;nbsp;@param&amp;nbsp;context 上下文數據
&amp;nbsp;*&amp;nbsp;@return&amp;nbsp;規則執行結果
&amp;nbsp;*/
public&amp;nbsp;MachineAuditRuleResult&amp;nbsp;executeRuleByCode(String&amp;nbsp;ruleCode,&amp;nbsp;Map&amp;lt;String,&amp;nbsp;Object&amp;gt; context, MachineAuditRuleProcessData ruleProcessData) {
&amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(StringUtils.isBlank(ruleCode)) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;throw&amp;nbsp;new&amp;nbsp;IllegalArgumentException("機審-通用協議-規則-規則名稱不能為空");
&amp;nbsp; &amp;nbsp; }


&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;//從緩存中獲取規則引擎


&amp;nbsp; &amp;nbsp;&amp;nbsp;//基於規則引擎執行 condition


&amp;nbsp; &amp;nbsp;&amp;nbsp;//統一日誌
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;※ 配置 demo&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;動態配置 JSON。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;{
&amp;nbsp; &amp;nbsp;&amp;nbsp;"ruleCode":&amp;nbsp;"demo1",
&amp;nbsp; &amp;nbsp;&amp;nbsp;"name":&amp;nbsp;"規則 demo1",
&amp;nbsp; &amp;nbsp;&amp;nbsp;"ruleType":&amp;nbsp;1,
&amp;nbsp; &amp;nbsp;&amp;nbsp;"priority":&amp;nbsp;100,
&amp;nbsp; &amp;nbsp;&amp;nbsp;"functions":&amp;nbsp;[
&amp;nbsp; &amp;nbsp;&amp;nbsp;],
&amp;nbsp; &amp;nbsp;&amp;nbsp;"conditions":&amp;nbsp;[
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"expression":&amp;nbsp;"result.code == null || result.code != 0",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"action":&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"type":&amp;nbsp;"NO_RESULT",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"messageExpression":&amp;nbsp;"'無結果'"
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;}
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;},
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"expression":&amp;nbsp;"result.data == 0",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"action":&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"type":&amp;nbsp;"PASS",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"messageExpression":&amp;nbsp;"'機審通過"
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;}
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;},
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"expression":&amp;nbsp;"result.data == 1",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"action":&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"type":&amp;nbsp;"REJECT",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"messageExpression":&amp;nbsp;"'異常結果 1'",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"suggestType":&amp;nbsp;2,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"suggestKey":&amp;nbsp;"imgId",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"preAuditSuggestKey":&amp;nbsp;"imgUrl"
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;}
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;},
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"expression":&amp;nbsp;"result.data == 2",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"action":&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"type":&amp;nbsp;"REJECT",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"messageExpression":&amp;nbsp;"'異常結果 2'",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"suggestType":&amp;nbsp;2,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"suggestKey":&amp;nbsp;"imgId",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"preAuditSuggestKey":&amp;nbsp;"imgUrl"
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;}
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;}
&amp;nbsp; &amp;nbsp;&amp;nbsp;],
&amp;nbsp; &amp;nbsp;&amp;nbsp;"defaultAction":&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"type":&amp;nbsp;"PASS"
&amp;nbsp; &amp;nbsp;&amp;nbsp;}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;span id="OSC_h1_8"&gt;&lt;/span&gt; 
&lt;h1&gt;八、關於數據分析&amp;amp;指標提升&lt;/h1&gt; 
&lt;p&gt;在經歷了 2-3 個版本搭建完新機審鏈路 + 數據埋點之後，指標一直沒有得到很好的提升，曾經一度只是維持在 20% 以內，甚至有部分時間降低到了 10% 以下；經過大量的數據分析之後，識別出了部分規則產品邏輯存在漏洞、算法存在誤識別等情況，並較為有效的通過數據推動了產品優化邏輯、部分類目規則調整、算法迭代優化等，在一系列的動作做完之後，指標提升了 50%+。&lt;/p&gt; 
&lt;p&gt;在持續了比較長的一段時間的 50%+覆蓋率之後，對數據進行了進一步的剖析，發現這 50%+在那個時間點應該是到了瓶頸，原因是像「標題描述包含顏色相關字樣」、「標題存在重複文案」以及部分輪播圖規則，實際就是會存在不符合預期的情況，因此緊急與產品溝通，後續的非緊急需求停止，先考慮將這部分天然不符合預期的情況進行處理。&lt;/p&gt; 
&lt;p&gt;之後指標提升的動作主要圍繞：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;算法側產出各算法能力的召回率、準確率，達標的算法由產品與業務拉齊，是否配置自動駁回的能力。&lt;/li&gt; 
 &lt;li&gt;部分缺乏自動修改能力的機審規則，補充臨時需求建設對應的能力。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;經過產研業務各方的配合，以最快速度將這些動作進行落地，指標也得到了較大的提升。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;往期回顧&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;1.營銷會場預覽直通車實踐｜得物技術&lt;/p&gt; 
&lt;p&gt;2.基於 TinyMce 富文本編輯器的客服自研知識庫的技術探索和實踐｜得物技術&lt;/p&gt; 
&lt;p&gt;3.AI 質量專項報告自動分析生成｜得物技術&lt;/p&gt; 
&lt;p&gt;4.社區搜索離線回溯系統設計：架構、挑戰與性能優化｜得物技術&lt;/p&gt; 
&lt;p&gt;5.eBPF 助力 NAS 分鐘級別 Pod 實例溯源｜得物技術&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;文 / 沃克&lt;/p&gt; 
&lt;p&gt;關注得物技術，每週更新技術乾貨&lt;/p&gt; 
&lt;p&gt;要是覺得文章對你有幫助的話，歡迎評論轉發點贊～&lt;/p&gt; 
&lt;p&gt;未經得物技術許可嚴禁轉載，否則依法追究法律責任。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/5783135/blog/18689561</link>
      <guid isPermaLink="false">https://my.oschina.net/u/5783135/blog/18689561</guid>
      <pubDate>Tue, 26 Aug 2025 07:04:07 GMT</pubDate>
      <author>原創</author>
    </item>
    <item>
      <title>阿里國際發佈並開源新一代多模態大模型 Ovis2</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;阿里國際&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FHTfKZDEzf197FzXEMn8htw" target="_blank"&gt;正式發佈&lt;/a&gt;新一代多模態大模型 Ovis2.5。Ovis2.5 是一款面向原生分辨率視覺感知、深度推理與高性價比場景設計的多模態大模型。據稱在主流多模態評測套件 OpenCompass 上的綜合得分相較 Ovis2 進一步提升，並在同類開源模型中繼續保持&amp;nbsp;SOTA 水平。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0826/150018_hr7W_2720166.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;本次開源包含兩個版本：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Ovis2.5-9B：OpenCompass 綜合得分&amp;nbsp;78.3，超越眾多更大參數量的模型，在 40B 以下參數規模的開源模型中排名第一。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Ovis2.5-2B：OpenCompass 綜合得分 73.9，延續了 Ovis 系列「小身板，大能量」的理念，在同尺寸模型中性能顯著領先，是端側和資源受限場景的理想選擇。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Ovis2 整體框架：&lt;/p&gt; 
&lt;p&gt;&lt;img height="1131" src="https://static.oschina.net/uploads/space/2025/0826/145646_bXvR_2720166.png" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;官方介紹稱，Ovis2.5 在架構、訓練與數據三方面進行了系統性創新。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;模型架構：延續 Ovis 系列創新的結構化嵌入對齊設計。Ovis2.5 由三大組件構成：動態分辨率 ViT 高效提取視覺特徵，Ovis 視覺詞表模塊實現視覺與文本嵌入的結構對齊，最後由強大的 Qwen3 作為語言基座，處理多模態嵌入並生成文本輸出。&lt;/li&gt; 
 &lt;li&gt;訓練策略：採用更精細的五階段訓練範式，從基礎的視覺預訓練、多模態預訓練、大規模指令微調，到利用 DPO 和 GRPO 等算法進行偏好對齊和推理能力強化，循序漸進構建模型能力。同時，通過多模態數據打包和混合並行等優化，實現了 3-4 倍的端到端訓練加速。&lt;/li&gt; 
 &lt;li&gt;數據工程：Ovis2.5 的數據規模相比 Ovis2 增加了 50%，重點聚焦視覺推理、圖表、OCR、Grounding 等關鍵方向。尤其是合成了大量與 Qwen3 深度適配的「思考（thinking）」數據，有效激發了模型的反思與推理潛能。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;詳情查看&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;代碼：https://github.com/AIDC-AI/Ovis&lt;br&gt; 模型： https://huggingface.co/AIDC-AI/&lt;br&gt; Ovis2.5-2B https://huggingface.co/AIDC-AI/&lt;br&gt; Ovis2.5-9B Demo： https://huggingface.co/spaces/AIDC-AI/&lt;br&gt; Ovis2.5-2B https://huggingface.co/spaces/AIDC-AI/&lt;br&gt; Ovis2.5-9B 技術報告: https://arxiv.org/abs/2508.11737&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368548</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368548</guid>
      <pubDate>Tue, 26 Aug 2025 07:03:07 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>AI 搜索創企 Perplexity AI 推出版權收入分成模式</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;AI 搜索創企 Perplexity AI 宣佈了一&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.bloomberg.com%2Fnews%2Farticles%2F2025-08-25%2Fperplexity-to-let-publishers-share-in-revenue-from-ai-searches" target="_blank"&gt;項重大舉措&lt;/a&gt;，將設立數百萬美元資金用於支付給媒體出版商和記者，旨在構建一種全新的搜索收入共享模式。&lt;/p&gt; 
&lt;p&gt;據這家總部位於舊金山的初創公司透露，其媒體合作伙伴很快將在作品被 Perplexity 的瀏覽器或 AI 助手用於解答用戶疑問與請求時獲得報酬。Perplexity 團隊在一篇博客文章中表示：「&lt;strong&gt;我們正以適合人工智能時代的模式對出版商進行補償。&lt;/strong&gt;」&lt;/p&gt; 
&lt;p&gt;此次資金髮放將通過一項名為 Comet Plus 的訂閲服務實施，該服務預計在未來幾個月推出。據悉，Perplexity 已預留 4250 萬美元資金用於與出版商分享，且該資金池有望隨時間增長。Comet Plus 每月訂閲費用為 5 美元，這對於已付費使用 Perplexity 高級版本的用戶來説是一項額外福利。&lt;/p&gt; 
&lt;p&gt;此前，Perplexity AI 面臨諸多媒體機構的訴訟，如《華爾街日報》《紐約時報》以及日本《讀賣新聞》等，這些機構指控該公司不正當地從其作品中獲利。而此次收入共享模式的推出，或可視為 Perplexity AI 向出版商拋出的 「橄欖枝」，以改善關係並應對相關指控。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368546</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368546</guid>
      <pubDate>Tue, 26 Aug 2025 06:53:07 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>宇樹科技人形機器人足專利獲授權</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;天眼查 App 顯示，杭州宇樹科技股份有限公司 8 月 26 日申請的「一種人形機器人足和一種人形機器人」專利獲授權。&lt;/p&gt; 
&lt;p&gt;&lt;img height="287" src="https://oscimg.oschina.net/oscnet/up-85540c9e38b8e5950068b200c4d25b74447.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;摘要顯示，本實用新型涉及人形機器人技術領域，提供的一種人形機器人足，包括足部外殼和彈性足底件，足部外殼和彈性足底件固定連接，彈性足底件內設有若干可形變的氣腔，氣腔連接有穿過足部外殼並與外部連通的氣管件。&lt;/p&gt; 
&lt;p&gt;本實用新型提供的一種人形機器人足，將傳感器等脆弱部件設置在遠離頻繁與地面撞擊的位置，僅在彈性足底件內設置形變氣腔，不易受到碰撞而損壞；通過氣管件與氣腔的空間變化傳遞擠壓信息，響應快，不受電機本身特性的影響；彈性足底件與氣腔一體化設計，更換維修方便快捷。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368544</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368544</guid>
      <pubDate>Tue, 26 Aug 2025 06:42:07 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>開發者反饋 DeepSeek-V3.1 出現嚴重 bug：返回內容隨機插入「極/極/extreme」等字符</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;近日有開發者&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flinux.do%2Ft%2Ftopic%2F898502" target="_blank"&gt;反饋&lt;/a&gt;DeepSeek V3.1 在生成文本時會在完全不可預期的位置插入「極」「極」「extreme」三個 token。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0826/142557_OpI9_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;開源社區用戶給出多組復現場景：在 Go 等語言生成裏，模型會把詞元「粘」到標識符中，`Second` 前隨機插入「極/極/extreme」，即便是 `top_k=1， temperature=1` 的保守解碼也躲不過。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-5528ee293cd32634855ecedec24f895f010.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;問題最早在火山、chutes 等第三方 API 被發現，最初懷疑與 IQ1_S 高壓縮量化、imatrix 校準數據異常或部署配置錯誤有關，但隨後測試證實官方網頁端在 FP8 全精度下亦出現同樣現象，&lt;strong&gt;且官方端出現概率最低，第三方顯著升高&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0826/142634_Kbh4_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;中文簡體「極」對應 ID 2577，繁體「極」對應 ID 16411，英文「extreme」對應 ID 15075。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;社區推測可能是訓練數據清洗殘留，或與模型「偷懶」機制相關，但部分案例仍無法解釋。一旦觸發，後續生成會愈發頻繁，已嚴重影響編程及任何對結構敏感的任務可用性。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368538</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368538</guid>
      <pubDate>Tue, 19 Aug 2025 06:27:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>新一代中國操作系統銀河麒麟 V11 正式發佈</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;全新一代中國操作系統——銀河麒麟操作系統 V11 在 2025 中國操作系統產業大會正式發佈。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="332" src="https://oscimg.oschina.net/oscnet/up-e78b1fe4f30a2acbeef45b2c7d54dfd9611.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;這款系統使用了全新磐石架構，操作體驗、安全性和生態豐富度大幅提升。作為首個突破百萬生態的國產操作系統，銀河麒麟的生態成熟度國內領先，與國產主流 CPU、GPU 及板卡實現全面兼容，構建起完整的國產化生態體系。已在嫦娥探月、天問探火等國家重大工程中發揮支撐作用，在政務領域保持較高覆蓋率，同時在金融、能源、教育、醫療等行業實現長期規模化應用。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;目前，銀河麒麟操作系統已部署超 1600 萬套。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368537</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368537</guid>
      <pubDate>Tue, 19 Aug 2025 06:23:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>68% 的科技專業人士對 AI 招聘工具不信任</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span&gt;最新&lt;/span&gt;發佈的 Dice 報告指出，68% 的科技專業人士對 AI 驅動的招聘系統表示不信任，同時 80% 的人更傾向於人類主導的招聘方法。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;報告顯示，近 30% 的受訪者考慮完全離開科技行業，因為他們對 AI 增強招聘過程的挫敗感更為強烈，尤其是女性羣體對此反應更為明顯。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在接受 TechRepublic 的電子郵件採訪時，Dice 首席執行官阿特・齊爾（Art Zeile）表示，儘管 AI 在提升招聘團隊的工作效率方面發揮了積極作用，但 68% 的不信任比例 「並非小數目，這一信號表明，從候選人的角度來看，招聘系統的根本性問題依然存在。」 他補充説，30% 科技工作者考慮退出行業的調查結果尤其令人擔憂。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;齊爾指出，這種不信任的根本原因在於 AI 在招聘中的應用方式，尤其是在缺乏透明度和人類監督的情況下。他強調：「候選人明確表示，當招聘過程像一個黑箱時，信任感就會消失。」 儘管僱主可能在提高運營效率，但如果這導致&lt;span&gt;頂尖&lt;/span&gt;人才的疏遠，那對於整個行業來説就是一個雙輸的局面。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;根據 Dice 報告，92% 的科技專業人士認為，由於 AI 工具偏向於關鍵詞優化，很多合格的候選人可能被忽視。齊爾表示：「這反映出一種認知，認為系統更看重一致性而非能力。」 此外，78% 的受訪者感覺目前的招聘實踐迫使他們誇大自己的資歷，65% 的人已經調整過簡歷，以便更好地符合 AI 篩選的標準。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;齊爾強調，Dice 報告表明 AI 並不是敵人，而是需要更具思考性的整合。科技專業人士並非拒絕創新，而是希望招聘過程更加公平、有人性和透明。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;他表示：「解決方案不是放棄 AI，而是更負責任地應用它。採用 AI 來支持而不是取代人類決策的混合招聘模型，其信任度是完全自動化方法的三倍。」&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368529</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368529</guid>
      <pubDate>Tue, 19 Aug 2025 05:46:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>通義萬相預告新模型 Wan2.2-S2V</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;通義萬相團隊深夜&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2FAlibaba_Wan%2Fstatus%2F1960012297059057935" target="_blank"&gt;發佈預告推文&lt;/a&gt;，稱即將推出新模型 Wan2.2-S2V，該模型將具備生成帶音頻視頻的能力。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1774" src="https://static.oschina.net/uploads/space/2025/0826/115937_9VKc_2720166.png" width="2356" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;通義萬相 Wan2.2 是一款開源視頻生成模型，它率先在視頻生成擴散模型中引入 MoE 架構，有效解決視頻生成處理 Token 過長導致的計算資源消耗大問題。Wan2.2 還首創了「電影美學控制系統」，光影、色彩、構圖、微表情等能力媲美專業電影水平。例如，用戶輸入「黃昏」、「柔光」、「邊緣光」、「暖色調」「中心構圖」等關鍵詞，模型可自動生成金色的落日餘暉的浪漫畫面；使用「冷色調」、「硬光」、「平衡圖」、「低角度」的組合，則可以生成接近科幻片的畫面效果。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368515</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368515</guid>
      <pubDate>Tue, 19 Aug 2025 04:04:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>阿里開源 Vivid-VR：AI 視頻修復神器</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;阿里雲推出了一款名為 Vivid-VR 的開源生成式視頻修復工具，基於先進的文本到視頻（T2V）基礎模型，結合 ControlNet 技術，確保視頻生成過程中的內容一致性。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="211" src="https://static.oschina.net/uploads/space/2025/0826/112840_Ip58_4252687.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;該工具能夠有效修復真實視頻或 AIGC(AI 生成內容) 視頻中的質量問題，消除閃爍、抖動等常見缺陷，為內容創作者提供了一個高效的素材補救方案。無論是對低質量視頻的修復，還是對生成視頻的優化，Vivid-VR 都展現出了卓越的性能。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;Vivid-VR 的核心技術在於其結合了 T2V 基礎模型與 ControlNet 的創新架構。T2V 模型通過深度學習生成高質量視頻內容，而 ControlNet 則通過精準的控制機制，確保修復後的視頻在幀間保持高度的時間一致性，避免了常見的閃爍或抖動問題。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;據悉，該工具在生成過程中能夠動態調整語義特徵，顯著提升視頻的紋理真實感和視覺生動性。這種技術組合不僅提高了修復效率，還為視頻內容保持了更高的視覺穩定性。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;Vivid-VR 的另一大亮點是其廣泛的適用性。無論是傳統拍攝的真實視頻，還是基於 AI 生成的內容，Vivid-VR 都能提供高效的修復支持。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;對於內容創作者而言，低質量素材常常是創作過程中的痛點，而 Vivid-VR 能夠通過智能分析和增強，快速修復模糊、噪點或不連貫的視頻片段，為短視頻、影視後期製作等領域提供了實用工具。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;此外，該工具支持多種輸入格式，開發者可以根據需求靈活調整修復參數，進一步提升創作效率。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368510</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368510</guid>
      <pubDate>Tue, 19 Aug 2025 03:31:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>微軟開源文本轉語音模型 VibeVoice，支持最多 4 位説話人同時發聲</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;微軟正式開源了其最新的文本轉語音（TTS）模型&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmicrosoft.github.io%2FVibeVoice%2F"&gt;VibeVoice-1.5B&lt;/a&gt;，該模型主打 「超長、多人、高壓縮」，單次即可生成長達 90 分鐘的連續語音，並支持最多 4 位説話人同時發聲。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-b4cb0ed81f165defca082e67898ced868fb.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;VibeVoice-1.5B 的核心創新在於其雙 Tokenizer 設計。模型分為兩個獨立但協同工作的模塊。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-0a84db4a09d68c549f237fc0f439ca53cec.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;1. 聲學 Tokenizer：負責保留聲音特徵並實現高壓縮率&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;採用變分自編碼器（VAE）的對稱編碼 - 解碼結構，解決了傳統 VAE 在長序列建模中容易出現的 「方差坍縮」 問題（即數據多樣性丟失）。&lt;/p&gt; 
&lt;p&gt;通過 7 階段的改進型 Transformer 模塊和 1D 深度可分離因果卷積，將 24kHz 採樣率的原始音頻壓縮為每秒僅 7.5 個潛在向量，累計壓縮率達 3200 倍，壓縮效率是主流 Encodec 模型的 80 倍。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2. 語義 Tokenizer：專注於提取與文本對齊的語義特徵。&amp;nbsp;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;架構與聲學 Tokenizer 的編碼器部分一致，但移除了變分自編碼器組件，以確保語義特徵的確定性。&lt;/p&gt; 
&lt;p&gt;訓練過程中，語義 Tokenizer 通過 「自動語音識別」 任務強制綁定語音與文本，最終捨棄解碼器以提升推理速度 40%。&lt;/p&gt; 
&lt;p&gt;這種分工協作的模式，既保留了語音的細節（如音色、節奏），又確保了內容與文本的語義一致性，避免了傳統模型中常見的 「音色與情緒不匹配」 問題。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;開源地址&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;https://github.com/microsoft/VibeVoice&lt;/em&gt;&lt;br&gt; https://huggingface.co/microsoft/VibeVoice-1.5B&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368509</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368509</guid>
      <pubDate>Tue, 19 Aug 2025 03:31:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>播放器視頻後處理實踐</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;span id="OSC_h1_1"&gt;&lt;/span&gt; 
&lt;h1&gt;&lt;strong&gt;&lt;strong&gt;1. 前言&lt;/strong&gt;&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;在播放器架構不斷演進的今天，視頻後處理技術正在成為提升用戶體驗的關鍵環節。相比傳統的解碼即播，現代播放器越來越多地引入後處理鏈路，通過增強畫質、渲染氛圍等手段，為用戶提供更具沉浸感的視聽體驗。&lt;/p&gt; 
&lt;p&gt;本系列文章將系統介紹我們在播放器視頻後處理模塊中的技術方案與工程實現，涵蓋從效果設計、算法選型，到性能優化和跨平台兼容的全鏈路細節。第一期內容聚焦在兩類核心能力：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;視頻增強：提升畫面清晰度、對比度與色彩表現，尤其針對暗場、低碼率等場景進行針對性優化；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;氛圍模式：基於視頻內容實時生成邊緣延展光效，打造更強沉浸感，適配大屏與移動端場景。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;本文將着重介紹我們如何在性能受限的設備上實現視頻增強效果，如何結合 GPU/OpenGL、Shader 編程以及平台圖像處理 API 構建高效可控的處理鏈路。後續我們將陸續推出如氛圍模式等視頻後處理文章，敬請期待。&lt;/p&gt; 
&lt;span id="OSC_h1_2"&gt;&lt;/span&gt; 
&lt;h1&gt;&lt;strong&gt;&lt;strong&gt;2. 視頻增強（亮度和色彩）&lt;/strong&gt;&lt;/strong&gt;&lt;/h1&gt; 
&lt;span id="OSC_h2_3"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;丨 2.1 什麼是視頻增強技術&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;視頻增強技術是指一系列用於改善視頻質量的技術手段，其目的是在不改變原始內容的情況下提升視頻的視覺效果。技術的應用場景包括視頻播放、編輯、傳輸、存儲等領域，常用於提高圖像清晰度、對比度、色彩飽和度等，使觀看者獲得更好的視覺體驗。&lt;/p&gt; 
&lt;span id="OSC_h2_4"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;丨 2.2 常見視頻增強技術&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-9e08c3b613e8e25991a9a403c48dbb6b777.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-dae5e8769705e37ee7fe3451db365b10a0c.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;移動端實踐：亮度與色彩增強。針對 Android/iOS 平台的視頻播放場景，我們重點實現了亮度增強與色彩增強兩項關鍵技術。本文將分享技術落地中的核心方案與優化經驗。&lt;/p&gt; 
&lt;span id="OSC_h2_5"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;丨 2.3 亮度增強&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-111882e6b535a736ac5c3ae7f921187c316.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;亮度增強效果示意圖（左：原圖，右：增強後）&lt;/p&gt; 
&lt;span id="OSC_h3_6"&gt;&lt;/span&gt; 
&lt;h3&gt;2.3.1 技術選型&lt;/h3&gt; 
&lt;p&gt;亮度增強是圖像/視頻處理中非常基礎且常見的操作，常見的亮度增強原理可以分為以下幾類，每種方式背後的核心思想略有不同。下面是詳細的分類和解釋：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;線性亮度增強（線性增益）&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;原理：RGB 整體直接乘以一個大於 1 的係數（或加一個偏移量）。&lt;/p&gt; 
&lt;p&gt;公式：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;color.rgb&amp;nbsp;= color.rgb * gain; &amp;nbsp; &amp;nbsp; &amp;nbsp; // 乘法增強 color.rgb = color.rgb + offset; &amp;nbsp; &amp;nbsp; // 加法增強

&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;簡而言之，這種做法就是簡單粗暴的在原本的 RGB 上進行提升，從這裏，可以想到 RGB 顏色調整後容易出現色偏。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;那麼我們可能會想到，如果先將 RGB 轉換為 YUV，調節 Y 分量，再反變換為 RGB。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;公式：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Y&amp;nbsp;=&amp;nbsp;0.299*R +&amp;nbsp;0.587*G +&amp;nbsp;0.114*B;
Y_new&amp;nbsp;= Y * gain;

&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;這確實是視頻增強中一種常用且理論上「更穩」的方式，因為它分離了亮度（Y）和色彩（UV / IQ / CbCr）信息。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;但這種處理方式有一個嚴重的問題，不處理圖像的對比度或中間的關係，且不能保留高光細節（Clipping），也就是調整後，超過範圍[0.0,1.0]的值會被截斷（clamp），造成高光過曝。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;直方圖均衡（Histogram Equalization）&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;原理：通過調整像素分佈，讓亮度值均勻分佈在整個區間，從而整體提升視覺亮度。&lt;/p&gt; 
&lt;p&gt;特點：增強暗部和亮部的對比，對低對比度圖像尤其有效。&lt;/p&gt; 
&lt;p&gt;實現相對複雜，不常用於實時 shader，考慮到其運算複雜性，我們也 pass 了這種方式。&lt;/p&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Gamma 變換（冪律調整）&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;原理：使用冪函數對像素進行非線性拉伸。&lt;/p&gt; 
&lt;p&gt;公式：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;color.rgb&amp;nbsp;= pow(color.rgb, vec3(gamma));

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;特點：γ &amp;lt; 1：圖像變亮，主要拉昇暗部；γ &amp;gt; 1：圖像變暗，壓縮亮部。&lt;/p&gt; 
&lt;p&gt;具有兩個優點：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;調整方式具有非線性特點，能更細膩地控制中間調亮度，避免簡單加法可能引起的局部過曝或暗部細節丟失。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;模擬現實中顯示設備的響應曲線，效果較為自然。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;這也是我們最後選擇的方式，他的運算量簡單，適合端上視頻播放的實時處理。&lt;/p&gt; 
&lt;span id="OSC_h3_7"&gt;&lt;/span&gt; 
&lt;h3&gt;2.3.2 背後的原理&lt;/h3&gt; 
&lt;p&gt;我們引申一下，這種方式的優點是怎麼得出來的呢。&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;為何能避免簡單加法可能引起的局部過曝或暗部細節丟失&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;從公式看，原本亮度較低的像素會被相對「提亮」更多，而原本亮度較高的像素提升幅度較小。暗部像素相對於原值會獲得更大的「提拉」，而亮部像素則變化較小，從而既能提升整體曝光，又能保留高光細節。&lt;/p&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;為什麼説模擬現實中顯示設備的響應曲線，更為自然呢？&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;因為顯示器、人眼視覺和視頻編碼，都是非線性系統，不是簡單線性變化。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;真實世界的光亮度是線性的，比如兩支燈加起來就是兩倍亮。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;但人眼感知亮度是對數感知的（小亮度變化很敏感，大亮度變化不敏感）。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;視頻和圖像在存儲時通常經過一個 Gamma 編碼，原本線性光 → 壓縮（比如取 1/2.2 次方） → 存成文件。這種光和電的轉換過程，就是 OETF/EOTF 響應曲線。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;所以這種 pow(color, gamma) 的調整方式，實際就是在模擬顯示端的響應曲線。&lt;/p&gt; 
&lt;p&gt;總結一句話：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;編碼有 Gamma，所以顯示端或後處理也必須按照 Gamma 空間規則來調節，才能保持自然感知。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;span id="OSC_h2_8"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;丨 2.4 色彩增強&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-329f6cfe91ee30986ddb96559406ffacab5.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;色彩增強效果示意圖（左：原圖，右：增強後）&lt;/p&gt; 
&lt;p&gt;從上圖可以看到山體、草地上的花，飽和度增強。&lt;/p&gt; 
&lt;span id="OSC_h3_9"&gt;&lt;/span&gt; 
&lt;h3&gt;2.4.1 調節的目標&lt;/h3&gt; 
&lt;p&gt;1. 增強色彩感知&lt;/p&gt; 
&lt;p&gt;提高圖像的「鮮豔度」或「視覺吸引力」，讓圖像更生動。&lt;/p&gt; 
&lt;p&gt;特別是在圖像顏色偏灰、曝光不佳或圖像壓縮後顏色損失的情況下。&lt;/p&gt; 
&lt;p&gt;2. 突出主體&lt;/p&gt; 
&lt;p&gt;通過飽和度調節，增強主體與背景之間的色彩對比，提高視覺聚焦度。&lt;/p&gt; 
&lt;p&gt;3. 修復/還原真實色彩&lt;/p&gt; 
&lt;p&gt;對攝像頭採集後色彩不足的圖像進行還原，尤其是膚色、植物、天空等自然色彩。&lt;/p&gt; 
&lt;p&gt;針對上述目標，我們主要依賴主觀評測感受，同時需要避免以下問題：&lt;/p&gt; 
&lt;p&gt;主觀評估（人眼視覺）&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;色彩鮮明但不刺眼：增強後色彩更加明顯但不過飽和。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;膚色自然：人臉或皮膚色調不過紅或黃（膚色是視覺最敏感區域）。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;色彩分佈均衡：圖像中顏色種類豐富但不過分集中某一色調。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;無色彩斷層：調節後顏色過渡應平滑，不能有色階突變。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="OSC_h3_10"&gt;&lt;/span&gt; 
&lt;h3&gt;2.4.2 技術選型&lt;/h3&gt; 
&lt;p&gt;目前業界對色彩增強主要有以下 2 種方向的研究：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;傳統 SDR 色彩增強。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;SDR2HDR，模擬 HDR 效果，達到增強目的。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;從實現方式上，主要也有 2 種主流方式：&lt;/p&gt; 
&lt;p&gt;1. 非神經網絡（傳統算法 or 結合 lut 查找表）&lt;/p&gt; 
&lt;p&gt;2. 基於神經網絡（模型）&lt;/p&gt; 
&lt;p&gt;模型需要較高的技術儲備，且在移動端運行耗時大，所以目前我們沒有選擇這種方式，而是尋找效果較好且可控的算法。&lt;/p&gt; 
&lt;span id="OSC_h4_11"&gt;&lt;/span&gt; 
&lt;h4&gt;2.4.2.1 色彩三要素&lt;/h4&gt; 
&lt;p&gt;我們先了解下「色彩三要素」。他們是色彩學中用於描述顏色感知的三個基本維度，分別是：色相、飽和度、明度。這三者共同定義了一個顏色的完整視覺特性。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-2b30a9ad27103861d53891b58e1a3e94cdb.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-732e1f0d9f8fb805edc7e32085ce32787f6.jpg" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;色相&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-84e06db649fa950d64da12884126416abef.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;飽和度&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-627291ade068565461ff9ac2bbf87a6ece7.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;明度&lt;/p&gt; 
&lt;p&gt;在色彩增強中，一般主要調節的是飽和度（Saturation），其次可能會適當調整明度（Brightness / Value） ，而色相（Hue）通常不會主動改變。原因如下：&lt;/p&gt; 
&lt;p&gt;常調節的要素及原因：&lt;/p&gt; 
&lt;p&gt;1. 飽和度（Saturation）&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;最常調節的要素，增強後畫面顯得更鮮豔、更有吸引力，尤其適用於風景、商品、動漫類畫面。可提升視覺衝擊力和色彩表現力。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;明度 / 亮度（Brightness / Value）&lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt;有時作為輔助增強項，提高整體圖像的通透感。與 Gamma 調節、曝光補償常一起使用，即配合使用上一章節的亮度調整即可。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;色相（Hue）&lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt;一般不調整，因為改變色相會改變物體本身顏色，可能導致不真實（如人臉偏色、草地變藍等）。只在需要藝術化或特殊濾鏡（如復古風格、紅外效果）時才會使用。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="OSC_h4_12"&gt;&lt;/span&gt; 
&lt;h4&gt;2.4.2.2 顏色空間的選擇&lt;/h4&gt; 
&lt;p&gt;選擇好色彩增強的調節方向為『飽和度』後，第二步，我們需要選擇好顏色空間。&lt;/p&gt; 
&lt;p&gt;當視頻一幀畫面作為 GL 紋理輸入到後處理鏈路時，為 RGB 顏色模型，我們想要調節飽和度，則需要將其轉換為其他顏色空間進行調節，那麼面臨的第一個問題是如何選擇合適的顏色模型去進行算法設計？&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;RGB&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;HSV&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;LCH/LAB&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="OSC_h4_13"&gt;&lt;/span&gt; 
&lt;h4&gt;2.4.2.3 基於 RGB 空間&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;基於 RGB 顏色直接調節&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;們可以理解，飽和度是色彩的純度，即色彩相對於灰度（無色）的程度。那麼我們可以基於 RGB 顏色模型，並根據灰度進行差值混合即可。&lt;/p&gt; 
&lt;p&gt;如 GPUImage 的 GPUImageSaturationFilter 提供了類似例子，它對飽和度調節，是基於 RGB 顏色，然後取出灰度值通過在原始顏色和灰度之間插值，mix(vec3(luma), color.rgb, saturation) 實現了飽和度的變化：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;插值因子 saturation 越接近 0，圖像越趨向於灰度；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;saturation 越高，圖像越接近原始顏色或超出原始飽和度，色彩更鮮豔。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;這種簡單的算法存在一個問題：原本局部飽和度已經比較高，如果依然提高飽和度，則局部細節消失。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-27d7b0994fc4fac57b89d002c584918656a.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;過飽和，細節丟失&lt;/p&gt; 
&lt;p&gt;2. 為瞭解決上述問題，我們基於自然飽和度的調整。&lt;/p&gt; 
&lt;p&gt;自然飽和度（Vibrance）的概念最先由 photoshop 提出，重點在於適應性，自然飽和度調整後一般比飽和度調整要自然。其核心特點：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-d0784de60346b9dcbeef45c7d5a1ca98fcc.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;進行自適應飽和度調節的流程：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;計算亮度（Luma）：使用加權平均公式從 RGB 獲取亮度：luma = 0.2126 * r + 0.7152 * g + 0.0722 * b&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;計算飽和度（Saturation）：使用 RGB 最大值和最小值之差估算色彩純度：saturation = max(r, g, b) - min(r, g, b)&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-1a7eddfe1e8eed43260770fef02643989f3.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;計算調節因子 k：根據當前飽和度和用戶設置的 Vibrance 強度進行非線性調節：k = 1.0 + Vibrance * (1.0 - saturation / 255.0)（Vibrance 取值範圍通常為 0.0 ~ 1.0）&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;應用顏色調整：將顏色向亮度方向插值，使低飽和度顏色更鮮豔，同時高飽和區域變化較小：color.rgb = mix(vec3(luma), color.rgb, k)&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;其調整傾向於將 RGB 值往同一個 luma 值進行靠近，也是無法保證顏色保持穩定，容易會發生偏色的情況。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-6e84cd82793fb316a2c29384bc13928c8cc.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;色彩增強效果示意圖（左：原圖，右：自然飽和度增強後）&lt;/p&gt; 
&lt;p&gt;於是，我們繼續探索其他的顏色模型。&lt;/p&gt; 
&lt;span id="OSC_h4_14"&gt;&lt;/span&gt; 
&lt;h4&gt;2.4.2.4 基於 HSV 顏色模型的飽和度調整&lt;/h4&gt; 
&lt;p&gt;基於 HSV 飽和度的調整方法是將 RGB 顏色模型轉換為 HSV 顏色模型，其中 HSV 分別表示色相（Hue）、飽和度（Saturation）、明度（Value）。只調整飽和度可以在不影響明暗和色相的情況下增強色彩的鮮豔程度。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-8f18a721fd76edb4555d4d7841ea77d99e9.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;將常見的調整方法有整體抬升，按比例增加，或者曲線調整，達到將整體飽和度提高的目的。但是飽和度調整同時提升所有顏色的強度，比較粗暴。&lt;/p&gt; 
&lt;p&gt;有可能導致：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;本來局部飽和度已經比較高，調節後過飽和，局部細節的消失。(和上一章節例子一樣)。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;本來局部飽和度較低，接近白色，加大飽和度後，容易出現色塊。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-57e3e6fbadb52f1a1f7d3ffa28d34c30508.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;普通調節&lt;/p&gt; 
&lt;p&gt;如何優化：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;對此引入對源的飽和度的檢測，設定上下限制，平滑調節。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;在 HSV 顏色模型上，引入了類似自適應飽和度調整的方式。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;目的：在低飽和度區域，避免突然增加飽和度。低飽和度的顏色（例如接近灰色的顏色）通常對飽和度調整非常敏感，因此需要一種平滑的方式。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;目的：在高飽和度區域減少權重，避免過度增強飽和度。高飽和度的區域本身已經很飽和，進一步增加飽和度會導致過飽和，視覺上顯得不自然。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-aba8ad7b559850b5ef9d9043a47954299ca.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;加入自適應後&lt;/p&gt; 
&lt;span id="OSC_h4_15"&gt;&lt;/span&gt; 
&lt;h4&gt;2.4.2.5 膚色保護&lt;/h4&gt; 
&lt;p&gt;採用 HSV 空間調整後，我們還需要考慮一個核心問題：&lt;/p&gt; 
&lt;p&gt;在圖像色彩增強（如飽和度調整、色調映射）時，膚色區域容易因過度調整而失真（如過紅、過黃或慘白）。需通過膚色識別技術，對檢測到的膚色區域進行保護，限制增強幅度，保持自然觀感。&lt;/p&gt; 
&lt;p&gt;在此引入了基於 HSV 色彩模型的膚色識別，HSV 色彩模型也同樣將亮度與顏色進行了分離，因此對於光照變化也有很強的抗幹擾能力，可以較好的識別出膚色。&lt;/p&gt; 
&lt;p&gt;結合 HSV 色彩模型和高斯概率模型實現膚色保護，具體步驟如下：參考 GPUImageSkinToneFilter 的膚色識別方法。&lt;/p&gt; 
&lt;p&gt;(1) RGB 轉 HSV 空間&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;將圖像從 RGB 轉換到 HSV 空間，分離色調（H）、飽和度（S）、亮度（V）。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;優勢：HSV 的色調通道（H）對光照變化魯棒，更適合膚色識別。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;(2) 膚色概率計算&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;膚色色調模型：&lt;/p&gt; &lt;p&gt;統計膚色色調的均值 skinHue = 0.05（典型值，對應黃紅色調）。&lt;/p&gt; &lt;p&gt;方差相關參數 skinHueThreshold = 40（控制膚色範圍寬度）。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;距離計算：&lt;/p&gt; &lt;p&gt;計算當前像素色調 h 與 skinHue 的歸一化距離。&lt;/p&gt; &lt;p&gt;dist = abs(h - skinHue) / 0.5 高斯權重（概率）。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;通過高斯函數計算膚色概率：&lt;/p&gt; &lt;p&gt;skinProb = exp(-dist * dist * skinHueThreshold) 結果範圍 [0, 1]，越接近 1 表示越可能是膚色。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;(3) 膚色區域保護&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;閾值分割：&lt;/p&gt; &lt;p&gt;設定閾值（如 skinProb &amp;gt; 0.95），二值化得到膚色掩膜（Mask）。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;動態衰減增強強度：&lt;/p&gt; &lt;p&gt;對檢測到的膚色區域，按 skinProb 權重衰減色彩增強效果。例如：enhanced_pixel = original_pixel * (1 - skinProb) + adjusted_pixel * skinProb * alphaalpha 為衰減係數（如 0.2），控制保護力度。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;增加膚色保護後，可以看到效果明顯更好，人臉不會有過於突兀的顏色變化。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-6986499c034b5ea751eaffd6e0ff1778e9d.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;左：增強（無保護）中：原圖 &amp;nbsp;右：增強（膚色保護）&lt;/p&gt; 
&lt;span id="OSC_h3_16"&gt;&lt;/span&gt; 
&lt;h3&gt;2.4.3 效果對比&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;HSV 空間的調節後色彩更加自然。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;RGB 空間調節則更加絢麗。但容易色偏。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;基於綜合考慮，我們採用 HSV 空間調節，以適應更多的源，避免色偏。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="OSC_h1_17"&gt;&lt;/span&gt; 
&lt;h1&gt;&lt;strong&gt;&lt;strong&gt;三. 總結與展望&lt;/strong&gt;&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;本研究聚焦於移動端視頻增強技術的工程化落地，重點驗證了亮度增強與色彩增強兩種核心算法的實際應用效果。從主觀評測效果看，在部分視頻上，兩項技術均能顯著提升視頻觀感質量，有效改善用戶體驗。&lt;/p&gt; 
&lt;p&gt;目前，亮度增強功能已在「好看 App」成功上線，且收穫了良好的應用效果。現階段，我們正着力研發亮度增強與色彩增強相疊加的綜合優化方案，計劃通過這一方案對更多視頻內容進行品質升級，從而為用戶帶來更優質的觀看體驗。以下為您呈現亮度增強結合色彩增強的部分應用案例：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-7b37220cf68793bbd80ae5e7ad807cc0e2f.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;例子 1：後層次感更好（右）&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-e3ba30fd9431f8cbf07932f3a0bb380f123.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;例子 2：色彩更鮮明（右）&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-c4cc30ba8a7c9cbe11075a6e6bac0dea018.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;例子 3：畫面更清晰明亮（右）&lt;/p&gt; 
&lt;p&gt;未來研究將圍繞以下方向展開：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;場景化優化：建立典型場景特徵庫，針對性優化算法參數配置。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;實時性提升：通過模型輕量化與硬件加速技術，更加快速的視頻實時處理。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/4939618/blog/18689451</link>
      <guid isPermaLink="false">https://my.oschina.net/u/4939618/blog/18689451</guid>
      <pubDate>Tue, 19 Aug 2025 03:20:00 GMT</pubDate>
      <author>原創</author>
    </item>
    <item>
      <title>英偉達發佈 Jetson AGX Thor 開發者套件： 基於 Blackwell 架構、專為物理 AI 和機器人打造</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;英偉達宣佈&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.nvidia.cn%2Fautonomous-machines%2Fembedded-systems%2Fjetson-thor%2F" target="_blank"&gt;NVIDIA Jetson AGX Thor™&lt;/a&gt;&amp;nbsp;開發者套件和量產級模組現已發售。這是一款功能強大的新一代機器人計算機，旨在為製造、物流、交通、醫療、農業和零售等行業的數百萬台機器人提供算力支持。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-2947794e4a78f4a5e0dfa4396d7c54809a3.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Jetson Thor 基於 NVIDIA Jetson™ 軟件平台，專為物理 AI 和人形機器人打造，支持所有主流 AI 框架與生成式 AI 模型。同時，它完全兼容 NVIDIA 從雲到邊緣的軟件棧，包括用於機器人仿真與開發的 NVIDIA Isaac、人形機器人基礎模型 Isaac GR00T、用於視覺 AI 的 NVIDIA Metropolis 以及用於實時傳感器處理的 NVIDIA Holoscan。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-35c925129f4a1c612c53f4f41decaa3e78b.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Jetson AGX Thor 核心信息：&amp;nbsp;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;套件搭載 &lt;strong&gt;Blackwell 架構 GPU&lt;/strong&gt;，峯值算力可達 &lt;strong&gt;2070 TFLOPS（FP4 稀疏運算）&lt;/strong&gt;，相較前代 Orin 提升 &lt;strong&gt;7.5 倍算力&lt;/strong&gt;、&lt;strong&gt;3.5 倍能效&lt;/strong&gt;。內置 &lt;strong&gt;128GB LPDDR5X 內存&lt;/strong&gt;，帶寬高達 &lt;strong&gt;273GB/s&lt;/strong&gt;。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;配備 14 核 Arm Neoverse-V3AE CPU、2560 CUDA 核心及 96 個 Tensor 核心，並支持 MIG 技術，可將 GPU 切分成多個隔離實例。功耗範圍 &lt;strong&gt;40–130W&lt;/strong&gt;，兼顧性能與能效。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;包含 Jetson T5000 模組、參考載板、主動散熱系統、電源適配器及豐富 I/O（如 QSFP28 100GbE、HDMI/DP、USB-C、1TB NVMe SSD、Wi-Fi 6E 等）。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;開發者套件售價 &lt;strong&gt;3499 美元&lt;/strong&gt;；量產模塊 &lt;strong&gt;T5000&lt;/strong&gt; 在千片級訂單中單價 &lt;strong&gt;2999 美元&lt;/strong&gt;。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;詳情查看&amp;nbsp;&lt;em&gt;https://blogs.nvidia.cn/blog/nvidia-blackwell-powered-jetson-thor-now-available-accelerating-the-age-of-general-robotics/&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368504</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368504</guid>
      <pubDate>Tue, 19 Aug 2025 03:18:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>亞馬遜 AGI 實驗室掌門人首度回應</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;去年，當亞馬遜以一種前所未有的方式招攬 AI 初創公司 Adept 的創始團隊時，整個行業都為之震動。這種被稱為"反向人才收購"的全新交易模式，讓大型科技公司無需完全收購初創企業，而是通過挖走核心團隊並獲得技術授權來達到目的。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;這場交易的核心人物 David Luan，從 Adept 聯合創始人兼 CEO 搖身一變，成為了亞馬遜全新 AGI 實驗室的掌舵人。如今，面對外界的質疑聲浪，Luan 終於打破沉默，在接受 The Verge 採訪時為自己的選擇進行了辯護。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="272" src="https://oscimg.oschina.net/oscnet/up-16d507b464b128d9ba6a42ba65af0fa1d69.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;當被問及這種"反向人才收購"趨勢時，Luan 的回應頗具深意。他坦言希望自己未來能夠"作為 AI 研究創新者而非交易結構創新者被人們銘記"。但從他的角度來看，像亞馬遜這樣的科技巨頭在當下"集中人才和計算資源的關鍵質量"是完全合理的戰略選擇。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;更加引人深思的是 Luan 對於離開自己創立公司的解釋。他毫不諱言地表示，自己並不願意將 Adept 打造成"一家只銷售小型模型的企業級公司"。在他眼中，還有"通向 AGI 的四個關鍵研究難題"亟待解決，而這正是他真正的使命所在。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;最令人震撼的是 Luan 對於資源需求的坦率描述。他直言不諱地指出，要解決這些核心問題，"每一個都需要價值數百億美元的計算集羣來運行"。面對如此天文數字般的資源需求，他反問道："除此之外，我還有什麼其他機會能夠實現這個目標呢?"&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368502</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368502</guid>
      <pubDate>Tue, 19 Aug 2025 03:09:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>谷歌 NotebookLM 升級：支持 80 種語言的視頻與音頻概述</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;谷歌&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.google%2Ftechnology%2Fgoogle-labs%2Fnotebook-lm-audio-video-overviews-more-languages-longer-content%2F"&gt;宣佈&lt;/a&gt;其 AI 研究助手 NotebookLM 迎來重大更新，其 「Video Overviews（視頻概述）」 功能現已支持 80 種語言（包括簡體中文），並同步升級了 Audio Overviews（音頻概述）。Video Overviews 最初於 7 月&lt;a href="https://www.oschina.net/news/362937" target="_blank"&gt;推出&lt;/a&gt;，此次更新後，全球用戶可用本地語言生成筆記本內容的視頻摘要。 &lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-11ee666c4dac7986a9fc9a46eb5a50015b2.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-ea7002e11e165dbd0fe526919e864976102.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;此次升級的核心亮點如下：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;視頻概述擴展至 80 種語言：NotebookLM 的 AI 講解視頻功能現已面向全球用戶，支持多達 80 種語言。用戶可將筆記內容自動生成帶有畫面和解説的視頻，更適合需要視覺化學習或分享的場景。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;音頻概述全面升級：原本僅提供簡短亮點的 「音頻概述」 如今擴展為完整版本，覆蓋與英文版同等質量的連貫講解，並同步支持多語言。這意味着無論用戶選擇何種語言，都能獲得深度的音頻總結體驗。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;內容更長，生成更靈活：Google 同時強化了 NotebookLM 的 Studio 面板。用戶現在可在一個筆記本中生成多個不同版本的音頻或視頻概述，以滿足不同受眾與學習目標的需求。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;從最初的 「播客式音頻總結」 到如今支持圖表、網頁、幻燈片甚至視頻講解，NotebookLM 正逐步演變為一個集多模態知識提煉、個性化講解於一體的 AI 學習與研究助手。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368497</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368497</guid>
      <pubDate>Tue, 19 Aug 2025 03:03:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>快手可靈&amp;港大提出 Context-as-Memory，上下文記憶力媲美 Genie3 且問世更早</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;香港大學和快手可靈團隊近日聯合&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FlsLYxySrtNN7QY3g7dVoZA" target="_blank"&gt;發表&lt;/a&gt;論文《Context as Memory: Scene-Consistent Interactive Long Video Generation with Memory Retrieval》，提出一種創新性方法：將歷史生成的上下文作為「記憶」（即 Context-as-Memory），通過 context learning 技術學習上下文條件，從而實現對長視頻前後場景一致性的有效控制。研究發現：視頻生成模型能夠隱式學習視頻數據中的 3D 先驗，無需顯式 3D 建模輔助，這一理念與 Genie 3 不謀而合。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;為了高效利用理論上可無限延長的歷史幀序列，論文還提出了基於相機軌跡視場（FOV）的記憶檢索機制（Memory Retrieval），從全部歷史幀中篩選出與當前生成視頻高度相關的幀作為記憶條件，大幅提升視頻生成的計算效率並降低訓練成本。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在數據構建上，研究團隊基於 Unreal Engine 5 收集了多樣化場景、帶有精確相機軌跡標註的長視頻，用於充分訓練和測試上述技術。用戶只需提供一張初始圖像，即可沿設定的相機軌跡自由探索生成的虛擬世界。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;根據介紹，Context as Memory 可以在幾十秒的時間尺度下保持原視頻中的靜態場景記憶力，並在不同場景有較好的泛化性。Context as Memory 方法旨在實現無需顯式三維建模的場景一致的長視頻生成。該方法的核心創新包括：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;提出了 Context as Memory 方法，強調將歷史生成的上下文作為記憶，無需顯式 3D 建模即可實現場景一致的長視頻生成。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;設計了 Memory Retrieval 方法，採用基於視場（FOV）重疊的相機軌跡規則進行動態檢索，顯著減少了需要學習的上下文數量，從而提高了模型訓練與推理效率。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;實驗結果表明，Context as Memory 在長視頻生成中的場景記憶力表現優越，顯著超越了現有的 SOTA 方法，並且能夠在未見過的開放域場景中保持記憶。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img height="193" src="https://oscimg.oschina.net/oscnet/up-a93eaecc14503bf0ea35d8a0966f2d4dbc6.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#3e3e3e; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;如上圖（a）所示，Context-as-Memory 的長視頻生成是通過基於 Context learning 的視頻自迴歸生成來實現的，其中，所有歷史生成的視頻幀作為 context，它們被視為記憶力的載體。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#3e3e3e; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;如上圖（b）所示，為了避免將所有歷史幀納入計算所帶來的過高計算開銷，提出了 Memory Retrieval 模塊。該模塊通過根據相機軌跡的視場（FOV）來判斷預測幀與歷史幀之間的重疊關係，從而動態篩選出與預測視頻最相關的歷史幀作為記憶條件。此方法顯著減少了需要學習的上下文數量，大幅提高了模型訓練和推理的效率。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#3e3e3e; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;在實驗中，研究者將 Context-as-Memory 與最先進的方法進行了比較，結果表明，Context-as-Memory 在長視頻生成的場景記憶力方面，相較於這些方法，表現出了顯著的性能提升。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#3e3e3e; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;img height="192" src="https://oscimg.oschina.net/oscnet/up-1dcc527f72fd57d136741cd74d4c8831830.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368496</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368496</guid>
      <pubDate>Tue, 19 Aug 2025 02:55:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>微軟分享有關開源 Windows 11 UI 的新細節</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;本月初， 微軟公佈了&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.neowin.net%2Fnews%2Fmicrosoft-is-taking-steps-to-open-sourcing-windows-11-user-interface-framework%2F" target="_blank"&gt;有關開源&lt;/a&gt;&amp;nbsp;Windows 11 用戶界面框架 WinUI 的細節。開源 WinUI 一直是開發者們的長期呼聲，但實現起來並非輕而易舉。由於 WinUI 在操作系統的專有層面「根深蒂固」，開源該框架需要謹慎且深思熟慮的方法。在首次發佈幾周後， 微軟準備分享更多關於 WinUI OSS 項目的信息。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-a574d1f8d2589c1f12d0d92889eb094019d.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;微軟希望分四個階段完成這項任務。第一階段主要是在 GitHub 上提供更多內部貢獻。第二階段將允許開發人員克隆存儲庫並在本地構建 WinUI。第三階段將允許第三方開發人員為該項目做出貢獻，最後階段將 GitHub 設為「開發、問題跟蹤和社區參與的主要場所」，並逐步淘汰內部存儲庫。&lt;/p&gt; 
&lt;p&gt;目前， &lt;strong&gt;微軟開發人員正忙於將 WinUI 從 Windows 中無法公開共享的專有部分中「解開」&lt;/strong&gt;。一旦 Windows App SDK 1.8 於本月晚些時候發佈（目前處於預覽階段，WinUI 與 WASDK 綁定），開發人員將開始在 GitHub 上實施拉取請求。 微軟計劃在 2025 年 10 月初完成第一階段。&lt;/p&gt; 
&lt;p&gt;至於允許第三方開發者克隆代碼庫並在本地構建，&amp;nbsp;微軟表示目前正在「積極探索」這個想法。這需要採取更加謹慎的態度，因此預計該公司還需要一些時間才能公佈更多細節。以下是該公司的聲明：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;由於 WinUI 的發佈計劃與 Windows App SDK 緊密相關，我們的分支策略現在允許我們在即將發佈的 WASDK 1.8 版本的同時開始完成 PR。1.8 預覽版已於 8 月 19 日發佈，穩定版也即將發佈，這為我們開始集成變更奠定了良好的基礎。基於此，我們計劃在 10 月初完成第一階段的工作。&lt;/p&gt; 
 &lt;p&gt;第二階段仍在積極探索中，雖然我們對此更加謹慎，但我們希望很快分享切實的進展。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fmicrosoft-ui-xaml%2Fdiscussions%2F10700" target="_blank"&gt;您可以在 GitHub 上&lt;/a&gt;關注有關 WinUI 開源的討論。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368494</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368494</guid>
      <pubDate>Tue, 19 Aug 2025 02:53:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>華為鴻蒙 HarmonyOS 5 終端設備數突破 1200 萬</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;鴻蒙生態發展再獲新進展。8 月 25 日，在智界及問界秋季新品發佈會上，華為技術有限公司（以下簡稱「華為」）常務董事、終端 BG 董事長餘承東宣佈搭載 HarmonyOS 5（以下簡稱「鴻蒙 5」）的終端設備數量突破 1200 萬台。&lt;/p&gt; 
&lt;p&gt;而這距離今年 7 月 30 日，餘承東透露鴻蒙 5 終端數量突破千萬台，僅不足一個月，再次創造了「鴻蒙速度」。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0826/104411_Nc6e_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;自鴻蒙 5 系統發佈以來，其終端設備數量呈現指數級增長。從 7 月 30 日突破 1000 萬台到 8 月 25 日已達 1200 萬台，鴻蒙 5 終端設備數量用了不到一個月時間新增 200 萬台，增速達 20%。&lt;/p&gt; 
&lt;p&gt;餘承東表示，鴻蒙生態的高速發展，是每一位開發者、夥伴和消費者的共同託舉，讓鴻蒙生態走向正循環。 &amp;nbsp;同時，鴻蒙生態向全場景滲透，覆蓋手機、平板、電腦、智能穿戴、智能家居及汽車等設備，其中華為 Mate 70 系列、Mate X6 系列等 50 餘款主力機型開啓 HarmonyOS 5.1 升級，進一步擴大用戶覆蓋面。 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;餘承東此前在接受採訪時透露，鴻蒙 5.0 設備已經超過 1000 萬，度過了鴻蒙生態的一個生死線。在生態這個領域，感謝中國整個科技產業的集體衝鋒，把不可能變成了可能。華為存量幾個億用戶的手機也會陸陸續續開放升級到鴻蒙 5.0，升級之後會更流暢，體驗更絲滑。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368493</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368493</guid>
      <pubDate>Tue, 19 Aug 2025 02:44:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>字節跳動內測全新 3D 模型生成工具 「3D Model Generator」</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;《&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FU5zMvqko4Ckz_YKa4WwQig" target="_blank"&gt;讀佳&lt;/a&gt;》消息稱，&lt;span style="background-color:#ffffff; color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;字節&lt;/span&gt;&lt;/span&gt;&lt;span style="background-color:#ffffff; color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;的&lt;/span&gt;&lt;/span&gt;&lt;span style="background-color:#ffffff; color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;豆包內部正在研發測試名為「3D Model Generator」的 3D 模型生成工具。該工具致力於可控大規模生成模型，為創建高質量 3D 資產提供有力支持，&lt;strong&gt;尤其在遊戲中的 3D 建模領域&lt;/strong&gt;。該工具或不久後對外開放使用。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="254" src="https://oscimg.oschina.net/oscnet/up-339b979e24b142fa3e8ef2b96f2731b7fef.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="background-color:#ffffff; color:rgba(0, 0, 0, 0.9)"&gt;從測試頁面可以看出，「3D Model Generator」支持兩種生成方式，一種是基於圖像生成，選取本地圖像文件，點擊「生成」，即可快速生成 3D 模型，降低了 3D 建模的入門門檻。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="228" src="https://oscimg.oschina.net/oscnet/up-8483f178e9e5f6f5cff323eb4fd921324db.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="background-color:#ffffff; color:rgba(0, 0, 0, 0.9)"&gt;另外一種是基於圖像+模型生成，通過圖像文件與模型文件的結合，實現更復雜或更具針對性的 3D 資產創作，這對於遊戲建模等需要大量 3D 素材的場景來説，具備實際應用價值。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="197" src="https://oscimg.oschina.net/oscnet/up-bc28c3c09e53aab094043b85aec04f160bd.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368492</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368492</guid>
      <pubDate>Tue, 19 Aug 2025 02:42:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>華為將發佈自研 AI SSD</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;華為將於 8 月 27 日發佈新品 AI SSD，目標直指 AI 存儲器市場。傳統 HBM 存在容量限制，而華為或將通過技術創新提供大容量 SSD。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="295" src="https://oscimg.oschina.net/oscnet/up-a05e595e422680f655edc3e44fc1a9c29bd.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;另據中國基金報記者報道，在當前的 AI 存儲器領域，HBM（高帶寬內存）佔據重要地位。HBM 是一種通過 3D 堆疊和超寬接口，實現極高數據傳輸帶寬的先進內存技術，通常直接封裝在 GPU（圖形處理器）卡中。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;問題在於，相比於之前的內存技術，HBM 犧牲容量換取極致帶寬和能效的策略，導致現有算力卡上 HBM 的容量比較有限。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;華為將發佈的全新 AI SSD 產品，可以有效滿足 AI 訓練推理過程中的超大容量和超強性能的需求，助力 AI 訓練推理、大模型部署。此舉將進一步強化華為存儲器在 AI 時代的競爭力，推動國產存儲生態發展。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;據悉，華為計劃與一體機廠商合作，改變現有局面，為 AI 存儲器市場注入新活力，帶來更多的可能性。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368488</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368488</guid>
      <pubDate>Tue, 19 Aug 2025 02:15:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
  </channel>
</rss>
