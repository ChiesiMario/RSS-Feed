<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - industry - 繁體中文（台灣）</title>
    <link>https://www.oschina.net/news/industry</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news/industry" rel="self" type="application/rss+xml"/>
    <description>已對該 RSS 進行格式化操作：中英字符之間插入空格、使用直角引號、標點符號修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-tw</language>
    <lastBuildDate>Fri, 05 Sep 2025 21:40:43 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>智譜推出「Claude API 用戶特別搬家計劃」</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;今日，美國頭部大模型公司&amp;nbsp;Anthropic&amp;nbsp;&lt;a href="https://www.oschina.net/news/370416" target="_blank"&gt;宣佈&lt;/a&gt;，將停止向多數股權由中國資本持有的集團出售 Claude 服務，範圍涵蓋中國大陸及通過海外註冊或雲服務間接使用的企業。&lt;/p&gt; 
&lt;p&gt;為幫助開發者平穩過渡，智譜正式推出「Claude API 用戶特別搬家計劃」。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;一鍵遷移，暢享 GLM-4.5&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;智譜已全面兼容 Claude 協議，用戶&lt;strong&gt;只需&lt;/strong&gt;&lt;strong&gt;&lt;strong&gt;替換 API URL&lt;/strong&gt;&lt;/strong&gt;，即可從 Claude 無縫切換至&amp;nbsp;&lt;strong&gt;&lt;strong&gt;GLM 模型 API&lt;/strong&gt;&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;智譜將為用戶提供：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;strong&gt;新用戶&lt;/strong&gt;&lt;/strong&gt;：贈送 2000 萬 Tokens 免費體驗；&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;開發者&lt;/strong&gt;：GLM-4.5 編碼專屬包月套餐，價格僅為 Claude&amp;nbsp;&lt;strong&gt;1/7&lt;/strong&gt;，用量提升&amp;nbsp;&lt;strong&gt;3 倍&lt;/strong&gt;、速度更快（平均 55 Tokens/s）；&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;遷移無憂&lt;/strong&gt;：從 Claude 到 GLM 的系統遷移教程，可便捷、快速地完成模型切換。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;為企業客戶額外提供：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;滿足業務需求的併發規模；&lt;/li&gt; 
 &lt;li&gt;更低成本的折扣優惠權益；&lt;/li&gt; 
 &lt;li&gt;1 對 1 的搬家顧問與解決方案服務。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;快速遷移教程&lt;/h2&gt; 
&lt;p&gt;如果你在使用 Claude API，訪問 bigmodel.cn，遷移到 GLM 非常簡單。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;替換你訪問的&amp;nbsp;base_url&amp;nbsp;為&amp;nbsp;https://open.bigmodel.cn/api/anthropic；&lt;/li&gt; 
 &lt;li&gt;在智譜開放平台申請&amp;nbsp;api_key；&lt;/li&gt; 
 &lt;li&gt;調用時使用智譜模型編碼即可。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;# 原來的 Claude 代碼
import anthropic
client = anthropic.Anthropic(
    base_url="your-base-url",
    api_key="your-api-key",
)
# 遷移到智譜 AI，只需要修改三個地方
client = anthropic.Anthropic(
    api_key="your-zhipuai-api-key",  # 替換為智譜 AI API Key
    base_url="https://open.bigmodel.cn/api/anthropic"  # 配置智譜 AI base_url
)
# 模型編碼使用，智譜 AI 模型，其他代碼保持不變
message = client.messages.create(
    model="glm-4.5",  # 使用智譜 AI 模型
    max_tokens=1024,
    messages=[{"role": "user", "content": "Hello!"}]
)&lt;/code&gt;&lt;/pre&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370531</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370531</guid>
      <pubDate>Wed, 03 Sep 2025 10:52:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>騰訊混元遊戲視覺生成平台正式發佈 2.0 版本</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;騰訊混元遊戲視覺生成平台正式發佈 2.0 版本，新增遊戲圖生視頻、自定義模型訓練、角色一鍵精修等能力，並大幅提升遊戲 2D 生圖模型能力，圖生視頻和文生圖模型在遊戲場景達到行業 SOTA 水平。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;此次升級進一步解決了遊戲美術設計與宣發中的動態內容生成、風格定製化、細節優化等痛點，幫助遊戲美術設計師提高效率。本次能力升級的同時，混元遊戲平台宣佈面向所有用戶開放，用戶可以通過騰訊混元官網體驗，登錄即可使用。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="274" src="https://oscimg.oschina.net/oscnet/up-881d8901997b0231d342668e831e17877df.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;新推出的遊戲 AI 動畫/CG 能力基於騰訊混元圖生視頻能力，可以讓靜態畫面秒變動畫，包括角色 360 度旋轉等遊戲。用戶上傳任意遊戲圖片並輸入動態描述，即刻生成高質量動態視頻，支持遊戲角色動作、場景特效及「萬物旋轉」展示，適用於遊戲 CG 預演、角色原畫三視圖創作、技能特效預覽，替代傳統逐幀繪製流程。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;自定義模型訓練大幅降低了生圖模型精調門檻，讓個人用戶也可以通過少量圖片微調自己專屬的 LoRA 模型，解決遊戲項目風格統一難題，尤其適合獨立工作室打造 IP 化美術資產。混元遊戲官網提供了預設風格，包括歐卡、二次元、寫實 CG 等，同時支持用戶使用個人數據集訓練專屬 LoRA 風格模型或者角色模型。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;自定義模型訓練能力基於混元生圖底模，簡化了 LoRA 模型的訓練流程，用戶只需上傳數十張圖片並設置觸發詞，系統自動打標，數小時即可完成模型訓練。整個訓練過程均為可視化操作，無需代碼基礎或複雜工具。該能力目前處於內測階段，用戶可以申請使用。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;角色一鍵精修能力主要用於對遊戲角色原畫的細節豐富或風格轉換，提供高一致性模式和高創意性模式。高一致性模式可保留原圖結構，精細化服飾紋理、光影層次，適用於角色定稿優化；高創意性模式支持將角色原畫轉換為國風、3D 化、二次元等風格並細化效果。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;混元遊戲 2.0 針對平台背後的 2D 生圖模型進行了升級，文生圖能力達到遊戲行業 SOTA 級別。混元遊戲大幅提升生圖模型的美學與構圖，使其更能滿足遊戲美術創作需求，同時針對遊戲獨有的場景進行優化，提供遊戲技能特效、環境特效與遊戲交互界面等生成能力，專項優化遊戲場景、遊戲道具物品、遊戲角色等生成效果。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370526</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370526</guid>
      <pubDate>Wed, 03 Sep 2025 10:22:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>聯想發佈全球首款垂直旋轉屏 AI PC</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;在德國柏林 IFA 期間舉行的 2025 聯想創新世界大會上，聯想帶來了諸多新品。其中最引人注意的就是一款 ThinkBook VertiFlex 概念機，同時這也是業界首款 14 英寸屏幕可垂直旋轉筆記本電腦。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="278" src="https://oscimg.oschina.net/oscnet/up-37aa8c07d9e00681e1351bdc757279581ab.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;這款概念機最大特點就是配備的旋轉顯示系統，可以在水平和垂直方向之間雙模式切換，採用 17.9 毫米和 1.39 公斤輕薄設計。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;垂直顯示模式適合分屏多任務、顯示代碼和查看文檔等場景，並且在垂直顯示模式下，智能手機可以通過聯想超級互聯連接到 PC 上，用於傳輸文件和手機鏡像。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="311" src="https://oscimg.oschina.net/oscnet/up-59757bf204952fc6b4dbc03f4a4839c6721.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;事實上，這並不是聯想首次在筆記本電腦屏幕上的創新，此前就曾發佈了透明屏、三摺疊屏，以及今年即將開賣的卷軸屏 PC。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370522</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370522</guid>
      <pubDate>Wed, 03 Sep 2025 10:01:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>全球首個「一站式」數智化生命科學研究平台 AI4S LAB 上線</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;北京大學深圳研究生院與百度智能雲近日聯合宣佈，雙方攜手打造的「一站式」數智化生命科學研究平台——AI4S LAB 正式上線。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;該平台深度整合算力、數據、模型、實驗四大要素，開發多智能體協同系統，為科研工作者帶來「AI 驅動、乾濕閉環、全鏈數智」的雲端科研體驗，極大提升科研效能與創新能力。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="285" src="https://oscimg.oschina.net/oscnet/up-4a6c75631c23cca67acff14e602a9274676.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;AI4S LAB 數智化支撐生態建設包括：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;算力：配備可伸縮的高性能計算集羣，搭載面向科學智能需求的超智融合算力調度系統。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;模型：基於百度智能雲千帆大模型平台開發私有化模型與數據管理能力，為 AI4S LAB 提供了 Agent 開發所需模型、Agent 編排、數據和定製化服務。匹配一站式模型效果調優工具鏈，為平台提供模型納管、精調與推理支持，尤其是生物領域大語言的場景化適配與調用。具有卓越的模型推理託管能力，在配備超 10 個可直接使用的通用與生命科學垂直領域代表性模型的同時，還支持各類主流推理框架和模型的自定義導入與部署，為科研工作者提供了高度靈活的開發環境。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;數據：配備超 15 個專業數據集，提供開放共享且持續產生新數據的知識平台，提供高效數據管理功能、智能可視化數據分析工具。實驗：集成超過 22 台套的先進高通量、自動化、自迭代智能實驗設備，面對生命合成領域，提供工程菌株構建與優化，蛋白表達與酶工程，代謝工程與調控，非天然氨基酸整合，合成噬菌體開發等多場景提供高效科研服務。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;北京大學深圳研究生院自主研發的 AI4S 原生多智能體系統——BIOMA，是平台全鏈路智能化實現的核心，提供高效協同的雲化研究能力，涵蓋了從理論預測、實驗設計、自動化執行到數據分析與迭代的各個環節，助力科研人員突破傳統研究的時空限制。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;BIOMA 多智能體系統的強大能力包括：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;逆向智能設計： 從期望的功能或性能指標出發，智能設計全新的實驗方案與材料。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;智能創制與表徵： 自動化地執行復雜的實驗流程，並對結果進行精確表徵。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;科研數據智能分析與迭代： 對海量實驗數據進行深度分析，並基於分析結果自主優化後續實驗方案。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="268" src="https://oscimg.oschina.net/oscnet/up-42c4dd9e2e3b6665fae4d5b9511c060a767.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;BIOMA 多智能體系統由一系列功能協同的智能體構成，每個智能體在科研流程中扮演着關鍵角色：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong style="color:#222222"&gt;理論科學家智能體（PredAgent）&lt;/strong&gt;&lt;strong style="color:#222222"&gt;，&lt;/strong&gt;在理論預測階段，解析科研人員以自然語言輸入的研究構想，並即時調用全球前沿的預測模型和工具進行模擬與計算。極大地提升了理論設計的效率，更通過算法優化增強了預測的準確性，為後續的實驗研究奠定堅實理論基礎。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;實驗規劃師智能體（ProAgent）&lt;/strong&gt;&lt;strong&gt;，&lt;/strong&gt;具備自主生成完整、可執行的標準化實驗方案的能力。在乾濕閉環&lt;strong&gt;驗證&lt;/strong&gt;&lt;strong&gt;階段&lt;/strong&gt;，通過與研究人員的多輪交互，精確提煉並完善濕實驗方案的每一個細節，包括試劑選擇、參數設定以及儀器調配等，從而構建出邏輯嚴謹、操作性強的實驗流程。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;智能實驗室自動化被《自然》期刊列為「2025 年值得關注的七大技術」之首。&lt;strong&gt;實驗室指揮官智能體（OperAgent）&lt;/strong&gt;&lt;strong&gt;，&lt;/strong&gt;負責在實驗執行階段，將 ProAgent 生成的複雜實驗方案轉化為機器可精確執行的指令。通過對實驗室超 22 台自動化設備的多線程精準調度與協同控制，成功打造 7x24 小時不間斷運行的「黑燈實驗室」，實現了真正意義上的無人化、自動化實驗流程。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;數據分析師智能體（ComAgent），&lt;/strong&gt;致力於構建一個一體化的科研數據生態系統，提供豐富的數據分析工具與可視化圖表，同時基於設定的關鍵性能指標，對實驗數據進行深度挖掘與洞察。通過自主分析，ComAgent 生成富有洞見的優化建議，並自動規劃下一輪的實驗方案，從而形成可持續進化的科研閉環，加速科學發現進程。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370509</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370509</guid>
      <pubDate>Wed, 03 Sep 2025 09:30:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Hopp - 開源結對編程應用程序</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                        
                                                                                    &lt;p style="text-align:start"&gt;&lt;span&gt;&lt;span&gt;&lt;span style="color:#1f2328"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style="background-color:#ffffff"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Hopp 是一款開源結對編程應用，可讓你與隊友結對編程。該應用使用 Tauri 構建，WebRTC 基礎架構由&lt;a href="https://livekit.io/"&gt;LiveKit&lt;/a&gt;提供支持。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p style="text-align:start"&gt;&lt;img height="132" src="https://static.oschina.net/uploads/space/2025/0903/144658_hgds_4252687.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

&lt;div style="text-align:start"&gt;
&lt;h4&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style="color:#1f2328"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style="background-color:#ffffff"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;特性&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/h4&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;超高品質屏幕共享&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://gethopp.app/blog/latency-exploration"&gt;優化了 WebRTC&lt;/a&gt;，以獲得最佳質量的屏幕共享&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.livekit.io/home/cloud/architecture/#distributed-mesh-architecture"&gt;依靠 LiveKit 網絡&lt;/a&gt;實現大規模低延遲&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong style="color:#1f2328"&gt;Mob&amp;nbsp;&lt;/strong&gt;&lt;strong&gt;編程&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;加入房間並立即與最多 10 名隊友配對&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;一鍵配對&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;不再在聊天中與隊友分享鏈接&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;開放式建造&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;希望與 OSS 社區共同打造 Hopp&lt;/li&gt;
&lt;li&gt;這帶來了自託管和社區創新的好處&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

                                                                    &lt;/div&gt;
                                                                </description>
      <link>https://www.oschina.net/p/hopp</link>
      <guid isPermaLink="false">https://www.oschina.net/p/hopp</guid>
      <pubDate>Wed, 03 Sep 2025 09:24:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenDataLab 與釘釘聯手推出面向企業用戶的文檔解析工具 DLU</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span&gt;OpenDataLab 和釘釘基於 MinerU &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FLg-4_0PNluM8l_pYQTjc7Q" target="_blank"&gt;推出&lt;/a&gt;了一款面向企業用戶的文檔解析工具——DLU&lt;/span&gt;&lt;span&gt;(Document Language Understanding)。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0905/172338_aEbD_2720166.gif" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;MinerU 是上海人工智能實驗室（上海 AI 實驗室）OpenDataLab 推出的智能文檔解析引擎，因精準解析能力及廣泛兼容性深受用戶青睞，在 GitHub 上已累計獲得超 4 萬星標。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0905/172239_vfIi_2720166.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;基於 MinerU 打造的 DLU 將於近期開源，其具備良好的文件格式兼容性，深層次的內容理解與精準的結構化輸出能力，不僅支持主流的 Office 文檔、PDF、Markdown 及代碼文件，還涵蓋釘釘自有的文檔、表格與 AI 表格格式；並支持提取純文本內容，精準解析圖表、公式、插圖乃至專業領域的化學分子式等複雜視覺元素，並將其有效轉換為適合大模型訓練的高質量語料。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370505</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370505</guid>
      <pubDate>Wed, 03 Sep 2025 09:23:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>拍我 AI 接入谷歌 Nano Banana，限時免費使用</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span&gt;拍我 AI 已正式接入谷歌 Nano Banana（Gemini 2.5 Flash Image），並同步開啓「拍我 AI 免費開放日」限時活動，從 9 月 5 日持續至 9 月 10 日，為期六天。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-31f43668eb292a7a399ef21c78092a4f3ae.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="font-family:-apple-system,BlinkMacSystemFont,&amp;quot;Apple Color Emoji&amp;quot;,&amp;quot;Segoe UI Emoji&amp;quot;,&amp;quot;Segoe UI Symbol&amp;quot;,&amp;quot;Segoe UI&amp;quot;,&amp;quot;PingFang SC&amp;quot;,&amp;quot;Hiragino Sans GB&amp;quot;,&amp;quot;Microsoft YaHei&amp;quot;,&amp;quot;Helvetica Neue&amp;quot;,Helvetica,Arial,sans-serif"&gt;在此期間，國內用戶可免費體驗 PixVerse Agent 創作助手，零成本生成各類創意短片。用戶只需選擇喜歡的模板並上傳一張圖片，Agent 即可自動識別圖像特徵，生成 5–30 秒的完整視頻。 &lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;作為國內首批接入 Nano Banana 的 AI 視頻生成平台，拍我 AI 國內版同步推出更多趣味模板，包括 3D 手辦製造局、和名人合照、捕獲心動角色、騎龍高手等，讓普通用戶也能輕鬆上手，製作精緻、有趣的短視頻作品，以及更進一步製作出自己喜愛的遊戲畫面！目前拍我 AI 網頁端和移動端 APP 均可同步體驗。&lt;/p&gt; 
&lt;p&gt;今年 6 月 6 日，PixVerse 上線了中國版本拍我 AI，目前平台的全球用戶規模已突破 1 億。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370503</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370503</guid>
      <pubDate>Wed, 03 Sep 2025 09:15:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>VC 投資人嘗試 20 天「氛圍編程」，稱成本高昂、易累積技術債</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Kevin Kuipers 是一名 VC 投資人，最近他&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fkevinkuipers.substack.com%2Fp%2Fvc-for-vibe-coding-a-fresh-new-start" target="_blank"&gt;分享&lt;/a&gt;了「Vibe Coding」的實踐經驗，通過 AI 編程構建面向 VC 的全新工作平台。&lt;/p&gt; 
&lt;p&gt;Kuipers 在暑假期間嘗試了沉浸式的「氛圍編程（Vibe Coding）」，目標是為自己的基金打造「AI-native」（AI 原生）管理系統。他先從 Telegram 智能體起步，逐步擴展到 Web 和桌面端應用，用 AI 自動整合文章、郵件、Pitch Deck、對話等信息，構建出一個「知識雲」，從非結構化數據中提煉趨勢與洞見。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0905/165630_7gXE_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在技術棧上，他大量依賴 Supabase、Orq.ai、Mem0、Koyeb、Linkup、ScrapingBee 等工具，並利用 Claude 直接生成 UI/UX，大幅減少了傳統 Figma 等設計流程。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0905/165641_hALP_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0905/165713_UDsC_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;他指出，AI 編程的優勢是能在幾周內完成原型，顯著提升迭代效率，但同時也伴隨挑戰：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;技術債容易累積，代碼質量難控；&lt;/li&gt; 
 &lt;li&gt;成本高昂，僅 20 天就消耗約 2600 美元 Token；&lt;/li&gt; 
 &lt;li&gt;LLM 的創造力和破壞力並存，需要工程師負責架構與質量。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Kuipers 強調，「vibe coding」核心不是寫代碼，而是「快速建造」，讓 VC 的知識管理和決策更高效。他認為，這種模式可能重塑風險投資的工作方式，也讓投資人與工程師保持更深度的互動。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370500/vc-for-vibe-coding-a-fresh-new-start</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370500/vc-for-vibe-coding-a-fresh-new-start</guid>
      <pubDate>Wed, 03 Sep 2025 08:57:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>「Nano Banana」上線不到 10 天，為谷歌 Gemini 吸引超過 1000 萬名新用戶</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;谷歌最新的 AI 實驗項目「Nano Banana」在上週爆火，谷歌實驗室副總裁 Josh Woodward 在 X 上&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Fjoshwoodward%2Fstatus%2F1963627742618165270" target="_blank"&gt;透露&lt;/a&gt;，自該功能上線以來，累計已完成超 2 億次圖像編輯，&lt;strong&gt;帶動超 1000 萬新用戶嘗試 Gemini 應用&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;對於這款產品的受歡迎程度，他形容稱導致「TPU 嚴重過載，SRE 警報不停。」&lt;/p&gt; 
&lt;p&gt;&lt;img height="854" src="https://static.oschina.net/uploads/space/2025/0905/164501_ZL7i_2720166.png" width="1280" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="background-color:#ffffff; color:#333333"&gt;Gemini 2.5 Flash Image&lt;/span&gt;（內部代號 Nano Banana）是谷歌&lt;span style="background-color:#ffffff; color:#333333"&gt;最先進的圖像生成與編輯模型，&lt;/span&gt;主要特點如下：&lt;/p&gt; 
&lt;ul style="list-style-type:disc; margin-left:0; margin-right:0"&gt; 
 &lt;li&gt;充分保持角色的一致性：它可以輕鬆地將同一個角色置於不同的環境中，或者從多個角度展示同一款產品，同時完美地保持其核心主體不變。&lt;/li&gt; 
 &lt;li&gt;基於提示的圖片編輯：允許用戶通過簡單的自然語言指令，對圖片進行精準的局部修改 。&lt;/li&gt; 
 &lt;li&gt;利用 Gemini 的現實世界知識：模型可藉助 Gemini 強大的世界知識庫，讓圖像生成變得更加「智能」。&lt;/li&gt; 
 &lt;li&gt;多幅圖像融合：可以將一張圖片中的物體「放」進另一張圖片的場景裏，整個過程只需一條提示指令就能完成。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;性能表現上，Gemini 2.5 Flash Image 在多項基準測試上均為第一名，超越 OpenAI ChatGPT 4o（GPT Image 1 high）、Qwen Image Edit 等模型。&lt;/p&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-bfee07407ab38e2b001fd0dbe895d36f242.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370497</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370497</guid>
      <pubDate>Wed, 03 Sep 2025 08:48:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>尤雨溪 VoidZero 公司 8 月成果速覽</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;尤雨溪 VoidZero 公司&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fvoidzero.dev%2Fposts%2Fwhats-new-aug-2025" target="_blank"&gt;發佈&lt;/a&gt;了 2025 年 8 月回顧，闡述了&amp;nbsp;Vite、Vitest、Oxc、Rolldown 的項目更新以及社區動態。&lt;/p&gt; 
&lt;p&gt;&lt;img height="250" src="https://oscimg.oschina.net/oscnet/up-bf866c877e66b526b8d1364ddab2a5ebfa3.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;具體包括：&lt;/p&gt; 
&lt;h4&gt;&lt;strong&gt;Oxlint：類型感知 linting 和自定義 JS 插件&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;Oxlint &lt;span style="color:#3d3d3d"&gt;旨在成為一款功能齊全、運行速度與原生速度一致的 Linting 替代品。本月發佈了兩項重大更新：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;類型感知 linting&lt;/strong&gt;：基於 TypeScript 的 Go 端口和 tsgolint，支持 40 個類型感知規則，如 no-floating-promises。性能保持高效，無需犧牲速度。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;自定義 JS 插件支持&lt;/strong&gt;：提供 ESLint 兼容 API，支持運行現有 ESLint 插件，而不犧牲性能。未來，幾乎所有 ESLint 插件都能無縫兼容 Oxlint。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;strong&gt;Vite&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Vite 現已通過&lt;code&gt;@vitejs/plugin-rsc&lt;/code&gt;引入 React Server Component 支持。目標是為每個基於 Vite 的 React 框架提供統一的解決方案。&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;@vitejs/plugin-react&lt;/code&gt;&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fvitejs%2Fvite-plugin-react%2Fblob%2Fmain%2Fpackages%2Fplugin-react%2FCHANGELOG.md%23500-beta0-2025-07-28" target="_blank"&gt;5.0 版本已發佈&lt;/a&gt;。當檢測到&lt;code&gt;rolldown-vite&lt;/code&gt;時，它會直接集成&lt;code&gt;@vitejs/plugin-react-oxc&lt;/code&gt;，因此不再需要額外安裝其他插件。&lt;/li&gt; 
 &lt;li&gt;Dev server 漏洞修復，修復源代碼泄露風險。詳情可&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgreen.sapphi.red%2Fblog%2Faddressing-source-code-leaks-across-the-ecosystem-a-retrospective" target="_blank"&gt;閲讀 Sapphi 的回顧博客文章&lt;/a&gt;。&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fvite-pwa%2Fvite-plugin-pwa%2Fpull%2F877" target="_blank"&gt;&lt;code&gt;vite-plugin-pwa&lt;/code&gt;（和其他 Vite 插件）&lt;/a&gt;的 Plugin Hooks 現已到位，使用&lt;code&gt;rolldown-vite&lt;/code&gt;時可顯著提升其運行速度&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4 style="margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span style="color:#000000"&gt;Vitest&lt;/span&gt;&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Vitest 在最新的 v4 測試版中&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbsky.app%2Fprofile%2Ferus.dev%2Fpost%2F3luzbsen2722x" target="_blank"&gt;支持可視化迴歸測試&lt;/a&gt;。&lt;/li&gt; 
 &lt;li&gt;v4 測試版通過平均縮短 Vitest 啓動時間 25%，進一步提升了測試速度。&lt;/li&gt; 
 &lt;li&gt;Vitest 的實驗性&amp;nbsp;programmatic API&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fvitest-dev%2Fvitest%2Fpull%2F8408" target="_blank"&gt;現在可以解析測試文件，&lt;/a&gt;而不是運行它們來收集測試數據。這對於第三方服務提供商尤其有用，並且有助於未來實現更快的過濾速度。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;strong&gt;Rolldown&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Rolldown-Vite&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fvitejs%2Frolldown-vite%2Fpull%2F168" target="_blank"&gt;開箱即用地支持原生插件&lt;/a&gt;。在原生標誌下進行改進，並解決所有生態系統 CI 問題後，第一組插件被認為足夠穩定，可以默認啓用，從而提升所有構建的速度，而無需任何配置。&lt;/li&gt; 
 &lt;li&gt;消除&amp;nbsp;Dead code elimination 和 treeshaking 優化是精簡 bundle 的關鍵。在最近的 Rolldown 版本中進行了&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Frolldown%2Frolldown%2Fpull%2F5826" target="_blank"&gt;多項&lt;/a&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Frolldown%2Frolldown%2Fpull%2F5829" target="_blank"&gt;改進&lt;/a&gt;，以進一步降低 bundle 大小。 
  &lt;ul&gt; 
   &lt;li&gt;新增&lt;code&gt;inlineConst&lt;/code&gt;&amp;nbsp;功能：在打包過程中內聯導入的常量值（而非引用它們）。由於減少了變量查找次數，此特性可縮小打包文件體積並提升運行時性能。自 1.0.0-beta.35 版本起，此優化將默認啓用。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Rolldown 現在有一個&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Frolldown.rs%2Freference%2Fconfig-options%23tsconfig" target="_blank"&gt;頂級&lt;code&gt;tsconfig&lt;/code&gt;選項&lt;/a&gt;。可以將其指向項目的 tsconfig 路徑，從而允許解析器遵循&lt;code&gt;compilerOptions.paths&lt;/code&gt;的別名設置，併為轉換配置建立默認值。此功能將取代先前引入的&lt;code&gt;resolve.tsconfigFilename&lt;/code&gt;選項。&lt;/li&gt; 
 &lt;li&gt;第一個案例研究已經發布：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fvoidzero.dev%2Fposts%2Fcase-study-plaid-rolldown" target="_blank"&gt;瞭解 PLAID Inc. 如何遷移到 Rolldown 並將其構建時間縮短 97%&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;strong&gt;Oxc&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Rolldown 團隊不僅致力於確保打包體積更小，Oxc 的壓縮工具現在&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Foxc-project%2Foxc%2Fpull%2F13026" target="_blank"&gt;也會多次運行 dead code 消除&lt;/a&gt;，類似於 Rollup。這可以進一步減小打包體積，同時只增加極小的開銷。&lt;/li&gt; 
 &lt;li&gt;如果你正在使用 React 和&lt;code&gt;styled-components&lt;/code&gt;，構建速度將顯著提升，因為 Oxc 現在將其大部分功能作為原生轉換支持。&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Frolldown%2Frolldown%2Fblob%2Fmain%2Fexamples%2Fstyled-components-native%2FREADME.md" target="_blank"&gt;如本例所示，&lt;/a&gt;它也可以在 Rolldown 中輕鬆啓用。&lt;/li&gt; 
 &lt;li&gt;提升性能&lt;code&gt;tsgolint&lt;/code&gt;。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;更多詳情可&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fvoidzero.dev%2Fposts%2Fwhats-new-aug-2025" target="_blank"&gt;查看官方公告&lt;/a&gt;。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370495/voidzero-whats-new-aug-2025</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370495/voidzero-whats-new-aug-2025</guid>
      <pubDate>Wed, 03 Sep 2025 08:45:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>這款插件讓你在開源圖像編輯器 GIMP 中體驗谷歌 Nano Banana</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;開發者 Josh Ellithorpe 近日&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fthoughts.greyh.at%2Fposts%2Fdream-prompter%2F" target="_blank"&gt;發佈&lt;/a&gt;&amp;nbsp;Dream Prompter 開源插件，將谷歌最新的 Gemini 2.5 Flash Image Preview 模型（代號 「Nano Banana」）引入 GIMP。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-57d084ce880042f5d91d09d2e5a1dc48003.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;該插件支持用戶在 GIMP 內直接通過文字提示生成新圖像，或對現有圖像進行自然語言編輯，無需切換到外部工具。使用 Dream Prompter 需綁定啓用計費的 Google Gemini API key，插件本身已開源並託管在 GitHub。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-7dee797f30e7632220f65ce17e0f9abb13d.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Ellithorpe 表示，他在 Claude 模型的幫助下快速完成了插件開發。這一集成讓 GIMP 用戶能夠在開源環境中享受與 Adobe 等商業軟件類似的 AI 創作體驗。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370492</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370492</guid>
      <pubDate>Wed, 03 Sep 2025 08:40:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>AI 生成優化 Metal 內核，PyTorch 推理速度提升 87%</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;根據 Gimlet Labs 的&lt;span&gt;最新&lt;/span&gt;研究，AI 能夠自動生成優化的 Metal 內核，使得 PyTorch 推理速度提升了 87%。這一突破性成果不僅提高了性能，還在測試的 215 個 PyTorch 模塊上實現了平均 1.87 倍的加速，某些工作負載的速度甚至提高了數百倍。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="236" src="https://oscimg.oschina.net/oscnet/up-bffbe7fb79340a185f9f81437c72a069abe.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;研究人員選取了來自多個&lt;span&gt;頂尖&lt;/span&gt;機構的八個 AI 模型，包括 Anthropic、DeepSeek 和 OpenAI，利用這些模型為蘋果設備生成優化的 GPU 內核。這一過程無需修改用戶代碼或使用新的框架，直接在蘋果硬件上提升模型性能。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在實驗中，研究團隊選擇了 Mac Studio （搭載 Apple M4Max 芯片） 進行測試，基準設置為 PyTorch 的 eager 模式。實驗採用了 KernelBench 數據集中的 215 個 PyTorch 模塊，這些模塊被分為三類，涵蓋從簡單的矩陣乘法到完整的模型架構。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;測試過程包括接收輸入和 PyTorch 代碼，生成 Metal 內核，並評估其正確性。數據顯示，隨着嘗試次數的增加，AI 生成內核的正確性逐步提升。例如，在第五次嘗試時，正確實現的比例達到了 94%。此外，模型們在生成內核時表現出了跨層級的能力，儘管非推理模型有時也能生成有效內核。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;實驗結果表明，GPT-5 模型在某些任務上實現了 4.65 倍的速度提升。更令人驚訝的是，o3 模型在某些情況下甚至將延遲降低了 9000 倍。研究還發現，單一模型在某些任務上並不總是表現&lt;span&gt;最好&lt;/span&gt;，多個模型的結合能夠生成更優的內核。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;為了進一步提升性能，研究者嘗試引入額外上下文信息，如 CUDA 實現和 gputrace 的性能分析數據，結果顯示這種方法在性能加速方面達到了平均 1.87 倍，相比於普通智能體的 1.31 倍提升了三倍。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;需要注意的是，研究人員強調，這一工作並不是為了展示最終的性能極限，而是為了驗證 AI 在內核生成中的可行性，希望通過自動化減少開發人員的負擔。整體而言，這項研究標誌着 AI 技術在硬件優化領域的一個重要進展。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370488</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370488</guid>
      <pubDate>Wed, 03 Sep 2025 08:27:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>微軟積極推動 Rust 在 Windows 驅動開發中的應用</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;微軟正在積極&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftechcommunity.microsoft.com%2Fblog%2Fwindowsdriverdev%2Ftowards-rust-in-windows-drivers%2F4449718" target="_blank"&gt;推動&lt;/a&gt; Rust 語言在 Windows 驅動開發中的應用。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0905/162214_t114_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;最新進展顯示，開發者已可通過多個 Rust crate 在 Windows 11 上編寫 WDM、KMDF 和 UMDF 驅動，並藉助 &lt;code&gt;cargo-wdk&lt;/code&gt; 工具快速生成模板。然而，目前驅動仍需依賴大量 &lt;code&gt;unsafe&lt;/code&gt; 代碼與操作系統交互，Rust 的安全優勢尚未完全發揮。&lt;/p&gt; 
&lt;p&gt;微軟內部團隊正開發「安全 Rust 抽象層」，以減少 &lt;code&gt;unsafe&lt;/code&gt; 的使用，並計劃讓工具鏈支持 ARM64 架構、自動依賴安裝及遠程部署測試。Surface 團隊也已貢獻了基於 Rust 的驅動代碼，推動生態完善。&lt;/p&gt; 
&lt;p&gt;不過，相關工具鏈和流程仍處早期階段。微軟明確指出，Rust 驅動暫不適合生產環境，提交 Windows 硬件兼容性計劃（WHCP）認證的流程也尚未成熟。同時，儘管 GitHub 的 CodeQL 已支持 Rust，但 WHCP 仍未正式認可最新版。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370484</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370484</guid>
      <pubDate>Wed, 03 Sep 2025 08:23:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>谷歌加碼推銷自研芯片：積極接觸 NVIDIA 芯片雲服務供應商</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span&gt;Google 近期與一批主要採購英偉達芯片的小型雲服務供應商接洽，商討在其數據中心中同時部署谷歌自研芯片。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;據知情人士透露，談判已取得初步進展：已與總部位於倫敦的 Fluidstack 達成協議，將在後者位於紐約的數據中心託管 Google&amp;nbsp;TPU。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-298295c91cb27c2c551ec24f520733eb813.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;strong&gt;除 Fluidstack 外，Google 還接觸了包括被稱為「英偉達親兒子」的 CoreWeave 在內的其他雲服務商。&lt;/strong&gt;這一系列舉動顯示，Google 正試圖拉近與那些「受英偉達扶持」的新興雲服務企業之間的關係。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;與 Google、亞馬遜等大型雲廠商不同，這類新興企業幾乎完全依賴英偉達芯片，並更積極採購多種英偉達產品。英偉達不僅向其中多家公司投資，還優先為它們供應當前最緊俏的芯片。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;Google 此次與 Fluidstack 達成合作的方式尤為直接：若 Fluidstack 無法承擔紐約數據中心的建設成本，Google 願意以「後備擔保」身份介入，並提供最高 32 億美元的資金支持。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;目前尚不清楚 Google 為何積極向外推廣其自研芯片。此前，Google 幾乎從不對外出售自研 TPU。據 Capvision 數據，GoogleTPU 70%–80% 的算力用於內部業務，其餘 20%–30% 以自建租賃方式對外開放。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;有分析認為，這一策略可能源於 Google 自建數據中心的進度難以匹配其芯片需求的快速增長，又或者是希望通過第三方雲服務商為 TPU 爭取更多客戶。若屬後者，則意味着 Google 正更直接地與英偉達展開競爭。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;分析進一步指出，Google 若擴大以租賃模式部署 TPU，將類似於當前雲服務商採用英偉達 GPU 的做法。而隨着 GoogleTPU 在這些數據中心中滲透率提升，相應場景中對英偉達 GPU 的採購需求或將減少。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370480</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370480</guid>
      <pubDate>Wed, 03 Sep 2025 08:04:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>谷歌壟斷案獲「階段性勝利」，得感謝 OpenAI</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;美國華盛頓特區地方法院近日就谷歌反壟斷案作出裁決，&lt;strong&gt;拒絕司法部提出的 「強制拆分 Chrome 瀏覽器」 等激進措施&lt;/strong&gt;。這一結果被視為谷歌的 「階段性勝利」，而幕後推手竟是 OpenAI 等生成式 AI 公司。&lt;/p&gt; 
&lt;p&gt;主審法官 Amit Mehta 在判決書中指出，&lt;strong&gt;ChatGPT 等 AI 工具正成為傳統搜索引擎的替代品，對谷歌構成 「新生的競爭威脅」&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;OpenAI 高管此前出庭作證，強調若能獲取谷歌搜索數據，將優化 ChatGPT 搜索體驗，進一步強化了 「AI 挑戰搜索霸權」 的敍事。&lt;/p&gt; 
&lt;p&gt;法院認為，在 AI 技術快速演進的背景下，傳統搜索引擎的壟斷地位已不再牢不可破，因此無需進行結構性拆分。這一裁決不僅為谷歌贏得喘息空間，也為科技巨頭在反壟斷訴訟中提供了新的辯護模板 ——「AI 威脅論」 或成未來關鍵論據。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-df103573bde542adfd1cea79dc07f1e2be9.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;儘管免於 「拆分」，谷歌也絕非毫髮無損。Mehta 法官在判決中對谷歌的行為施加了一系列限制性措施，旨在削弱其在搜索市場上的壟斷地位，為競爭對手創造喘息空間。&lt;/p&gt; 
&lt;p&gt;具體而言，法院禁止谷歌與其合作伙伴簽訂排他性的搜索引擎分銷協議，並要求谷歌向競爭對手分享部分搜索數據，以幫助它們改進自身產品。此外，判決還禁止谷歌將在 Android 系統上授權其應用商店（Google Play Store）與必須預裝其他谷歌應用（如搜索、Chrome 或 Gemini）的做法捆綁在一起。&lt;/p&gt; 
&lt;p&gt;總的來説，谷歌能 「躲過一劫」，不是因為它沒壟斷，而是因為 OpenAI 的 AI 浪潮讓法院相信：搜索市場的遊戲規則，正在被重寫。&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;相關閲讀&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/news/369982" target="news"&gt;美國法院裁定谷歌無需出售 Chrome 瀏覽器&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/news/366896" target="news"&gt;多家企業盯上谷歌 Chrome，奧特曼表態 OpenAI 或考慮收購&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/news/346989/google-chrome-suffer-if-forced-to-sell-parisa-tabriz" target="news"&gt;谷歌認為自己是唯一能運營 Chrome 的公司，如若轉手，將「萬劫不復」&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370477</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370477</guid>
      <pubDate>Wed, 03 Sep 2025 07:48:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>系統梳理 Test-Time Compute 的主要實現路徑</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;編者按：&lt;/strong&gt; AI 真的在"思考"嗎？當模型面對數學推理、代碼生成或複雜決策時，它是如何一步步推演出答案的？如果你曾困惑於大模型在關鍵任務中表現不穩定、缺乏可解釋性，甚至生成結果難以驗證，那麼你並不孤單。這些痛點不僅影響研發效率，更直接制約了 AI 在高風險場景中的落地可靠性。&lt;/p&gt; 
 &lt;p&gt;本文系統梳理了測試時計算（test-time compute）的三大實現路徑：N 選 1 採樣、多數投票及相關方法、思維鏈（Chain-of-Thought）自我推理，到融合搜索算法與世界模型的結構化推理系統，還深入探討了驗證器設計、獎勵機制、隱空間推理與智能體行為優化等關鍵挑戰。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;作者 | Davis Treybig&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;編譯 | 嶽揚&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;當前大語言模型（LLM）最有趣的研究趨勢之一，是推理模型的興起 ------ 這類模型在給出答案前會花費時間進行思考。&lt;/p&gt; 
&lt;p&gt;這種技術通常被稱為「測試時計算」（test-time compute），即在推理階段進行深度推理。其實在模型推理過程中應用搜索或深度推理的思路早已存在（例如 AlphaZero[1]，以及 Transformer 誕生之前就嘗試用類似方法解決旅行商問題的論文[2]），但 o1 的出現讓這一理念重新回到了主流視野。&lt;/p&gt; 
&lt;p&gt;最令人興奮的是，這種測試時計算可能展現出與預訓練相似的擴展規律 ------ 換言之，&lt;strong&gt;就像增加訓練計算量能帶來模型能力的指數級提升一樣，若在推理階段分配更多計算資源（延長思考時間），模型性能理論上也會出現可預測的指數級增長。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-2cd5f7b45300d59d739523bdcd8b93f024e.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;OpenAI 發佈的關於 o1 模型測試時計算擴展效果的圖示表明：模型的準確率相對於對數尺度的計算量呈現可預測的增長，表明存在指數關係&lt;/p&gt; 
&lt;p&gt;但像 o1 這類模型背後的實現原理究竟是什麼？測試時計算擴展（test-time compute scaling）又有哪些不同的實現機制與技術路徑？目前我尚未找到關於此技術直觀系統的綜述，而 OpenAI 對其技術細節守口如瓶，因此本文將嘗試構建一個解讀框架。&lt;/p&gt; 
&lt;p&gt;本篇博客將結合近期的大量文獻研究以及與多家實驗室機器學習研究者的交流，系統梳理實現測試時計算擴展的主要技術路徑。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;01 測試時計算的基本實現機制&lt;/strong&gt;&lt;/h1&gt; 
&lt;h2&gt;&lt;strong&gt;1.1 N 選 1 採樣、多數投票（majority voting）及相關方法&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;其核心思想是讓語言模型在推理階段生成多個可能的輸出，然後通過採樣、投票或其他評估/驗證器方法來選出最佳答案。&lt;/strong&gt; 這種方法無需改變模型的訓練方式，但確實能作為一個有效的基線方案。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-8257e1d517a101818c839f17c356858539a.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Large Language Monkeys[3]&lt;/p&gt; 
&lt;p&gt;其中的第一點細微差異在於驗證器的設計。多數投票（majority voting）等簡單方法通用性雖強但效果有限。代碼、數學等特定領域可採用專用的驗證器（如代碼的單元測試與編譯器、數學的符號計算引擎），但缺乏普適性。目前的主流趨勢是通過微調大語言模型構建專用驗證器（參見此案例[4]）。&lt;/p&gt; 
&lt;p&gt;另一個問題在於，對於許多更復雜的問題，傳統的採樣方法可能永遠無法生成正確答案（或者需要耗費大量計算資源才能以足夠高的概率生成正確答案）。後續我們將看到，解決這一問題的正確方法要麼基於優質推理軌跡進行訓練，要麼通過獎勵機制引導模型完成複雜推理。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;1.2 思維鏈（Chain of thought）&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;第二種方法是讓語言模型生成極其詳細的長鏈思維推理軌跡，以此提升推理能力。&lt;/strong&gt; 這種方式本質上是單一模型通過自迴歸方式產生大量 token 的自我對話過程 ------ 並不依賴外部系統或控制流程。OpenAI 在其 o1 公告中展示了此類案例[5]。&lt;/p&gt; 
&lt;p&gt;雖然基礎版本可通過提示詞工程實現（例如"逐步思考"），但其進階版本需要專門的預訓練與後訓練技術，以優化這類長鏈推理軌跡的生成效果。&lt;/p&gt; 
&lt;p&gt;這裏的關鍵差異在於模型如何通過訓練提升長鏈推理能力。大致有以下實現路徑：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;1）監督學習（Supervised learning）&lt;/strong&gt; ------ 理論上可通過大量人工撰寫的長鏈思維樣本進行訓練。但實踐中難以獲取足夠的高質量數據：公開領域的高水平長篇幅推理樣本稀缺，且人工製作成本極高。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2）合成推理軌跡（Synthetic reasoning traces）&lt;/strong&gt; ------ 在特定問題領域，可通過程序化方法生成複雜的推理軌跡。例如這項研究[6]利用知識圖譜生成保證正確性的問題/推理/答案三元組。在數學和計算機科學領域，還可使用形式化系統（如符號計算引擎、Lean 語言[7]、編譯器與構建系統）產生合成推理鏈，作為模型的訓練樣本。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;3）採樣&amp;amp;驗證&lt;/strong&gt; ------ 要求大語言模型生成多個推理輸出，通過驗證機制或獎勵模型區分優劣推理鏈，進而構建用於後訓練的強化學習數據集。核心區別在於使用結果獎勵模型（ORM，驗證最終輸出的正確性）還是過程獎勵模型[8]（PRM，對局部推理鏈進行獎勵評估）。該領域存在非常多的方法：包括採樣生成方式、驗證器的訓練或設計、以及整合獎勵信號的強化學習系統架構等。&lt;/p&gt; 
&lt;p&gt;此處的考量在於：如何在 a. 數據規模 b. 計算可行性 c. 人力成本這三個維度實現高效擴展？OpenAI 強調其 o1 技術具備"數據高效特性（data-efficient）"，暗示其很可能深度融合了合成數據與基於強化學習的驗證技術，而非某種依賴人工標註的推理數據集。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;合成數據技術雖有效，但通常侷限於特定領域和更易量化的問題類型，因此其泛化能力仍存疑。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;採樣技術面臨的挑戰在於，許多複雜問題的推理搜索空間過大，既無法進行窮舉生成，也難以高效驗證。&lt;/strong&gt; 這與機器人等強化學習領域面臨的問題相似 ------ 需要巧妙地模擬或"搜索"結果空間，並設計獎勵函數。&lt;/p&gt; 
&lt;p&gt;這正是過程獎勵模型（PRM）的價值核心 ------ 它能提前終止錯誤的推理路徑，聚焦於成功概率較高的中間狀態進行分支（相關論述參見該論文[9]第 3.3 節）。&lt;/p&gt; 
&lt;p&gt;關於如何構建推理軌跡結構以提升訓練效果，當前存在大量前沿探索：Dualformer[10] 在訓練過程中有選擇性地遮蔽部分推理軌跡，旨在讓模型習得類似人類"系統 1"的心理啓發式思維；Stream of Search[11] 研究則發現包含錯誤回溯、自我修正的"不完美"推理軌跡，相比完美的線性推理更具訓練價值；另有論文[12]證實帶回溯糾錯的錯誤推理鏈對訓練有益；Beyond A[13] 甚至通過 A* 等經典搜索算法構建訓練樣本，來教導模型如何進行搜索。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;1.3 推理時搜索（及其他輔助系統）&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;實現推理階段計算擴展的第三大路徑，是在推理過程中實際採用某種搜索技術。&lt;/strong&gt; 這意味着推理不再僅僅是模型推理問題，更演變為系統工程問題 ------ 需要引入某種控制流或流程編排機制，而非單純依賴單一模型的詞元輸出。&lt;/p&gt; 
&lt;p&gt;一些有趣的例子表明，這種範式不僅限於"標準"的大語言模型。例如，AlphaZero[14] 通過訓練後的神經網絡指導蒙特卡洛樹搜索算法選擇最佳落子位置；AlphaProof[15] 則結合預訓練大語言模型與強化學習算法生成候選解決方案，再通過 Lean 證明輔助語言（proof assistant language）進行驗證。&lt;/p&gt; 
&lt;p&gt;當前 LLM 研究中，最常見的實現形式是在推理階段集成某種"搜索+驗證"技術：模型首先生成 N 個候選的推理步驟，經驗證器或獎勵模型評分篩選後，然後在最優候選子集中重複此過程。值得注意的是，前文討論的"N 選 1 採樣"方法可視為該體系的子集。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-58d7426c8fc0e5267ace67aa3ad9ece203b.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;HuggingFace 關於通過搜索+過程獎勵模型實現測試時計算的綜述&lt;/p&gt; 
&lt;p&gt;該領域的優秀研究案例包括：Tree of Thoughts[16]、Self-Evaluation Guided Beam Search for Reasoning[17] 以及 Reasoning with Language Model is Planning with World Model[18]。這些方法均融合了搜索技術（廣度優先搜索、深度優先搜索、波束搜索、蒙特卡洛樹搜索）與驗證機制來引導語言模型推理生成。LLM Reasoners[19] 論文中的可視化呈現（如下圖所示）直觀展示了這些技術的運作方式。這些方法在覈心思路上高度一致。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-f0095e8a0625a7ce2c03476b6113eba37dc.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;值得注意的是，這種"搜索技術+驗證器+生成模型"的組合範式與前文所述的思維鏈技術幾乎同構 ------ 唯一區別在於這些技術是離線應用於生成後訓練強化學習數據集，還是在推理時在線應用。&lt;strong&gt;但兩種方式都實現了測試時計算擴展：前者通過訓練使模型在測試時進行更長時間的推理，而後者則在推理過程中引導模型生成更大量的輸出。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;除搜索算法外，還可集成其他類型的輔助系統來增強生成模型。RAP 論文[18]便是一個典型範例：研究者使用一個輔助 LLM 作為"世界模型"來追蹤環境狀態。換句話説，當生成式 LLM 持續輸出回溯、思考、權衡等推理動作時，世界模型會同步跟蹤每個動作執行後的"世界狀態"。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-6f2ffd7e8607c9b8103c6e8245f58d62e57.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;標準思維鏈動作序列與世界模型方法的可視化對比（後者在每個動作後均保留了"世界狀態"）&lt;/p&gt; 
&lt;p&gt;從理論上講，這種方式讓模型能更輕鬆地推斷後續動作產生的影響。相較於單一思維鏈的輸出，模型必須隱式回放整個動作序列才能理解當前世界狀態。&lt;/p&gt; 
&lt;p&gt;上文提到的推理研究論文[19]提出了一個有趣的形式化框架，試圖將多數投票、思維鏈、搜索技術等不同方法統一到同一個理論體系中。&lt;/p&gt; 
&lt;p&gt;研究者認為這些技術本質上都是以下三要素的組合：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;1）用於確定不同推理步驟優先級的獎勵函數&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2）用於定義推理狀態轉換的世界模型&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;3）用於探索廣闊推理空間的搜索算法&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在此框架下，標準的思維鏈推理的獎勵函數等同於默認模型似然輸出，其世界模型僅簡單地將推理動作持續追加到完整動作歷史中，並採用始終對輸出概率分佈進行單次採樣的"貪婪"搜索算法。&lt;/p&gt; 
&lt;p&gt;筆者認為這種分析視角頗具啓發性。該論文還通過基準測試發現：搜索技術持續優於思維鏈推理，而 RAP（世界模型+搜索技術）則始終超越純搜索方法。&lt;/p&gt; 
&lt;p&gt;斯坦福大學近期對推理模型的元綜述（meta overview）[20]也描述了類似的思維框架 ------ 認為這些方法大多都是"生成器、驗證器和搜索組件的集成"，這本質上是相同的框架。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;02 其他考量因素&lt;/strong&gt;&lt;/h1&gt; 
&lt;h2&gt;&lt;strong&gt;2.1 驗證器機制&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;如我們所見，這些技術的效果很大程度上取決於驗證器的質量及其驗證能力。啓發式/自動驗證器（Heuristic/automatic verifiers）雖有效但天然具有領域侷限性（例如，編程題目中的測試用例）。學習型驗證器（Learned verifiers）雖可行，但需要特定領域的高質量訓練數據 ------ 可參考 OpenAI 這篇早期的論文[21]，他們訓練了用於數學問題的學習型驗證器。直接使用 LLM 用作驗證器雖已取得顯著進展，但該方法的可行性仍存在一定侷限。基於過程的驗證器（Process based verifiers）非常重要，但其實現難度遠高於基於結果的驗證器（outcome based verifiers）。&lt;/p&gt; 
&lt;p&gt;MuZero[22] 為此領域的發展提供了一個重要參照 ------ 這個無模型的強化學習系統能掌握多種複雜遊戲並達到頂尖水平。"無模型（Model-free）"意味着其強化學習算法中並未編碼任何特定遊戲規則。&lt;/p&gt; 
&lt;p&gt;這種領域無關的驗證器設計似乎對模型在推理能力上實現普遍提升非常重要。當然，關鍵問題在於，相較於圍棋、國際象棋、將棋和 Atari 遊戲等獎勵函數明確的領域，如何在獎勵機制更模糊的領域實現類似效果仍待探索。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;2.2 泛化能力存疑&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;這篇精彩的博文深入探討了將強化學習應用於推理領域的挑戰[23]，特別是在 OpenAI 的 o1 模型這個具體背景下來討論這個問題。&lt;strong&gt;o1 採用強化學習技術，而強化學習在獎勵信號清晰且頻繁的領域效果最佳，但現實是大多數領域缺乏這種明確的獎勵機制。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;......&lt;/p&gt; 
&lt;p&gt;OpenAI 承認 o1 是在易於驗證的領域進行訓練的，但希望其推理能力能泛化到所有領域。這種跨領域的泛化能力能否實現，是一個價值萬億美元的問題。我先直截了當地説出我的觀點：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;⚠️ o1 風格的推理模型無法實現超越訓練領域的有效泛化&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;從實際案例來看，當前多數測試時計算模型在特定問題領域（如數學、邏輯、計算機科學）表現突出，但在其他領域並未展現明顯優勢。許多體驗過這類模型的研究者反饋，它們在傳統生成任務上的表現反而明顯下降。基於強化學習的推理技術能否有效泛化到驗證難度更高的領域，仍是一個值得探索的開放性問題。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;2.3 詞元空間與隱空間中的推理&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;與上述所有方法形成有趣對照的是：詞元空間究竟是否為模型推理的最優方式？現有研究開始探索讓模型直接在隱空間[24]中推理 ------ 即在推理過程中將隱藏狀態反饋給模型，而非解碼後的詞元。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-e997cb1967f1e2ad035093d95b7c697c221.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;從理論上講，隱空間推理可能更具優勢，因為隱藏狀態（hidden state）代表了下一詞元生成的概率分佈，而詞元本質上是該分佈的"採樣樣本"。相較於僅選擇一個狀態，在所有可能狀態下進行推理更接近人類的推理模式，可能有提升效果。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;這種方法的潛在缺陷是，此類模型不會向用戶"展示推理過程"。但考慮到 OpenAI 等公司已經開始隱藏推理步驟，這個缺點或許無關緊要。理論上仍可可視化詞元輸出而同時在隱空間推理，但這可能導致用戶所見與模型實際推理過程出現偏差。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;2.4 智能體推理機制&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;我特別關注這些技術如何映射到智能體領域。優化模型的多步驟複雜推理軌跡，與優化智能體的多步驟推理軌跡存在高度相似性 ------ 唯一區別在於智能體的子步驟被拆分為不同的模型調用，且通常涉及更多動態組件（如函數調用等）。&lt;/p&gt; 
&lt;p&gt;觀察到許多領先的智能體創業公司（如 Cognition、Basis 等）都將這些理念融入其智能體設計。例如，多家智能體公司會採集智能體的運行軌跡，通過搜索技術+獎勵模型進行回放來推演反事實推理路徑（counterfactual reasoning paths），並將這些反事實軌跡（counterfactual trajectories）作為微調樣本用於提升智能體系統性能。&lt;/p&gt; 
&lt;p&gt;對於需要 50-100+ 次鏈式 LLM 調用來在複雜工具環境中完成任務的智能體而言，這種方法至關重要 ------ 因為單次請求下智能體可執行的動作組合複雜度極高。&lt;/p&gt; 
&lt;p&gt;特別值得關注的是，相較於在模型層通用地解決多步推理問題，設計針對特定領域的搜索算法和過程獎勵模型顯然更具可行性。&lt;/p&gt; 
&lt;p&gt;這恰好印證了前文提及的那篇博客文章的觀點：這些技術可能難以實現泛化。&lt;strong&gt;複雜推理的強化學習技術在模型提供商層面或許難以泛化，反而會成為垂直領域智能體創業公司的核心護城河&lt;/strong&gt; ------ 尤其是在需要高度複雜推理的領域（如會計、稅務、金融、建築等）。&lt;/p&gt; 
&lt;p&gt;預計未來將出現專門支持此類任務的開發工具（類似微調領域的 MosaicML 生態），幫助智能體創業公司更便捷地構建"搜索技術+驗證"層，併為特定應用場景生成訓練數據集。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;END&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;本期互動內容 🍻&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;❓文章指出，測試時計算的泛化能力是"價值萬億美元的問題"。您認為，基於強化學習的推理技術能否有效泛化到數學、代碼之外，缺乏明確獎勵信號的"模糊"領域（如創意寫作、戰略規劃）？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;文中鏈接&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;[1]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeepmind.google%2Fdiscover%2Fblog%2Falphazero-shedding-new-light-on-chess-shogi-and-go%2F" target="_blank"&gt;https://deepmind.google/discover/blog/alphazero-shedding-new-light-on-chess-shogi-and-go/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[2]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F1611.09940" target="_blank"&gt;https://arxiv.org/abs/1611.09940&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[3]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2407.21787" target="_blank"&gt;https://arxiv.org/pdf/2407.21787&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[4]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2408.15240" target="_blank"&gt;https://arxiv.org/pdf/2408.15240&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[5]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopenai.com%2Findex%2Flearning-to-reason-with-llms%2F" target="_blank"&gt;https://openai.com/index/learning-to-reason-with-llms/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[6]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fextrasensoryai.github.io%2Fenki%2Fblog%2Fsynthetic-data-cot%2F" target="_blank"&gt;https://extrasensoryai.github.io/enki/blog/synthetic-data-cot/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[7]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flean-lang.org%2Fabout%2F" target="_blank"&gt;https://lean-lang.org/about/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[8]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2410.08146" target="_blank"&gt;https://arxiv.org/pdf/2410.08146&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[9]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2501.04682" target="_blank"&gt;https://arxiv.org/pdf/2501.04682&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[10]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2410.09918" target="_blank"&gt;https://arxiv.org/pdf/2410.09918&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[11]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2404.03683" target="_blank"&gt;https://arxiv.org/abs/2404.03683&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[12]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2408.16293" target="_blank"&gt;https://arxiv.org/abs/2408.16293&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[13]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2402.14083" target="_blank"&gt;https://arxiv.org/pdf/2402.14083&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[14]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeepmind.google%2Fdiscover%2Fblog%2Falphazero-shedding-new-light-on-chess-shogi-and-go%2F" target="_blank"&gt;https://deepmind.google/discover/blog/alphazero-shedding-new-light-on-chess-shogi-and-go/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[15]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeepmind.google%2Fdiscover%2Fblog%2Fai-solves-imo-problems-at-silver-medal-level%2F" target="_blank"&gt;https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[16]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2305.10601" target="_blank"&gt;https://arxiv.org/pdf/2305.10601&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[17]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2305.00633" target="_blank"&gt;https://arxiv.org/pdf/2305.00633&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[18]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2305.14992" target="_blank"&gt;https://arxiv.org/pdf/2305.14992&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[19]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2404.05221" target="_blank"&gt;https://arxiv.org/pdf/2404.05221&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[20]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2501.04682" target="_blank"&gt;https://arxiv.org/pdf/2501.04682&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[21]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2110.14168" target="_blank"&gt;https://arxiv.org/pdf/2110.14168&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[22]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeepmind.google%2Fdiscover%2Fblog%2Fmuzero-mastering-go-chess-shogi-and-atari-without-rules%2F" target="_blank"&gt;https://deepmind.google/discover/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[23]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Faidanmclaughlin.notion.site%2Freasoners-problem" target="_blank"&gt;https://aidanmclaughlin.notion.site/reasoners-problem&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[24]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2412.06769" target="_blank"&gt;https://arxiv.org/pdf/2412.06769&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;原文鏈接：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.innovationendeavors.com%2Finsights%2Fmechanisms-for-test-time-compute" target="_blank"&gt;https://www.innovationendeavors.com/insights/mechanisms-for-test-time-compute&lt;/a&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/IDP/blog/18690730</link>
      <guid isPermaLink="false">https://my.oschina.net/IDP/blog/18690730</guid>
      <pubDate>Wed, 03 Sep 2025 07:25:00 GMT</pubDate>
      <author>原創</author>
    </item>
    <item>
      <title>英偉達新款「中國特供」 AI 芯片性能強於 H20，但價格翻番</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.reuters.com%2Fworld%2Fchina%2Fchinese-firms-still-want-nvidia-chips-despite-government-pressure-not-buy-2025-09-04%2F"&gt;路透社獨家報道&lt;/a&gt;了英偉達新款中國特供芯片的更多細節。&lt;/p&gt; 
&lt;p&gt;據瞭解，該芯片暫定名為 B30A，基於英偉達 Blackwell 架構，性能更強大。&lt;/p&gt; 
&lt;p&gt;兩位知情人士表示，如果美國批准英偉達對華銷售 B30A，其價格很可能是 H20 的兩倍左右。目前，H20 的售價介於 1 萬美元至 1.2 萬美元之間。&lt;/p&gt; 
&lt;p&gt;在性能方面，一位知情人士稱，B30A 的性能有望高達 H20 的六倍。B30A 和 H20 均為降級版芯片，專為遵守美國對華出口管制為中國市場開發。&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/news/367317"&gt;結合之前的報道&lt;/a&gt;，該芯片採用單芯片設計，預計其算力約為旗艦級 B300 加速卡雙芯片配置的一半。此外，該芯片將配備高帶寬內存（HBM）和 NVLink 技術，以提升處理器間的數據傳輸效率。單芯片設計指所有主要電路都製作在同一塊連續的硅晶圓上，而不是分散在多個芯片上。&lt;/p&gt; 
&lt;p&gt;當被問及在中國市場與對手的競爭格局時，英偉達在一份聲明中表示：「競爭無疑已經到來」。該公司拒絕進一步置評。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370467</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370467</guid>
      <pubDate>Wed, 03 Sep 2025 07:07:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>華為 AI 模型運行專利公佈</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;天眼查 App 資料顯示，華為技術有限公司申請的「AI 模型的運行方法、裝置、程序產品和存儲介質」專利於 9 月 5 日公佈。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="273" src="https://oscimg.oschina.net/oscnet/up-9727352c7cbd66eb90acc6a5d3ef583d555.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;摘要顯示，本公開提供了一種 AI 模型的運行方法、裝置、程序產品和存儲介質，屬於機器學習技術領域。該方法應用於主機，主機包括處理器，並連接計算卡，該方法包括：處理器確定 AI 模型相鄰的兩組輸入數據中在第二數據組中但不在第一數據組中的第一數據，第二數據組在第一數據組之後訓練，將第一數據對應的第一嵌入向量預取至處理器的第一內存，並確定第一數據對應的第一嵌入向量信息，在第二數據組在計算卡上處理過程中，根據第一嵌入向量信息將第一嵌入向量從第一內存預取至計算卡的第二內存。採用本公開所示的方案，能夠減少嵌入向量搬運帶來的處理延時。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370466</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370466</guid>
      <pubDate>Wed, 03 Sep 2025 07:06:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>DuckDuckGo 在其訂閲計劃中添加了對高級 AI 模型的訪問權限</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;注重隱私的消費科技公司 DuckDuckGo 去年推出了一項訂閲計劃，該公司週四表示，現在訂閲服務允許用戶通過 Duck.ai 訪問最新的 AI 模型，無需額外付費。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-37ceb4f3efe0160bc85873fa2aeee1e4e0c.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Duck.ai 聊天機器人可以讓用戶免費使用，包括 Anthropic 的 Claude 3.5 Haiku、Meta 的 Llama 4 Scout、Mistral AI 的 Mistral Small 3 24B 和 OpenAI 的 GPT-4o mini 等模型。&lt;/p&gt; 
&lt;p&gt;通過 DuckDuckGo 每月 9.99 美元的計劃，用戶將能夠訪問更新的模型，包括 OpenAI 的 GPT-4o 和 GPT-5、Anthropic 的 Claude Sonnet 4 和 Meta 的 Llama Maverick。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-05a531bb9bcf7d8ae1588a8570acc2ba232.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-ff24034c7a8ed96e1594e923557bcafac9d.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fspreadprivacy.com%2Fp%2Fab42d8ac-7e7d-4610-86a7-fbb9c36d058d%2F"&gt;該公司在一篇博文中表示&lt;/a&gt;：「這些更大的模型更擅長遵循詳細的指令，在長時間的聊天中保持語境，並提供更深入、更細緻的回應。DuckDuckGo 訂閲提供了一種使用其中一些模型的方式，但同時又能更好地保護隱私。」&lt;/p&gt; 
&lt;p&gt;對於那些想要訪問最新模型而不侷限於單一供應商的人來説，這是一個絕佳的機會。此外，Quora 的 Poe 也提供一系列模型的訪問權限。您可以以每月 5 美元起的價格購買 Poe 的訂閲服務。&lt;/p&gt; 
&lt;p&gt;DuckDuckGo 表示，將繼續在其付費產品中增加更昂貴的套餐，提供 「更大、更先進的型號」。該公司並未具體説明當前套餐是否有使用限制。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370464</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370464</guid>
      <pubDate>Wed, 03 Sep 2025 06:57:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>TikTok 與美攝的知識產權糾紛案將於 10 月在美國開庭審理</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fusaherald.com%2Ftiktok-chinese-company-845m-ip-fight-moves-forward-to-october-jury-trial%2F"&gt;據《美國先驅報》報道&lt;/a&gt;，北京美攝網絡科技有限公司與 TikTok 的訴訟官司將於 10 月在美國開庭審理，加州一名聯邦法官拒絕完全批准 TikTok 提出的簡易判決或終止制裁的動議。&lt;/p&gt; 
&lt;p&gt;該案涉及商業機密盜竊和版權侵權指控，涉案金額達 8.45 億美元，將於 10 月 27 日開始為期 12 天的陪審團審理。&lt;/p&gt; 
&lt;p&gt;在一份長達 36 頁的裁決中，美國地區法官蘇珊・伊爾斯頓（Susan Illston）裁定，關於美攝的版權所有權和涉嫌盜用商業機密的行為仍然存在實質性的事實爭議。&lt;/p&gt; 
&lt;p&gt;法院駁回了針對 TikTok 母公司字節跳動有限公司的訴訟，但允許針對 TikTok 的核心知識產權訴訟繼續進行。&lt;/p&gt; 
&lt;p&gt;該訴訟於 2021 年提起，後轉至加州北區聯邦地區法院，指控美攝公司一名前員工盜用了高度機密的源代碼，隨後 TikTok 和字節跳動的關聯公司使用這些代碼開發視頻編輯功能。美攝公司聲稱，這些被盜用的代碼價值約 8.45 億美元。&lt;/p&gt; 
&lt;p&gt;字節跳動於 2023 年解僱了涉案員工，但保留了提起相關訴訟的權利。此後，雙方進行了廣泛的動議實踐，包括針對概括對專家證詞的判斷和證據質疑。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-c9598d0dbf07971e421eff82551bfa14faf.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;伊爾斯頓法官駁回了 TikTok 終止制裁的請求，認為儘管美攝公司的部分證人行為存在疑問，但不足以構成撤銷制裁的理由。&lt;/p&gt; 
&lt;p&gt;法院還裁定，美攝公司與新奧特（XAT）之間相關中國訴訟及許可協議的關鍵文件可予採納，從而強化了美攝公司對爭議知識產權的所有權主張。&lt;/p&gt; 
&lt;p&gt;「這項裁決確保了 TikTok 這家中國公司 8.45 億美元知識產權糾紛的核心問題將由陪審團審理並裁決，[美攝公司發言人 / 代表表示。「我們對自己的立場充滿信心，並期待着陳述我們的主張。」&lt;/p&gt; 
&lt;p&gt;TikTok 及其代表尚未對該裁決發表評論。&lt;/p&gt; 
&lt;p&gt;該案是北京美攝網絡科技有限公司訴 TikTok Inc. 等案，案號 3:23-cv-06012，在美國加州北區地方法院審理。&lt;/p&gt; 
&lt;p&gt;這個案件在國內已經終審完畢，上述信息是在美國的進展。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;相關閲讀；&lt;a href="https://www.oschina.net/news/333672" target="_blank"&gt;抖音副總裁回應字節跳動抄襲案：美攝曾要求披露 TikTok 全部源代碼&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370460</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370460</guid>
      <pubDate>Wed, 03 Sep 2025 06:49:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
  </channel>
</rss>
