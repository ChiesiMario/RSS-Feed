<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - industry - 繁體中文（台灣）</title>
    <link>https://www.oschina.net/news/industry</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news/industry" rel="self" type="application/rss+xml"/>
    <description>已對該 RSS 進行格式化操作：中英字符之間插入空格、使用直角引號、標點符號修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-tw</language>
    <lastBuildDate>Tue, 05 Aug 2025 07:40:54 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>谷歌 AI 編程 Agent 「Jules」 支持創建 PR</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;谷歌 AI 編程 Agent 「Jules」&amp;nbsp;新增創建拉取請求（PR）的功能，實現了從編碼到提交的完整開發閉環。&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fjules.google%2Fdocs%2Fchangelog%2F" target="_blank"&gt;根據 Jules 的更新日誌&lt;/a&gt;，它現在可以將代碼變更整合並創建拉取請求（Pull Request）。完成任務後，用戶可以要求 Jules 打包變更、撰寫摘要並開啓一個待審查的 PR，實現了從規劃、編碼、提交到 PR 的完整閉環。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-be3ad0bacfe87f8715b643becb4e4d845d2.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="636" src="https://static.oschina.net/uploads/space/2025/0805/152238_a4vc_2720166.png" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Jules 官方稱，本週是「Jules 發佈周」，預計將會有更多功能升級。Google AI 開發者關係負責人 Logan Kilpatrick 也在社交媒體上發帖稱「big week ahead!」（未來將是重要的一週！），不確定是否僅與 Jules 有關。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0805/152248_VoyB_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364327</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364327</guid>
      <pubDate>Tue, 05 Aug 2025 07:24:52 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>警惕 AI 數據投毒，0.01% 虛假訓練文本可致有害內容增加 11.2%</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;&lt;span style="color:#000000"&gt;國家安全部發布安全提示&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FjJ33UUZzgVn3LD9BpJe1bA" target="_blank"&gt;文章&lt;/a&gt;指出，當前，人工智能已深度融入經濟社會發展的方方面面，在深刻改變人類生產生活方式的同時，也成為關乎高質量發展和高水平安全的關鍵領域。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;然而，人工智能的訓練數據存在良莠不齊的問題，其中不乏虛假信息、虛構內容和偏見性觀點，造成數據源污染，給人工智能安全帶來新的挑戰。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;人工智能的三大核心要素是算法、算力和數據，其中數據是訓練 AI 模型的基礎要素，也是 AI 應用的核心資源。&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;提供 AI 模型的原料。&lt;/strong&gt;海量數據為 AI 模型提供了充足的訓練素材，使其得以學習數據的內在規律和模式，實現語義理解、智能決策和內容生成。同時，數據也驅動人工智能不斷優化性能和精度，實現模型的迭代升級，以適應新需求。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;影響 AI 模型的性能。&lt;/strong&gt;AI 模型對數據的數量、質量及多樣性要求極高。充足的數據量是充分訓練大規模模型的前提；高準確性、完整性和一致性的數據能有效避免誤導模型；覆蓋多個領域的多樣化數據，則能提升模型應對實際複雜場景的能力。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;促進 AI 模型的應用。&lt;/strong&gt;數據資源的日益豐富，加速了「人工智能+」行動的落地，有力促進了人工智能與經濟社會各領域的深度融合。這不僅培育和發展了新質生產力，更推動我國科技跨越式發展、產業優化升級、生產力整體躍升。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;高質量的數據能夠顯著提升模型的準確性和可靠性，但數據一旦受到污染，則可能導致模型決策失誤甚至 AI 系統失效，存在一定的安全隱患。&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;投放有害內容。&lt;/strong&gt;通過篡改、虛構和重複等「數據投毒」行為產生的污染數據，將幹擾模型在訓練階段的參數調整，削弱模型性能、降低其準確性，甚至誘發有害輸出。研究顯示，當訓練數據集中僅有 0.01% 的虛假文本時，模型輸出的有害內容會增加 11.2%；即使是 0.001% 的虛假文本，其有害輸出也會相應上升 7.2%。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;造成遞歸污染。&lt;/strong&gt;受到數據污染的人工智能生成的虛假內容，可能成為後續模型訓練的數據源，形成具有延續性的「污染遺留效應」。當前，互聯網 AI 生成內容在數量上已遠超人類生產的真實內容，大量低質量及非客觀數據充斥其中，導致 AI 訓練數據集中的錯誤信息逐代累積，最終扭曲模型本身的認知能力。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;引發現實風險。&lt;/strong&gt;數據污染還可能引發一系列現實風險，尤其在金融市場、公共安全和醫療健康等領域。在金融領域，不法分子利用 AI 炮製虛假信息，造成數據污染，可能引發股價異常波動，構成新型市場操縱風險；在公共安全領域，數據污染容易擾動公眾認知、誤導社會輿論，誘發社會恐慌情緒；在醫療健康領域，數據污染則可能致使模型生成錯誤診療建議，不僅危及患者生命安全，也加劇偽科學的傳播。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;為了應對數據污染帶來的威脅，國家安全部建議：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;加強源頭監管，防止污染數據的產生。&lt;/strong&gt;以《網絡安全法》《數據安全法》《個人信息保護法》等法律法規為依據，建立 AI 數據分類分級保護制度，從根本上防範污染數據的產生，助力有效防範 AI 數據安全威脅。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;強化風險評估，保障數據流通。&lt;/strong&gt;加強對人工智能數據安全風險的整體評估，確保數據在採集、存儲、傳輸、使用、交換和備份等全生命週期環節安全。同步加快構建人工智能安全風險分類管理體系，不斷提高數據安全綜合保障能力。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;末端清洗修復，構建治理框架。&lt;/strong&gt;定期依據法規標準清洗修復受污數據。依據相關法律法規及行業標準，制定數據清洗的具體規則。逐步構建模塊化、可監測、可擴展的數據治理框架，實現持續管理與質量把控。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364320</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364320</guid>
      <pubDate>Tue, 05 Aug 2025 07:06:52 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>阿里巴巴 2026 秋季校招計劃超 6 成 AI 相關崗位</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p style="color:#2a3840; margin-left:.2rem; margin-right:0; text-align:justify"&gt;阿里巴巴 2026 屆秋季校園招聘正式&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftalent-holding.alibaba.com%2Fcampus%2Fhome%3Flang%3Dzh" target="_blank"&gt;啓動&lt;/a&gt;，計劃發出超過 7000 個錄用通知。此次招聘涵蓋阿里巴巴控股集團、淘天、阿里雲、阿里國際、通義實驗室、智能信息、釘釘、高德等 15 個業務集團和公司。&lt;/p&gt; 
&lt;p style="color:#2a3840; margin-left:.2rem; margin-right:0; text-align:justify"&gt;與春季招聘相比，秋季校招顯著加大了 AI 人才招聘力度。AI 相關崗位佔比超過六成。部分 AI 業務部門的招聘比例更為突出，阿里雲、阿里國際、釘釘的 AI 崗位佔比達到 80%，高德的相關比例也達到 75%。&lt;/p&gt; 
&lt;p style="color:#2a3840; margin-left:.2rem; margin-right:0; text-align:justify"&gt;&lt;img height="341" src="https://oscimg.oschina.net/oscnet/up-fcbb54eb5d1a7776ad186909f3ec872638f.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2a3840; margin-left:.2rem; margin-right:0; text-align:justify"&gt;據悉，阿里國際在 2026 屆校招中，80% 的職位均為 AI 崗位。這些職位包括 AI 算法工程師、研發工程師以及 AI 產品經理等關鍵技術崗位。與此同時，阿里國際啓動了面向全球的頭部 AI 科技人才培養計劃 Bravo102。&lt;/p&gt; 
&lt;p style="color:#2a3840; margin-left:.2rem; margin-right:0; text-align:justify"&gt;阿里巴巴集團董事會主席蔡崇信此前在香港舉行的滙豐全球投資峯會上表示，阿里巴巴的員工數量已觸底。公司將重新啓動並重新招聘，標誌着人才戰略的重要轉向。&lt;/p&gt; 
&lt;p style="color:#2a3840; margin-left:.2rem; margin-right:0; text-align:justify"&gt;集團 CEO 吳泳銘在財報分析師會上明確表態，未來三年將圍繞 AI 戰略核心加大投入。投入重點包括 AI 和雲計算的基礎設施建設、AI 基礎模型平台及 AI 原生應用，以及現有業務的 AI 轉型升級三個方面。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364315</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364315</guid>
      <pubDate>Tue, 05 Aug 2025 06:56:52 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Google Kaggle 舉辦 AI 國際象棋錦標賽，評估領先模型的推理能力</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;OpenAI 的 o3 和 04-mini、Google LLC 的 Gemini 2.5 Pro 和 Gemini 2.5 Flash、Anthropic 的 Claude Opus 4 以及 xAI Corp. 的 Grok 4 等全球性能最強的人工智能模型將在棋盤上展開正面交鋒。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-559fc85f92024cf582bdb7dd5bb9b495a9f.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fsiliconangle.com%2F2019%2F11%2F04%2Fgoogle-makes-cloud-automl-service-available-kaggle%2F" target="_blank"&gt;這場為期三天的人工智能象棋對決是 Google 數據科學社區 Kaggle&lt;/a&gt;&amp;nbsp;即將在新開發的 Game Arena 舉辦的一系列錦標賽的首場。在那裏，模型將在一系列旨在評估其思維和推理能力的戰略遊戲中相互競爭。&lt;img alt="" src="https://static.cnbetacdn.com/article/2025/0805/8cbf492b712fcfe.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Google DeepMind 和 Kaggle 將與 Chess.com、國際象棋應用程序 Take Take Take 以及傳奇國際象棋直播主播 Levy Rozman 和 Hikaru Nakamura 合作舉辦此次比賽，首場模擬比賽將於明天開始。&lt;/p&gt; 
&lt;p&gt;Kaggle&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fkaggle.com%2Fgame-arena" target="_blank"&gt;Game Arena&lt;/a&gt;是一個全新的 AI 基準測試平台，旨在測試大型語言模型在圍棋和狼人殺等一系列戰略遊戲中的競爭力。首先登場的是 AI 國際象棋表演賽，該表演賽將於 8 月 5 日至 7 日舉行，模擬比賽將在 Kaggle.com 上進行直播。&lt;/p&gt; 
&lt;p&gt;Hikaru Nakamura 將對每場比賽進行評論，而 Levy Rozman 將在 GothamChess&amp;nbsp;YouTube 頻道上提供每日比賽的回顧和分析。比賽結束時，Magnus Carlsen 將在&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.youtube.com%2F%40TakeTakeTakeApp" target="_blank"&gt;Take Take Take YouTube 頻道&lt;/a&gt;上直播冠軍對決和賽事回顧。&lt;/p&gt; 
&lt;p&gt;八位選手將角逐國際象棋霸主地位：Gemini 2.5 Pro、Gemini 2.5 Flash、Claude Opus 4、DeepSeek-R1、Moonshot 的 Kimi 2-K2-Instruct、o3、o4-mini 和 Grok 4。比賽將採用標準的單敗淘汰賽制，每場比賽的勝負將通過四局兩勝制決出。Kaggle Game Arena 每天將直播一輪比賽，因此第一輪四分之一決賽將進行四場八個模型的對決，第二天將進行兩場半決賽，第三天將進行一場決賽。&lt;/p&gt; 
&lt;p&gt;Google 在一篇博客文章中概述了一系列&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.kaggle.com%2Fbenchmarks%2Fkaggle%2Fchess-fen%2Fversions%2F1%2Ftournament" target="_blank"&gt;規則&lt;/a&gt;，稱這些模型將響應基於文本的輸入。所有參賽模型都不得訪問任何第三方工具，因此它們無法直接使用 Stockfish 國際象棋引擎來識別任何情況下的最佳走法。相反，它們必須自行思考。&lt;/p&gt; 
&lt;p&gt;模型不會獲得所有可能的合法走法列表，如果模型嘗試走法，則允許重試三次。如果模型未能走法，則將棄權。此外，每步走法都有 60 分鐘的時間限制。&lt;/p&gt; 
&lt;p&gt;直播將嘗試展示每個競爭模型如何「推理」其下一步行動，以及對任何失敗行動的反應。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.cnbetacdn.com/article/2025/0805/3c8d2544fd8be20.png" referrerpolicy="no-referrer"&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-b875aad4f4c3af6d1e66faa049d0decc0e4.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;除了比賽之外，Kaggle 還將創建一個更全面的排行榜，根據每個模型在數百場非直播的「幕後」比賽中的表現進行排名。每個模型將與競爭對手進行多次對決，對決雙方隨機選擇。此舉旨在幫助 Kaggle 創建一個更強大的排行榜，作為衡量每個模型棋藝的綜合基準。&lt;/p&gt; 
&lt;p&gt;Kaggle 產品經理 Meg Risdal 表示：「雖然比賽是一種有趣的方式，可以觀看並瞭解不同模型在遊戲競技場環境中如何下棋，但最終的排行榜將代表我們長期以來對模型下棋能力的嚴格基準。」&lt;/p&gt; 
&lt;p&gt;Google 表示，推出 Kaggle 遊戲競技場是因為國際象棋等遊戲是評估法學碩士推理能力的最佳方式之一。&lt;/p&gt; 
&lt;p&gt;這是因為遊戲能夠抵禦 Google 所謂的「飽和度」，換句話説，可以用標準公式來解決。國際象棋、圍棋和其他遊戲極其複雜，每場比賽都是獨一無二的，這意味着隨着每個參賽者的進步，難度也會隨之增加。而狼人殺遊戲則能夠考驗企業的基本技能，例如在不完整信息中導航，以及在合作與競爭之間取得平衡。&lt;/p&gt; 
&lt;p&gt;此外，Google 表示，遊戲就像現實世界技能的代理，可以測試模型在戰略規劃、記憶、推理、適應、欺騙和「心智理論」（即預測對手想法的能力）方面的能力。同時，像「狼人殺」這樣的團隊遊戲可以幫助評估每個模型的溝通和協調能力。&lt;/p&gt; 
&lt;p&gt;Kaggle 的全新 Game Arena 將展示當前和即將舉行的直播比賽，每場比賽都將擁有專屬頁面，列出排名模型的排行榜、比賽結果以及開源遊戲環境及其規則的具體細節。隨着每個模型玩更多比賽，以及更新的模型添加到排名中，排行榜將動態更新。&lt;/p&gt; 
&lt;p&gt;未來，Kaggle Game Arena 將擴展到包括更復雜的多人視頻遊戲和真實世界模擬，以生成更全面的基準來評估不斷擴展的 AI 模型技能。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364296</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364296</guid>
      <pubDate>Sun, 03 Aug 2025 06:16:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>deepin 技術雙週報 | DDE 穩定性顯著提升，6.6 內核大量優化</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;&lt;span&gt;&lt;span&gt;2025 年第 10 期 deepin 雙週技術進展報告現已正式&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fy0-tetQxAjDGl7ooLrAmTw" target="_blank"&gt;發佈&lt;/a&gt;，詳細梳理了 deepin 各技術組在過去兩週內的工作成果，並對未來兩週的工作計劃進行簡要説明。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;h4 style="margin-left:0px; margin-right:0px; text-align:justify"&gt;&lt;strong&gt;&lt;span&gt;01&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;DDE&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt; 
&lt;p style="margin-left:0px; margin-right:0px"&gt;&lt;strong&gt;&lt;span&gt;進展&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul style="list-style-type:disc; margin-left:0; margin-right:0"&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;提升 dde-shell 的穩定性，避免部分場景下偶現的更新過程中任務欄崩潰的現象；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;修復拖拽未駐留在任務欄的圖標導致圖標被駐留的問題；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;修復部分場景中，任務欄駐留的圖標可能重複的問題；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;修正通知中心空白圖標的問題，並增加無通知場景的相應狀態；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;針對全屏啓動器的應用右鍵菜單支持跟隨主題色變化；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;為啓動器增加 F1 幫助快捷鍵；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;修正部分場景下切換小窗口啓動器的分類模式可能導致啓動器崩潰的問題；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;針對 wine 程序提供更好的卸載功能集成支持；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;修正控制中心時區菜單激活色不正確的問題，以及諸多其他類似 UI 問題調整；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;修正控制中心調整音量時可能產生的音頻反饋問題；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;修正控制中心藍牙界面展示的設備排序問題；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;其他各類瑣碎的問題修正和功能開發。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&lt;strong&gt;&lt;span&gt;計劃&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul style="list-style-type:disc; margin-left:0; margin-right:0"&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;繼續針對已發現問題進行修正；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;針對 TreeLand 環境進行積極適配。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4 style="margin-left:0px; margin-right:0px; text-align:justify"&gt;&lt;strong&gt;&lt;span&gt;02&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;系統研發&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt; 
&lt;p style="margin-left:0px; margin-right:0px"&gt;&lt;strong&gt;&lt;span&gt;進展&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul style="list-style-type:disc; margin-left:0; margin-right:0"&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;SW64 架構工具鏈 patch 合入主線；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;usb.ids 版本更新；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;dh-builtusing 版本更新；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;apache2 版本更新；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;git-buildpackage 版本更新；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;box64 版本更新；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;db5.3 版本更新；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;djvulibre 版本更新；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;fastfetch 版本更新；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;jq 版本更新；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;redis 版本更新；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;usbutils 版本更新；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;sane-airscan 版本更新；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;sqlite3 版本更新；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;CVE 漏洞修復。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&lt;strong&gt;&lt;span&gt;計劃&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul style="list-style-type:disc; margin-left:0; margin-right:0"&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;軟件包更新；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;推進 CVE 安全漏洞修復。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4 style="margin-left:0px; margin-right:0px; text-align:justify"&gt;&lt;strong&gt;&lt;span&gt;03&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;strong&gt;&lt;span&gt;內核&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&lt;strong&gt;&lt;span&gt;進展&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul style="list-style-type:disc; margin-left:0; margin-right:0"&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;小版本更新補丁合入到上游 6.6.100 內核版本和 6.12.40 內核版本；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;6.6 內核分支合入並在 x86 和 ARM64 上啓用 ashmem 功能；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;6.6 內核分支合入補丁，優化對 RISC-V 上的 amdgpu 支持；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;6.6 內核分支合入來自申威的補丁 (kvm,acpi,pci 等等)，優化對申威架構的支持；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;6.6 內核分支合入來自飛騰的補丁，修復當 SMMU 事件類型為 0x10 且故障轉換地址為 0x0 時，跳過該錯誤信息的打印；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;6.6 內核分支合入來自海光的補丁，優化對海光 tdm、ccp 功能的支持；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;6.6 內核分支合入上游補丁，移除了部分上游已移除的內容，例如 wait bookmarks；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;6.6 內核分支合入上游補丁，優化 pipe 鎖的性能；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;6.6 內核分支合入上游補丁，優化 udp 的性能；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;6.6 內核分支支持成都華瑞數鑫 D3100s sas/sata raid 卡驅動；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;6.6 內核分支上進行對 IEE 的優化，加入 SLAB_NO_MERGE 標誌，防止緩存合併，確保使用獨立的內存池；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;6.6 內核回合 mt7925 網卡驅動的修復，修復了 reset 進程可能失敗的問題和硬件掃描中可能存在不合法的數組下標問題；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;修正了來自 Debian 固件加載失敗補丁的錯誤，只有在所有壓縮方式嘗試都失敗的情況下打印日誌，而不是在不壓縮的情況下嘗試失敗就打印日誌，導致大量誤報問題；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;調整 x86 的編譯配置，防止非 root 用戶可以訪問內核日誌以增強安全性；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;在 ARM64 架構上打開內核的 wireguard 編譯。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&lt;strong&gt;&lt;span&gt;計劃&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul style="list-style-type:disc; margin-left:0; margin-right:0"&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;代碼評審合入廠商的提交。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364294</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364294</guid>
      <pubDate>Sun, 03 Aug 2025 06:12:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>GitHub CEO：開發者必須積極擁抱 AI，否則可能被淘汰</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在近期的一篇博客文章中，微軟旗下 GitHub 的首席執行官託馬斯・多姆克（Thomas Dohmke）對全球的軟件工程師發出了重要警告。他指出，開發者必須積極擁抱人工智能 (AI)，否則可能面臨被行業淘汰的風險。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;多姆克在文章中提到，軟件開發不僅僅是在編寫代碼，而是正在經歷身份的轉變。他提到，已經有 22 位開發者分享了他們在工作中深度融入 AI 工具的經歷，表明 AI 已成為他們日常工作中不可或缺的夥伴。其中一些開發者最初對 AI 持懷疑態度，但隨着時間的推移，他們逐漸認識到 AI 工具的價值，並開始將其視為協作的關鍵。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;「要麼擁抱 AI，要麼就離開這個行業。」 這句引人注目的話來自於一位開發者，體現了當前開發者面臨的壓力和挑戰。多姆克進一步強調，開發者的角色正在轉變，從傳統的編碼者變成 AI 戰略家，他們不僅負責代碼的編寫，還需要管理和審核 AI 生成的代碼。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;隨着 AI 技術的迅速發展，預計未來 90% 的代碼編寫將實現自動化。這種變化意味着開發者需要掌握新的技能，包括系統設計、AI 應用的熟練度以及任務分配等。多姆克認為，早期採用 AI 工具的開發者已經獲得了先發優勢，而不是被取代。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;這種轉型不僅涉及技術的更新，更是思維方式的變化。開發者們需要把重心從單純追求速度和效率轉向如何利用 AI 提升工作的質量和創意。儘管改變是困難的，許多人可能對此感到抗拒，但多姆克認為，未來只有那些能夠適應這一變化的人才能在行業中立足。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在文章的最後，多姆克直言不諱地表示，不願意改變的人應該考慮尋找其他職業道路。這一觀點無疑對許多開發者提出了挑戰，同時也在呼喚着未來軟件開發的新標準。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364283</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364283</guid>
      <pubDate>Sun, 03 Aug 2025 05:49:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Anthropic 已開始內部測試 Claude Opus 4.1</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Fbtibor91%2Fstatus%2F1952366658326036781" target="_blank"&gt;根據社交媒體上流傳的截圖&lt;/a&gt;，Anthropic 已開始對其下一代大模型 Claude Opus 4.1 進行內部測試。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1477" src="https://static.oschina.net/uploads/space/2025/0805/115400_Jq4N_2720166.png" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;泄露的信息顯示，該模型的內部代號為 claude-leopard-v2-02-prod。一張截圖中的宣傳語寫道：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Opus 4.1 is here - Try our latest model for more problem solving power.&lt;/p&gt; 
 &lt;p&gt;Opus 4.1 來了——試試我們最新的模型，獲得更強的問題解決能力。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;這預示着新模型可能在推理和解決複雜問題的能力上會有顯著提升。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364260</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364260</guid>
      <pubDate>Sun, 03 Aug 2025 03:55:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>谷歌稱其基於 AI 開發的漏洞獵手「Big Sleep」已報告 20 個安全漏洞</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;谷歌安全副總裁 Heather Adkins 週一&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Fargvee%2Fstatus%2F1952390039700431184" target="_blank"&gt;宣佈&lt;/a&gt;，其大語言模型漏洞研究員「Big Sleep」在多種流行開源軟件中發現並報告了 20 個漏洞。這些漏洞主要存在於音頻和視頻庫 FFmpeg 和圖像編輯套件 ImageMagick 等開源軟件中。「Big Sleep」由谷歌人工智能部門 DeepMind 及其精英黑客團隊 Project Zero 開發。&lt;/p&gt; 
&lt;p&gt;鑑於這些漏洞尚未修復，我們目前尚不清楚其影響或嚴重程度，因為谷歌&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgoogleprojectzero.blogspot.com%2F2025%2F07%2Freporting-transparency.html" target="_blank"&gt;目前不願提供詳細信息&lt;/a&gt;，而這在等待漏洞修復時是常規做法。但 Big Sleep 發現這些漏洞這一簡單事實意義重大，因為它表明這些工具開始取得實際成效，即使此案例中涉及人為因素。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0805/113208_FaQ8_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fgoo.gle%2Fbigsleep" target="_blank"&gt;http://goo.gle/bigsleep&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;谷歌發言人金伯利·薩姆拉 (Kimberly Samra) 表示：「為了確保報告的高質量和可操作性，我們在報告之前會聘請一位人類專家參與，但每個漏洞都是由人工智能代理發現並重現的，無需人工幹預。」&lt;/p&gt; 
&lt;p&gt;谷歌工程副總裁 Royal Hansen&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Froyalhansen%2Fstatus%2F1952424018663162235" target="_blank"&gt;在 X 上寫道&lt;/a&gt;，這一發現表明「自動化漏洞發現領域開闢了新領域」。 基於 LLM 的工具能夠查找和發現漏洞已經成為現實。除了 Big Sleep，還有&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.runsybil.com%2F" target="_blank"&gt;RunSybil&lt;/a&gt;&amp;nbsp;和 XBOW 等。&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fxbow.com%2Fblog%2Ftop-1-how-xbow-did-it" target="_blank"&gt;XBOW 在漏洞賞金平台 HackerOne 的美國排行榜上名列前茅&lt;/a&gt;後，引起了媒體的關注。值得注意的是，在大多數情況下，這些報告在流程的某個階段都會有人工參與，以驗證人工智能漏洞獵人是否發現了合法的漏洞，Big Sleep 就是這種情況。&lt;/p&gt; 
&lt;p&gt;RunSybil 是一家開發人工智能漏洞獵手的初創公司，其聯合創始人兼首席技術官 Vlad Ionescu 介紹説，Big Sleep 是一個「合法」的項目，因為它「設計精良，背後的人知道自己在做什麼，Project Zero 擁有漏洞查找經驗，而 DeepMind 擁有強大的資源來支持它」。&lt;/p&gt; 
&lt;p&gt;這些工具顯然前景光明，但也存在一些明顯的缺陷。一些維護不同軟件項目的人抱怨説，他們的錯誤報告實際上是幻覺，有些人甚至稱其為「漏洞賞金計劃」版的人工智能垃圾。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364254</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364254</guid>
      <pubDate>Sun, 03 Aug 2025 03:33:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>ChatGPT 用戶數暴漲至 7 億，OpenAI 年化收入飆升至 120 億美元</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;&lt;span style="color:#000000"&gt;OpenAI 正在經歷前所未有的爆炸式增長。該公司週一宣佈，其旗艦產品 ChatGPT 的周活躍用戶數已達到 7 億，同比增長超過四倍。除了周活躍用戶數的大幅攀升，其日均用戶消息量也突破了 30 億條大關。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="279" src="https://oscimg.oschina.net/oscnet/up-1c5aba8b9f39a0fcea4f5232267cb5fc41e.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在商業化層面，OpenAI 付費商業用戶數量從今年 6 月的 300 萬激增至 500 萬，增長幅度超過 66%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;財務表現方面，OpenAI 今年前七個月的收入實現了翻番增長，年化收入達到 120 億美元。相比 2024 年約 40 億美元的收入水平，這一增長幅度堪稱驚人，甚至有望超越此前預期的 2025 年 127 億美元收入目標。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;OpenAI 產品副總裁 Nick Turley 在宣佈用戶數據時表示："每天，個人和團隊都在學習、創造和解決更棘手的問題。接下來將是一週重要的時刻。"這一表態引發了業界的廣泛猜測。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;分析人士普遍認為，Turley 所提及的"重要的一週"很可能暗示着 GPT- 5 的即將發佈。根據此前的報道，該版本原計劃於 8 月初發布，預計將集成推理功能（o3）、推出迷你和納米版本，並在編碼和性能方面實現顯著提升。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;如果 GPT- 5 確實即將面世，這將是 OpenAI 在技術能力和市場競爭力方面的又一次重大升級。新版本的發佈有望進一步鞏固 ChatGPT 在 AI 聊天機器人領域的領先地位，並可能帶來新一輪的用戶增長和收入提升。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;儘管增長迅速，但在與科技巨頭的競爭中，ChatGPT 仍面臨不小的挑戰。谷歌母公司 Alphabet 首席執行官 Sundar Pichai 在最近的財報電話會議上透露，其 AI 搜索摘要產品 AI Overviews 目前在 200 多個國家擁有約 20 億月度用戶，而 AI 聊天機器人 Gemini App 的月活躍用戶也超過了 4. 5 億。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;從用戶規模對比來看，谷歌的 AI 產品在覆蓋面上仍然具有明顯優勢。不過，ChatGPT 在用戶活躍度和付費轉化方面的表現更為突出，這反映出兩家公司在 AI 產品策略上的不同側重點。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在用戶基數快速擴張的同時，OpenAI 也在積極優化產品體驗。本週 ChatGPT 推出的更新中，新增了休息提醒功能，旨在為用戶提供更健康、更有目標的使用方式。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;此外，新版本還致力於改善用戶的情緒和精神狀態，為重大個人決策提供指導，並整合了來自醫生、研究人員和心理健康顧問的專業意見。這些功能的加入使 ChatGPT 從單純的對話工具向更全面的 AI 助手發展。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364252</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364252</guid>
      <pubDate>Sun, 03 Aug 2025 03:27:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>OpenAI 為 ChatGPT 增加長時間使用提醒</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;OpenAI&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopenai.com%2Findex%2Fhow-we%27re-optimizing-chatgpt%2F" target="_blank"&gt;宣佈&lt;/a&gt;正在為 ChatGPT 推出一系列新功能，旨在促進用戶健康和目標驅動的使用。其中一項已經推出的功能是「休息提醒」，當用戶與 ChatGPT 進行長時間對話後，系統會彈出窗口建議用戶休息。&lt;/p&gt; 
&lt;p&gt;據介紹，ChatGPT 將出於用戶健康考慮，為 ChatGPT 增加溫和的長時間使用提醒，當用戶在某一對話中沉浸過長時間時會有彈窗提示。同時，ChatGPT 將加強對幻覺與情感依賴等對話內容的識別，並與專業人士、研究團隊合作，以更好回覆精神與情感困難問題。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1080" src="https://static.oschina.net/uploads/space/2025/0805/111637_278T_2720166.png" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;OpenAI 表示，其目標是幫助用戶善用注意力，而非僅僅吸引注意力。除了休息提醒，公司還在改進對處於困境中用戶的支持，並開發更好的生活建議功能。所有這些更新都是在醫生、研究人員和心理健康顧問等專家的指導下進行的。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364246</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364246</guid>
      <pubDate>Sun, 03 Aug 2025 03:17:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>WAIC 2025 超擎數智圓滿收官！AI 全棧火爆出圈，加速 AI 應用變革新引擎</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:justify"&gt;7 月 29 日，為期四天的 2025 世界人工智能大會（WAIC）在上海世博展覽館圓滿落幕。本屆大會以「智能時代，同球共濟」為主題，吸引了全球 800 多家企業參展，線下參觀人數達 30.5 萬人次，全面呈現 AI 在覈心技術、行業應用、智能終端與生態鏈接的全景落地。&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:justify"&gt;作為領先的人工智能核心產品與整體解決方案提供商，NVIDIA Compute（GPU）、Networking（網絡）的雙 Elite 精英級合作伙伴，超擎數智以「AI 全棧 · 數智賦能」為主題，攜 AI 應用全棧方案、AI+教科研、醫療健康、金融服務、具身智能應用、聯 NVIDIA 建設交付的中國首個 L20 千卡燈塔集羣等驚豔亮相，開啓了一場 AI 加速千行百業應用「成果爆炸」之旅，讓我們一起回顧超擎數智在 WAIC 2025 的高光時刻吧！&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:justify"&gt;&lt;strong&gt;AI 應用全棧方案重磅亮相&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left"&gt;超擎數智攜自主品牌的元景系列 H20 AI 服務器（8U8 卡 NVLink）、擎天系列 L20 AI 服務器（4U8 卡 PCle）、鋒鋭系列 L20 AI 服務器（2U4 卡 PCIe）系列 AI 服務器，集計算、存儲、網絡於一體的超擎數智 AI 一體機櫃，NVIDIA DGX Spark 桌面級 AI 超級計算機、800G 交換機、ConnectX-8 網卡等多樣 AI 算力產品集體亮相，為大規模數據訓練和推理提供強勁性能，幫助 AI 用戶高效構建 AI 基礎設施和應用環境，滿足 AI 場景下的多元算力需求，為 AI 新質生產力提供強勁引擎。&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:center"&gt;&lt;img height="427" src="https://oscimg.oschina.net/oscnet//61d24d3709bd8a4fd6818e160a14d0de.jpeg" width="640" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left"&gt;前沿尖端網絡產品現場集結，1.6T XDR IB 交換機、800G 交換機、400G RoCE 交換機、NVIDIA ConnectX-8 800G 網卡、BlueField-3 DPU 等眾多前沿網絡產品集體亮相，盡現超擎數智構建高性能、安全、可擴展的下一代 AI 無損網絡的領先方案與技術服務能力！&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:center"&gt;&lt;img height="427" src="https://oscimg.oschina.net/oscnet//2649b35637292fa1a4ff96d31623761b.jpeg" width="640" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left"&gt;光電零損，算力全開。現場展示 1.6T、800G 光模塊、浸沒式液冷光模塊以及 AOC/DAC/AEC/ACC 高速線纜等領先產品，以多樣化 AI 零損光電聯接方案以及領先液冷部署實踐展示，助力多場景應用落地，賦能下一代 AI 集羣。&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:center"&gt;&lt;img height="425" src="https://oscimg.oschina.net/oscnet//6cb0d5b6bb0d3c8d0285a8c82797184c.jpeg" width="640" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left"&gt;超擎數智自主研發的 Al Engine 人工智能開發平台，提供從數據處理、模型開發、訓練到部署的全流程支持，以強大的 AI 開發和推理環境，助力各行業加速智能化轉型。&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:center"&gt;&lt;img height="424" src="https://oscimg.oschina.net/oscnet//67f196af6522b41186e60992bb458d41.jpeg" width="640" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left"&gt;超擎數智擁有專業的審計調優和交付驗收技術服務團隊，超擎數智 CQIS 服務，保障用戶 AI 集羣實現卓越性能、高度可靠性和嚴密的安全性，可顯著縮短模型的訓練週期，提升模型的性能和精度，體驗從算力集羣、存儲、智算網絡到安全的最優算力整體方案，確保用戶的 AI 集羣始終處於最佳狀態。&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:center"&gt;&lt;img height="433" src="https://oscimg.oschina.net/oscnet//ec1d2f9cd2c60be70caf2cebb04eb973.jpeg" width="640" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left"&gt;算力設計與運營服務是超擎數智的核心優勢，現場呈現超擎數智從 0 到 1，為用戶構建適合其業務需求的算力系統的整體方案，從需求分析、方案設計、成本評估、風險預估與管理到實施交付、算力調優，幫助用戶實現算力資源的最大化利用和業務價值的提升。&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:center"&gt;&lt;img height="479" src="https://oscimg.oschina.net/oscnet//d1e6e40a19d1744dfceedcaa6b6fa0fd.jpeg" width="640" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:justify"&gt;&lt;strong&gt;成果展示：超擎數智建設交付的中國首個 L20 千卡燈塔集羣項目引關注熱潮&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left"&gt;超擎數智聯合 NVIDIA 建設交付的中國首個 L20 千卡燈塔集羣項目在本次 WAIC 重磅亮相，現場向觀眾展示了超擎數智的 AI 集羣方案建設及高效交付能力，引得一眾圍觀，現場嘉賓與觀眾紛紛與超擎業務及技術團隊交流探討集羣方案設計、整體方案交付、產品技術支持、調優、運維服務等經驗。項目一期規模為 L20 千卡集羣，二期將擴展到 L20 萬卡集羣，已經為眾多模型廠商、遊戲企業、短視頻產業鏈、工業製造、生物製藥、高校科研機構及金融數據客戶提供高效的算力服務。&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:center"&gt;&lt;img height="427" src="https://oscimg.oschina.net/oscnet//f72aae4e489c964fcf72e06b4705849c.jpeg" width="640" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left"&gt;&lt;strong&gt;AI+行業，解鎖數智賦能無限可能&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:center"&gt;&lt;img height="427" src="https://oscimg.oschina.net/oscnet//8fb6817bb5fec4cfad9ddc84f7e7ea30.jpeg" width="640" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left"&gt;超擎數智全棧場景落地，賦能千行百業，現場展示面向高等教育和研究、醫療健康、金融服務、具身智能等行業應用方案，面向不同行業打造全新場景化落地方案，洞見未來 AI 業務場景。&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left"&gt;&lt;strong&gt;六場 AI 大咖演講，深度解析 AI 技術應用突破與全棧賦能&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left"&gt;WAIC 期間，超擎數智邀請人工智能行業領導企業多位 AI 大咖親臨展台，現場分享 AI 加速行業變革的真知灼見和實踐經驗，引發現場觀眾的熱情關注和駐足觀摩。&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:center"&gt;&lt;img height="480" src="https://oscimg.oschina.net/oscnet//c8df7ec8d4b7299b29b57e9382f0f0eb.jpeg" width="640" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left"&gt;超擎數智產品技術負責人帶來《NVIDIA InfiniBand XDR 高性能 AI 網絡平台》、《超擎數智 AI 應用全棧產品與落地方案》報告分享，從 AI 全棧產品與落地方案，重新定義 AI 時代的數據中心網絡與應用加速路徑。&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left"&gt;思科大中華區首席架構師蔣星帶來《思科 AI 創新與實踐》的主題報告，深度探討 AI 創新與實踐的高效變革之路，與現場觀眾一起沉浸式洞悉 AI 無界未來。&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left"&gt;思科大中華區首席架構師魏航現場分享《AI 體驗躍升：邁向高效、可觀察與安全之路》，系統介紹了 AI 時代，藉助值得信賴的基礎設施為 AI 賦能並保駕護航，助力客戶充分發揮 AI 的價值。&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:center"&gt;&lt;img height="421" src="https://oscimg.oschina.net/oscnet//4da469dfa1c336726b81e65bc8dac777.jpeg" width="640" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:center"&gt;&lt;img height="427" src="https://oscimg.oschina.net/oscnet//ab114c8611f5812317ef12cb739328aa.jpeg" width="640" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left"&gt;超擎數智解決方案技術部負責人帶來《Al Engine 人工智能開發平台加速 AI 應用落地》主題分享，為加速 AI 的開發與部署，提供從數據處理、模型開發、訓練到部署的全流程支持解決方案。&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left"&gt;福鑫科創 CEO 吳笛介紹《GenAl 醫療場景化實踐》，深入解讀了生成式人工智能（GenAI）在醫療行業的場景應用與系統性落地路徑，為行業帶來一場聚焦技術與實踐的思想碰撞。&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left"&gt;超擎數智本次亮相 2025 世界人工智能大會，向行業客戶全面展示了 AI 賦能千行百業的前瞻技術、創新方案、成功實踐與生態構建。未來，超擎數智立足開放格局，鏈接全球前沿智慧，聚焦應用場景突破，創新解決方案，深化產業協作，共建 AI 繁榮生態，攜手行業夥伴共繪智能時代新圖景！&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364243</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364243</guid>
      <pubDate>Sun, 03 Aug 2025 03:13:00 GMT</pubDate>
      <author>作者: 開源科技</author>
    </item>
    <item>
      <title>微軟正在分階段開源 Windows 11 用戶界面框架 WinUI</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;微軟擁有大量開源項目，如今，該公司對社區貢獻的態度也更加開放。儘管如此，仍有大量代碼保持閉源，公司也不斷收到更多開放請求。其中之一就是 Windows 11 的用戶界面框架 WinUI。儘管微軟雖然尚未完全開放，但該公司分享了未來六個月計劃的細節，其中包括「產品工作和基礎變革，以支持更加開放和協作的未來」。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-a574d1f8d2589c1f12d0d92889eb094019d.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fmicrosoft-ui-xaml%2Fdiscussions%2F10700" target="_blank"&gt;微軟表示&lt;/a&gt;，由於其複雜性和連接性，開源 WinUI 不可能輕而易舉地完成。Windows 11 的用戶界面利用了操作系統的許多專有層，這些層無法直接發佈。因此，微軟需要區分哪些內容可以與社區共享，哪些內容不能共享：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;你們中很多人問到關於真正開源這個代碼庫的問題。雖然我們還沒有準備好承諾完成所有里程碑的具體截止日期，但我們正在積極努力。這不是一個瞬間就能實現的計劃，而是一個經過深思熟慮的過程。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;此外，團隊需要優先考慮其他事項，包括安全性、穩定性和對現有產品的支持。&lt;/p&gt; 
&lt;p&gt;微軟計劃分階段開放 WinUI 的 GitHub 存儲庫：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;第一階段：提高鏡像頻率。WASDK 1.8 發佈（8 月底）後，我們將開始更頻繁地將內部提交鏡像到 GitHub，以提高透明度並顯示進度。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;第二階段：第三方開發者本地構建。外部開發者將能夠在本地克隆和構建 repo，並提供文檔來指導設置和依賴關係。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;第三階段：第三方開發者貢獻並運行測試。貢獻者將能夠在本地提交 PR 並運行測試。我們正在努力理清私有依賴關係，並將測試基礎設施開放給公眾訪問。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;第四階段：GitHub 成為重心。GitHub 將成為開發、問題跟蹤和社區參與的主要平台。內部鏡像將被逐步淘汰。&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0805/104430_LP4V_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;讓 WinUI 更加開放將是一個漸進的過程，您可以在&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Forgs%2Fmicrosoft%2Fprojects%2F1868%2Fviews%2F1" target="_blank"&gt;GitHub 上的這個頁面上&lt;/a&gt;跟蹤它。同時，開發人員可以通過分享反饋、提交清晰且寫得好的問題以及點贊現有反饋來做出貢獻。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364233</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364233</guid>
      <pubDate>Sun, 03 Aug 2025 02:44:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Chrome 139 開發者工具增強 AI 輔助功能</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;Chrome 139 開發者工具&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.chrome.com%2Fblog%2Fnew-in-devtools-139%3Fhl%3Dzh-cn" target="_blank"&gt;已發佈&lt;/a&gt;，此次更新重點在於提升產品的可靠性和效率，解決了大量已知問題——從長期存在的視覺故障、可用性問題和設計不一致問題到性能和功能問題。總體而言，將未結問題的數量減少了 27%。&lt;/p&gt; 
&lt;p&gt;在 AI 輔助功能方面，新版本增強了樣式設置的交互性。用戶現在不僅可以截取屏幕截圖，還可以上傳任意圖片到「AI 輔助」面板與 Gemini 的對話中，以提供更直觀的視覺上下文。&lt;/p&gt; 
&lt;p&gt;&lt;img height="682" src="https://static.oschina.net/uploads/space/2025/0805/103609_kGTN_2720166.png" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;此外，「網絡」面板也得到了改進，用戶現在可以右鍵點擊請求表格的列標題，選擇並添加多個請求標頭作為新的列，方便查看和分析。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-58bc3a421212513fb74c256cf34c5fc155a.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364232</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364232</guid>
      <pubDate>Sun, 03 Aug 2025 02:37:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>智譜推出 Zread.ai 開發效率工具，搭載 GLM-4.5</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span&gt;智譜許&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FkgkxULD3SOu5f9gS4XQH1g" target="_blank"&gt;宣佈&lt;/a&gt;推出基於大模型的開發效率工具 Zread.ai，旨在通過 AI 技術一站式解決開發者在接手舊項目、文檔撰寫以及理解開源項目時的常見痛點。Zread.ai 的核心功能包括一鍵理解代碼、生成知識以及促進協作，能夠顯著提升開發效率。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;Zread.ai 的核心功能主要體現在三個方面：源項目的深度學習、快速接手歷史代碼庫以及構建團隊知識協作系統。開發者可以通過輸入任意 GitHub 倉庫鏈接，讓 Zread 生成包含架構解析、模塊説明、設計模式的 Guide，同時支持多倉庫對比、分層解讀與 GitHub Trending 項目邏輯拆解。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;此外，Zread 還能夠自動梳理項目結構、模塊依賴，生成系統性文檔，幫助開發者快速進入狀態，即便面對複雜的代碼也能快速上手。Zread 還提供貢獻者圖譜、社區評論聚合、交互式批註與問答，支持上傳私有項目，構建團隊內部的知識庫和技術文檔體系。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="282" src="https://oscimg.oschina.net/oscnet/up-b39df6faff0e30bca10a982d65b27f2c4bc.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在構建 Zread.ai 的過程中，智譜公司評估了多種大語言模型，最終選擇了 GLM-4.5 作為代碼分析與文檔生成的核心底座。GLM-4.5 在模型代碼理解能力、低幻覺、支持 Deep Research 以及 Agent 能力適配等方面表現出色。它能夠準確識別代碼模塊之間的調用關係、架構層級與依賴結構，為生成高質量技術文檔和項目導讀提供了堅實基礎。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在複雜代碼場景下，GLM-4.5 的輸出穩定性較高，誤解代碼意圖或編造邏輯的情況明顯減少，尤其適合用於代碼解讀和技術問答類任務。針對大型代碼庫，GLM-4.5 能夠進行多輪深入解析，結合上下文與語義線索，對關鍵技術設計進行追問與深挖，幫助開發者獲取更具洞察力的解答。在長上下文理解與技術問答的響應速度、準確率方面，GLM-4.5 也表現穩定，提升了整體交互體驗。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;使用 Zread.ai 非常簡單，只需四步即可快速上手。首先，打開 Zread.ai，輸入 GitHub 倉庫鏈接，系統將自動識別代碼結構與核心組件。接着，系統會自動生成項目導讀（Guide），包含架構拆解、模塊説明與設計範式。然後，開發者可以使用「Ask」功能提問關鍵技術細節，支持深入代碼問答與跨模塊追蹤。最後，上傳私有項目，生成團隊專屬知識庫，為項目構建可持續的文檔資產。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;智譜公司表示，GLM-4.5 不僅是模型提供方，更是 Zread.ai 實現「讀懂代碼、生成知識、服務協作」的核心支撐。未來，智譜將繼續探索 GLM-4.5 在智能體集成、團隊知識協同等場景下的深度應用，為開發者提供更強大的工具，提升開發效率。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364230</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364230</guid>
      <pubDate>Sun, 03 Aug 2025 02:35:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>疑似回應「全員裁員」傳言，硅基智能稱預計全年新增崗位數百個</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;近日，一家成立 8 年的 AI 獨角獸南京硅基智能科技有限公司（下稱「硅基智能」）捲入「全員裁員」傳言。脈脈平台顯示，當前「硅基智能 CEO:準備全員裁員，養不起你們」佔據熱榜第四。&lt;/p&gt; 
&lt;p&gt;8 月 3 日，硅基智能在微信公眾號發出&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fmq8xKJFLo6gCwvG_Vn_rEA" target="_blank"&gt;聲明&lt;/a&gt;，稱「目前擁有一支穩定的產研與銷售團隊，且持續在全球範圍內擴大招聘規模。2025 年，我們將重點佈局杭州、嘉興、香港、新加坡等地，預計全年新增崗位數百個，2026 年將達到新增數千人的擴張節奏。」&lt;/p&gt; 
&lt;p&gt;&lt;img height="293" src="https://oscimg.oschina.net/oscnet/up-e9194ccac4d60e4e961661160dfc6b4e061.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;幾天前，一則疑似硅基智能創始人司馬華鵬在公司工作羣發言的截圖在業內流傳，引發關注。&lt;/p&gt; 
&lt;p&gt;截圖內文文字顯示，司馬華鵬@所有人並表示：「各位，昨天我去看研發，只有徐超一個人在加班，公司今天已經做好了全員裁員的計劃，算法給港科大和清華做，工程化留幾個骨幹，其他的都自尋出路，硅基養不起這樣的團隊，請大家見諒。」&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;針對傳言及相關聲明，記者於 8 月 4 日向硅基智能求證。硅基智能相關人士表示，「我們暫時不對外發聲。團隊專心做好產品和業務，團隊穩定且在擴展。」&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;根據前述聲明，硅基智能稱，「2025 年上半年，公司超額完成銷售目標，盈利能力逐步增強；下半年剛剛開始一個月，我們已鎖定超 3 億元的 AIGC 訂單，並將加快推動「新質生產力賦能中心」在浙江等地落地。同時，新加坡、香港的出海團隊也已初具規模。」&lt;/p&gt; 
&lt;p&gt;據瞭解，硅基智能已於 2025 年 6 月完成來自浙江嘉興的新一輪數億元融資，同時也拿到各大銀行提供的數億元授信額度。目前其賬面現金可支持 120 個月以上的工資發放，同時擁有近億元規模的算力硬件資產。&lt;/p&gt; 
&lt;p&gt;硅基智能成立於 2017 年，提供企業級 AIGC 數字人解決方案。其總部位於南京，屬於專精特新小巨人企業、高新技術企業，當前已擁有發明授權專利一百多項，其中包括二十多項海外專利。股東包括騰訊、招銀國際、國新央企、海松資本、紅杉資本、浦信資本、奇虎 360 等，最新估值近 10 億美元。&lt;/p&gt; 
&lt;p&gt;硅基智能聯合創始人、高級副總裁孫凱彼時在 WAIC 某沙龍上談及公司戰略時透露，硅基智能正在從傳統的工具收費向」按結果付費」轉型。「我們的海外合作伙伴依靠硅基智能提供 AI 數字人技術接口，今年收入就已超過 1 億美元。真正優秀的 AI 員工，不僅要賺月薪，更應成為客戶的合夥人。」&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364227</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364227</guid>
      <pubDate>Sun, 03 Aug 2025 02:28:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>深度解析 RocketMQ 核心組件：ConsumeQueue 的設計與實現</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                                        &lt;h2&gt;導語&lt;/h2&gt; 
&lt;p&gt;在分佈式消息隊列 RocketMQ 中，ConsumeQueue（消費隊列） 是消息消費的核心組件之一。它作為 &amp;nbsp;CommitLog 的索引機制，幫助消費者快速定位並拉取消息。如果沒有 ConsumeQueue，消費者將無法高效地從海量消息中篩選出自己訂閲的數據。&lt;/p&gt; 
&lt;p&gt;本文將基於 RocketMQ 5.0 源碼，深入探討 ConsumeQueue 的設計原理與實現細節。&lt;/p&gt; 
&lt;h2&gt;為什麼需要 ConsumeQueue？&lt;/h2&gt; 
&lt;p&gt;在深入探討 ConsumeQueue 之前，我們有必要先了解 RocketMQ 的消息寫入和存儲方式。&lt;/p&gt; 
&lt;p&gt;CommitLog 是 RocketMQ 的消息存儲模塊，用戶生產的所有消息都持久化存儲在該模塊中，它具備兩個特點：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;使用的持久化存儲是一個文件隊列，文件保存於指定目錄下，每個文件的大小是固定的，通常是 1GB。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;只有一個文件可寫入，且僅支持追加寫，文件寫滿後自動切換至新的文件。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;RocketMQ 設計者出於寫入優先的考慮，沒有為不同 Topic 隊列的消息分配不同的存儲文件，而是將消息直接寫入 CommitLog，不同 Topic 的消息混合分佈在 CommitLog 的文件中。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/f7014d6f-45a9-4e8c-aca4-4eca355e5e5b.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;從上圖中可以看出，儘管消息的寫入非常高效，但是消費者需要按照其訂閲的 Topic 來從 CommitLog 中讀取該 Topic 的消息，顯而易見，RocketMQ 需要一種索引機制來快速讀取指定 Topic 隊列的消息，這正是 ConsumeQueue 要做的事情。&lt;/p&gt; 
&lt;h2&gt;ConsumeQueue 的設計原理&lt;/h2&gt; 
&lt;p&gt;ConsumeQueue 作為 RocketMQ 的消息索引樞紐，其設計核心在於高效映射邏輯隊列與物理存儲。我們通過下面的圖示來介紹 ConsumeQueue 的核心設計：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/36fce070-c2fd-4a3d-9449-0650f6c9946f.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;每個 Topic 隊列有其對應的唯一的 ConsumeQueue，當一條消息寫入到 CommitLog 後，RocketMQ 會構建該消息的索引，按異步方式將其寫入到對應 Topic 隊列的 ConsumeQueue 中。使用索引可以快速定位到消息在 CommitLog 文件的位置並讀取它。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;消息索引對象在 ConsumeQueue 中的位置被稱為 Offset，是個從 0 開始的序號數，maxOffset 即 ConsumeQueue 索引的最大 Offset，會隨着新消息的寫入遞增。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;基於這個設計，消費者通過與 ConsumeQueue 的 Offset 交互來實現消息的消費。最常見的場景就是，我們記錄消費組在 ConsumeQueue 上當前消費的 Offset，那麼消費者下線後再上線仍然可從上次消費的位置繼續消費。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;基於文件的傳統實現方案&lt;/h2&gt; 
&lt;h3&gt;數據存儲與格式&lt;/h3&gt; 
&lt;p&gt;與 CommitLog 類似，ConsumeQueue 使用文件隊列來持久化存儲消息索引。ConsumeQueue 使用的文件目錄所在路徑由其對應的 Topic 隊列確定，舉例説明，一個名為 ABC 的 Topic，其隊列 0 所在的文件目錄路徑是 /data/rocketmq_data/store/consumequeue/abc/0/。消息的索引對象是固定的 20 個字節大小，其內部格式定義見下圖。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/9c1e2702-a9f4-4d84-9bd8-3c5f1b5d7b18.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;為了方便描述，從這裏開始我們將索引對象叫作 CqUnit。ConsumeQueue 的文件隊列中每個文件的大小是固定的，默認配置可存儲 30 萬個 CqUnit，當文件寫滿後，會切換到新文件進行寫入。文件名稱的命名方式是有講究的，它以文件存儲的第一個 CqUnit 的 Offset 作為名稱，這樣做的好處是，按 Offset 查詢 CqUnit 時，可以根據文件名稱，快速定位到該 Offset 所在的文件，大幅減少對文件的讀取操作頻次。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/dc6e56f6-83c4-4e99-92f5-4c2702207472.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;構建過程&lt;/h3&gt; 
&lt;p&gt;當消息寫入到 CommitLog 後，該消息對於消費者是不可見的，只有在 ConsumeQueue 中增加這條消息的 CqUnit 後，消費者才能消費到這條消息，因此寫入消息時須立刻往 ConsuemQueue 寫入消息的 CqUnit。我們需要給每一條消息指定其在 ConsumeQueue 中的 Offset，QueueOffsetOperator 類維護了一個 Topic 隊列與其當前 &amp;nbsp;Offset 的表，當寫入一條新消息時，DefaultMessageStore 從 QueueOffsetOperator 中取出該 Topic 隊列的當前 Offset，將其寫入到消息體中，在消息成功寫入到 CommitLog 後，指示 QueueOffsetOperator 更新為當前 Offset + 1。為了防止其他寫入線程併發訪問 Topic 隊列的當前 Offset，在讀取和修改 Offset 期間，會使用一個 ReentrantLock 鎖定該 Topic 隊列。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/bd666fd0-892a-4923-8f29-f459737f4981.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;ReputMessageService 作為異步任務，會不停的讀取 CommitLog，當有新的消息寫入，它會立即讀取到該消息，然後根據消息體構建一個 DispatchRequest 對象，CommitLogDispatcherBuildConsumeQueue 處理 DispatchRequest 對象，最終將 CqUnit 寫入到 ConsumeQueue 的存儲中。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/89cd3d56-7582-4cbe-ae34-1478e76b2a7c.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;按 Offset 查找消息&lt;/h3&gt; 
&lt;p&gt;消費者通常是從某個 Offset 開始消費消息的，比如消費者下線後再次上線會從上次消費的 Offset 開始消費。DefaultMessageStore 的 GetMessage 方法實現從一個 Topic 隊列中拉取一批消息的功能，每次拉取要指定讀取的起始 Offset 以及該批次讀取的最大消息數量。下面截取了部分源碼展示實現的基本思路：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp;@Override
    public GetMessageResult getMessage(final String group, final String topic, final int queueId, final long offset,
        final int maxMsgNums, final int maxTotalMsgSize, final MessageFilter messageFilter) {
        long beginTime = this.getSystemClock().now();
        GetMessageStatus status = GetMessageStatus.NO_MESSAGE_IN_QUEUE;
        long nextBeginOffset = offset;
        long minOffset = 0;
        long maxOffset = 0;
        GetMessageResult getResult = new GetMessageResult();
        final long maxOffsetPy = this.commitLog.getMaxOffset();
        ConsumeQueueInterface consumeQueue = findConsumeQueue(topic, queueId);
        if (consumeQueue != null) {
            minOffset = consumeQueue.getMinOffsetInQueue();
            maxOffset = consumeQueue.getMaxOffsetInQueue();
            if (maxOffset == 0) {
            //             
            } else {
                long maxPullSize = Math.max(maxTotalMsgSize, 100);
                if (maxPullSize &amp;gt; MAX_PULL_MSG_SIZE) {
                    LOGGER.warn("The max pull size is too large maxPullSize={} topic={} queueId={}", maxPullSize, topic, queueId);
                    maxPullSize = MAX_PULL_MSG_SIZE;
                }
                status = GetMessageStatus.NO_MATCHED_MESSAGE;
                long maxPhyOffsetPulling = 0;
                int cqFileNum = 0;
                while (getResult.getBufferTotalSize() &amp;lt;= 0
                    &amp;amp;&amp;amp; nextBeginOffset &amp;lt; maxOffset
                    &amp;amp;&amp;amp; cqFileNum++ &amp;lt; this.messageStoreConfig.getTravelCqFileNumWhenGetMessage()) {
                    ReferredIterator&amp;lt;CqUnit&amp;gt; bufferConsumeQueue = null;
                    try {
                        bufferConsumeQueue = consumeQueue.iterateFrom(group, nextBeginOffset, maxMsgNums);
                        long nextPhyFileStartOffset = Long.MIN_VALUE;
                        long expiredTtlOffset = -1;
                        while (bufferConsumeQueue.hasNext() &amp;amp;&amp;amp; nextBeginOffset &amp;lt; maxOffset) {
                            CqUnit cqUnit = bufferConsumeQueue.next();
                            long offsetPy = cqUnit.getPos();
                            int sizePy = cqUnit.getSize();
                            SelectMappedBufferResult selectResult = this.commitLog.getMessage(offsetPy, sizePy);
                            getResult.addMessage(selectResult, cqUnit.getQueueOffset(), cqUnit.getBatchNum());
                            status = GetMessageStatus.FOUND;
                            nextPhyFileStartOffset = Long.MIN_VALUE;
                        }
                    } catch (RocksDBException e) {
                        ERROR_LOG.error("getMessage Failed. cid: {}, topic: {}, queueId: {}, offset: {}, minOffset: {}, maxOffset: {}, {}",
                            group, topic, queueId, offset, minOffset, maxOffset, e.getMessage());
                    } finally {
                        if (bufferConsumeQueue != null) {
                            bufferConsumeQueue.release();
                        }
                    }
                }
                long diff = maxOffsetPy - maxPhyOffsetPulling;
                long memory = (long) (StoreUtil.TOTAL_PHYSICAL_MEMORY_SIZE
                    * (this.messageStoreConfig.getAccessMessageInMemoryMaxRatio() / 100.0));
                getResult.setSuggestPullingFromSlave(diff &amp;gt; memory);
            }
        } else {
            status = GetMessageStatus.NO_MATCHED_LOGIC_QUEUE;
            nextBeginOffset = nextOffsetCorrection(offset, 0);
        }
        getResult.setStatus(status);
        getResult.setNextBeginOffset(nextBeginOffset);
        getResult.setMaxOffset(maxOffset);
        getResult.setMinOffset(minOffset);
        return getResult;
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;上述代碼片段的要點：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Topic 隊列的 ConsumeQueue 的 IterateFrom 方法依據 Offset 生成一個 Iterator 對象。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;在 Iterator 有效的情況，不斷從 Iterator 拉取 CqUnit 對象，即按 Offset 順序讀取 CqUnit。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;使用 CqUnit 對象中的 OffsetPy 和 SizePy 從 CommitLog 中讀取消息內容，返回給消費者。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;接下來，我們介紹 ConsumeQueue 的 IterateFrom 方法是如何讀取 CqUnit 的。從下面的源碼中可以看到，GetIndexBuffer 方法先從 MappedFileQueue 中找到 Offset 所在的 MappedFile，然後找到 Offset 在 MappedFile 中的位置，從該位置讀取文件剩餘的內容。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;public SelectMappedBufferResult getIndexBuffer(final long startIndex) {
        int mappedFileSize = this.mappedFileSize;
        long offset = startIndex * CQ_STORE_UNIT_SIZE;
        if (offset &amp;gt;= this.getMinLogicOffset()) {
            MappedFile mappedFile = this.mappedFileQueue.findMappedFileByOffset(offset);
            if (mappedFile != null) {
                return mappedFile.selectMappedBuffer((int) (offset % mappedFileSize));
            }
        }
        return null;
    }
    @Override
    public ReferredIterator&amp;lt;CqUnit&amp;gt; iterateFrom(long startOffset) {
        SelectMappedBufferResult sbr = getIndexBuffer(startOffset);
        if (sbr == null) {
            return null;
        }
        return new ConsumeQueueIterator(sbr);
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ConsumeQueueIterator 的 Next 方法和 hasNext 方法是對 getIndexBuffer 方法返回的 SelectMappedBufferResult 對象，即文件內容的 ByteBuffer，進行訪問。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp;private class ConsumeQueueIterator implements ReferredIterator&amp;lt;CqUnit&amp;gt; {
        private SelectMappedBufferResult sbr;
        private int relativePos = 0;
        public ConsumeQueueIterator(SelectMappedBufferResult sbr) {
            this.sbr = sbr;
            if (sbr != null &amp;amp;&amp;amp; sbr.getByteBuffer() != null) {
                relativePos = sbr.getByteBuffer().position();
            }
        }
        @Override
        public boolean hasNext() {
            if (sbr == null || sbr.getByteBuffer() == null) {
                return false;
            }
            return sbr.getByteBuffer().hasRemaining();
        }
        @Override
        public CqUnit next() {
            if (!hasNext()) {
                return null;
            }
            long queueOffset = (sbr.getStartOffset() + sbr.getByteBuffer().position() - relativePos) / CQ_STORE_UNIT_SIZE;
            CqUnit cqUnit = new CqUnit(queueOffset,
                sbr.getByteBuffer().getLong(),
                sbr.getByteBuffer().getInt(),
                sbr.getByteBuffer().getLong());
            return cqUnit;
        }
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;我們再講下 MappedFileQueue 的 FindMappedFileByOffset 方法，該方法從其維護的文件隊列中查找到 Offset 所在的文件。前面我們介紹過，ConsumeQueue 的文件隊列中的文件是按 Offset 命名的，MappedFile 的 GetFileFromOffset 就是文件的名稱，那麼只需要按照 Offset 除以文件的大小便可得文件在隊列中的位置。這裏要注意的是，這個位置必須要先減去 FirstMappedFile 的位置後才是有效的，因為 ConsumeQueue 會定期清除過期的文件，所以 ConsumeQueue 管理的 MappedFileQueue 的第一個文件對應的 Offset 未必是 0。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp;public MappedFile findMappedFileByOffset(final long offset, final boolean returnFirstOnNotFound) {
        try {
            MappedFile firstMappedFile = this.getFirstMappedFile();
            MappedFile lastMappedFile = this.getLastMappedFile();
            if (firstMappedFile != null &amp;amp;&amp;amp; lastMappedFile != null) {
                if (offset &amp;lt; firstMappedFile.getFileFromOffset() || offset &amp;gt;= lastMappedFile.getFileFromOffset() + this.mappedFileSize) {
                    LOG_ERROR.warn("Offset not matched. Request offset: {}, firstOffset: {}, lastOffset: {}, mappedFileSize: {}, mappedFiles count: {}",
                        offset,
                        firstMappedFile.getFileFromOffset(),
                        lastMappedFile.getFileFromOffset() + this.mappedFileSize,
                        this.mappedFileSize,
                        this.mappedFiles.size());
                } else {
                    int index = (int) ((offset / this.mappedFileSize) - (firstMappedFile.getFileFromOffset() / this.mappedFileSize));
                    MappedFile targetFile = null;
                    try {
                        targetFile = this.mappedFiles.get(index);
                    } catch (Exception ignored) {
                    }
                    if (targetFile != null &amp;amp;&amp;amp; offset &amp;gt;= targetFile.getFileFromOffset()
                        &amp;amp;&amp;amp; offset &amp;lt; targetFile.getFileFromOffset() + this.mappedFileSize) {
                        return targetFile;
                    }
                    for (MappedFile tmpMappedFile : this.mappedFiles) {
                        if (offset &amp;gt;= tmpMappedFile.getFileFromOffset()
                            &amp;amp;&amp;amp; offset &amp;lt; tmpMappedFile.getFileFromOffset() + this.mappedFileSize) {
                            return tmpMappedFile;
                        }
                    }
                }
                if (returnFirstOnNotFound) {
                    return firstMappedFile;
                }
            }
        } catch (Exception e) {
            log.error("findMappedFileByOffset Exception", e);
        }
        return null;
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;按時間戳查找消息&lt;/h3&gt; 
&lt;p&gt;除了從指定 Offset 消費消息這種方式，消費者還有回溯到某個時間點開始消費的需求，這要求 RocketMQ 支持查詢指定的 Timestamp 所在的 Offset，然後從這個 Offset 開始消費消息。&lt;/p&gt; 
&lt;p&gt;我們可以從 ConsumeQueue 的 GetOffsetInQueueByTime 方法直接瞭解按時間戳查找消息的具體實現。&lt;/p&gt; 
&lt;p&gt;消息是按時間先後寫入的，ConsumeQueue 文件隊列中的 CqUnit 也是按時間先後排列的，那麼每個 MappedFile 都對應一段時間區間內的 CqUnit。從下面代碼可以看出，我們可以先根據 Timestamp 找到其落在時間區間的 MappedFile，然後在該 MappedFile 裏查找最接近該 Timestamp 的 CqUnit。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@Override
    public long getOffsetInQueueByTime(final long timestamp, final BoundaryType boundaryType) {
        MappedFile mappedFile = this.mappedFileQueue.getConsumeQueueMappedFileByTime(timestamp,
            messageStore.getCommitLog(), boundaryType);
        return binarySearchInQueueByTime(mappedFile, timestamp, boundaryType);
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;GetConsumeQueueMappedFileByTime 的具體實現主要分為兩個部分：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;找到每個 MappedFile 的 StartTimestamp 和 StopTimestamp，即 MappedFile 裏第一個 CqUnit 對應消息的時間戳和最後一個 CqUnit 對應消息的時間戳，需要訪問兩次 CommitLog 來得到消息內容。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;使用 Timestamp 和每個 MappedFile 的 StartTimestamp 和 StopTimestamp 比較。當 Timestamp 落在某個 MappedFile 的 StartTimestamp 和 StopTimestamp 區間內時，那麼該 MappedFile 是下一步查找 CqUnit 的目標。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;接下來，要按照二分查找法在該 MappedFile 中找到最接近 Timestamp 的 CqUnit。根據二分查找的法則，每次查找需要比較中間位置的 CqUnit 引用消息的存儲時間和目標 Timestamp 以確定下一個查找區間，直至 CqUnit 滿足最接近目標 Timestamp 的條件。要注意的是，獲取 CqUnit 引用消息的存儲時間需從 CommitLog 中讀取消息。&lt;/p&gt; 
&lt;h2&gt;基於 RocksDB 的優化方案&lt;/h2&gt; 
&lt;p&gt;儘管基於文件的實現比較直觀，但是當 Topic 隊列達到一定數量後，會出現明顯的性能和可用性問題。Topic 隊列數量越多，代表着 ConsumeQueue 文件越多，產生的隨機讀寫也就越多，這會影響系統整體的 IO 性能，導致出現生產消費 TPS 不斷下降，延遲不斷增高的趨勢。在我們內部的測試環境和客戶的生產環境中，我們都發現使用的隊列數過多直接影響系統的可用性，而且我們無法通過不斷升級 Broker 節點配置來消除這種影響，因此我們騰訊雲 TDMQ RocketMQ 版在產品控制枱上會限制客戶可創建的 Topic 數量以確保消息服務的穩定性。&lt;/p&gt; 
&lt;p&gt;那麼有沒有辦法能夠解決上面的問題讓服務能夠承載更多的 Topic 呢？我們可以把 ConsumeQueue 提供的功能理解為使用 Topic 隊列的 Offset 來找到 CqUnit，那麼 Topic 隊列和 Offset 構成了 Key，CqUnit 是 Value，是一個典型的 KV 使用場景。在單機 KV 存儲的軟件裏，最著名的莫過於 RocksDB 了，它被廣泛使用於 Facebook，LinkedIn 等互聯網公司的業務中。從下面的設計圖看，RocksDB 基於 SSTable + MemTable 的實現能夠提供高效寫入和查找 KV 的能力，有興趣的讀者可以研究下 RocksDB 的具體實現 (&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Ffacebook%2Frocksdb%2Fwiki%2FRocksDB-Overview" target="_blank"&gt;https://github.com/facebook/rocksdb/wiki/RocksDB-Overview&lt;/a&gt;)，這裏不展開説明。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/097e131f-0b3f-41fa-9973-57cab02459f0.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;如果我們使用 RocksDB 讀寫 CqUnit，那麼 ConsumeQueue 文件數量不會隨着 Topic 隊列的數量線性增長，便不必擔心由此帶來的 IO 開銷。&lt;/p&gt; 
&lt;p&gt;下面我們來介紹如何使用 RocksDB 來實現 ConsumeQueue。&lt;/p&gt; 
&lt;h3&gt;數據存儲與格式&lt;/h3&gt; 
&lt;p&gt;在基於 RocksDB 的實現裏，RocketMQ 使用兩個 ColumnFamily 來管理不同類型的數據，這裏不熟悉 RocksDB 的讀者可以將 ColumnFamily 視作 MySQL 裏的 Table。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;第一個 ColumnFamiliy，簡稱為 DefaultColumnFamily，用於管理 CqUnit 數據。&lt;/p&gt; &lt;p&gt;Key 的內容格式定義參考下圖，其包含 Topic 名稱、QueueId 和 ConsumeQueue 的 Offset。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/831d29f2-7280-4583-8cf7-794df3d6d057.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Value 的內容格式，與前文中文件實現裏的索引對象定義類似，但是多了一個消息存儲時間的字段。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/cbba8329-dbd2-4dd0-9557-a8e455bcb831.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;第二個 ColumnFamily，簡稱為 OffsetColumnFamily，用於管理 Topic 隊列的 MaxOffset 和 MinOffset。&lt;/p&gt; &lt;p&gt;MaxOffset 是指 Topic 隊列最新一條消息在 ConsumeQueue 中的 Offset，隨着消息的新增而變化。MinOffset 是指 Topic 隊列最早一條消息在 ConsumeQueue 中的 Offset，當消息過期被刪除後發生變化。MaxOffset 和 MinOffset 確定消費者可讀取消息的範圍，在基於文件的實現裏，通過訪問 ConsumeQueue 文件隊列裏的隊尾和隊首文件得到這兩個數值。而在 RocksDB 的實現裏，我們單獨保存這兩個數值。&lt;/p&gt; &lt;p&gt;下圖是 Key 的格式定義，其包含 Topic 名稱、QueueId 以及用於標記是 MaxOffset 或 MinOffset 的字段。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/84dd4df8-a509-4f23-8467-cd67f7b5be20.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Value 保存 ConsumeQueue 的 Offset，以及該 Offset 對應消息在 CommitLog 的位置。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/0e4c53a0-12b1-44fb-a457-3694f870b0c9.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;構建過程&lt;/h3&gt; 
&lt;p&gt;ConsumeQueue 的 CqUnit 的構建過程與前文中基於文件的實現的過程一致，此處不再贅述，不同的是前文中 ReputMessageService 使用的 ConsumeQueueStore 被替換為 RocksDBConsumeQueueStore。在這個過程中，RocksDBConsumeQueueStore 主要完成兩件事：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;往 DefaultColumnFamily 寫入消息對應的 CqUnit。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;往 OffsetColumnFamily 更新消息對應 Topic 隊列的 maxOffset。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp;private boolean putMessagePosition0(List&amp;lt;DispatchRequest&amp;gt; requests) {
        if (!this.rocksDBStorage.hold()) {
            return false;
        }
        try (WriteBatch writeBatch = new WriteBatch(); WriteBatch lmqTopicMappingWriteBatch = new WriteBatch()) {
            final int size = requests.size();
            if (size == 0) {
                return true;
            }
            long maxPhyOffset = 0;
            for (int i = size - 1; i &amp;gt;= 0; i--) {
                final DispatchRequest request = requests.get(i);
                DispatchEntry entry = DispatchEntry.from(request);
                dispatch(entry, writeBatch, lmqTopicMappingWriteBatch);
                dispatchLMQ(request, writeBatch, lmqTopicMappingWriteBatch);
                final int msgSize = request.getMsgSize();
                final long phyOffset = request.getCommitLogOffset();
                if (phyOffset + msgSize &amp;gt;= maxPhyOffset) {
                    maxPhyOffset = phyOffset + msgSize;
                }
            }
            // put lmq topic Mapping to DB if there has mapping exist
            if (lmqTopicMappingWriteBatch.count() &amp;gt; 0) {
                // write max topicId and all the topicMapping as atomic write
                ConfigHelperV2.stampMaxTopicSeqId(lmqTopicMappingWriteBatch, this.topicSeqIdCounter.get());
                this.configStorage.write(lmqTopicMappingWriteBatch);
                this.configStorage.flushWAL();
            }
            this.rocksDBConsumeQueueOffsetTable.putMaxPhyAndCqOffset(tempTopicQueueMaxOffsetMap, writeBatch, maxPhyOffset);
            this.rocksDBStorage.batchPut(writeBatch);
            this.rocksDBConsumeQueueOffsetTable.putHeapMaxCqOffset(tempTopicQueueMaxOffsetMap);
            long storeTimeStamp = requests.get(size - 1).getStoreTimestamp();
            if (this.messageStore.getMessageStoreConfig().getBrokerRole() == BrokerRole.SLAVE
                || this.messageStore.getMessageStoreConfig().isEnableDLegerCommitLog()) {
                this.messageStore.getStoreCheckpoint().setPhysicMsgTimestamp(storeTimeStamp);
            }
            this.messageStore.getStoreCheckpoint().setLogicsMsgTimestamp(storeTimeStamp);
            notifyMessageArriveAndClear(requests);
            return true;
        } catch (Exception e) {
            ERROR_LOG.error("putMessagePosition0 failed.", e);
            return false;
        } finally {
            tempTopicQueueMaxOffsetMap.clear();
            consumeQueueByteBufferCacheIndex = 0;
            offsetBufferCacheIndex = 0;
            this.rocksDBStorage.release();
        }
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;按 offset 查找消息&lt;/h3&gt; 
&lt;p&gt;在前文中我們已介紹過按 Offset 查找消息的流程，RocksDB 的實現裏，DefaultMessageStore 的 GetMessage 方法中使用的 ConsumeQueue 被替換成了 RocksDBConsumeQueue。這裏我們只關注其 IterateFrom 方法的實現，以下是該方法的代碼片段。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;public ReferredIterator&amp;lt;CqUnit&amp;gt; iterateFrom(String group, long startIndex, int count) throws RocksDBException {
        long maxCqOffset = getMaxOffsetInQueue();
        if (startIndex &amp;lt; maxCqOffset) {
            int num = Math.min((int) (maxCqOffset - startIndex), count);
            if (MixAll.isLmq(topic) || PopAckConstants.isStartWithRevivePrefix(topic)) {
                return iterateUseMultiGet(startIndex, num);
            }
            if (num &amp;lt;= messageStore.getMessageStoreConfig().getUseScanThreshold()) {
                return iterateUseMultiGet(startIndex, num);
            }
            if (!messageStore.getMessageStoreConfig().isEnableScanIterator()) {
                return iterateUseMultiGet(startIndex, num);
            }
            final String scannerIterKey = group + "-" + Thread.currentThread().getId();
            ScanRocksDBConsumeQueueIterator scanRocksDBConsumeQueueIterator = scanIterators.get(scannerIterKey);
            if (scanRocksDBConsumeQueueIterator == null) {
                if (RocksDBConsumeQueue.this.messageStore.getMessageStoreConfig().isEnableRocksDBLog()) {
                    LOG.info("new ScanIterator Group-threadId{} Topic:{}, queueId:{},startIndex:{}, count:{}",
                        scannerIterKey, topic, queueId, startIndex, num);
                }
                ScanRocksDBConsumeQueueIterator newScanIterator = new ScanRocksDBConsumeQueueIterator(startIndex, num);
                scanRocksDBConsumeQueueIterator = scanIterators.putIfAbsent(scannerIterKey, newScanIterator);
                if (scanRocksDBConsumeQueueIterator == null) {
                    scanRocksDBConsumeQueueIterator = newScanIterator;
                } else {
                    newScanIterator.closeRocksIterator();
                }
                return scanRocksDBConsumeQueueIterator;
            }
            if (!scanRocksDBConsumeQueueIterator.isValid()) {
                scanRocksDBConsumeQueueIterator.closeRocksIterator();
                if (RocksDBConsumeQueue.this.messageStore.getMessageStoreConfig().isEnableRocksDBLog()) {
                    LOG.info("new ScanIterator not valid Group-threadId{} Topic:{}, queueId:{},startIndex:{}, count:{}",
                        scannerIterKey, topic, queueId, startIndex, count);
                }
                ScanRocksDBConsumeQueueIterator newScanIterator = new ScanRocksDBConsumeQueueIterator(startIndex, num);
                scanIterators.put(scannerIterKey, newScanIterator);
                return newScanIterator;
            } else {
                if (RocksDBConsumeQueue.this.messageStore.getMessageStoreConfig().isEnableRocksDBLog()) {
                    LOG.info("new ScanIterator valid then reuse Group-threadId{} Topic:{}, queueId:{},startIndex:{}, count:{}",
                        scannerIterKey, topic, queueId, startIndex, count);
                }
                scanRocksDBConsumeQueueIterator.reuse(startIndex, num);
                return scanRocksDBConsumeQueueIterator;
            }
        }
        return null;
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;在上面的代碼中，首先通過 GetMaxOffsetInQueue 方法獲取該 Topic 隊列 ConsumeQueue 的 MaxOffset，MaxOffset 結合 Count 參數共同指定 Iterator 掃描的 Offset 區間。&lt;/p&gt; 
&lt;p&gt;然後，我們可以看到 IterateFrom 方法中根據不同的條件判斷分支返回不同類型的 Iterator 類對象，RocksDBConsumeQueueIterator 和 ScanRocksDBConsumeQueueIterator。下面是 &amp;nbsp;IteratorUseMultiGet 方法中創建 RocksDBConsumeQueueIterator 對象的調用鏈中最核心的代碼， RangeQuery 方法根據 StartIndex 和 Num 構建了要查詢的 Key 列表，然後調用 RocksDB 的 MultiGet 方法查詢到 Key 列表對應的 Value 列表，RocksDBConsumeQueueIterator 使用該 Value 列表上提供迭代器的功能。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;nbsp; &amp;nbsp;public List&amp;lt;ByteBuffer&amp;gt; rangeQuery(final String topic, final int queueId, final long startIndex,
        final int num) throws RocksDBException {
        final byte[] topicBytes = topic.getBytes(StandardCharsets.UTF_8);
        final List&amp;lt;ColumnFamilyHandle&amp;gt; defaultCFHList = new ArrayList&amp;lt;&amp;gt;(num);
        final ByteBuffer[] resultList = new ByteBuffer[num];
        final List&amp;lt;Integer&amp;gt; kvIndexList = new ArrayList&amp;lt;&amp;gt;(num);
        final List&amp;lt;byte[]&amp;gt; kvKeyList = new ArrayList&amp;lt;&amp;gt;(num);
        for (int i = 0; i &amp;lt; num; i++) {
            ByteBuffer keyBB;
            // must have used topicMapping
            if (this.topicMappingTable != null) {
                Long topicId = topicMappingTable.get(topic);
                if (topicId == null) {
                    throw new RocksDBException("topic: " + topic + " topicMapping not existed error when rangeQuery");
                }
                keyBB = buildCQFixKeyByteBuffer(topicId, queueId, startIndex + i);
            } else {
                keyBB = buildCQKeyByteBuffer(topicBytes, queueId, startIndex + i);
            }
            kvIndexList.add(i);
            kvKeyList.add(keyBB.array());
            defaultCFHList.add(this.defaultCFH);
        }
        int keyNum = kvIndexList.size();
        if (keyNum &amp;gt; 0) {
            List&amp;lt;byte[]&amp;gt; kvValueList = this.rocksDBStorage.multiGet(defaultCFHList, kvKeyList);
            final int valueNum = kvValueList.size();
            if (keyNum != valueNum) {
                throw new RocksDBException("rocksdb bug, multiGet");
            }
            for (int i = 0; i &amp;lt; valueNum; i++) {
                byte[] value = kvValueList.get(i);
                if (value == null) {
                    continue;
                }
                ByteBuffer byteBuffer = ByteBuffer.wrap(value);
                resultList[kvIndexList.get(i)] = byteBuffer;
            }
        }
        final int resultSize = resultList.length;
        List&amp;lt;ByteBuffer&amp;gt; bbValueList = new ArrayList&amp;lt;&amp;gt;(resultSize);
        for (int i = 0; i &amp;lt; resultSize; i++) {
            ByteBuffer byteBuffer = resultList[i];
            if (byteBuffer == null) {
                break;
            }
            bbValueList.add(byteBuffer);
        }
        return bbValueList;
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ScanRocksDBConsumeQueueIterator 則是使用了 RocksDB 的 Iterator 特性（&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Ffacebook%2Frocksdb%2Fwiki%2FIterator%EF%BC%89%EF%BC%8C%E7%9B%B8%E6%AF%94" target="_blank"&gt;https://github.com/facebook/rocksdb/wiki/Iterator），相比&lt;/a&gt; MultiGet，其擁有更好的性能。&lt;/p&gt; 
&lt;p&gt;下面是 ScanQuery 的實現，代碼比較簡潔，指定 Iterator 的 BeginKey 和 UpperKey，再調用 RocksDB 的 API 返回 Iterator 對象。&lt;/p&gt; 
&lt;p&gt;BeginKey 是通過 Topic 隊列信息和 StartIndex 參數構造的 Key。UpperKey 的構造比較精妙，還記得在 DefaultColumnFamily 介紹裏 Key 的格式吧，Key 的倒數第二個部分是 CTRL_1，作為 CqUnit 的 Key 時是個常量，Unicode 值為 1。構造 UpperKey 時，CTRL_1 被替換為 CTRL_2， Uinicode 值為 2，這樣能保證 Iterator 掃描區間的上限不超過 Topic 隊列 Offset 的理論最大值。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;public RocksIterator scanQuery(final String topic, final int queueId, final long startIndex,
        ReadOptions scanReadOptions) throws RocksDBException {
        final ByteBuffer beginKeyBuf = getSeekKey(topic, queueId, startIndex);
        if (scanReadOptions.iterateUpperBound() == null) {
            ByteBuffer upperKeyForInitScanner = getUpperKeyForInitScanner(topic, queueId);
            byte[] buf = new byte[upperKeyForInitScanner.remaining()];
            upperKeyForInitScanner.slice().get(buf);
            scanReadOptions.setIterateUpperBound(new Slice(buf));
        }
        RocksIterator iterator = this.rocksDBStorage.scan(scanReadOptions);
        iterator.seek(beginKeyBuf.slice());
        return iterator;
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;按時間戳查找消息&lt;/h3&gt; 
&lt;p&gt;與基於文件的實現類似，使用 RocksDB 來按時間戳查找消息，首先也需要確定 Topic 隊列 ConsumeQueue 的 MinOffset 和 MaxOffset，然後使用二分查找法查找到最接近指定時間戳的 CqUnit。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp;@Override
    public long getOffsetInQueueByTime(String topic, int queueId, long timestamp,
        BoundaryType boundaryType) throws RocksDBException {
        final long minPhysicOffset = this.messageStore.getMinPhyOffset();
        long low = this.rocksDBConsumeQueueOffsetTable.getMinCqOffset(topic, queueId);
        Long high = this.rocksDBConsumeQueueOffsetTable.getMaxCqOffset(topic, queueId);
        if (high == null || high == -1) {
            return 0;
        }
        return this.rocksDBConsumeQueueTable.binarySearchInCQByTime(topic, queueId, high, low, timestamp,
            minPhysicOffset, boundaryType);
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;與基於文件的實現不同的是，由於 RocksDB 的 CqUnit 裏保存了消息存儲的時間，比較時間戳時不必再讀取 CommitLog 獲取消息的存儲時間，這樣提升了查找的時間效率。&lt;/p&gt; 
&lt;h2&gt;總結及展望&lt;/h2&gt; 
&lt;p&gt;本文和讀者分享了 ConsumeQueue 的設計與實現，着重介紹其在消息消費場景的應用。鑑於篇幅限制，仍有許多細節未涉及，比如 ConsumeQueue 的容錯恢復、過期清理機制等。近些年，RocketMQ 往 Serveless 化方向發展，在 5.0 的架構裏，已經將計算和存儲分離，Proxy 作為計算集羣，Broker 作為存儲集羣。從實際應用上來講，Broker 作為存儲角色，從計算的角色釋放出來之後，多出的性能和資源應該用於承載更多的 Topic，而基於文件的 ConsumeQueue 實現限制了 Broker 的上限，因此我們需要 RocksDB 的實現方案來解決這個問題。&lt;/p&gt; 
&lt;p&gt;目前，騰訊雲的 TDMQ RabbitMQ Serveless、MQTT 產品均基於 RocketMQ 5.0 的架構部署運行，Broker 集羣已採用 RocksDB 的方案支持百萬級的 Topic 隊列，滿足 RabbitMQ 和 MQTT 協議需要大量 Topic 支持的場景。在騰訊雲 RocketMQ 5.0 的產品上，我們開始逐漸在新版本中灰度開啓該方案，為客戶提供更好性能更穩定的消息隊列服務。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/4587289/blog/18499646</link>
      <guid isPermaLink="false">https://my.oschina.net/u/4587289/blog/18499646</guid>
      <pubDate>Sun, 03 Aug 2025 02:23:00 GMT</pubDate>
      <author>原創</author>
    </item>
    <item>
      <title>阿里通義發佈開源文生圖模型 Qwen-Image</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;阿里通義千問團隊開源了其首個圖像生成基礎模型 Qwen-Image。該模型是一個擁有 200 億參數的 MMDiT（多模態擴散 Transformer）模型，基於 Apache 2.0 許可證開源。&lt;/p&gt; 
&lt;p&gt;Qwen-Image 在複雜文本渲染和精確圖像編輯方面取得了顯著進展，尤其在中文文本渲染上表現卓越。&lt;/p&gt; 
&lt;p&gt;Qwen-Image 的主要特性包括：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;卓越的文本渲染能力:&amp;nbsp;Qwen-Image 在複雜文本渲染方面表現出色，支持多行佈局、段落級文本生成以及細粒度細節呈現。無論是英語還是中文，均能實現高保真輸出。&lt;/li&gt; 
 &lt;li&gt;一致性的圖像編輯能力:&amp;nbsp;通過增強的多任務訓練範式，Qwen-Image 在編輯過程中能出色地保持編輯的一致性。&lt;/li&gt; 
 &lt;li&gt;強大的跨基準性能表現:&amp;nbsp;在多個公開基準測試中的評估表明，Qwen-Image 在各類生成與編輯任務中均獲得 SOTA，是一個強大的圖像生成基礎模型。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;如需體驗 Qwen-Image，訪問 QwenChat（chat.qwen.ai) 並選擇「圖像生成」功能。同時該模型已在魔搭社區與 Hugging Face 開源。&lt;/p&gt; 
&lt;p&gt;ModelScope：https://modelscope.cn/models/Qwen/Qwen-Image&lt;br&gt; Hugging Face：https://huggingface.co/Qwen/Qwen-Image&lt;br&gt; GitHub：https://github.com/QwenLM/Qwen-Image&lt;br&gt; Technical report：https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/Qwen_Image.pdf&lt;br&gt; Demo:&amp;nbsp;https://modelscope.cn/aigc/imageGeneration?tab=advanced&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;示例展示&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;宮崎駿的動漫風格。平視角拍攝，陽光下的古街熱鬧非凡。一個穿着青衫、手裏拿着寫着「阿里雲」卡片的逍遙派弟子站在中間。旁邊兩個小孩驚訝的看着他。左邊有一家店鋪掛着「雲存儲」的牌子，裏面擺放着發光的服務器機箱，門口兩個侍衞守護者。右邊有兩家店鋪，其中一家掛着「雲計算」的牌子，一個穿着旗袍的美麗女子正看着裏面閃閃發光的電腦屏幕；另一家店鋪掛着「雲模型」的牌子，門口放着一個大酒缸，上面寫着「千問」，一位老闆娘正在往裏面倒發光的代碼溶液。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0805/101844_6QPa_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364222</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364222</guid>
      <pubDate>Sun, 03 Aug 2025 02:18:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>KiCad - 開源免費電子設計自動化（EDA）套件</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                        
                                                                                    &lt;p style="color:#000000; margin-left:auto; margin-right:auto; text-align:start"&gt;KiCad EDA 是一款開源的電子設計自動化（EDA）軟件，基於 GPLv3 開源協議，最初由法國人 Jean-Pierre Charras 於 1992 年推出，現由 KiCad 開源社區維護。&lt;/p&gt;

&lt;p style="color:#000000; margin-left:auto; margin-right:auto; text-align:start"&gt;KiCad 提供了一個完整的設計流程，從原理圖到 PCB 佈局，以及 3D 模型和 BOM 生成。KiCad 支持多種文件格式，可以與其他 EDA 軟件兼容，並且可以在多種操作系統上運行，包括 Windows，Linux 和 Mac OS X，軟件包含工程項目管理、原理圖設計、線路板繪製、符號庫設計、封裝庫設計、線路板 3D 顯示、Gerber 查看、線路板實用計算等工具。&lt;/p&gt;

&lt;p style="color:#000000; margin-left:auto; margin-right:auto; text-align:start"&gt;&lt;img alt="" height="1080" src="https://oscimg.oschina.net/oscnet/up-416b95962f155bd5c9c0b3172706d97017f.png" width="1920" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

&lt;p style="color:#000000; margin-left:auto; margin-right:auto; text-align:start"&gt;&lt;img alt="" height="1080" src="https://oscimg.oschina.net/oscnet/up-a671e278e4aae67095a053103e543bc62a3.png" width="1920" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

&lt;p style="color:#000000; margin-left:auto; margin-right:auto; text-align:start"&gt;&lt;img alt="" height="1080" src="https://oscimg.oschina.net/oscnet/up-6501cd15c8e91e73f511b9edeaeed9ac221.png" width="1920" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

&lt;p&gt;KiCad EDA 官網：&lt;a href="https://www.kicad.org/"&gt;https://www.kicad.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;KiCad EDA 開源中國&amp;nbsp;&lt;a href="https://gitee.com/kicad-eda"&gt;https://gitee.com/kicad-eda&lt;/a&gt;&lt;/p&gt;

                                                                    &lt;/div&gt;
                                                                </description>
      <link>https://www.oschina.net/p/kicad</link>
      <guid isPermaLink="false">https://www.oschina.net/p/kicad</guid>
      <pubDate>Sun, 03 Aug 2025 02:11:00 GMT</pubDate>
    </item>
    <item>
      <title>​Perplexity AI 被指控祕密抓取被禁止的網站內容</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p style="margin-left:0; margin-right:0"&gt;根據互聯網基礎設施提供商 Cloudflare 的&lt;span&gt;最新&lt;/span&gt;研究&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.cloudflare.com%2Fperplexity-is-using-stealth-undeclared-crawlers-to-evade-website-no-crawl-directives%2F" target="_blank"&gt;報告&lt;/a&gt;，人工智能初創公司 Perplexity 被指控在抓取網站內容時忽視了明確的阻止指令。Cloudflare 表示，他們觀察到 Perplexity 在嘗試抓取網頁時隱藏了自己的身份，以此規避網站的偏好設置。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&lt;img height="312" src="https://oscimg.oschina.net/oscnet/up-cc5a99ff2f39168f9451614222dc2d73118.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Perplexity 等人工智能產品通常依賴於從互聯網收集大量數據，而這些初創公司長期以來在未獲得許可的情況下抓取文本、圖像和視頻，以便支持其產品的正常運作。近年來，許多網站通過使用標準的 Robots.txt 文件來應對這一問題，該文件指示搜索引擎和 AI 公司哪些頁面可以被索引，哪些頁面不可以。然而，當前這些努力的成效並不顯著。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;根據 Cloudflare 的分析，Perplexity 似乎通過更改其機器人的 「用戶代理」 來繞過這些限制。「用戶代理」 是指用於識別網站訪問者的設備和版本類型的信號。Cloudflare 還提到，Perplexity 更改了其自治系統網絡（ASN），這是一個識別互聯網上大型網絡的數字標識。Cloudflare 在數萬個域名和數百萬個請求中觀察到了這一行為，憑藉機器學習和網絡信號的結合成功識別了這一爬蟲。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Perplexity 的發言人 Jesse Dwyer 對 Cloudflare 的指控表示反駁，並稱其博客文章為 「推銷」。他補充稱，文中截圖顯示並沒有訪問內容。他進一步聲稱，Cloudflare 所提到的爬蟲並非其所擁有的。Cloudflare 表示，他們最初注意到這些問題是由於客戶投訴 Perplexity 仍在抓取其網站內容，儘管這些網站已通過 Robots 文件阻止了該爬蟲的訪問。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Cloudflare 的分析表明，Perplexity 不僅使用了其聲明的用戶代理，還在其被阻止時利用一個模擬 Google Chrome 的通用瀏覽器。最終，Cloudflare 決定將 Perplexity 的爬蟲從其驗證列表中移除，並採取新的技術來阻止其活動。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;值得注意的是，Cloudflare 最近對人工智能爬蟲表示反對，並推出了一個市場，允許網站所有者向訪問其網站的 AI 爬蟲收費。Cloudflare 的首席執行官馬修・普林斯曾警告稱，人工智能正在破壞互聯網的商業模式，尤其是出版商的盈利模式。這並非 Perplexity&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;第一&lt;/span&gt;次面臨未經授權抓取的指控，早在去年，《連線》雜誌等媒體就曾指控 Perplexity 抄襲其內容。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364217</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364217</guid>
      <pubDate>Sun, 03 Aug 2025 01:59:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>「香山」實現業界首個開源芯片的產品級交付與首次規模化應用</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 開源高性能 RISC-V 處理器核「香山」產業落地取得里程碑式突破。7 月 16-19 日，在上海舉辦的 2025 RISC-V 中國峯會期間，北京開源芯片研究院（以下簡稱開芯院）在大會報告中宣佈第三代「香山」（昆明湖）IP 核已實現了首批量產客戶的產品級交付。7 月 26-28 日，世界人工智能大會期間，集成了第二代「香山」（南湖）IP 核的某國產量產 GPGPU 芯片正式亮相，基於該芯片的智能加速卡出貨量已上萬——「香山」（南湖）IP 核實現規模化應用。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 香山 IP 核在業界首次實現了產品級交付與規模化應用，標誌着開源高性能處理器 IP 核正式進入產業落地階段，為 RISC-V 產業技術研發、商業落地開闢了一條不同於傳統 ARM 模式、基於開源模式的新路徑。香山 IP 核的首次產品級交付與規模化應用，就如 1990 年代中期開源操作系統 Linux 首次在企業中部署應用，具有重要的里程碑意義，必將產生深遠影響。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:center"&gt;&lt;strong&gt;開源高性能 RISC-V 處理器核&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:center"&gt;&lt;strong&gt;第二代「香山」（南湖）已實現首次規模化應用&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 開源高性能 RISC-V 處理器核「香山」，源於中國科學院在 2019 年的前瞻佈局。中國科學院計算技術研究所（以下簡稱計算所）於 2021 年 6 月成功研製了第一代開源高性能 RISC-V 處理器核「香山（雁棲湖）」，性能對標 ARM A73，SPECINT2006 7 分/GHz，是同期全球性能最高的開源處理器核。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 為了加速香山的技術演進和應用落地，加快 RISC-V 生態建設, 2021 年北京市與中國科學院達成戰略合作，發揮北京市應用牽引和芯片定義的優勢，組織 18 家行業龍頭企業和國內頂尖科研單位共同發起成立開芯院。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 2023 年 5 月 26 日，開芯院在中關村論壇上正式對外發布由多家單位聯合開發的第二代「香山」（南湖）。這是一款性能對標 ARM Cortex-A76 的高性能開源 RISC-V 處理器核，主頻 2GHz@14nm，SPECCPU2006 分值達到 10 分/GHz，專門針對工業控制、汽車、通信等泛工業領域。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 「香山」（南湖）成功帶動一批企業加速佈局 RISC-V 產品線，開始在一些芯片產品中集成「香山」（南湖）IP 核，取得積極成效。近日，在上海舉辦的世界人工智能大會上，某國產 GPU 芯片廠商展示的自研智算加速卡中成功集成了「香山」（南湖）IP 核。據瞭解，該國產 GPU 公司已經實現全國產千卡千億模型算力集羣的交付，正朝着萬卡智算集羣加速迭代。這標誌着「香山」（南湖）IP 核首次實現規模化應用，也推動了 RISC-V 在人工智能智算集羣中的產業化落地。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 「香山」(南湖)IP 核作為高性能片內主控 CPU，也被用於芯動科技高性能全功能 GPGPU 芯片「風華 3 號"中,實現 CPU+AI 強強聯手。其中，香山 CPU 核負責從主 CPU 卸載的一些關鍵功能，包括處理跨芯片通訊與數據搬運、啓動控制及片內 IP 配置、實現低功耗與動態功耗控制、確保系統穩定運行、提供異常處理能力等。香山 CPU 核與高性能風華 GPU 的結合，在重度負載渲染、高性能 AI 計算、多芯片集羣互聯等使用場景中能發揮各自的優勢，提供了高性能低功耗、靈活定製和成本效益的處理器的解決方案。據悉，該產品即將面市。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:center"&gt;&lt;strong&gt;基於開源模式聯合研發&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:center"&gt;&lt;strong&gt;第三代「香山」（昆明湖）&lt;/strong&gt;&lt;strong&gt;已&lt;/strong&gt;&lt;strong&gt;實現產品級交付&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 2022 年 8 月，開芯院牽頭聯合計算所、騰訊、阿里、中興通訊、中科創達、奕斯偉、算能等形成了聯合研發團隊，在全球首次採用基於開源的處理器核聯合研發模式，共同研製第三代「香山」（昆明湖）開源高性能 RISC-V 處理器核，性能對標 ARM N2。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 2024 年 4 月，「香山」（昆明湖）正式發佈，SPECCPU2006 分值達到 15 分/GHz，符合 RVA23 標準，性能進入全球 RISC-V 處理器第一梯隊。同時，「香山」開源芯片項目在全球最大的開源項目託管平台 GitHub 上獲得超過 6500 個星標（Star），形成超過 780 個分支（Fork），遠超其他開源硬件項目，成為國際開源社區性能最強、最活躍的 RISC-V 處理器核。「香山」及其敏捷開發基礎設施入選「計算機體系結構領域年度全球十二大亮點成果」，連續兩年入選「2024 中關村論壇 10 項重大科技成果」和 2025 中關村論壇「北京重大開源成果」。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 在發佈後的一年多時間中，開芯院聯合企業共同完成了針對「香山」（昆明湖）的產品級驗證工作，包括按規模量產芯片企業要求構建了一套嚴格的測試驗證流程，形成了「單元級測試 UT集成級測試 IT系統測試 ST原型系統測試 Prototype」四個層次的驗證規範，開發了超過 2 萬個測試用例，建立了一套包含數十個商業工具、開源工具、形式化工具等驗證工具箱等等。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 實踐表明，基於開源的聯合開發模式，平攤了驗證成本，提升了驗證效率——用戶/企業貢獻了近 1600 個測試用例，發現的 1470 項 BUG 中企業累計提交了 492 項。通過開芯院與多家企業的共同努力， 「香山」（昆明湖）最終實現了驗證覆蓋率近 100%，同時大幅降低了企業獲得性能對標 ARM N2 的產品級 RISC-V IP 核的成本。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 目前，「香山」（昆明湖）已實現首批量產客戶的產品級交付。進迭時空正基於「香山」（昆明湖）自研 X200 核，並研發其第三代旗艦 RISC-V AI CPU 芯片，預計 2026 年底進入量產。同時，進迭時空研發的首款 RISC-V 服務器芯片將於近期流片，其中內置 6 個「香山」（昆明湖）核。在雙方團隊的努力下，進迭時空服務器芯片已在 FPGA 平台上穩定地運行 Linux 操作系統及虛擬機。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364198</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364198</guid>
      <pubDate>Sun, 03 Aug 2025 01:06:00 GMT</pubDate>
      <author>作者: 開源科技</author>
    </item>
  </channel>
</rss>
