<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - industry - 繁體中文（台灣）</title>
    <link>https://www.oschina.net/news/industry</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news/industry" rel="self" type="application/rss+xml"/>
    <description>已對該 RSS 進行格式化操作：中英字符之間插入空格、使用直角引號、標點符號修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-tw</language>
    <lastBuildDate>Wed, 13 Aug 2025 02:40:44 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>阿里通義升級 Qwen Chat 的 Deep Research （深入研究）功能</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;阿里通義 Qwen 團隊宣佈對 Qwen Chat 的 Deep Research 能力進行了升級。此次升級旨在提供更智能、更具洞察力的研究報告。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1200" src="https://static.oschina.net/uploads/space/2025/0813/102724_zT9i_2720166.png" width="900" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;新版本通過更深入的搜索來獲取更豐富的研究發現，並提高了信息準確性以減少幻覺現象。技術上，新版本支持模塊化工具和並行執行。&lt;/p&gt; 
&lt;p&gt;此外，一個重要的新增功能是多模態輸入支持，允許用戶上傳文件和圖像進行研究。用戶可通過官方鏈接體驗此項新功能。&lt;/p&gt; 
&lt;p&gt;https://chat.qwen.ai/?inputFeature=deep_research&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365895</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365895</guid>
      <pubDate>Wed, 13 Aug 2025 02:31:40 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>騰訊混元發佈 52B 參數多模態理解模型 Large-Vision</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;騰訊混元團隊近日&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FgjZygQA9mRm-fYLWa1YqMA" target="_blank"&gt;發佈&lt;/a&gt;了全新的多模態理解模型——混元 Large-Vision，該模型採用騰訊混元擅長的 MoE（專家混合）架構，激活參數達到 52B 規模，在性能與效率之間實現了良好平衡。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="314" src="https://oscimg.oschina.net/oscnet/up-c4400b13dcd221f1bca8f7c1cfe0d3ae958.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="221" src="https://oscimg.oschina.net/oscnet/up-e4ccecb8e928aa25ac259f743c1c80510d1.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;混元 Large-Vision 的核心亮點在於其強大的多模態輸入支持能力。該模型不僅支持任意分辨率的圖像處理，還能處理視頻和 3D 空間輸入，為用戶提供了全方位的視覺理解體驗。這一技術突破意味着用戶可以直接輸入各種格式和尺寸的視覺內容，無需進行復雜的預處理操作。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;MoE 架構通過動態激活部分專家網絡來處理不同類型的輸入，既保證了模型的強大性能，又避免了全參數激活帶來的計算資源浪費。52B 的激活參數規模在當前多模態模型中處於先進水平，能夠處理複雜的視覺理解任務。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;該模型還重點提升了多語言場景理解能力，這對於全球化應用具有重要意義。在處理包含多種語言文字的圖像或視頻時，混元 Large-Vision 能夠準確識別和理解不同語言環境下的視覺內容，為跨語言的多模態應用提供了技術基礎。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;混元 Large-Vision 支持任意分辨率圖像輸入的特性尤其值得關注。傳統的視覺模型往往需要將輸入圖像調整到固定尺寸，這可能導致信息丟失或畫質下降。而混元 Large-Vision 能夠直接處理原始分辨率的圖像，保持了視覺信息的完整性，這對於需要精細視覺分析的應用場景具有重要價值。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;3D 空間輸入支持則進一步擴展了模型的應用範圍，為虛擬現實、增強現實、3D 建模等領域的 AI 應用提供了強有力的技術支撐。結合視頻處理能力，該模型有望在智能監控、視頻分析、內容創作等多個行業發揮重要作用。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365893</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365893</guid>
      <pubDate>Wed, 13 Aug 2025 02:27:40 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>崑崙萬維開源 Skywork UniPic 2.0</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"&gt;崑崙萬維&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FCN8NaVnxsqxFe6WF_eo2iA" target="_blank"&gt;宣佈&lt;/a&gt;正式開源 Skywork UniPic 2.0 模型——面向統一多模態建模的高效訓練和推理框架，圍繞生成和編輯模塊輕量化、連接多模態理解模型進行聯合訓練，構建了理解、生圖、編輯一體化的核心能力，旨在實現「高效、高質、統一」的多模態生成模型。&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"&gt;目前，Skywork UniPic 2.0 及其系列模型已全面開源，涵蓋模型權重、推理代碼、強化策略等。&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"&gt;Skywork UniPic 2.0 由三個核心模塊組成：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;生圖編輯（下圖中）：&lt;/strong&gt;基於 SD3.5-Medium 架構將原本只支持文本輸入的模型改進成也接受文本圖像同時輸入，然後通過高質量圖像生成和編輯數據的訓練將原本生圖能力擴展成生圖、編輯雙能力。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;統一模型能力（下圖左側與中間）：&lt;/strong&gt;通過凍結生圖編輯模塊，多模態模型（Qwen2.5-VL-7B），Pre-Train 連接器來構建出理解生成編輯一體化能力，再通過連接器和生圖編輯模塊一起聯合微調，實現最終的一體化理解、生圖、編輯模型。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;生圖編輯後訓練（下圖右）：&lt;/strong&gt;為提升生圖編輯整體性能，設計了基於 Flow-GRPO 的漸進式雙任務強化策略，實現了生成與編輯任務在不互相干擾下的協同優化，在預訓練的基礎上進一步提升了模型性能。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img height="203" src="https://oscimg.oschina.net/oscnet/up-67f45853462faa0c1bce2a535fd0a260701.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"&gt;Skywork UniPic 2.0 的核心優勢包括有：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;生成模塊輕量高效，性能拉滿&amp;nbsp;&lt;/strong&gt;生成模塊基於 2B 參數的 SD3.5-Medium 架構訓練，生圖和編輯指標超越生成模塊具有 7B 參數的 bagel，4B 參數的 OmniGen2，12B 參數的 UniWorld-V1 和 Flux-kontext 模型。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;引入強化學習，效果顯著&amp;nbsp;&lt;/strong&gt;基於 Flow-GRPO 首創漸進式雙任務強化策略，有效提升模型對複雜指令的理解能力與圖像生成和編輯的一致性，兩大任務協同優化、互不幹擾。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;一體化靈活切換，拓展能力強&amp;nbsp;&lt;/strong&gt;將生圖編輯的 Kontext 模型與多模態模型端到端整合，微調輕量連接器，即可快速構建統一理解-生成-編輯模型，並且生圖和編輯的性能進一步提升。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;更多詳情可&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FCN8NaVnxsqxFe6WF_eo2iA" target="_blank"&gt;查看官方公告&lt;/a&gt;。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365887</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365887</guid>
      <pubDate>Wed, 13 Aug 2025 02:07:40 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Fedora 43 獲準支持 Hare 編程語言，默認啓用硬鏈接</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Fedora 工程與指導委員會 (FESCo) 本週批准了即將發佈的 Fedora Linux 43 版本的多項新增功能。其中包括獲準發佈 Hare 軟件包，Hare 是一種新的系統編程語言，旨在簡化、穩定和健壯。&lt;/p&gt; 
&lt;p&gt;Hare 本身仍在開發中，但 FESCo 現已批准將 Hare 工具鏈打包併發布到 Fedora 43 的倉庫中。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0812/191447_Wl7H_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;FESCo 還批准在 Fedora 43 中發佈即將發佈的 PHP 8.4 版本，這並不令人意外。FESCo 還批准棄用 YASM，轉而使用 NASM。YASM 彙編器目前無人維護，而 NASM 的情況也好多了。&lt;/p&gt; 
&lt;p&gt;作為英特爾 oneAPI 線程構建版本 (TBB) 的最新更新，Threaded Building Blocks 2022.2 也已獲批准發佈。FESCo 本週還批准了默認對 Fedora RPM 軟件包中，相同的 /usr 文件進行硬鏈接的提案。&lt;/p&gt; 
&lt;p&gt;有關 Fedora 43 版本中這些新批准更改的更多詳細信息，&lt;u&gt;&lt;em&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flists.fedoraproject.org%2Farchives%2Flist%2Fdevel%40lists.fedoraproject.org%2Fthread%2FMVPWNTBSZUUJINZX6PZQGTYE2BA7NFKL%2F" target="_blank"&gt;請通過此 FESCo 郵件列表帖子獲取&lt;/a&gt;&lt;/em&gt;&lt;/u&gt;，該版本將於今年晚些時候發佈。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365798</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365798</guid>
      <pubDate>Mon, 11 Aug 2025 11:15:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>One Million Screenshots：收集了超過 100 萬張網站截圖的網站</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;「One Million Screenshots」 是一個專門收集網站截圖的網站，聲稱截圖了超過 100 萬個熱門 Web 主頁。此外還提供了搜索相似網站的功能，以及查看網站截圖的歷史變化。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1856" src="https://static.oschina.net/uploads/space/2025/0812/185333_PgpB_2720166.png" width="3360" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;https://onemillionscreenshots.com/&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;沒想到 OSCHINA 也榮幸出鏡了：&lt;em&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fonemillionscreenshots.com%2Foschina.net%2Fscreenshot" target="_blank"&gt;https://onemillionscreenshots.com/oschina.net/screenshot&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="6306" src="https://static.oschina.net/uploads/space/2025/0812/185758_Ydik_2720166.png" width="1604" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;下面是關於該網站的常見問題：&lt;/p&gt; 
&lt;p&gt;&lt;img height="2386" src="https://static.oschina.net/uploads/space/2025/0812/185221_4Hkz_2720166.png" width="1514" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365794</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365794</guid>
      <pubDate>Mon, 11 Aug 2025 11:00:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>人工智能正在降低知識的價值，大學應該重新考慮所教授的內容？</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;生成式人工智能，尤其是大型語言模型（LLM）的興起，正以前所未有的速度改變知識獲取的格局。奧克蘭大學商學院教授帕特里克·多德在《對話》(The Conversation) 上撰文指出，隨着 AI 以低成本、高效率的方式提供知識，大學作為傳統知識來源的價值正在受到挑戰。他認為，大學必須重新審視其核心功能，以適應這個由 AI 驅動的新時代。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;多德教授分析，大學長期以來奉行「知識稀缺」的原則，通過提供獨家課程和學位證書來證明學生獲取知識的能力。然而，AI 技術的進步已大大降低了獲取專業知識的門檻，LLM 不僅能檢索事實，還能進行解釋、翻譯和總結，使得曾經「稀缺」的知識價值大打折扣。這種變化已經在勞動力市場顯現，自 ChatGPT 問世以來，英國入門級職位空缺減少了約三分之一，美國部分州甚至取消了公共部門職位的學位要求。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;然而，多德強調，並非所有知識都同等貶值。雖然基礎知識的價值下降，但&lt;strong&gt;隱性知識&lt;/strong&gt;，如團隊協作、倫理判斷、創造力以及解決複雜問題的能力，仍是 AI 無法取代的稀缺資源。他指出，未來教育的重點應從傳授信息轉向培養這些關鍵的人類技能。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;為應對這一挑戰，多德教授為大學提出了四項轉型建議：&lt;/span&gt;&lt;/p&gt; 
&lt;ol style="margin-left:0; margin-right:0"&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;評估轉型&lt;/strong&gt;：將課堂評估重點從單純的知識記憶轉向&lt;strong&gt;判斷和綜合能力&lt;/strong&gt;的考察。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;體驗式學習&lt;/strong&gt;：投入資源開發導師指導項目、模擬現實場景，並利用 AI 作為工具進行倫理決策研究。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;技能微證書&lt;/strong&gt;：創建針對協作、自主學習和倫理判斷等關鍵能力的微證書。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;深化產學研合作&lt;/strong&gt;：大學提供專業知識，企業提供真實案例，學生則專注於驗證和完善想法，共同培養適應未來市場的複合型人才。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;多德總結道，如果大學想要在未來立於不敗之地，就必須從一個單純的&lt;strong&gt;信息來源&lt;/strong&gt;轉變為一個&lt;strong&gt;判斷力中心&lt;/strong&gt;，教會學生如何與 AI 協同思考，而非與之競爭。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365792</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365792</guid>
      <pubDate>Mon, 11 Aug 2025 10:40:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>開源編程字體「Hack」創始人 Christopher Simpkins 去世</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Christopher Eric Simpkins 是知名開源編程字體「Hack」創始人，他於 2025 年 6 月 20 日在新罕布什爾州漢諾威突然&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftypo.social%2F%40Hilary%2F114845913381245488" target="_blank"&gt;去世&lt;/a&gt;，享年 51 歲。&lt;/p&gt; 
&lt;p&gt;&lt;img height="904" src="https://static.oschina.net/uploads/space/2025/0812/175131_lS6n_2720166.png" width="1150" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Christopher Simpkins 訃告頁面&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.legacy.com%2Fus%2Fobituaries%2Fvnews%2Fname%2Fchristopher-simpkins-obituary%3Fid%3D58856786" target="_blank"&gt;顯示&lt;/a&gt;，他&lt;span&gt;在佐治亞理工學院取得計算機博士學位後先在美軍服役，退役又完成醫學訓練成為一名器官移植外科醫生&lt;/span&gt;。他醫術精湛、待人溫和，被譽為「溫柔的巨人」，拯救了許多生命並屢獲教學獎。&lt;/p&gt; 
&lt;p&gt;後來他轉向科技領域，&lt;span&gt;加入 Google Fonts 團隊任&lt;/span&gt;高級用戶體驗項目經理&lt;span&gt;，&lt;/span&gt;專注字體開發&lt;span&gt;，發起 Codeface 項目為開發者整理並推薦高可讀性的編程字體，持續推動開源字體生態。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0812/175331_L0CQ_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;2015 年，&lt;/span&gt;Christopher Simpkins 創造了&lt;span&gt;開源 Hack 字體，這款基於 DejaVu Sans Mono 重新調校的等寬字體迅速成為程序員最喜愛的編輯器字體之一。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;谷歌近期發佈的開源字體&lt;/span&gt;&lt;a href="https://www.oschina.net/news/363609/googlesans-code" target="_blank"&gt;&amp;nbsp;Google Sans Code &lt;/a&gt;正是由&amp;nbsp;Christopher Simpkins 負責主導。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1710" src="https://static.oschina.net/uploads/space/2025/0812/180516_SXlX_2720166.png" width="1686" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365788</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365788</guid>
      <pubDate>Mon, 11 Aug 2025 10:07:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>英偉達推出 Cosmos 與 Nemotron 模型，推動物理 AI 與智能體發展</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblogs.nvidia.cn%2Fblog%2Fnvidia-opens-portals-to-world-of-robotics-with-new-omniverse-libraries-cosmos-physical-ai-models-and-ai-computing-infrastructure%2F" target="_blank"&gt;據英偉達官方消息&lt;/a&gt;，英偉達在技術領域再推新進展。其推出的 NVIDIA Cosmos 平台，整合前沿生成式世界基礎模型（WFM）、先進分詞器、護欄以及高效數據處理和管理工作流，旨在加速物理 AI 開發。該平台的世界基礎模型經 2000 萬小時真實世界數據訓練，能預測和生成虛擬環境未來狀態，助力開發者構建新一代機器人和自動駕駛汽車。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0812/174129_nIuV_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;同時，英偉達宣佈推出 Nemotron 模型系列。Llama Nemotron 基於熱門開源模型 Llama 構建，經剪枝和訓練，在指令遵循等方面表現出色，能為 AI 智能體開發提供優化基礎模組。Cosmos Nemotron 視覺語言模型（VLM）則可助力開發者構建智能體，使其能分析圖像和視頻並做出響應。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-9ddc7846bd0a958a9b0a9772dcf6c6a4e47.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;目前，已有眾多物理 AI 領域的領先者，如機器人公司，以及自動駕駛汽車開發商等開始與 Cosmos 協作，加速模型開發進程。開發者可在 NVIDIA API 目錄預覽相關模型，並從 NGC 目錄和 Hugging Face 下載模型系列與微調框架。&lt;/p&gt; 
&lt;p&gt;https://docs.nvidia.com/cosmos/&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365780</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365780</guid>
      <pubDate>Mon, 11 Aug 2025 09:41:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Altman：計劃在未來 5 個月內將算力集羣擴容一倍</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;OpenAI CEO 薩姆・奧爾特曼（SamAltman）在社交平台發文上表示，鑑於 GPT-5 帶來的需求激增，該公司計劃在未來幾個月的算力優先分配如下：&lt;/span&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;首先確保當前付費版 ChatGPT 用戶的總可用量比 GPT-5 推出前更多。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;其次優先滿足 API 需求，直至達到當前分配的產能和已對客戶做出的承諾。（粗略估計，以現有產能可在當前基礎上再支持約 30% 的新 API 增長。）&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;提升 ChatGPT 免費版的服務質量。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;再優先滿足新的 API 需求。計劃在未來 5 個月內將算力集羣擴容一倍，因此這一情況有望改善。&lt;/span&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="450" src="https://oscimg.oschina.net/oscnet/up-546ae50cc300f2af8894d1b266b905551e4.png" width="300" referrerpolicy="no-referrer"&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365777</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365777</guid>
      <pubDate>Mon, 11 Aug 2025 09:39:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>OpenAI 發佈面向 GPT-5 的 Prompt 指南</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;OpenAI 官方寫的 GPT-5 prompt 指南來了，看看官方是怎麼讓 GPT-5 表現更好的。該指南融匯貫通後，還可用於其他 AI 大模型。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0812/172857_F753_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;1、 明確角色和目標 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;開頭就讓 AI 模型知道它是誰、要做什麼，比如：&lt;/p&gt; 
&lt;p&gt;你是資深前端工程師，請幫我在現有 React 項目裏實現...&lt;/p&gt; 
&lt;p&gt;2、 設定工作方式 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;用分步指令，讓模型按既定節奏走，而非一次性輸出：&lt;/p&gt; 
&lt;p&gt;- 先分析需求和不確定點&lt;br&gt; - 再給執行計劃 &amp;nbsp; &amp;nbsp;&lt;br&gt; - 按計劃分步完成&lt;br&gt; - 每步結束時總結進度&lt;/p&gt; 
&lt;p&gt;3、 控制主動性 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;想要它多動腦，就加：在不確定時自行推斷並執行，完成後再告知用戶。 &amp;nbsp;&lt;br&gt; 想讓它少跑偏，就加：僅按已知信息執行，不額外探索。&lt;/p&gt; 
&lt;p&gt;4、 給出完成標準 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;告訴模型何時算任務完成，比如： &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;當所有代碼改動已在/app/theme 目錄生效，並通過現有測試時，結束任務。&lt;/p&gt; 
&lt;p&gt;5、 嵌入風格與規範 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;在提示裏放工程或寫作規範，讓它自動匹配你的需求： &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;變量用駝峯命名，CSS 類名用 BEM 規範，註釋保持英文簡短描述。&lt;/p&gt; 
&lt;p&gt;6、 善用示例 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;給它 1-2 個高質量示例，讓它照着學，比空口説效果好得多。&lt;/p&gt; 
&lt;p&gt;7、 善用「工具前言」&lt;/p&gt; 
&lt;p&gt;工具前言可以寫：先複述目標，再列計劃，執行時簡短説明當前步驟，最後單獨總結成果。&lt;/p&gt; 
&lt;p&gt;8、 清除歧義 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;檢查提示裏是否有前後矛盾或模糊指令，否則 GPT-5 會花大量精力試圖自圓其説，反而降低效率。&lt;/p&gt; 
&lt;p&gt;記住一個公式：角色+目標+步驟+完成標準+風格+示例，如此 GPT-5 才會既有創造力又不跑偏。&lt;/p&gt; 
&lt;p&gt;這本指南還涵蓋了 API 參數具體怎麼調，感興趣的開發者可以看看。&lt;/p&gt; 
&lt;p&gt;地址：cookbook.openai.com/examples/gpt-5/gpt-5_prompting_guide&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365773/openai-gpt-5-prompting-guide</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365773/openai-gpt-5-prompting-guide</guid>
      <pubDate>Mon, 11 Aug 2025 09:29:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>字節跳動推出視頻字幕無痕擦除方案，基於 DiT 大模型打造</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;字節跳動技術團隊&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FKsl_lF8KNwM0vRtsjzWaBA" target="_blank"&gt;宣佈&lt;/a&gt;推出一項創新技術，基於 DiT 大模型與字體級分割的視頻字幕無痕擦除方案，旨在助力短劇等視頻內容的全球化傳播。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在全球化內容製作中，原始視頻的中文字幕對於海外觀眾而言不僅是無效信息，還嚴重影響觀看體驗。傳統的字幕添加或馬賽克、GAN（生成對抗網絡）等字幕擦除方案，往往導致畫面雜亂、模糊或幀間閃爍，無法徹底解決這一問題。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;火山引擎視頻點播推出的這一方案，通過兩大核心技術突破和強大的工程能力，重新定義了字幕擦除標準，實現了全片真實自然的「無痕擦除」，並支持多字幕框、指定時間段的精準擦除。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="262" src="https://oscimg.oschina.net/oscnet/up-e6a7ee75485165b360e216e57f4f3f2e85f.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;該方案的核心在於兩個技術突破：一是 DiT 視頻字幕擦除模型，二是字體級分割模型。DiT 模型通過強魯棒性預訓練基底、擺脫輔助先驗依賴、兩階段訓練策略提升魯棒性與修復精細度，實現了像素級無痕修復。字體級分割模型則通過精準定位目標區域，實現了從「粗放擦除」到「像素級修復」的轉變，有效避免了傳統塊填充導致的背景模糊或紋理重複問題。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="143" src="https://oscimg.oschina.net/oscnet/up-dc217440e603a0e87eec97675dbaaf606fc.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;火山引擎多媒體實驗室聯合工程團隊構建了兼顧精度與效率的技術體系，經過超萬集視頻數據集驗證，擦除任務成功率達到 100%。創新的視頻分鏡技術結合服務器集羣分佈式計算，顯著提升了視頻處理效率。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;此外，該方案還支持多語言內容流轉，突破了中英文限制，支持多個小語種字幕擦除，為全球內容流轉提供了雙向通道。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365771</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365771</guid>
      <pubDate>Mon, 11 Aug 2025 09:23:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>谷歌向發現 Chrome 高危漏洞的安全研究員獎勵 25 萬美元</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span&gt;谷歌近日依據漏洞獎勵計劃（VRP）向一名安全研究員&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fissues.chromium.org%2Fissues%2F412578726" target="_blank"&gt;發放 25 萬美元（約合 179.8 萬元人民幣）獎金&lt;/a&gt;，獎勵其發現 Chrome 瀏覽器高危漏洞。 &lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0812/170559_YQ7o_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;該研究員於 4 月 23 日報告了一個「沙盒逃逸」漏洞，編號為 CVE-2025-4609，存在於 Chrome 內核的 IPCZ 通信系統中。攻擊者可通過誘導用戶訪問惡意網站，利用該漏洞突破瀏覽器沙箱限制，實現遠程代碼執行。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;儘管研究員最初將其標記為「中等危害」，但谷歌評估其嚴重性為 S0/S1 級，並列為 P1 優先級修復。 谷歌已於 5 月發佈的 Chrome 更新中修復該漏洞，並在 8 月 12 日公開披露細節。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;根據谷歌「漏洞獵人（&lt;/span&gt;Google Bug Hunters&lt;span&gt;）」計劃，提交包含 RCE 演示的高質量非沙盒進程逃逸或內存損壞漏洞報告，可獲 2.5 萬至 25 萬美元獎勵，此次為頂格獎勵。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365764</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365764</guid>
      <pubDate>Mon, 11 Aug 2025 09:06:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Actual - 個人理財工具</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                        
                                                                                    &lt;p&gt;&lt;span style="background-color:#ffffff; color:#1f2328"&gt;Actual 是一款本地優先的個人理財工具。它 100% 免費開源，使用 NodeJS 編寫，並具備同步功能，方便用戶在不同設備之間輕鬆遷移更改。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img height="271" src="https://static.oschina.net/uploads/space/2025/0806/154746_TajJ_4252687.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

                                                                    &lt;/div&gt;
                                                                </description>
      <link>https://www.oschina.net/p/actual</link>
      <guid isPermaLink="false">https://www.oschina.net/p/actual</guid>
      <pubDate>Mon, 11 Aug 2025 08:53:00 GMT</pubDate>
    </item>
    <item>
      <title>360 智腦推出 Light-IF 系列模型</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;360 智腦團隊&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FnwyQDZxYGFFA5pTmkxn3JQ" target="_blank"&gt;宣佈&lt;/a&gt;推出全新的 Light-IF 系列模型，這一創新框架旨在顯著提升大型語言模型（LLM）在複雜指令遵循方面的能力。隨着人工智能技術的不斷進步，儘管 LLM 在數學、編程等領域已經展現出了卓越的推理能力，但在遵循複雜指令方面仍存在不足。為瞭解決這一問題，360 智腦團隊提出了以預覽-自檢式推理和信息熵控制為核心的 Light-IF 框架。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;Light-IF 框架通過五個關鍵環節來提升模型性能:難度感知指令生成、Zero-RL 強化學習、推理模式提取與過濾、熵保持監督冷啓動、熵自適應正則強化學習。這一框架的提出，旨在破解當前推理模型中存在的「懶惰推理」現象，即模型在思考階段僅複述指令而不主動檢查約束是否被滿足，導致指令執行不準確的問題。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;img height="316" src="https://oscimg.oschina.net/oscnet/up-30ae24d430fc7fd7a393ecaf7c48fbadefc.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;在實驗中，Light-IF 系列模型在 SuperCLUE、IFEval、CFBench 及 IFBench 四個中文和跨語言指令遵循基準上均取得了顯著提升。特別是 32B 版本的 Light-IF-32B，其在 SuperClue 得分達到了 0.575，比下一個最佳模型高出 13.9 個百分點。此外，參數規模僅為 1.7B 的 Light-IF-1.7B 在 SuperClue 和 IFEval 上的表現甚至超過了 Qwen3-235B-A22B 等體量更大的模型。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;360 智腦團隊表示，Light-IF 系列模型的推出，不僅為開源社區提供了一套可復現的完整路線和配套的開源代碼，而且全系模型將陸續開放，供社區使用、對比與復現。同時，訓練中使用的冷啓動數據集也將同步開放。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;此外，360 與 SuperCLUE 聯合推出的中文精確指令遵循測評基準 SuperCLUE-CPIFOpen 也將在 Github 上開放，便於研究者評測模型的中文精確指令遵循能力。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365748</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365748</guid>
      <pubDate>Mon, 11 Aug 2025 08:31:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>MiniMax 發佈全球首個可交易 Agent Remix Marketplace</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;MiniMax 稀宇科技宣佈推出全球首個 Agent Remix Marketplace，並啓動了一項獎金高達 15 萬美金的全球挑戰賽。這一創新平台旨在將個人的想法轉化為商業價值，讓每個人都能成為「個體 GDP 創造者」。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;Agent Remix Marketplace 是一個允許用戶一鍵提效的工具，用戶可以通過點擊「Remix」對已發佈的成熟作品進行再創作，無需從零開始，從而將效率提升 10 倍。此外，用戶還可以通過發佈自己的 Agent 作品至 Gallery 並允許他人 Remix，每次作品被 Remix 都能獲得 100 積分的收益。這不僅是一個創作和分享的平台，也是一個漲粉和建立個人品牌的利器。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="285" src="https://oscimg.oschina.net/oscnet/up-d064cf16f7166c2be8c20d9ed0ee1bc940e.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;MiniMax 強調，這一平台是「Agent 全民經濟」的顛覆性突破，具有四大獨家優勢。用戶可以輕鬆地 Remix 各種模板，如香氛蠟燭電商模板，快速開啓自己的電商創業。此外，用戶還可以定製任何行業或主題的 Daily Newsletter，甚至將 Netflix 和 Bilibili、LinkedIn 和 Tinder 等不同平台的功能進行 Remix，創造出全新的用戶體驗。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在技術層面，MiniMax 在研發過程中注重 Agent 的可靠性，包括上下文壓縮總結、API 信息脫敏引擎以及多 Agent 任務路由等技術，以確保用戶數據的安全和隱私。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;全球挑戰賽面向所有人開放，鼓勵參與者用自己的想法挑戰 15 萬美金的獎池。挑戰賽分為原創和 Remix 雙賽道，無論是原創作品還是基於已發佈作品的二創，都有機會獲獎。參與者無需代碼能力，即可參與這一全球智能普惠的活動。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;體驗地址：&lt;/span&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fminimax-agent-hackathon.space.minimax.io%2F" target="_blank"&gt;https://minimax-agent-hackathon.space.minimax.io/&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365744</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365744</guid>
      <pubDate>Mon, 11 Aug 2025 08:20:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>華為發佈 AI 推理創新技術 UCM：可實現高吞吐、低時延推理體驗，計劃 9 月開源</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbaijiahao.baidu.com%2Fs%3Fid%3D1840229710674780713%26wfr%3Dspider%26for%3Dpc" target="_blank"&gt;根據報道&lt;/a&gt;，華為正式發佈了 AI 推理創新技術 UCM（推理記憶數據管理器）。&lt;/p&gt; 
&lt;p&gt;華為推出的 UCM（推理記憶數據管理器）是一款以 KV Cache 為中心的推理加速套件，融合多類型緩存加速算法工具，通過分級管理推理過程中產生的 KV Cache 記憶數據，擴大推理上下文窗口，實現高吞吐、低時延的推理體驗，，降低每 Token 推理成本。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0812/160552_0ocB_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;此外，華為計劃於 2025 年 9 月正式開源 UCM，屆時將在魔擎社區首發，後續逐步貢獻給業界主流推理引擎社區，並共享給業內所有 Share Everything (共享架構) 存儲廠商和生態夥伴。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365742</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365742</guid>
      <pubDate>Mon, 11 Aug 2025 08:06:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Claude 新增聊天記錄記憶功能</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Anthropic 為其 Claude 聊天機器人推出備受期待的&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Fclaudeai%2Fstatus%2F1954982275453686216" target="_blank"&gt;「記憶」功能&lt;/a&gt;，用戶可讓機器人檢索並參考過往對話內容。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0812/155847_bdCE_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;該功能支持網頁、桌面及移動端，能區分不同項目和工作區。用戶只需在 「個人資料」 的 「設置」 中開啓 「搜索和查看聊天記錄」，即可使用。&lt;/p&gt; 
&lt;p&gt;目前，Claude 的 Max、Team 和 Enterprise 訂閲層級已率先上線，其他套餐將在近期開放。與 ChatGPT 的持續記憶不同，Claude 的記憶功能為被動觸發模式，僅在用戶明確要求時才檢索過往對話，且不會構建用戶畫像。&lt;/p&gt; 
&lt;p&gt;作為 AI 領域的頭部企業，Anthropic 與 OpenAI 競爭激烈，雙方在語音模式、上下文窗口、訂閲服務等方面不斷角力。此次記憶功能的推出，旨在提升用戶黏性和使用時長。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365741</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365741</guid>
      <pubDate>Mon, 11 Aug 2025 07:59:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>微軟為 Excel 加入 AI 公式講解，內聯解釋直達單元格</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span&gt;微軟&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftechcommunity.microsoft.com%2Fblog%2Fexcelblog%2Fexplain-formulas-with-copilot%25E2%2580%2594now-on-the-grid%2F4424028" target="_blank"&gt;宣佈&lt;/a&gt;，其電子表格工具 Excel 迎來一項重要更新：由 Copilot 驅動的&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;strong&gt;「解釋此公式」（Explain Formula）&lt;/strong&gt;&lt;span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;功能正式上線，旨在幫助用戶快速理解複雜公式，顯著提升數據處理效率。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;該功能的&lt;span&gt;最大&lt;/span&gt;亮點在於操作簡便。用戶無需單獨打開聊天面板，只需點擊包含有效公式的單元格，並在旁邊的 Copilot 圖標中選擇「解釋此公式」，即可在單元格內直接獲得內聯解釋。這些解釋基於當前工作表的上下文生成，比傳統網絡搜索更精準、更貼合實際工作場景。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="225" src="https://oscimg.oschina.net/oscnet/up-68c25bd98edf5ade9b15bb52dea74ff751f.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;微軟表示，Copilot 能夠分解並逐步講解各種複雜程度的公式，幫助用戶快速掌握其邏輯。默認情況下，解釋會以內聯形式顯示;若 Copilot 聊天面板已開啓，內容將優先在面板中呈現。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;目前，該功能正分階段向 Windows 版和網頁版 Excel 用戶推送。微軟鼓勵用戶在每次使用後通過點贊或點踩反饋，協助優化 AI 解釋效果。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365740</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365740</guid>
      <pubDate>Mon, 11 Aug 2025 07:54:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>「字節跳動靜態資源公共庫」因黑產原因下線</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;從 2025 年 6 月份開始，就有諸多站長髮現字節跳動旗下的靜態資源公共庫存在調用問題，包括部分資源連接超時或者直接 HTTP 404，這導致網站無法正常加載內容。 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;當時測試發現諸如 jQuery 等還可以調用，其他部分資源出現錯誤無法調用，因此並不清楚字節跳動哪裏出問題才會導致部分資源有效、部分資源無效。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0812/154152_ycjH_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;https://cdn.bytedance.com/&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;現在字節跳動已經明確靜態資源公共庫下線，當前所有靜態資源已經全部處於 404 狀態，字節跳動稱是「黑產原因」。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365736</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365736</guid>
      <pubDate>Mon, 11 Aug 2025 07:40:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>deepin 亮相首屆世界 RISC-V 日，分享最新 RISC-V 進展</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;div&gt; 
 &lt;div&gt; 
  &lt;div&gt; 
   &lt;div&gt; 
    &lt;p style="text-align:left"&gt;&lt;span&gt;&lt;span&gt;2025 年 8 月 8 日，由 RISC-V 國際基金會重磅推出的首屆世界 RISC-V 日 (World RISC-V Days) 在北京開源芯片研究院舉行。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&lt;img align="left" src="https://oscimg.oschina.net/oscnet//3f4075ef6e79ddf2ab5c098c63d555b7.jpg" width="840" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&lt;strong&gt;&lt;strong&gt;deepin 社區技術委員會成員、苦芽科技工程師李程&lt;/strong&gt;&lt;/strong&gt;&lt;span&gt;參加了此活動，並於會議上帶大家系統回顧了 deepin-ports SIG 的發展歷程，並重點分享了 deepin-ports SIG 在 RISC-V 方向上的最新進展，包括但不限於 deepin RISC-V 生態適配、社區協作模式優化等方面。&lt;/span&gt;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&lt;img align="left" src="https://oscimg.oschina.net/oscnet//10b8829d40f7888c675d6790b227fb75.jpg" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
    &lt;p style="text-align:center"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:center"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:center"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:center"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:center"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:center"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:center"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:center"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:center"&gt;&lt;span&gt;&lt;span&gt;李程，deepin 社區技術委員會成員、苦芽科技工程師&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&lt;span&gt;&lt;span&gt;deepin 對 RISC-V 架構的支持並非一日之功。自 2022 年 2 月起，deepin 就建立了對應 SIG，開始了 RISC-V 架構的適配工作，現已成功支持了大量主流的 RISC-V 硬件和開發板。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&lt;span&gt;&lt;span&gt;在軟件方面，deepin 也完成了對 RISC-V 開源軟件生態的適配，提供了超過 27,000 個軟件包，併為 RISC-V 開發板提供了內核、GPU、VPU、NPU 等驅動解決方案，確保了這些關鍵組件能夠長期、及時、良好地維護。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&lt;img align="left" src="https://oscimg.oschina.net/oscnet//92d10ca4ef881324c89df1a23d1a392e.jpg" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&lt;span&gt;&lt;span&gt;作為中國桌面操作系統的核心力量，deepin 積極響應國家戰略，深度參與「甲辰計劃」，全力投入 RISC-V 開源新生態建設。迄今，deepin 操作系統已成功適配了幾乎所有可公開獲取的桌面級 RISC-V 設備，並提供了關鍵的 GPU、NPU、VPU 等硬件加速支持。&lt;/span&gt;&lt;/span&gt;&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;deepin 23&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt; 穩定版及 &lt;/span&gt;&lt;/span&gt;&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;deepin 25 預覽版&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;均已為 RISC-V 平台提供官方鏡像並持續更新。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&lt;span&gt;&lt;span&gt;通過在硬件適配、軟件生態構建、社區協作及戰略規劃上的不懈努力，deepin 已成為 RISC-V 生態的重要貢獻者，並有力推動着 RISC-V 桌面操作系統的普及與應用。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&lt;img align="left" src="https://oscimg.oschina.net/oscnet//7215a0a6d6d54c51ac5e45faa8fd4f4a.jpg" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&lt;span&gt;&lt;span&gt;李程還介紹到，deepin 作為「甲辰計劃」的重要參與社區之一，為給更多同學提供深入 deepin、RISC-V 等技術項目的機會，將與甲辰計劃聯合提供近 80 個實習 HC。李程先生將作為該實習崗位的首席導師（Principle Mentor），協調實習工作內容，並負責 mentor 的招募和崗前培訓。進一步瞭解：&lt;/span&gt;&lt;/span&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzA5NzE0Mjg4Ng%3D%3D%26mid%3D2650457142%26idx%3D2%26sn%3D4b83559f9c00f28f1df379af82b1ab5a%26scene%3D21%23wechat_redirect" target="_blank"&gt;&lt;span&gt;&lt;span&gt;新增實習機會！RISC-V deepin 操作系統開發實習生正在招募&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&lt;span&gt;&lt;span&gt;由於時間限制未設置現場問答環節，但與會者還是對技術路線和實習計劃展現出了濃厚興趣，眾多參會者主動拍攝 PPT 關鍵內容頁，期待進一步交流探討。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&lt;img align="left" src="https://oscimg.oschina.net/oscnet//cd1393b25b25a0f66f8bf6cc4337cb36.jpg" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;p style="text-align:left"&gt;&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;附：&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;span&gt;&lt;span&gt;deepin-ports SIG 主頁：&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;https://github.com/deepin-community/sig-deepin-ports&lt;/span&gt;&lt;/span&gt;&lt;/li&gt; 
    &lt;/ul&gt; 
   &lt;/div&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/365735</link>
      <guid isPermaLink="false">https://www.oschina.net/news/365735</guid>
      <pubDate>Mon, 11 Aug 2025 07:39:00 GMT</pubDate>
      <author>來源: 資訊</author>
    </item>
  </channel>
</rss>
