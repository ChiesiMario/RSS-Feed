<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>開源中國-綜合資訊</title>
        <link>https://www.oschina.net/news/industry</link>
        <atom:link href="http://8.134.148.166:30044/oschina/news/industry" rel="self" type="application/rss+xml"></atom:link>
        <description>開源中國-綜合資訊 - Powered by RSSHub</description>
        <generator>RSSHub</generator>
        <webMaster>contact@rsshub.app (RSSHub)</webMaster>
        <language>en</language>
        <lastBuildDate>Wed, 26 Mar 2025 07:37:31 GMT</lastBuildDate>
        <ttl>5</ttl>
        <item>
            <title>清華大學開源 Video-T1：無需重新訓練 AI 視頻秒變高清大片</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;清華大學的研究團隊近日開源了其最新的研究成果——Video-T1。這項技術的核心在於測試時縮放 （Test-Time Scaling， TTS），旨在通過在視頻生成過程的推理階段投入更多的計算資源，顯著提升生成視頻的質量和與文本提示的一致性，而無需重新進行昂貴的模型訓練。這一創新性的方法為視頻生成領域帶來了新的可能性。&lt;/span&gt;&lt;/p&gt; 
&lt;h3 style=&quot;margin-left:0px; margin-right:0px; text-align:start&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;何為「測試時縮放」?&lt;/span&gt;&lt;/h3&gt; 
&lt;p style=&quot;color:#242424; margin-left:0px; margin-right:0px; text-align:start&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;在大型語言模型 （LLMs） 領域，研究人員已經發現，通過在測試階段增加計算量可以有效提升模型性能。Video-T1 借鑑了這一思路，並將其應用於視頻生成領域。簡單來説，傳統的視頻生成模型在接收到文本提示後，會直接生成一段視頻。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#242424; margin-left:0px; margin-right:0px; text-align:start&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;而採用了 TTS 的 Video-T1，則像是在生成視頻的過程中進行多次「搜索」和「篩選」，&lt;strong&gt;通過生成多個候選視頻，並利用「測試驗證器」進行評估，最終選擇質量最高的視頻&lt;/strong&gt;。這就像一位精雕細琢的藝術家，在完成最終作品前會嘗試多種不同的方法和細節。&lt;/span&gt;&lt;/p&gt; 
&lt;h3 style=&quot;margin-left:0px; margin-right:0px; text-align:start&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;Video-T1 的核心技術&lt;/span&gt;&lt;/h3&gt; 
&lt;p style=&quot;color:#242424; margin-left:0px; margin-right:0px; text-align:start&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;Video-T1 並沒有直接增加訓練成本，而是專注於如何更有效地利用現有模型的能力。其核心方法可以理解為在模型的「噪聲空間」中尋找更優的視頻生成軌跡。為了實現這一目標，研究團隊提出了兩種主要的搜索策略:&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#242424; margin-left:0px; margin-right:0px; text-align:start&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;strong&gt;隨機線性搜索 （Random Linear Search）&lt;/strong&gt;:這種方法通過&lt;strong&gt;隨機採樣多個高斯噪聲&lt;/strong&gt;，讓視頻生成模型對這些噪聲進行逐步去噪，生成多個候選視頻片段，然後利用測試驗證器對這些候選視頻進行評分，最終選擇得分最高的視頻。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#242424; margin-left:0px; margin-right:0px; text-align:start&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;strong&gt;幀樹搜索 （Tree-of-Frames， ToF）&lt;/strong&gt;:考慮到同時對所有幀進行全步去噪會帶來巨大的計算成本，ToF 採用了一種更高效的策略。它將視頻生成過程分為三個階段:首先進行&lt;strong&gt;圖像級別的對齊&lt;/strong&gt;，這會影響後續幀的生成;其次，在測試驗證器中使用&lt;strong&gt;動態提示&lt;/strong&gt;，重點關注&lt;strong&gt;運動的穩定性&lt;/strong&gt;和&lt;strong&gt;物理上的合理性&lt;/strong&gt;，並根據反饋指導搜索過程;最後，評估視頻的整體質量，並選擇與文本提示對齊度最高的視頻。ToF 這種自迴歸的方式能夠更智能地探索視頻生成的可能性。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#242424; margin-left:0px; margin-right:0px; text-align:start&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;img height=&quot;291&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-a31db7613d8e9d4e81825607ee221dc4a49.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#242424; margin-left:0px; margin-right:0px; text-align:start&quot;&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h3 style=&quot;margin-left:0px; margin-right:0px; text-align:start&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;TTS 的顯著效果&lt;/span&gt;&lt;/h3&gt; 
&lt;p style=&quot;color:#242424; margin-left:0px; margin-right:0px; text-align:start&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;實驗結果表明，隨着測試時計算量的增加（即生成更多候選視頻），模型性能會持續提升。這意味着，通過投入更多的推理時間，即使是同一個視頻生成模型，也能夠產生&lt;strong&gt;更高質量、與文本提示更加一致的視頻&lt;/strong&gt;。研究人員在多個視頻生成模型上進行了實驗，結果都顯示出 TTS 能夠穩定地帶來性能提升。同時，不同的測試驗證器關注的評估方面有所不同，因此在性能提升的速率和程度上也存在差異。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#242424; margin-left:0px; margin-right:0px; text-align:start&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;Video-T1 的 TTS 方法在常見的提示類別（如場景、物體）和容易評估的維度 (如圖像質量) 上取得了顯著的改進。通過觀察官方提供的視頻演示可以看出，經過 TTS 處理後的視頻在&lt;strong&gt;清晰度、細節和與文本描述的貼合度&lt;/strong&gt;上都有明顯的提升。例如，描述「戴着太陽鏡在泳池邊當救生員的貓」的視頻，在經過 TTS 處理後，貓的形象更加清晰，救生員的動作也更加自然。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#242424; margin-left:0px; margin-right:0px; text-align:start&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;img height=&quot;293&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-97931a836bd63018b344817d13e7d029863.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;h3 style=&quot;margin-left:0px; margin-right:0px; text-align:start&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;挑戰與展望&lt;/span&gt;&lt;/h3&gt; 
&lt;p style=&quot;color:#242424; margin-left:0px; margin-right:0px; text-align:start&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;儘管 TTS 在許多方面都帶來了顯著的進步，但研究人員也指出，對於一些難以評估的潛在屬性，例如&lt;strong&gt;運動的流暢性&lt;/strong&gt;和&lt;strong&gt;時序上的一致性&lt;/strong&gt;（避免畫面閃爍），TTS 的改進效果相對有限。這主要是因為這些屬性需要對跨幀的運動軌跡進行精確控制，而目前的視頻生成模型在這方面仍然面臨挑戰。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#242424; margin-left:0px; margin-right:0px; text-align:start&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;清華大學開源的 Video-T1 通過創新的測試時縮放策略，為提升視頻生成質量提供了一種新的有效途徑。它無需昂貴的重新訓練，而是通過更智能地利用推理時的計算資源，讓現有模型煥發出更強的能力。隨着未來研究的深入，我們有理由期待 TTS 技術在視頻生成領域發揮越來越重要的作用。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/341094</link>
            <guid isPermaLink="false">https://www.oschina.net/news/341094</guid>
            <pubDate>Wed, 26 Mar 2025 07:19:27 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>哥倫比亞大學研發 3D 光子電子芯片，突破 AI 數據傳輸瓶頸</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;美國哥倫比亞大學工程團隊與康奈爾大學工程團隊合作成功開發出全球首款&lt;strong&gt;三維集成光子-電子芯片&lt;/strong&gt;，實現了前所未有的效率和帶寬。&lt;/p&gt; 
&lt;p&gt;相關研究論文已於&amp;nbsp;3 月 21 日發表於《自然・光子學》上：&amp;nbsp;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.nature.com%2Farticles%2Fs41566-025-01633-0&quot; target=&quot;_blank&quot;&gt;DOI:&amp;nbsp;10.1038/s41566-025-01633-0&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-131d9e739b45c53af3cedf7d25d9ece622b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;▲&amp;nbsp;電氣工程教授 Keren Bergman，及電氣研究生、論文合著者 Michael Cullen&lt;/p&gt; 
&lt;p&gt;他們通過深度融合光子技術與先進的互補金屬氧化物半導體電子技術，讓這種新型三維光電子芯片實現了 800Gb/s 超高帶寬與 120 飛焦 / 比特的極致能效，帶寬密度達 5.3 Tb/s/mm² 遠超現有基準。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-fd9154017bb1eae29074e948da27f8417bc.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;這項突破性的技術有望重塑 AI 硬件，使未來的智能系統能夠以更快的速度傳輸數據，同時顯著降低能耗，這對於智能汽車、大規模 AI 模型等未來技術至關重要。&lt;/p&gt; 
&lt;p&gt;Bergman 教授表示：「我們展示了一種能夠以空前之低的能耗傳輸大量數據的技術。這項創新突破了長期以來限制傳統計算機和 AI 系統數據傳輸的能源壁壘。」&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/341093</link>
            <guid isPermaLink="false">https://www.oschina.net/news/341093</guid>
            <pubDate>Wed, 26 Mar 2025 07:15:27 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>OpenBao 獲採納為 EdgeX 的祕密存儲</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;為增強安全性和開放性，EdgeX Foundry 正式將 OpenBao 作為 EdgeX 4.0 版本的默認祕密存儲。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;EdgeX Foundry 是一個開源的 IoT/邊緣計算框架，由 Linux 基金會託管。它旨在通過靈活的微服務架構，實現設備、應用和服務之間的無縫通信。無論你在自動化、能源還是建築管理領域，EdgeX 都能以標準化的方式將一切連接起來。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;152&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-c1956e610d420b596ef4a166f66529a5a0b.png&quot; width=&quot;700&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;此前，EdgeX 主要依賴 HashiCorp Vault 安全存儲敏感信息。然而，隨着 Vault 轉向商業源許可證（BSL），EdgeX 社區選中了 OpenBao 作為未來的替代方案。OpenBao 是一個由社區驅動的開源項目，屬於 Linux 基金會。它提供基於身份的祕密和加密管理系統，確保敏感數據得到保護。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;公告&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FMZsyEUjRY2JkmfAaY9o-BA&quot; target=&quot;_blank&quot;&gt;指出&lt;/a&gt;，選擇 OpenBao 的原因包括：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;無縫遷移 – OpenBao 設計上與其上游 API 兼容，切換過程順利且無憂。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;開放和供應商中立的許可證 – 開源自由和長期社區合作。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;安全優先的方法 – 強加密和基於身份的訪問控制確保祕密安全。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;活躍的社區支持 – 專注團隊確保持續改進、安全更新和功能增強。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;對於已經在使用 EdgeX 的用戶，這一變更幾乎不會造成影響。核心服務已更新以支持 OpenBao，同時保持與之前相同的 API，意味着幹擾最小。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/341087</link>
            <guid isPermaLink="false">https://www.oschina.net/news/341087</guid>
            <pubDate>Wed, 26 Mar 2025 07:05:27 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>頂尖 AI 專家齊國君自美歸國：加盟西湖大學、拿過華為總裁獎</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.scmp.com%2Fnews%2Fchina%2Fscience%2Farticle%2F3303527%2Fai-expert-guo-jun-qi-leaves-us-china&quot; target=&quot;_blank&quot;&gt;根據《南華早報》的報道&lt;/a&gt;&lt;/u&gt;，屢獲殊榮的人工智能（AI）專家和計算機科學家齊國君在美國工作十幾年後，已回國加盟位於杭州的西湖大學領導 「MAPLE 實驗室」 團隊。&lt;/p&gt; 
&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.westlake.edu.cn%2Ffaculty%2Fguojun-qi.html&quot; target=&quot;_blank&quot;&gt;據西湖大學官網介紹&lt;/a&gt;&lt;/u&gt;，齊國君，安徽合肥人，國際電氣和電子工程師協會會士（IEEE Fellow）、國際模式識別聯合會會士（IAPR Fellow）、國際計算機協會（ACM）傑出科學家，中國科學技術大學郭沫若獎得主。&lt;/p&gt; 
&lt;p&gt;齊國君課題組主要開展機器感知和學習方向的研究，致力於研發對虛實場景進行多模態感知、生成與交互的人工智能系統，並應用於多媒體計算、基於 AIGC 的智慧創作等領域。他在人工智能多模態算法與模型、智慧創作與虛擬現實等多個領域取得了多項開創性成果。已在相關國際會議與雜誌上發表論文共計 200 餘篇，引用近 20000 次。&lt;/p&gt; 
&lt;p&gt;西湖大學形容，通俗地講，齊國君可以被看做機器的 「養育者」，他開發模型，教機器能夠同時理解文字、圖片、音頻、視頻等多種媒介傳遞的信息，讓機器變得 「更智能」。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0326/145658_Rof3_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;從 2014 年到 2024 年，齊國君在美國做了十年研究，他先加入了 IBM T.J. Watson 研究中心擔任研究員，之後進入美國中佛羅裏達大學執教一年。然後離開美國中佛羅裏達大學，加入華為美國研究中心，擔任技術副總裁（Technical VP）和首席 AI 科學家（Chief AI Scientist）。&lt;/p&gt; 
&lt;p&gt;在華為美國研究中心，他主要負責華為雲 EI 智能體項目，主持設計 TrafficGo 智慧城市系統，優化整合了每天數億級的多模態數據，對 200 多個路口的複雜路況進行實時調控，極大提升了交通出行效率和應急事件處理速度，因此獲得華為總裁獎。&lt;/p&gt; 
&lt;p&gt;再然後，他去了 OPPO，一手創立了 OPPO 西雅圖研究中心，還是立足於多模態數據的處理，把研究的邊界往虛擬現實領域拓寬了一步。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/341086</link>
            <guid isPermaLink="false">https://www.oschina.net/news/341086</guid>
            <pubDate>Wed, 26 Mar 2025 07:00:27 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>中國大模型密集開源，影響幾何？</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;今年以來，中國大模型開源的消息一個接一個。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;阿里雲通義千問從除夕夜開源全新的視覺模型 Qwen2.5-VL，再到本月初發布並開源了全新推理模型 QwQ-32B，在開源當日就登頂全球主流 AI 開源社區 Hugging Face 的趨勢榜。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;DeepSeek(深度求索) 達成過「開源周」，其在 2 月末連續五天發佈五個代碼庫，並於近日繼續開源上線了升級後的 DeepSeek-V3 模型。 階躍星辰則在一個月左右時間開源三款多模態大模型，其最新開源的是圖生視頻模型 Step-Video-TI2V，支持生成的視頻具備運動幅度可控和鏡頭運動可控兩大核心特點，同時自帶一定的特效生成能力。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;為何開源大模型成為中國當前的發展潮流？FutureLabs 未來實驗室首席專家胡延平對中新社記者表示，大模型廠商普遍選擇開源，且有強勁的市場爆發力，是因為人工智能發展處在四個重要時刻。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;一是端側智能的需求崛起，包括個人單機部署 AI 方面的需求，推動端側智能快速發展。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;二是企業行業 AI 部署的需求驅動，千行百業 AI 需求激增，但通用雲端大模型難以滿足差異化的業務場景與數據隱私保護的需要。開源憑藉靈活性和定製化能力，成為企業實現差異化部署的首選，開源模型體現出隨需應變的明顯優勢。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;中新社記者獲取的數據顯示，截至 3 月 25 日，通義千問開源模型 Qwen 系列的全球下載量已超 2 億。通過千千萬萬的開發者和中小企業，通義大模型深入千行百業，包括醫療、教育、金融、電力、交通、計算機等。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;三是 AI 產業生態化進入加速時刻，出現分工協作體系，上下游協作關係更為清晰。頭部企業聚焦模型能力強化，中小企業則基於開源模型開發細分場景應用，形成企業數量更大的產業腰部、大模型後市場，這是一個分工日趨明確的產業生態。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;四是 AI 大模型能力提升顯著，從「可用」進入「高可用」時刻，用戶、應用由此進入爆發性增長時刻。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;據中國工信部官方消息，目前，中國已成為全球開源參與者數量排名第二、增長速度最快的國家。另有數據顯示，阿里通義開源模型的衍生模型數量已突破 10 萬個，成為全球最大的開源模型族羣。&lt;/span&gt;&lt;/p&gt; 
&lt;h4&gt;&lt;strong&gt;&lt;span style=&quot;color:#000000&quot;&gt;中國大模型密集開源，影響幾何？&lt;/span&gt;&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;中國科學院院士梅宏曾表示，大語言模型在未來需要像互聯網一樣，走向開源，由全世界共同維護一個開放共享的基礎模型，盡力保證其與人類知識的同步。否則，任何一個機構所掌控的基礎模型都難以讓其他機構用戶放心地上傳應用數據，也就很難產生足以滿足各行各業業務需求的大量應用。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;胡延平説，以通義千問為代表的中國大模型正藉助這一波開源大勢，縮小與全球領先 AI 技術的差距，最重要的是中國開源的生態化獲得極大成功，為今後發展積蓄了較強勢能。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;阿里雲高級總監朱迅垚認為，現在大家逐步認識到，開源模型將成為推動中國人工智能發展最強勁的引擎。下一步，建議從國家到地方再到企業，以更加積極的態度擁抱開源，同時在佈局智能算力、構建高質量數據集、上雲用雲等方面加快創新步伐，緊跟世界先進水平。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;有外媒近日報道稱，中國科技公司選擇開源路線，不僅是為了與同類型公司展開競爭，更是為了加速 AI 的採用和創新。開源模型降低了成本，為產品創新打開了大門。這一趨勢不僅將推動中國 AI 領域的快速發展，甚至可能縮短技術差距。&amp;nbsp;(中新社，記者，夏賓)&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/341084</link>
            <guid isPermaLink="false">https://www.oschina.net/news/341084</guid>
            <pubDate>Wed, 26 Mar 2025 06:57:27 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>蘋果花費約 10 億美元採購英偉達 AI 服務器</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;蘋果公司曾公開宣稱其正在使用 Apple Silicon 服務器來支持&amp;nbsp;Apple Intelligence&amp;nbsp;的運行，&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.investors.com%2Fnews%2Ftechnology%2Fapple-stock-apple-joins-ai-data-center-race%2F&quot; target=&quot;_blank&quot;&gt;但根據 Loop Capital 分析師 Ananda Baruah 的説法&lt;/a&gt;&lt;/u&gt;，&lt;strong&gt;該公司現在也在花費 10 億美元購買 NVIDIA 的 AI 服務器&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;他在給投資者的一份報告中寫道： 「&amp;nbsp;AAPL 正式加入大型服務器集羣 Gen AI 遊戲，超微[Super Micro Computer] 和戴爾是關鍵的服務器合作伙伴。雖然我們仍在收集更全面的信息，但這似乎有可能成為 Gen AI LLM（大型語言模型）集羣。」&lt;/p&gt; 
&lt;p&gt;Baruah 聲稱，蘋果正在購買 250 台 NVIDIA NVL72 服務器，每台服務器的成本在 370 萬至 400 萬美元之間。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;梳理事件時間線如下：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;2024 年 4 月：消息稱蘋果將使用自研芯片搭建 AI 服務器&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;2024 年 6 月：消息稱蘋果數據中心將全面採用 Apple Silicon&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;2024 年 9 月：蘋果軟件工程高級副總裁 Craig Federighi 公開確認，Apple Intelligence 服務完全運行在自研服務器上，稱這是「行業雲端處理新標準」。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;2025 年 3 月：分析師披露蘋果訂購 250 台英偉達 NVL72 服務器，單台成本 370 萬至 400 萬美元（現匯率約合 2685.9 萬至 2903.7 萬元人民幣），總價近 10 億美元（現匯率約合 72.59 億元人民幣）。&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;據 NVIDIA 稱，其 NVL72 服務器包含 36 個 Grace CPU 和 72 個 Blackwell GPU。該公司還表示，截至 2025 年 3 月 18 日，該服務器尚未上市。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-f4be572287f934556dc646afc95fcd6fca2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;毫無疑問，蘋果現在可以預訂服務器，而且該公司認為有必要擴大其服務器，這並不奇怪。從數量來看，這可能是為了開發目的，而不是面向公眾，但現在還無法判斷——這是假設報道是正確的。&lt;/p&gt; 
&lt;p&gt;如果它的目的不僅僅是開發，那麼這與 Federighi 的説法並不完全相符，他認為使用 Apple Silicon 服務器「為行業雲端處理樹立了新標準」。&lt;/p&gt; 
&lt;p&gt;他説：「在我們之前沒有 Apple Silicon 服務器的情況下，在數據中心構建服務器，並構建一個在數據中心運行的自定義操作系統，這是一項艱鉅的任務。」「[創建]信任模型，除非服務器正在運行的所有軟件的簽名已發佈到透明日誌中，否則您的設備將拒絕向服務器發出請求，這無疑是解決方案中最獨特的元素之一，並且對信任模型至關重要。」&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/341081/apple-spend-1-billion-on-nvidia-servers-for-ai-analyst</link>
            <guid isPermaLink="false">https://www.oschina.net/news/341081/apple-spend-1-billion-on-nvidia-servers-for-ai-analyst</guid>
            <pubDate>Wed, 26 Mar 2025 06:50:27 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>DistilQwen2.5-R1 發佈：知識蒸餾助推小模型深度思考</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                                                                                                                                        &lt;p&gt;作者：蔡文睿（清素）、汪誠愚（熊兮）、嚴俊冰（玖燭）、黃俊（臨在）&lt;/p&gt; 
&lt;h1&gt;引言&lt;/h1&gt; 
&lt;p&gt;隨着 DeepSeek-R1 和 QwQ-32B 等面向深度推理的大語言模型的開源，&quot;大模型+慢思考&quot;已成為拓展大語言模型智能邊界的標準配置。然而，這些模型在資源受限的移動設備和邊緣計算場景中的普及仍面臨巨大挑戰。因此，學術界和工業界迫切需要解決如何有效利用知識蒸餾技術，將這些超大規模深度推理模型的知識遷移到小模型中，從而提升計算效率並降低部署成本的問題。為此，我們在 DistilQwen2.5 系列蒸餾小模型（看&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.aliyun.com%2Farticle%2F1653842&quot; target=&quot;_blank&quot;&gt;這裏&lt;/a&gt;）的基礎上，推出了更為強大的 DistilQwen2.5-R1 系列深度推理模型。&lt;/p&gt; 
&lt;p&gt;DistilQwen2.5-R1 系列以少量來自 DeepSeek-R1 的思維鏈蒸餾數據為基礎，通過一系列創新的蒸餾策略，有效強化了小模型的深度思考能力。實驗評估結果顯示，DistilQwen2.5-R1 系列中的多種小規模模型在各項基準測試中表現優異（見下圖）。例如，DistilQwen2.5-R1-7B 性能顯著超越了其他開源蒸餾模型，包括 OpenThinker-7B。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//ef4e1a2e97f3cb673adaa04eb8d7d3ff.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//af944f244cb9a874812fd4148abf399f.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;為方便開發者和企業在實際應用中使用 DistilQwen2.5-R1 系列模型，其所有的 Checkpoint 已在 Hugging Face 和 Model Scope 開源社區中公開。本文將深入闡述 DistilQwen2.5-R1 的蒸餾算法、性能評估，並且提供在阿里雲人工智能平台 PAI 上的使用指南及相關下載教程。&lt;/p&gt; 
&lt;h1&gt;DistilQwen2.5-R1 中的知識蒸餾技術&lt;/h1&gt; 
&lt;p&gt;本節中，我們主要描述 DistilQwen2.5-R1 模型訓練中使用的數據增強與知識蒸餾技術。&lt;/p&gt; 
&lt;p&gt;由於自身參數量的顯著差異，大模型與小模型的認知與推理軌跡有時並不完全一致。以數學問題為例：對於有的數學問題，小模型由於自身參數量的限制，會傾向於使用更基礎的方法去解決問題。而大模型基於其強大的推理能力，會採用較為高階的方法。比如經典的雞兔同籠問題，小模型傾向於使用簡單枚舉法逐一試錯，而大模型會直接通過列方程的較高級方法求解。&lt;/p&gt; 
&lt;p&gt;正是由於大小模型的認知軌跡偏差，小模型有時無法有效理解大模型的思維鏈，此時如果直接該思維鏈（Chain-of-Thought，CoT）蒸餾到小模型中，往往效果不佳。為此，我們設計了一種小型推理模型訓練框架，以消除這種認知軌跡偏差帶來的負面影響。在後續訓練中，我們還利用這種偏差數據進一步提升小模型的推理能力，最終推出基於該訓練框架的 DistilQwen2.5-R1 系列模型。我們提出的訓練技術框架包含兩個階段：CoT 數據&quot;評價-改進-驗證&quot;機制，以及基於不同認知軌跡數據的偏好優化算法。總體而言，DistilQwen2.5-R1 模型蒸餾的詳細算法框架如下圖所示：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//df8e195e8ebd7ef30667779a2dcb9f01.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;給定原始的大模型思維鏈數據集，例如從 DeepSeek-R1 蒸餾的數據集，在一階段，我們先對其進行數據難度評價，接着根據數據的難度等級對其進行相應的優化，優化之後還要對結果進行驗證。我們使用改進且被驗證的 CoT 數據集對模型進行 SFT 訓練，獲取模型的基礎推理能力。在二階段，我們利用一階段已有的不同難度的 CoT 數據構造偏好數據集，在一階段的基礎上進一步提升小模型的推理能力。&lt;/p&gt; 
&lt;h2&gt;CoT 數據&quot;評價-改進-驗證&quot;機制&lt;/h2&gt; 
&lt;p&gt;正如上文中提到的，大小模型間的認知推理軌跡有時存在顯著偏差。因此，對於待蒸餾的大模型思維鏈數據集，小模型無法完全理解。階段一正是基於這種認知偏差對數據集進行優化，採用了 LLM-as-a-Judge 的範式，對大模型的推理過程進行評價並改進。&lt;/p&gt; 
&lt;p&gt;給定問題、大模型的推理過程和問題的答案，我們使用模型判斷這個推理過程是簡單、中等還是困難。難度等級的核心標準是小模型是否能夠遵循給定的推理過程得到問題的答案。以下是思維鏈的難度等級及定義：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;中等： 小模型可以遵循該推理過程得到問題的答案。&lt;/li&gt; 
 &lt;li&gt;簡單： 給定的推理過程過於簡單，缺少小模型所需的必要步驟，導致大模型依賴其強大的推理能力解決問題，而小模型無法遵循該過程得到答案。&lt;/li&gt; 
 &lt;li&gt;困難： 給定的推理過程過於複雜或過於困難，導致小模型無法遵循該過程得到答案。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;基於一個大模型的問題與思維鏈集合，我們可以將其分為簡單、中等和困難三類。對於評級為中等的部分，我們予以保留。對於被評為簡單和困難的數據，我們使用模型對思維鏈進行改進。具體來説：對於簡單部分，我們擴展其推理過程，直至小模型可以遵循擴展的過程得到答案。對於評級為困難的部分，我們精簡其推理過程，直至小模型可以遵循精簡的過程得到答案。&lt;/p&gt; 
&lt;p&gt;我們之後對改進結果進行進一步驗證，包括：對改進後的思維鏈再次評價難度等級，檢測其是否被歸類為中等難度，以及驗證小模型是否能夠遵循改進的思維鏈解決問題。如果改進後的思維鏈通過驗證，説明改進有效，該數據可以被小模型有效理解，我們將其保留。如果驗證不通過，説明改進無效，我們將返回到改進步驟，重新進行改進，直至通過驗證。最終，我們獲取了優化後的思維鏈數據集，其組成部分如下：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;初始難度評級為中等的數據。&lt;/li&gt; 
 &lt;li&gt;初始難度評級為簡單，經過改進擴展後評為中等並通過驗證的數據。&lt;/li&gt; 
 &lt;li&gt;初始難度評級為困難，經過改進精簡後評為中等並通過驗證的數據。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;此時，數據集內所有思維鏈的最終難度評級均為中等，意味着小模型可以有效理解數據集內的所有思維鏈，並能遵循這些思維鏈解決相應推理問題。上文提到的大小模型認知軌跡偏差問題在改進後的數據集中得到妥善解決，其可能帶來的負面影響也被消除。我們使用優化後的思維鏈數據集對 Qwen2.5 系列基座模型進行監督微調（SFT），得到 DistilQwen2.5-R1 系列模型的基礎結果。&lt;/p&gt; 
&lt;h2&gt;基於多種認知軌跡數據的偏好優化&lt;/h2&gt; 
&lt;p&gt;在第二階段，我們基於第一階段得到的不同難度等級數據對模型進行進一步提升。&lt;/p&gt; 
&lt;p&gt;具體來説，在第一階段中，評級難度為中等的思維鏈數據是正確且適合小模型的思維鏈，小模型能夠有效理解該思維鏈並解決問題。而難度評級為簡單或困難的思維鏈數據依然是正確的思維鏈，只是不適合小模型。在此基礎上，我們使用模型將正確的推理過程改寫為一個錯誤的推理過程。錯誤的推理過程沒有邏輯性，且會誤導小模型，使得小模型完全無法遵循該錯誤的推理過程解決問題。&lt;/p&gt; 
&lt;p&gt;基於改寫得到的錯誤思維鏈，我們將其與簡單、中等和困難的思維鏈進行兩兩組合，組成多種偏好數據對。這些偏好數據對中有的偏差大，有的偏差小。基於不同種類的偏好數據對及其特點，我們分別使用針對性的參數配置，在第一階段模型的基礎上，採用 DPO 算法進一步優化小模型的推理能力。&lt;/p&gt; 
&lt;p&gt;最終，我們利用第一階段得到的不同難度等級的認知軌跡（思維鏈）數據以及基礎模型結果，得到了 DistilQwen2.5-R1 系列模型。&lt;/p&gt; 
&lt;h1&gt;DistilQwen2.5-R1 模型效果評測&lt;/h1&gt; 
&lt;p&gt;在本節中，我們從多個角度評測 DistilQwen2.5-R1 系列蒸餾小模型的實際效果；同時，我們將 DistilQwen2.5-R1 系列模型和當前業界的前沿模型對比效果。&lt;/p&gt; 
&lt;h2&gt;模型綜合能力評測&lt;/h2&gt; 
&lt;p&gt;我們在多個模型推理能力評測基準上測試了 DistilQwen2.5-R1 系列模型的能力，涵蓋數學、代碼和科學問題三個主流推理領域。&lt;/p&gt; 
&lt;p&gt;在數學領域，我們使用 AIME2024 和 MATH-500 這兩個基準進行測試，AIME2024 是美國數學邀請賽的 2024 年測試集，包含 30 道高難度數學題，用於評估大語言模型在複雜數學推理和問題解決能力，尤其考察代數、幾何等領域的綜合應用。MATH-500 是一個數學推理能力的基準測試，包含 500 個測試樣本，旨在全面考察模型在數學解題上的能力。它與 AIME2024 類似，但有其獨特的測試目標和對比結果，用於衡量模型在不同數學題目上的準確性。&lt;/p&gt; 
&lt;p&gt;在代碼領域，我們使用 LiveCodeBench 基準，LiveCodeBench 是一個動態更新的基準測試平台，用於全面評估大型語言模型在複雜編碼場景中的能力。它通過從頂級競賽平台收集高難度編程任務來測試模型的代碼生成、自我修復代碼執行和測試等能力，是一個綜合性、無污染的評價基準。在本次評測中，我們使用 LiveCodeBench 基準的 V2 版本，其包含 2023 年 5 月-2024 年 5 月的 511 個代碼問題。&lt;/p&gt; 
&lt;p&gt;在科學問題領域，我們使用 GPQA-Diamond（Grade-Level Problems in Question Answering Diamond）基準，其由紐約大學、CohereAI 及 Anthropic 的研究人員聯合發佈，包含 198 條結果，是 GPQA 系列中最高質量的評測數據，用於評估模型解決專家級科學問題的能力。&lt;/p&gt; 
&lt;p&gt;如下圖所示，DistilQwen2.5-R1 系列模型在 3B、7B、14B 和 32B 四個參數量級的模型中，與原始 Qwen2.5 模型的效果進行了對比。可以看出，本文描述的小型推理模型訓練框架顯著提升了現有語言模型的推理能力，並在多個評測基準上取得了一致而明顯的效果提升。&lt;/p&gt; 
&lt;p&gt;| AIME2024 實驗結果對比：&lt;br&gt; &lt;img src=&quot;https://oscimg.oschina.net/oscnet//207b970abad48f4d17f71e0d222d2f8e.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt; | MATH-500 實驗結果對比：&lt;br&gt; &lt;img src=&quot;https://oscimg.oschina.net/oscnet//1056c98b3384267199dea8c9af2ea521.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt; | | --- | --- | | GPQA Diamond 實驗結果對比：&lt;br&gt; &lt;img src=&quot;https://oscimg.oschina.net/oscnet//9a42d598276ac93534537cecbe86b9d5.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt; | LiveCodeBench V2 實驗結果對比：&lt;br&gt; &lt;img src=&quot;https://oscimg.oschina.net/oscnet//b971800722e3b79bcf1d45428aa572a3.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt; |&lt;/p&gt; 
&lt;h2&gt;與其他模型能力對比&lt;/h2&gt; 
&lt;p&gt;為了橫向比較同期發佈的不同參數規模的推理模型效果，下表分別是 DistilQwen2.5-R1 系列模型在各個參數量級上與其他前沿推理模型在上文提到的 4 個基準的評測結果。我們重點對比了 DistilQwen2.5-R1 系列與 OpenThinker、DeepSeek-R1-Distill-Qwen 等系列模型。&lt;/p&gt; 
&lt;p&gt;以下是 7B 量級的對比結果，可以看出，DistilQwen2.5-R1-7B 模型超越了 Bespoke-Stratos-7B 和 OpenThinker-7B。值得注意的是，相較於 OpenThinker-7B，DistilQwen2.5-R1-7B 在使用更少訓練數據的情況下在所有基準上達到了更高的結果。DeepSeek-R1-Distill-Qwen-7B 使用了 800k 閉源訓練數據，而 DistilQwen2.5-R1-7B 使用了開源數據進行訓練（OpenThoughts 數據集過濾和改寫得到的子集），在基於開源數據模型領域內處於領先地位。&lt;/p&gt; 
&lt;p&gt;| &lt;strong&gt;模型&lt;/strong&gt; | &lt;strong&gt;訓練數據量&lt;/strong&gt; | &lt;strong&gt;AIME2024&lt;/strong&gt; | &lt;strong&gt;MATH-500&lt;/strong&gt; | &lt;strong&gt;GPQA Diamond&lt;/strong&gt; | &lt;strong&gt;LiveCodeBench V2&lt;/strong&gt; | | --- | --- | --- | --- | --- | --- | | &lt;em&gt;DeepSeek-R1-Distill-Qwen-7B (reported)&lt;/em&gt; | &lt;em&gt;800k&lt;/em&gt; | &lt;em&gt;55.5&lt;/em&gt; | &lt;em&gt;92.8&lt;/em&gt; | &lt;em&gt;49.1&lt;/em&gt; | &lt;em&gt;-&lt;/em&gt; | | Bespoke-Stratos-7B (reported) | 17k | 20.0 | 82.0 | 37.8 | 36.1 | | OpenThinker-7B (reported) | 114k | ++31.3++ | ++83.0++ | ++42.4++ | ++39.9++ | | **DistilQwen2.5-R1-7B ** | &lt;strong&gt;105k&lt;/strong&gt; | &lt;strong&gt;43.33&lt;/strong&gt; | &lt;strong&gt;88.4&lt;/strong&gt; | &lt;strong&gt;42.93&lt;/strong&gt; | &lt;strong&gt;46.38&lt;/strong&gt; |&lt;/p&gt; 
&lt;p&gt;以下是 32B 量級的對比結果。同樣地，DistilQwen2.5-R1-32B 在所有已知基準上超越了 Sky-T1-32B-Preview，以及在絕大多數基準上超越了 OpenThinker-32B。&lt;/p&gt; 
&lt;p&gt;| &lt;strong&gt;模型&lt;/strong&gt; | &lt;strong&gt;訓練數據量&lt;/strong&gt; | &lt;strong&gt;AIME2024&lt;/strong&gt; | &lt;strong&gt;MATH-500&lt;/strong&gt; | &lt;strong&gt;GPQA Diamond&lt;/strong&gt; | &lt;strong&gt;LiveCodeBench V2&lt;/strong&gt; | | --- | --- | --- | --- | --- | --- | | &lt;em&gt;DeepSeek-R1-Distill-Qwen-32B (reported)&lt;/em&gt; | &lt;em&gt;800k&lt;/em&gt; | &lt;em&gt;72.6&lt;/em&gt; | &lt;em&gt;94.3&lt;/em&gt; | &lt;em&gt;62.1&lt;/em&gt; | &lt;em&gt;-&lt;/em&gt; | | Sky-T1-32B-Preview (reported) | 17k | 43.3 | 86.4 | 56.8 | &lt;em&gt;-&lt;/em&gt; | | OpenThinker-32B (reported) | 114k | ++66.0++ | ++90.6++ | ++61.6++ | &lt;strong&gt;68.9&lt;/strong&gt; | | &lt;strong&gt;DistilQwen2.5-R1-32B&lt;/strong&gt; | &lt;strong&gt;105k&lt;/strong&gt; | &lt;strong&gt;70.0&lt;/strong&gt; | &lt;strong&gt;93.8&lt;/strong&gt; | &lt;strong&gt;62.12&lt;/strong&gt; | ++65.95++ |&lt;/p&gt; 
&lt;h2&gt;模型多次推理評測&lt;/h2&gt; 
&lt;p&gt;我們還測試了 DistilQwen2.5-R1 系列模型在上文提到的四個基準上多次推理的結果，模型會對同一個問題生成 k 個回答進行評測，即 Pass&lt;a href=&quot;https://my.oschina.net/kaiprince&quot;&gt;@k&lt;/a&gt; 指標。以下是 DistilQwen2.5-R1-7B 和 DistilQwen2.5-R1-32B 在四個基準上 Pass@k 結果（k=2、4、8、16、32、64）。&lt;/p&gt; 
&lt;p&gt;可以看出，隨着模型推理次數 k 的逐步增加，兩個模型在所有基準上的評測準確率大幅提高。值得注意的是，隨着 k 的增加，DistilQwen2.5-R1-7B 在 MATH-500 和 GPQA-Diamond 上漲幅巨大，並且不斷逼近 DistilQwen2.5-R1-32B 水準。這表明我們的推理模型訓練框架在小模型領域內擁有巨大潛力。我們可以通過多次推理的方式使 7B 模型擁有媲美 32B 模型的能力，極大減少了推理所需的計算資源。&lt;/p&gt; 
&lt;p&gt;| &lt;img src=&quot;https://oscimg.oschina.net/oscnet//ce3b51665de26ca24a5594370d2c0b98.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt; | | --- | | &lt;img src=&quot;https://oscimg.oschina.net/oscnet//d87903ad8d3726d0b56720b8f76f97a1.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt; | | &lt;img src=&quot;https://oscimg.oschina.net/oscnet//b7da22ba53431b02d34fff15a7cc1ac3.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt; |&lt;/p&gt; 
&lt;h2&gt;模型輸出&lt;/h2&gt; 
&lt;p&gt;對同一數學問題，我們對比了 DistilQwen2.5-R1 系列模型在 7B、32B 量級和同等量級模型的推理結果。從輸出結果可以看出，DistilQwen2.5-R1 系列模型在同量級推理模型中處於領先地位。&lt;/p&gt; 
&lt;h1&gt;模型下載和使用&lt;/h1&gt; 
&lt;h2&gt;DistilQwen2.5-R1 在阿里雲人工智能平台 PAI 上的實踐&lt;/h2&gt; 
&lt;p&gt;以下 HuggingFace transformers 庫為例，簡要介紹如何在 PAI-DSW 上使用 DistilQwen2.5-R1 模型。首先需要保證 PAI-DSW 鏡像內 transformers 版本大於等於 4.37.0，否則會在加載模型時報錯：&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;KeyError: &#39;qwen2&#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;以 DistilQwen2.5-R1-7B 為例，我們可以使用如下代碼調用模型：&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = &quot;alibaba-pai/DistilQwen2.5-R1-7B&quot;

model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = &quot;xxxxx&quot;
messages=[
    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Your role as an assistant involves thoroughly exploring questions through a systematic long thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracing, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution. In the Thought section, detail your reasoning process using the specified format: &amp;lt;|begin_of_thought|&amp;gt; {thought with steps separated with &#39;\n\n&#39;} &amp;lt;|end_of_thought|&amp;gt; Each step should include detailed considerations such as analisying questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The solution should remain a logical, accurate, concise expression style and detail necessary step needed to reach the conclusion, formatted as follows: &amp;lt;|begin_of_solution|&amp;gt; {final formatted, precise, and clear solution} &amp;lt;|end_of_solution|&amp;gt; Now, try to solve the following question through the above guidelines:&quot;},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt},
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors=&quot;pt&quot;).to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;DistilQwen2.5-R1 在開源社區的下載&lt;/h2&gt; 
&lt;p&gt;我們在 Hugging Face 和 Model Scope 上開源了我們蒸餾後的模型，分別為&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Falibaba-pai%2FDistilQwen2.5-R1-3B&quot; target=&quot;_blank&quot;&gt;DistilQwen2.5-R1-3B&lt;/a&gt;、&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Falibaba-pai%2FDistilQwen2.5-R1-7B&quot; target=&quot;_blank&quot;&gt;DistilQwen2.5-R1-7B&lt;/a&gt;、&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Falibaba-pai%2FDistilQwen2.5-R1-14B&quot; target=&quot;_blank&quot;&gt;DistilQwen2.5-R1-14B&lt;/a&gt;、&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Falibaba-pai%2FDistilQwen2.5-R1-32B&quot; target=&quot;_blank&quot;&gt;DistilQwen2.5-R1-32B&lt;/a&gt;。以 Hugging Face 為例，用戶可以使用如下代碼下載這兩個模型：&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from huggingface_hub import snapshot_download

model_name = &quot;alibaba-pai/DistilQwen2.5-R1-3B&quot;
snapshot_download(repo_id=model_name, cache_dir=&quot;./DistilQwen2.5-R1-3B/&quot;)

model_name = &quot;alibaba-pai/DistilQwen2.5-R1-7B&quot;
snapshot_download(repo_id=model_name, cache_dir=&quot;./DistilQwen2.5-R1-7B/&quot;)

model_name = &quot;alibaba-pai/DistilQwen2.5-R1-14B&quot;
snapshot_download(repo_id=model_name, cache_dir=&quot;./DistilQwen2.5-R1-14B/&quot;)

model_name = &quot;alibaba-pai/DistilQwen2.5-R1-32B&quot;
snapshot_download(repo_id=model_name, cache_dir=&quot;./DistilQwen2.5-R1-32B/&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;小結與未來工作&lt;/h1&gt; 
&lt;p&gt;本文介紹了 DistilQwen2.5-R1 系列深度推理模型，它在少量來自 DeepSeek-R1 的思維鏈數據基礎上，通過創新蒸餾策略增強了小模型的深度思考能力。實驗結果表明，該系列模型在多個基準測試中表現出色，尤其是 DistilQwen2.5-R1-7B 的性能全面超越了其他開源蒸餾模型。為了方便實際應用，這些模型的 Checkpoint 已在 Hugging Face 和 Model Scope 社區中公開，並提供了在阿里雲人工智能平台 PAI 上的操作指南。在未來，隨着大語言模型和知識蒸餾技術更進一步的發展，我們將推出各種領域、各種規格的 DistilQwen 系列模型，充分促進大語言模型在實際應用中的降本增效。&lt;/p&gt; 
&lt;h1&gt;參考資料&lt;/h1&gt; 
&lt;p&gt;相關發表論文&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Yuanhao Yue, Chengyu Wang, Jun Huang, Peng Wang. Building a Family of Data Augmentation Models for Low-cost LLM Fine-tuning on the Cloud. COLING 2025&lt;/li&gt; 
 &lt;li&gt;Yuanhao Yue, Chengyu Wang, Jun Huang, Peng Wang. Distilling Instruction-following Abilities of Large Language Models with Task-aware Curriculum Planning. EMNLP 2024&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;技術文章&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;DistilQwen2.5 發佈：通義千問蒸餾小模型再升級：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.aliyun.com%2Farticle%2F1653842&quot; target=&quot;_blank&quot;&gt;https://developer.aliyun.com/article/1653842&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;DistilQwen2：通義千問大模型的知識蒸餾實踐：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.aliyun.com%2Farticle%2F1633882&quot; target=&quot;_blank&quot;&gt;https://developer.aliyun.com/article/1633882&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;DistilQwen2 蒸餾小模型的訓練、評測、壓縮與部署實踐：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhelp.aliyun.com%2Fzh%2Fpai%2Fuser-guide%2Ftraining-evaluation-compression-and-deployment-of-distilqwen2%3Fspm%3Da2c4g.11186623.help-menu-30347.d_2_3_0_2_5.111b25e7cqc8bb&quot; target=&quot;_blank&quot;&gt;https://help.aliyun.com/zh/pai/user-guide/training-evaluation-compression-and-deployment-of-distilqwen2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;大語言模型數據增強與模型蒸餾解決方案：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhelp.aliyun.com%2Fzh%2Fpai%2Fuser-guide%2Fllm-data-enhancement-and-model-distillation-solution%3Fspm%3Da2c4g.11186623.help-menu-30347.d_2_3_0_2_6.7b2a25e7Ft8jcP&quot; target=&quot;_blank&quot;&gt;https://help.aliyun.com/zh/pai/user-guide/llm-data-enhancement-and-model-distillation-solution&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h1&gt;技術交流答疑羣&lt;/h1&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//fa60dfa833375be7ed5b66008fb94ed5.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/u/5583868/blog/18007679</link>
            <guid isPermaLink="false">https://my.oschina.net/u/5583868/blog/18007679</guid>
            <pubDate>Wed, 26 Mar 2025 06:45:27 GMT</pubDate>
            <author>原創</author>
        </item>
        <item>
            <title>FaunaDB 將於 5 月停止服務，承諾開源核心技術版本</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;NoSQL 數據庫 FaunaDB 的供應商 Fauna &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ffauna.com%2Fblog%2Fthe-future-of-fauna&quot; target=&quot;_blank&quot;&gt;表示&lt;/a&gt;，由於缺乏支持和營銷數據庫服務所需的資金，它將在 5 月底關閉該服務。&lt;/span&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;「推動廣泛採用一種在全球範圍內以服務形式運行的新運營數據庫需要大量資金。在當前的市場環境下，我們的董事會和投資者已確定不可能單獨籌集實現該目標所需的資金。雖然我們將不再接受新客戶，但現有的 Fauna 客戶不會立即發生變化。我們將逐步讓客戶退出 Fauna，並致力於確保未來幾個月內順利完成這一過程。」&lt;/span&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;262&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-ce52cf07537bfbb44698b63ccd43c383c6d.webp&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;該公司表示，所有 Fauna 企業客戶都需要在太平洋時間 5 月 30 日中午之前將其應用程序和數據移出 Fauna。在此之後，所有 Fauna 帳戶及其相關數據將被永久刪除。關閉運營的舉措預計將影響多家企業的 195 多個數據庫和 3,000 多個開發團隊。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;此外，Fauna 在通告中作出承諾，計劃在之後發佈 Fauna 核心數據庫技術的開源版本以及現有的開源驅動程序和 CLI 工具。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;「這確保了 Fauna 的獨特功能（我們的事務功能、文檔關係數據模型和我們的數據庫語言 (FQL)）將可供社區使用。我們希望這既能成為數據庫從業者的寶貴參考，又能為更廣泛的開發者社區提供持續的價值。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Fauna 為受影響的客戶提供了遷移期間的支持，詳情可查看&lt;/span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.fauna.com%2Ffauna%2Fcurrent%2Fmigrate&quot; target=&quot;_blank&quot;&gt;遷移指南&lt;/a&gt;。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/341068/the-future-of-fauna</link>
            <guid isPermaLink="false">https://www.oschina.net/news/341068/the-future-of-fauna</guid>
            <pubDate>Sun, 23 Mar 2025 06:27:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>Cloudflare 支持遠程 MCP 服務器部署</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;Cloudflare&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.cloudflare.com%2Fremote-model-context-protocol-servers-mcp%2F&quot; target=&quot;_blank&quot;&gt;宣佈&lt;/a&gt;&lt;/u&gt;，開發者現在可以在他們的平台上構建和部署遠程 MCP（模型上下文協議）服務器。這意味着你不再需要在本地電腦上運行這些服務器，而是可以把它們放到雲端，讓更多用戶輕鬆訪問。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0326/142009_bpL3_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Cloudflare 提供了四個核心組件，大大簡化遠程 MCP Server 的構建過程：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;workers-oauth-provider&lt;/strong&gt;：簡化認證和授權&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;McpAgent&lt;/strong&gt;：處理遠程傳輸&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;mcp-remote&lt;/strong&gt;：允許現有 MCP 客戶端連接到遠程服務器的適配器&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;AI playground&lt;/strong&gt;：作為遠程 MCP 客戶端， 一個帶身份驗證的聊天界面，可直接連接遠程 MCP Server&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;想要開始構建自己的遠程 MCP 服務器？只需幾個簡單步驟：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;訪問 Cloudflare 的 MCP 服務器指南 -&amp;nbsp;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdevelopers.cloudflare.com%2Fagents%2Fguides%2Fremote-mcp-server%2F&quot; target=&quot;_blank&quot;&gt;https://developers.cloudflare.com/agents/guides/remote-mcp-server/&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;使用提供的模板創建你的第一個 MCP 服務器&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;自定義服務器功能&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;部署到 Cloudflare 的全球網絡&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;與 AI 助手連接&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;em&gt;詳情查看：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.cloudflare.com%2Fremote-model-context-protocol-servers-mcp%2F&quot; target=&quot;_blank&quot;&gt;https://blog.cloudflare.com/remote-model-context-protocol-servers-mcp/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/341063/remote-model-context-protocol-servers-mcp</link>
            <guid isPermaLink="false">https://www.oschina.net/news/341063/remote-model-context-protocol-servers-mcp</guid>
            <pubDate>Sun, 23 Mar 2025 06:20:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>智源發佈關於被美國商務部列入實體清單的聲明</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;針對被美國商務部工業與安全局將北京智源人工智能研究院（簡稱「智源」）列入實體清單一事。&lt;/p&gt; 
&lt;p&gt;智源研究院發佈&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F1Miy6dODbT0X0tAjG8tCpA&quot; target=&quot;_blank&quot;&gt;聲明稱&lt;/a&gt;，「我們對於民辦非營利科研機構被加入實體清單表示震驚，強烈反對這一毫無事實依據的錯誤決定，要求美國相關部門予以撤回。」&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;268&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-4ed2791aab0b687e1345f168830cb44ab87.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;智源作為非營利科研機構，一直堅持開源開放的原則，所有科研技術成果向全球公開共享，並積極參與 AI 安全國際對話，推動人工智能增進社會福祉。人工智能是人類公器，如同電力等所有重大技術革命一樣，開源開放是必然趨勢，智能時代是全人類共建共享的時代。美國商務部的行為違背科技創新精神，嚴重損害全球人工智能領域的開放合作。我們呼籲國際社會共同構建開放、包容、合作的技術未來。&lt;/p&gt; 
 &lt;p&gt;智源打造了覆蓋模型、算法、數據、評測、系統的大模型開源技術體系。截至目前，智源已累計開源約 200 個模型和近百個數據集，其中，模型全球總下載量近 6 億次，開源數據集下載量近 39 萬次，開源項目代碼下載量超 95 萬次，為人工智能技術普惠做出持續貢獻。&lt;/p&gt; 
 &lt;p&gt;智源將繼續堅持開源開放非營利的原則，向全球分享世界一流的大模型技術及人工智能領域的前沿探索，營造最佳的學術和技術創新生態，促進人類、環境和智能的可持續發展。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;相關閲讀：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li style=&quot;text-align:start&quot;&gt;&lt;a href=&quot;https://www.oschina.net/news/340997&quot; target=&quot;_blank&quot;&gt;特朗普政府將多家中國科技公司列入 「實體清單」&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/341061</link>
            <guid isPermaLink="false">https://www.oschina.net/news/341061</guid>
            <pubDate>Sun, 23 Mar 2025 06:16:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>報告：開源解決方案繼續主導可觀測性策略</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Grafana Labs 最新發布的一份可觀測性調查報告&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgrafana.com%2Fobservability-survey%2F&quot; target=&quot;_blank&quot;&gt;指出&lt;/a&gt;，75% 受訪者的表示他們在可觀測性工作中使用開源解決方案。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;30% 的受訪者表示他們只使用開源軟件，36% 的受訪者主要使用開源軟件。另一方面，8% 的受訪者只使用商業解決方案，16% 的受訪者主要使用商業解決方案，10% 的受訪者使用開源和商業解決方案的比例大致相等。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;70% 的受訪者在某種程度上使用了 Prometheus 和 OpenTelemetry，一半的受訪者表示，他們對這兩項技術的投資在去年有所增加。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;報告還發現，組織仍然依賴日誌（Logging）和指標（Metrics），但追蹤（Traces）和配置文件（Profiles）越來越受歡迎。57% 的受訪者使用追蹤，16% 的受訪者使用配置文件，指標和日誌分別有 95% 和 87% 的人使用。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;123&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-d0ef509d2cbba28c981f44f12e8c64a7936.png&quot; width=&quot;700&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;公司還在其堆棧中利用了許多不同的可觀測性工具。Grafana Labs 發現，受訪者使用了 101 種不同的可觀測性工具，平均每家公司使用的工具數量為 8 種，比去年的平均 9 種有所下降。但是有 64% 的公司使用 5 種或更少的工具，只有 2% 的公司使用超過 50 種工具。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;規模較大的公司往往擁有更多的數據源，員工人數超過 5,000 人的公司平均擁有 24 個數據源，而員工人數不超過 10 人的公司平均擁有 6 個數據源。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;對於組織來説，最大的可觀測性挑戰是複雜性，而警報疲勞（alert fatigue）是更快響應事件的最大障礙。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Grafana Labs 發現，雖然可觀測性解決方案的成本是選擇可觀測性工具時最重要的因素，但它並不一定至關重要。不到三分之一的受訪者擔心可觀測性成本，大多數人只是想確保他們從所投資的工具中獲得價值。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;選擇工具時的其他因素包括易用性、與其他工具的互操作性、是否開源、將來是否易於切換到其他工具、組織內部的熟悉程度以及 AI/ML 功能。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Grafana Labs 首席技術官 Tom Wilkie 表示：「我們的 2025 年可觀測性調查證實，組織正在採用多元化、以開源為中心的可觀測性方法。隨着團隊管理的工具和數據源比以往任何時候都多，調查結果顯示覆雜性仍然是最大的挑戰。我們正在努力直接解決這些痛點，方法是增強 OpenTelemetry 和 Prometheus 等技術之間的互操作性，通過人工智能功能減少認知負荷，並提供開箱即用的集成解決方案，如 Kubernetes 監控。」&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/341059/grafana-observability-survey</link>
            <guid isPermaLink="false">https://www.oschina.net/news/341059/grafana-observability-survey</guid>
            <pubDate>Sun, 23 Mar 2025 06:06:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>【直播】AutoDev 即 MCP 服務</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                                                                                                                                        &lt;div&gt; 
 &lt;p&gt;當我們想讓 AI 輔助開發的時候，不僅僅是想要讓它寫代碼，而是能像真人一樣自如地「使用各種工具」，比如 AI 生成代碼後，自動調用 Git 提交、觸發 Jenkins 構建，並通過 Docker 部署到測試環境，等等。&lt;/p&gt; 
 &lt;p&gt;但實際上，當前主流 AI 編程工具確實主要聚焦於 IDE 內部的代碼補全與建議功能，其核心能力基於當前編輯上下文進行代碼生成，無法直接操作構建工具（如 Maven/Gradle）、測試框架（如 JUnit）或部署系統等外部工具鏈。&lt;/p&gt; 
 &lt;p&gt;不過，有了 MCP&lt;span style=&quot;background-color:#ffffff&quot;&gt;（Model Context Protocol）&lt;/span&gt;，一切都不一樣了。&lt;span style=&quot;background-color:#ffffff&quot;&gt;MCP 是由 Anthropic 公司（Claude 模型） 推出的一個協議，它通過提供一種標準化的接口，LLM 應用就可以訪問外部信息、工具和資源。&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Thoughtworks AI 輔助開發負責人&lt;/strong&gt;&lt;span style=&quot;background-color:#ffffff&quot;&gt;&lt;strong&gt;黃峯達花了一天時間，在 AutoDev 中實現了相關的功能&lt;/strong&gt;：即 AutoDev 作為一個 MCP 服務，可以被任何 Agent Tool 調用；AutoDev 作為一個 MCP 客戶端，可以調用任何 MCP 服務。&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img height=&quot;283&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-c3e807d75bc0740ed7ca7429e859f6ad312.png&quot; width=&quot;400&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
 &lt;div&gt; 
  &lt;p&gt;AutoDev 是一個開源的 AI 輔助研發插件，在 Intellij IDEA 等 IDE 中，提供了類似於 Cursor Composer、Windsurf 的 AI 程序員自動編程能力。&lt;strong&gt;&lt;span style=&quot;background-color:#ffffff&quot;&gt;3 月 31 日晚，黃峯達將做客開源中國直播間——「OSC 開源社區」視頻號《技術領航》欄目，分享 &lt;/span&gt;AutoDev 如何輔助開發&lt;span style=&quot;background-color:#ffffff&quot;&gt;：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;MCP 為什麼會改變 AI 輔助開發？&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;第二代 AI IDE 的基本思路與架構，以及 AutoDev 的落地實現&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;實操演示：AutoDev 與 MCP 的雙向服務&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;AutoDev 未來演進計劃，以及正在實現的新功能&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;微信掃碼，預約直播：&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img height=&quot;715&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-b8329395a8e51db29496ab664de93d48c92.png&quot; width=&quot;400&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;直播標題：&lt;/strong&gt;下一代 AI IDE：AutoDev x MCP 的雙向服務&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;直播時間：&lt;/strong&gt;3 月 31 日（週一 ）19:00-20:00&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;直播平台：&lt;/strong&gt;視頻號 「OSC 開源社區」&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;直播嘉賓：&lt;/strong&gt;黃峯達（Phodal）&lt;/p&gt; 
 &lt;div&gt; 
  &lt;div&gt; 
   &lt;p&gt;&lt;strong&gt;嘉賓介紹：&lt;/strong&gt;黃峯達（Phodal），Thoughtworks AI 輔助開發與開源解決方案負責人，開源 Unit Mesh AI 輔助研發方案的發起人，包含 AI IDE 插件 AutoDev 等工具；智能體編程語言 Shire 的創始人，架構治理平台 ArchGuard 的核心開發者。他在生成式 AI 輔助需求分析、開發和質量保障方面為多家金融和互聯網企業提供落地支持，著有《前端架構：從入門到微前端》《自己動手設計物聯網》等多本書籍。&lt;/p&gt; 
   &lt;hr&gt; 
   &lt;div&gt; 
    &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;另外，我們還建了一個交流羣，一起聊聊自己喜歡的開源項目～～當然啦，如果你有什麼特別棒的開源項目，可以推薦過來呀～&lt;/p&gt; 
    &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;img height=&quot;300&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-0e7c43c0b0553350855a379af00c6c7c15d.jpg&quot; width=&quot;300&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
    &lt;div&gt; 
     &lt;hr&gt; 
     &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;「&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;技術領航&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;」&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;是開源中國 OSCHINA 推出的一檔直播欄目，旨在為&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;開源軟件、商業產品、前沿技術、知名品牌活動等各類項目&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;提供一個展示平台，每週五晚上開播&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;欄目邀請項目的創始人、核心團隊成員或資深用戶作為嘉賓，通過路演式直播分享項目的亮點和經驗，有助於提高項目的知名度，吸引更多的用戶和開發者關注。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
     &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span&gt;&lt;span style=&quot;background-color:#ffffff&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;如果你手上也有好的項目，想要跟同行交流分享，歡迎聯繫我，欄目隨時開放～&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
     &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:center&quot;&gt;&lt;img height=&quot;537&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-4dd54c1b0b817689ceefa15aa66d79cfae8.png&quot; width=&quot;400&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
    &lt;/div&gt; 
   &lt;/div&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/u/3859945/blog/18006529</link>
            <guid isPermaLink="false">https://my.oschina.net/u/3859945/blog/18006529</guid>
            <pubDate>Sun, 23 Mar 2025 03:56:00 GMT</pubDate>
            <author>原創</author>
        </item>
        <item>
            <title>寶馬與阿里巴巴達成 AI 領域戰略合作</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;寶馬中國宣佈與阿里達成 AI 領域戰略合作，聚焦大語言模型等技術，阿里通義大模型將應用於中國市場的寶馬新世代系列車型。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;326&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-ae0f47ae0e235363ed9ac48a8bb6615d880.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;據瞭解，基於阿里巴巴通義大模型，以斑馬元神 AI 為基礎，全新 BMW 智能個人助理採用與阿里巴巴共同開發的寶馬定製 AI 引擎，將於 2026 年率先搭載於中國生產的 BMW 新世代車型。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;其核心能力將包括擬人化溝通、多智能體協同及開放生態整合，能夠實現精準意圖捕捉、複雜指令解析、模糊語義理解及嚴謹邏輯推演，令互動體驗更加自然流暢。寶馬還將首次推出主動交互推薦功能，利用車內傳感器和攝像頭獲取到的數據進行分析，主動給予客戶服務與關懷，比如當客戶遺落個人物品在座位時系統將主動提醒乘客。上述 AI 技術和體驗將覆蓋寶馬全動力系統和全車型陣列，油電同智。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;日前，寶馬還發布中國 360 度全鏈 AI 戰略，包含 AI 聚焦提升用戶體驗，AI 賦能業務流程提質增效，供應鏈合作共贏 3 大支柱和 AI 企業理念。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;今年 2 月，阿里巴巴還與蘋果公司達成合作，為國行版的 iPhone 用戶提供 AI 功能。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/341045</link>
            <guid isPermaLink="false">https://www.oschina.net/news/341045</guid>
            <pubDate>Sun, 23 Mar 2025 03:55:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>全新開源！邊緣設備也可運行的推理模型 RWKV7-G1 0.4B 正式發佈</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;2025 年 3 月 25 日，RWKV 基金會開源了一箇中低端設備也可以運行的推理模型（Reasoning Model）：&lt;strong&gt;RWKV7-G1&lt;/strong&gt; 0.4B。&lt;/p&gt; 
&lt;p&gt;RWKV7-G1 0.4B 具備其它同尺寸模型不具備的&lt;strong&gt;推理能力&lt;/strong&gt; ，同時還支持現實世界 100+ 種語言。在實際測試中，RWKV7-G1 0.4B 模型已經能夠完成難度較高的&lt;strong&gt;多語言和代碼任務&lt;/strong&gt;。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;RWKV7-G1 0.4B 推理模型基於 World v3.5 數據集訓練。它比此前發佈的 RWKV7-G1 0.1B 更強，且性能超越了同參數量的 Transformer 架構模型。&lt;/p&gt; 
 &lt;p&gt;World v3.5 數據集包含更多小説、網頁、數學、代碼和 reasoning 數據，總數據為 5.16T tokens。我們隨機採樣了 2T token 的數據來訓練 RWKV7-G1 0.4B。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;我們也&lt;strong&gt;開源了 RWKV 模型端聊天 APP&lt;/strong&gt;，方便大家體驗 RWKV-7 模型。&lt;/p&gt; 
&lt;h2&gt;模型評測&lt;/h2&gt; 
&lt;h3&gt;英語和多語言能力&lt;/h3&gt; 
&lt;p&gt;RWKV7-G1 0.4B 英語和多語言能力&lt;strong&gt;顯著領先&lt;/strong&gt;於同參數的開源模型：&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;2025-03-25-RWKV7-G1-eval-en&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-f4b63fb9cda392c8b9c1ce4afb3b56cefbd.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;無法作弊的評測&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fspaces%2FJellyfish042%2FUncheatableEval&quot; target=&quot;_blank&quot;&gt;Uncheatable Eval&lt;/a&gt; 是&quot;無法作弊的評測&quot;，它使用最新的論文和新聞文章等實時數據，評估開源大語言模型的真實建模能力和泛化能力。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;RWKV7-G1 0.4B 的 Uncheatable Eval 綜合得分&lt;strong&gt;在同參數規模的開源模型中處於領先地位&lt;/strong&gt;：&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;RWKV7-G1-Uncheatable-Eval&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-d787fdf3f4b39c4b56fcf0b0ae0a310a2a4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;RWKV7-G1 0.4B 甚至&lt;strong&gt;超越了部分 1.5B 模型&lt;/strong&gt;：&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;RWKV-7 G1 0.4B 超越部分 1.5B 模型&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-45b50e8249cb35497eeb008f29cb005c91e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;模型實測&lt;/h2&gt; 
&lt;h3&gt;多語言能力&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;RWKV7-G1 0.4B 的多語言能力比 G1 0.1B 更強。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;下面是 G1 0.4B 把中文翻譯為英語和德語的推理過程和翻譯結果，&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;漢語到英語&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-d19025d44d8f04737b5df32096e42e1f57d.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;漢語到德語&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-f1b9a3ef2b3edb2b4e3589f201bd6a0a6f9.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;可以在 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fspaces%2FBlinkDL%2FRWKV-Gradio-2&quot; target=&quot;_blank&quot;&gt;Hugging Face Gradio Demo&lt;/a&gt; 體驗更多語言。&lt;/p&gt; 
&lt;h2&gt;代碼能力&lt;/h2&gt; 
&lt;p&gt;RWKV7-G1 0.4B 已經擁有能準確完成一些進階任務的能力，下面是使用 RWKV7-G1 0.4B 寫歸併排序的示例。 &lt;img alt=&quot;歸併排序&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-1c8d6c1459b4c9e3fe3dfff81e6647a6064.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;模型試用&lt;/h2&gt; 
&lt;p&gt;我們提供了多個在線 demo ，也提供移動端聊天 APP。&lt;/p&gt; 
&lt;h2&gt;在線 demo（續寫模式）&lt;/h2&gt; 
&lt;p&gt;可以在 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fspaces%2FBlinkDL%2FRWKV-Gradio-2&quot; target=&quot;_blank&quot;&gt;Hugging Face Gradio Demo&lt;/a&gt; 試用 RWKV7-G1 0.4B 模型。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Hugging Face Gradio Demo：&lt;/strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fspaces%2FBlinkDL%2FRWKV-Gradio-2&quot; target=&quot;_blank&quot;&gt;https://huggingface.co/spaces/BlinkDL/RWKV-Gradio-2&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;RWKV G1 的整體 prompt 格式與 RWKV-7-World 模型類似，可選使用 &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; 標籤開啓 reasoning 功能：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;User: 我已經是全速前進了!

Assistant: &amp;lt;think&amp;gt;


&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;在線 demo（聊天模式）&lt;/h3&gt; 
&lt;p&gt;為了方便社區體驗 RWKV-G1 模型，我們也提供了&lt;strong&gt;聊天模式&lt;/strong&gt;的在線 demo。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Hugging Face&lt;/strong&gt;：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fspaces%2FRWKV-Red-Team%2FRWKV-LatestSpace&quot; target=&quot;_blank&quot;&gt;https://huggingface.co/spaces/RWKV-Red-Team/RWKV-LatestSpace&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;魔搭 demo&lt;/strong&gt;：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmodelscope.cn%2Fstudios%2FRWKV-Red-Team%2FRWKV-LatestSpace%2Fsummary&quot; target=&quot;_blank&quot;&gt;https://modelscope.cn/studios/RWKV-Red-Team/RWKV-LatestSpace/summary&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;可在此體驗已完成訓練的 RWKV-7 G1 0.1B 和 0.4B 模型，也可以切換到其他正在&lt;strong&gt;訓練中&lt;/strong&gt;的 G1 模型，如 G1 1.5B/2.9B。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;chat-demo&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-3c5f79dcc16fccfaab0af74c3812b401026.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;這個精美的 RWKV 對話界面由 RWKV 社區成員 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fleoncat.top%2F&quot; target=&quot;_blank&quot;&gt;@Leon&lt;/a&gt; 開發，並在 GitHub 倉庫 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FSolomonLeon%2Fweb-rwkv-realweb&quot; target=&quot;_blank&quot;&gt;web-rwkv-realweb&lt;/a&gt;中開源。&lt;/p&gt; 
&lt;h3&gt;RWKV 端側聊天 APP&lt;/h3&gt; 
&lt;p&gt;我們也開發了處於內測階段的 RWKV 端側聊天 APP（Android 和 iOS 版本）。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;RWKV-7 G1 0.4B 扮演朋友&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-73017fcfb107a314eab6b36f0bcd4a6bfc9.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;在下列地址下載 APP：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Android&lt;/strong&gt; : &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.pgyer.com%2Frwkvchat&quot; target=&quot;_blank&quot;&gt;https://www.pgyer.com/rwkvchat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;iOS (TestFlight)&lt;/strong&gt; : &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftestflight.apple.com%2Fjoin%2FDaMqCNKh&quot; target=&quot;_blank&quot;&gt;https://testflight.apple.com/join/DaMqCNKh&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;貫徹開源開放的宗旨，RWKV 端側聊天 APP 也已開源&lt;/strong&gt; ，在 GitHub &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FMollySophia%2Frwkv_mobile_flutter&quot; target=&quot;_blank&quot;&gt;rwkv_mobile_flutter&lt;/a&gt; 倉庫中可以看到項目代碼。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;測試數據顯示：經過 NPU 優化後，RWKV-7 1.5B 模型在高通 8Gen3 手機芯片實現了 &lt;strong&gt;62 token/s&lt;/strong&gt; 的推理速度，G1 0.1B 模型的推理速度則高達 &lt;strong&gt;170 token/s&lt;/strong&gt;。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;模型下載&lt;/h2&gt; 
&lt;p&gt;下載已完成訓練的 RWKV7-G1 0.1B/0.4B 模型：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Hugging Face：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2FBlinkDL%2Frwkv7-g1%2Ftree%2Fmain&quot; target=&quot;_blank&quot;&gt;https://huggingface.co/BlinkDL/rwkv7-g1/tree/main&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;魔搭社區：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmodelscope.cn%2Fmodels%2FRWKV%2Frwkv7-g1%2Ffiles&quot; target=&quot;_blank&quot;&gt;https://modelscope.cn/models/RWKV/rwkv7-g1/files&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;WiseModel：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwisemodel.cn%2Fmodels%2Frwkv4fun%2FRWKV-7-G1%2Ffile&quot; target=&quot;_blank&quot;&gt;https://wisemodel.cn/models/rwkv4fun/RWKV-7-G1/file&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;下載其他訓練中的 RWKV7-G1 模型：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Hugging Face：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2FBlinkDL%2Ftemp-latest-training-models%2Ftree%2Fmain&quot; target=&quot;_blank&quot;&gt;https://huggingface.co/BlinkDL/temp-latest-training-models/tree/main&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;魔搭社區：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmodelscope.cn%2Fmodels%2FRWKV%2Ftemp-latest-training-models%2Ffiles&quot; target=&quot;_blank&quot;&gt;https://modelscope.cn/models/RWKV/temp-latest-training-models/files&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;G1 模型發佈計劃&lt;/h2&gt; 
&lt;p&gt;當前已發佈 G1 0.1B/0.4B 模型，正在訓練 G1 1.5B/2.9B，具體發佈計劃如下：&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;發佈計劃&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-19b1612de03f0a59d6fb7a00335bad56ff4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;我們也在同時準備更大更優的數據集 &lt;strong&gt;World v3.7&lt;/strong&gt;，用於 G1 7B 訓練。&lt;/p&gt; 
&lt;h2&gt;llama.cpp 已適配 RWKV-7&lt;/h2&gt; 
&lt;p&gt;隨着 RWKV 社區開發者 Molly 的 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fggml-org%2Fllama.cpp%2Fpull%2F12412&quot; target=&quot;_blank&quot;&gt;PR 被合併&lt;/a&gt;，llama.cpp 現已支持 RWKV-7 模型。&lt;/p&gt; 
&lt;p&gt;我們也會繼續向 llama.cpp 推送 RWKV-7 G1 模型的聊天模板，以支持 G1 模型，的推理（Reasoning）功能。&lt;/p&gt; 
&lt;h2&gt;加入 RWKV 社區&lt;/h2&gt; 
&lt;p&gt;歡迎大家加入 RWKV 社區，可以從 RWKV 中文官網瞭解 RWKV 模型，也可以加入 RWKV 論壇、QQ 頻道和 QQ 羣聊，一起探討 RWKV 模型。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;📖 RWKV 中文文檔：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.rwkv.cn&quot; target=&quot;_blank&quot;&gt;https://www.rwkv.cn&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;💬 RWKV 論壇：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcommunity.rwkv.cn%2F&quot; target=&quot;_blank&quot;&gt;https://community.rwkv.cn/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;🐧 QQ 頻道：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fpd.qq.com%2Fs%2F9n21eravc&quot; target=&quot;_blank&quot;&gt;https://pd.qq.com/s/9n21eravc&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;📺 BiliBili 視頻教程：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fspace.bilibili.com%2F3546689096910933&quot; target=&quot;_blank&quot;&gt;https://space.bilibili.com/3546689096910933&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;歡迎大家基於 RWKV-7 進行創業、科研，我們也會為基於 RWKV 的項目提供技術支持。&lt;/p&gt; 
 &lt;p&gt;如果您的團隊正在基於 RWKV 創業或開展研究，請聯繫我們！（在&quot;RWKV 元始智能&quot;微信公眾號留言您的聯繫方式，或發送郵件到&quot;contact@rwkvos.com&quot;。）&lt;/p&gt; 
&lt;/blockquote&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/341044</link>
            <guid isPermaLink="false">https://www.oschina.net/news/341044</guid>
            <pubDate>Sun, 23 Mar 2025 03:50:00 GMT</pubDate>
            <author>來源: 投稿</author>
        </item>
        <item>
            <title>直播間互動框架性能優化與穩定性實踐</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                                                                                                                                        &lt;h1&gt;導讀&lt;/h1&gt; 
&lt;p&gt;直播間互動體驗框架技術實踐，揭祕性能與穩定性優化之道，快來探索吧！在百度直播間歌會紅包等活動中，我們創新性地將紅包互動與高質內容深度融合，通過技術架構升級與系統性優化，打造了&quot;音樂+紅包&quot;（邊聽歌邊搶紅包）的沉浸式體驗。本次實踐顯著提升了直播間的併發承載能力、實時互動響應速度和用戶參與滿意度，同時沉澱出可複用的技術方案，為後續大型直播活動奠定堅實基礎。&lt;/p&gt; 
&lt;h1&gt;01&amp;nbsp;&lt;strong&gt;百度直播間歌會紅包運營活動介紹&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;為提升直播內容質量和用戶粘性，需注入多元化內容，增強直播間多樣性和觀賞性。同時，通過活動裂變擴大影響力，吸引特定用戶羣體，保持用戶新鮮感和期待感，為平台長期發展奠定基礎。&lt;/p&gt; 
&lt;p&gt;為落實直播歌會目標要求，需加快直播間互動體驗框架建設，探索新型混合模式和沉澱通用能力，着力適配重點業務場景，打造&quot;音樂+紅包&quot;的互動體驗，提升直播間品質：&lt;/p&gt; 
&lt;p&gt;一是通用基礎。主要包括組件複用、大圖壓縮等減少產物體積，頁面異常、性能、白屏監控，BFF 服務編排擴縮、穩定性監控等。&lt;/p&gt; 
&lt;p&gt;二是訪問保障。主要包括頁面多域名容災、開啓強緩存；字體、圖片、CSS、JS 等靜態文件單獨 CDN 強緩存域名，開啓多級緩存等。&lt;/p&gt; 
&lt;p&gt;三是紅包性能。主要包括頁面預靜態化、數據預加載、文檔預取、資源預取、視圖預渲染、動效降級等。&lt;/p&gt; 
&lt;p&gt;四是開發體驗。主要基於直播前端一站式，建強隊伍，確保項目開發流程規範統一，搭建增質增效的研發環境等。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-a06a28d331513eaca8f04e99d8cdeb0e87a.jpg&quot; alt=&quot;圖片&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h1&gt;02 體積&lt;/h1&gt; 
&lt;h2&gt;&lt;strong&gt;2.1 頁面劃分&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;在大型產品需求中，通過合理的頁面劃分策略，實現高效開發與維護。面對產品需求中羅列的多樣玩法功能和 19 種以上的紅包狀態，研發團隊面臨的首要挑戰是如何將這些功能合理的拆解成多個頁面承載。合理的頁面劃分不僅關乎用戶體驗的流暢性，更是減小產物體積、提升跨頁面資源緩存利用率的關鍵。通過深入分析業務邏輯與用戶行為路徑，我們精心設計了頁面邊界，確保每個頁面和組件都承載着唯一元素，同時避免了冗餘代碼的產生。此外，這一策略還極大地促進了開發團隊的協作效率，明確的頁面劃分減少了代碼衝突的可能性，使得團隊成員可以高效並行集成，從而加速了開發迭代週期。在直播間端能力的規範化構建上，同樣遵循了通用化這一原則。&lt;/p&gt; 
&lt;p&gt;在頁面劃分時，我們非常注重跨頁面資源的最優利用，通過策略性地緩存 HTML、CSS 和 JavaScript 等資源，確保一旦用戶在任意時刻首次觸發了紅包彈出事件，這些資源即可被全面緩存，使用戶在後續的頁面切換過程中無需再次加載這些核心資源。&lt;/p&gt; 
&lt;p&gt;通過一系列設計舉措，劃分多頁應用（MPA）10+個、單頁應用（SPA）20+個、紅包組件狀態（Component）19+個、規範化直播間端能力（Scheme）30+個，每一項都經過精心設計，共同作用於提升應用的整體性能，為用戶帶來更加輕盈、快速且協同良好的使用體驗。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;2.2&amp;nbsp;性能優化&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;在直播歌會搶紅包運營活動中，Web 頁佔據了 80% 的比重，對於每一個依賴較多網絡資源的玩法頁面，在直播間中實現即時加載和快速展現確實面臨較大挑戰，尤其是在高併發、低延遲的場景下。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-d5e35a55efae3715032ff0e568633c1d733.jpg&quot; alt=&quot;圖片&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;△頁面展現過程&lt;/p&gt; 
&lt;p&gt;為了有效應對這些挑戰，通過深入分析頁面展現過程中的各個環節，直播間互動框架提煉出七種通用的優化方案。旨在提升用戶交互體驗、增強系統的整體性能。並確保直播間玩法在高併發場景下依然能夠流暢運行。這些優化方案覆蓋了從頁面加載、資源獲取到實時交互的各個方面，形成了一個全方位的性能提升樣板，具體方案如下：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-691f932fd36dd48901276bce6434bfa98b5.png&quot; alt=&quot;圖片&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;2.3&amp;nbsp;頁面預靜態化（SSG）&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;在直播歌會搶紅包的場景中，所有不變的內容（如活動規則、活動主頁框架等）使用 SSG 能夠顯著提高頁面通用靜態內容的加載速度，同時通過集成 CSR 可以實現部分動態內容的及時更新。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-0f6976513c5f401b3f75425663a7bc78360.jpg&quot; alt=&quot;圖片&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;△框架原生 SSG Webpack 插件&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-f217081c740651f27fbd3fe8736d8fc59d7.png&quot; alt=&quot;圖片&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;△圖 1：活動規則 △圖 2：攢百元半屏頁 △圖 3：支線攢碎片&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;2.4&amp;nbsp;頁面靜態化（SSR)&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;在直播歌會搶紅包的場景中，節目單頁作為用戶獲取歌曲節目信息的第一入口，其快速加載至關重要。SSR 提供快速的節目單頁初始加載，後續通過客戶端的 JavaScript 動態增強功能（如進度提醒、節目回放等）獲得更豐富的交互體驗。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-c8d4e5358f9b2488dd9dbdc7708913653ab.png&quot; alt=&quot;圖片&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;2.5&amp;nbsp;增量靜態渲染（ISR）&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;在直播歌會搶紅包的場景中，對於實時性要求極高的紅包搶奪場景，ISR 的動態更新和實時交互特性為活動的各個環節提供了實時回顯的用戶體驗：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;首先，在全屏紅包彈窗頁（如初始紅包、任務紅包和裂變紅包）中，ISR 使得頁面無需刷新即可實時更新用戶的紅包狀態。當用戶參與活動或完成任務時，ISR 的快速響應確保用戶能即時獲得任務狀態和獎勵領取情況，增強了用戶的參與感與互動性。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;對於實時輪次切換功能，ISR 保障用戶迅速在遊戲階段間切換，提升了同頁面不同狀態的連續性。當活動結束時，系統能夠快速通知用戶並更新活動狀態為下線，避免誤導用戶繼續參與。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;在內容分享與社交互動方面，ISR 處理高效的頁面加載與實時顯示，如微信邀請和海報分享，保證用戶能快速刷新助力進度。在邀請分享頁，主態用戶能立即看到受邀好友的參與情況和貢獻，進一步增強社交互動體驗。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-caeada216acb857ffbe6901babb24ff4074.png&quot; alt=&quot;圖片&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;2.6&lt;/strong&gt;&amp;nbsp;數據預取（&lt;strong&gt;Prefetch Data）&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;在直播歌會搶紅包的場景中，通過 NA 與 H5 之間的有效數據預取和緩存銜接，實現了端數據的複用，有效減少與服務器的交互頻率，消除了數據加載的等待時間，確保了在直播環境中用戶能夠高效參與活動：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;預取皮膚元素配置，進入直播間後，NA 會預取皮膚元素配置，預加載與活動相關的皮膚素材，並將這些信息進行緩存，包括頁面主題色和紅包動畫等。同時，前端 JavaScript 能夠在頁面彈出時，通過端能力或全局變量直接獲取相關數據，用戶不需要等待皮膚配置加載，界面視覺能夠立即呈現，從而實現在操作上的流暢體驗。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;攢百元紅包的進度更新，在活動進行中，用戶需要實時查看攢百元紅包的進度，通過數據預取的方式，能夠迅速更新至用戶界面。在啓動 WebView 的同時，NA 實現數據的並行獲取。這意味着在用戶點擊掛件後，相關的數據請求會立即開始，前端 JavaScript 則能夠在執行時通過端能力直接獲取這些已經預取的數據，降低了數據延遲加載等待時間，增強了用戶參與活動的效率。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-956e641a7cef65f27ac27b6f1f75e9fa42f.png&quot; alt=&quot;圖片&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;2.7&amp;nbsp;文檔預取 (Prefetch HTML)&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;在互動性較強的直播歌會搶紅包的場景中，用戶不僅可以觀看演出，還能參與搶紅包活動。為提供最佳的用戶體驗，確保用戶在首次點擊不同功能時能夠快速上屏相關內容，採用文檔預取能力在後台主動下載歌會相關 HTML 內容，如攢百元半屏頁、節目單頁等。當用戶最終點擊某個鏈接時，直接從內存中讀取 HTML 文檔內容，無需網絡請求，從而顯著提高頁面加載速度，確保用戶在直播間裏的互動預期。&lt;/p&gt; 
&lt;p&gt;通過數據結果來看，文檔預取的效果非常顯著。在優化了節目單頁的性能後，Android 用戶的首屏加載時間從 3 秒級減少到 500 毫秒級，iOS 用戶的首屏加載時間從 2.5 秒級減少到 500 毫秒級。這樣的性能提升顯然改善了用戶體驗，使得用戶能夠快速獲取所需信息，進而積極參與到活動中，營造出活躍的直播間氛圍。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-321ce9dfad640aac1a82e6ac3361f1663dd.jpg&quot; alt=&quot;圖片&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;△Prefetch SSR/CSR HTML&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-3e468dfa83832e491549cbc9e62ba8750d4.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;2.8&amp;nbsp;資源預取（&lt;strong&gt;Prefetch Resource）&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;在直播歌會的場景中，用戶參與搶紅包是一個關鍵的互動環節。在此過程中，確保紅包彈出的多層動畫和紅包圖能夠迅速、完整地展示對於增強用戶體驗至關重要。為此，資源預取在這一場景中得到了有效應用，在正式直播活動開始前期，後台服務主動下載、緩存、更新關鍵資源，包括紅包的動畫文件和高質量的紅包皮圖像。以確保當紅包正式彈出時，最新的文件已被準備妥當，用戶能夠立即看到完整的紅包圖和流暢的動畫效果，避免了圖片逐塊加載造成的卡頓和不完整展示。&lt;/p&gt; 
&lt;p&gt;通過數據結果來看，資源預取的效果非常顯著。Android 用戶資源加載耗時提升幅度約 46.7%，iOS 用戶資源加載耗時提升幅度達 86.1%，大幅提升了整體互動體驗，使用戶在關鍵時刻享受到快速且流暢的操作體驗。&lt;/p&gt; 
&lt;p&gt;為了確保資源預取的有效性，需要注意以下幾點：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;預取的資源應以用戶行為的合理預測為基礎，避免過度預取，從而造成帶寬浪費。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;採用分模塊的離線包設計，將每個模塊的資源單獨管理。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;在活動結束後，應及時下線不再需要的資源，釋放帶寬和用戶手機空間。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-432cea91c994b4897164e59311d45182bf1.jpg&quot; alt=&quot;圖片&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-280432b02417e32f6ec6fc85535f7f08a79.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;2.9&amp;nbsp;視圖預渲染（&lt;strong&gt;Prerender WebView）&lt;/strong&gt;&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;在直播歌會的場景中，觀眾們期待快速響應的搶紅包互動體驗，此時視圖預渲染能力發揮了重要作用。當用戶進入直播間後，提前在後台加載並渲染搶紅包頁面內容，並註冊頁面可見性監聽器。即使用戶專注於觀看直播，紅包頁面也已準備妥當。用戶點擊按鈕，搶紅包頁面便迅速顯示，無需等待加載和渲染時間，同時觸發監聽器實時更新數據。這樣的即時反饋使得用戶幾乎可以瞬間查看搶紅包的結果，極大提升了參與的積極性和體驗感，進一步增強了直播的互動樂趣。&lt;/p&gt; 
&lt;p&gt;在預渲染過程中，僅對用戶頻繁訪問的頁面進行預渲染，避免資源浪費，確保當前視圖性能不受影響。由於預渲染佔用內存資源，因此需要控制 WebView 的數量，防止內存泄漏。在實施時應關注內存管理、時機選擇、兼容性和安全性，以靈活適應具體應用場景。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-383b13dd43ff5d680e92e00fc4adc64519a.jpg&quot; alt=&quot;圖片&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-fa16963f047697d9fc6bfef362543f8d08f.gif&quot; alt=&quot;圖片&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h1&gt;03 穩定性&lt;/h1&gt; 
&lt;h2&gt;&lt;strong&gt;3.1 彈窗穩定性&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;保障直播間紅包彈層的進退場穩定性，防止透明彈層卡住以致用戶無法互動，是一項關鍵挑戰。在直播間中，紅包彈層通過覆蓋全屏透明 WebView 實現，且與動畫控制密切相關，用戶在拆紅包動畫播放過程中無法進行任何交互，關閉按鈕在動畫結束後才會顯示。這要求我們確保紅包動畫的持續時間和效果穩定，以便在合適的時機正確顯示關閉按鈕。為確保紅包彈窗正常退出，尤其是在 H5 頁面渲染異常或網絡不穩定的情況下，用戶也能得到一個狀態友好的反饋。保障直播間搶紅包互動的穩定性，我們設計了「一次握手」和「雙重兜底」策略：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;一次握手，即 Web 內容健康檢查：&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;JavaScript 通過 WebContentLoaded 端能力，表示 H5 成功接管用戶交互，並通知 Native 端取消 WebView 的超時銷燬策略，以確保全屏紅包彈窗能夠穩定展示。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;如果 H5 接管未在規定時間內完成，Native 端將銷燬上層全屏透明的 WebView。這一措施確保用戶不會因彈窗問題而中斷觀看體驗，從而能夠持續與直播間進行交互。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;雙重兜底，即 NA 兜底頁和 H5 兜底頁：&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;NA 兜底頁，當 HTML 入口文件請求異常時，展示 Native 兜底頁面，確保用戶有可見的替代內容。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;H5 兜底頁，在 JS 業務組件發生異常（例如接口請求異常、端能力調用失敗、組件內部異常、重要資源缺失）時，展示 H5 兜底內容，為用戶提供實質反饋。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-a618284a079f94a36954256647ec878cc28.jpg&quot; alt=&quot;圖片&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-9643dfcee6987b9b6f75ca7a9978c27b0b5.png&quot; alt=&quot;圖片&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&amp;nbsp; &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;△圖 1：NA 兜底頁 △圖 2：H5__兜底頁 △圖 3：請求______兜底&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;3.2&amp;nbsp;動效降級&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;炫酷的動效的表現直接影響用戶的體驗，在直播歌會場景中，紅包動效由複雜的元素組成，包括 Lottie 動畫、AFX 透明視頻和 CSS 動畫。炫酷的動效雖然可以增強視覺吸引力，但在低端手機上可能導致卡頓。為確保所有用戶可以順暢參與活動，我們實施了分級動效降級策略：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;高性能設備（High）：在高性能設備上，展示完整的動畫和豐富的動態效果，享受到豐富的視覺效果。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;低性能設備（Low）：對於低端手機，複雜的動效將被簡化為靜態圖像或低複雜度的 CSS 動畫。例如，紅包拆開時只展示基本的靜態圖形，替代激烈的動態效果，確保用戶能夠正常閲讀紅包金額，而不至於因動效卡頓而影響體驗。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;分級動效降級策略能夠根據當前手機的實時性能情況，在用戶點擊拆紅包時動態調整展示的動效級別，確保以最佳效果參與活。這種適應性有效地解決了不同設備用戶在參與紅包活動時可能遇到的性能問題，從而提升整體用戶體驗的品質。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-7968ce45d177cbd1d1d3dd1b6d7f0c28f4b.jpg&quot; alt=&quot;圖片&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-5c9ea82e15ec367fd493696b4b23d42eb40.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;3.3&amp;nbsp;組件響應&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;隨着用戶體驗的不斷優化，直播歌會搶紅包活動中頁面組件的運行環境日益複雜。特別是在複雜組件的開發中，組件開發者必須意識到各項適配工作的必要性，以確保用戶體驗與開發體驗之間的平衡。為了有效滿足用戶需求並提升開發效率，我們需要綜合考慮多個環境及其不同狀態。至此，在一個組件的設計和實現過程中，需要針對以下五種狀態進行響應和適配：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;SSG 環境（編譯線環境）：組件在編譯過程中，通過 Node.js 將公共的信息（如活動規則）提前生成靜態內容，以提供快速響應時間。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;SSR 環境（服務端環境）：組件服務器集羣上，通過 Node.js 根據用戶請求動態生成相應的內容（如歌會節目單），減去客戶端 JavaScript 加載執行時間，加快頁面首屏展示速度。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ISR 環境（客戶端環境）：組件在瀏覽器中，通過 JavaScript 進行動態渲染、響應用戶點擊、滑動等操作，通過異步接口獲取最新數據（如紅包金額、助力信息）並即時更新界面，保證用戶體驗的實時性和互動性。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;頁面可見（Visibility）：組件在瀏覽器中，通過 JavaScript 控制組件的渲染時機，僅在內容需要展示時才進行渲染（如播放紅包動畫），減少不必要的 DOM 操作，提升性能並降低資源消耗。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;動效級別（High / Low）：組件在瀏覽器中，通過 Native 端能力獲取用戶設備的性能，動態調整組件中的動效，在高性能設備上展示更炫酷的動效，在低性能設備上則展示更簡單的動效，確保流暢體驗。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h1&gt;04 總結&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;沉澱直播框架能力&lt;/strong&gt;：通過優化直播間視圖容器組件，並形成標準化的組合能力樣板，拉昇直播間活動頁面的性能水準，這些方案具備良好複用性，適用於未來各種直播活動。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;系統穩定性保障&lt;/strong&gt;：組件複用、性能監控和容錯機制，減少重複開發和維護成本，進行壓力測試與優化，提升系統可靠性和用戶體驗，確保高峯流量下的穩定性。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;強化互動性體驗&lt;/strong&gt;：在直播歌會中建立綜合能力框架，特別是在搶紅包等互動性強的活動中，確保用戶在享受歌會演出的同時體驗流暢的互動，鼓勵積極參與&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;————END————&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;推薦閲讀&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247604026%26idx%3D1%26sn%3Db6c6367e9bcbe2dab70a1282140ea740%26scene%3D21%23wechat_redirect&quot; target=&quot;_blank&quot;&gt;百度網盤防雪崩架構實踐&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247603987%26idx%3D1%26sn%3D4a88159ec791a37e75053139e0b4682c%26scene%3D21%23wechat_redirect&quot; target=&quot;_blank&quot;&gt;如何在百度百舸部署滿血版 DeepSeek-V3、DeepSeek-R1 模型&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247603986%26idx%3D1%26sn%3Dc10b89bf5b0e34c616c8ba230b3405ae%26scene%3D21%23wechat_redirect&quot; target=&quot;_blank&quot;&gt;首日調用客戶破 1.5 萬！DeepSeek-V3/R1 上線背後的超低推理成本技術揭祕&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247603905%26idx%3D1%26sn%3D8a870420c3865822760331c3a62d1678%26scene%3D21%23wechat_redirect&quot; target=&quot;_blank&quot;&gt;喚醒 AI 算力，專有云 ABC Stack 面向企業級智算平台的 GPU 提效實踐&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247603901%26idx%3D1%26sn%3D6d46e002ddb623aa67c0eaa2d804fea4%26scene%3D21%23wechat_redirect&quot; target=&quot;_blank&quot;&gt;百度 APP iOS 端磁盤優化實踐（上）&lt;/a&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/u/4939618/blog/17679358</link>
            <guid isPermaLink="false">https://my.oschina.net/u/4939618/blog/17679358</guid>
            <pubDate>Sun, 23 Mar 2025 03:47:00 GMT</pubDate>
            <author>原創</author>
        </item>
        <item>
            <title>蘋果向浙江大學捐贈 3000 萬元，支持編程教育</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;Apple 今日宣佈向浙江大學捐贈 3,000 萬元人民幣，深化其對中國下一代開發者的支持。該合作基於其對移動應用創新賽十年的支持，繼續加強 Apple 在大中華區長期開展的學生和開發者的教育支持。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;1458&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0326/113851_E8YI_2720166.png&quot; width=&quot;2320&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Apple 將與移動應用創新賽的合作伙伴浙江大學共同設立 Apple 移動應用孵化基金，提供最前沿的技術培訓，包括 app 開發、產品設計、市場營銷和商業運營等專業課程。該基金將通過研討會、實習和導師制等方式，建立學生開發者與行業領袖和投資者之間的聯繫，為他們提供更多商業培訓，助其在不斷發展的 iOS 應用經濟中取得成功。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0326/114039_pIPv_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Apple CEO Tim Cook 表示：「&lt;span style=&quot;color:#27ae60&quot;&gt;&lt;strong&gt;我們相信編程是一項強大的工具，它能讓人們以全新的方式進行創造、交流和解決問題。我們很榮幸能深化與浙江大學長達十年的合作關係，為下一代開發者提供技能支持，幫助他們開發創新應用，創立充滿生機的業務。&lt;/strong&gt;&lt;/span&gt;」&lt;/p&gt; 
&lt;p&gt;Apple 對移動應用創新大賽的支持已持續十年，累計共有大中華區近千所高校的 30,000 多名參賽者從中受益。許多參賽者成長為 app 工程師、企業家和教育工作者，不僅獲得了個人發展的新機遇，且通過編程為所在社區帶來積極的變化。&lt;/p&gt; 
&lt;p&gt;「我們很高興與 Apple 繼續緊密而深入地合作。」浙江大學黨委書記任少波表示，「我們將發揮雙方各自優勢，通過 Apple 移動應用孵化基金，共同攜手培育知識、能力、素質、人格全面發展的新質人才。」&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/341040</link>
            <guid isPermaLink="false">https://www.oschina.net/news/341040</guid>
            <pubDate>Sun, 23 Mar 2025 03:37:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>商湯合作歸墟機器人，推出新一代具有情感陪伴功能的人形機器人</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;歸墟機器人與商湯科技合作，&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FzwNQ15Ktqv1H9TwiBRHHDQ&quot; target=&quot;_blank&quot;&gt;推出&lt;/a&gt;新一代具有情感陪伴功能的人型機器人飛燕。該機器人搭載商湯日日新融合大模型交互版「SenseNova-5o」，具有情感陪伴和心理健康篩查干預功能。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;「通過深度應用具備實時交互能力的融合大模型，飛燕具備了強大的全景視界感知能力和深度思考能力，可實現對整個物理世界的信息理解和交互，將推動智能健康和心智陪伴領域實現人機交互體驗的突破，引領具身智能領域的創新發展。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;290&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-b8a5d350d8491fce8a473fe8166f6204cac.png&quot; width=&quot;300&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;根據介紹，「SenseNova-5o」是商湯「日日新」融合大模型的交互版本，擁有強大的實時交互、視覺識別、記憶思考、持續對話和複雜推理等能力，幫助 AI 與人類更自然、更流暢地交流。基於「SenseNova-5o」，機器人已經實現全景視界感知，可結合空間感知理解能力，合理規劃路徑與物體交互動作。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;目前，飛燕機器人已經在江蘇省第一人民醫院投入應用，通過情緒識別、多模態感知與交互等技術，完成一系列心理健康篩查、個性化幹預、情感支持等任務，不僅能減少用戶在面對真人時的焦慮，還能借助「人形」更流暢地展開溝通。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/341016</link>
            <guid isPermaLink="false">https://www.oschina.net/news/341016</guid>
            <pubDate>Sun, 23 Mar 2025 02:30:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>樹莓派推出 Raspberry Pi PoE+ 供電器</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;樹莓派基金會日前與 Microchip 合作推出&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.raspberrypi.com%2Fproducts%2Fpoe-plus-injector%2F&quot; target=&quot;_blank&quot;&gt;適用於 Raspberry Pi 設備的 PoE+ 供電器&lt;/a&gt;&lt;/u&gt;，幫助那些不太容易連接電源的樹莓派設備獲得供電。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-f0a9e1c0c64406ff8549b5357bee63eebe8.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;供電器由 Microchip 製造，設備本身提供 1 個電源連接器和 2 個網絡接口。該供電器適用於 Raspberry Pi 3B+ 及更高版本，&lt;strong&gt;售價為 25 美元&lt;/strong&gt;（不包含網絡線材）。&lt;/p&gt; 
&lt;p&gt;除此之外，樹莓派還透露即將發佈專為樹莓派 5 設計的 Raspberry Pi PoE+ HAT+。這款新品被稱為公司「最小、最高效」的受電設備（PD）配件。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/341012/raspberrypi-poe-plus-injector</link>
            <guid isPermaLink="false">https://www.oschina.net/news/341012/raspberrypi-poe-plus-injector</guid>
            <pubDate>Sun, 23 Mar 2025 02:24:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>快手：可靈 AI 累計營業收入超 1 億元</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;3 月 25 日，快手科技發佈 2024 年第四季度及全年業績。全年營收達 1269 億元，同比增長 11.8%。全年經調整淨利潤同比增長 72.5% 至 177 億元，連續兩年實現規模化盈利。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;第四季度營收為 354 億元，同比增長 8.7%；調整後淨利潤 47.01 億元，同比增長 7.8%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;快手科技創始人兼首席執行官程一笑表示：&lt;/span&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;「快手的視覺生成可靈 AI 大模型自從去年 6 月推出後持續迭代，保持全球領先的行業地位，深受國內外創作者好評。快手如今正處在人工智能技術和視頻大模型重塑視頻內容創作、提升用戶體驗並拓寬商業生態邊界的行業變革前沿。展望未來，我們會始終堅定執行 AI 戰略，深耕用戶需求，建設基於信任社區的 AI 內容與商業生態，為用戶、合作伙伴和股東創造長期價值。」&lt;/span&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;img height=&quot;337&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-62df1c7cdb5b88d4f6914a9276bb9481985.png&quot; width=&quot;300&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;在快手 2024 年第四季度及全年業績電話會上，程一笑稱，2024 年第四季度，快手平台上的 AIGC 營銷素材和虛擬數字人直播解決方案的日均消耗超過 3000 萬元。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;程一笑表示，根據快手內部測算，AI 大模型預計可以把客戶的短視頻營銷素材製作成本降低 60-70% 甚至更高。目前快手正致力於逐步把磁力引擎全面升級下一代的 AI 智能商業引擎。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;他還透露，自商業化以來截至 2025 年 2 月底，可靈 AI 的累計營業收入超 1 億元。除了 C 端用戶訂閲，可靈 AI 也面向 B 端商家提供 API 接入等服務。目前，可靈 AI 已與包括小米、亞馬遜雲科技、Freepik、藍色光標等在內的數千家國內外企業客戶建立了合作關係。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/341004</link>
            <guid isPermaLink="false">https://www.oschina.net/news/341004</guid>
            <pubDate>Sun, 23 Mar 2025 02:17:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>李開復稱 DeepSeek 將中美 AI 差距縮小至 3 個月</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;新加坡《聯合早報》報道稱，中國初創企業零一萬物首席執行官李開復説，在人工智能（AI）發展方面，中國已將與美國在某些領域的差距縮小至僅 3 個月，因為中國初創企業深度求索（DeepSeek）等公司已經研究出如何更有效地使用芯片和應用算法。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;164&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-eb70519debbb9b97b1ee7ee16d5fbaf9f89.png&quot; width=&quot;700&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;李開復在香港接受路透社採訪時説，DeepSeek 的推出表明，中國已經在基礎設施軟件工程等領域取得領先地位。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;「之前我認為差距是 6 到 9 個月，在各方面都落後。現在我認為，在一些核心技術上可能落後 3 個月，但實際上在某些特定領域領先。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;李開復形容華盛頓的半導體制裁是一把「雙刃劍」，既帶來了短期挑戰，也迫使中國企業在約束下進行創新，並提到中國企業如何開發自己的算法。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;「DeepSeek 能夠通過一種新的強化學習方式來弄清楚思路鏈，這要麼是在趕超美國，要麼是在快速學習，甚至可能更具創新性。」（參考消息網）&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/340999</link>
            <guid isPermaLink="false">https://www.oschina.net/news/340999</guid>
            <pubDate>Sun, 23 Mar 2025 02:11:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
    </channel>
</rss>