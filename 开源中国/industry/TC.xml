<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>開源中國-綜合資訊</title>
        <link>https://www.oschina.net/news/industry</link>
        <atom:link href="http://8.134.148.166:30044/oschina/news/industry" rel="self" type="application/rss+xml"></atom:link>
        <description>開源中國-綜合資訊 - Powered by RSSHub</description>
        <generator>RSSHub</generator>
        <webMaster>contact@rsshub.app (RSSHub)</webMaster>
        <language>en</language>
        <lastBuildDate>Mon, 21 Apr 2025 16:36:31 GMT</lastBuildDate>
        <ttl>5</ttl>
        <item>
            <title>預計中國市場 2025 年人形機器人本體產值將超 45 億元</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;根據 TrendForce 集邦諮詢最新數據，中國市場已有 11 家主流人形機器人本體廠商啓動 2024 年量產計劃。其中，宇樹科技、優必選、智元機器人、銀河通用、眾擎機器人、樂聚機器人等 6 家領先企業更是將 2025 年的量產規劃設定在千台以上。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style=&quot;color:#242424; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;TrendForce 集邦諮詢預計，2025 年中國市場人形機器人本體產值有望突破 45 億元人民幣。加上馬斯克關於 Tesla Optimus 2025 年數千台量產目標，預計頭部本體廠商的量產計劃將拉動中國市場人形機器人零部件供應鏈生態佈局與完整性。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#242424; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;img height=&quot;336&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-545b82ebd4c04ad7f2c9a719209e9659c66.png&quot; width=&quot;700&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#242424; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span style=&quot;background-color:#ffffff; color:rgba(0, 0, 0, 0.9)&quot;&gt;當下人形機器人產品主要應用在 B 端工業場景、高校科研以及少部分 B 端商用場景，而 C 端家用場景要求人形機器人功能多元，對機器人數據處理和自主交互能力要求較高。人形機器人從 B 端跨越到 C 端應用場景需要政策、法規、技術等行業多方面的共同努力，C 端應用場景的商業化落地仍任重道遠。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/345784</link>
            <guid isPermaLink="false">https://www.oschina.net/news/345784</guid>
            <pubDate>Sun, 13 Apr 2025 09:43:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>得物自研 DGraph4.0 推薦核心引擎升級之路</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                    
                                                                                                                                                    &lt;span id=&quot;OSC_h1_1&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;一、前言&lt;/h1&gt; 
&lt;p&gt;DGraph 是得物自主研發的新一代推薦系統核心引擎，基於 C++語言構建，自 2021 年啓動以來，經過持續迭代已全面支撐得物社區內容分發、電商交易等核心業務的推薦場景。DGraph 在推薦鏈路中主要承擔數據海選和粗排序功能，為上層精排提供高質量候選集。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;核心技術特性：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;索引層 - 支持 KV（鍵值）、KVV（鍵-多值）、INVERT（倒排）、DENSE-KV（稠密鍵值）等。&lt;strong&gt;索引存儲&lt;/strong&gt;支持磁盤 &amp;amp; 內存兩種模式，在預發等延遲壓力低場景，通過磁盤索引使用低規格服務器提供基本服務。線上場景使用內存索引保證服務穩定性，提供毫秒級延遲響應。&lt;strong&gt;索引更新&lt;/strong&gt;支持雙 buff 熱更新【內存足夠】、服務下線滾動更新【內存受限】、Kafka 流式數據實時更新等三種模式。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;查詢層 - 支持向量檢索 IVF &amp;amp; HNSW、鍵值 (KV) 查詢、倒排檢索、X2I 關聯查詢、圖查詢。對外提供 JavaSDK &amp;amp; C++ SDK。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;系統依賴架構：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;索引全生命週期管理由得物索引平台 DIP 統一管控。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;服務發現基於 ZooKeeper(zk)。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;集羣資源調度基於得物容器平台，目前已經支持 HPA。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;服務規模：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;目前在線 100+集羣，2024 年雙 11 在線突破了 100W qps。&lt;/p&gt; 
&lt;p&gt;本文主要介紹 DGraph 系統在 2024 年的一些重要改進點。主要包括兩次架構調整 + 性能優化 + 用戶體驗提升方面的一些工作。&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_2&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;二、架構升級&lt;/h1&gt; 
&lt;span id=&quot;OSC_h2_3&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;2.1 垂直拆分業務集羣支持&lt;/h2&gt; 
&lt;p&gt;在 2023 年前，DGraph 系統始終採用單一集羣架構提供服務。該架構模式在平台發展初期展現出良好的經濟性和運維便利性，但隨着業務規模擴張，單集羣架構在系統層面逐漸顯露出三重剛性約束：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;存儲容量瓶頸 - 單節點內存上限導致數據規模受限；&lt;/li&gt; 
 &lt;li&gt;網絡帶寬瓶頸 - 單物理機 Pod 共享 10Gbps 帶寬，實際可用帶寬持續承壓，推薦引擎業務中部分核心集羣 200 餘張數據表（單表需 20 分鐘級更新）的實時處理需求已遭遇傳輸瓶頸；&lt;/li&gt; 
 &lt;li&gt;計算能力瓶頸 - 單實例最大 64 核的算力天花板，難以支撐複雜策略的快速迭代，核心場景響應時效與算法複雜度形成顯著衝突；&lt;/li&gt; 
 &lt;li&gt;穩定性 - 大規格集羣對於容器調度平台不友好，在擴容、集羣故障、集羣發佈時耗時較久；基於得物平台推薦數據量增長和算法迭代需求，我們實施業務垂直拆分的多集羣架構升級，通過資源解耦與負載分離，有效突破了單節點資源約束，為複雜算法策略的部署預留出充足的技術演進空間。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;系統改進點是在 DGraph 中增加了訪問了其他 DGraph 集羣 &amp;amp; FeatureStore 特徵集羣的能力 (圖 1)。為了成本考慮，我們複用了之前系統的傳輸協議 flatbuffers，服務發現仍基於 ZooKeeper。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-c817ffca6dd3d7ecea36fdd95d20183d04a.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;圖 1 DGraph 訪問架構改進&lt;/p&gt; 
&lt;p&gt;改造的難點在圖化集羣！&lt;/p&gt; 
&lt;p&gt;目前推薦業務的核心場景都進行了圖化改造，圖化查詢是把多路召回、打散、融合、粗排等策略打包到一個 DAG 圖中一次發送到 DGraph，DGraph 的算子調度模塊根據 DAG 的描述查詢索引數據 &amp;amp; 執行算子最終把結果返回給業務系統，但這些 DAG 圖規模都很大，部分業務 DAG 圖涉及 300+算子，因此如何在垂直拆分業務中把這些 DAG 圖拆分到不同的 DGraph 集羣中是一個非常複雜的問題，我們主要做了三方面改進：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;DAG 管理 - 集羣分主集羣和從集羣【多個】，DAG 圖部署在存在主集羣中，DIP 平台會分析 DAG 的拓步結構並把屬於從集羣的部分複製出來分發給從集羣，為了保證 DAG 的一致性，只允許從主集羣修改 DAG 圖；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;集羣劃分 - 通常按召回劃分，比如 Embedding 召回、X2I 召回、實驗召回可以分別部署在不同的集羣，另外也可以把粗排等算力需求大的部分單獨放在一個集羣，具體根據業務場景調整；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;性能優化 - 核心表多個集羣存放，減少主集羣和從集羣間數據交換量。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-ed9ac500b1e0e379e07f0f870f4f6ae62bb.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;圖 2 DGraph 業務垂直拆分集羣&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_4&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;2.2 分佈式能力支持&lt;/h2&gt; 
&lt;p&gt;垂直拆分集羣，雖然把推薦 N 路召回分散到了 M 個集羣，但是每個集羣中每個表依然是全量。隨着得物業務的發展，擴類目、擴商品，部分業務單表的數據量級已經接近單集羣的存儲瓶頸。因此需要 DGraph 中引入數據水平拆分的能力。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-3da38581237c44ef46782fe58c5c0566ed6.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;圖 3 DGraph 分佈式集羣架構圖&lt;/p&gt; 
&lt;p&gt;在 DGraph 分佈式架構設計中，重點考慮了部署成本優化與業務遷移工作量：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;分佈式集羣採用【分片數 2】×【雙活節點 2】×【數據副本數 2】的最小拓撲結構，理論上需要 8 台物理節點保障滾動更新與異常容災時的穩定性。針對 CPU 負載較輕的場景，為避免獨立 Proxy 集羣帶來的額外資源開銷，DGraph 將 Proxy 模塊和 DGraph 引擎以對稱架構部署到所有節點，通過本地優先的智能路由策略（本地節點輪詢優先於跨節點訪問）實現資源利用率與訪問效率的平衡；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;在業務兼容性方面，基礎查詢接口（KV 檢索、倒排索引、X2I 關聯查詢）保持完全兼容以降低遷移成本，而 DAG 圖查詢需業務側在查詢鏈路中明確指定 Proxy 聚合算子的位置以發揮分佈式性能優勢。數據鏈路層面，通過 DIP 平台實現索引無縫適配，支持 DataWorks 原有任務無需改造即可對接分佈式集羣，同時增量處理模塊內置分片過濾機制，可直接複用現有 Flink 實時計算集羣進行數據同步。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;span id=&quot;OSC_h1_5&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;三、性能優化&lt;/h1&gt; 
&lt;span id=&quot;OSC_h2_6&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;3.1 算子執行框架優化&lt;/h2&gt; 
&lt;p&gt;在 DGraph 中，基於 DGraph DAG 圖 (參考圖 9) 的一次查詢就是圖查詢，內部簡稱 graphSearch。在一個 DAG 圖中，每個節點都是一個算子 (簡稱 Op)，算子通過有向邊連接其他算子，構成一個有向無環圖，算子執行引擎按 DAG 描述的關係選擇串行或者併發執行所有算子，通過組合不同算子 DAG 圖能在推薦場景中靈活高效的完成各種複雜任務。&lt;/p&gt; 
&lt;p&gt;在實際應用場景中受 DAG 圖規模 &amp;amp; 超時時間 (需要控制在 100ms 內) 限制，算子執行框架的效率非常重要。在最開始的版本中我們使用過 Omp &amp;amp; 單隊列線程池，集羣在 CPU 負載低於 30% 時表現尚可，但在集羣 CPU 負載超過 30% 後，rt99 表現糟糕。在降本增效的背景下，我們重點對算子執行框架進行了優化，引入了更高效的線程池 &amp;amp; 減少了調度過程中鎖的使用。優化後目前 DGraph 在 CPU 壓力超過 60% 依然可以提供穩定服務。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-314750530e377031be7838774eae9b7fecd.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;圖 4 DGraph 算子執行框架優化&lt;/p&gt; 
&lt;p&gt;線程池優化：將原 1:N 的線程池-隊列架構調整為 M:N 分組模式。具體實現為將 N 個工作線程劃分為 M 個執行組（每組 N/M 線程），各組配備獨立任務隊列。任務提交採用輪詢分發機制至對應組隊列，通過資源分區有效降低線程調度時的鎖競爭強度。&lt;/p&gt; 
&lt;p&gt;調度器優化：在 DAG 調度過程中存在兩個典型多寫場景&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;前驅算子節點完成時需並行更新後繼節點標記；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;DAG 全局任務計數器歸零判斷。原方案通過全局鎖（Graph 鎖+Node 鎖）保障原子性，但在高負載場景引發顯著鎖競爭開銷，影響線程執行效率。經分析發現這兩個狀態變更操作符合特定併發模式：所有寫操作均為單調增減操作，因此可將鎖機制替換為原子變量操作。針對狀態標記和任務計數場景，分別採用原子變量的 FetchAdd 和 FetchSub 指令即可實現無鎖化同步，無需引入 CAS 機制即滿足線程安全要求。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;span id=&quot;OSC_h2_7&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;3.2 傳輸協議編碼解碼優化&lt;/h2&gt; 
&lt;p&gt;優化 JavaSDK - DGraph 數據傳輸過程：在 DGraph 部分場景，由於請求引擎返回的數據量很大，解碼編碼耗時佔整個請求 20% 以上。分析已有的解碼編碼模塊，引擎在編碼階段會把待傳輸數據編碼到一個 FlatBuffer 中，然後通過 rpc 協議發送到業務側的 JavaSDK，sdk 解碼 FlatBuffer 封裝成 List&amp;lt;map&amp;gt; 返回給業務代碼，業務代碼再把 List&amp;lt;map&amp;gt; 轉化成 List&amp;lt;業務 Object&amp;gt;。過程中沒有併發 &amp;amp; sdk 側多了一層冗餘轉換。&lt;/p&gt; 
&lt;p&gt;優化方案如下：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;串行編碼調整為根據文檔數量動態調整編碼塊數量。各子編碼塊可以併發編碼解碼，加快編碼&amp;amp;解碼速度，提升整體傳輸性能；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;sdk 側由 Doc -&amp;gt; Map -&amp;gt; JavaObject 的轉化方式調整為 Doc -&amp;gt; JavaObject，減少解碼端算力開銷。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-781cc5f8b62293731a12f4578bddbafd4b1.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;圖 5 DGraph 傳輸編碼解碼過程優化&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_8&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;四、用戶體驗優化&lt;/h1&gt; 
&lt;span id=&quot;OSC_h2_9&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;4.1 DAG 圖調試功能優化&lt;/h2&gt; 
&lt;p&gt;目前我們已經把 DGraph DAG 圖查詢的調試能力集成到 DIP 平台。其原理是：DGraph 的算子基類實現了執行結果輸出，由於算子的中間結果數據量極大，當調試模塊發現調試標誌後會先把當前算子的中間結果寫入日誌中，數據按 TraceID + DAGID+ NodeID 組織，最終這些數據被採集到 SLS 日誌平台。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-2d918720f61f5f0f7d019c68f05998aecca.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;圖 6 DGraph DAG 圖查詢調試&lt;/p&gt; 
&lt;p&gt;從 DIP 平台調試 DAG 圖請求，首先通過 DGraph JavaSDK 的調試入口拿到 DAG 圖請求 json，填入 DIP 平台圖請求調試入口，發起請求。索引平台會根據請求體自動關聯 DAG 圖並結合最終執行結果通過頁面的方式展示。DIP 平台拿到結果後，在 DAG 圖中成功的算子節點標記為綠色，失敗的節點標記為紅色 (圖 6)。點擊任意節點可以跳轉到日誌平台查看該節點的中間結果輸出。可用於分析 DAG 圖執行過程中的各種細節，提升業務排查業務問題效率。&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_10&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;4.2 DAG 圖支持 TimeLine 分析&lt;/h2&gt; 
&lt;p&gt;基於 Chrome 瀏覽器中的 TimeLine 構建，用於 DGraph DAG 圖查詢時算子性能分析優化工作。TimeLine 功能集成在算子基類中，啓動時會記錄每個算子的啓動時間、等待時間、完成時間、執行線程 pid 等信息，這些信息首先輸出到日誌，然後被 SLS 日誌平台採集。用戶可以使用查詢時的 TraceID 在日誌平台搜索相關的 TimeLine 信息。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-8775652c29620ccf9e0595b35dd3c9cebd3.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;圖 7 DGraph DAG 圖例子&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-6a92e653aa398223b8be061bf6a425c268d.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;圖 8 使用瀏覽器查看 DGraph DAG 圖 TimeLine&lt;/p&gt; 
&lt;p&gt;當我們拿到請求的 TimeLine 信息後，通過瀏覽器加載可以通過圖形化的方式分析 DAG 執行過程中耗時分佈。圖 7 是一個 DAG 請求，它有 9 個算子節點，圖 8 是它的一次請求的 TimeLine。通過分析這些算子的耗時，可以幫助我們定位當前 DAG 圖查詢的瓶頸點在哪裏，從而精準去解決性能方面的問題。&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_11&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;4.3 DAG 圖支持動態子圖&lt;/h2&gt; 
&lt;p&gt;在 DAG 圖召回中，業務的召回通常都帶有一些固定模式，比如一個業務在一個 DAG 圖召回中有 N 路召回，每一路召回都是：① 查找數據；② 關聯可推池；③ 打散； 它們之間的區別可能僅僅是召回數據表名不同或者傳遞的參數不同。通常我們業務調整或者算法實驗調整隻需要增加或者減少部分召回，原有模式下這些操作需要去新增或者修改 DAG 圖，加上算法實驗很多，業務維護 DAG 圖的成本會非常高。&lt;/p&gt; 
&lt;p&gt;DAG 動態子圖的引入就是為瞭解決這類問題，首先我們在 DAG 圖中配置一個模板子圖，它僅僅描述一個行為模式，代表會涉及幾個算子，算子之間的關係如何，實際的參數以及召回路的數量則由業務方在發起請求時動態決定。子圖的執行和主圖的執行共用同一套調度框架，共享運行時資源以降低運行開銷。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-08756615857b6bf3b35868c2f1b8d772291.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;圖 9 DGraph 子圖&lt;/p&gt; 
&lt;p&gt;圖 9 是一個 DAG 召回使用 DAG 子圖後的變化，它有 8 路召回，一個 Merge 節點，這些召回分為兩類，一類是基於 KV 表 (ForwardSearch) 觸發的向量召回，另外一類是基於 KVV 表 (IvtSearch) 觸發的向量召回。引入 DAG 子圖後，在主圖中節點數量由 17 個降為 3 個。&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_12&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;五、展望未來&lt;/h1&gt; 
&lt;p&gt;過去四年，DGraph 聚焦於實現得物推薦引擎體系從 0 到 1 的突破，重點完成了核心系統架構搭建、算法策略支持及業務迭代空間拓展，取得多項基礎性成果。基於 2024 年底的用戶調研反饋結合 DGraph 當前的發展，後續將重點提升產品易用性、開發與運維效能及用戶體驗，同時在系統穩定性、可擴展架構和平台化建設方面持續深化。&lt;/p&gt; 
&lt;p&gt;算法團隊大量 HC，歡迎加入我們：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzkxNTE3ODU0NA%3D%3D%26mid%3D2247537831%26idx%3D1%26sn%3Ddb1464cd87a75dd8f7bcf512fb50bf70%26scene%3D21%23wechat_redirect&quot; target=&quot;_blank&quot;&gt;得物技術大量算法崗位多地上線，「職」等你來！&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;文 / 尋風&lt;/p&gt; 
&lt;p&gt;關注得物技術，每週一、三更新技術乾貨&lt;/p&gt; 
&lt;p&gt;要是覺得文章對你有幫助的話，歡迎評論轉發點贊～&lt;/p&gt; 
&lt;p&gt;未經得物技術許可嚴禁轉載，否則依法追究法律責任。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/u/5783135/blog/18181570</link>
            <guid isPermaLink="false">https://my.oschina.net/u/5783135/blog/18181570</guid>
            <pubDate>Sun, 13 Apr 2025 09:35:00 GMT</pubDate>
            <author>原創</author>
        </item>
        <item>
            <title>字節系 Agent 產品 —— 釦子空間 (Coze Space) 開啓內測</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;4 月 18 日晚間，字節系 Agent 產品 —— 釦子空間 (Coze Space) &lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FJlWyPmOwIYTXUD7tCvS1lg&quot; target=&quot;_blank&quot;&gt;宣佈&lt;/a&gt;&lt;/u&gt;開啓內測，定位通用 Agent。與其他類似產品如 manus 一樣，釦子空間採用了邀請碼制。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0421/173325_Er29_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0421/173418_u2N4_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;釦子空間有什麼特點？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;🌟從回答問題，到解決問題，讓 Agent 幫你完成更多的工作&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;自動分析需求，拆解為多個子任務&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;自主調用工具（瀏覽器、代碼編輯器等），執行任務&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;輸出完整的結果報告，例如網頁、PPT 、飛書文檔等&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;🌟&lt;strong&gt;專家 Agent 生態，讓更專業 Agent 來為你提供服務&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;「華泰 A 股觀察助手」可以為你進行每日早報生成；針對股票分析問題，助手也能可以為你答疑解惑&lt;br&gt; &lt;img height=&quot;567&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0421/173807_CWme_2720166.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;「用戶研究專家 」可以協助你進行用研資料深度分析，省時省力地助你獲取更多用戶洞察&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;🌟&lt;strong&gt;探索/規劃雙模式，更好地和 Agent 一起協作完成高難度任務&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;探索模式：讓 AI 自主動態探索，完成速度更快&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;規劃模式：讓 AI 深度思考，適合高複雜性任務&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;🌟&lt;strong&gt;MCP 擴展集成，無限拓展 Agent 能力邊界&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;首批官方支持飛書多維表格、高德地圖、圖像工具、語音合成等 MCP&lt;br&gt; &lt;img height=&quot;566&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0421/173727_B7jG_2720166.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;即將支持「釦子開發平台」發佈 MCP 至「釦子空間」&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;釦子空間官網：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fspace.coze.cn%2F&quot; target=&quot;_blank&quot;&gt;https://space.coze.cn/&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/345779/coze-space-preview</link>
            <guid isPermaLink="false">https://www.oschina.net/news/345779/coze-space-preview</guid>
            <pubDate>Sun, 13 Apr 2025 09:34:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>Meta 旗下 APP 禁用蘋果 Apple Intelligence</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;外媒報道稱，蘋果最新推出的 Apple Intelligence 功能在 Meta 旗下應用（包括 Facebook、Instagram、WhatsApp 和 Threads）中遭到禁用，用戶無法使用其核心功能，如寫作工具 (Writing Tools) 和自定義表情符號生成器 (Genmoji)。此舉被認為與 Meta 推動自家 Meta AI 工具的戰略有關。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;223&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-2e17e1e4324f772634e33ac142954924659.png&quot; width=&quot;300&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Apple Intelligence 是蘋果於 2024 年隨 iOS18 推出的 AI 功能套件，旨在通過智能寫作、圖像生成和個性化體驗提升用戶生產力。其中，寫作工具可實現文本校對、改寫和總結，Genmoji 則允許用戶生成定製化表情符號。這些功能通常通過長按 iOS 文本輸入框激活，理論上適用於大多數應用。然而，Meta 旗下應用已明確禁用這些功能。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;自 2024 年 12 月起，Meta 開始逐步移除對 Apple Intelligence 的支持。用戶在 Facebook、Instagram、WhatsApp 和 Threads 中無法調用寫作工具或 Genmoji，甚至此前可在 Instagram Stories 中使用的 Memoji 和鍵盤貼紙功能也被移除。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;相比之下，X（原 Twitter）、Bluesky 和 Signal 等第三方應用仍支持 Apple Intelligence 的寫作工具。值得注意的是，Apple Intelligence 在瀏覽器版本的 Meta 服務中仍可正常使用，因為瀏覽器環境不受 Meta 應用的限制。蘋果開發者文檔顯示，iOS 和 iPadOS 應用需主動選擇啓用 Apple Intelligence 功能，而 Meta 顯然選擇了禁用。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Meta 並未公開解釋禁用 Apple Intelligence 的原因，但業內普遍認為，此舉旨在推廣其自研的 Meta AI 工具。Meta AI 基於 Llama 模型，已深度整合至 Facebook、Instagram、WhatsApp 和 Threads，提供文本生成、圖像創作和搜索增強等功能。例如，在 Instagram 中，用戶嘗試編輯文本時，會看到「Write with AI」選項，引導至 Meta AI 界面，而非 Apple Intelligence。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/345768</link>
            <guid isPermaLink="false">https://www.oschina.net/news/345768</guid>
            <pubDate>Sun, 13 Apr 2025 08:47:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>OpenAI CEO：對 GPT 説謝謝會帶來千萬開銷</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;OpenAI CEO Sam Altman 近日在社交媒體回覆網友稱，OpenAI 僅僅為了處理用戶日常的寒暄和禮貌性交流，就需要花費「數千萬美元」。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-fd16677fc16d984d5d09457cb0d9c467367.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;據悉，「謝謝」「請」，這些看似微不足道的禮貌用語，雖然在情感上讓用戶與 AI 的互動顯得更有「人味」，但其背後卻帶來高昂的能源消耗。&lt;/p&gt; 
&lt;p&gt;最新報告指出，即使是像「不客氣」這樣的簡短回覆，大型語言模型（LLM）也需要消耗大約 40-50 毫升的水。&lt;/p&gt; 
&lt;p&gt;雖然禮貌的用語會增加 OpenAI 每月的資源支出，但該公司似乎並不介意。目前，許多用戶已經不再將 AI 看作一個冷冰冰的工具，而是能夠帶情感化交流的「虛擬用戶」。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-c2a9253b61a8988e0b8eb110384ab80d1b2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;而據 OpenAI 和麻省理工學院的研究人員指出，隨着 AI 對話越來越難以與人類對話區分開來，用戶可能會對 AI 聊天機器人產生情感依賴，甚至出現成癮的情況。而這種成癮可能會導致用戶在離開 AI 時出現類似戒斷反應的症狀。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/345767</link>
            <guid isPermaLink="false">https://www.oschina.net/news/345767</guid>
            <pubDate>Sun, 13 Apr 2025 08:36:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>Intel 開源專為本地生成式 AI 設計的 AI Playground</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Intel 近日宣佈，其專為本地生成式 AI 設計的 AI Playground 軟件正式開源，為 Intel Arc GPU 用戶提供了一個強大的 AI 模型運行平台。AI Playground 支持多種圖像、視頻生成模型以及大型語言模型（LLMs），通過優化本地計算資源，顯著降低了 AI 應用的硬件門檻。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;401&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-0f8a10ae7e6da81cf50fdc5a2b76750462c.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color:#000000&quot;&gt;核心功能：多模態 AI 模型一站式支持&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;AI Playground 作為一款用戶友好的「AI 中心」，集成了豐富的生成式 AI 功能，涵蓋圖像生成、圖像風格化、文本生成與聊天機器人等場景。AIbase 梳理了其支持的模型與功能:&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;圖像與視頻生成：支持 Stable Diffusion1.5、SDXL、Flux.1-Schnell 和 LTX-Video 模型，可實現文本到圖像、圖像風格化以及文本到視頻生成，生成結果在分辨率與細節上表現出色。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;大型語言模型：兼容 Safetensor PyTorch 格式的 DeepSeek R1、Phi3、Qwen2、Mistral，以及 GGUF 格式的 Llama3.1、Llama3.2，結合 OpenVINO 優化的 TinyLlama、Mistral7B、Phi3mini 和 Phi3.5mini，提供高效的本地聊天與推理能力。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;ComfyUI 工作流：通過集成 ComfyUI，AI Playground 支持高級圖像生成工作流，如 Line to Photo HD 與 Face Swap，提升創作靈活性。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;AI Playground 不直接附帶模型，用戶需從 Hugging Face 或 CivitAI 下載模型並放置於指定文件夾，平台提供直觀的模型加載界面，確保操作簡便。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color:#000000&quot;&gt;技術架構：OpenVINO 優化本地性能&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;AI Playground 基於 Intel 的 OpenVINO 框架，針對 Arc GPU 與 Core Ultra 處理器進行了深度優化。AIbase 分析，其關鍵技術包括:&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;OpenVINO 加速：為聊天與圖像生成提供高效推理支持，顯著提升低 vRAM 設備（如 8GB Arc GPU）的性能。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;Llama.cpp 與 GGUF 支持：實驗性後端擴展了 GGUF 模型的兼容性，預填充模型列表簡化用戶配置。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;模塊化設計：通過 Add Model 功能，用戶可直接輸入 Hugging Face 模型 ID 或本地路徑，靈活加載自定義模型。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;硬件要求方面，AI Playground 支持 Intel Core Ultra-H/V 處理器或 Arc A/B 系列 GPU（最低 8GB vRAM）。儘管為開源 Beta 版，Intel 提供了詳細的故障排查指南，確保用戶快速上手。低 vRAM 設備在運行 SDXL 等高分辨率模型時可能速度較慢，建議優先使用 Flux.1-Schnell 等輕量化模型。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/345762</link>
            <guid isPermaLink="false">https://www.oschina.net/news/345762</guid>
            <pubDate>Sun, 13 Apr 2025 08:25:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>【直播】如何讓 AI 「跑得快」 又 「用得好」？</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                    
                                                                                                                                                    &lt;div&gt; 
 &lt;p style=&quot;text-align:left&quot;&gt;當前，人工智能技術正加速向大模型時代邁進，在政務、金融、醫療、工業等領域展現出顛覆性潛力。然而，大模型的訓練與部署面臨算力成本高、技術生態依賴性強、行業落地門檻高三大挑戰。在此背景下，昇騰與國產大模型的深度結合，為破解上述瓶頸提供了新路徑。&lt;/p&gt; 
 &lt;p style=&quot;text-align:left&quot;&gt;為加速技術普惠，&lt;span style=&quot;color:#2980b9&quot;&gt;4 月 23 日晚&lt;/span&gt;，開源中國直播欄目《數智漫談》邀請&lt;span style=&quot;color:#2980b9&quot;&gt;昇騰生態技術專家與行業先行者&lt;/span&gt;，分享一線開發經驗，聊一聊昇騰結合大模型，如何促進創新，助力開發者與企業用戶抓住國產 AI 新紅利。&lt;/p&gt; 
 &lt;p style=&quot;text-align:left&quot;&gt;🌟&lt;strong&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;演講議題 1：昇騰插件化接入 vLLM 加速大模型推理創新最佳實踐&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p style=&quot;text-align:left&quot;&gt;&lt;strong&gt;演講專家：&lt;/strong&gt;姚聖偉，華為雲 HCDE、微軟 Insider Dev Tour China&lt;/p&gt; 
 &lt;p style=&quot;text-align:left&quot;&gt;&lt;strong&gt;議題簡介：&lt;/strong&gt;隨着大模型技術的快速發展，如何高效部署與加速推理成為業界核心挑戰。基於自主研發的昇騰處理器及 CANN 異構計算架構，昇騰推出插件化接入方案，與開源推理框架 vLLM 深度適配，為大模型推理提供高性能、低時延的創新實踐。用戶可以實現自己的 Woker、ModelRunner、Attention、Communicator 以及自定義算子。在進一步促進 vLLM 多樣性發展的同時，儘可能的解決了兼容性、可維護性的問題。實踐案例覆蓋自然語言處理、多模態交互等場景，驗證了昇騰生態的開放性與技術普適性，為行業提供可複用的國產化大模型部署範式，推動 AI 基礎設施高效進化。&lt;/p&gt; 
 &lt;hr&gt; 
 &lt;p style=&quot;text-align:left&quot;&gt;🌟&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;演講議題 2：基於昇騰+大模型的國內智慧園區項目實踐&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
 &lt;p style=&quot;text-align:left&quot;&gt;&lt;strong&gt;演講專家：&lt;/strong&gt;李小雨，唐山愛尚產品總監，AI 應用探索者與出海實踐者&lt;/p&gt; 
 &lt;p style=&quot;text-align:left&quot;&gt;&lt;strong&gt;議題介紹：&lt;/strong&gt;年初 Deepseek 引發了國產大模型浪潮，國內湧現大量需要通過 AI 提效、優化體驗的需求。但是目前大部分傳統行業對於 AI 提效的具體實踐還在探索中，沒有明確的 AI 落地場景。智慧園區為我們為某國企開發的系統，包括車輛道閘、人臉終端消費、考勤機、監控筒機等多種類業務、多種類設備，是集成一臉通、一平台、數據共享、數據可視的完整解決方案。本次分享，我們站在企業的角度分析 AI，能給實際業務帶來哪些方式的效率提升，並結合實際的某智慧園區項目，分享如何結合昇騰與大模型，在產品體驗和功能形態上做出創新和提效。&lt;/p&gt; 
 &lt;hr&gt; 
 &lt;p style=&quot;text-align:left&quot;&gt;🌟&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;演講議題 3：基於香橙派 AI Studio 實現本地大模型部署和應用最佳實踐&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
 &lt;p style=&quot;text-align:left&quot;&gt;&lt;strong&gt;演講專家：&lt;/strong&gt;徐洋帆，香橙派系統工程師，昇騰社區核心開發者&lt;/p&gt; 
 &lt;p style=&quot;text-align:left&quot;&gt;&lt;strong&gt;議題簡介：&lt;/strong&gt;隨着大模型技術的快速發展，個人和企業對大模型的需求呈現爆發式增長。在雲端大模型層出不窮的同時，隱私安全問題也日漸嚴峻。因此實現低成本的本地化 AI 大模型部署和應用勢在必行。香橙派攜手華為昇騰，推出了 orangePi AI studio 和 orangePi AI studio Pro 產品，旨在為用戶提供低成本的本地化 AI 大模型部署能力。在本次議題中，香橙派將展示算力高達 352Tops，超大的 192G 顯存的 orangePi AI studio Pro 產品上極簡部署 AI 大模型的步驟，半小時實現從 0 到體驗本地 AI 聊天機器人。&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;微信掃碼，預約直播：&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img height=&quot;930&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-7d0ffe73416168db3a2cde17f3629df8acb.jpg&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
 &lt;p&gt;另外，我們還建了一個交流羣，一起聊聊自己 AI 技術～～當然啦，如果你有什麼特別棒的開源項目，可以推薦過來呀～&lt;/p&gt; 
 &lt;div&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;img height=&quot;559&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-579ffec27c910f889cf6d9bbcfc234e8144.jpg&quot; width=&quot;400&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div&gt; 
  &lt;hr&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span&gt;&lt;span style=&quot;background-color:#ffffff&quot;&gt;&lt;span&gt;&lt;strong&gt;&lt;strong&gt;&lt;span&gt;&lt;span style=&quot;background-color:#ffffff&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#27ae60&quot;&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;【數智漫談】&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span&gt;&lt;span style=&quot;background-color:#ffffff&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;OSCHINA 視頻號直播暢聊欄目【數智漫談】，每期一個技術話題，三五位專家圍坐，各抒己見，暢聊開源。給大家帶來最新的行業前沿、最熱門的技術話題、最有趣的開源項目、最犀利的思想交鋒。如果你手上也有新點子、好項目，想要跟同行交流分享，歡迎聯繫我們，講壇隨時開放～&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:center&quot;&gt;&lt;img height=&quot;537&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-4dd54c1b0b817689ceefa15aa66d79cfae8.png&quot; width=&quot;400&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/div&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/u/3859945/blog/18211605</link>
            <guid isPermaLink="false">https://my.oschina.net/u/3859945/blog/18211605</guid>
            <pubDate>Sun, 13 Apr 2025 08:02:00 GMT</pubDate>
            <author>原創</author>
        </item>
        <item>
            <title>GoFr —— 微服務開發框架</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                                                                        
                                                                                    &lt;p&gt;GoFr 旨在&lt;strong style=&quot;color:#1f2328&quot;&gt;簡化微服務開發&lt;/strong&gt;，重點關注&lt;strong style=&quot;color:#1f2328&quot;&gt;Kubernetes 部署&lt;/strong&gt;和&lt;strong style=&quot;color:#1f2328&quot;&gt;開箱即用的可觀察性&lt;/strong&gt;。雖然它能夠構建通用應用程序，但&lt;strong style=&quot;color:#1f2328&quot;&gt;微服務&lt;/strong&gt;仍然是其核心。&lt;/p&gt;

&lt;div style=&quot;text-align:start&quot;&gt;
&lt;h4&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;color:#1f2328&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;background-color:#ffffff&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;主要特點&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;/div&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;簡單的 API 語法&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;默認的 REST 標準&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;配置管理&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://gofr.dev/docs/quick-start/observability&quot;&gt;可觀察性&lt;/a&gt;&lt;/strong&gt;（日誌、跟蹤、指標）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;內置&lt;a href=&quot;https://gofr.dev/docs/advanced-guide/http-authentication&quot;&gt;身份驗證中間件&lt;/a&gt;&lt;/strong&gt;和自定義中間件支持&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://gofr.dev/docs/advanced-guide/grpc&quot;&gt;gRPC 支持&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;支持斷路器的&lt;strong&gt;&lt;a href=&quot;https://gofr.dev/docs/advanced-guide/http-communication&quot;&gt;HTTP 服務&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://gofr.dev/docs/advanced-guide/using-publisher-subscriber&quot;&gt;發佈/訂閲&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;所有數據源的&lt;strong&gt;&lt;a href=&quot;https://gofr.dev/docs/advanced-guide/monitoring-service-health&quot;&gt;健康檢查&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://gofr.dev/docs/advanced-guide/handling-data-migrations&quot;&gt;數據庫遷移&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://gofr.dev/docs/advanced-guide/using-cron&quot;&gt;計劃任務&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;支持無需重啓即可&lt;a href=&quot;https://gofr.dev/docs/advanced-guide/remote-log-level-change&quot;&gt;更改日誌級別&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://gofr.dev/docs/advanced-guide/swagger-documentation&quot;&gt;Swagger 渲染&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://gofr.dev/docs/advanced-guide/handling-file&quot;&gt;Abstracted File Systems&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://gofr.dev/docs/advanced-guide/handling-file&quot;&gt;WebSockets&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

                                                                    &lt;/div&gt;
                                                                </description>
            <link>https://www.oschina.net/p/gofr</link>
            <guid isPermaLink="false">https://www.oschina.net/p/gofr</guid>
            <pubDate>Sun, 13 Apr 2025 07:59:00 GMT</pubDate>
        </item>
        <item>
            <title>微軟近 5 萬 star 的開源項目 —— MarkItDown 已支持 MCP</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;MarkItDown 是微軟開源的 Python 實用工具庫，支持將各種文件轉換為 Markdown 格式，適用於索引、文本分析等用途。&lt;/p&gt; 
&lt;p&gt;MarkItDown 目前支持以下文件：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PDF&lt;/li&gt; 
 &lt;li&gt;PowerPoint&lt;/li&gt; 
 &lt;li&gt;Word&lt;/li&gt; 
 &lt;li&gt;Excel&lt;/li&gt; 
 &lt;li&gt;圖片（EXIF 元數據和 OCR）&lt;/li&gt; 
 &lt;li&gt;音頻（EXIF 元數據和語音轉錄）&lt;/li&gt; 
 &lt;li&gt;HTML&lt;/li&gt; 
 &lt;li&gt;基於文本的格式（CSV、JSON、XML）&lt;/li&gt; 
 &lt;li&gt;ZIP 文件（遍歷內容）&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;該項目最近發佈了一項「史詩級」更新 —— 支持 MCP。MarkItDown 現已提供 MCP（模型上下文協議）服務器 (MarkItDown-MCP)，以便與 LLM 應用程序如 Claude Desktop 集成。&lt;/p&gt; 
&lt;p&gt;MarkItDown-MCP 提供兩種主要的服務器模式：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;STDIO 模式&lt;/strong&gt;（默認）：通過標準輸入/輸出進行通信，非常適合與命令行工具和腳本集成。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;SSE 模式&lt;/strong&gt;&lt;/strong&gt;：作為服務器發送事件 (Server-Sent Events) 服務器在指定主機和端口上運行，支持基於 Web 和網絡的集成。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Docker 支持&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;為了增強可移植性和隔離性，MarkItDown-MCP 提供了 Docker 支持。這在以下情況特別有用：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;確保在不同系統上的環境一致性&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;將轉換過程與主機系統隔離&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;與 Claude Desktop 等遠程服務協作&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Docker 集成包括掛載本地目錄的功能，允許容器訪問和轉換本地文件，同時維持安全邊界。&lt;/p&gt; 
&lt;p&gt;更多信息查看&amp;nbsp;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fmarkitdown%2Ftree%2Fmain%2Fpackages%2Fmarkitdown-mcp&quot; target=&quot;_blank&quot;&gt;markitdown-mcp&lt;/a&gt;。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/345742/markitdown-mcp-server</link>
            <guid isPermaLink="false">https://www.oschina.net/news/345742/markitdown-mcp-server</guid>
            <pubDate>Sun, 13 Apr 2025 07:37:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>宇樹科技將舉辦全球首場「人形機器人格鬥大賽」</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;2025 年 5 月至 6 月，杭州的宇樹科技將舉辦全球首場「人形機器人格鬥大賽」。據悉，宇樹科技的技術團隊在過去數週內進行高強度的算法訓練與硬件調試，為這場比賽打造了最強的參賽機器人。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;此次比賽將通過中央廣播電視總枱的相關平台全網直播。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;360&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-b3acd1231b7817a91dee16dee6276eecb65.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;為了預熱賽事，宇樹科技發佈了視頻《Unitree 鐵甲拳王：覺醒！》。視頻中，參賽的 G1 人形機器人不僅展現了卓越的靈活性與迅猛的出拳能力，還能夠完成左右勾拳、側踢等高難度格鬥動作。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;特別引人注目的是，在被擊倒後，G1 能夠迅速自我恢復並重新投入戰鬥。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/345738</link>
            <guid isPermaLink="false">https://www.oschina.net/news/345738</guid>
            <pubDate>Sun, 13 Apr 2025 07:29:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>H.266 席捲頭部平台成主流：滲透率超 70%、比 H.265 視頻減小一半</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;在近期由阿里巴巴達摩院舉辦的視頻技術前沿研究與應用研討會上，達摩院視頻技術實驗室負責人葉琰介紹，新一代視頻編解碼標準 H.266 正從成熟走向主流，在頭部視頻平台的滲透率已超 70%。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-3d7871b9c0453ba3ab7b384ad1bb0145b8f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;葉琰還表示，達摩院正在積極推進自研視頻編解碼方案 DAMO266 的應用推廣與生態共建。&lt;/p&gt; 
&lt;p&gt;當前視頻應用消耗全網超過 80% 的流量，且 4K 等超高清內容佔比持續上升，這就需要算法更先進、壓縮性能更強的編解碼技術，&lt;strong&gt;相較於 H.265 標準，H.266 在保證相同視頻質量下，可減少約 50% 的數據大小與帶寬成本&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0421/151436_BZat_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;葉琰指出，H.266 標準正在成為越來越多企業的選擇，在國內頭部視頻平台中，top 5 短視頻平台均已上線 H.266 服務，top 5 長視頻平台中 2 家已上線、2 家正調研或計劃上線。&lt;/p&gt; 
&lt;p&gt;對於企業而言，採用 H.266 標準可獲得單流 50% 的帶寬節省收益，約等同於 16% 的綜合成本下降，並提升用戶體驗，如流量消耗減半、卡頓率減半等。&lt;/p&gt; 
&lt;p&gt;而阿里巴巴自研 DAMO266 是業內少數較為成熟的 H.266 標準解決方案，已在多個國民級應用落地，處理的日均視頻播放量（VV）已破億，超過 99% 的移動設備支持 DMAO266 軟解。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0421/151459_Y29u_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;達摩院還積極與行業夥伴共建 H.266 生態，聯合優酷、vivo 推出了業內首個 H.266 手機軟解異構優化方案，在 1080P 60fps 的優酷幀享視頻播放場景下實現 17% 的解碼提速和 13% 的功耗下降；與高通合作，在搭載驍龍 X Elite 的 Windows 11 AI PC 上首次實現 4K 120fps 視頻的流暢播放。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;參考：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FTCb8FhZdf-ctegim-4N1wA&quot; target=&quot;_blank&quot;&gt;https://mp.weixin.qq.com/s/TCb8FhZdf-ctegim-4N1wA&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/345734</link>
            <guid isPermaLink="false">https://www.oschina.net/news/345734</guid>
            <pubDate>Sun, 13 Apr 2025 07:15:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>OpenAI 詳解 o3、o4-mini 和 o3-mini 使用限制</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;OpenAI 在最近更新的一份文檔中詳細&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhelp.openai.com%2Fen%2Farticles%2F9824962-openai-o3-o4-mini-and-o3-mini-usage-limits-on-chatgpt-and-the-api%23h_0151d07654&quot; target=&quot;_blank&quot;&gt;闡述&lt;/a&gt;了 o3、o4-mini 和 o3-mini 三種新推理在 ChatGPT 和 API 上的使用限制。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;具體來説，ChatGPT Plus、Team 或 Enterprise 帳戶，每週可以使用 o3 訪問 50 條消息，每天可以使用 o4-mini 訪問 150 條消息，每天可以使用 o4-mini-high 訪問 50 條消息。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;對於 ChatGPT Pro 用戶，OpenAI 稱其提供「接近無限制」的 o3、o4-mini 和 4o 訪問權限。前提是必須遵守一些&lt;/span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopenai.com%2Fterms%2F&quot; target=&quot;_blank&quot;&gt;使用條款&lt;/a&gt;，&lt;span style=&quot;color:#1a1a1a&quot;&gt;禁止以下行為：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style=&quot;color:#1a1a1a&quot;&gt;濫用，例如自動或以編程方式提取數據。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style=&quot;color:#1a1a1a&quot;&gt;共享帳戶憑據或向任何其他人提供帳戶。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;轉售訪問權限或使用 ChatGPT 為第三方服務提供支持。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:start&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;OpenAI 在文檔中指出：「我們已設置安全防護措施以防止濫用，並始終致力於改進我們的系統。這可能偶爾會涉及暫時限制您的使用。發生這種情況時，我們會通知您。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;該公司預計將在幾周內發佈 OpenAI o3‑pro，並提供全面的工具支持。目前，Pro 用戶仍然可以使用 o1‑pro。&amp;nbsp;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;所有付費使用套餐的 API 用戶均可使用 o1、o3 和 o4-mini 模型。可參閲&lt;/span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fplatform.openai.com%2Fdocs%2Fguides%2Frate-limits%2Fusage-tiers&quot; target=&quot;_blank&quot;&gt;平台文檔&lt;span style=&quot;color:#000000&quot;&gt;，&lt;/span&gt;&lt;/a&gt;&lt;span style=&quot;color:#000000&quot;&gt;查看各套餐的速率限制。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/345733/openai-o3-o4-mini-and-o3-mini-usage-limits</link>
            <guid isPermaLink="false">https://www.oschina.net/news/345733/openai-o3-o4-mini-and-o3-mini-usage-limits</guid>
            <pubDate>Sun, 13 Apr 2025 07:12:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>深圳大學人工智能學院正式揭牌成立</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;今天（4 月 21 日）深圳大學人工智能學院正式揭牌成立。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;853&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0421/150304_YzCt_2720166.png&quot; width=&quot;1280&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;據悉，深圳大學人工智能學院，是響應國家人工智能發展戰略，契合大灣區產業蓬勃發展需求，在國家戰略引領下積極佈局的前沿學院，致力打造人工智能領域的教育與科研高地。&lt;/p&gt; 
&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FyLhYKWCdvOSrwJE94SzOjQ&quot; target=&quot;_blank&quot;&gt;據報道&lt;/a&gt;&lt;/u&gt;，深圳大學人工智能學院首批匯聚了一批國內外頂尖人才，構建包含 2 位中國科學院院士、1 位日本工程院院士、5 位國家級人才、2 位國家青年人才的約 80 人教研團隊。&lt;/p&gt; 
&lt;p&gt;學院構建「需求牽引、突破關鍵、百花齊放」的科研體系，依託全國重點實驗室、國家工程實驗室等強大平台，建設基礎學科研究中心和算力平台，與騰訊雲共建產業學院，為科研創新、技術轉化和人才培養提供堅實保障。&lt;/p&gt; 
&lt;p&gt;學院以創新的學科佈局，構建起全面的本碩博一體化專業體系，學科方向覆蓋人工智能基礎理論、具身智能等前沿。&lt;/p&gt; 
&lt;p&gt;今年 2 月 13 日，香港中文大學（深圳）正式成立人工智能學院，計劃於 2025 年 9 月招收首批學生，擬開設人工智能本科專業及人工智能哲學碩士-博士項目。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/345730</link>
            <guid isPermaLink="false">https://www.oschina.net/news/345730</guid>
            <pubDate>Sun, 13 Apr 2025 07:05:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>羅永浩創業公司細紅線招聘多名算法工程師</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;羅永浩今天在微博發佈了一則招聘公告，為創業公司細紅線招聘多名算法、研發工程師。工作地點均為上海：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;資深互聯網產品經理（5 名）&lt;/li&gt; 
 &lt;li&gt;多模態大語言模型微調算法工程師（6 名）&lt;/li&gt; 
 &lt;li&gt;多模態信息檢索算法工程師（4 名）&lt;/li&gt; 
 &lt;li&gt;大語言模型微調算法工程師（5 名）&lt;/li&gt; 
 &lt;li&gt;桌面端研發工程師（5 名）&lt;/li&gt; 
 &lt;li&gt;資深後端研發工程師（搜索方向）（4 名）&lt;/li&gt; 
 &lt;li&gt;AI 後端開發工程師（5 名）&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img height=&quot;453&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-f5ab9b4bd073e70cc29c7664c17f163eaab.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;據 XR Vision 昨日報道，羅永浩旗下細紅線科技早在 2024 年已放棄 AR 智能眼鏡類產品研發，繼而轉向為 AI 智能硬件和 AI 大模型的研發。但 2025 年年初在 AI 智能硬件完成之後，整個硬件團隊已被全部裁撤，只留下 20 多個軟件工程師負責 AI 軟件相關產品的研發和打磨，繼續完成軟硬件一體的產品在海外上市和銷售。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/345697</link>
            <guid isPermaLink="false">https://www.oschina.net/news/345697</guid>
            <pubDate>Sun, 13 Apr 2025 05:40:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>訊飛星火 X1 全新升級，基於全國產算力訓練的深度推理大模型</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;科大訊飛今日官宣，訊飛星火 X1 全新升級，號稱是「&lt;strong&gt;當前&lt;strong&gt;&lt;strong&gt;業界&lt;/strong&gt;&lt;/strong&gt;唯一&lt;/strong&gt;的基於全國產算力訓練的深度推理大模型」，&lt;/p&gt; 
&lt;p&gt;本次升級有這些關鍵信息⬇️&lt;/p&gt; 
&lt;p&gt;✨實現了數學、代碼、邏輯推理、文本生成、語言理解、知識問答等通用任務效果顯著提升，&lt;strong&gt;在模型參數&lt;/strong&gt;&lt;strong&gt;&lt;strong&gt;比業界同類模型小一個數量級&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;的情況下，&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;整體效果&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;對&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;標&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;OpenAI o1 和 DeepSeek R1&lt;/strong&gt;，再次證明瞭&lt;/strong&gt;基於國產算力訓練的全棧自主可控大模型具備登頂業界最高水平的實力和持續創新的潛力。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;✨融入了更多場景複雜類型數據，模型的泛化性也取得了進步，多個行業任務上展現出了業界領先的能力，&lt;strong&gt;在重點行業如教育、醫療、司法等進一步擴大了領先優勢&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;✨&lt;strong&gt;首發快思考、慢思考統一模型&lt;/strong&gt;，由一個模型同時支持兩種思考模式，私有化部署簡便；&lt;strong&gt;全新升級模型定製優化工具鏈&lt;/strong&gt;，支持 SFT、強化學習兩種模型定製優化方案，&lt;strong&gt;定製門檻低。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;✨&lt;strong&gt;三大核心技術創新&lt;/strong&gt;——大規模多階段強化學習訓練方法、基於快慢思考的統一訓練方法、工程技術系統創新保障基於國產算力的高效長穩訓練，助力星火 X1 全面升級。&lt;/p&gt; 
&lt;p&gt;✨&lt;strong&gt;星火 X1 API 已同步上線訊飛開放平台&lt;/strong&gt;，面向廣大開發者和企業開放服務。&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;據介紹，此次星火 X1 升級，在多個任務上效果繼續突破，展現出優異的性能。根據最新測試集評測結果，&lt;strong&gt;星火 X1 在通用任務效果評測中全面對標 OpenAI o1 和 DeepSeek R1&lt;/strong&gt;，在數學、知識問答等方面表現突出。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;857&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0421/115133_nUoJ_2720166.png&quot; width=&quot;1280&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;星火 X1 此次全新升級，背後有三大技術創新：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;1、大規模多階段強化學習訓練方法&lt;/strong&gt;：提出基於問題難度的大規模多階段強化學習方法，在複雜推理、數學、代碼、語言理解等場景全面提升模型效果及泛化性；同時提出強化學習動態更新算法，基於樣本採樣長度動態調整強化學習更新速度，進一步提升深度思考強化學習效率及效果。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2、基於快慢思考的統一訓練方法&lt;/strong&gt;：提出統一模型下快慢思考混合訓練方法，充分發揮快慢思考數據相互促進作用，實現基於系統指令控制模型是否深度思考，支撐下游更高效便捷地部署使用。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;3、工程技術系統創新保障基於國產算力的高效長穩訓練&lt;/strong&gt;：實現多項工程技術創新，顯存動態卸載技術大幅提升長文本推理併發、訓推共卡協同實現高效訓推資源轉換、推理引擎冬眠機制實現快速拉起和恢復，實現國產算力平台上高效和穩定的強化學習訓練全流程。&lt;/p&gt; 
&lt;p&gt;訪問&amp;nbsp;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fxinghuo.xfyun.cn%2Fsparkapi&quot; target=&quot;_blank&quot;&gt;https://xinghuo.xfyun.cn/sparkapi&lt;/a&gt;&amp;nbsp;體驗星火 X1 API&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/345688</link>
            <guid isPermaLink="false">https://www.oschina.net/news/345688</guid>
            <pubDate>Sun, 13 Apr 2025 03:53:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>CISA 擴大資金投入，確保「關鍵 CVE 服務不出現中斷」</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;CISA 表示，美國政府已擴大資金投入，以確保關鍵的通用漏洞和暴露 (CVE) 計劃不會出現連續性問題。這家美國網絡安全機構表示：「CVE 項目對網絡社區至關重要，也是 CISA 的首要任務。昨晚，CISA 執行了合同中的選擇期，以確保關鍵的 CVE 服務不會出現中斷。我們感謝合作伙伴和利益相關者的耐心。」&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-1f100a8af5b380712b4c8e12772bf8e3305.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;此前，MITRE 副總裁 Yosry Barsoum 曾警告稱，政府對 CVE 和 CWE 項目的資助將於今天（4 月 16 日）到期，這可能會導致整個網絡安全行業出現大範圍混亂。&lt;/p&gt; 
&lt;p&gt;Barsoum 表示：「如果服務中斷，我們預計 CVE 將受到多重影響，包括國家漏洞數據庫和公告、工具供應商、事件響應操作以及各種關鍵基礎設施的惡化。」&lt;/p&gt; 
&lt;p&gt;MITRE 維護着&amp;nbsp;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcve.mitre.org%2F&quot; target=&quot;_blank&quot;&gt;CVE&lt;/a&gt;，這是一個被廣泛採用的計劃，它在討論安全漏洞時提供準確性、清晰度和共享標準，資金來自美國國土安全部 (DHS) 的國家網絡安全部門。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;新成立的 CVE 基金會&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在 CISA 宣佈這一消息之前，一組 CVE 董事會成員宣佈成立 CVE 基金會，這是一個非營利組織，旨在確保 CVE 計劃的獨立性，因為 MITRE 警告稱美國政府可能不會續簽管理該計劃的合同。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0417/165002_yPMw_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.thecvefoundation.org%2Fhome&quot; target=&quot;_blank&quot;&gt;他們在上週三的新聞稿中表示&lt;/a&gt;： 「自成立以來，CVE 項目一直由美國政府資助，並通過合同進行監督和管理。雖然這種結構支持了項目的發展，但也引發了 CVE 董事會成員長期以來的擔憂，他們擔心一個全球依賴的資源與單一政府資助機構捆綁在一起，其可持續性和中立性會受到影響。」&lt;/p&gt; 
&lt;p&gt;在過去的一年裏，參與啓動的人員一直在制定一項戰略，將該計劃過渡到這個專門的基金會，消除「漏洞管理生態系統中的單點故障」，並確保「CVE 計劃仍然是一個全球信賴的、社區驅動的計劃」。&lt;/p&gt; 
&lt;p&gt;雖然 CVE 基金會計劃在未來幾天發佈有關其過渡計劃的更多信息，但下一步行動仍不明確，特別是考慮到 CISA 已確認 MITRE 合同的資金已延長。&lt;/p&gt; 
&lt;p&gt;歐盟網絡安全局 (ENISA) 還推出了歐洲漏洞數據庫 (&amp;nbsp;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Feuvd.enisa.europa.eu%2F&quot; target=&quot;_blank&quot;&gt;EUVD&lt;/a&gt;&amp;nbsp;)，該數據庫「通過從多個來源收集公開的漏洞信息，採取多利益相關方方式」。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;相關閲讀&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/news/345117/the-cve-foundation&quot; target=&quot;news&quot;&gt;CVE 基金會成立&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/news/345038&quot; target=&quot;news&quot;&gt;美國政府不再為 CVE/CWE 項目提供資金支持&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/345685</link>
            <guid isPermaLink="false">https://www.oschina.net/news/345685</guid>
            <pubDate>Sun, 13 Apr 2025 03:46:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>「DeepSeek-V3 技術解析」：DeepSeek-V3-Base 預訓練階段解析</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                    
                                                                                                                                                    &lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;編者按：&lt;/strong&gt; 這篇技術解析詳細闡述了 DeepSeek-V3-Base 的預訓練階段所採用的關鍵技術。&lt;/p&gt; 
 &lt;p&gt;文章重點介紹了三項核心技術：Document Packing 技術有效解決了輸入序列長度差異導致的資源浪費問題；Fill-in-the-Middle（FIM）採用 PSM 框架和特殊 tokens，使模型具備上下文感知的中間內容生成能力；基於 YaRN 的長上下文窗口擴展技術則通過頻率插值策略解決了位置編碼的擴展挑戰。&lt;/p&gt; 
 &lt;p&gt;隨後，文章詳細描述了 DeepSeek-V3-Base 的預訓練過程，包括數據構建、訓練策略和評估結果。&lt;/p&gt; 
 &lt;p&gt;評估顯示，這些技術組合使 DeepSeek-V3 每訓練 1T token 僅需 180K NVIDIA H800 GPU 小時數，並在&quot;大海撈針&quot;測試中展現卓越的長文本理解能力，為後續 RL 階段奠定了優質基座。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;作者 | Shirley Li&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;編譯 | 嶽揚&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;這是 DeepSeek 系列文章的第五篇，也是首篇聚焦 DeepSeek-V3 [1, 2] 訓練流程的文章。&lt;/p&gt; 
&lt;p&gt;如下圖所示，DeepSeek-V3 的訓練分為多個階段：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;產出 DeepSeek-V3-Base 基礎模型的預訓練階段&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;基於 DeepSeek-V3-Base，通過大規模強化學習（RL）分別訓練出 DeepSeek-R1-Zero（無需監督式微調冷啓動）和 DeepSeek-R1（含有監督式微調）&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;利用 DeepSeek-R1 生成推理數據，用於 DeepSeek-V3 的監督式微調（SFT），接着是未在圖中展示的 RL 階段。&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-71379f71e34c5adf46d001de06b46394737.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;圖 1. DeepSeek-V3 訓練流程示意圖（由原文作者繪製）&lt;/p&gt; 
&lt;p&gt;本文將重點關注產出 DeepSeek-V3-Base 的預訓練階段，闡述該階段實現高效預訓練的關鍵技術。後續文章將涵蓋：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;羣組相對策略優化（GRPO）[7]&lt;/li&gt; 
 &lt;li&gt;DeepSeek-R1-Zero 和 DeepSeek-R1 的訓練細節&lt;/li&gt; 
 &lt;li&gt;DeepSeek-V3 的後訓練階段（監督式微調與 RL 階段）&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;目錄&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;技術背景&lt;/strong&gt;：解析 DeepSeek-V3 預訓練階段的相關技術，包括 Document Packing，Fill-in-Middle 和 long context extension。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;預訓練階段&lt;/strong&gt;：詳解如何構建預訓練數據、強調一些關鍵的訓練策略，並回顧評估結果。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;總結&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;參考文獻&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;&lt;strong&gt;01 技術背景&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;本節將介紹預訓練 DeepSeek-V3 過程中使用的幾種技術，包括 document packing、Fill-in-the-Middle（FIM）和基於 YaRN 的長上下文窗口擴展技術。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;1.1 Document Packing&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;要理解為什麼需要 document packing，我們首先需要回顧一下 Transformer 模型是如何構建輸入序列 tokens 的。&lt;/p&gt; 
&lt;p&gt;Transformer 模型默認情況下需要固定長度的 token 序列作為輸入，然而同一 batch 的文本輸入往往長度不同。為了適應這種情況，文本輸入通常需要經過以下預處理步驟：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;將所有原始文本輸入分詞為 token 序列&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;將 token 序列截斷或填充到預定義的固定長度（max_seq_len）：若原始序列過長則截斷，否則用特殊 [PAD] token 進行填充&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;生成掩碼 IDs 使模型在訓練時能忽略填充的 token&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;為了更清晰地展示這個過程，以下這個示例我們將使用 GPT-2 [10]的分詞器處理兩個句子：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-1b9f3887a8de379742b0f676da612804903.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;運行上述腳本後，會得到如下輸出，其中：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;第一句話被填充了 4 個額外的 padding token，體現在 input_ids 和 mask_ids 中；&lt;/li&gt; 
 &lt;li&gt;第二句被截斷，因此無需添加 padding token。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-4e4ef991506b81b0784aa5c3673b1c09afb.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;圖 2. 填充操作示例（此圖由作者繪製）&lt;/p&gt; 
&lt;p&gt;上述截斷和填充方法雖然能讓模型處理不同長度的輸入，但當輸入序列長度差異過大時（這在 LLM 訓練中非常常見）會引發一系列問題：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;對超長序列，截斷可能導致有用信息丟失&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;對較短的序列，填充過多 token 會造成計算資源浪費&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;因此，LLM 訓練通常採用 document packing 技術來處理輸入序列。&lt;/p&gt; 
&lt;p&gt;更具體地説，如果給定若干長度不同的文檔，我們首先將其分割為較小的塊（chunk），如下圖所示（用不同顏色代表不同文檔）：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-f01ac2b3145cefa739b7a42248a7574c35c.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;圖 3. 文檔分割（圖片改編自文獻[3]）&lt;/p&gt; 
&lt;p&gt;隨後，我們將不同文檔的塊（chunk）進行拼接，以避免對長文檔進行截斷和對短文檔進行填充：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-ac5be1d745432dc595122a730f8cb882143.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;圖 4. 傳統拼接方式（圖片改編自文獻[3]）&lt;/p&gt; 
&lt;p&gt;在上例中：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;第一個輸入（譯者注：圖 4 第一行）僅包含文檔 1 的 tokens&lt;/li&gt; 
 &lt;li&gt;第二個輸入（譯者注：圖 4 第二行）拼接自文檔 1 和文檔 2 的 tokens&lt;/li&gt; 
 &lt;li&gt;第三個輸入（譯者注：圖 4 第三行）拼接自文檔 2 和文檔 3 的 tokens&lt;/li&gt; 
 &lt;li&gt;第四個輸入（譯者注：圖 4 第四行）拼接自文檔 3、4、5 的 tokens&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;這種方法雖能在一定程度上避免進行填充和截斷，但由於僅按數據中的相對順序拼接來自不同文檔的塊（chunks），無法控制最終輸入序列的構建方式。&lt;/strong&gt; 例如：文檔 3（紫色）被不必要地分割為兩部分，儘管其實際長度小於 max_seq_len，可以完整放入。&lt;/p&gt; 
&lt;p&gt;為瞭解決這個問題，文獻 [3] 提出了 Best-fit Packing 技術，通過兩個步驟完全消除不必要的分割：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Step 1：將每個文檔分割為更小的塊。&lt;/li&gt; 
 &lt;li&gt;Step 2：以一種智能的方式將這些塊（chunks）分組為訓練序列，確保在不進一步分割任何塊（chunks）的前提下生成最少量的序列。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-409730df94907d865b72f59522cd798a544.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;圖 5. Best-fit packing 技術（此圖改編自文獻[3]）&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;1.2 Fill-in-the-Middle（FIM）&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;在傳統的自迴歸生成中，只能以從左到右的方式訓練模型，即模型只能根據前面的 tokens 預測下一個 token。&lt;strong&gt;然而在實際應用中，模型常需根據上下文生成中間缺失的內容。&lt;/strong&gt; 尤其在代碼生成場景中 ------ 我們常會給定輸入/輸出和部分代碼片段，要求模型填充中間邏輯，如下例所示：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-e731fcaf7f46097251e8e7202a61173df28.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;為了適配此類需求，文獻 [4] 提出了一種簡單有效的方法，稱為 &quot;fill-in-the-middle&quot;：即將文檔隨機切分為 prefix、middle 和 suffix 三部分，然後將 middle 部分移至末尾：&lt;/p&gt; 
&lt;p&gt;由於數據組織形式為 &quot;Prefix-Suffix-Middle&quot;，該方法常被稱為 PSM 框架。實際實現時通過添加特殊 token 來標記各部分的邊界：&lt;/p&gt; 
&lt;p&gt;其中：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&amp;lt;|fim_begin|&amp;gt;和&amp;lt;|fim_hole|&amp;gt;標記 prefix 部分&lt;/li&gt; 
 &lt;li&gt;&amp;lt;|fim_hole|&amp;gt;和&amp;lt;|fim_end|&amp;gt;標記 suffix 部分&lt;/li&gt; 
 &lt;li&gt;&amp;lt;|fim_end|&amp;gt;和&amp;lt;|eos_token|&amp;gt;標記 middle 部分&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;以如下輸入為例：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-f337b48ebcf7340d63b8119d0d1842db85d.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;若需模型預測第二行代碼，可將該行作為 middle 部分，並構造 FIM 輸入如下：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-826b6da51a2ee6ac5b1847427cb9b4ebece.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;圖 6. PSM 框架示意圖（此圖由作者繪製）&lt;/p&gt; 
&lt;p&gt;此時模型的預期輸出應為：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-85f95b26c05ae4243e57b8fbca16df200af.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;1.3 基於 YaRN 的長上下文窗口擴展技術&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;現代 LLM 常需處理極長的提示詞（如整個代碼倉庫），但直接使用 128K 等長上下文窗口進行預訓練並不現實。多數 LLM 採用分階段漸進式擴展策略：先在較小的上下文窗口進行預訓練，再分多個階段逐步擴展到更長的上下文窗口，從而大大降低訓練成本。&lt;/p&gt; 
&lt;p&gt;例如，在 DeepSeek-V3 中，模型首先使用 4K 的上下文窗口完成預訓練，然後再分兩階段擴展到 128K：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;第一階段：從 4K 到 32K（1000 steps）&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;第二階段：從 32K 到 128K（再 1000 steps）&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;需特別指出的是，這種擴展不能通過簡單調大上下文窗口實現，而需藉助基於旋轉位置編碼（RoPE）改進的 YaRN（Yet another RoPE extensioN）技術對位置編碼進行修改。&lt;/p&gt; 
&lt;p&gt;關於 RoPE 的詳細介紹，請參閲我們之前的文章《「DeepSeek-V3 技術解析」：多頭潛在注意力機制（MLA）》。&lt;/p&gt; 
&lt;p&gt;RoPE 是一種相對位置編碼方法，其核心思想是通過使用複雜的旋轉嵌入修改 Query 和 Key，使得二者的內積依賴於它們的相對位置：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-b844eb693468aac0d86585c525f0128cec8.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;然而，由於餘弦函數和正弦函數是週期性的，(pos_i, pos_j) 之間的內積可能看起來與 (pos_i, pos_k) 之間的內積相似，因此在固定 θ 的情況下，僅使用 1K tokens（即位置索引 1~1000） 進行預訓練的模型在測試時可能會混淆，因為測試時遇到的位置索引（如 5K 或 10K）可能遠遠超出了預訓練時的上下文窗口。&lt;/p&gt; 
&lt;p&gt;下圖展示了這種現象：&lt;strong&gt;當 32K 上下文窗口的預訓練模型在超出該窗口的位置測試時，困惑度（Perplexity）急劇上升&lt;/strong&gt;：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-b4570f8269f6199cd4f7f261e30d709ef95.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;圖 7. 困惑度與上下文窗口的關係（此圖由作者繪製）&lt;/p&gt; 
&lt;p&gt;那麼，YaRN 是如何應對這一挑戰的呢？&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;既然外推法（extrapolate）效果欠佳，YaRN 轉而採用插值頻率（interpolate the frequency）的策略。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;假設我們有一個在 4 個 token 長度的輸入上訓練的模型，希望將其擴展到 8 個 token，且基礎頻率 θ=0.5。&lt;/p&gt; 
&lt;p&gt;對於原始 RoPE，直接使用 cos(θ×pos) 和 sin(θ×pos) 對 Query 和 Key 進行旋轉即可。&lt;/p&gt; 
&lt;p&gt;而對於 YaRN：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;首先，計算擴展後的上下文長度與原始長度的比值作為縮放因子，本例中為 2。&lt;/li&gt; 
 &lt;li&gt;然後，生成新頻率 θ&#39; = θ / 2 = 0.25。&lt;/li&gt; 
 &lt;li&gt;再使用新頻率對 Query 和 Key 進行旋轉，即 cos(θ&#39;×pos) 和 sin(θ&#39;×pos)。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;下圖對比了 RoPE 與 YaRN 的 cos 和 sin 值：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-250d721643a9dbe2ab68b4192345a39774f.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;圖 8. YaRN 工作原理示意圖（此圖由作者繪製）&lt;/p&gt; 
&lt;p&gt;通過該圖可觀察到：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;在 RoPE 中，cos 和 sin 值會隨位置索引的增加而快速振盪，導致擴展到更長的上下文時出現問題。&lt;/li&gt; 
 &lt;li&gt;而在 YaRN 中，原始的餘弦和正弦函數通過頻率縮放被插值到擴展後的上下文長度（如藍色高亮區域所示），實現了更平滑的過渡，使得模型能夠更有效地處理長序列。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;下圖展示了 DeepSeek-V3 在&quot;大海撈針&quot;（Needle In A Haystack，NIAH）測試中的表現，表明其在 128K 以下的上下文窗口長度中均表現出色：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-db027db8caa140504a6c552ecc8436d2bae.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;圖 9. DeepSeek-V3 的&quot;大海撈針&quot;測試結果（引自文獻[2]）&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;02 預訓練階段&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;本節將介紹 DeepSeek-V3-Base 的訓練方法，重點解析數據構建流程，並強調預訓練階段中的一些關鍵策略。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;2.1 數據構建&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;數據規模與質量對 LLM 訓練至關重要。DeepSeek-V3 的預訓練語料庫通過持續優化策略構建，具體優化路徑如下：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;在 DeepSeek 67B [8] 中，&lt;strong&gt;訓練語料採用去重-過濾-再混合策略構建。&lt;/strong&gt; 首先對 Common Crawl 語料進行去重，隨後通過嚴格的文檔質量評估標準進行過濾，最後通過數據再混合階段解決數據不平衡問題。&lt;/li&gt; 
 &lt;li&gt;在 DeepSeek-V2 [9] 中，通過以下方式擴展訓練語料：1) &lt;strong&gt;增加更多中文數據及來自不同來源的高質量數據&lt;/strong&gt; ；2) &lt;strong&gt;通過優化數據清洗流程，恢復大量此前在文獻 [8] 的策略中被刪除的數據。同時，通過改進基於質量的過濾算法提升數據質量。&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;在 DeepSeek-V3 [2] 中，&lt;strong&gt;預訓練語料進一步擴充，加入更多數學與編程樣本，以及除中英文之外的多語言樣本。&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;收集的預訓練語料會通過前文提出的 Prefix-Suffix-Middle（PSM）框架結合 FIM（Fill-in-Middle）策略進行預處理，並應用 document-packing 技術。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;2.2 訓練策略&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;原論文[2]對預訓練參數進行了詳細描述，此處我們僅強調幾個關鍵點：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;長上下文窗口擴展&lt;/strong&gt;：首先在 14.8T token 上以 4K 上下文窗口進行預訓練，隨後通過 1000 steps 擴展到 32K 上下文，最終再通過 1000 steps 擴展到 128K 上下文。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;多詞元預測&lt;/strong&gt;：如我們本系列前一篇文章《「DeepSeek-V3 技術解析」：多詞元預測技術（Multi-Token Prediction, MTP）》所述，DeepSeek-V3 採用了優化版的多詞元預測機制，允許模型同時解碼多個詞元（tokens），以加速訓練中的解碼過程。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;以 FP8 精度進行訓練&lt;/strong&gt;：DeepSeek-V3 採用混合精度計算提升效率，對部分計算使用低精度格式（如 8-bit 浮點數），在不過度影響精度的前提下減少內存佔用並加速計算。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;學習率的調度&lt;/strong&gt;：在前 2K steps 中，學習率（learning rate）從 0 線性增長至 2.2e--4，並在 10T token 的訓練過程中保持恆定；隨後在 4.3T token 的訓練過程中按照餘弦曲線下降至 2.2e-5；在最後 500B token 的訓練過程中，前 333B token 保持恆定的學習率，剩餘 167B token 進一步降至 7.3e-6。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Batch size 的調度&lt;/strong&gt;：在前 469B token 的訓練過程中，Batch size 從 3072 逐步提升至 15360，後續訓練中保持恆定。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;strong&gt;2.3 評估結果&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;下表對比了 DeepSeek-V3 與其他開源基座模型在不同任務上的表現。&lt;strong&gt;其中 DeepSeek-V3 在多數數據集上都取得了最佳性能，尤其是在數學與代碼相關的任務中表現突出。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;需特別説明，得益於本系列文章中介紹的各項創新技術，DeepSeek-V3 的優異性能是在極高的訓練效率下實現的。具體而言，&lt;strong&gt;DeepSeek-V3 每訓練 1T token 僅需 180K H800 GPU hours，遠低於訓練 72B 或 405B 稠密模型的成本。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-29ea2b2418d59f8a4f9333eed10ff77c37d.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;文獻[2]中的表 3&lt;/p&gt; 
&lt;p&gt;文獻 [2] 還通過全面的消融實驗驗證了無輔助損失函數的負載均衡、多詞元預測等關鍵技術。由於我們已在前文中討論過相關內容，此處不再贅述。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;03 總結&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;本文探討了 DeepSeek-V3 預訓練策略中的關鍵創新，旨在提升效率、可擴展性與性能。由此產生的 DeepSeek-V3-Base 模型成為更高級推理模型（如 DeepSeek-R1-Zero 和 DeepSeek-R1）的基礎，而這些模型又通過知識蒸餾反哺優化 DeepSeek-V3。&lt;/p&gt; 
&lt;p&gt;除此前討論的架構創新 ------ 多頭潛在注意力（Multi-head Latent Attention）、DeepSeekMoE、無輔助損失函數的負載均衡及多詞元預測（Multi-token Prediction）外，本文還引入了包括 document packing、Fill-in-the-Middle（FIM）和基於 YaRN 的長上下文窗口擴展在內的多項技術。&lt;/p&gt; 
&lt;p&gt;這些技術共同推動了大語言模型效率與可擴展性邊界的突破，為高性能 AI 模型設立了新標杆。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;參考文獻&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;[1] DeepSeek（&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.deepseek.com%2F%EF%BC%89&quot; target=&quot;_blank&quot;&gt;https://www.deepseek.com/）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[2] DeepSeek-V3 Technical Report（&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fdeepseek-ai%2FDeepSeek-V3%2Fblob%2Fmain%2FDeepSeek_V3.pdf%EF%BC%89&quot; target=&quot;_blank&quot;&gt;https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[3] Fewer Truncations Improve Language Modeling（&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2404.10830%EF%BC%89&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2404.10830）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[4] Efficient Training of Language Models to Fill in the Middle（&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2207.14255%EF%BC%89&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2207.14255）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[4] DeepSeek-Coder: When the Large Language Model Meets Programming --- The Rise of Code Intelligence（&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2401.14196%EF%BC%89&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2401.14196）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[5] DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence（&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2406.11931%EF%BC%89&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2406.11931）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[6] YaRN: Efficient Context Window Extension of Large Language Models（&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2309.00071%EF%BC%89&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2309.00071）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[7] DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models（&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2402.03300%EF%BC%89&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2402.03300）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[8] DeepSeek LLM: Scaling Open-Source Language Models with Longtermism（&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2401.02954%EF%BC%89&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/pdf/2401.02954）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[9] DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model（&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2405.04434%EF%BC%89&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2405.04434）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[10] Language Models are Unsupervised Multitask Learners（&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcdn.openai.com%2Fbetter-language-models%2Flanguage_models_are_unsupervised_multitask_learners.pdf%EF%BC%89&quot; target=&quot;_blank&quot;&gt;https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Thanks for reading!&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Hope you have enjoyed and learned new things from this blog!&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;strong&gt;About the author&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Shirley Li&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;I am a Machine Learning Engineer working on building multi-modality models to solve real-world problems.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;END&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;本期互動內容 🍻&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;❓&lt;strong&gt;當前位置編碼方案（RoPE/YaRN）已支持 128K 上下文，但人類書籍平均長度約 200K tokens。要實現真正無損的長文檔理解，您認為下一代位置編碼需要突破哪些理論瓶頸？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;原文鏈接：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmedium.com%2Fdata-science-collective%2Fdeepseek-explained-5-deepseek-v3-base-86c078ed5504&quot; target=&quot;_blank&quot;&gt;https://medium.com/data-science-collective/deepseek-explained-5-deepseek-v3-base-86c078ed5504&lt;/a&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/IDP/blog/18210553</link>
            <guid isPermaLink="false">https://my.oschina.net/IDP/blog/18210553</guid>
            <pubDate>Sun, 13 Apr 2025 03:31:00 GMT</pubDate>
            <author>原創</author>
        </item>
        <item>
            <title>3D 空間視頻生成技術探索與應用</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                    
                                                                                                                                                    &lt;span id=&quot;OSC_h1_1&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;1. 背景&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h1&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;近年來，隨着社交媒體、流媒體平台以及 XR 設備的快速發展，沉浸式 3D 空間視頻的需求迅猛增長，尤其是在短視頻、直播和電影領域，正在重新定義觀眾的觀看體驗。2023 年，蘋果公司發佈的空間視頻技術為這一趨勢注入了新的活力，2025 年以來，輕量化 AI/AR 眼鏡迎來爆發，持續推動對 3D 空間視頻內容的需求。然而，儘管消費端對 3D 內容的需求不斷上升，供給端仍面臨創作瓶頸，主要體現在可用於拍攝 3D 視頻內容的專業相機設備稀缺、製作專業度要求高以及成本高昂等問題。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;我們創新性地提出了一種基於 3D 視覺和 AIGC 生成技術的方法，將存量的 2D 視頻資源不斷轉化為 3D 空間視頻資源，極大降低了 3D 內容的供給成本，提升了覆蓋量。最新的研究成果已被多媒體領域的旗艦會議 ICME 2025 接受，並在京東.Vision 視頻頻道等業務場景落地。ICME（International Conference on Multimedia and Expo）是由 IEEE 主辦的國際多媒體與博覽會，2025 年會議將在法國舉行，主題涵蓋 3D 多媒體、增強現實（AR）、虛擬現實（VR）、沉浸式多媒體和計算機視覺等領域。本次會議共計收到來自全球 3700 多篇投稿，錄用率為 27%。我們提出的基於人工智能的 2D 視頻轉換為 3D 空間視頻的方法，涉及深度估計、圖像生成等算法，並構建了一個 3D 視頻數據集，作為後續行業發展的評測基準。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//2440c913a035dcf280447f858d1fda4e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;圖 1 研究成果被 ICME 2025 接收&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_2&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;2. 技術方案&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h1&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;3D 空間視頻生成屬於新視角合成任務（Novel View Synthesis），指的是在給定源圖像和目標姿態的情況下，通過算法渲染生成與目標姿態對應的圖像。最新的通用新視角合成方案包括基於 NeRF 神經輻射場、Gaussian Splatting 高斯噴射以及 Diffusion Model 擴散模型等。與通用的任意視角合成不同，3D 空間視頻需為雙眼分別提供具有視角差的畫面，算法需根據輸入的一幀左視角圖像，生成對應的固定姿態右眼視角圖像。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;為了實現端到端的 3D 空間視頻生成，我們的算法技術方案主要包含三個部分，分別是單目深度估計、新視角合成（包括視差圖計算、Warp 和空洞區域填充）以及 MV-HEVC 編碼，整體方案如下圖 2 所示。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//5a917dda91662d6cf46790c1c9b32119.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;圖 2 3D 空間視頻生成架構&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;我們的最新研究成果基於上述架構，針對單目深度估計、新視角合成和 MV-HEVC 編碼等三個核心模塊進行了創新和優化。此外，考慮到該領域內用於訓練與評測的 Benchmark 數據集在質量和規模上普遍較差的現狀，我們創建了一個高質量、大規模的立體視頻數據集 StereoV1K。該數據集包含在各種真實場景中捕獲的 1000 個視頻，分辨率為 1180×1180，總幀數超過 50 萬幀。StereoV1K 將作為該領域的重要基準數據集。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_3&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;2.1 單目深度估計&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;深度估計是計算機視覺領域的一個基礎性問題，旨在從圖像或視頻中推斷出場景中物體的距離或深度信息。這項技術對增強現實、虛擬現實、機器人導航以及自動駕駛汽車等應用至關重要。深度估計的目標是根據給定的輸入圖像，預測每個像素點或圖像中物體的相對距離或真實深度值。常見的深度估計方法包括基於深度相機等 TOF（Time of Flight）和激光雷達（LiDAR）硬件設備的方案、基於雙目圖像的立體匹配算法方案，以及基於單目深度估計（Monocular Depth Estimation, MDE）算法模型的方案。其中，單目深度估計由於成本較低、適用場景廣泛，更容易普及，但算法的難度也相對較大。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//c0a8b225eef3135a88ff5f660e9c98db.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;圖 3 基於雙目圖像立體匹配以及硬件的深度估計方案&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;伴隨着 AI 大模型算法的快速發展，單目深度估計在技術方案上經歷了從傳統方法到基於深度學習的方法，再到最新的基於大模型或生成式方法的演變。根據處理對象的不同，單目深度估計可以進一步細分為圖像深度估計和視頻深度估計。通常情況下，圖像深度估計在細節表現上更為出色，而視頻深度估計則在時序一致性方面表現更佳。此外，從估計結果的角度來看，單目深度估計還可以分為絕對深度估計和相對深度估計。絕對深度估計指的是從圖像中估計出每個像素到攝像機的真實物理距離，而相對深度估計則關注圖像中物體之間的深度關係，而非絕對距離。基於我們的應用場景，我們採用了一種結合圖像和視頻深度估計優點的單目相對深度估計算法。該算法架構如下：我們使用 DINO v2 作為 Backbone，並結合 DPT Head，同時嘗試引入多幀序列的 memory bank 和注意力機制，以提升深度估計結果在時序上的準確性與穩定性。算法架構如圖 4 所示。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//35458a1826f0810bd2491c396110af32.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;圖 4 視頻單目深度估計算法架構&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;通過在短視頻等數據上構建百萬級的偽標籤訓練數據集，並採用 SFT（Supervised Fine-Tuning）和蒸餾等技術手段，我們對開源模型進行了優化。效果如圖 5 所示，可以明顯看到，我們不僅提升了深度估計的細節表現，還確保了估計結果的時序穩定性。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//99d3c7e01c2af117db99dca6172b7e33.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt; &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;span&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//f8858f3e1c4cb53c4264c3a5ed60e5b3.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;圖 5 視頻單目深度估計算法優化效果對比（原始輸入-開源模型-優化後模型）&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_4&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;2.2 新視角合成&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;新視角合成是視覺領域中的一項關鍵任務，其目標是在有限的視圖基礎上生成場景或物體的其他視角。這項技術在虛擬現實、增強現實、電影特效和遊戲開發等領域具有廣泛應用。儘管基於 NeRF、3DGS 和 Diffusion 的方法近年來取得了顯著進展，但仍面臨諸多挑戰。例如，NeRF 和 3DGS 方法通常只能針對單一場景進行建模，而擴散模型在生成視頻時難以保證穩定性和一致性。在分析任務的特殊需求時，我們發現只需生成固定姿態的右眼視角圖像，且場景具有位移小、豐富多樣以及視頻穩定性和一致性要求高等特點。基於這些考慮，我們最終選擇採用深度 Warp 和空洞區域填充 InPaint 的方法來完成新視角合成任務。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;在 2.1 部分獲取到視頻對應的深度信息後，我們首先計算視差圖，並引導輸入的單目視頻進行 Warp 操作，從而生成對應的待填充右視角視頻和掩碼視頻。接下來，我們將這些數據輸入到我們設計的 InPaint 填充框架中，以完成空洞區域的補全，最終得到完整的新視角結果。整體框架圖如圖 6 所示：&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//dc6228cb2a058ec55408c71fab0b45d3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;圖 6 新視角合成端到端算法架構&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h3_5&quot;&gt;&lt;/span&gt; 
&lt;h3&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;2.2.1 多分支 InPaint 模塊&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;為了獲得高質量和高一致性的 InPaint 填充效果，我們採用了多分支填充策略。該模塊集成了三種 InPaint 分支：傳統多邊形插值修復（Poly-base）、深度學習神經網絡修復（DL-base）和視差擴展策略修復（DE-base）。每個分支各有優缺點：(i) Poly-base 能夠保證視頻的穩定性並減少字幕抖動，但在邊緣填充時容易出現像素拉伸和毛刺；(ii) DL-base 在前景和背景邊緣的填充效果良好，但視頻穩定性較差，可能導致字幕抖動和前景滲透；(iii) DE-base 優化了像素拉伸和前景滲透問題，但在複雜背景和幾何結構處可能提供錯誤的參考像素。我們結合這些分支的互補優勢，以實現更好的填充效果。下圖展示了我們視差擴展策略的有效性。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//342752027cd84ce3c173756a91127463.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;圖 7 視差擴展策略與優化效果&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;為了更好地融合上述三個分支的結果&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;, &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;並進行進一步優化，我們提出了一種新穎的基於層級化特徵更新的掩碼融合器。該融合器的輸入為多分支填充模塊三個分支的輸出結果，輸出則為三張單通道的掩碼圖 M1，M2，M3 和一張三通道的內容圖 C，通過融合這些信息，我們能夠獲得最終的生成結果。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;我們的結果在與當前先進模型的定量和定性比較中均表現出色，達到了 SOTA（State-of-the-Art）水平。特別是在 LPIPS 指標上，我們的方法相比其他方法提升了超過 28%，充分體現了結果的真實性和優越性。同時，在可視化效果方面，我們的方法顯著減少了生成區域中的模糊偽影和前背景的錯誤拉伸，呈現出更加清晰自然的邊緣和內容，如圖 8 所示。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//95b10ed1bf652a34643ecb41a3df42c1.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;圖 8 多分支 InPaint 方法與其他方法結果對比&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h3_6&quot;&gt;&lt;/span&gt; 
&lt;h3&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;2.2.2 &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;StereoV1K 立體視頻數據集&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;在 3D 空間視頻生成領域，現有的公開數據集存在量級小、分辨率低、場景單一和真實性差等問題，限制了行業算法的發展與提升。為瞭解決這些問題，我們創建了 StereoV1K，這是第一個高質量的真實世界立體視頻數據集。我們使用最新的佳能 RF-S7.8mm F4 STM DUAL 鏡頭和 EOS R7 專業相機，在室內和室外場景中拍攝了 1000 個空間視頻。每個視頻裁剪後的分辨率為 1180×1180，時長約 20 秒，錄製速度為 50 fps，最終整個數據集的總幀數超過了 500,000 幀。圖 9 展示了與其他數據集的對比以及我們數據集的示例。該數據集將作為該領域的基準數據集，推動行業的發展。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//139902fb31116a46abe4f65d987b27cd.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;圖 9 StereoV1K 數據集與現有數據集對比&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_7&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;2.3 MV-HEVC 編碼&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;通過上述算法框架，我們能夠生成高質量的雙目 3D 視頻，包括左眼視頻和右眼視頻，其數據量是傳統 2D 視頻的兩倍。因此，高效壓縮和編碼 3D 視頻在實際應用中顯得尤為重要，這直接關係到在線播放視頻的清晰度和帶寬。目前，3D 視頻編碼主要分為兩類方法：傳統的 SBS（Side-by-Side）HEVC 編碼方式以及 MV-HEVC（Multi-View HEVC）編碼。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;span&gt;•&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;SBS-HEVC：該方法將 3D 視頻在相同時間點的左右眼畫面拼接為一個普通的 2D 畫面，並採用傳統的 HEVC 編碼技術進行壓縮。該方案實現簡單，可以使用如 ffmpeg 等開源軟件進行處理。然而，SBS-HEVC 的編碼壓縮率較低，因此需要更大的傳輸帶寬。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;•&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;MV-HEVC：該方法將 3D 視頻的不同視角編碼到同一碼流中，允許用戶在不同視角之間自由切換。編碼器可以利用左右眼畫面之間的相似性來進一步減少冗餘，從而顯著提升壓縮編碼效率。MV-HEVC 編碼是對標準 HEVC 的擴展，目前除了蘋果 AVFoundation 框架中提供的閉源工具外，尚無自主可控的編碼軟件可供部署，用戶需要自定義編碼器來實現這一功能。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//ff52e63f038ab9e962bc58dad16c896e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;圖 10 SBS-HEVC 和 MV-HEVC 編碼方式對比&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;以 1920x1080 的視頻為例，在 SBS-HEVC 編碼流程中，畫面以「左+右」的形式合併為 3840x1080 的新視頻幀，然後作為普通視頻進行 HEVC 編碼，此時只能使用幀間預測（Inter frame prediction）。而在 MV-HEVC 編碼流程中，左眼和右眼分別被稱為基本層（Layer 0）和增強層（Layer 1）。除了幀間預測外，MV-HEVC 還可以利用「視間預測」（Inter view prediction），因為同一時間點的左眼和右眼畫面之間具有較高的相似性和冗餘性，因此視間預測能夠進一步提升壓縮效率。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;我們在標準 HEVC 編碼器的基礎上添加了對 MV-HEVC 擴展的支持，從而在編碼性能和編碼速度上都取得了顯著提升。在典型測試場景中，MV-HEVC 相比 SBS-HEVC 的 BD-Rate 降低了 33.28%，這意味着在相同畫質下，視頻帶寬可以減少 33%；同時，編碼速度平均提升了 31.62%，具體數據如圖 11 所示。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//14e0056f1c083fe7ef0e093d97f36b44.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;span&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//ea3722a6f0223d2402f05f622da27a8e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;圖 11 SBS-HEVC 和 MV-HEVC 的編碼 RD 性能對比&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;在使用 MV-HEVC 解決雙目 3D 視頻的壓縮編碼問題後，還需要將視頻和音頻數據打包存儲，以實現在線流媒體播放。蘋果為 MV-HEVC 定製了封裝格式，但通過 ffmpeg、mp4box 等開源媒體工具封裝的文件在 Vision Pro、iPhone 等蘋果設備上無法正常顯示立體視頻。為此，我們對由 AVFoundation 封裝出的正常碼流進行了逆向分析，從中提取出與蘋果設備兼容的碼流格式，並在自研編碼器中實現了這一格式。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;color:#333333&quot;&gt;與蘋果設備兼容的碼流格式為：&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;span&gt;•&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;使用&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;color:#333333&quot;&gt;mov 格式，&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;整體符合 QuickTime File Format Specification；同時符合 mp4 格式標準&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;color:#333333&quot;&gt;ISO/IEC 14496-15 的標準定義。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;•&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;在視頻軌道的描述信息中，在 stsd 中重新定義&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;hvc1&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;、&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;hvcC&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;、&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;lhvC&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;，增加對於不同視角視頻的描述，其中&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;hvcC&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;描述基本層碼流信息，包括&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;VPS, SPS, PPS, SEI&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;這 4 個 HEVC &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;NAL&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;頭信息；&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;lhvC&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;描述增強層碼流信息，包括&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt; SPS, PPS&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;信息。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;•&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;增加蘋果自定義信息&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;vexu、hfov&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;，用於描述自定義信息，其數據結構如圖 12 所示，其中關鍵字段有：&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;◦&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;blin&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;：定義&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;baseline&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;，表示相機基線，以實際數值的 1000 倍來記錄，例如 Vision Pro 的 63.54mm 記錄為 63540。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;◦&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;dadj&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;：定義&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;disparity&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;，表示水平視差調整，以實際數值的 10000 倍來記錄，例如 Vision Pro 的 2.93% 記錄為 293。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;◦&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;hfov&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;：表示水平視場角，以實際數值的 1000 倍來記錄，例如 Vision Pro 的 71.59 度記錄為 71590。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//3d0c538634b101790f04b888c79426cc.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;圖 12 蘋果自定義信息 vexu、hfov 示意圖&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_8&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;2.4 應用與落地&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;為了在實際業務中落地，我們首先簡化了單目深度估計模型的尺寸，採用 ViT-S 作為特徵編碼器，並對模型進行了 SFT 微調。隨後，我們在自建的 StereoV1K 數據集上訓練了多分支 InPaint 模型，並將論文中的 InPaint 基礎模型更換為更輕量的 Transformer 網絡。通過這些手段，我們實現了速度與質量的平衡。在對實際業務中的大量視頻進行測試後，我們發現我們的算法生成的 3D 空間視頻很好地滿足了業務需求，但仍有少數生成結果存在一些不理想的情況。未來，我們將持續迭代優化相關模型。此外，當前的生成速度也是一個重點優化方向。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;當前，3D 空間視頻可以在多種 XR 設備上觀看與體驗，包括 Vision Pro、Pico、Quest 以及 AI 眼鏡等雙目設備。例如，我們為京東.Vision 視頻頻道提供了空間視頻內容的算法服務。Vision Pro 視頻頻道的效果如圖 13 左圖所示：通過將 2D 商品短視頻、宣傳片和發佈會等資源轉換為 3D 立體空間視頻，極大提升了用戶的沉浸式和立體觀看體驗。此外，在更輕量的 AI/AR 眼鏡中，用戶也可以方便地體驗到 3D 視頻內容帶來的震撼與沉浸感，如圖 13 右圖所示。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//6f20321a8f00def4d8dd5343264adf72.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt; &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;span&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//c829dd078afa100e0d732b7ef12b83da.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;圖 13 京東.Vision 視頻頻道以及 XREAL AR 眼鏡觀看效果（圖中無法顯示 3D 效果，實際體驗為 3D 效果）&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_9&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;3. 未來展望&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h1&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;上述介紹的 3D 空間視頻為用戶帶來了全新的沉浸式體驗，併為 3D 視頻域提供了批量內容供給。然而，3D 領域的內容表現形式還有很多種，例如 3D 模型、3D/4D 空間和完整世界等。隨着大模型的快速發展，算法對人類世界的建模正經歷以下幾個階段：大語言生成模型 → 圖像生成模型 → 視頻、3D/4D 生成模型 → 世界模型。可以預見，未來將有更多的工作集中在 AIGC 3D/4D 和世界模型生成等方向。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_10&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;3.1 AIGC 3D/4D&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;2024 年，3D/4D 領域的 AIGC 發展迅速，尤其是從下半年開始，呈現出加速趨勢。同時，新的發展方向也開始顯現。從技術路線來看，有 Google 的 CAT3D，通過單圖到多圖再到 3D 表示的方式；還有使用 LRM 的單圖到 3D 表示的方案，如 InstantMesh，以及近期基於結構化 3D 表徵的 Trellis。此外，一些學者正在基於 4D Gaussian Splatting 實現空間序列的建模。值得一提的是，3D/4D 模型的可編輯性是一個重要的關注點，因為即使是專業建模師也要在生產過程中需要不斷編輯和修改。最新的研究方向也開始關注生成過程的可控性與可編輯性等屬性。圖 14 所示為 AIGC 3D 模型與 4D 視頻生成示例。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//842829b6a978dceae11af20e0594ff40.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt; &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;span&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//d0acb4fba1f4557975f9affa379de194.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;圖 14 AIGC 3D 模型與 4D 視頻生成示例&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;在當前的 AIGC 3D 模型生成技術中，像 Trellis 這樣採用 3D 表徵的端到端訓練方案展現出顯著優勢。通過對 3D 表徵進行直接的結構化編碼，該模型在幾何形狀和紋理貼圖的生成上實現了更高的準確性和魯棒性，能夠生成高質量且多樣化的 3D 資產，具備複雜的形狀和紋理細節。此外，由於模型處理的是結構化信息，它支持靈活的 3D 編輯，例如根據文本或圖像提示進行局部區域的刪除、添加和替換，如圖 15 左圖所示。在 4D 視頻生成技術中，當前主流的方案是採用帶有時序的 Gaussian Splatting 表徵進行建模，如圖 15 右圖所示。由於高斯表徵本身的大小以及存在維度提升，4D 視頻面臨着數據體量大、模型複雜度高、渲染性能壓力大等挑戰。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//7cf9795f5440bc61bb3bae767681c1d3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt; &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;span&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//d6979590f8cd845640d885df37f9ad1d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;圖 15 Trellis 根據文本提示詞進行紋理材質以及幾何結構的局部編輯以及典型 4D Gaussian Splatting 架構&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_11&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;3.2 世界模型&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;目前，世界模型在學術界和工業界尚未形成明確的概念，關於其是模擬世界還是感知世界也沒有統一的範式。然而，從近期的進展來看，世界模型需要具備時序和立體空間的結構化建模能力。建模後的數據應具有稠密的語義表徵和局部可編輯性，同時整個時序與空間域需具備可交互性。最終目標是實現對現實空間的復刻，甚至對現實空間進行創作與未來預測。圖 16 所示為 World Labs 世界模型以及 Meta orion AI 眼鏡空間萬物感知。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//5800f8336c22fb37004ee21f37bcd088.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt; &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;span&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//831ec7557a877625cde34a1729efe8cb.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;圖 16 World Labs 世界模型以及 Meta orion AI 眼鏡空間萬物感知&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;我們將持續關注並深入跟進 3D 領域的最新進展，特別是在技術創新和應用實踐方面的動態。結合京東廣泛的業務場景，我們致力於將這些前沿技術落地並轉化為實際應用，以滿足用戶日益增長的需求。通過不斷探索 3D 技術在電商、廣告、內容等多個領域的潛力，我們希望為用戶帶來全新的體驗，提升他們的購物樂趣和互動感。我們的目標是通過創新的解決方案，推動行業的發展，為用戶創造更高的價值和更豐富的體驗。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_12&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;4.參考文獻&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h1&gt; 
&lt;div&gt; 
 &lt;span&gt;1.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Zhang J, Jia Q, Liu Y, et al. SpatialMe: Stereo Video Conversion Using Depth-Warping and Blend-Inpainting[J]. arXiv preprint arXiv:2412.11512, 2024.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;2.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Yang, Sung-Pyo, et al. &quot;Optical MEMS devices for compact 3D surface imaging cameras.&quot;Micro and Nano Systems Letters7 (2019): 1-9.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;3.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Bhat S F, Birkl R, Wofk D, et al. Zoedepth: Zero-shot transfer by combining relative and metric depth[J]. arXiv preprint arXiv:2302.12288, 2023.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;4.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;LiheYang, BingyiKang, ZilongHuang, ZhenZhao, XiaogangXu, Jiashi Feng, and Hengshuang Zhao, 「Depth anything v2,」 arXiv preprint arXiv:2406.09414, 2024.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;5.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Teed Z, Deng J. Raft: Recurrent all-pairs field transforms for optical flow[C]//Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part II 16. Springer International Publishing, 2020: 402-419.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;6.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Zhang K, Fu J, Liu D. Flow-guided transformer for video inpainting[C]//European conference on computer vision. Cham: Springer Nature Switzerland, 2022: 74-90.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;7.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Shangchen Zhou, Chongyi Li, Kelvin CK Chan, and Chen Change Loy, 「Propainter: Improving propagation and transformer for video inpainting,」 in ICCV, 2023, pp. 10477–10486.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;8.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Han Y, Wang R, Yang J. Single-view view synthesis in the wild with learned adaptive multiplane images[C]//ACM SIGGRAPH 2022 Conference Proceedings. 2022: 1-8.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;9.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Wang L, Frisvad J R, Jensen M B, et al. Stereodiffusion: Training-free stereo image generation using latent diffusion models[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 7416-7425.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;10.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Zhen Lv, Yangqi Long, Congzhentao Huang, Cao Li, Chengfei Lv, and Dian Zheng, 「Spatialdreamer: Self-supervised stereo video synthesis from monocular input,」 arXiv preprint arXiv:2411.11934, 2024.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;11.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Mildenhall B, Srinivasan P P, Tancik M, et al. Nerf: Representing scenes as neural radiance fields for view synthesis[J]. Communications of the ACM, 2021, 65(1): 99-106.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;12.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Kerbl B, Kopanas G, Leimkühler T, et al. 3d gaussian splatting for real-time radiance field rendering[J]. ACM Trans. Graph., 2023, 42(4): 139:1-139:14.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;13.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Gao R, Holynski A, Henzler P, et al. Cat3d: Create anything in 3d with multi-view diffusion models[J]. arXiv preprint arXiv:2405.10314, 2024.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;14.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Xu J, Cheng W, Gao Y, et al. Instantmesh: Efficient 3d mesh generation from a single image with sparse-view large reconstruction models[J]. arXiv preprint arXiv:2404.07191, 2024.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;15.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Xu Z, Xu Y, Yu Z, et al. Representing long volumetric video with temporal gaussian hierarchy[J]. ACM Transactions on Graphics (TOG), 2024, 43(6): 1-18.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;16.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Dosovitskiy A, Beyer L, Kolesnikov A, et al. An image is worth 16x16 words: Transformers for image recognition at scale[J]. arXiv preprint arXiv:2010.11929, 2020.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;17.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Xiang, Jianfeng, et al. &quot;Structured 3d latents for scalable and versatile 3d generation.&quot; arXiv preprint arXiv:2412.01506 (2024).&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;18.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Wu G, Yi T, Fang J, et al. 4d gaussian splatting for real-time dynamic scene rendering[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2024: 20310-20320.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;19.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F17003931453&quot; target=&quot;_blank&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;https://zhuanlan.zhihu.com/p/17003931453&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;20.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F15449644319&quot; target=&quot;_blank&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;https://zhuanlan.zhihu.com/p/15449644319&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;21.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Gerhard T, Ying Chen, Karsten Müller, et al. Overview of the Multiview and 3D Extensions of High Efficiency Video Coding. IEEE TRANS. ON CSVT, VOL. 26, NO. 1, JANUARY 2016&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;22.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.apple.com%2Fav-foundation%2FHEVC-Stereo-Video-Profile.pdf&quot; target=&quot;_blank&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;https://developer.apple.com/av-foundation/HEVC-Stereo-Video-Profile.pdf&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;23.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;H.265 : High efficiency video coding Spec, &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.itu.int%2Frec%2FT-REC-H.265-202407-I%2Fen&quot; target=&quot;_blank&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;https://www.itu.int/rec/T-REC-H.265-202407-I/en&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;24.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;ISO/IEC 14496-15:2022(en), &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.iso.org%2Fobp%2Fui%2Fen%2F%23iso%3Astd%3Aiso-iec%3A14496%3A-15%3Aed-6%3Av1%3Aen&quot; target=&quot;_blank&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;https://www.iso.org/obp/ui/en/#iso:std:iso-iec:14496:-15:ed-6:v1:en&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;25.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;QuickTime File Format, &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.apple.com%2Fdocumentation%2Fquicktime-file-format&quot; target=&quot;_blank&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;https://developer.apple.com/documentation/quicktime-file-format&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;26.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbbs.nreal.cn%2Finfo%2F613d924045b547599e2495a9509f6bc0%3Fcsr%3D1&quot; target=&quot;_blank&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;https://bbs.nreal.cn/info/613d924045b547599e2495a9509f6bc0?csr=1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;27.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.worldlabs.ai%2F&quot; target=&quot;_blank&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;https://www.worldlabs.ai/&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;28.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fabout.fb.com%2Fnews%2F2024%2F09%2Fintroducing-orion-our-first-true-augmented-reality-glasses%2F&quot; target=&quot;_blank&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;https://about.fb.com/news/2024/09/introducing-orion-our-first-true-augmented-reality-glasses/&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;/span&gt; 
&lt;/div&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/u/4090830/blog/18190045</link>
            <guid isPermaLink="false">https://my.oschina.net/u/4090830/blog/18190045</guid>
            <pubDate>Sun, 13 Apr 2025 03:07:00 GMT</pubDate>
            <author>原創</author>
        </item>
        <item>
            <title>OpenAI 新的推理模型幻覺更嚴重</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;OpenAI 近日推出的 &lt;u&gt;&lt;a href=&quot;https://www.oschina.net/news/345032/openai-gpt-o3-and-o4-mini&quot;&gt;o3/o4-mini&lt;/a&gt;&lt;/u&gt; 雖然在多方面有了不小的進步，然而新模型在「幻覺」內容（虛構的內容）方面，相較於舊模型會產生更多。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;552&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0421/110513_bW9f_2720166.png&quot; width=&quot;1378&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;在近期發佈的 o3 和 o4-mini 模型的系統卡（技術報告）中，OpenAI 承認這些模型出現了比其前代模型更頻繁的「幻覺」（編造不實信息）現象。&lt;/p&gt; 
&lt;p&gt;在一項衡量模型對人物知識準確性的基準測試中，o3 的幻覺率達到 33%，接近前代模型 o1 和 o3-mini 的兩倍；而 o4-mini 的表現更差，幻覺率達 48%。&lt;/p&gt; 
&lt;p&gt;OpenAI 承認目前尚不完全清楚為何，並指出這可能與模型做出更多宏觀陳述有關。第三方研究也印證了這一問題，包括髮現 o3 有時會編造實際上無法執行的中間步驟，或者生成無效鏈接。&lt;/p&gt; 
&lt;p&gt;據 OpenAI 的內部測試，o3 和 o4-mini 兩款新模型比 OpenAI 此前的推理模型（o1、o1-mini、o3-mini）以及傳統的「非推理」模型，都更容易產生幻覺。報道還表示，更令人擔憂的是連 ChatGPT 的開發人員都不知道為何會這樣：&lt;/p&gt; 
&lt;p&gt;OpenAI 在 o3/o4-mini 的技術報告中表示，需要更多的研究內容來瞭解「為什麼隨着推理模型的發展，反而幻覺情況反而更糟糕」這一問題。報道指出，儘管 o3/o4-mini 在編程和數學等方面優於以往的模型，但由於模型輸出的答案總量增加，導致其會給出更多準確的判斷，同時也不可避免地出現更多錯誤的內容甚至是「幻覺」。&lt;/p&gt; 
&lt;p&gt;在 OpenAI 設計的內部基準測試 PersonQA（用於衡量模型對知識準確性的基準測試）中，o3 出現幻覺的比例達到 33%，約是前代推理模型 o1（16%）和 o3-mini（14.8%）的兩倍。在同一基準測試中，o4-mini 的表現更差，幻覺率高達 48%。&lt;/p&gt; 
&lt;p&gt;另據第三方機構 Transluce 的測試，o3 在回答問題時經常會編造出某些「過程操作」。據一次測試顯示，o3 聲稱自己在一台 2021 款 MacBook Pro 上通過「ChatGPT 之外」的方式運行了生成的代碼，並將結果複製到答案中。但事實上，o3 只是擁有一部分工具的訪問權，但並不具備執行操作的能力。&lt;/p&gt; 
&lt;p&gt;另據斯坦福大學兼職教授 Kian Katanforoosh 告訴 TechCrunch，其團隊測試 o3 的編程能力時發現，o3 經常會援引錯誤的網站鏈接，提供的網站實際上是不存在的。&lt;/p&gt; 
&lt;p&gt;報道指出，提高模型準確性的一種辦法即是「聯網搜索」，OpenAI 的 GPT-4o 便是依靠聯網搜索能力在 SimpleQA 中獲得 90% 的準確率。&lt;/p&gt; 
&lt;p&gt;目前，OpenAI 發言人 Niko Felix 回應表示：「解決幻覺問題是我們一直在推進的重點研究方向，OpenAI 也在不斷努力提升模型的準確性與可靠性。」&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/345677/openais-new-reasoning-ai-models-hallucinate-more</link>
            <guid isPermaLink="false">https://www.oschina.net/news/345677/openais-new-reasoning-ai-models-hallucinate-more</guid>
            <pubDate>Sun, 13 Apr 2025 03:07:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>微軟開源 MineWorld：基於 Minecraft 的實時交互式世界模型</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;微軟發佈了名為「MineWorld」的開源項目，這是一個基於 Minecraft（&lt;em&gt;《我的世界》&lt;/em&gt;）的實時交互式世界模型。&lt;/p&gt; 
&lt;p&gt;MineWorld 以 Transformer&amp;nbsp;為核心，並結合大熱門沙盒遊戲《我的世界》開發而成。這是因為遊戲是評估、訓練 Agent 在感知、決策、預測，以及在動態複雜環境的綜合處理能力的最佳場景之一。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-7cfc4ac028c3a02164b8a1dddab6f340d5c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;作為世界模型，MineWorld 可依據過去觀察和當前動作預測未來遊戲狀態，智能體藉此評估不同行動後果，選擇最優策略，例如，在遊戲中根據預測狀態決定前進、後退等動作以達成目標。&lt;/p&gt; 
&lt;p&gt;MineWorld 還在訓練過程中學習到的狀態與動作關係，幫助智能體更好理解動作效果，精準執行決策，提高行動成功率。在與環境交互時，實時性十分關鍵。MineWorld 通過創新的並行解碼算法，實現每秒生成&amp;nbsp; 4-7 幀的速度，快速響應玩家動作輸入。這使得智能體在與玩家或其他智能體交互時，能及時獲取最新環境信息並做出相應反應。&lt;/p&gt; 
&lt;p&gt;根據測試數據顯示，MineWorld 在多方面遠超知名世界模型 Oasis。視頻質量上，3 億參數的 MineWorld 的 FVD 值 246 低於 Oasis 的 377，SSIM 值 0.38 高於 Oasis 的 0.36。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0421/105514_it9H_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;可控性方面，MineWorld 的 3 億和 7 億參數模型 F1 分數達 0.70，12 億參數模型為 0.73，遠高於 Oasis 的 0.41；相機控制 L1 損失也更低。推理速度上，MineWorld 每秒生成 5.91 幀，遠超 Oasis 的 2.58 幀。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;開源地址：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2FMineWorld&quot; target=&quot;_blank&quot;&gt;https://github.com/microsoft/MineWorld&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/345675/microsoft-opensource-mineworld</link>
            <guid isPermaLink="false">https://www.oschina.net/news/345675/microsoft-opensource-mineworld</guid>
            <pubDate>Sun, 13 Apr 2025 02:56:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
    </channel>
</rss>