<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - industry - 繁體中文（台灣）</title>
    <link>https://www.oschina.net/news/industry</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news/industry" rel="self" type="application/rss+xml"/>
    <description>已對該 RSS 進行格式化操作：中英字符之間插入空格、使用直角引號、標點符號修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-tw</language>
    <lastBuildDate>Thu, 11 Sep 2025 16:42:02 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>快手發佈開源多模態大模型 Kwai Keye-VL-1.5</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;快手近日正式發佈多模態大語言模型 Keye-VL-1.5-8B。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0911/195624_2HB4_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;https://huggingface.co/Kwai-Keye/Keye-VL-1_5-8B&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;據介紹，與之前的版本相比，Keye-VL-1.5 的綜合性能實現顯著提升，尤其在基礎視覺理解能力方面，包括視覺元素識別、推理能力以及對時序信息的理—表現尤為突出。Keye-VL-1.5 在同等規模的模型中表現出色，甚至超越了一些閉源模型如 GPT-4o。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-99ae906bd3166733efda4ecc20d5895a63d.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Keye-VL-1.5 採用四階段漸進式訓練流水線，以系統化方式提升模型性能。在視覺編碼器預訓練階段，使用 SigLIP-400M 權重初始化 ViT，並通過 SigLIP 對比損失持續預訓練以適應內部數據分佈。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-09d1b4cf72c51b253843c3541b0130725c4.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;第一階段重點優化投影 MLP 層，實現跨模態特徵的穩固對齊；第二階段解凍全部參數進行端到端多任務預訓練，顯著增強基礎視覺理解能力；第三階段進行退火訓練，利用高質量數據微調模型，彌補上一階段中高質量樣本接觸不足的問題，同時將序列長度擴展至 128K、調整 RoPE 逆頻率配置，並引入長視頻、長文本和大尺度圖像等長上下文數據。 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;最終，通過同質-異質融合技術對不同數據混合比例下的模型權重進行平均，減少固定數據比例帶來的內在偏差，在保持多樣化能力的同時提升模型的魯棒性。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371648</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371648</guid>
      <pubDate>Wed, 10 Sep 2025 12:00:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>AI 編程公司 Replit 發佈第三代自主編碼 Agent</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Replit&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Freplit.com%2Fagent3" target="_blank"&gt;宣佈&lt;/a&gt;推出第三代自主編碼 Agent（Agent 3），官方稱其自主性提升至前代的 10 倍，單次可連續運行 200 分鐘，全程無需人工幹預。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;自主性增強&lt;/strong&gt;：Agent 3 可以自主測試和修復代碼，甚至在後台持續改進用戶的應用，將用戶從重複性工作中解放出來。它能夠像人類一樣在瀏覽器中 「點擊」 和 「操作」，檢查應用中的按鈕、表單和 API，確保一切正常運行。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;持續運行能力&lt;/strong&gt;：該版本能夠持續自主運行超過三小時，相比之前的版本有了很大的進步。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;提升開發效率&lt;/strong&gt;：Agent 3 能夠根據用戶需求生成高質量代碼，並主動提供優化建議，從而提升開發效率。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0911/194603_LoIb_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;新系統通過自研測試框架在瀏覽器內自動點擊按鈕、填寫表單、調用 API 並修復錯誤，其速度比主流 Computer Use 模型快 3 倍，成本則降低了 90%。&lt;/p&gt; 
&lt;p&gt;Agent 3 支持自然語言提示，用戶可以用簡單描述啓動複雜項目，並在手機端通過 Live Monitoring 實時查看進度。其另一項突破是能夠生成子 Agent 與自動化流程，成品可直接接入 Slack、Notion、郵件等平台，進一步擴展工作流。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371645/replit-agent3</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371645/replit-agent3</guid>
      <pubDate>Wed, 10 Sep 2025 11:48:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>谷歌推遲發佈 Android 16 QPR1 的 AOSP 源代碼，引發擔憂</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.androidauthority.com%2Fandroid-16-qpr1-source-code-delay-3596650%2F"&gt;根據科技媒體 Android Authority 的報道&lt;/a&gt;，谷歌近期已向 Pixel 設備推送 Android 16 QPR1 更新，但遲遲未按慣例在 48 小時內同步開放 AOSP 源碼，引發第三方 ROM 開發者擔憂。&lt;/p&gt; 
&lt;p&gt;Android 16 QPR1（Quarterly Platform Release 1）是谷歌 Android 系統最近發佈的一個更新，其中包含了 Material 3 Expressive 等新特性。&amp;nbsp;通常在發佈類似更新後，谷歌會在 1-2 天內將對應的源代碼上傳至 AOSP，方便第三方定製 ROM 的開發人員同步或使用這些更新特性。&lt;/p&gt; 
&lt;p&gt;但截至目前，一週過去了，AOSP 上的源代碼還沒能找到。 谷歌對外表示，源代碼會在 「接下來幾周內」（「in the coming weeks」）發佈，但未給出更具體的時間表也沒有解釋延遲原因。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-f069cf8e6cb319a979b8b5816b2b9f523c7.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;定製 ROM 社區（如 LineageOS 等）以及使用 AOSP 的開發者對此延遲表示擔憂，因為他們依賴谷歌的及時源代碼發佈來更新他們的系統、添加新特性或修復兼容性問題。&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/news/341310/google-android-development-aosp" target="_blank"&gt;谷歌在年初已經將部分 Android 的開發流程 「完全私有化」&lt;/a&gt;（即不再在公共視線中進行部分開發）以簡化流程。雖然該公司曾多次公開承諾 AOSP 不會取消，但這些動作加劇了對其未來戰略走向的猜測。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371641/android-16-qpr1-source-code-delay</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371641/android-16-qpr1-source-code-delay</guid>
      <pubDate>Wed, 10 Sep 2025 11:31:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>宇樹科技創始人王興興：AI 時代，小組織的爆發力會越來越強</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;9 月 11 日，宇樹科技創始人兼 CEO 王興興出席了 2025 外灘大會，這也是宇樹官宣 IPO 計劃后王興興首次公開發聲。&lt;/p&gt; 
&lt;p&gt;他認為&lt;span&gt;AI 時代的組織管理是一門新課題。王興興表示，宇樹科技是一家以硬件為主要產品的公司，隨着業務快速發展，人員規模更大之後，可能會帶來協作效率的降低，需要花時間探索更高效的組織管理方式。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0911/192019_miGZ_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="font-family:-apple-system,BlinkMacSystemFont,&amp;quot;Apple Color Emoji&amp;quot;,&amp;quot;Segoe UI Emoji&amp;quot;,&amp;quot;Segoe UI Symbol&amp;quot;,&amp;quot;Segoe UI&amp;quot;,&amp;quot;PingFang SC&amp;quot;,&amp;quot;Hiragino Sans GB&amp;quot;,&amp;quot;Microsoft YaHei&amp;quot;,&amp;quot;Helvetica Neue&amp;quot;,Helvetica,Arial,sans-serif"&gt;儘管存在挑戰，但王興興對未來依舊十分樂觀，他認為，現在創新創業的門檻已經大幅降低，年輕創新者迎來了好時代。真正可以用 AI 工具去實現新創意，並且在 AI 時代，小組織的爆發力會越來越強。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;王興興還提到：「頂尖的 AI 人才肯定是缺的，我相信這是每個大公司共同的渴求。」&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371637</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371637</guid>
      <pubDate>Wed, 10 Sep 2025 11:21:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>AgentFly —— 基於記憶增強的在線強化學習框架</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                        
                                                                                    &lt;p&gt;AgentFly 是基於記憶增強的在線強化學習框架，通過記憶庫存儲經驗軌跡並利用神經案例選擇策略實現 LLM 代理的持續適應能力，無需對底層 LLM 參數進行微調。&lt;/p&gt;

&lt;p&gt;該方法將決策過程建模為記憶增強的馬爾可夫決策過程（M-MDP），通過非參數或參數化記憶模塊存儲過往經驗，並基於軟 Q 學習優化案例檢索策略。&lt;/p&gt;

&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-070739b87c472bf439392dc3bc00bb25ddc.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-3e6c7c875e8356c92b9a62c600cca2f9f9e.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

&lt;p&gt;實驗表明，該方法通過記憶庫的持續更新實現高效在線學習，在複雜工具調用和多輪推理任務中展現出顯著優勢，為構建具備持續學習能力的通用型 LLM 代理提供了新範式。&lt;/p&gt;

                                                                    &lt;/div&gt;
                                                                </description>
      <link>https://www.oschina.net/p/agentfly</link>
      <guid isPermaLink="false">https://www.oschina.net/p/agentfly</guid>
      <pubDate>Wed, 10 Sep 2025 11:18:00 GMT</pubDate>
    </item>
    <item>
      <title>小米 Kaldi 團隊開源零樣本語音合成模型模型 ZipVoice</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;近日，小米集團新一代 Kaldi 團隊發佈了基於 Flow Matching 架構的 ZipVoice 系列語音合成（TTS）模型——ZipVoice（零樣本單説話人語音合成模型）與 ZipVoice-Dialog（零樣本對話語音合成模型）。&lt;/p&gt; 
&lt;p&gt;作為 zipformer 在語音生成任務上的應用和探索，ZipVoice 解決了現有零樣本語音合成模型的參數量大、合成速度慢的痛點，在輕量化建模和推理加速上取得了重要突破。ZipVoice-Dialog 則解決了現有對話語音合成模型在穩定性和推理速度上的瓶頸，實現了又快又穩又自然的語音對話合成。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-207702be1087c5d602185a495fa12acd620.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;ZipVoice 系列的模型文件、訓練代碼和推理代碼以及 6.8k 小時的語音對話數據集 OpenDialog 已全部開源：https://github.com/k2-fsa/ZipVoice&lt;/p&gt; 
&lt;p&gt;Zipvoice 論文：https://arxiv.org/pdf/2506.13053&lt;/p&gt; 
&lt;p&gt;樣例體驗請訪問：https://zipvoice.github.io&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371625</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371625</guid>
      <pubDate>Wed, 10 Sep 2025 10:56:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>AI 編程公司 Replit 融資 2.5 億美元，估值達 30 億美元</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;AI 編程公司 Replit &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Freplit.com%2Fnews%2Ffunding-announcement" target="_blank"&gt;宣佈&lt;/a&gt;完成了一輪 2.5 億美元融資，使其估值達到了約 30 億美元，較 2023 年上一輪融資增長近三倍。在過去一年中，Replit 的年收入從 280 萬美元飆升至 1.5 億美元，目前。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;公告稱，Replit 的用戶數量已達到 4000 萬。此次融資正值該公司年化收入在不到一年的時間裏從 280 萬美元增長至 1.5 億美元之際（增幅超過 50 倍）。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="291" src="https://oscimg.oschina.net/oscnet/up-37388b8353eaedc1a8426cc5a02204bd9d6.png" width="600" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;此次融資由 Prysm Capital 主導，參與投資的還有 Amex Ventures 和谷歌的 AI Futures Fund。此外，Replit 的現有投資者，包括 Y Combinator、Craft Ventures、Andreessen Horowitz、Coatue Management 和 Paul Graham 等也參與了這一輪融資。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;與此同時，Replit 還宣佈推出了 Agent 3，並聲稱是其迄今為止自主性最強的 agent。Agent 3 的自主性比之前的版本提高了十倍，能夠測試和修復代碼，並構建自定義代理和工作流，從而能夠自動執行任何類型的複雜或重複性任務，而不僅僅是軟件工程。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371618</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371618</guid>
      <pubDate>Wed, 10 Sep 2025 10:31:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>聚焦結構化注意力，探索提升多模態大模型文檔問答性能</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;blockquote&gt; 
 &lt;p&gt;作者：vivo 互聯網算法團隊&lt;/p&gt; 
 &lt;p&gt;本文聚焦多模態大語言模型（MLLMs）在文檔問答（DocQA）任務中的性能提升，提出無需改動模型架構或額外訓練的結構化輸入方法，通過保留文檔層次結構與空間關係（如標題、表格、圖像位置）優化理解能力。研究發現，傳統無結構 OCR 輸入導致注意力分散，性能下降，而 LaTeX 範式結構化輸入顯著提升表現。注意力分析揭示其誘導"結構化注意力"，減少無關區域幹擾，聚焦語義核心。在 MMLongBench、PaperTab 等四個數據集上驗證，該方法尤其在複雜圖表任務中效果顯著，為智能文檔處理與自動問答提供高效的解決方案。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;本文提供配套演示代碼，可下載體驗：&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fvivo%2FStructureMatters" target="_blank"&gt;Github |&lt;/a&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fvivo%2FStructureMatters" target="_blank"&gt;StructureMatters&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;一、引言&lt;/h1&gt; 
&lt;p&gt;多模態大語言模型（Multimodal Large Language Models, MLLMs）蓬勃發展的今天，文檔理解（Document Understanding）作為一項涉及文本、圖表和圖像的複雜任務，依然面臨諸多挑戰。如何高效整合多源信息、理解文檔的層次結構，成為提升 MLLMs 性能的關鍵問題。研究發現了一種無需修改模型架構或額外訓練的新方法：僅通過結構化輸入提升 MLLMs 在文檔問答（DocQA）任務中的表現，同時通過注意力分析實踐探尋結構化輸入帶來性能提升的深層原因。&lt;/p&gt; 
&lt;h1&gt;二、文檔理解的核心挑戰&lt;/h1&gt; 
&lt;p&gt;文檔理解要求模型同時處理文本、圖表、圖像等多模態信息，並準確回答問題。然而，現有方法多依賴於擴展上下文窗口或優化檢索增強生成（RAG），忽略了一個關鍵問題：輸入格式如何影響模型的理解能力？&lt;/p&gt; 
&lt;p&gt;研究發現，傳統的無結構 OCR 文本輸入在某些 case 下未提升模型性能，反而因注意力分散和結構丟失導致性能下降。例如，在 MMLongBench 數據集上，加入無結構 OCR 文本後，模型準確率從 0.389 下降至 0.370。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-12cc41cc30fcd9ed83f9ed46f2f8fc29448.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;當前主流多模態大模型已經具備處理多模態信息的能力，其中 Qwen2.5-VL-7B-Instruct，Phi-3.5-Vision-Instruct，SmolVLM-Instruct 等在多個多模態任務上達到了 SOTA，但在文檔閲讀任務中仍表現不佳。以往文檔閲讀模型通過訓練得到專用模型來進行文檔閲讀理解，並基於文檔回答問題，如 mPLUG-DocOwl，Textmonkey 等模型。但隨着 RAG 的快速發展，像 ColBERT 和 ColPali 這樣的 RAG 方法在分別檢索文本或視覺信息方面已被證明有效，當前主流方法通常基於 RAG 檢索證據頁面，然後將證據信息直接輸入多模態大模型中以便回答 DocQAs。但當問題需要整合來自兩種模態的信息時，它們通常表現不佳。&lt;/p&gt; 
&lt;p&gt;隨着通用大模型的發展和 AGI 概念的普及，如何直接利用通用多模態大模型達到目的，不額外進行訓練成為研究熱點。改變輸入結構能否幫助多模態大模型進行高效推理為本文探討的重點。本文致力於探尋通用多模態大模型在何種條件下能夠具有更加高效的推理理解能力，能否具備在 trainning free 的條件下達到較高的多元素文檔理解能力。&lt;/p&gt; 
&lt;h1&gt;三、創新方法：結構化輸入與注意力分析&lt;/h1&gt; 
&lt;p&gt;為解決這一問題，提出了一種基於 LaTeX 範式的結構保留方法。該方法通過保留文檔的層次結構和空間關係（如標題、表格、圖像的位置），從而為模型提供更清晰的語義引導。&lt;/p&gt; 
&lt;p&gt;具體流程包括：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;結構化編碼&lt;/strong&gt;：將 OCR 文本和圖像輸入 MLLMs，提示模型儘可能保留圖表、表格和文本的結構，生成 LaTeX 格式的表示。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;聯合輸入&lt;/strong&gt;：將結構化文本與原始圖像一同輸入模型，指導其在回答問題時關注關鍵區域。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;注意力分析&lt;/strong&gt;：通過比較僅圖像輸入、圖像加無結構文本、圖像加結構化文本三種情況的注意力分佈，發現結構化輸入顯著減少了注意力浪費，引導模型聚焦於語義相關的文本和圖像區域。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;實驗結果表明，該方法在多個文檔理解基準數據集上顯著提升了模型性能。例如，在 MMLongBench 上，QWEN2.5-VL-7B-INSTRUCT 的準確率從 0.389 提升至 0.435；在 PaperTab 數據集上，準確率提升高達 20%，得益於 LaTeX 格式對錶格和圖表的精準解析。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-51b199d7da4f2b8e482da26b791c77d6d76.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h1&gt;四、通過注意力機制進行深層原因探究&lt;/h1&gt; 
&lt;p&gt;進一步的，通過注意力分析揭示了結構化輸入的內在機制。無結構文本輸入導致模型注意力分佈散亂，浪費在圖像邊緣或無關區域；而結構化文本添加了結構化約束，誘導模型形成"結構化注意力"模式，聚焦於文檔的核心內容（如圖表、文本塊）。例如，在一個案例中，模型需根據圖表回答"西德居民對美俄關係的看法比例"。無結構輸入下，注意力分散在圖像空白區域；結構化輸入後，注意力集中於圖表和相關文本，顯著提高答案准確性。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-c1cf6c7f984dc7670fa8d60b87bd138c1bf.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;結構化輸入幫助減少 MLLMs 對於圖片邊界 token 的關注度，提高了模型對於文章主體部分的注意力得分。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-4266ab22bf20fc556f2d3de92b379f90a11.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;具體實例分析，證明結構化輸入的重要意義。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-8d93f9bca6600aaabfd3cab20c1a73d141c.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h1&gt;五、實驗驗證與數據支持&lt;/h1&gt; 
&lt;p&gt;在四個文檔理解基準數據集（MMLongBench、LongDocUrl、PaperTab、FetaTab）上測試 4 種 MLLMs 模型（如 QWEN2-VL-7B-INSTRUCT、Phi-3.5-Vision-Instruct）。結果顯示，結構化輸入在所有數據集上均提升了模型性能，尤其在包含複雜圖表的 PaperTab 數據集上效果顯著。消融實驗進一步證明，僅用結構化文本或僅用圖像的性能均低於兩者結合，驗證了結構化輸入與圖像聯合使用的必要性。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-41cf00ff446e8353ac63034f7b316b625ac.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-b172ced302ebcd0de4814ef3027cb3a7f41.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h1&gt;六、總結與展望&lt;/h1&gt; 
&lt;p&gt;實踐研究揭示了輸入格式對 MLLMs 文檔理解能力的關鍵影響，提出了一種簡單而高效的結構化輸入方法。未來可進一步探索更先進的結構提取技術或設計注意力控制插件，以進一步釋放 MLLMs 在文檔理解中的潛力。該研究提供了一種無需重訓模型即可提升性能的實用方案，適用於智能文檔處理、自動問答等場景。在沒有額外訓練和架構修改的前提下，通過簡單的結構化文本輸入，可以提升現有多模態大模型在文檔理解任務中的表現。此項研究可以幫助用戶分析、工作解析等場景中更準確地提取信息，提升工作效率。同時，RAG（檢索增強生成）系統也能結合結構化輸入來降低信息檢索中的噪聲，從而更高效地利用檢索到的證據頁面，為未來文檔處理與分析提供了新的實踐路徑。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/vivotech/blog/18691398</link>
      <guid isPermaLink="false">https://my.oschina.net/vivotech/blog/18691398</guid>
      <pubDate>Wed, 10 Sep 2025 10:08:00 GMT</pubDate>
      <author>原創</author>
    </item>
    <item>
      <title>支付寶推出國內首個 AI 付</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;支付寶宣佈推出國內首個「AI 付」服務，面向 AI 時代為智能體提供支付服務，並率先在瑞幸咖啡的 AI 點單助手「Lucky AI」上線，用戶可在瑞幸支付寶小程序或瑞幸咖啡 App，用説話的方式完成下單並支付。這也是行業首次打通智能體內的下單與支付全鏈路，實現無縫的 AI 服務體驗。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;以點咖啡為例，此前用戶通過 AI 智能體下單，仍需跳轉至支付頁面再予手動確認。現在，用戶在瑞幸支付寶小程序或瑞幸 App 喚起「Lucky AI」，不僅可以説句話點咖啡，還可以直接説句「下單」，完成身份核驗後即支付。整個過程用戶無需離開 AI 對話界面，像日常和店員聊天一樣簡單自然。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="496" src="https://oscimg.oschina.net/oscnet/up-ed3ccbc5548251ac7a86f54e281b209e58a.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;螞蟻集團數字支付事業羣首席技術官朱林表示：支付寶始終致力於通過產品創新，解決支付中存在的安全信任和便捷兩大課題，此次為智能體提供「AI 下單+支付」解決方案，旨在服務好用戶、產業和時代的需求，做好激活 AI 產業生態的一把鑰匙。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;朱林認為，伴隨 AI 產業的爆發式增長和智能終端的普及，預計未來 5 年內更自然的新交互支付佔比或超 50%，多樣化的智能設備支付將增長 10 倍，而更智能的 AI 支付市場規模可達萬億級。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;支付寶正着力打造面向 AI 時代的支付服務，構築融入 AI 交互服務的「支付新基建」。目前已推出包括:&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;國內第一個「支付 MCP Server」，讓 AI 智能體可一鍵接入支付寶支付服務;&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;國內第一個「AI 打賞」服務，為在 AI 智能體內有收取讚賞、小費等需求的開發者提供便捷收款能力;&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;國內第一個「AI 訂閲付費」功能，支持開發者在智能體中便捷接入，按服務次數或時長定價並收款;&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;全球第一個智能眼鏡支付「看一下支付」服務，用戶佩戴眼鏡看商家收款碼或收款設備即可支付;&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;國內第一個智能體支付服務「支付寶 AI 付」，用戶説説話可點單又能支付。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371606</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371606</guid>
      <pubDate>Wed, 10 Sep 2025 09:41:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>微軟再次提醒 Windows 將逐步淘汰 VBScript，分三階段進行</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;近期微軟再次提醒用戶，VBScript 將被逐步淘汰，並分享了相關建議。&lt;/p&gt; 
&lt;p&gt;VBScript，即 Visual Basic Script，是微軟在近三十年前開發的腳本語言，曾默認包含在 Windows 中，主要用於自動化任務，不過近年來，該語言也常被黑客利用。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-9c905bf5e0ab3ab7646889588b6b63a0c65.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;早在 2023 年 10 月，微軟就&lt;a href="https://www.oschina.net/news/261322/microsoft-deprecated-vbscript-in-window" target="_blank"&gt;宣佈&lt;/a&gt;將在未來的 Windows 版本中淘汰 VBScript，並在 2024 年 5 月發佈了詳細的時間表，如今微軟又提供了更多細節。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-589957b28bdb1eaa90fcbc1849da2ea8216.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;VBScript 主要用於通過執行外部.vbs 腳本或引用正則表達式庫來擴展 Office 應用程序的功能，不過，隨着 VBScript 的淘汰，這些開發人員和用處將受到影響。&lt;/p&gt; 
&lt;p&gt;微軟將淘汰 VBScript 分為三個階段，第一階段已經啓動，可能會持續到 2026 年或 2027 年。在此期間，VBScript 將作為「按需功能」（FOD）默認啓用，現有項目不會受到影響。&lt;/p&gt; 
&lt;p&gt;當第二階段開始時，該 FOD 將被禁用，最後，在第三階段，VBScript 將被完全移除，這將直接影響之前提到的用處。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;微軟建議客戶使用 Office 2508 版本中包含的 RegExp 類，並默認使用 Microsoft 365 訂閲，這將使開發人員能夠在 VBScript 中繼續使用 RegExp。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;微軟還建議客戶通過 Microsoft 365 升級到最新版本的 Office，以利用新的 RegExp 實現，這將允許開發人員在 Visual Basic 編輯器（VBE）中使用該功能，而無需添加 vbscript.dll。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371602</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371602</guid>
      <pubDate>Wed, 10 Sep 2025 09:28:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Stability AI 發佈專業音頻生成模型 Stable Audio 2.5</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Stability AI 推出專業音頻生成模型&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fstability.ai%2Fnews%2Fstability-ai-introduces-stable-audio-25-the-first-audio-model-built-for-enterprise-sound-production-at-scale" target="_blank"&gt;Stable Audio 2.5&lt;/a&gt;，藉助 Adversarial Relativistic-Contrastive（ARC）後訓練技術，實現複雜音樂結構的高效生成。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-e2eb31cef809fb7d09a49502cdb9a67f522.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在英偉達 H100 GPU 上，模型可在 2 秒內完成最長 3 分鐘的音頻創作，支持前奏、發展、尾聲等多段落結構，並集成音頻修復功能，允許用戶上傳現有音頻進行續寫。&lt;/p&gt; 
&lt;p&gt;該模型同步推出移動端輕量版 Stable Audio Open Small，可在手機端 7 秒內生成 11 秒立體聲。為確保商用合規，Stable Audio 2.5 基於 licensed 數據集訓練，並通過版權識別系統限制用戶上傳版權受限內容。&lt;/p&gt; 
&lt;p&gt;Stability AI 希望該技術能應用於廣告、零售、品牌音效等多個領域，與 WPP 旗下的音效品牌代理機構 Amp 合作，為大型客戶提供一致的音頻識別服務。&lt;/p&gt; 
&lt;p&gt;Stability AI 的音頻團隊還可以根據公司的音效庫調整模型，打造獨特的音頻標識。Stable Audio2.5 將通過 WPP Open 平台面向 WPP 的全球客戶開放。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371600/stability-ai-introduces-stable-audio-2-5</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371600/stability-ai-introduces-stable-audio-2-5</guid>
      <pubDate>Wed, 10 Sep 2025 09:21:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>字節跳動發佈 Seedream 4.0 圖像創作模型，並免費上線豆包</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="text-align:left"&gt;近日，字節跳動 Seed 團隊宣佈推出豆包圖像創作模型 Seedream 4.0。該模型支持文生圖、圖像編輯及多圖參考等功能，多模態生圖效果、速度和可用性在專業評測中達到業界領先水平。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;目前，Seedream 4.0 已在豆包 App、即夢 AI、釦子等產品正式上線，用戶可以免費體驗。該模型也已通過火山引擎開放給企業客戶。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;Seed 團隊表示，「Seedream 4.0 不僅僅是一個圖像生成模型，更是一個具備知識和思考能力的多模態創意引擎。」&lt;/p&gt; 
&lt;p style="text-align:left"&gt;測試案例顯示，Seedream 4.0 不僅能理解物理規律與時間約束、三維空間等複雜語境，還能在解謎、填字、續寫漫畫等任務中保持風格一致與細節精緻，邏輯推理和創意生成能力表現出色。&lt;/p&gt; 
&lt;p style="text-align:center"&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//3284d12cbee79faafba2468e30b8a639.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="text-align:center"&gt;Seedream 4.0 測試效果（Prompt：六個小時後這個圖片的場景是什麼樣子）&lt;/p&gt; 
&lt;p style="text-align:left"&gt;據介紹，Seedream 4.0 可靈活支持文本、圖像的組合輸入，抽取不同圖片元素進行創作，還可一次生成角色連貫、風格統一的組圖，實現表情包、連環畫等各類創意玩法。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;同時，該模型支持高度自由的藝術風格遷移，最高可生成 4K 分辨率的商用級圖像，並具備出色的文字渲染能力，還可處理基礎的公式、表格、統計圖等複雜排版，廣泛適用於教育、電商、廣告設計、影視後期等應用場景。&lt;/p&gt; 
&lt;p style="text-align:center"&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//808dcef2ea27c4e500493eeb6ed82c6e.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="text-align:center"&gt;Seedream 4.0 測試效果（Prompt：參考圖 2 的風格，將圖 1 做風格轉換）&lt;/p&gt; 
&lt;p style="text-align:left"&gt;基於高效的模型架構和多層推理加速，Seedream 4.0 實現了高質量和高效生成的平衡。Seed 官網顯示，Seedream 4.0 在各維度專業評測的綜合表現排名業界前列，視覺美感、速度等關鍵指標成績突出，並展現出較強的可靠性。&lt;/p&gt; 
&lt;p style="text-align:center"&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//7f85c62003df2d8193e4830a17209985.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="text-align:center"&gt;圖：MagicBench 「文生圖」及「單圖編輯」人工評測基準（數據來源：Seed 官網）&lt;/p&gt; 
&lt;p style="text-align:left"&gt;Seed 團隊表示，圖像創作正在從文生圖進入多模態交互的新階段，Seedream 4.0 已具備通用多模態創意引擎的雛形。團隊將繼續探索更實時的交互式生成體驗，進一步深度融合多模態推理與世界知識，更好地幫助用戶激發靈感、實現創意。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371598</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371598</guid>
      <pubDate>Wed, 10 Sep 2025 09:17:00 GMT</pubDate>
      <author>作者: 開源科技</author>
    </item>
    <item>
      <title>zebra-bpm 一款用戶友好、快速上手的工作流系統</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#3c3c43; margin-left:0; margin-right:0; text-align:start"&gt;zebra-bpm 在 Flowable 基礎上進行研發而成，其中拖拽式表單和可視化流程設計器，區別於傳統工作流系統，傳統工作流系統表單設計器和流程設計器晦澀難懂，對於普通企業用戶使用門檻偏高，沒有經過專業培訓根本無從下手，需要相關專業人員輔助來創建流程。而本設計器界面簡單，符合普通大眾的思維邏輯，易於理解和上手使用。&lt;/p&gt; 
&lt;h3&gt;優勢&lt;a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fzebra.zhanghongbin.xyz%2Fproduct%2Fworkflow.html%23%25E4%25BC%2598%25E5%258A%25BF" target="_blank"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;‌&lt;strong&gt;✓ 簡化複雜的流程設計&lt;/strong&gt;‌&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;可視化的流程設計器，讓流程設計更直觀。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;‌&lt;strong&gt;✓ 符合用戶操作習慣&lt;/strong&gt;‌&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;優化用戶操作體驗，快速掌握工作流設計工具。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;‌&lt;strong&gt;✓ 更靈活的與第三方系統對接&lt;/strong&gt;‌&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;提供眾多接口，方便對接三方系統如：用戶，部門，流轉條件等。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#3c3c43; margin-left:0; margin-right:0; text-align:start"&gt;演示地址:&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fbpm.zhanghongbin.xyz%2F" target="_blank"&gt;http://bpm.zhanghongbin.xyz&lt;/a&gt;&lt;/p&gt; 
&lt;p style="color:#3c3c43; margin-left:0; margin-right:0; text-align:start"&gt;文檔地址&amp;nbsp;http://zebra.zhanghongbin.xyz/product/workflow.html&lt;/p&gt; 
&lt;table cellspacing="0" style="-webkit-text-stroke-width:0px; background-color:#ffffff; border-collapse:collapse; box-sizing:border-box; color:#3c3c43; display:block; font-family:&amp;quot;LXGW WenKai Lite&amp;quot;,&amp;quot;Inter var experimental&amp;quot;,&amp;quot;Inter var&amp;quot;,-apple-system,BlinkMacSystemFont,&amp;quot;Segoe UI&amp;quot;,Roboto,Oxygen,Ubuntu,Cantarell,&amp;quot;Fira Sans&amp;quot;,&amp;quot;Droid Sans&amp;quot;,&amp;quot;Helvetica Neue&amp;quot;,sans-serif; font-size:16px; font-style:normal; font-variant-caps:normal; font-variant-ligatures:normal; font-weight:400; letter-spacing:normal; margin:20px 0px; orphans:2; overflow-x:auto; text-align:start; text-decoration-color:initial; text-decoration-style:initial; text-decoration-thickness:initial; text-transform:none; white-space:normal; widows:2; word-spacing:0px"&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;th style="background-color:var(--vp-c-bg-soft); text-align:left"&gt;用戶&lt;/th&gt; 
   &lt;th style="background-color:var(--vp-c-bg-soft); text-align:left"&gt;賬號&lt;/th&gt; 
   &lt;th style="background-color:var(--vp-c-bg-soft); text-align:left"&gt;密碼&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td style="border-style:solid; border-width:1px"&gt;超級管理員&lt;/td&gt; 
   &lt;td style="border-style:solid; border-width:1px"&gt;admin&lt;/td&gt; 
   &lt;td style="border-style:solid; border-width:1px"&gt;admin123&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371595</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371595</guid>
      <pubDate>Wed, 10 Sep 2025 09:05:00 GMT</pubDate>
      <author>來源: 資訊</author>
    </item>
    <item>
      <title>​字節 Seed 推出全新 AgentGym-RL 框架</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;字節跳動 Seed 研究團隊推出了名為 AgentGym-RL 的新框架，專注於通過強化學習訓練 LLM 代理，使其能夠進行多輪互動決策。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;該框架具有模塊化和解耦的架構，提供了極高的靈活性和擴展性。AgentGym-RL 覆蓋了多種真實場景，能夠支持主流的強化學習算法，幫助代理全面提升其決策能力。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="372" src="https://oscimg.oschina.net/oscnet/up-6607f56f1b2f55c4cb23f3482def33cf8f5.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;為了進一步優化訓練效果，研究團隊還提出了一種名為 ScalingInter-RL 的訓練方法。該方法通過階段性調整交互次數，幫助代理在早期專注於掌握基本技能，隨後逐漸增加交互次數，以鼓勵更多樣化的問題解決策略。這種探索與利用的平衡設計，有助於代理在面對複雜任務時保持穩定的學習和決策能力。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;在實驗過程中，研究者們採用了 Qwen2.5-3B 和 Qwen2.5-7B 作為基礎模型，評估了 AgentGym-RL 和 ScalingInter-RL 在五個不同場景中的表現。結果顯示，使用 AgentGym-RL 的代理在 27 個任務中，表現優於多個商業模型。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;研究團隊計劃將整個 AgentGym-RL 框架，包括代碼和數據集，開源，以支持更多研究者開發智能代理。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;AgentGym-RL 框架涉及的多種場景包括網絡導航、深度搜索、數字遊戲、體感任務和科學實驗等，代理在這些場景中需具備強大的決策能力和適應能力，才能完成複雜的任務。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371591</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371591</guid>
      <pubDate>Wed, 10 Sep 2025 08:40:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Salesforce 開源深度研究 Agent：SFR-DeepResearch (SFR-DR)</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Salesforce &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2FCaimingXiong%2Fstatus%2F1965440617334685886" target="_blank"&gt;發佈&lt;/a&gt;了開源深度研究 Agent：SFR-DeepResearch（SFR-DR）。該模型基於 OpenAI 的小型開源權重模型，通過強化學習進行訓練，具備推理、搜索與代碼執行能力，可自主完成深度研究任務。&lt;/p&gt; 
&lt;p&gt;SFR-DR-20B 版本僅依靠網頁搜索、瀏覽器和 Python 解釋器，在純文本的 Humanity's Last Exam 基準測試中取得了 28.7% 的成績。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-1e558bc3cb2f0faa6a5717b108de0484740.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;SFR-DR 亮點特性如下&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;核心能力&lt;/strong&gt;：基於強化學習（RL）訓練的自主研究代理，能夠獨立推理、搜索和編程，完成深度研究任務。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;性能表現&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;SFR-DR-20B&lt;/strong&gt; 在 Humanity's Last Exam（純文本）上取得 &lt;strong&gt;28.7%&lt;/strong&gt; 的成績&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;僅依賴網絡搜索、網頁瀏覽和 Python 解釋器&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;超越 OpenAI o3 DeepResearch 和 Kimi Researcher&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;訓練方式&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;端到端 RL，從優化推理能力的基礎模型開始訓練&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;保留推理能力的同時提升研究執行能力&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;自主性&lt;/strong&gt;：無需預定義多代理工作流，可自主規劃、推理、提出方案並執行行動&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;論文地址：&lt;em&gt;https://arxiv.org/abs/2509.06283&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371589</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371589</guid>
      <pubDate>Wed, 10 Sep 2025 08:33:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>香港科技大學（廣州）&amp;點動科技行業智能體聯合實驗室簽約儀式圓滿舉行</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#2c3e50; margin-left:0px; margin-right:0px; text-align:center"&gt;&lt;img alt="https://img2.danews.cc/upload/images/20250911/335597000129451c4b99c38ebaa51987.png" src="https://oscimg.oschina.net/oscnet//e243bde9227fd33846dcf43e9ba49f7f.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;2025 年 9 月 8 日，香港科技大學（廣州）與廣州點動信息科技股份有限公司（下文簡稱：點動科技）成功舉行了行業智能體聯合實驗室簽約暨揭牌儀式。&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;香港科技大學（廣州）校長倪明選教授、署理副校長（研究）李世瑋教授、協理副校長（知識轉移）熊輝教授、點動科技董事長陳科斌先生、點動科技深圳公司總經理陳永潔女士、點動科技董事張培揚先生、智算中心總經理楊雪峯先生以及聯合實驗室技術管理委員會成員、人工智能學域師生代表出席了本次啓動儀式。&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0px; margin-right:0px; text-align:center"&gt;&lt;img alt="https://img2.danews.cc/upload/images/20250911/00efd1cf1afe47160e9910ec586193fe.jpg" src="https://oscimg.oschina.net/oscnet//0e4e7d4a02895df18bd7c5a1c4579c06.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:center"&gt;&lt;img alt="https://img2.danews.cc/upload/images/20250911/a1b9ac73ebad1147fe14334994c23956.jpg" src="https://oscimg.oschina.net/oscnet//d3773435a576065839820477223e18a1.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;啓動儀式現場&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;香港科技大學（廣州）-點動科技行業智能體聯合實驗室，旨在依託雙方優勢，聚焦行業智能體領域的前沿技術研發，推動相關理論創新和應用落地，並積極培養高素質人才。實驗室的成立，不僅標誌着學校在推動研究與實踐深度結合上邁出了堅實一步，更彰顯了對交叉學科融合理念的積極踐行與深入探索。&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;香港科技大學（廣州）校長倪明選教授在致辭中談到，學校自成立以來，始終致力於創新和交叉學科的發展，鼓勵教師與企業開展合作，&lt;strong&gt;目前全校已設立 15 個校企聯合實驗室&lt;/strong&gt;。&amp;nbsp;此次聯合實驗室的成立，不僅有助於科研成果的產業化轉化，也為學生提供了更多的實踐機會。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:center"&gt;&lt;img alt="https://img2.danews.cc/upload/images/20250911/81f40384ad8fe10f8cfbf1eeb1a520e5.jpg" src="https://oscimg.oschina.net/oscnet//35337ec52cdbf3341d93028cd47a6ee2.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;香港科技大學（廣州）校長，倪明選教授&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;廣州點動信息科技股份有限公司董事長&amp;amp;總裁陳科斌先生在發言中提出，本次合作標誌着人工智能領域基礎算力與前沿研究的深度融合。聯合實驗室將圍繞量化金融、數字文娛與智能調度三大戰略性課題展開攻關，充分依託香港科技大學（廣州）在前沿科技領域的創新優勢，加速核心技術突破與產業轉化，切實服務社會高質量發展。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:center"&gt;&lt;img alt="https://img2.danews.cc/upload/images/20250911/9b9746bf77e32c6f7ab4be61a7f32786.jpg" src="https://oscimg.oschina.net/oscnet//eeb06cd2a0691b1940013b5c55160fe4.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;廣州點動信息科技股份有限公司董事長&amp;amp;總裁，陳科斌先生&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;香港科技大學（廣州）-點動科技行業智能體聯合實驗室主任熊輝教授在發言中形象地稱聯合實驗室是「 AI 裝修隊」，能讓年輕老師發揮知識力量，並誠邀合作伙伴攜手共進，實現資源互補，推動項目規模化、高質量發展。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:center"&gt;&lt;img alt="https://img2.danews.cc/upload/images/20250911/50fd0a81584472e270821f4e7dddb8ca.jpg" src="https://oscimg.oschina.net/oscnet//74ad049f14755f16a9425e0ad2283acf.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;聯合實驗室主任，熊輝教授&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;隨後，進入期待已久的簽約及揭牌儀式，在全場嘉賓的見證下，雙方正式簽署合作協議，標誌着聯合實驗室的正式成立。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:center"&gt;&lt;img alt="https://img2.danews.cc/upload/images/20250911/ef091a3ce3e4df72572c2df5b4886dff.jpg" src="https://oscimg.oschina.net/oscnet//fa2e7288b8492592dde810e224d1088e.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;簽約及揭牌儀式儀式現場&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;多位領導和嘉賓共同為聯合實驗室揭牌，隨着全場倒計時的結束，開啓了聯合實驗室的新篇章，香港科技大學（廣州）-點動科技行業智能體聯合實驗室正式揭幕！這一儀式不僅象徵着開啓校企合作的新序幕，更是通往未來技術創新的窗口。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:center"&gt;&lt;img alt="https://img2.danews.cc/upload/images/20250911/d6f2434b538f024f5724f6a84b9feb6d.jpg" src="https://oscimg.oschina.net/oscnet//ad1117d54c26c4260f5f6fe48e0b765f.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;簽約及揭牌儀式儀式現場&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;香港科技大學（廣州）校長倪明選教授和廣州點動信息科技股份有限公司董事長&amp;amp; 總裁陳科斌先生為聯合實驗室技術管理委員會成員頒發聘書。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:center"&gt;&lt;img alt="https://img2.danews.cc/upload/images/20250911/7caa75654b24a333f0602f3213db6189.jpg" src="https://oscimg.oschina.net/oscnet//25570379565719828d20f36d678e7dfe.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;頒發聘書&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;儀式圓滿結束，活動最後進入了青年學者論壇環節，聯合實驗室成員袁子軒助理教授（張譯助理教授代為講解）、於佳冬助理教授、嶽玉濤副教授、劉李助理教授、王澤宇助理教授、楊萌林助理教授，分享了他們在人工智能和物聯網領域的最新研究成果。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:center"&gt;&lt;img alt="https://img2.danews.cc/upload/images/20250911/5792908e2187918dd7a8b87ce56565ea.jpg" src="https://oscimg.oschina.net/oscnet//41306b6555cd6b9e3bea9eff78911364.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;張譯助理教授&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;《自然語言處理在量化金融裏的應用》&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;張譯指出，量化投資以數據為燃料、算力為引擎，AI 與大模型的接入把信號維度從「萬」推往「億」，以海量數據為基、靠統計模型追求風險可控收益，兼具高度分散、策略長期穩定等特點，可借 AI 處理各類數據信號。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:center"&gt;&lt;img alt="https://img2.danews.cc/upload/images/20250911/9051c590a4d358fedd40d8a43b34a1a0.jpg" src="https://oscimg.oschina.net/oscnet//2a46f18eb324c459a2abf6770b60c73c.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;於佳冬助理教授&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;《雲邊端協同:計算與通信智能資源分配研究》&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;於佳冬分享，其題組聚焦數據中心異構算力調度，以「讓算力像電力一樣即取即用」為願景，用強化學習在邊端協同的異構、高動態、受限環境中做序貫決策，統一優化通信、計算與多目標權衡，已在多模態 AI 訓練與 VR 渲染場景驗證，顯著提效。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:center"&gt;&lt;img alt="https://img2.danews.cc/upload/images/20250911/e13ff6df53ef81ad3cff381f72ba5cea.jpg" src="https://oscimg.oschina.net/oscnet//293cc339e8187874c620002320105687.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;嶽玉濤副教授&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;《面向能夠「懂人」和「擬人」的情感陪伴多模態智能體》&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;嶽玉濤分享其團隊深耕多模態領域，工作涉及多模態在情緒智能中的應用、情感互動內容生成、多模態情緒感知系統研發等方向，相關成果可應用於藝術創作、心理治療、醫療、公安及人車行為預測等多個領域。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:center"&gt;&lt;img alt="https://img2.danews.cc/upload/images/20250911/6f0417df1290c78dc0dc499c9485b5c9.jpg" src="https://oscimg.oschina.net/oscnet//427b413b05031df5bfddbd72fe82d139.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;劉李助理教授&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;《情智一體的視聽內容生成研究進展》&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;劉李分享，現有語音生成在情感感染力和擬人化方面不足，團隊開發多層系統&lt;strong&gt;，目標是生成像真人一樣的語音對話&lt;/strong&gt;；提出自動評估系統，展示了根據圖片、文本生成有聲讀物和對話的效果。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:center"&gt;&lt;img alt="https://img2.danews.cc/upload/images/20250911/b9c1e5a6928ca130176da727fd6a5200.jpg" src="https://oscimg.oschina.net/oscnet//69a0979c6621649c222f32752d258cf3.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;王澤宇助理教授&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;《智能三維數字人的保真重建與可控生成》&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;王澤宇分享，其團隊在智能三維數字人領域取得重要突破，&lt;strong&gt;研發出 SplattingAvatar、HeadEvolver 等創新方案&lt;/strong&gt;。這些成果解決了三維屬性兼容編輯、動態渲染等難題，支持藝術家直觀操作，推動數字資產創作與三維動畫發展，為該領域研究提供新方向。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:center"&gt;&lt;img alt="https://img2.danews.cc/upload/images/20250911/5996133cf75d0cff3d2bd82d80606773.jpg" src="https://oscimg.oschina.net/oscnet//5ee1241e889768049f2258caeae4cd0d.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;楊萌林助理教授&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;《個性化推薦與社交智能化匹配 》&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;楊萌林在分享中重點介紹了&amp;nbsp;AI 寵物項目的個性化推薦工作，並表達了與點動深化合作的期待。他談到，社交場景序列推薦方面，用戶與寵物的行為序列對個性化推薦的指導意義；並討論到結合大模型如何篩選最佳結果，對產品功能垂直化起到重要作用。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:center"&gt;&lt;img alt="https://img2.danews.cc/upload/images/20250911/1ceee3424990260776c1054226c6c4e5.jpg" src="https://oscimg.oschina.net/oscnet//578edd7a3a0e9ec7fef547c2fd6b9968.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;全體大合照&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;到此聯合實驗室簽約儀式暨揭牌儀式就畫上了圓滿的句號，這場儀式不僅是雙方產學研協同的全新起點，也標誌着高校前沿學術智慧與產業實踐需求的深度耦合正式啓航，更打破了 AI 技術從 「理論研究」 到 「場景落地」 的壁壘。期待未來雙方以實驗室為紐帶，共同為人工智能推動區域產業高質量發展注入持久活力，書寫產學研協同創新的精彩新篇章！&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;活動主辦：香港科技大學（廣州）&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;廣州點動信息科技股份有限公司&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371573</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371573</guid>
      <pubDate>Wed, 10 Sep 2025 07:58:00 GMT</pubDate>
      <author>作者: 開源科技</author>
    </item>
    <item>
      <title>騰訊開源圖檢索增強生成框架 Youtu-GraphRAG</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;騰訊優圖實驗室開源了 Youtu-GraphRAG，這是一個全新的圖檢索增強生成框架，旨在通過大語言模型+RAG 模式，將知識組織成圖譜，再交給大語言模型進行檢索和推理，從而提高模型在處理複雜問答任務時的準確性和可追溯性。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;Youtu-GraphRAG 特別適用於企業知識庫問答、科研文檔解析、個人知識管理等知識密集型場景。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;Youtu-GraphRAG 通過三大創新實現了從圖構建到索引、再到檢索的垂直統一和認知閉環。首先，它採用了四層知識樹結構，將知識拆解成屬性、關係、關鍵詞和社區四個層次，使得大模型在回答問題時能夠沿着知識樹定位信息，推理路徑清晰可見。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;其次，社區檢測升級不僅關注「誰和誰有關」，還結合語義理解「為什麼它們有關」，生成簡明摘要，幫助用戶快速抓住問題本質。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;最後，智能迭代檢索機制允許用戶提出複雜問題時，將其拆解成多個子問題並行檢索，並通過迭代反思機制對結果進行補充和修正，最終給出更完整、更可靠的回答。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="314" src="https://oscimg.oschina.net/oscnet/up-ab798039306f65590d7249203cb4394d0d9.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;Youtu-GraphRAG 在實踐檢驗中表現出色。在六個&lt;span&gt;權威&lt;/span&gt;基準測試中，&lt;span&gt;最高&lt;/span&gt;可節省 90.71% 的 Token 成本，複雜推理任務的準確率&lt;span&gt;最高&lt;/span&gt;提升 16.62%。此外，該框架支持中英文雙語，跨領域應用無需重構，具有很高的靈活性。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;使用 Youtu-GraphRAG 非常簡單，只需四步即可上手。首先，通過命令行獲取項目代碼。其次，進行環境配置，包括獲取遠程調用模型的憑證 API key 並創建配置文件。然後，一鍵部署項目。最後，通過 curl 命令體驗交互。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371566</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371566</guid>
      <pubDate>Wed, 10 Sep 2025 07:30:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>字節跳動發佈開源多模態模型 Mini-o3</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;字節跳動發佈開源多模態模型 Mini-o3，通過擴展推理模式和交互輪次提升視覺搜索性能，在複雜場景中實現顯著突破。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0911/151240_xzZ9_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;https://mini-o3.github.io/&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;據介紹，Mini-o3 是一個完全開源的多模態模型，專為「邊看邊想」的視覺搜索任務設計。它通過強化學習將工具調用次數擴展到數十輪，在 VisualProbe、V* Bench、HR-Bench、MME-Realworld 等基準上取得了 7B 量級的最佳成績。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-a111377d25b371aba3ccf675445d406ca6b.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-71b6b52f2c739864eda55692e83e9bbe7ce.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;項目公開了訓練代碼、模型權重以及包含 4,500 條數據的 Visual Probe 數據集，允許研究者在非商業許可下復現 OpenAI o3 風格的深度推理行為。&lt;/p&gt; 
&lt;p&gt;Mini-o3 支持深度優先搜索、試錯等多樣化推理模式，測試時交互輪次可擴展至 32 輪以上，準確率隨輪次增加顯著提升（如 VisualProbe-Hard 任務準確率從 35.1% 提升至 48.0%）。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;核心創新&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;挑戰性數據集構建&lt;/strong&gt;：推出 VisualProbe 數據集，包含高分辨率圖像、小目標和密集幹擾物場景，強制模型進行多輪探索。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;迭代數據收集&lt;/strong&gt;：通過冷啓動數據生成多樣化推理軌跡，覆蓋回溯、假設驗證等策略，解決預訓練模型缺乏多輪交互能力的問題。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Over-Turn Masking 策略&lt;/strong&gt;：在強化學習中避免對超輪次響應的懲罰，支持模型深度探索，訓練時輪次上限設為 6 輪，測試時可擴展至 32 輪以上。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;應用案例&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-32f28f4c28db5eb7a911b5d6fe714e2b11b.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371562</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371562</guid>
      <pubDate>Wed, 10 Sep 2025 07:18:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>🔥🔥AI 能打造盲人的第三隻眼？</title>
      <description/>
      <link>https://www.oschina.net/ai-creation/details/2194</link>
      <guid isPermaLink="false">https://www.oschina.net/ai-creation/details/2194</guid>
      <pubDate>Wed, 10 Sep 2025 07:03:00 GMT</pubDate>
    </item>
    <item>
      <title>🔥🔥智能植物收割？能割韭菜不？</title>
      <description/>
      <link>https://www.oschina.net/ai-creation/details/2195</link>
      <guid isPermaLink="false">https://www.oschina.net/ai-creation/details/2195</guid>
      <pubDate>Wed, 10 Sep 2025 07:03:00 GMT</pubDate>
    </item>
  </channel>
</rss>
