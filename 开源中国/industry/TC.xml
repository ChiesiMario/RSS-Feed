<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - industry - 繁體中文（台灣）</title>
    <link>https://www.oschina.net/news/industry</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news/industry" rel="self" type="application/rss+xml"/>
    <description>已對該 RSS 進行格式化操作：中英字符之間插入空格、使用直角引號、標點符號修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-tw</language>
    <lastBuildDate>Thu, 24 Jul 2025 07:46:26 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>從頻繁告警到平穩發佈：服務冷啓動 CPU 風暴優化實踐</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;blockquote&gt; 
 &lt;p&gt;作者：vivo 互聯網服務器團隊- Xie Xiaopeng&lt;/p&gt; 
 &lt;p&gt;本文針對服務啓動後幾分鐘內 CPU 持續處於高峯狀態的問題，提出了自己的分析思路與解決方案。最終線上效果比較顯著，成功解決了每次發版過程中頻繁告警、業務受損以及用戶體驗不佳的問題，為服務的高可用性增添了一道重要保障。本文的重點在於問題的發現、分析及解決思路。對於 CPU 相關的問題，火焰圖和 Arthas 是非常有效的工具，建議大家在遇到類似情況時，積極嘗試使用這些工具進行排查和解決。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;1 分鐘看圖抓住核心觀點👇&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//ac1252a6e0f78995afecc8da9af3fe0b.gif" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h1_1"&gt;&lt;/span&gt; 
&lt;h1&gt;一、背景&lt;/h1&gt; 
&lt;p&gt;最近我們的服務在發佈或重啓時頻繁產生告警，這種情況從發版開始一直持續到發版結束後幾分鐘內，規律非常明顯。&lt;/p&gt; 
&lt;p&gt;起初，我們懷疑是流量接入過快導致了此問題。在服務啓動後，CICD 會檢測 &lt;a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fcheck.do" rel="nofollow" target="_blank"&gt;check.do&lt;/a&gt; 接口，以確認服務是否準備就緒。我們推測，&lt;a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fcheck.do" rel="nofollow" target="_blank"&gt;check.do&lt;/a&gt; 接口成功返回後，CICD 立即接入線上流量，這才引發非常多的異常告警。&lt;/p&gt; 
&lt;p&gt;為瞭解決這一問題，我們與運維團隊進行了溝通，決定將流量接入的時機延遲 30 秒。延遲 30 秒後問題還是沒有得到解決，告警依然持續不斷。&lt;/p&gt; 
&lt;span id="OSC_h1_2"&gt;&lt;/span&gt; 
&lt;h1&gt;二、 問題表象&lt;/h1&gt; 
&lt;p&gt;以線上某一台機器為例，它的啓動步驟如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;2024-09-0416:09:50&amp;nbsp;INFO - 啓動應用 ，執行成功。
2024-09-0416:12:36&amp;nbsp;WARN - 檢查接口：check.do，響應結果：ok
2024-09-0416:13:07&amp;nbsp;INFO - 啓動後等待時間（秒）：30
2024-09-0416:13:07&amp;nbsp;INFO - 恢復 Dubbo 流量成功
2024-09-0416:13:39&amp;nbsp;INFO - 恢復 HTTP 流量成功！
&lt;/code&gt;&lt;/pre&gt; 
&lt;span id="OSC_h2_3"&gt;&lt;/span&gt; 
&lt;h2&gt;2.1 Dubbo 接口超時嚴重&lt;/h2&gt; 
&lt;p&gt;恢復 HTTP 流量後，很多調用下游的 Dubbo 接口發生超時，以畫像接口為例，告警開始時間為：2024-09-04 16:14:07.251，結束時間為：2024-09-04 16:17:31.224，期間超時請求數為：578。&lt;/p&gt; 
&lt;span id="OSC_h2_4"&gt;&lt;/span&gt; 
&lt;h2&gt;2.2 HTTP 接口超時嚴重&lt;/h2&gt; 
&lt;p&gt;大部分 HTTP 接口也超時嚴重，P95 響應時間從正常的幾十毫秒飆升至幾秒鐘，16:17:30 後逐漸恢復至正常水平。&lt;/p&gt; 
&lt;span id="OSC_h2_5"&gt;&lt;/span&gt; 
&lt;h2&gt;2.3 CPU 異常&lt;/h2&gt; 
&lt;p&gt;服務發佈前後 CPU 表現異常，啓動過程 CPU 存在突刺，接入線上流量後一段時間內 CPU 使用率將近 100%，16:17:30 後逐步下降，恢復到正常水平。下圖為服務發佈前後 CPU 的使用率截圖。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//e1cf61c214438c27647181f67523a5bc.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;服務發佈前後 CPU 使用率&lt;/p&gt; 
&lt;span id="OSC_h2_6"&gt;&lt;/span&gt; 
&lt;h2&gt;2.4 Runnable、Blocked 線程突刺&lt;/h2&gt; 
&lt;p&gt;下圖為線程數的相關監控指標，我們可以看到：在服務發佈期間，活躍線程數持續增加，啓動期間線程劇增，接入線上流量後線程逐步增加，16:17 分之後趨於平穩，其中 16:12:30-16:12:40 期間活躍線程數從 249 增加到 1026（啓動期間業務側有很多任務均會創建線程池，不影響本次分析）&lt;/p&gt; 
&lt;p&gt;Runnable 線程數與 Blocked 線程數也有突刺，時間是 16:13:30-16:17:30，與接入 HTTP 流量時間相符，與 CPU 突刺時間完全一致。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//0284bd6e471cdeab5825a6b1a4076566.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Runnable 線程&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//997e2e5b9b6042e8231d8768f6ac37b6.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Blocked 線程&lt;/p&gt; 
&lt;span id="OSC_h2_7"&gt;&lt;/span&gt; 
&lt;h2&gt;2.5 老年代上漲快&lt;/h2&gt; 
&lt;p&gt;在查看 GC 老年代內存使用情況時，我們發現啓動後未接入流量時，老年代內存為 985.84MB。而在接入流量後，截止到 16:17:30，內存使用量已經上升至 1.36GB，期間老年代的內存增長速度較快。&lt;/p&gt; 
&lt;span id="OSC_h2_8"&gt;&lt;/span&gt; 
&lt;h2&gt;2.6 下游依賴正常&lt;/h2&gt; 
&lt;p&gt;從上游視角查看下游依賴的情況，隨便挑一個 Dubbo 接口超時嚴重的下游依賴，我們查看一下服務的監控指標，發現服務的請求量在啓動期間有突刺（業務側在啓動期間會主動發起調用，刷新一些緩存，正常現象），啓動後流量幾乎沒變，但是成功率卻有明顯下降，且最大響應時間顯著增加。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//c15914b39be50c378c9e269f81b7c872.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;上游視角&lt;/p&gt; 
&lt;p&gt;但是從下游視角再看服務相關指標，接口成功率正常，且最大響應時間也正常，説明不是下游服務的問題。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//3138ef8521191df0a25e0843279fb18d.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;下游視角&lt;/p&gt; 
&lt;span id="OSC_h1_9"&gt;&lt;/span&gt; 
&lt;h1&gt;三、原因初步判斷&lt;/h1&gt; 
&lt;p&gt;從監控數據來看，在線上流量恢復後，我們的服務當前擁有的線程數不足以處理這些業務請求，因此導致系統大量創建業務線程。由於 CPU 的時間片調度策略，線程之間會頻繁發生上下文切換，從而引發 CPU 負載的劇烈上升，甚至達到飽和狀態。&lt;/p&gt; 
&lt;p&gt;因此通過初步分析，我們得到以下&lt;strong&gt;結論&lt;/strong&gt;：&lt;/p&gt; 
&lt;p&gt;引起 CPU 飆升的原因主要是由於過多的 Runnable 狀態線程以及頻繁的線程上下文切換所導致。我們觀察到系統中存在大量已啓動的線程，這些線程的狀態在 Blocked（鎖等待、IO 等待等）和 Runnable 之間不斷變化。當鎖競爭激烈時，CPU 飆升的現象就很容易出現。&lt;/p&gt; 
&lt;span id="OSC_h1_10"&gt;&lt;/span&gt; 
&lt;h1&gt;四、嘗試初步解決&lt;/h1&gt; 
&lt;span id="OSC_h2_11"&gt;&lt;/span&gt; 
&lt;h2&gt;4.1 流量逐步灰度&lt;/h2&gt; 
&lt;p&gt;既然我們懷疑是流量全部接入後，線程不足導致的問題，因此需要嘗試流量緩慢接入是否能解決這個問題。&lt;/p&gt; 
&lt;p&gt;我們與運維同學線上隨機找了一台機器進行流量灰度實驗，具體時間節點如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;2024-09-0509:55:21&amp;nbsp;啓動成功
2024-09-0509:56:17&amp;nbsp;灰度 1%
2024-09-0509:57:19&amp;nbsp;灰度 5%
2024-09-0509:58:31&amp;nbsp;灰度 44%
2024-09-0510:03:51&amp;nbsp;開始操作全量
2024-09-0510:08:10&amp;nbsp;全量完成
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;再觀察一下相關指標，我們發現各項指標均正常：CPU 使用率不再有突刺，Runnable 線程數和 Blocked 線程數也保持穩定，之前的負載尖峯現象已消失。同時異常超時的日誌記錄也不再出現，老年代內存的增長速度緩慢，HTTP 接口、Dubbo 接口 P95 響應時間正常。由此可見，流量灰度可以解決該問題。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;① CPU 使用率情況如下：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//45fcaa86e0dd82eed37f4645725774ae.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;CPU 使用率&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;② 線程指標情況如下：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//cd1e221cd3022fbb8e17ac382f336365.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Runnable 線程&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//75a065693cd049c3c8d50ca0f2807af2.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Blocked 線程&lt;/p&gt; 
&lt;span id="OSC_h2_12"&gt;&lt;/span&gt; 
&lt;h2&gt;4.2 緩存預熱&lt;/h2&gt; 
&lt;p&gt;在前文中提到，接入線上流量後，老年代的內存增長較快，因此我們推測在服務啓動初期，由於尚未加載相關的緩存數據，當大量請求湧入時，未命中緩存的情況頻繁發生，這迫使系統不斷向下遊請求以加載緩存，從而導致接口響應變慢。為此，我們需要驗證預熱緩存的有效性，以確定是否能夠改善這一問題。&lt;/p&gt; 
&lt;p&gt;緩存預熱的&lt;strong&gt;主要作用和目的&lt;/strong&gt;如下：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;提高緩存命中率&lt;/strong&gt;：通過預先加載熱點數據，能夠顯著提升緩存的命中率，從而減少對後端數據源（如數據庫）的訪問，降低系統負載。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;保持服務性能穩定&lt;/strong&gt;：在服務啓動或緩存失效之後，緩存預熱可以有效防止請求對後端數據源施加突發壓力，從而確保服務性能的穩定性。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;優化用戶體驗&lt;/strong&gt;：由於熱點數據已被預先加載到緩存中，用戶在請求這些數據時能夠獲得更快的響應速度，從而顯著提升用戶體驗。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;方案&lt;/strong&gt;如下：&lt;/p&gt; 
&lt;p&gt;我們將梳理本地緩存信息，根據訪問量和緩存大小區分數據的重要程度，定期將重要的緩存信息刷新至 Redis 中。在服務啓動後，未接入線上流量之前，我們將優先從 Redis 中進行數據的預加載。通過這一措施，確保系統在高流量環境下的穩定性和性能。&lt;/p&gt; 
&lt;p&gt;實驗結果顯示，增加緩存預熱後，問題並未得到有效解決，表現差異微乎其微。&lt;/p&gt; 
&lt;p&gt;僅僅預熱重要緩存無法解決當前問題。系統在啓動時需要預熱的內容相對較多，同時各類中間件也有自身的緩存需要預熱。因此僅預熱業務自定義的內存緩存，效果非常有限。&lt;/p&gt; 
&lt;p&gt;回顧之前的原因分析，我們僅僅關注了表面現象，如 CPU 的上漲和線程數的增加，而未深入挖掘問題的本質。我們需要探討線程數為何上升、CPU 為何飆升，接下來將進行更深入的分析，以找出問題的根本原因。&lt;/p&gt; 
&lt;span id="OSC_h1_13"&gt;&lt;/span&gt; 
&lt;h1&gt;五、分析問題&lt;/h1&gt; 
&lt;span id="OSC_h2_14"&gt;&lt;/span&gt; 
&lt;h2&gt;5.1 初步分析堆棧與 CPU 火焰圖&lt;/h2&gt; 
&lt;p&gt;我們選擇了一台線上機器進行服務重啓，並在接入線上流量後的幾分鐘內導出了程序的線程堆棧信息，進行分析：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Runnable 線程數顯著增多，佔比達到 29%，通常情況下，Runnable 線程數約為 70 個，而此時卻激增至 462 個；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;進一步查看 Runnable 線程，發現大部分線程為 catalina-exec 線程（380 個），這是 Tomcat 用於執行 Spring MVC 應用中 Servlet 請求的線程。正常情況下，這類線程的數量僅有幾個；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;在這些 Runnable 線程中，有 201 個線程均被阻塞在 org.springframework.beans.PropertyMatches.calculateStringDistance(PropertyMatches.java:170) 這個方法上，我們需要進一步分析其原因。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;看了堆棧信息後，應該有個疑問：為什麼啓動了這麼多的 tomcat 線程？&lt;/p&gt; 
&lt;p&gt;我們推測原因在於服務剛啓動時，系統尚未加載任何緩存，所有數據都需要進行首次加載。在這種情況下，服務無法快速響應用戶請求，導致接口的響應時間（RT）顯著上升。在相同的 QPS 的情況下，為了處理不斷增加的業務請求，系統不得不創建更多的 Tomcat 線程。&lt;/p&gt; 
&lt;p&gt;接下來我們接入 Arthas 工具，採集 CPU 火焰圖以進行深入分析，CPU 火焰圖如下：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//aa098319277f41bc34e91ca21c250e33.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;異常 CPU 火焰圖&lt;/p&gt; 
&lt;p&gt;分析結果顯示，CPU 耗時主要集中在 calculateStringDistance 方法，這與我們之前的線程堆棧分析結果一致。在服務啓動時的 CPU 火焰圖中，calculateStringDistance 方法的 CPU 消耗佔比高達 16.68% + 39.09% + 8.38% = 64.15%，整體 CPU 使用率接近 97%。&lt;/p&gt; 
&lt;p&gt;經過一段時間的運行後，再觀察正常情況下的 CPU 火焰圖，calculateStringDistance 方法的 CPU 消耗佔比降至 3.39% + 8.57% + 1.78% = 13.74%，整體 CPU 使用率則徘徊在 25% 至 42% 之間。&lt;/p&gt; 
&lt;p&gt;這一變化表明，隨着系統的穩定運行，CPU 負載逐漸得到緩解，但 calculateStringDistance 方法仍然是性能瓶頸之一。它雖然不是 CPU 使用率飆升的根因，但它在服務啓動後進一步加劇了 CPU 的負載。&lt;/p&gt; 
&lt;span id="OSC_h3_15"&gt;&lt;/span&gt; 
&lt;h3&gt;5.1.1 calculateStringDistance 加劇 CPU 暴漲&lt;/h3&gt; 
&lt;p&gt;在相同 QPS 的情況下，為什麼在服務啓動後的幾分鐘內 calculateStringDistance 方法消耗的 CPU 資源嚴重，而經過一段時間後，這一消耗又有所減小？&lt;/p&gt; 
&lt;p&gt;前文的分析指出，服務剛啓動時，流量瞬間恢復，導致系統需要創建大量的業務線程。這些線程在處理請求時，都會執行 calculateStringDistance 方法。由於該方法本身的計算開銷較大，且併發執行的線程數量越多，CPU 的消耗就會越顯著。因此在服務啓動初期，CPU 的負載急劇上升。隨着運行時間的延長，業務線程的創建和執行也趨於平衡，併發執行的線程數量大大減小，CPU 消耗也隨之減小。&lt;/p&gt; 
&lt;span id="OSC_h3_16"&gt;&lt;/span&gt; 
&lt;h3&gt;5.1.2 calculateStringDistance 源碼分析&lt;/h3&gt; 
&lt;p&gt;calculateStringDistance 方法的功能是根據 Levenshtein 算法計算給定兩個字符串之間的距離或相似度。通過分析其源代碼，我們可以發現，在比較兩個字符串時，該方法採用了嵌套的 for 循環結構。在這些循環中，涉及到 length、chatAt 和 Math.min 函數的調用，這使得該方法的計算複雜度相對較高。調用量越大，CPU 消耗就會越嚴重。根據 CPU 火焰圖的分析，發現這三個函數的 CPU 消耗佔比與 calculateStringDistance 方法的 CPU 消耗佔比之間的比例高達 78%。因此在調用該方法時要小心，在高併發場景下，該方法很有可能成為系統的性能瓶頸，對 CPU 產生影響。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;private&amp;nbsp;static&amp;nbsp;int&amp;nbsp;calculateStringDistance(String s1, String s2){
&amp;nbsp;&amp;nbsp;if&amp;nbsp;(s1.isEmpty()) {
&amp;nbsp; &amp;nbsp;&amp;nbsp;return&amp;nbsp;s2.length();
&amp;nbsp; }
&amp;nbsp;&amp;nbsp;if&amp;nbsp;(s2.isEmpty()) {
&amp;nbsp; &amp;nbsp;&amp;nbsp;return&amp;nbsp;s1.length();
&amp;nbsp; }

&amp;nbsp;&amp;nbsp;int[][] d = newint[s1.length() +&amp;nbsp;1][s2.length() +&amp;nbsp;1];
&amp;nbsp;&amp;nbsp;for&amp;nbsp;(int&amp;nbsp;i =&amp;nbsp;0; i &amp;lt;= s1.length(); i++) {
&amp;nbsp; &amp;nbsp; d[i][0] = i;
&amp;nbsp; }
&amp;nbsp;&amp;nbsp;for&amp;nbsp;(int&amp;nbsp;j =&amp;nbsp;0; j &amp;lt;= s2.length(); j++) {
&amp;nbsp; &amp;nbsp; d[0][j] = j;
&amp;nbsp; }

&amp;nbsp;&amp;nbsp;for&amp;nbsp;(int&amp;nbsp;i =&amp;nbsp;1; i &amp;lt;= s1.length(); i++) {
&amp;nbsp; &amp;nbsp;&amp;nbsp;char&amp;nbsp;c1 = s1.charAt(i -&amp;nbsp;1);
&amp;nbsp; &amp;nbsp;&amp;nbsp;for&amp;nbsp;(int&amp;nbsp;j =&amp;nbsp;1; j &amp;lt;= s2.length(); j++) {
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;int&amp;nbsp;cost;
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;char&amp;nbsp;c2 = s2.charAt(j -&amp;nbsp;1);
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(c1 == c2) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; cost =&amp;nbsp;0;
&amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;else&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; cost =&amp;nbsp;1;
&amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp; d[i][j] = Math.min(Math.min(d[i -&amp;nbsp;1][j] +&amp;nbsp;1, d[i][j -&amp;nbsp;1] +&amp;nbsp;1), d[i -&amp;nbsp;1][j -&amp;nbsp;1] + cost);
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; }

&amp;nbsp;&amp;nbsp;return&amp;nbsp;d[s1.length()][s2.length()];
}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;calculateStringDistance 方法是如何觸發的？通過查詢堆棧信息並查看源代碼，我們發現這是 Spring 框架在解析請求參數並注入屬性的過程中所觸發的。堆棧信息如下，從上到下逐步分析堆棧，我們重點分析 setPropertyValues 和 createNotWritable-&lt;/p&gt; 
&lt;p&gt;PropertyException 這兩個方法。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;"catalina-exec-485"&amp;nbsp;#975 daemon prio=5 os_prio=0 tid=0x00007f50e825f000 nid=0x3375 runnable [0x00007f5043ea4000]
&amp;nbsp; &amp;nbsp;java.lang.Thread.State: RUNNABLE
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.beans.PropertyMatches.calculateStringDistance(PropertyMatches.java:170)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.beans.PropertyMatches.access$100(PropertyMatches.java:44)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.beans.PropertyMatches$BeanPropertyMatches.calculateMatches(PropertyMatches.java:208)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.beans.PropertyMatches$BeanPropertyMatches.&amp;lt;init&amp;gt;(PropertyMatches.java:193)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.beans.PropertyMatches.forProperty(PropertyMatches.java:68)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.beans.PropertyMatches.forProperty(PropertyMatches.java:58)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.beans.BeanWrapperImpl.createNotWritablePropertyException(BeanWrapperImpl.java:237)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.beans.AbstractNestablePropertyAccessor.processLocalProperty(AbstractNestablePropertyAccessor.java:435)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.beans.AbstractNestablePropertyAccessor.setPropertyValue(AbstractNestablePropertyAccessor.java:290)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.beans.AbstractNestablePropertyAccessor.setPropertyValue(AbstractNestablePropertyAccessor.java:278)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.beans.AbstractPropertyAccessor.setPropertyValues(AbstractPropertyAccessor.java:95)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.validation.DataBinder.applyPropertyValues(DataBinder.java:860)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.validation.DataBinder.doBind(DataBinder.java:756)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.bind.WebDataBinder.doBind(WebDataBinder.java:192)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.bind.ServletRequestDataBinder.bind(ServletRequestDataBinder.java:106)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.servlet.mvc.method.annotation.ServletModelAttributeMethodProcessor.bindRequestParameters(ServletModelAttributeMethodProcessor.java:152)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.method.annotation.ModelAttributeMethodProcessor.resolveArgument(ModelAttributeMethodProcessor.java:111)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.method.support.HandlerMethodArgumentResolverComposite.resolveArgument(HandlerMethodArgumentResolverComposite.java:121)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.method.support.InvocableHandlerMethod.getMethodArgumentValues(InvocableHandlerMethod.java:158)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest$original$3Q7HrFjh(InvocableHandlerMethod.java:128)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest$original$3Q7HrFjh$accessor$ykGmQRZT(InvocableHandlerMethod.java)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.method.support.InvocableHandlerMethod$auxiliary$Wny4v5BZ.call(Unknown Source)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:97)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:849)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:760)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:967)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:901)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:872)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at javax.servlet.http.HttpServlet.service(HttpServlet.java:650)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at javax.servlet.http.HttpServlet.service(HttpServlet.java:731)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:303)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:347)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:263)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:347)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:263)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:197)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.StandardWrapperValve.invoke$original$Y7IhKDGv(StandardWrapperValve.java:219)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.StandardWrapperValve.invoke$original$Y7IhKDGv$accessor$4IDmuys6(StandardWrapperValve.java)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.StandardWrapperValve$auxiliary$1SL1DIkO.call(Unknown Source)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:110)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:494)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:169)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:104)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:445)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1136)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:637)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1775)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1734)
&amp;nbsp; &amp;nbsp; &amp;nbsp; - locked &amp;lt;0x000000070f1dc100&amp;gt; (a org.apache.tomcat.util.net.NioChannel)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
&amp;nbsp; &amp;nbsp; &amp;nbsp; at java.lang.Thread.run(Thread.java:748)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;先分析 setPropertyValues 方法，該方法負責將請求中的參數映射到目標對象的屬性上，主要是遍歷屬性列表進行賦值並進行異常統一處理，單個屬性的注入繼續看 setPropertyValue 方法。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;public&amp;nbsp;void&amp;nbsp;setPropertyValues(PropertyValues pvs,&amp;nbsp;boolean&amp;nbsp;ignoreUnknown,&amp;nbsp;boolean&amp;nbsp;ignoreInvalid)
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;throws&amp;nbsp;BeansException {
&amp;nbsp; &amp;nbsp;&amp;nbsp;// 聲明 PropertyAccessException 集合，保存單個屬性注入時拋出的 PropertyAccessException 異常
&amp;nbsp; List&amp;lt;PropertyAccessException&amp;gt; propertyAccessExceptions =&amp;nbsp;null;
&amp;nbsp; &amp;nbsp;&amp;nbsp;// 獲取屬性列表
&amp;nbsp; List&amp;lt;PropertyValue&amp;gt; propertyValues = (pvs&amp;nbsp;instanceof&amp;nbsp;MutablePropertyValues ?
&amp;nbsp; &amp;nbsp; &amp;nbsp; ((MutablePropertyValues) pvs).getPropertyValueList() : Arrays.asList(pvs.getPropertyValues()));
&amp;nbsp;&amp;nbsp;for&amp;nbsp;(PropertyValue pv : propertyValues) {
&amp;nbsp; &amp;nbsp;&amp;nbsp;try&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;// 單個屬性的注入，注意：此方法可能會引發任意的 BeansException，如果存在嚴重故障（例如沒有匹配的字段），則不會在此處捕獲該異常。我們可以嘗試只處理不太嚴重的異常。
&amp;nbsp; &amp;nbsp; &amp;nbsp; setPropertyValue(pv);
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp;&amp;nbsp;catch&amp;nbsp;(NotWritablePropertyException ex) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;// 默認是 true，忽略未知屬性，因此不會拋異常
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(!ignoreUnknown) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;throw&amp;nbsp;ex;
&amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;// Otherwise, just ignore it and continue...
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp;&amp;nbsp;catch&amp;nbsp;(NullValueInNestedPathException ex) {
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(!ignoreInvalid) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;throw&amp;nbsp;ex;
&amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;// Otherwise, just ignore it and continue...
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp;&amp;nbsp;catch&amp;nbsp;(PropertyAccessException ex) {
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(propertyAccessExceptions ==&amp;nbsp;null) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; propertyAccessExceptions =&amp;nbsp;new&amp;nbsp;LinkedList&amp;lt;PropertyAccessException&amp;gt;();
&amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp; propertyAccessExceptions.add(ex);
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; }

&amp;nbsp;&amp;nbsp;// 如果 propertyAccessExceptions 不為空，需要整合起來，拋一個複合異常 PropertyBatchUpdateException
&amp;nbsp;&amp;nbsp;if&amp;nbsp;(propertyAccessExceptions !=&amp;nbsp;null) {
&amp;nbsp; &amp;nbsp; PropertyAccessException[] paeArray =
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; propertyAccessExceptions.toArray(new&amp;nbsp;PropertyAccessException[propertyAccessExceptions.size()]);
&amp;nbsp; &amp;nbsp; thrownew&amp;nbsp;PropertyBatchUpdateException(paeArray);
&amp;nbsp; }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;propertyValues 屬性的結構如下，它包含了從上游傳遞過來的所有參數。這些參數被封裝成一個集合，便於後續的處理和注入。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//cc70849606402c94a225c47243073954.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;propertyValues 屬性&lt;/p&gt; 
&lt;p&gt;分析 setPropertyValue 方法，該方法主要作用是解析屬性值，如果存在嵌套屬性，則遞歸解析設置最終對應的屬性值，方法最後都會調用 setPropertyValue(tokens, pv) 方法。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;public&amp;nbsp;void&amp;nbsp;setPropertyValue(PropertyValue pv)&amp;nbsp;throws&amp;nbsp;BeansException {
&amp;nbsp;&amp;nbsp;PropertyTokenHolder&amp;nbsp;tokens&amp;nbsp;=&amp;nbsp;(PropertyTokenHolder) pv.resolvedTokens;
&amp;nbsp;&amp;nbsp;if&amp;nbsp;(tokens ==&amp;nbsp;null) {
&amp;nbsp; &amp;nbsp;&amp;nbsp;String&amp;nbsp;propertyName&amp;nbsp;=&amp;nbsp;pv.getName();
&amp;nbsp; &amp;nbsp; AbstractNestablePropertyAccessor nestedPa;
&amp;nbsp; &amp;nbsp;&amp;nbsp;try&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;// 確定給定屬性路徑中的第一個嵌套屬性分隔符，忽略鍵中的點（如 「map[my.key]」）。
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;// 當配置的屬性名 propertyName 中包含'.'這樣字符時，代表需要設置嵌套屬性
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;// 如果存在嵌套屬性，Spring 會遞歸向下獲取最終設置的屬性，比如：a.b.c，Spring 會遞歸調用獲取到 b，c 是需要設置的屬性
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;// 如果沒有嵌套屬性的話。會返回自身
&amp;nbsp; &amp;nbsp; &amp;nbsp; nestedPa = getPropertyAccessorForPropertyPath(propertyName);
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp;&amp;nbsp;catch&amp;nbsp;(NotReadablePropertyException ex) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; thrownew&amp;nbsp;NotWritablePropertyException(getRootClass(),&amp;nbsp;this.nestedPath + propertyName,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"Nested property in path '"&amp;nbsp;+ propertyName +&amp;nbsp;"' does not exist", ex);
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;// 將給定的屬性名稱解析為相應的屬性名稱令牌，如果沒有[]，則 tokens 中的 keys 為空，且 actualName、canonicalName 都等於 propertyName&amp;nbsp;
&amp;nbsp; &amp;nbsp; tokens = getPropertyNameTokens(getFinalPath(nestedPa, propertyName));
&amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(nestedPa ==&amp;nbsp;this) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; pv.getOriginalPropertyValue().resolvedTokens = tokens;
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;// 設置屬性
&amp;nbsp; &amp;nbsp; nestedPa.setPropertyValue(tokens, pv);
&amp;nbsp; }
&amp;nbsp;&amp;nbsp;else&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;// 設置屬性
&amp;nbsp; &amp;nbsp; setPropertyValue(tokens, pv);
&amp;nbsp; }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;分析 setPropertyValue(tokens, pv) 方法，該方法是用來區分數組類型跟非數組類型的，大部分屬性都是非數組類型，我們分析非數組類型方法。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;protected&amp;nbsp;void&amp;nbsp;setPropertyValue(PropertyTokenHolder tokens, PropertyValue pv)&amp;nbsp;throws&amp;nbsp;BeansException {
&amp;nbsp; &amp;nbsp;&amp;nbsp;// 如果屬性中存在[]，説明是數組，則進入該方法
&amp;nbsp;&amp;nbsp;if&amp;nbsp;(tokens.keys !=&amp;nbsp;null) {
&amp;nbsp; &amp;nbsp; processKeyedProperty(tokens, pv);
&amp;nbsp; }
&amp;nbsp;&amp;nbsp;else&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;// 大部分都走這個方法
&amp;nbsp; &amp;nbsp; processLocalProperty(tokens, pv);
&amp;nbsp; }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;processLocalProperty 方法的作用就是獲取屬性值，利用反射完成屬性注入。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;private&amp;nbsp;void&amp;nbsp;processLocalProperty(PropertyTokenHolder tokens, PropertyValue pv){
&amp;nbsp; &amp;nbsp;&amp;nbsp;// 獲取屬性對應的 PropertyHandler
&amp;nbsp;&amp;nbsp;PropertyHandler&amp;nbsp;ph&amp;nbsp;=&amp;nbsp;getLocalPropertyHandler(tokens.actualName);
&amp;nbsp; &amp;nbsp;&amp;nbsp;// 如果不存在對應的 handler 或者，屬性是不可寫的（沒有 setter 方法）
&amp;nbsp;&amp;nbsp;if&amp;nbsp;(ph ==&amp;nbsp;null&amp;nbsp;|| !ph.isWritable()) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;// 如果屬性是 optional 類型，則直接返回
&amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(pv.isOptional()) {
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(logger.isDebugEnabled()) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; logger.debug("Ignoring optional value for property '"&amp;nbsp;+ tokens.actualName +
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"' - property not found on bean class ["&amp;nbsp;+ getRootClass().getName() +&amp;nbsp;"]");
&amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;return;
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp;&amp;nbsp;else&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;// 其他情況則拋出不可寫屬性異常，佔用 CPU 較多的方法由此進入
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;throw&amp;nbsp;createNotWritablePropertyException(tokens.canonicalName);
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; }

&amp;nbsp;&amp;nbsp;Object&amp;nbsp;oldValue&amp;nbsp;=&amp;nbsp;null;
&amp;nbsp;&amp;nbsp;try&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;// 獲取屬性值
&amp;nbsp; &amp;nbsp;&amp;nbsp;Object&amp;nbsp;originalValue&amp;nbsp;=&amp;nbsp;pv.getValue();
&amp;nbsp; &amp;nbsp;&amp;nbsp;Object&amp;nbsp;valueToApply&amp;nbsp;=&amp;nbsp;originalValue;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;// 如果需要轉換，則進入此分支
&amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(!Boolean.FALSE.equals(pv.conversionNecessary)) {
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;// 如果已經完成類型轉換，則直接使用
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(pv.isConverted()) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; valueToApply = pv.getConvertedValue();
&amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;else&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;// 如果需要讀取舊值，默認是 false &amp;amp;&amp;amp; 值可讀（有 getter 方法）
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(isExtractOldValueForEditor() &amp;amp;&amp;amp; ph.isReadable()) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;try&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; oldValue = ph.getValue();
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;catch&amp;nbsp;(Exception ex) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(ex&amp;nbsp;instanceof&amp;nbsp;PrivilegedActionException) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; ex = ((PrivilegedActionException) ex).getException();
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(logger.isDebugEnabled()) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; logger.debug("Could not read previous value of property '"&amp;nbsp;+
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;this.nestedPath + tokens.canonicalName +&amp;nbsp;"'", ex);
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;// 類型轉換
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; valueToApply = convertForProperty(
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; tokens.canonicalName, oldValue, originalValue, ph.toTypeDescriptor());
&amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp; pv.getOriginalPropertyValue().conversionNecessary = (valueToApply != originalValue);
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp;&amp;nbsp;// 完成屬性注入
&amp;nbsp; &amp;nbsp; ph.setValue(this.wrappedObject, valueToApply);
&amp;nbsp; }
&amp;nbsp;&amp;nbsp;catch&amp;nbsp;(TypeMismatchException ex) {
&amp;nbsp; &amp;nbsp;&amp;nbsp;throw&amp;nbsp;ex;
&amp;nbsp; }
&amp;nbsp;&amp;nbsp;catch&amp;nbsp;(InvocationTargetException ex) {
&amp;nbsp; &amp;nbsp;&amp;nbsp;PropertyChangeEvent&amp;nbsp;propertyChangeEvent&amp;nbsp;=&amp;nbsp;new&amp;nbsp;PropertyChangeEvent(
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;this.rootObject,&amp;nbsp;this.nestedPath + tokens.canonicalName, oldValue, pv.getValue());
&amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(ex.getTargetException()&amp;nbsp;instanceof&amp;nbsp;ClassCastException) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; thrownew&amp;nbsp;TypeMismatchException(propertyChangeEvent, ph.getPropertyType(), ex.getTargetException());
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp;&amp;nbsp;else&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;Throwable&amp;nbsp;cause&amp;nbsp;=&amp;nbsp;ex.getTargetException();
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(cause&amp;nbsp;instanceof&amp;nbsp;UndeclaredThrowableException) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;// May happen e.g. with Groovy-generated methods
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; cause = cause.getCause();
&amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp; thrownew&amp;nbsp;MethodInvocationException(propertyChangeEvent, cause);
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; }
&amp;nbsp;&amp;nbsp;catch&amp;nbsp;(Exception ex) {
&amp;nbsp; &amp;nbsp;&amp;nbsp;PropertyChangeEvent&amp;nbsp;pce&amp;nbsp;=&amp;nbsp;new&amp;nbsp;PropertyChangeEvent(
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;this.rootObject,&amp;nbsp;this.nestedPath + tokens.canonicalName, oldValue, pv.getValue());
&amp;nbsp; &amp;nbsp; thrownew&amp;nbsp;MethodInvocationException(pce, ex);
&amp;nbsp; }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;在該方法中，我們注意到堆棧信息中 createNotWritablePropertyException 方法的調用。實際上 calculateStringDistance 方法的高 CPU 消耗正是由此引發的。當拋出不可寫屬性異常時，系統會計算字符串的相似度，主要目的是為了向用戶提供更友好的提示，幫助他們識別哪些屬性與當前屬性相似，從而判斷是否在傳遞參數時出現了錯誤。&lt;/p&gt; 
&lt;p&gt;Spring 這種設計不僅提升了用戶體驗，還降低了因參數錯誤而導致的調試難度。通過提供相似屬性的建議，用戶能夠更快速地發現並糾正輸入錯誤，確保請求的正確性。以下為調試過程中的部分提示：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Bean property&amp;nbsp;'questionValidatorInterface'&amp;nbsp;is not writable or has an invalid&amp;nbsp;setter&amp;nbsp;method. Does the parameter type of the&amp;nbsp;setter&amp;nbsp;match the&amp;nbsp;return&amp;nbsp;type of the&amp;nbsp;getter?
bean property&amp;nbsp;'users'&amp;nbsp;is not writable or has an invalid&amp;nbsp;setter&amp;nbsp;method. did you mean&amp;nbsp;'user'?
&lt;/code&gt;&lt;/pre&gt; 
&lt;span id="OSC_h3_17"&gt;&lt;/span&gt; 
&lt;h3&gt;5.1.3 calculateStringDistance 流程總結&lt;/h3&gt; 
&lt;p&gt;結合 Spring MVC 解析 HTTP 的請求流程，calculateStringDistance 方法的進入流程如下：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//c5a198998b410036c75859840afa1f60.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;解析參數流程&lt;/p&gt; 
&lt;p&gt;Spring MVC 在解析 HTTP 請求參數時會找到對應的參數解析器，因為我們的項目中大部分都是自定義的複雜對象，因此採用的參數解析器為 ServletModelAttributeMethodProcessor。該解析器在數據綁定過程中，會循環遍歷每個參數，通過反射完成屬性注入。但是我們自定義的複雜對象在某些接口下，定義的屬性不合理，導致拋出 createNotWritablePropertyException 異常。&lt;/p&gt; 
&lt;p&gt;我們深入分析一下源碼，看看怎樣避免拋出 createNotWritablePropertyException 異常。&lt;/p&gt; 
&lt;p&gt;根據源碼，我們發現拋出不可寫屬性異常的條件是（屬性不存在對應的 handler 或者，屬性不可寫）並且屬性不是 optional 類型，只要我們保證不滿足這個條件，那麼就可以有效避免拋出該異常。&lt;/p&gt; 
&lt;p&gt;説明一下這三個條件：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;屬性不存在對應的 handler 即 request 中不存在該屬性。比如請求參數中帶 version 字段，但是服務端在接受 request 中並未定義 version 字段，那麼此處 ph == null 判斷條件就成立&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;屬性不可寫，即屬性沒有對應的 setter 方法&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;屬性是 optional 類型，即屬性的數據類型是 Optional 類型&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;通過查看業務側的代碼，我們發現請求（request）中的所有屬性都已經定義了相應的 setter 方法，而且不存在 optional 類型的屬性。因此我們只需要關注請求中是否存在未定義的屬性。&lt;/p&gt; 
&lt;span id="OSC_h3_18"&gt;&lt;/span&gt; 
&lt;h3&gt;5.1.4 排查大流量及核心接口參數&lt;/h3&gt; 
&lt;p&gt;由於服務提供的接口非常多，因此僅排查流量較高和核心的接口。經過分析，我們發現幾乎所有接口都存在未定義的屬性。&lt;/p&gt; 
&lt;p&gt;這主要是因為客戶端很多參數都是公參，在傳參時會將這些公參全部透傳給服務端，但是服務端並不需要處理所有的參數，因此沒有在 request 中定義。特別備註：接口若未定義請求參數接收，則不會走上述流程。&lt;/p&gt; 
&lt;span id="OSC_h3_19"&gt;&lt;/span&gt; 
&lt;h3&gt;5.1.5 解決方案&lt;/h3&gt; 
&lt;p&gt;既然已經明確問題的根源是請求中存在未定義的屬性，那麼接下來我們將針對這一情況進行優化。方案主要有兩個：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;在底層請求中加入客戶端公參：對所有公參進行接收，確保它們能夠被正確處理。需要注意的是，參數接收將會涉及屬性注入，而屬性注入是通過反射機制實現的。這一過程可能對 CPU 和接口性能產生影響，因此我們也需要進行實驗，以評估這些參數解析的實際效果。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;在 filter 層針對接口去除相關字段：通過在過濾器層面過濾掉不必要的字段，避免接口中出現未定義的屬性。&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;最終我們混合兩種方案：對於大部分公共參數，定義到底層 request 中；對於非公共參數，針對接口進行移除。&lt;/p&gt; 
&lt;p&gt;我們針對大流量接口及核心接口進行了優化，優化後效果如下：&lt;/p&gt; 
&lt;p&gt;結論：整體效果顯著，但仍存在一些不足之處。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;CPU 使用情況&lt;/strong&gt;：在高峯期重啓應用時，CPU 的突發情況明顯減弱，持續時間從 5 分鐘縮短至 1 分鐘。同時 CPU 和 Runnable 線程數仍會出現小幅波動，但 Runnable 線程數的波動持續時間已從 6 分鐘縮減至 40 秒，波動峯值也由 600 降低至 280。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;接口性能&lt;/strong&gt;：接口的 P95 和 P99 耗時均有所降低，其中 P95 峯值從 53 秒降至 3.4 秒，P99 峯值從 1 分 50 秒降至 50 秒。此外，響應時間較長的時間段也得到了縮短，持續時間從 7 分鐘減少到不到 2 分鐘。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;發版及日常運行&lt;/strong&gt;：在發版期間及日常運行中，CPU 峯值普遍降低。與前 1 天和前 7 天的平均 CPU 使用率相比，最大和最小使用率均有所下降，幅度明顯。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;① 啓動後 CPU 使用率情況如下：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//4912336ab2d7bdc8083d05cf3d17cd1a.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;CPU 使用率&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;② 線程數情況如下：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//91e04839dc6184825966b004e62ffa91.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Runnable 線程數&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//f1879456a68bab5f04dd6cefc79aaa5a.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Blocked 線程數&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;③ 接口響應時間情況如下：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//9de3e1b9a261f06dbfd86fd6a64c5ab9.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;接口響應時間&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;④ 運行一段時間後，CPU 使用率情況如下：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//c3f85b69c1dd77a5861070bdd0f90c38.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;CPU 使用率&lt;/p&gt; 
&lt;span id="OSC_h2_20"&gt;&lt;/span&gt; 
&lt;h2&gt;5.2 優化後再次分析 CPU 火焰圖&lt;/h2&gt; 
&lt;p&gt;優化後效果雖然好了很多，但是 CPU 和 Runnable 線程數仍會出現小幅波動，接口的響應時間在 1 分鐘內仍有上漲。這是我們接下來要繼續優化的目標。&lt;/p&gt; 
&lt;span id="OSC_h3_21"&gt;&lt;/span&gt; 
&lt;h3&gt;5.2.1 編譯階段消耗 CPU 佔比高&lt;/h3&gt; 
&lt;p&gt;再次使用 arthas 進行監測，查看正常情況與啓動後（異常情況）的 CPU 消耗情況，我們可以觀察到：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;runWoker 部分：&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;該部分的 CPU 佔用比例正常，與平時的表現一致，未見異常波動。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;編譯相關的 CPU 佔用：&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;CompileBroker::invoke_compiler_on_method(CompileTask*) 佔用 CPU 較大，特別是 C2Compiler::compile_method(ciEnv*, ciMethod*, int) 的佔比顯著&lt;/p&gt; 
&lt;p&gt;由此我們得出結論：編譯階段的 CPU 消耗佔比異常，可能是導致 CPU 負載突刺的重要因素。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;① 異常情況下 CPU 火焰圖：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//358b54cf05ca85c707e7a9b636e4ba11.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;異常 CPU 火焰圖&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;② 正常情況下 CPU 火焰圖：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//8736bb5cd5d70f06959491e2d9729028.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;正常 CPU 火焰圖&lt;/p&gt; 
&lt;span id="OSC_h3_22"&gt;&lt;/span&gt; 
&lt;h3&gt;5.2.2 用 arthas 換個角度驗證&lt;/h3&gt; 
&lt;p&gt;CPU 火焰圖是基於啓動後 3 分鐘內的綜合數據採集而生成的，雖然能夠提供整體的 CPU 使用情況，但無法反映 CPU 的實時變化。因此，為了更準確地驗證編譯階段是否確實消耗了 CPU，我們需要登錄到機器上，使用 Arthas 進行實時監測。&lt;/p&gt; 
&lt;p&gt;機器啓動後，運行 dashboard 命令，重點關注屏幕上方的進程信息，以識別哪些線程佔據了較高的 CPU 資源，以下為其中一次波動的截圖，前幾次波動 CPU 佔比都差不多：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//e26f168e8937e370278c49578b95b790.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;dashboard 命令&lt;/p&gt; 
&lt;p&gt;從圖中可以看到， CompilerThread 的三個線程佔用了較高的 CPU 資源，尤其是 C2 CompilerThread 的佔比明顯，這與之前通過火焰圖所反映的情況一致。&lt;/p&gt; 
&lt;span id="OSC_h3_23"&gt;&lt;/span&gt; 
&lt;h3&gt;5.2.3 CompilerThread 是什麼&lt;/h3&gt; 
&lt;p&gt;C1 C2 CompilerThread 是 Java HotSpot 虛擬機中的兩個即時編譯器，主要作用是將 Java 字節碼在運行時編譯成本地機器碼，以提高程序的執行效率。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;C1 Compiler（也稱為客戶端編譯器），主要用於快速編譯，優化較少，適合需要快速啓動的應用。它的編譯速度較快，但生成的機器碼執行效率相對較低。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;C2 Compiler（也稱為服務端編譯器），主要用於高性能的編譯，優化程度較高，適合長時間運行的應用。C2 編譯器會花費更多時間進行優化，以生成更高效的機器碼，適合對性能要求較高的場景。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;在 HotSpot 虛擬機中，Java 程序最初都是通過解釋器（Interpreter）進行解釋執行的，解釋器的優點是啓動快，省去編譯的時間，能夠快速運行代碼。但隨着程序的執行，某些方法或代碼塊可能會被多次調用，這些被頻繁調用的代碼被稱為「熱點代碼」（Hot Spot Code）。當虛擬機識別到熱點代碼時，它會啓動 JIT 編譯器（C1 或 C2）將這些代碼編譯成本地機器碼，以提高執行效率。&lt;/p&gt; 
&lt;p&gt;HotSpot 虛擬機是解釋器與即時編譯器並存的架構，兩者經常是相輔相成地配合工作。由於即時編譯器編譯本地代碼需要佔用程序運行時間，編譯出優化程度更高的代碼所需的時間也會相應增加。此外為了實現更高的優化，解釋器需要為編譯器收集性能監控信息，這在一定程度上也會影響解釋執行階段的速度。為瞭解決這一問題，並在程序啓動響應速度與運行效率之間達到最佳平衡，HotSpot 虛擬機在其編譯子系統中引入了分層編譯的功能。通過這一機制，HotSpot 能夠根據代碼的執行頻率和性能需求，逐步將字節碼編譯為本地機器碼，從而在保證快速啓動的同時，優化長時間運行的代碼性能。&lt;/p&gt; 
&lt;span id="OSC_h3_24"&gt;&lt;/span&gt; 
&lt;h3&gt;5.2.4 解決方案&lt;/h3&gt; 
&lt;p&gt;截止到現在，問題的原因就變得十分清晰了：當流量湧入時，HotSpot 虛擬機啓動了分層編譯機制，期間大部分代碼迅速轉變為熱點代碼。在這個過程中，C2 編譯器需要頻繁佔用 CPU 資源進行編譯，導致 CPU 使用率顯著上升。隨着大部分熱點代碼的優化完成，C2 編譯器對 CPU 的佔用將逐漸減少，CPU 使用率也會隨之下降。這一編譯過程的持續時間與監控圖上的 CPU 波動情況高度一致。&lt;/p&gt; 
&lt;p&gt;C1 和 C2 編譯器雖然提供了關閉的參數選項，但關閉這些編譯器無疑會對服務的運行性能產生負面影響。網絡上也有相關實驗案例表明，對於需要長期運行的 Java 後端服務，禁用這些編譯器將導致性能顯著下降。因此這種關閉編譯器的方式並不值得嘗試。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;解決方案一：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在前文中，我們已經驗證了流量逐步放量對機器的影響：採用灰度發佈，對機器幾乎沒什麼影響，各項指標表現都很平穩。由於歷史原因，我們的服務當前無法支持灰度發佈，因此還需要探索其他有效的解決方案。&lt;/p&gt; 
&lt;p&gt;我們可以換個角度思考：是否可以通過降低接口的請求 QPS，並將發版時間固定在每天流量最低的時段，以觀察對服務啓動的影響。&lt;/p&gt; 
&lt;p&gt;首先，我們可以優先關注大流量接口，並嘗試減少這些接口的 QPS。通過優化接口請求的頻率，我們或許能夠在發版過程中減輕對系統的壓力。&lt;/p&gt; 
&lt;p&gt;降低接口 QPS，調整重啓服務的時間（非高峯期），12:13:59 恢復流量成功，實驗效果如下：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;恢復流量後 CPU 最高峯值為 61.5%（依舊有小突刺，但是對業務無影響）&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Runnable、Blocked 線程數不再有突刺&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;接口響應時間（RT）也比較平穩&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;日誌不再告警，無 error 錯誤日誌&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;① CPU 使用率情況如下：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//626e46d5af3e6130174792c68bea26c6.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;CPU 使用率&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;② 線程指標如下：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//c4b821d566b0c09cb9848e68b198ad1b.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Runnable 線程數&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//a8f80feebc04db763c19655bede37681.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Blocked 線程數&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;③ 接口響應時間情況如下：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//41d2850bb678d230ac1c1add16b70335.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;接口響應時間&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;解決方案二：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;方案一雖然能夠在一定程度上緩解問題，但治標不治本。此外我們也無法固定發版時間，因此最有效的策略是進行預熱。與前文不同的點在於，此處是 JVM 預熱。&lt;/p&gt; 
&lt;p&gt;方案為：在系統成功啓動（監測 check.do 返回成功）後，接入線上 HTTP 流量之前，針對大流量接口及核心接口進行 HTTP 接口調用，頻控次數為配置項，方便線上動態調整。特別注意：在剛啓動時，如果機器或下游依賴出現故障，此處的額外調用會加劇系統或下游的負擔，因此調用次數需要合理配置。&lt;/p&gt; 
&lt;p&gt;此方式可以讓 C2 編譯器提前對熱點代碼進行優化，在系統在系統穩定後再將流量接入生產環境，從而避免對用戶造成任何影響。&lt;/p&gt; 
&lt;p&gt;觀察啓動後的各項指標，14:56:25 恢復 HTTP 流量成功，實驗效果如下：&lt;/p&gt; 
&lt;p&gt;整體表現與之前的方案一相似，但是有一個顯著的區別：在恢復 HTTP 流量之前，Runnable 線程數出現了明顯的突刺，而在流量恢復後，這種突刺現象則不再出現，線程數已經趨於平穩。我們注意到突刺出現的時間節點是 14:55:25，這個時間點正好是我們預熱時發起 HTTP 接口調用的時間。&lt;/p&gt; 
&lt;p&gt;這表明通過預熱策略，我們有效地前置了系統的負載波動。當真正的用戶請求到達時，系統已經趨於平穩，服務響應速度保持穩定，從而為用戶提供了更加流暢的體驗。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;① CPU 使用率情況如下：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//3ede52f825b29ea6085afd0dbf6c32c2.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;CPU 使用率&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;② 線程指標如下：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//b4e11728f36cf7ed5f7490ab024b857f.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Runnable 線程數&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//32152718fa8265f6e8e7524b94e66a5b.png" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Blocked 線程數&lt;/p&gt; 
&lt;span id="OSC_h1_25"&gt;&lt;/span&gt; 
&lt;h1&gt;六、總結&lt;/h1&gt; 
&lt;p&gt;本文針對服務啓動後幾分鐘內 CPU 持續處於高峯狀態的問題，提出了自己的分析思路與解決方案。最終線上效果比較顯著，成功解決了每次發版過程中頻繁告警、業務受損以及用戶體驗不佳的問題，為服務的高可用性增添了一道重要保障。最初的分析不夠深入，導致在內存緩存預熱方面的努力未能產生預期效果。因此在未來遇到類似問題時，我們必須深入挖掘，直至找到問題的根本原因。&lt;/p&gt; 
&lt;p&gt;本文的重點在於問題的發現、分析及解決思路。對於 CPU 相關的問題，火焰圖和 Arthas 是非常有效的工具，建議大家在遇到類似情況時，積極嘗試使用這些工具進行排查和解決。&lt;/p&gt; 
&lt;p&gt;此外 HTTP 請求未定義屬性的問題普遍存在，特別是在服務未進行預熱啓動時，會加劇 CPU 的負載。對於大流量服務而言，遇到此類問題時，需規範請求參數，以減輕 CPU 負擔。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/vivotech/blog/18685693</link>
      <guid isPermaLink="false">https://my.oschina.net/vivotech/blog/18685693</guid>
      <pubDate>Thu, 24 Jul 2025 07:12:24 GMT</pubDate>
      <author>原創</author>
    </item>
    <item>
      <title>特朗普希望重命名「人工智能」術語，改為「天才智能」</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;當地時間 7 月 23 日，美國總統特朗普在華盛頓特區舉行的人工智能峯會上&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2FAcyn%2Fstatus%2F1948137088945369220" target="_blank"&gt;發言&lt;/a&gt;，他表示自己不喜歡「人工智能」（ Artificial Intelligence）這個術語表述，建議改名「天才智能」。&lt;/p&gt; 
&lt;p&gt;特朗普在講話中稱，自己不喜歡 AI 中「Artificial」一詞，「我忍不了任何造作的東西，我甚至不喜歡人造東西這個名字」。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0724/145318_T5L4_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;他還表示，「&lt;strong&gt;AI 是天才的，是純粹的天才，我建議把它改名天才智能。&lt;/strong&gt;」特朗普在講話中強調，自己這個想法是「認真的」。&lt;/p&gt; 
&lt;p&gt;根據英國牛津英語詞典，Artificial 一詞除了「人工、人造」外，也有「矯揉造作」的含義。&lt;/p&gt; 
&lt;p&gt;值得一提的是，據外媒相關報道：&lt;a href="https://www.oschina.net/news/362050"&gt;美國政府將簽署大量有關 AI 產業的行政令&lt;/a&gt;。&lt;/p&gt; 
&lt;p&gt;據介紹，這些行政令內容涵蓋多個方面，包括建立支持數據中心、半導體製造工廠建設的舉措，完善國家電力網絡，以及消除 AI 大模型對話中所謂的「意識形態偏見」等。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362115</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362115</guid>
      <pubDate>Thu, 24 Jul 2025 06:54:24 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>脈脈：超四成國內 AI 頭部公司員工欲跳槽</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="background-color:#ffffff; color:#242424"&gt;脈脈平台&lt;/span&gt;&lt;span style="background-color:#ffffff; color:#242424"&gt;最新&lt;/span&gt;&lt;span style="background-color:#ffffff; color:#242424"&gt;數據顯示，截至 2025 年 7 月，國內 AI 頭部公司員工的跳槽意願顯著高於其他行業。高達&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;strong style="color:#242424"&gt;41.07%&lt;/strong&gt;&lt;span style="background-color:#ffffff; color:#242424"&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;的 AI 從業者目前處於「正在看機會」的求職狀態，這一比例遠超互聯網行業的&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;strong style="color:#242424"&gt;14.65%&lt;/strong&gt;&lt;span style="background-color:#ffffff; color:#242424"&gt;。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;自今年 2 月以來，每月新增上萬名 AI 人才將其求職狀態更新為「正在看機會」，這充分體現了 AI 人才市場的高度活躍性。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="216" src="https://oscimg.oschina.net/oscnet/up-3e5948b9574d255f2fff9c5a87be1ffdfe1.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;與此同時，企業間的「搶人大戰」已進入白熱化階段。目前已有超過&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;strong&gt;1000 家 AI 公司&lt;/strong&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;在脈脈平台發佈了 AI 相關崗位。為了吸引&lt;span&gt;頂尖&lt;/span&gt;人才，包括華為、小紅書、DeepSeek 等在內的知名企業高管也親自上陣，在個人主頁簽名中明確標註「長期招人」。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;此外，HR 和獵頭在平台上的活躍度達到「分鐘級」，AI 人才的個人主頁訪問量也因此激增，顯示出市場對 AI 人才的迫切需求。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362114</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362114</guid>
      <pubDate>Thu, 24 Jul 2025 06:53:24 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>基於 Python-use 範式的開源 Agent</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;p style="text-align:left"&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;「傳統 Agent 框架更像是用低代碼拖拽的「機器人編排器」，&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;Python-use 則是直接用 Python 把 Agent 邏輯實現出來，讓代碼就是 Agent。」&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;當大多數 Agent 框架還在把「工具」當作黑盒 API 時，知道創宇 AI 業務部總經理王利偉和其團隊在思考另一種方式——如果代碼就是工具，而 LLM 恰好擅長寫代碼，為什麼不乾脆讓 AI 自己用 Python 把任務跑出來？&lt;/p&gt; 
&lt;p&gt;在這篇訪談中，王利偉系統闡述了「Python-use 範式」——一種把 Agent 邏輯直接寫成可執行 Python 的極簡思路。它拋棄繁複的 Schema 註冊、Workflow 編排和多 Agent 協商，實現細粒度代碼控制，邏輯可控、可調試、最少 Token 浪費。&lt;/p&gt; 
&lt;p&gt;本週六，王利偉將出席【Al Agent：從工具助手到自主行動】OSC 源創會·杭州站活動，並發表《基於 Python-use 範式的開源 Agent》主題演講，介紹如何撮合 LLM ➕Python 生態形成強大的智能體，通過獨創的 Python-use 範式，讓 AI 不光會調用工具，也會自己造工具。&lt;/p&gt; 
&lt;p&gt;即刻報名：&lt;a href="https://www.oschina.net/event/8597955"&gt;https://www.oschina.net/event/8597955&lt;/a&gt;&lt;/p&gt; 
&lt;p style="text-align:center"&gt;&lt;img height="1067" src="https://oscimg.oschina.net/oscnet/up-f2c70b12c7d35ded92d220bd2c56ab5987b.png" width="800" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&lt;span style="color:#27ae60"&gt;&lt;strong&gt;問：您提出「Python-use 範式」與傳統 Agent 開發框架的核心差異是什麼？它如何解決現有 Agent 工具調用能力的侷限性？&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;答：&lt;/p&gt; 
&lt;p style="text-align:left"&gt;回答這個問題之前，我們先定義一下什麼是「工具」，眾所周知「工具」調用是 Agent 的基本能力之一。工具到底是什麼呢？是各種應用程序，接口對吧。從根本上來講都是代碼，代碼組成了 MCP 工具、API 工具以及各類應用程序。Python use 範式是迴歸第一性原理，把 code 當成工具，code 是所有工具的最基本構成，code 可以組成各種各樣的工具，而 LLM 對 code 的理解和編寫能力都足夠強，相比依賴於現成的工具，Python use 是從代碼出發，具有靈活性、擴展性。當然，在這過程 Python use 也是支持現有工具的調用的，比如 MCP、browser use 等等。而對於一些碎片化的場景，沒有標準工具、現成工具可以用的場景，Python use 可以依賴於 Python 編碼自行找到更具創造性的方案。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;一句話總結：&lt;/p&gt; 
&lt;p style="text-align:left"&gt;傳統 Agent 框架更像是用低代碼拖拽的「機器人編排器」；&lt;/p&gt; 
&lt;p style="text-align:left"&gt;Python-use 則是直接用 Python 把 Agent 邏輯實現出來，讓代碼就是 Agent&lt;/p&gt; 
&lt;div&gt; 
 &lt;table cellspacing="0" style="border-collapse:collapse; border:none; table-layout:fixed; width:500px"&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td style="border-color:#dee0e3; border-style:solid; border-width:1px; height:39px; vertical-align:top"&gt; &lt;p style="text-align:left"&gt;維度&lt;/p&gt; &lt;/td&gt; 
    &lt;td style="border-color:#dee0e3; border-style:solid; border-width:1px; height:39px; vertical-align:top"&gt; &lt;p style="text-align:left"&gt;傳統 Agent 開發框架&lt;/p&gt; &lt;/td&gt; 
    &lt;td style="border-color:#dee0e3; border-style:solid; border-width:1px; height:39px; vertical-align:top"&gt; &lt;p style="text-align:left"&gt;Python-use 範式&lt;/p&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td style="border-color:#dee0e3; border-style:solid; border-width:1px; height:39px; vertical-align:top"&gt; &lt;p style="text-align:left"&gt;任務驅動邏輯&lt;/p&gt; &lt;/td&gt; 
    &lt;td style="border-color:#dee0e3; border-style:solid; border-width:1px; height:39px; vertical-align:top"&gt; &lt;p style="text-align:left"&gt;通過「規劃 → 調度 → 工具調用 →反饋」的多層 Agent、子-Agent、workflow 實現任務拆解和執行。往往是圖狀、嵌套、多 Agent。&lt;/p&gt; &lt;/td&gt; 
    &lt;td style="border-color:#dee0e3; border-style:solid; border-width:1px; height:39px; vertical-align:top"&gt; &lt;p style="text-align:left"&gt;直接寫出「任務目標 → 代碼邏輯 → 執行」的 Python 腳本來解決任務，代碼即規劃+工具調用+執行的統一體。&lt;/p&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td style="border-color:#dee0e3; border-style:solid; border-width:1px; height:39px; vertical-align:top"&gt; &lt;p style="text-align:left"&gt;工具調用&lt;/p&gt; &lt;/td&gt; 
    &lt;td style="border-color:#dee0e3; border-style:solid; border-width:1px; height:39px; vertical-align:top"&gt; &lt;p style="text-align:left"&gt;工具通常封裝為 function calling / Tool 類、API schema，由 Agent 通過有限的模板化調用（受限於預定義接口和框架支持的函數集合）。&lt;/p&gt; &lt;/td&gt; 
    &lt;td style="border-color:#dee0e3; border-style:solid; border-width:1px; height:39px; vertical-align:top"&gt; &lt;p style="text-align:left"&gt;直接調用 Python 生態中任意庫、API、命令行、HTTP、數據庫等，甚至動態生成和運行代碼，無需提前註冊工具。&lt;/p&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td style="border-color:#dee0e3; border-style:solid; border-width:1px; height:39px; vertical-align:top"&gt; &lt;p style="text-align:left"&gt;靈活性&lt;/p&gt; &lt;/td&gt; 
    &lt;td style="border-color:#dee0e3; border-style:solid; border-width:1px; height:39px; vertical-align:top"&gt; &lt;p style="text-align:left"&gt;強調框架內一致性和安全性，但犧牲了靈活性。增加一個新工具需要寫 schema、註冊、重訓練或適配。&lt;/p&gt; &lt;/td&gt; 
    &lt;td style="border-color:#dee0e3; border-style:solid; border-width:1px; height:39px; vertical-align:top"&gt; &lt;p style="text-align:left"&gt;由於直接寫 Python 代碼，可以隨時引入任何新工具、任意組合庫、甚至嵌入 shell/JS 等。靈活性最大。&lt;/p&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td style="border-color:#dee0e3; border-style:solid; border-width:1px; height:39px; vertical-align:top"&gt; &lt;p style="text-align:left"&gt;執行粒度&lt;/p&gt; &lt;/td&gt; 
    &lt;td style="border-color:#dee0e3; border-style:solid; border-width:1px; height:39px; vertical-align:top"&gt; &lt;p style="text-align:left"&gt;依賴大量 LLM 推理+中間規劃，執行粒度粗，容易浪費 token、出錯。&lt;/p&gt; &lt;/td&gt; 
    &lt;td style="border-color:#dee0e3; border-style:solid; border-width:1px; height:39px; vertical-align:top"&gt; &lt;p style="text-align:left"&gt;細粒度代碼控制，邏輯可控、可調試、最少 token 浪費。&lt;/p&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;至於如何解決現有 Agent 工具調用能力的侷限性大體分析如下：&lt;/p&gt; 
&lt;p style="text-align:left"&gt;現有 Agent 框架在工具調用上主要有兩個侷限：&lt;/p&gt; 
&lt;p style="text-align:left"&gt;1、工具註冊繁瑣且封閉：需要開發者把工具寫成符合接口的形式並註冊進 Agent 系統。靈活性低、擴展慢。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;2、推理成本高+錯誤多：每次工具調用都可能需要 LLM 去推理哪個工具+如何填參數，容易出錯，且慢。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;Python-use 通過：&lt;/p&gt; 
&lt;p style="text-align:left"&gt;代碼即接口：不需要任何預定義 schema、function calling 註冊。Python 裏能 import / pip install 的庫、調用的 API，都是工具。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;動態生成工具：Python 裏可以即時生成函數、類、模塊，甚至臨時下載或拼接代碼然後執行，完全不受限。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;全棧生態：Python 能調用系統命令、數據庫、網絡請求、爬蟲、機器學習、雲 API… 不再被框架內置的工具集限制。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;例如：&lt;/p&gt; 
&lt;p style="text-align:left"&gt;傳統 Agent 框架裏，你要增加對某個第三方 CRM 的支持，得寫 Tool 類、註冊 schema、讓 LLM 學會調用。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;Python-use 裏，你直接用 requests 或 SDK 寫個接口調用完事。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;傳統 Agent 範式假設：&lt;/p&gt; 
&lt;p style="text-align:left"&gt;人類用自然語言説「你去幹 X」，AI 負責拆解成多步計劃+調度各種工具完成。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;Python-use 範式更像：&lt;/p&gt; 
&lt;p style="text-align:left"&gt;人類寫出一段 Python 程序告訴 AI 怎麼幹，或者 AI 直接生成出一段 Python 程序來幹。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;即：&lt;/p&gt; 
&lt;p style="text-align:left"&gt;傳統是 LLM+流程編排器+有限工具集&lt;/p&gt; 
&lt;p style="text-align:left"&gt;Python-use 是 LLM+Python 解釋器+全 Python 生態&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&lt;span style="color:#27ae60"&gt;&lt;strong&gt;問：「讓 AI 自己造工具」是演講的亮點。能否解釋 LLM 在 Python-use 範式中如何完成從「使用工具」到「生成工具」的跨越？關鍵技術難點是什麼？&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&lt;strong&gt;答：&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;使用工具其實只是一個思維方式的差別生產工具，只是一張窗戶紙，只是大家對 LLM 的理解以及應用方式的差別。使用工具 tool use 是假定要處理的任務都有各種現成的工具可以使用，Python use 一樣也具備這個能力，並不是説它就不支持現有工具的調用，Python use 認為 code is agent ，code is everything，Python 可以 use network、use computer 可以 use 各類工具，它可以 use code 去編碼、寫工具。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;在傳統 Agent 中：&lt;/p&gt; 
&lt;p style="text-align:left"&gt;LLM 能做到的通常是：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;選擇一個已有工具&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;正確填寫參數調用&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;（最多）按照文檔組合幾個已有工具完成目標&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p style="text-align:left"&gt;它的「能力邊界」被框架裏預定義的 function/schema 限死了。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;在 Python-use 中：&lt;/p&gt; 
&lt;p style="text-align:left"&gt;LLM 不光能調用庫和工具，還可以：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;根據任務需要動態生成代碼段（工具）&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;把這段代碼封裝成函數/類/模塊/腳本&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;並且可以即時運行、測試、調試它&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p style="text-align:left"&gt;也就是説，它不只是「調用工具」，它還能寫工具！&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;舉個例子：&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&lt;span style="color:#d83931"&gt;「幫我把一堆 Excel 按部門拆分成不同的 PDF 併發郵件」&lt;/span&gt;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;傳統 Agent：找不到現成的「拆 Excel 發 PDF」工具，任務失敗或需要人手擴展工具。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;Python-use：LLM 生成一個函數&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&lt;span style="color:#2ea121"&gt;def split_excel_and_send():&lt;/span&gt;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&lt;span style="color:#2ea121"&gt;# pandas, fpdf, smtplib 邏輯&lt;/span&gt;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;運行測試、修復 bug、保存。這段代碼就是一個新造出來的「工具」，下次還能直接用。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;為什麼 Python-use 能支持「造工具」？關鍵在於：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;LLM 生成的就是代碼，代碼本身就是工具&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Python 解釋器支持動態定義、動態執行、動態 import 模塊&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;全 Python 生態的庫讓「造工具」成本極低&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;人類可以隨時 review、微調、持久化新工具&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;當然，這個跨越不是輕易做到的，主要有幾個挑戰：&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&lt;strong&gt;代碼生成的正確性&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;LLM 寫出的代碼可能語法正確但邏輯錯誤&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;對外部庫版本/接口調用不熟導致出錯&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;沒有即時驗證的環境，bug 率高&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="text-align:left"&gt;&lt;strong&gt;上下文管理&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;造出來的工具需要有清晰的輸入輸出和作用域&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;如果任務複雜，代碼的組織結構（函數拆分、模塊化）很容易混亂&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="text-align:left"&gt;&lt;strong&gt;安全性&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;動態生成的代碼有潛在的安全風險（注入惡意代碼、破壞環境、泄露數據）&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;需要沙箱或審核機制&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="text-align:left"&gt;��&lt;/p&gt; 
&lt;p style="text-align:left"&gt;怎麼克服這些難點也有對應的思路和方案，比如：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;內置單元測試和驗證，讓 LLM 順便生成測試用例或自動運行測試，提高正確性。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;設計合理的 prompt 模式，指導 LLM 輸出模塊化、註釋良好、易維護的代碼。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;用虛擬環境+沙箱，讓生成和執行的代碼不破壞主環境，保障安全。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;em&gt;版本控制+註冊，把造出來的工具保存到 Git、註冊到私有 PyPI 或工具庫中。&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;總結一句話：&lt;/p&gt; 
&lt;p style="text-align:left"&gt;在 Python-use 中，LLM 不只是「選工具」，而是可以直接寫出滿足當前任務的新工具、即寫即用；&lt;/p&gt; 
&lt;p style="text-align:left"&gt;而傳統 Agent 則停留在「調用已有工具」階段，受限於框架的工具集。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&lt;span style="color:#27ae60"&gt;&lt;strong&gt;問：Python 生態有海量開源庫，但 LLM 常因依賴、環境問題調用失敗。Python-use 如何實現 LLM 與本地 Python 環境的高效安全交互？&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&lt;strong&gt;答：&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;這個問題提的非常好，確實是有各類的版本問題、兼容性、依賴關係問題等等。解決方案是它在執行任務的時候不侷限一個方案，一個不行會切換到另外的方案，大模型知道怎麼解決。如今 vibe coding 都是差不多的思路，有錯誤，再重新丟給大模型去分析提出修正就好了，直到運行成功。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;另外一個方法是，在執行任務的時候會把用戶系統相關的版本信息、環境信息做收集，發給模型和 TrusToken-也就是我們的 token 分發平台及網關，TrusToken 上會集成很多場景的「最佳實踐」形成經驗庫、知識庫，從而幫會根據用戶環境做最優匹配，可以理解是 TrusToken 上面做了很多優化。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;至於安全問題，上個問題也提到過，理論上確實存在安全風險，我們也有考慮安全模塊，也有方案，還沒來得及做。一個安全公司在做產品的時候並沒有把安全機制放在首位是有其他考慮，我們完全可以做個沙盒，但是為什麼不做沙盒，放到沙盒限制了太多功能，實質上我們電腦上大多數軟件都是運行在本機，並沒有沙盒，只有殺毒軟件才會有。理論上安全風險幹什麼都存在，與安全風險共舞，不因噎廢食。實質上，從現在幾萬註冊用戶的使用反饋來講，還沒有安全問題被提出。當然，隨着項目的成熟會把響應的機制逐漸完善，現在是有想法沒精力，從技術上來講不是不可解決的難題。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&lt;span style="color:#27ae60"&gt;&lt;strong&gt;問：在操作物聯網設備中，智能體如何統一處理不同品牌/協議設備的接口差異？是否依賴預設插件？&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&lt;strong&gt;答：&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;充分信任和利用大模型，他對現有的品牌協議他都懂，主流的接口標準、協議他都學習過的，這些知識他比人熟。如果是定製化的軟件它沒有學習過，直接寫到 API 描述裏，大模型通過 API 描述學習，當然對 api 描述就有一定的要求，實在它不懂的就給他外掛説明。AiPy 操作物聯網設備並不是依賴插件，主要是通過 API Calling ，當然有插件可以調用也是極好的，實際上我們也在準備發佈插件商城。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;提到這個問題不得不提一下我們團隊的另外一個產品 ZoomEye.org，它是全球領先的網絡空間資產測繪平台，它通過對全球 IPv4 和 IPv6 地址進行探查，能夠識別數十億聯網設備的開放端口、服務類型、協議棧、操作系統、硬件廠商、固件版本等關鍵資產信息。換句話説，ZoomEye 就像是整個網絡世界的「顯微鏡」或「地圖系統」，讓你可以一眼看清某個 IP 背後部署了哪些設備、跑着什麼服務、使用了什麼協議。它支持的協議識別範圍極廣，涵蓋操作系統、網絡設備等傳統 IT 系統、工業控制系統（如 Modbus、BACnet）、攝像頭設備（如 ONVIF）、網絡存儲（如 NAS）、IoT 中控網關、智能家居等，這些恰恰是大多數傳統 Agent 系統難以應對的「黑盒」。我們正在探索將 ZoomEye 的識別能力與 AiPy 結合：AI 可以在執行任務前，通過 ZoomEye 自動識別目標設備類型、開放接口、固件版本，進一步提高調用準確率和安全性。這種從「識別 → 理解 → 控制」的閉環，將極大提升 AI 操控物聯網設備的普適性與穩定性。現在 ZoomEye 也已經發布了 MCP 和 API，大家可以去體驗。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&lt;span style="color:#27ae60"&gt;&lt;strong&gt;問：如何吸引開發者加入 Python-use 生態？會提供哪些 SDK 或工具鏈降低接入成本？&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&lt;strong&gt;答：&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;因為項目還在初期，暫時還沒有 SDK 之類的工具，為了方便開發者調試，給大家的支持就是提供了大量 Token 進行試錯調試，默認 1000 萬 token，開發者可以憑貢獻持續兌換。我們後續會開放商城，商城裏可以發佈各種插件、成果、知識庫、角色、API、MCP 等等，開發者也可以貢獻各類插件或應用到商城，優秀的成果我們也會做一些激勵措施。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;隨着項目的推進我們會持續優化改進生態，也歡迎大家提意見。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&lt;strong&gt;問：對於想嘗試 Agent 開發的團隊，您認為切入此領域最應優先掌握的三大能力是什麼？&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&lt;strong&gt;答：&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;説實話這個問題我並不太敢回答，一是因為我們走的路和別人不一樣，二我們自己還並沒有成功，沒有資格去給別人指點什麼。只能單純的分享自己的幾個感受：&lt;/p&gt; 
&lt;p style="text-align:left"&gt;模型能力足夠強，有很大的挖掘潛力。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;以前是語料驅動模型，現在是數據驅動 Agent，對要做的場景 know how 掌握了多少是關鍵。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;不管你啥範式，啥技術，不出 1 個月時間大家都能做到，大家也看到了現在大模型之間的能力差距差別是越來越小了，技術之外的優勢可能才是競爭力。&lt;/p&gt; 
&lt;p style="text-align:center"&gt;&lt;em&gt;&lt;img height="2676" src="https://oscimg.oschina.net/oscnet/up-aeeb1d545d5e5ab92f8a3b5d959269e8da3.png" width="800" referrerpolicy="no-referrer"&gt;&lt;/em&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/4489239/blog/18685742</link>
      <guid isPermaLink="false">https://my.oschina.net/u/4489239/blog/18685742</guid>
      <pubDate>Thu, 24 Jul 2025 06:48:24 GMT</pubDate>
      <author>原創</author>
    </item>
    <item>
      <title>中國證明開放權重模型優於 GPU 算力資源</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;國外科技媒體 The Register 近日&lt;u&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.theregister.com%2F2025%2F07%2F19%2Fopenai_us_china%2F" target="_blank"&gt;發文&lt;/a&gt;&lt;/u&gt;討論了開放權重模型對 AI 技術進步的正面影響，稱中國企業通過開放分享和底層創新，比如 DeepSeek 和 Kimi 系列模型，展現了更高的效率和更強的競爭力。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0724/143507_tWgV_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;文章標題十分坦誠——&lt;em&gt;《&lt;strong&gt;China proves that open models are more effective than all the GPUs in the world&lt;/strong&gt;》&lt;/em&gt;，直接提出「&lt;strong&gt;中國證明開放權重模型比 GPU 更有效&lt;/strong&gt;」。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;核心內容&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;1. OpenAI 延遲發佈「開放權重」模型&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;自 GPT‑2 以來，OpenAI 已多年未對外開源其模型。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;原計劃在本週發佈一個社區友好型開源模型，但因安全審查推遲。CEO Sam Altman 表示，「一旦權重公佈，就無法撤回，我們必須確保萬無一失」。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2. 美國雖投資重金，但開放模型依然乏善可陳&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;美國在 GPU、計算資源上投入數百億美元，卻僅湧現出少數效率和實力不足的開源模型。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;譬如 Meta 發佈的 Llama 4&amp;nbsp;遭遇爭議與冷淡反響；微軟、IBM、谷歌亦推出體量較小、功能侷限的模型&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;3. 中國在開源領域反超&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;中國開發者不僅率先發布公開可用的大規模模型，而且算法創新表現突出。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;DeepSeek R1（DeepSeek）早在年初便問世，後來 Moonshot AI 於 7 月推出的 Kimi 2 更聲稱已實現萬億參數規模 MoE（專家專家模型），並宣佈超越包括西方頂尖私有模型在內的技術水平。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;文章強調，儘管美企掌控大量計算資源，但因開源保守與發佈緩慢，在社區驅動的模型研發上落後於中國。從戰略上看，美國若想保持 AI 領導力，除了硬件投入，更應適當開放、加快社區驅動的模型生態——否則將繼續被中國「公開優先」（open-first）的路線追趕。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362109</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362109</guid>
      <pubDate>Thu, 17 Jul 2025 06:37:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>開源鴻蒙機器人操作系統 M-Robots OS 正式開源</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;深開鴻宣佈 &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fatomgit.com%2Fm-robots" target="_blank"&gt;M-Robots&lt;/a&gt; 開源項目正式啓動。該項目由開放原子開源基金會孵化、深開鴻牽頭髮起，旨在以開源共建的方式打造基於開源鴻蒙的統一機器人操作系統 M-Robots OS，推動機器人行業生態融合、能力複用、智能協同。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;根據介紹，M-Robots OS 是全國首個基於開源鴻蒙構建的分佈式異構多機協同機器人操作系統，具備多機實時協同、多硬件形態兼容、AI 原生以及豐富 API 與開發工具鏈四大核心能力，為行業提供「底層統一、場景多元」的全棧式系統平台。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="333" src="https://oscimg.oschina.net/oscnet/up-16538ff9cdf050ddcfc6cd2069b9c349cfe.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;M-Robots OS 開源計劃將以分階段、全棧式策略推進，逐步釋放關鍵能力，推動機器人生態融合：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;2025 年 7 月 24 日：首期開源， 已上線開源鴻蒙機器人核心子系統、核心三方中間件庫、包管理器、可視化開發／調試工具；&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;2025 年 12 月：發佈公板芯片適配、專屬驅動框架、分佈式反控機制、DFX 能力；&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;2026 年 6 月：開放混合部署架構、超級設備支持、軟總線增強、融合組網技術及 AI 訓練工具；&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;2026 年 12 月：推出 M-DDS 分佈式通信框架、分佈式算力調度、多機協同支持、AI-Agent 框架與仿真工具鏈；&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;2027 年：實現基於 Agent 的羣體智能協作體系，推出統一機器人 IDE 開發環境。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;M-Robots OS 將按照「每年兩大版本」的節奏持續演進，開源範圍可能會根據技術發展、場景變化、需求優先級進行調整。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;目前，該項目已匯聚包括深開鴻、中軟國際、數字華夏、樂聚機器人、哈工大重慶研究院、北京工業大學、北京理工大學在內的 21 家「產學研用」成員單位，成立項目管理委員會（PMC）與多個 SIG 技術組，覆蓋架構、運動控制、具身智能等關鍵方向，推動跨廠商協作與產業場景落地。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362103</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362103</guid>
      <pubDate>Thu, 17 Jul 2025 06:21:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>OpenAI 應用部門迎來新任首席執行官</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Instacart 首席執行官 Fidji Simo&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Ffidjissimo%2Fstatus%2F1947341053209501716" target="_blank"&gt;宣佈&lt;/a&gt;，將在 8 月 18 日正式加入 OpenAI，並擔任新部門的 CEO。Simo 同時在 OpenAI 官網發佈了一篇&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopenai.com%2Findex%2Fai-as-the-greatest-source-of-empowerment-for-all%2F" target="_blank"&gt;深度長文&lt;/a&gt;，主要闡述了她對 AI 如何賦能和改變人類的看法。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1040" src="https://static.oschina.net/uploads/space/2025/0724/141245_kGM8_2720166.png" width="2244" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;幾周後，我將加入 OpenAI 擔任應用部門首席執行官，致力於讓 OpenAI 的技術惠及全球更多人羣。&lt;/p&gt; 
 &lt;p&gt;我一直認為自己是一名務實的技術從業者我熱愛技術，並非因其本身，而是因其能對人們的生活產生直接影響。這正是這份工作令人興奮之處，因為我相信，AI 將比歷史上任何其他技術為更多人帶來更多機遇。如果我們能正確運用 AI，它將賦予每個人前所未有的力量。&lt;/p&gt; 
 &lt;p&gt;但我也明白，這些機遇不會憑空出現。&lt;/p&gt; 
 &lt;p&gt;每一次重大的技術變革，都可能拓寬人們獲取權力的渠道這種權力包括做出更明智決策、塑造周遭世界以及以新方式掌控自身命運的能力。但與此同時，技術變革也可能導致財富和權力進一步集中在少數人手中通常是那些本就擁有金錢、資歷和人脈的人。&lt;/p&gt; 
 &lt;p&gt;因此，我們必須有意識地規劃這些技術的構建與共享方式，以確保它們能為更多人帶來更多機遇和繁榮。我們當下的選擇，將決定這場即將到來的變革會讓所有人都獲得更多賦能，還是讓少數人進一步集中財富和權力。&lt;/p&gt; 
 &lt;p&gt;我們可以從確保賦能與機遇的關鍵要素被廣泛獲取做起，這些要素包括知識、健康、創造性表達、經濟自由、時間和支持。下文將詳細闡述 AI 在改變人們生活的這些方面所具有的潛力。&lt;/p&gt; 
 &lt;p&gt;如果我們能讓智能無處不在、人人可用且通俗易懂，就能打造出世界上最強大的機遇引擎，幫助更多人過上更美好的生活。我期待與 OpenAI 才華橫溢的新同事們共同構建這樣的未來，也會在不久後分享更多內容。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;公開資料顯示，Simo 職業生涯始於 eBay，曾擔任戰略團隊成員，專注於本地商務和分類廣告項目的開發。2011 年，她加入了 Facebook，並逐步晉升為 Facebook 應用的負責人，領導包括 NewsFeed、Stories、Groups、Video、Marketplace、Gaming、News、Dating 和廣告等核心產品的開發。在她的推動下，Facebook 的視頻戰略取得了顯著進展，推出了自動播放視頻、FacebookLive 和 FacebookWatch 等功能。她還帶領團隊構建了 Facebook 的移動廣告業務，為公司的發展做出了重要貢獻。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362101</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362101</guid>
      <pubDate>Thu, 17 Jul 2025 06:14:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>預計 2029 年中國數據倉庫軟件市場規模將達 20.9 億美元</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;國際數據公司（IDC）於近日發佈了《&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmy.idc.com%2Fgetdoc.jsp%3FcontainerId%3DprCHC53700025" target="_blank"&gt;2024 年下半年中國數據倉庫軟件市場跟蹤報告&lt;/a&gt;》。IDC 數據顯示，2024 下半年中國數據倉庫軟件市場規模為 5.5 億美元，同比增長 8.7%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;其中，本地部署數據倉庫軟件規模 2.8 億美元，同比增長 6.8%；公有云數據倉庫軟件規模 2.6 億美元，同比增長 10.9%。IDC 預測， 到 2029 年，中國數據倉庫軟件市場規模將達到 20.9 億美元，2024-2029 的 5 年市場年複合增長率（CAGR）為 15.5%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="391" src="https://oscimg.oschina.net/oscnet/up-eb766774b570abb458036289ca15326fbde.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;2024 下半年，中國數據倉庫&lt;strong style="color:#01010f"&gt;本地部署模式&lt;/strong&gt;市場前五大廠商總計佔比 57.7%。出於數據安全和合規性的考慮，金融、政府、能源等行業，以及大型企更傾向於本地部署模式的數據倉庫產品。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="406" src="https://oscimg.oschina.net/oscnet/up-05482245a6774b1ed848792b16fadf3762d.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;與本地部署市場相比，&lt;strong style="color:#01010f"&gt;公有云&lt;/strong&gt;數據倉庫服務的市場集中度更高，2024 下半年，前五大廠商份額共計達到 90.2%。隨着中國泛互聯網行業和傳統企業的互聯網業務的快速發展，企業已經在公有云上積累了大量的數據，為雲上數倉的使用創造了前提和基礎。2024 年，公有云數據倉庫市場規模已超過本地部署市場，佔比 50.3%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="455" src="https://oscimg.oschina.net/oscnet/up-6402c7dfdd4a171b4ece232ca18651694c4.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong style="color:#01010f"&gt;IDC 中國企業軟件市場研究經理王楠表示&lt;/strong&gt;，存算分離架構、實時分析能力以及湖倉一體技術已經成為數據倉庫產品應具備的基礎能力，也是客戶進行數倉產品選型時考察和評估的重點。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;擁抱生成式 AI 和大模型已經成為數據倉庫下一步產品能力升級的核心，在 AI for DB 層面實現自然語言交互式查詢、智能調優、智能診斷等能力，使數倉產品的使用和運維更加便捷；在 DB for AI 層面支撐向量引擎、庫內機器學習能力，實現正真的智能問數 AI 加持下的數倉產品將使企業的數據分析能力進一步提高， 獲得更精確的預測和洞察能力。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362100</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362100</guid>
      <pubDate>Thu, 17 Jul 2025 06:11:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>字節發佈端到端同聲傳譯模型 Seed LiveInterpret 2.0</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;字節跳動 Seed 團隊宣佈正式推出端到端同聲傳譯模型 Seed LiveInterpret 2.0 —— 首個延遲&amp;amp;準確率接近人類水平的產品級中英語音同傳系統，在中英同傳翻譯質量達到業界 SOTA 的同時，實現了極低的語音延遲水平。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;公告稱，Seed LiveInterpret 2.0 基於全雙工端到端語音生成理解框架，支持中英互譯，可實時處理多人語音輸入，像人類同傳譯員一樣以極低的延遲 「邊聽邊説」，一邊接收源語言語音輸入，一邊直接輸出目標語言的翻譯語音。同時，Seed LiveInterpret 2.0 還支持 0 樣本聲音復刻，讓溝通更加流暢自然。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在測試中，Seed LiveInterpret 2.0 面對 40 秒的大段中文表達，能夠低延遲地絲滑輸出同款音色的英語翻譯。Seed LiveInterpret 2.0 還能快速學習音色，即便此前未「聽」過角色的聲音，依然能通過實時交互進行現場演繹。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;相比傳統機器同傳系統，Seed LiveInterpret 2.0 模型具備以下優勢：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;接近真人同傳的翻譯準確率&lt;/strong&gt;&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;精準的語音理解能力保障了翻譯準確度，在多人會議等複雜場景中英雙向翻譯準確率超 70%，單人演講翻譯準確率超 80%，接近真人專業同傳水平。&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;&lt;strong&gt;極低延遲的 「邊聽邊説」 能力&lt;/strong&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;採用全雙工語音理解生成框架，翻譯延遲可低至 2-3 秒，較傳統機器同傳系統降低超 60%，實現了真正的 「邊聽邊説」 翻譯。&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;&lt;strong&gt;零樣本聲音復刻，音色真實自然&lt;/strong&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;只需採樣實時語音信號，便能提取聲音特徵，用説話人的音色特質實時 「説出」 外語，提升交流的沉浸感和親和力。&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;&lt;strong&gt;智能平衡翻譯質量、延遲和語音輸出節奏&lt;/strong&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;可根據語音清晰度、流暢度、複雜程度，調整輸出節奏，並適配不同語言特性。面對超長信息，依然能保證傳譯語音節奏的自然流暢。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;目前，Seed LiveInterpret 2.0 技術報告已公佈，模型基於火山引擎對外開放。此外，Ola Friend 耳機也將在 8 月底接入 Seed LiveInterpret 2.0，成為首個支持該模型的智能硬件設備。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:8px; margin-right:8px; text-align:justify"&gt;&lt;span style="color:#000000"&gt;評測結果顯示，在語音到文本的同傳任務中，Seed LiveInterpret 2.0 中英互譯平均翻譯質量的人類評分達到 74.8（滿分 100，評估譯文準確率），較排名第二的基準系統（47.3 分）超出 58%。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:8px; margin-right:8px; text-align:justify"&gt;&lt;span style="color:#000000"&gt;在語音到語音中英同傳任務中，僅 3 個測評的翻譯系統支持該能力，其中 Seed LiveInterpret 2.0 中英互譯平均翻譯質量達到 66.3 分（滿分 100，除評估譯文準確率，還評估語音輸出時延、語速、發音、流暢性等指標），遠超其他基準系統，&lt;strong&gt;達到接近專業真人同傳的水平&lt;/strong&gt;。同時，大部分基準系統也不支持聲音復刻功能。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:8px; margin-right:8px; text-align:justify"&gt;&lt;span style="color:#000000"&gt;&lt;img height="367" src="https://oscimg.oschina.net/oscnet/up-5db0506a8b7711255ee86d2bb6986dc7f78.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:8px; margin-right:8px; text-align:justify"&gt;&lt;span style="color:#000000"&gt;&lt;img height="360" src="https://oscimg.oschina.net/oscnet/up-03b5d4e3cac39425b43d1c9044266bcc0e7.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:8px; margin-right:8px; text-align:justify"&gt;&lt;span style="color:#000000"&gt;在延遲表現上，Seed LiveInterpret 2.0 在語音到文本場景中，&lt;strong style="color:rgba(0, 0, 0, 0.9)"&gt;輸出首字平均延遲僅 2.21 秒，在語音到語音場景中，輸出延時僅 2.53 秒，做到了對翻譯質量以及時延的均衡。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:8px; margin-right:8px; text-align:justify"&gt;&lt;span style="color:#000000"&gt;&lt;img height="255" src="https://oscimg.oschina.net/oscnet/up-86b7ef584ede587d9df7ad36aafec6d1ab2.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:8px; margin-right:8px; text-align:justify"&gt;&lt;span style="color:#000000"&gt;&lt;img height="260" src="https://oscimg.oschina.net/oscnet/up-8ec35eff6c30fc2e8e276f0f36159d3cf7d.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:8px; margin-right:8px; text-align:justify"&gt;&lt;span style="color:#000000"&gt;不過，字節方面也坦承儘管 Seed LiveInterpret 2.0 已初步展現出一定優勢，&lt;strong style="color:rgba(0, 0, 0, 0.9)"&gt;其邊界仍有拓展空間&lt;/strong&gt;。比如，在語言覆蓋方面，目前模型主要支持中英互譯，其他語種尚未較好支持。此外，其聲音復刻的穩定性、語音表現力、情緒復刻能力、極複雜情況下的翻譯準確性等仍有進步空間。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:8px; margin-right:8px; text-align:justify"&gt;&lt;span style="color:#000000"&gt;更多詳情可&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fvjq_cwneALGoPf6RgxwuLQ" target="_blank"&gt;查看官方公告&lt;/a&gt;。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362097</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362097</guid>
      <pubDate>Thu, 17 Jul 2025 05:55:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>智能體時代，如何避免大廠壟斷 AI ？</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;「過去十幾年裏，&lt;strong&gt;互聯網的開放性正在逐步消失。&lt;/strong&gt;越來越多的服務、數據、用戶，被鎖定在幾個大型平台的生態裏。協議的邊界被平台所取代，數據也變成了平台資產而不是網絡資源。&lt;strong&gt;我們不希望智能體時代重複這一切。&lt;/strong&gt;」&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;近日，ANP 開源技術社區發起人常高偉在接受國際著名科學雜誌《New Scientist》採訪時指出了當前互聯網的封閉性，並擔心智能體時代將會重蹈覆轍。&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;目前來看，大部分 AI 協議都是由大型科技公司提出的，比如 Anthropic、Google 這些企業，他們推動了 MCP、A2A 等協議的發展，也讓更多的人看到協議對智能體的價值。但這些協議的設計，很多時候是基於他們自己的產品路徑和商業利益出發的。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;常高偉認為，這本身沒有錯——商業公司有自己的考量和節奏。「但問題是，如果整個智能體互聯網的底層協議都由幾家公司來主導，那我們可能會再次走上平台封閉化的老路。&lt;strong&gt;就像今天的社交平台、應用商店、廣告系統，數據和權限越來越集中在少數大公司手裏。&lt;/strong&gt;」&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;要打破這種封閉性困局，常高偉等人於 2024 年 4 月開源的 &lt;strong&gt;ANP（Agent Network Protocol）&lt;/strong&gt; 提供了新路徑。與 Anthropic 主導的 MCP、Google 推動的 A2A 不同的是，&lt;span&gt;ANP 從一開始就關注智能體之間的身份認證問題。這樣一來，任何兩個智能體——不管是誰開發的，來自哪家公司，都能通過標準協議完成安全的雙向身份認證。&lt;/span&gt;這一設計使 ANP 成為首個真正面向開放互聯網的智能體協議。&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;更重要的是，ANP 由全球開發者社區共建&lt;/strong&gt;。其發起團隊明確強調：這是一個&lt;strong&gt;不追求盈利的非商業化組織&lt;/strong&gt;，成員包含極客、學者與創業者。常高偉表示，AI 不應該被壟斷，它的連接能力、協作能力，應該像空氣和水一樣，向所有人開放。ANP&amp;nbsp;通過完全開源和去中心化架構，讓智能體間的協作迴歸以協議為中心的開放連接，打破平台封閉化的老路和數據孤島，確保連接權回到每一個人手裏，&lt;strong&gt;讓互聯網重新成為創造力的土壤，而不是流量的圍牆。&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;為了在全球範圍內推動智能體協議標準化的共識與合作，&lt;/span&gt;ANP&amp;nbsp;&lt;span&gt;開源技術社區牽頭在 W3C 發起了 "AI Agent Protocol" 社區組（Community Group）。W3C 一直是互聯網協議發展的重要推動者，從 HTTP 到 HTML，它見證並塑造了多個開放技術的誕生。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#2980b9"&gt;&lt;strong&gt;以下為《New Scientist》雜誌採訪常高偉全文：&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;New Scientist：Why are protocols important to enable agentic AI?（為什麼協議對實現 Agentic AI 至關重要？）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;常高偉&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：想象一下，如果你讓一個 AI 去使用 Excel 表格、點開網頁、登錄郵箱，才能獲取信息，它要麼得模仿人類的鼠標操作，要麼得反覆破解界面背後的邏輯。這種方式其實非常不自然——&lt;/span&gt;&lt;strong&gt;&lt;span&gt;AI 並不擅長使用為"人類"設計的軟件，它更擅長的是直接處理數據&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;從這個角度看，我們其實應該反過來思考：讓數據為 AI 所用，而不是讓 AI 學着像人那樣"用工具"。這就需要一種標準方式，把數據、身份、能力、安全都打包好，直接交給智能體使用。這種"標準方式"，就是&lt;/span&gt;&lt;strong&gt;&lt;span&gt;協議&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;。協議是承載數據，最好的容器。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;我認為這才是協議為什麼對 AI 如此重要的最根本的原因。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;但協議的重要性不單單體現在讓 AI 與數字世界交互，更重要的是，它會推動 Agentic web 的到來。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;Agentic Web&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;，我們可以把它簡單理解為"&lt;/span&gt;&lt;strong&gt;&lt;span&gt;為 AI 而設計的下一代互聯網&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;"。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;在今天的 Web 世界裏，網頁是給人看的，數據往往被封裝在前端頁面中，只有人類點擊、滑動、輸入後，背後的系統才會做出響應。這種設計模式是典型的"人機交互優先"。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;但 Agentic Web 的出發點不同：它是為 AI 與 AI 的交互而構建的。在 Agentic Web 中，智能體將成為第一公民——他們是互聯網中最重要的參與者，智能體之間相互協作，幫助人類完成繁瑣、複雜的任務。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;支撐這種協作的關鍵工具，正是智能體協議。協議將成為 Agentic Web 的基礎設施，它不僅定義了身份、通信、能力調用等核心機制，還讓來自不同平台、不同組織、不同個人的智能體能夠自由連接與協作。無論一個智能體屬於哪個公司或個人，只要遵循相同的協議，它就能融入這個新型網絡。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;這將徹底改變現有互聯網的格局。Agentic Web 有潛力打破今天由平台主導的數據孤島，實現真正開放、互聯、去中心化的網絡結構，為下一代互聯網打開新的可能性。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;New Scientist：Who has been developing these protocols so far?（到目前為止，這些協議都是由誰在開發的？）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;常高偉&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：目前全球有十幾個智能體相關的協議項目，有些是由科技巨頭主導，有些是由開源社區或小公司推動。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;第一個是 MCP（Model Context Protocol），這個是由 Anthropic 推動的。Anthropic 是 OpenAI 的主要對手之一，他們覺得大模型光靠預訓練是不夠的，還得"實時連上工具"，才能真正解決問題。MCP 就像是一個標準接口，讓模型可以安全、統一地調用外部系統，比如搜索、數據庫、插件等。現在包括微軟、OpenAI、谷歌、亞馬遜等都在支持這個協議。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;第二個是 A2A（Agent-to-Agent Protocol），這是 Google 推出來的，重點不是模型和工具的連接，而是"智能體和智能體之間怎麼説話"。比如一個智能體説"我不會訂機票"，另一個説"我來幫你"，A2A 定義了這背後的語言和流程。目前還在早期階段，但被不少開發者看好，尤其適合企業內部多個 AI 系統之間的互聯。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;第三個是 ANP（Agent Network Protocol），這是由我們開源社區發起的項目，也是目前全球最早關注"去中心化智能體通信"的協議，我們研究這個領域比 MCP 和 A2A 更早。我們希望構建一個安全、高效、開放的智能體互聯網，在這個網絡中，所有的智能體都將不受大型互聯網平台的限制，相互之間都可以進行通信與協作。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;MCP 關注的是模型如何連接到工具和資源，A2A 解決的問題是智能體如何在企業內部進行連接與協作，ANP 解決的問題是智能體在互聯網上如何進行連接與協作。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;除此之外，還有其他一些協議，比如 Cisco 旗下的 AGNTCY 社區主導的 Agent Connect Protocol，IBM Research 主導的 Agent Communication Protocol，以及一些研究機構的項目比如 agora protocol，他們都有不同的技術路線。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;我這裏有幾篇智能體協議相關的 paper： &lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;https://arxiv.org/abs/2504.16736 &lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;https://arxiv.org/pdf/2505.07176v1 &lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;https://arxiv.org/pdf/2505.02279v1&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;New Scientist：What are the issues with them and why do we need this version?（它們存在哪些問題？為什麼我們需要這個版本？）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;常高偉&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：其實我們在 2024 年 4 月就啓動了 ANP 這個開源項目，那個時候還沒有 MCP，也沒有 A2A。我們是最早一批真正從"智能體協作"角度出發，來思考協議應該怎麼設計的團隊。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;當時我們就有一個很強的直覺：協議會是智能體之間協作的關鍵基礎設施。於是我們去研究了很多現有協議，包括 HTTP/HTML，發現它們本質上都是為"人-網頁"交互設計的，不適用於"智能體-智能體"的通信。比如説：&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;兩個智能體怎麼發現彼此？&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span&gt;如何互相認證身份、交換數據？&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span&gt;通信過程中如何保證安全性和隱私？&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span&gt;現有的協議在這些方面幾乎是空白的。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;帶着這些問題，我們設計了 ANP。它從一開始就關注智能體之間的身份認證問題，我們希望任何兩個智能體，不管是誰開發的，來自哪家公司，都能通過標準協議完成安全的雙向身份認證。這一點，其實是我們和 MCP、A2A 最大的差異之一。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;我們常常用"email 模式"來比喻 ANP 的身份體系：只要你有一個"智能體地址"，你就能跟全世界的智能體建立聯繫。這跟 MCP、 A2A 那種比較中心化方式不太一樣，我認為 MCP 和 A2A 其實並沒有很好的解決智能體的身份問題，特別是智能體在互聯網上的身份問題。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;在架構層面上，ANP 和 MCP 也有很大的區別。MCP 是典型的客戶端-服務器架構（Client-Server），也就是説智能體要主動連接服務端，服務端是不能主動發起連接的。它更像一種"單向調用"。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;&lt;span&gt;而 ANP 是一個真正的點對點架構（&lt;/span&gt;&lt;span&gt;Peer-to-Peer&lt;/span&gt;&lt;span&gt;），任何兩個智能體之間都可以對等地通信、交互。這種設計更符合未來智能體之間頻繁互動、協同執行任務的需求。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;至於和 A2A 的差異，我們認為最重要的一點是：A2A 是基於任務傳遞的協作機制。一個智能體把一個"任務包"交給另一個智能體去執行。這種模式在企業內部還好，但放到開放的互聯網環境中就會遇到隱私和權限的問題。比如説，我想訂酒店，用 A2A 的方式，我可能要在任務中告訴對方智能體我喜歡什麼、不喜歡什麼，這種個人偏好數據一旦傳出去，就存在泄露的風險。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;而 ANP 在設計之初就考慮了"智能體在互聯網中協作"這一複雜現實場景，ANP 採用的是一種 Linked-data 的方案，可以將智能體對外公開信息編織成一個數據網絡，一個智能體可以像爬蟲一樣將另外一個智能體的信息爬取下來，然後在本地進行分析與決策，從而避免用戶的隱私泄漏。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;總體來説，&lt;/span&gt;&lt;strong&gt;&lt;span&gt;ANP 是一個非常有創造力、非常獨特的嘗試，它不是對現有協議的小修小補，而是從底層重新出發，真正為"智能體互聯網"準備的協議。&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;New Scientist：How important is it that we have protocols developed outside of big tech companies?（由大科技公司之外的組織制定協議有多重要？）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;常高偉&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：這是一個非常關鍵的問題。我們現在看到的很多 AI 協議，確實是由大型科技公司提出的，比如 Anthropic、Google 這些企業，他們推動了 MCP、A2A 等協議的發展，也讓更多的人看到協議對智能體的價值。但我們也需要看到，這些協議的設計，很多時候是基於他們自己的產品路徑和商業利益出發的。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;這本身沒有錯——商業公司有自己的考量和節奏。但問題是，如果整個智能體互聯網的底層協議都由幾家公司來主導，那我們可能會再次走上"平台封閉化"的老路。就像今天的社交平台、應用商店、廣告系統，數據和權限越來越集中在少數大公司手裏。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;我們不希望"智能體互聯網"成為另一個"數據孤島聯盟"。如果我們真的相信 AI 是一項改變人類社會的重要技術，那就更需要有一個開放、中立的社區來推動協議的設計，確保它的未來是屬於每個人的，而不是某幾家公司的。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;這也是我們發起 ANP 開源社區的初衷，我們有自己的理念，我們希望自己的理念能夠實現。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;ANP 社區非常特別，它不是一個傳統意義上的商業團隊，它完全是一個不追求盈利的非商業化組織。我們來自各個方向——有極客、有創業者、有學者，大部分都是對未來充滿熱情的理想主義者。大家聚在一起，是因為共同相信：AI 不應該被壟斷，它的連接能力、協作能力，應該像空氣和水一樣，向所有人開放。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;同時我們也意識到，光靠一個社區的努力是不夠的，還需要在全球範圍內推動標準化的共識與合作。這也是為什麼我們牽頭在 W3C 發起了 "AI Agent Protocol" 社區組（Community Group）。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;W3C 一直是互聯網協議發展的重要推動者，從 HTTP 到 HTML，它見證並塑造了多個開放技術的誕生。它是一箇中立、開放、面向全球的標準化組織，不屬於任何一家公司，也不服務於某個商業利益集團。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;我們相信，在 W3C 這樣的國際平台上推動 Agent 協議的討論，有助於吸引全球更多開發者、研究者、公司和組織共同參與，真正形成一個開放、協作、可信的技術生態。這也與我們在 ANP 社區裏的初心是一致的：協議不應該屬於某家公司，它應該屬於整個智能體社會。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;New Scientist：What do you hope the protocol will provide?（你希望這個協議能帶來什麼？）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;常高偉&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：這個問題其實也是我們做這個協議的初衷。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;我們一直堅信一個非常古老但至今依然重要的互聯網信條：連接即權力（Connection is Power）。只要一個人能夠自由地連接到工具、連接到他人、連接到信息，他就具備改變世界的能力。這就是互聯網最初帶給我們的力量——讓一個普通人也能撬動整個系統。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;但我們也看到，過去十幾年裏，互聯網的"開放性"正在逐步喪失。越來越多的服務、數據、用戶，被鎖定在幾個大型平台的生態裏。協議的邊界被平台所取代，數據也變成了"平台資產"而不是"網絡資源"。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;我們不希望智能體時代重複這一切。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;ANP 想要實現的，是讓未來的智能體互聯網，從以平台為中心的封閉生態，迴歸到"以協議為中心"的開放連接。它不依賴某個平台，不綁定某個技術棧，只要你遵循這個協議，無論你是誰、你在哪、你由誰開發，你的智能體都能被識別、被發現、被調用，真正融入這個網絡。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;在這樣的網絡裏，智能體不僅是信息的接收者，還是服務的提供者；不是被平台分發的"插件"，而是彼此對等的節點，可以自由協作、交易、共享能力。這意味着任何一個開發者，只要有想法和能力，就可以進入這個生態，而不必依賴大公司賦權。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;這也是我們特別在意"非封閉、非壟斷"的原因。我們希望 AI 技術的紅利能夠真正普惠全球，而不是被少數平台控制。我們也相信，一個真正開放的智能體互聯網，會比封閉平台更有活力，會誕生出更多天馬行空的創意與合作。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;所以，如果你問我，我們希望 ANP 這個協議帶來什麼？&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;那就是：讓智能體的連接權利回到每一個人手裏，讓互聯網重新成為創造力的土壤，而不是流量的圍牆。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;New Scientist：What's the next step? Will the W3C choose a "winner"? How long does this process usually take?（下一步是什麼？W3C 會選出一個"勝出者"嗎？這個過程通常需要多久？）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;常高偉&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：感謝這個很好的問題。如果您指的是我們在 W3C 的下一步工作，我需要坦誠地説，我們之前主要關注 W3C 相關的技術標準，但並沒有深度參與到標準制定的具體流程中。因此，對於標準制定的週期以及一些未知問題的解決時間，我們目前還不能給出確切的判斷。但我們希望能在 W3C 這個平台上全力推進標準的制定和行業共識的形成。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;關於"勝出者"這個話題，我想澄清一點：W3C 是一個致力於制定開放標準（Royalty-free）的組織，標準的制定是建立在整個行業共識基礎上的。我們選擇來到這裏，正是看重這一點。我們的目標是做好一個智能體交互的協議標準，我們希望聽取更多意見來打磨一個優秀的行業協議，並不希望也不打算與誰競爭。實際上，在我們創立社區組之初，W3C 智能體相關的工作組已經發出了聯絡邀請，希望共同協作，這也正是我們期望看到的。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;New Scientist：You are both proposing the protocol and serving as the group chair—doesn't that pose a conflict of interest? （你本人既提出協議，又擔任小組主席，這是否存在利益衝突？）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;常高偉&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：這是一個很重要的問題。首先，我想強調 ANP 社區是一個開放中立、非盈利性的社區， ANP 協議，本身是完全開源的，我們非常願意與其他協議項目共同探索，最終落地成為一個具有行業共識的標準協議。從我們開始探索協議設計之初，我們就希望能夠聽取最廣泛的意見。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;擔任小組主席的角色與我們 ANP 以及 W3C 的願景是完全一致的。我們希望匯聚來自不同行業、不同國家的聲音，共同推進一個適合全人類的、具有共識基礎的標準。從這個角度看，不僅不存在利益衝突，更準確地説，我們實際上並沒有什麼商業利益考量——無論是 W3C 社區組還是 ANP 都是如此。如果非要説有什麼"利益"的話，那就是我們希望實現 Agentic Web 這一技術願景。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;W3C 有着"Web for All"的願景，我們也願意在這一願景基礎上推進 Agentic Web 的實現，所以這些目標之間並不衝突，而是相互促進的。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;New Scientist：What happens if the big tech companies ultimately decide to adopt their own protocols? （如果大型科技公司最終決定採用自己的協議，會發生什麼？）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;常高偉&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：如果大型科技公司願意採用我們的協議，那當然是非常好的結果。我們對自己的協議方案很有信心，相信它能夠解決他們在智能體互聯和協作方面遇到的關鍵問題。我們也非常願意配合大公司來推進協議的實際落地應用。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;當然，我們更歡迎大公司能夠參與到我們協議的制定和優化過程中來。我們是開源開放的，我們的最終目的是實現 Agentic Web 這一願景，而不是推廣某一份特定的協議或標準。如果通過開放合作能夠產生更好的解決方案，我們完全支持這樣的結果。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;歸根結底，我們關注的是整個智能體生態的健康發展，而不是某個特定協議的"勝負"。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;New Scientist：Can we provide some examples of successful or failed standardization efforts led by the W3C in recent years? （我們能否提供一些近年來 W3C 推行標準時成功或失敗的案例？）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;常高偉&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：正如我在第一個問題中提到的，這是我第一次深度參與 W3C 的標準制定工作，這個問題可能需要站在 W3C 工作人員的角度來回答，我很難給出權威的答案。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;但從我個人的理解來看：正如我們選擇 W3C 的原因一樣，這裏是一個開放的標準組織。如果某個領域存在真實需求，自然會有相關的羣體來制定或推進相應的標準。所以在我的概念裏，這個過程不存在簡單的"成功"或"失敗"。如果一份標準被某個社羣制定出來，那這份標準對他們來説就是有意義和價值的；如果一份標準後來被其他標準所替代，這説明技術的變革和迭代在發生，而這本身就是技術發展中一直在發生的自然過程。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:0px; margin-right:0px; text-align:left"&gt;&lt;span&gt;重要的是保持開放的心態，&lt;/span&gt;&lt;strong&gt;&lt;span&gt;讓最好的技術方案在公開、透明的環境中得到充分討論和驗證。&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/3859945/blog/18685716</link>
      <guid isPermaLink="false">https://my.oschina.net/u/3859945/blog/18685716</guid>
      <pubDate>Thu, 17 Jul 2025 04:38:00 GMT</pubDate>
      <author>原創</author>
    </item>
    <item>
      <title>浙江文交所與鯨探攜手「科技+文化」 實現文化數字資產跨平台「秒轉」流通</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span&gt;7 月 7 日，浙江文化產權交易所（以下簡稱「浙江文交所」&lt;/span&gt;&lt;span&gt;）與螞蟻集團旗下鯨探科技（上海）有限公司（以下簡稱「鯨探科技」）在浙江文化大廈舉行了數字資產跨系統轉移啓動儀式，標誌着數字資產在跨平台之間的「秒轉」，從而實現了浙江文交所與鯨探科技在數字資產的跨平台便捷交易的領域合作更進一步。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//4bce8fe9605b497d2c31027b8c9d4961.jpeg" referrerpolicy="no-referrer"&gt;&lt;br&gt; 左二，螞蟻集團鯨探首席運營官，陳萌&lt;/p&gt; 
&lt;p&gt;右二，浙江文交所董事長、總經理，林文火&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;「秒轉」功能的開通,使得用戶能夠在「鯨探」和浙江文交所「樂數通」平台之間通過一鍵式操作實現文化數字資產在數文鏈和螞蟻鏈之間的相互轉換,創新「瞬時交割、一鍵完成」的數字資產交易模式，實現文化數字資產交易跨平台的全流程智能化貫通，有效推動文化數字資產廣泛流通，促進交易市場繁榮，提升廣大用戶的交易體驗。&lt;/p&gt; 
&lt;p&gt;此前，浙江文交所已攜手「鯨探」推出了杭州國家版本館館藏的《西湖：雙峯插雲》，國畫大師戴敦邦的《白蛇乙巳風華》、《水滸傳人物譜》、全國美展獲獎藝術家洪萬裏《水滸武松》等多款重磅級文化數字資產，深受用戶的喜愛與追捧。此次「秒轉」首批面向其他發行方已在鯨探上發行的《憤怒小魚竿》《響堂山石窟》《雙頭人面蛇身俑》《小幽靈》《於楓》等權益類文化數字資產，後續浙江文交所將為鯨探平台上的優質文化數字資產交易流通提供便捷服務。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;為慶祝此次「秒轉」功能的開通，當天首款擁有該功能的數字資產《東坡行旅·石刻循跡》於「鯨探」平台同步發售，發行數量共 12000 份。該數字資產由浙江文交所和杭州西湖風景名勝區資產經營集團聯合發行，以蘇東坡「四大名碑之首」的《表忠觀碑》為載體，採用數字盲盒形式，疊加文旅權益，旨在通過「數實融合」，以數字化手段重構東坡文化傳播範式，將傳統文化、數字資產、特色文創和文旅產業緊密結合在一起，為公眾帶來全新的、更具創意的互動體驗。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//4929406ab91bbe87409cd8d99e465de8.jpeg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;該功能的上線不僅是浙江文交所與鯨探科技的又一次密切合作，更是積極推動文化數字資產交易試點、貫徹落實省委省政府高質量文化強省建設的重要舉措和落地實踐，為後續打造一個科技創新、產業協同的「文化數字資產＋文旅行業應用」生態體系奠定了堅實的技術基礎，將有力推動中華優秀傳統文化創造性轉化和創新性發展。&lt;/p&gt; 
&lt;p&gt;未來，浙江文交所將進一步深化與鯨探科技的合作，攜手發掘優秀傳統文化，打造高質量 IP 內容，並始終堅持用戶至上的原則，持續完善平台建設，創新技術服務，為廣大用戶提供公平公正、公開透明的交易環境，充分發揮行業「領頭羊」作用，促進文化數字資產交易市場有序、規範發展，助力中華文化全景呈現、中華文化數字化成果全民共享。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362077</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362077</guid>
      <pubDate>Thu, 17 Jul 2025 04:06:00 GMT</pubDate>
      <author>作者: 開源科技</author>
    </item>
    <item>
      <title>谷歌推出能分析古代文本的 AI 模型 Aeneas</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;谷歌 DeepMind 推出了新 AI 模型 Aeneas，旨在幫助歷史學家更好地理解古代文本，其能夠對公元前 7 世紀至公元 8 世紀的拉丁銘文進行分析。這是第一個專門為古代銘文提供上下文解讀的人工智能工具。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="280" src="https://oscimg.oschina.net/oscnet/up-869ea2ab4dcdab6c7ca5844886c3d70e2f2.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;傳統上，歷史學家依賴自身的專業知識和資源來尋找文本之間的相似性，即所謂的 「平行文本」。而 Aeneas 則通過處理數以千計的拉丁銘文，將這一過程大大加速，能在幾秒鐘內提供相關的文本和上下文平行例證，從而幫助歷史學家進行更深入的解讀和研究。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;該模型不僅限於拉丁文，還可以擴展到其他古代語言、文字和材料，如紙莎草文和貨幣，進一步豐富了歷史研究的可能性。Aeneas 的多模態輸入能力，意味着它能夠同時處理文本和圖像信息，從而提高對銘文的地理來源的判斷。這一工具的先進性體現在能夠恢復長度不確定的文本缺口，和在對歷史文本的恢復和預測方面設立了新的基準。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;為了訓練 Aeneas，研究團隊整合了來自多個歷史數據庫的數據，包括羅馬銘文數據庫、海德堡銘文數據庫等，創建了一個包含超過 176000 條古羅馬銘文的拉丁銘文數據集。通過這種方式，Aeneas 能夠有效識別並對銘文進行分類，為歷史學家的研究提供了強有力的支持。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在一項評估中，23 位歷史學家參與了使用 Aeneas 進行銘文的恢復、鑑定和年代定位的研究。結果顯示，當歷史學家結合 Aeneas 提供的上下文信息與模型的預測時，取得了最佳的研究成果。許多參與者表示，Aeneas 加速了他們的工作流程並提高了對複雜銘文任務的信心。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362067</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362067</guid>
      <pubDate>Thu, 17 Jul 2025 03:24:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>2025 開放原子開源生態大會在北京開幕</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;7 月 23 日，&lt;strong&gt;以「開源賦能產業，生態共築未來」為主題的 2025 開放原子開源生態大會在北京開幕&lt;/strong&gt;。大會開幕式上，北京國際開源社區「開源先鋒企業」引領啓航，「源創部落」正式啓動建設；多個開源成果現場發佈。工業和信息化部副部長熊繼軍、北京市人民政府副祕書長許心超出席大會並致辭。北京經濟技術開發區工委副書記、管委會主任王磊出席大會開幕式並參與相關發佈儀式。&lt;/p&gt; 
&lt;p&gt;&lt;img height="720" src="https://static.oschina.net/uploads/space/2025/0724/105745_8YjD_2720166.png" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;「2025 年首批開源先鋒企業」授牌儀式。&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;北京國際開源社區經過兩年建設取得積極進展，在開源基礎設施建設、開發者服務、開源項目孵化運營方面持續突破，逐步構建起多維協同的開源生態。&lt;/strong&gt;一批優秀企業入駐，依託「開源+」模式驅動產業創新融合，為社區開源生態建設注入先鋒力量。大會上，工業和信息化部信息技術發展司司長王彥青，北京經濟技術開發區工委副書記、管委會主任王磊，北京市經濟和信息化局副局長顧瑾栩，開放原子開源基金會理事長程曉明為「2025 年首批開源先鋒企業」授牌。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0724/105836_QVwf_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;「源創部落」啓動儀式。&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;現場，&lt;strong&gt;「源創部落」建設同步啓動。&lt;/strong&gt;作為面向開源中小企業及初創團隊的實體孵化平台，「源創部落」將提供「項目孵化、初創扶持、社區賦能、生態共建」四大能力，降低開源創新創業門檻，釋放開源技術商業化潛力，推動北京國際開源社區向「實體空間+生態賦能」雙輪驅動的 2.0 階段升級，加速構建吸引開源企業「近悅遠來」的「強磁場」。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0724/105948_g9xR_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;開源項目捐贈儀式。&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;開放原子開源基金會作為科技公益性服務機構，始終致力於打造以開發者為本的開源生態。&lt;/strong&gt;現場，開放原子開源基金會與 Mobius 大模型、OpenLoong、openDACS、IvorySQL、LLMOne、OpenIBC 增強型區塊鏈平台、Codeya IDE 和 GeniusAI 算法研發平台完成捐贈簽約，涵蓋人工智能、具身智能、基礎軟件、區塊鏈等多個關鍵技術領域，極大豐富和完善了開源生態，實現「工具-技術-場景」閉環。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0724/110007_9U1J_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;開源項目應用案例發佈儀式。&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;從代碼迭代到產業落地，從技術共享到生態共建，開源的力量正賦能千行百業。&lt;strong&gt;會上發佈了 100 餘家單位的 150 餘個開源項目應用案例&lt;/strong&gt;，案例覆蓋電力、通信、醫療、教育、金融、交通等 10 餘個關係國計民生的關鍵行業，彰顯了開源技術在促進產業升級、行業創新方面的強大動力。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0724/110036_jKd3_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;開放原子電鴻開源社區啓動儀式。&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0724/110057_9LJz_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;開放原子旋武開源社區啓動儀式。&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;開源的蓬勃發展，離不開產業界協同共建。會上舉行了電鴻開源·旋武聚力——社區啓動儀式。&lt;strong&gt;開放原子電鴻開源社區&lt;/strong&gt;作為基金會首個行業應用社區，聚焦全產業鏈協同與行業場景深度融合，聚力構建服務新型工業化、綠色化、智能化轉型的核心工業底座。&lt;strong&gt;開放原子旋武開源社區&lt;/strong&gt;的核心使命是集合社區力量，推動 Rust 普及應用、培養匯聚專業人才、促進生態系統建設，進而強化數字基座安全性。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0724/110116_j7go_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;第三屆開放原子大賽啓動儀式。&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;開放原子大賽着眼於解決「真問題」，形成了多項具有應用價值的技術成果。&lt;strong&gt;現場第三屆開放原子大賽正式啓動，首批 12 個賽項同步發佈。&lt;/strong&gt;大賽重點覆蓋基礎軟件、工業軟件、人工智能等領域，設置巔峯挑戰賽、實戰競技賽、訓練學習賽等多種類型，總獎金 1500 萬元，致力於匯聚全球開源智慧，破解產業發展中的核心技術難題，加速創新成果轉化與高水平開源人才培養。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0724/110142_tXmN_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;會上，&lt;strong&gt;多位專家圍繞 2024 年度我國開源發展整體態勢、近三年來國內外開源項目與開發者數據&lt;/strong&gt;，分別從開源許可證、代碼託管平台、地方開源產業佈局、重點技術領域、行業應用、開源安全、開源教育和開源學術、商業化等重點方向進行了解讀。&lt;/p&gt; 
&lt;p&gt;華為終端 BG 首席執行官何剛，理想汽車 CTO 謝炎，深圳開鴻數字產業發展有限公司 CEO 王成錄，vivo 副總裁、OS 產品副總裁、vivo AI 全球研究院院長周圍，樂聚機器人董事長冷曉琨先後發表主題演講。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;AI 與開源融合成為本屆大會亮點之一，大會特別設置圓桌論壇環節。&lt;/strong&gt;麒麟軟件首席科學家韓乃平主持，國家地方共建人形機器人創新中心首席科學家江磊，CSDN 創始人＆董事長、中國開源推進聯盟副主席蔣濤，軟通動力董事兼首席技術官劉會福，openKylin 生態委員會主任李震寧，開源中國董事長馬越圍繞「智能時代的開源開發探索與實踐」主題展開深入探討。&lt;/p&gt; 
&lt;p&gt;本次大會由開放原子開源基金會主辦，來自工業和信息化部、相關省市工業和信息化主管部門、部分地方國資委、市縣政府有關負責同志以及參與開源生態建設的央企、國企、科技龍頭企業和中小企業代表；高校、研究機構代表；相關學會、協會、基金會等社會組織代表；開放原子開源基金會的理事、捐贈人、開源貢獻人代表等千餘人蔘加。大會將持續至 7 月 24 日。期間，&lt;strong&gt;共設置 26 場分論壇、多場交流會及開源生態交流區等形式，打通政、產、學、研、用、金等多方資源，為開源合作伙伴、項目設置充分的合作交流場景，協同各方形成合力為項目社區及生態發展賦能。&lt;/strong&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362063</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362063</guid>
      <pubDate>Thu, 17 Jul 2025 03:08:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>科研人員違規使用 AI 致泄密，國安部發布警示案例</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;國家安全部官方公眾號&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fl3Kjg5L4SgI_rrvQg8NM-w" target="_blank"&gt;發文&lt;/a&gt;公佈了一些涉及信息泄露的典型案例，特別是關於科研人員和公職人員在使用人工智能工具時的違規行為。提醒廣大工作人員，務必在處理涉密信息時保持高度警惕，避免因一時疏忽而造成嚴重後果。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="334" src="https://oscimg.oschina.net/oscnet/up-3646e4ef5741e7aaae0c9a9adb3516a6676.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;案例中提到的一名叫小李的是某科研機構研究人員，在撰寫一份研究報告時為了圖方便，違規使用了一款 AI 應用軟件。並擅自將核心數據和實驗成果上傳至該軟件，結果導致該研究領域的涉密信息泄露。事後，小李受到了嚴厲的處理。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;為避免類似事件再次發生，國家安全部特別提醒公眾，規範使用智能工具，防範技術風險。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;在使用生成式人工智能工具時，務必杜絕在互聯網或非涉密環境中處理任何涉密信息。此外，使用的人工智能應用軟件應從正規渠道下載，避免因使用山寨軟件而導致信息安全風險。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362061</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362061</guid>
      <pubDate>Thu, 17 Jul 2025 03:04:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>vivo 正式開源基於 Rust 編寫的藍河操作系統內核</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;7 月 23 日，vivo AI 全球研究院院長周圍在 2025 開放原子開源生態大會上宣佈&lt;strong&gt;藍河操作系統內核正式開源&lt;/strong&gt;。藍河操作系統（BlueOS）是 vivo 自主研發的行業首個從內核到系統框架全棧使用 Rust 語言編寫的操作系統。&lt;/p&gt; 
&lt;p&gt;&lt;img height="607" src="https://static.oschina.net/uploads/space/2025/0724/104535_5ou6_2720166.png" width="1280" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ul&gt; 
&lt;/ul&gt; 
&lt;p&gt;據介紹，由 Rust 語言編寫的藍河操作系統內核（以下簡稱「藍河內核」），具備安全、輕量、通用的三大特性。&lt;/p&gt; 
&lt;p&gt;在安全方面，藍河內核全棧使用 Rust 語言開發，也是行業首款適用於嵌入式平台和移動設備的開源 Rust 內核，基於編譯期所有權系統，通過所有權、借用、生命週期的靜態規則，編譯期確保內存安全，而在運行時通過智能指針，靈活管理內存，無額外內存回收性能損耗，讓內存安全從被動防禦到主動掌控。&lt;/p&gt; 
&lt;p&gt;得益於對基礎數據結構高性能低開銷的設計，藍河內核對硬件資源需求低，最小內核內存佔用僅 13KB，能夠以更低的成本滿足各類終端產品的需求。&lt;/p&gt; 
&lt;p&gt;另外，藍河內核兼容 RISC-V、ARM 等多芯片架構，可滿足開發者在不同平台的業務需要，也支持兼容 POSIX 接口的標準庫，拓展支持已有的生態，具有出色的通用性。&lt;/p&gt; 
&lt;ul&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img height="1656" src="https://static.oschina.net/uploads/space/2025/0724/104516_JDCC_2720166.png" width="2690" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;藍河內核具備完整的系統調度、內存管理、文件系統、網絡、和設備驅動五大內核能力。在系統調度上，藍河內核支持主流的調度算法，包括基於時間片輪轉調度和基於優先級隊列的實時調度；在內存管理方面，將 Rust 語言內存安全核心特性和智能指針相結合保障內存安全，同時支持多種內存分配算法，可適用於不同場景，供開發者基於自己的業務場景靈活選擇。&lt;/p&gt; 
&lt;p&gt;藍河內核的文件系統則採用了經典的層次化結構設計，實現了對文件和 inode 等數據結構的抽象操作，支持快速適配不同的文件系統。&lt;/p&gt; 
&lt;p&gt;而對於網絡，藍河內核支持基礎的 TCP/IP 協議棧，能夠以阻塞模式和非阻塞模式調用，可支持接入多網卡設備，也基於 Rust Zero-Copy 零拷貝設計，消除數據傳輸過程的堆分配開銷，支持了 socket api。&lt;/p&gt; 
&lt;p&gt;設備管理上，藍河內核通過硬件抽象等一系列方式，提升了對於 CPU 架構和驅動的兼容能力，支持 Rust 語言開發驅動，也支持兼容已有的 C 語言內核的外設驅動。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;藍河內核開源代碼：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;AtomGit：https://atomgit.com/vivoblueos&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub：https://github.com/vivoblueos&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362058</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362058</guid>
      <pubDate>Thu, 17 Jul 2025 02:48:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>抖音集團基於 Flink 的億級 RPS 實時計算優化實踐</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-299b0ad831e8aa86ddb446bf5085c13099e.jpg" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;**摘要：**本文整理自抖音集團數據工程師陶王飛和羊藝超老師，在&amp;nbsp;Flink&amp;nbsp;Forward&amp;nbsp;Asia&amp;nbsp;2024&amp;nbsp;生產實踐（一）專場中的分享主要內容主要分為以下四個部分：&lt;/p&gt; 
 &lt;p&gt;1、現狀與痛點&lt;/p&gt; 
 &lt;p&gt;2、鏈路通用優化&lt;/p&gt; 
 &lt;p&gt;3、業務場景優化&lt;/p&gt; 
 &lt;p&gt;4、未來規劃&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;span id="OSC_h2_1"&gt;&lt;/span&gt; 
&lt;h2&gt;01、現狀與痛點&lt;/h2&gt; 
&lt;span id="OSC_h3_2"&gt;&lt;/span&gt; 
&lt;h3&gt;1.1&amp;nbsp;業務現狀&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//31bfc2c668fca07ca815665dc86dfe11.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;抖音的主要業務場景為視頻和直播，實時數據在其中有着廣泛應用，如實時大屏、實時預警、對內實時分析（如大盤生態監控）、實時榜單以及為推薦提供實時特徵數據等。&lt;/p&gt; 
&lt;p&gt;視頻場景流量巨大，晚高峯整體流量達億級 RPS；直播場景則狀態數據量大，因為業務上直播間開關播時間無限制，導致存在許多超長週期存儲聚合需求。&lt;/p&gt; 
&lt;span id="OSC_h3_3"&gt;&lt;/span&gt; 
&lt;h3&gt;1.2&amp;nbsp;問題挑戰&lt;/h3&gt; 
&lt;p&gt;（1）實時數倉架構圖&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//68bd7c819883b62cc8ca999547327819.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;數據源層包括客戶端埋點、服務端日誌以及業務庫數據。數倉的分層使用 Flink 計算，依次為 ODS 層（數據源層）、DWD 層（進行維表關聯與簡單數據處理）、DWS 層（指標計算）和 APP 層（針對具體應用場景開發），最終將數據輸出至下游存儲。下游存儲依據業務場景選擇不同，ToC 場景多使用內部的 KV 存儲引擎 Abase，分析型場景及對內產品、平台則使用 ClickHouse 或 Doris，以供下游業務使用。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;（2）問題挑戰&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在開發過程中，主要面臨着三個問題：&lt;/p&gt; 
&lt;p&gt;其一，由於數據量大且計算複雜，致使鏈路穩定性差，任務頻繁失敗；&lt;/p&gt; 
&lt;p&gt;其二，資源消耗巨大，整體計算資源已達 30 萬 core；&lt;/p&gt; 
&lt;p&gt;其三，任務異常恢復緩慢，晚高峯時異常恢復時長 30+分鐘。&lt;/p&gt; 
&lt;span id="OSC_h2_4"&gt;&lt;/span&gt; 
&lt;h2&gt;02、鏈路通用優化&lt;/h2&gt; 
&lt;span id="OSC_h3_5"&gt;&lt;/span&gt; 
&lt;h3&gt;2.1&amp;nbsp;通用優化方案&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//a5d8b340159cba4ee3ab177fb8d29116.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在 DWD 明細層，優化關聯操作，核心維表在此層關聯，而非核心維表及字段則在 DWD 擴展層關聯。DWS 層和 APP 層計算直播及視頻的天級累計指標。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;（1）分層建模優化&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在分層建模時，將相同實體、不同維度的數據合併為一張維表，以降低下游消費 RPS。在維度關聯時，使用 Keyby 提升本地緩存命中率。對於 DWD 大作業，將其拆分成多個小任務灰度上線。在視頻大流量場景下，採用寬表模型輸出指標，即將所有數據置於一行，存儲在一個 Map 中輸出；直播場景則使用窄表 Anchor 模型，一條數據對應一個指標一行數據。這兩種模型在新增指標時均可實現狀態兼容。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;（2）作業性能優化&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在任務層面進行性能優化以降低資源消耗，具體在後文中進行介紹。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;（3）鏈路保障優化&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;對任務和隊列進行分級，並構建全鏈路血緣來保障分級的準確性。對於高優任務，建立熱備鏈路以及自動化容災切換能力，提升鏈路大盤的容災能力。&lt;/p&gt; 
&lt;span id="OSC_h3_6"&gt;&lt;/span&gt; 
&lt;h3&gt;2.2&amp;nbsp;技術手段優化&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//c28a4dcfac0a889caab651ef1a3670f3.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在抖音短視頻業務中，流量呈現高度集中，top20 個的任務佔據 40% 以上的計算資源，這些任務不僅成本高，還會降低穩定性，加大運維難度。&lt;/p&gt; 
&lt;p&gt;從頭部任務開發分析，部分簡單的 Pipeline 任務消耗大量計算資源，可以結合火焰圖查找問題的根源。如上圖中左下角的火焰圖，Calc 算子消耗佔比達 56%，這對於非計算型的任務是明顯異常情況。經分析是 JIT 及時編譯優化失效所致。&lt;/p&gt; 
&lt;p&gt;再結合右側重新開啓優化後的圖分析，大任務資源消耗下降約 40%。此外，火焰圖中 Calc 佔比較大的情況常出現在大併發 Hash 場景，上游併發×下游併發的數據輸出隊列會導致任務 Shuffle 利用率低，資源消耗較大。我們前後發現了十幾項優化項，並推廣至其他業務，最終 top 20 任務資源消耗下降約 25%。&lt;/p&gt; 
&lt;span id="OSC_h2_7"&gt;&lt;/span&gt; 
&lt;h2&gt;03、業務場景優化&lt;/h2&gt; 
&lt;span id="OSC_h3_8"&gt;&lt;/span&gt; 
&lt;h3&gt;3.1&amp;nbsp;視頻場景痛點優化&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//b679d7cd16af45c7c78f1261ae8bc6d9.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在 DWD 明細層，關聯大量維表時面臨巨大壓力。大流量場景下整體流量達億級規模，無法直接請求維表，即便開啓維表 LRU cache 請求量也達千萬級，這帶來了成本和穩定性難題。在指標聚合計算時，大流量下解決重複數據問題挑戰巨大。&lt;/p&gt; 
&lt;p&gt;（1）大流量維表關聯優化&lt;/p&gt; 
&lt;p&gt;①場景分析&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//591030223c0f730869a04f404767f0cc.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;高 QPS 的維表訪問導致 Abase 集羣壓力大，Flink 任務穩定性差，關聯維表成為瓶頸。雖提升維表關聯緩存命中率可降低外部請求 QPS，但目前緩存命中率已達 90% 以上，提升空間有限。且並非所有維表都超大且時效性要求高，如離線用戶維表和百萬級監控規則表都相對較小。數倉大量使用 Abase 這種 KV 存儲支持大訪問 QPS，但當超出其承受能力時，會帶來不可控，因此需擺脫對 KV 引擎的依賴，引入新的維表存儲方式。&lt;/p&gt; 
&lt;p&gt;②解決方案&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//00d80b09acbb9c9efed83450f3201c2f.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;整體思路是將外部組件訪問轉化為本地訪問，最直接的方式就是將數據加載到內存中完成計算。基於此，我們通過開發 UDF 將 Hive 和 MySQL 中的數據加載到內存中關聯，但其並不通用，每次查詢 Hive 和 MySQL 語句時需要單獨指定，且只能加載少量數據，於是，我們將 UDF 升級為 Flink Broadcast join 功能。&lt;/p&gt; 
&lt;p&gt;該功能設計分為三個模塊，以 Hive 為例。分區發現模塊通過 Broadcast 算子監測 Hive 分區，發現新分區時，即向下游下發 Watermark 和表元數據信息；數據構建模塊的數據讀取算子，可配置大併發用於讀取 Hive 維表數據；數據分發模塊可以將讀取的數據分發到各個 TM 中，根據數據量不同有兩種分發方式， 即 Broadcast 方式（將全量數據 copy 分發）或根據主鍵 Hash 分發（適用於數據量較大場景）。&lt;/p&gt; 
&lt;p&gt;在抖音內部場景，該功能支持了千億級別的維表關聯，主要適用於大流量場景下維表關聯業務，對維表更新的感知在分鐘級以上。功能上線後，它替代了部分 Abase 關聯任務，減少約 400 萬 QPS ，相關任務在追溯場景下無外部訪問瓶頸。&lt;/p&gt; 
&lt;p&gt;（2）大流量冪等計算&lt;/p&gt; 
&lt;p&gt;①場景分析&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//bd261bf4c464cd22ed668087c1c2df7b.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在大流量聚合計算優化方面，由於數據量大，短視頻的小時和天級指標從分鐘中間層聚合，這會導致分鐘聚合輸出有重複數據、亂序數據甚至回撤數據。如果直接通過先取 max 等方式聚合，其計算成本高且會引入回撤流問題，導致原本遞增的埋點指標下降。以分鐘向小時聚合為例，小時任務需要每一分鐘的最後一條數據。&lt;/p&gt; 
&lt;p&gt;②解決方案&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//f515a4786be013304ee91c7ff5a26886.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;針對以上問題，我們引入了 Bucket 的思路，即每一分鐘維護一個 Bucket，保留最後一條數據，新數據到來時，將其對應的 Bucket 與原本的數據相較，僅下發正序遞增數據。這樣，從分鐘級向小時級聚合時，即使分鐘級流量達百萬 RPS，最終也只有 60 個 Bucket。&lt;/p&gt; 
&lt;p&gt;基於此簡化模型，如上圖左下角展示的分鐘數據輸出，第一列是分鐘值，即 Bucket key；第二列是時間位移，用於 Bucket 的時間比較；第三列是指標值。第一、二條數據均為 58 分鐘，因此，其屬於同一個 Bucket，數據也是正序到來的，因此，Bucket 記錄為 30 秒；指標值為 100 的數據，第三條數據正常輸出，第四條和第五條數據存在亂序，40 秒的數據先到，20 秒的數據後到，因此，Bucket 只記錄 40 秒的數據，在 20 秒的數據進入後不再更新。這樣，通過 Bucket 機制可有效處理重複下發、亂序和回撤數據，不影響小時及天指標聚合結果。&lt;/p&gt; 
&lt;p&gt;③性能優化&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//f0385e2fac946c801ad7ba46390c9b47.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;基於 Bucket 計算資源消耗仍較大，通過火焰圖分析，發現 state 的序列化環節和 GC 環節佔比較大，表明在狀態和計算上仍有優化空間。我們從數據結構和業務兩方面入手。&lt;/p&gt; 
&lt;p&gt;其一，優化 Bucket 結構，將多層嵌套結構改為兩個數組，兩個數組的長度等於 Bucket 的長度，無需要存儲 Bucket key，按照數組的順序取對應 Bucket。一個數組存儲時間戳位移，將 long 格式的時間戳存儲為 int 類型（減少存儲佔用），另一個數組通過字符串拼接指標值，將原本的 Map 中指標的 value 拼接成一行，節省 state 中的空間佔用。&lt;/p&gt; 
&lt;p&gt;其二，進行 Bucket 時間壓縮，從分鐘向天級聚合最多 1440 個 Bucket，根據業務實際情況，將六個小時之前的 Bucket 壓縮，Bucket 數量從 1440 個降至 378 個，降低約 70%。&lt;/p&gt; 
&lt;p&gt;完成這些優化後，整體資源消耗下降約 30%。&lt;/p&gt; 
&lt;span id="OSC_h3_9"&gt;&lt;/span&gt; 
&lt;h3&gt;3.2&amp;nbsp;直播場景痛點問題&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//8ff92760d26c1e1097ebc17a1402f5b0.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;直播場景的痛點問題主要分為計算大狀態和回溯大流量兩類。&lt;/p&gt; 
&lt;p&gt;首先是直播間場次聚合計算大狀態。抖音直播中，直播間最長可開播 30 天，但目前 Flink 作業的 DWS 和 APP 層只計算開播後七天狀態的數據，即 state TTL 為七天。其原因是目前單作業資源消耗大，最高已達 2000 核，狀態可達 18T，穩定性不佳，無法簡單橫向擴展資源解決問題。而業務有查看直播間 30 天累計指標的訴求。&lt;/p&gt; 
&lt;p&gt;其次是冷啓動和故障回溯大流量。在此場景下，回溯數據從小時到天級不等，DWS 作業向下游下發數據時會重複且大量下發，影響下游 MQ 及 Redis 等 QA 存儲組件穩定性。此外，Flink 作業運行雖有資源投入，但回溯數據仍較慢，從業務視角看，存在數據恢復慢和性能指標上線週期長的問題。&lt;/p&gt; 
&lt;p&gt;針對這些問題，我們提出了相應的解決方案。&lt;/p&gt; 
&lt;p&gt;（1）大狀態優化&lt;/p&gt; 
&lt;p&gt;①場景分析&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//56fdd1dd529ad5affae89677e9e98656.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;對於大狀態優化場景，直播間開關播時間和時長不固定，最短不到分鐘級，最長 30 天，平均在小時級別。分析 Flink 作業中不同開播時長的狀態大小佔比發現，state TTL 為七天時，開播時長一天的直播間狀態大小佔 98%，這部分多存儲六天；大於一天小於七天的佔 1%，也存在多存情況；大於八天的僅佔 0.5‰，存在少存情況。該問題的核心是狀態固定的 TTL 與直播間動態的 TTL 矛盾，導致 99% 的狀態多存，0.5‰狀態少存。&lt;/p&gt; 
&lt;p&gt;解決思路是對齊兩者 TTL，實現直播間關播後刪除狀態。&lt;/p&gt; 
&lt;p&gt;②方案設計&lt;/p&gt; 
&lt;p&gt;最初設計了兩種方案。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//a9521b265a30d6d8e8de8f9de9f4645d.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;第一種基於 Retract 機制刪除狀態。在 Flink 作業中，同時消費直播間流量數據和關播數據，將兩條數據進行聚合。按直播間 ID 分組聚合計算，關播 source 收到數據後向下遊聚合算子發送 delete 消息，下游的聚合算子在接受到該消息後刪除狀態。&lt;/p&gt; 
&lt;p&gt;但該方案存在問題，用戶開發成本高，刪除狀態邏輯與 SQL 邏輯強耦合，導致改造作業時開發成本大；擴展性差，僅適用於 group key 為直播間 ID 的場景，而實際業務中 group key 可能包含更多內容，如用戶畫像或主播畫像等，則不適用。&lt;/p&gt; 
&lt;p&gt;總結問題後，發現其核心是刪除狀態邏輯與 SQL 邏輯強耦合，進而設計了第二種方案作為 TTL CompactionFilter 方案的擴展，即自定義 RocksDB CompactionFilter 方案。兩者執行時機相同，自定義 RocksDB CompactionFilter 方案支持通過 Java UDF 為指定狀態設定 CompactionFilter 。兩者的區別在於，TTL CompactionFilter 執行時解析狀態中的時間戳判斷狀態是否刪除，而自定義方案在 RocksDB 執行時，通過 JNI 將狀態數據傳給 Flink TM，解析直播間 ID 作為 CompactionFilter 入參，訪問直播間 Abase 維表，判斷是否關播，若關播，則 CompactionFilter 返回 true，刪除狀態。此方案實現了直播間關播後的狀態刪除，且與 SQL 邏輯完全解耦，解決了方案一中的問題。&lt;/p&gt; 
&lt;p&gt;③落地方案&lt;/p&gt; 
&lt;p&gt;與&amp;nbsp;Flink&amp;nbsp;架構組共建，實現了該方案的落地。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//4a72afb9ccd8582789e772b28472bfa9.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Flink 架構組在 RocksDB 層面支持 CompactionFilter 能力，在 SQL 層面支持用戶為指定聚合算子指定 CompactionFilter。如代碼示例所示，設置狀態 TTL 為 30 天，通過 ADD RESOURCES 語句引入 CompactionFilter 的 jar 包，通過 SQL hint 指定聚合算子的 CompactionFilter，參數包括 path（路徑） 和 filed（直播間 ID）。&lt;/p&gt; 
&lt;p&gt;實現過程中對性能問題進行了優化，如 CompactionFilter 查詢性能優化，將實時訪問 Abase 優化為批量加載關播直播間數據到本地，判斷是否關播，避免 Compaction 執行過程中， CompactionFilter 訪問外部組件查詢阻塞，減少 CP 的時長；Cache 選擇優化，將本地存儲關播直播間的 cache 從內存優化到磁盤，降低 GC 時長；CompactionFilter 調用頻次優化，設定 state 存儲時長超過兩天才調用 CompactionFilter，減少未關播直播間頻繁調用導致的 CPU 浪費，同時在 RocksDB C++側緩存，直播間開關播的結果（CompactionFilter 結果），利用 RocksDB 存儲機制，將直播間 ID 放在 group by 語句最前面，順序存儲相同 ID 的狀態數據，複用 CompactionFilter 調用結果，避免 JNI 調用帶來的性能損耗。&lt;/p&gt; 
&lt;p&gt;通過該方案，業務上支持了直播間 30 天累計指標，技術上直播間場次作業狀態平均下降 60%，CPU 資源使用下降 70%。&lt;/p&gt; 
&lt;p&gt;（2）大流量回溯優化&lt;/p&gt; 
&lt;p&gt;①場景分析&lt;/p&gt; 
&lt;p&gt;分析無&amp;nbsp;lag&amp;nbsp;場景和追 lag&amp;nbsp;場景下作業期望目標。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//23ffaab6834affe69f04a03720743f1d.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;無 lag 場景追求低延遲，數據實時產出，期望數據處理模式為流處理；追 lag 場景追求短時間內快速恢復，數據高吞吐，期望處理模式為批處理。&lt;/p&gt; 
&lt;p&gt;但當前 Flink 流處理作業僅能以流處理運行，設置 Minibatch 為 30 秒，在無 lag 場景可行，而在追 lag 場景則存在吞吐低、恢復慢的問題，與預期高吞吐目標不符。&lt;/p&gt; 
&lt;p&gt;為解決此問題，分析 Flink 流處理和批處理在引擎實現上的差異，在滿足 Flink 流處理低延遲特性的同時，實現 Flink 批處理的高吞吐。流處理通過 Minibatch 機制保證低延遲，但其 RocksDB 隨機訪問和 Retract 機制限制了吞吐；批處理雖有高延遲，但通過 sort 排序處理且無 Retract 機制，吞吐較高。因此，我們提出在流作業中動態監測消費積壓情況，判斷作業對高吞吐或低延遲的傾向性，在當前算子引入 sort 排序算子和動態調整 Minibatch 大小的能力，實現流批執行模式的動態切換。&lt;/p&gt; 
&lt;p&gt;②方案設計&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//d1272c4b617b4e584d685b0d1961502a.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;該方案核心步驟包括積壓檢測、檢測結果傳遞和動態啓用 sorter 算子並調整 Minibatch 大小。Flink 作業運行時，Source 算子動態監測 lag size；當 lag size 超過指定值時，向下遊算子發送數據時，標記 isBackLog 為 true，聚合算子接收數據後解析該字段，若為 true，則認為當前作業傾向於批處理，啓用 sorter，將 Minibatch 的大小間隔調整為 CP 的間隔。&lt;/p&gt; 
&lt;p&gt;此方案實現了流作業執行過程中流處理和批處理模式的動態切換，且作業的 DAG 不變，狀態完全兼容。&lt;/p&gt; 
&lt;p&gt;③落地方案&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//258cc68cab26724db1a0d2f863dcd020.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Flink 流批傾向性指標有三個，即自動檢測積壓狀態時間間隔、觸發切換批模式處理的平均 lag size 上限、觸發切換流處理平均 lag size 下限。SQL 使用案例如上圖左下角所示：首先，在運行參數中設置 backlog mode 開啓，然後在 source 算子中指定以上三個參數，進而實現 Flink 作業流批融合的處理。&lt;/p&gt; 
&lt;p&gt;通過該方案，技術上，追 lag 場景回溯數據結果下發量減少，下游組件穩定性提升；業務上，批模式追溯速度比流模式提升一倍。&lt;/p&gt; 
&lt;span id="OSC_h2_10"&gt;&lt;/span&gt; 
&lt;h2&gt;04、未來規劃&lt;/h2&gt; 
&lt;p&gt;在抖音一級 RPS 場景下，未來優化分為通用優化和個性化場景優化。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//ccf7f6a093cc54cd46625cbd98349751.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h3_11"&gt;&lt;/span&gt; 
&lt;h3&gt;4.1&amp;nbsp;通用優化&lt;/h3&gt; 
&lt;p&gt;作業重啓時內存 localcache 失效導致的緩存穿透優化；使用 Paimon 維表能力減少對 Redis 等 K-V 存儲的請求，使用 PaimonWithMQ 能力減少 MQ dump 派作業，節約資源；豐富 AutoScaling 的資源優化規則，獲取更多收益。&lt;/p&gt; 
&lt;span id="OSC_h3_12"&gt;&lt;/span&gt; 
&lt;h3&gt;4.2&amp;nbsp;個性化場景優化&lt;/h3&gt; 
&lt;p&gt;解決多任務比值類指標分子分母更新快慢不一致導致的波動明顯問題；實現 broadcast join 支持狀態的增量加載，針對千億級維表，拒絕全量加載，而是定時加載僅變化的增量數據，提升作業穩定性；進行內存優化。&lt;/p&gt; 
&lt;span id="OSC_h3_13"&gt;&lt;/span&gt; 
&lt;h3&gt;更多內容&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//6041760444b05a21431ac6efcf7680be.jpg" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;span id="OSC_h3_14"&gt;&lt;/span&gt; 
&lt;h3&gt;活動推薦&lt;/h3&gt; 
&lt;p&gt;阿里雲基於 Apache Flink 構建的企業級產品-實時計算 Flink 版現開啓活動： 新用戶複製點擊下方鏈接或者掃描二維碼即可 0 元免費試用 Flink + Paimon &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ffree.aliyun.com%2F%3Futm_content%3Dg_1000395379%26productCode%3Dsc" rel="nofollow" target="_blank"&gt;實時計算 Flink 版&lt;/a&gt;（3000CU*小時，3 個月內） 瞭解活動詳情：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ffree.aliyun.com%2F%3Futm_content%3Dg_1000395379%26productCode%3Dsc" rel="nofollow" target="_blank"&gt;https://free.aliyun.com/?utm_content=g_1000395379&amp;amp;productCode=sc&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//9dca849c94b7e32b3cff482a77660ce0.jpg" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/2828172/blog/18685623</link>
      <guid isPermaLink="false">https://my.oschina.net/u/2828172/blog/18685623</guid>
      <pubDate>Thu, 17 Jul 2025 02:34:00 GMT</pubDate>
      <author>原創</author>
    </item>
    <item>
      <title>白宮發佈《美國 AI 行動計劃》</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;白宮發佈了名為&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.whitehouse.gov%2Farticles%2F2025%2F07%2Fwhite-house-unveils-americas-ai-action-plan%2F" target="_blank"&gt;《贏得 AI 競賽：美國 AI 行動計劃》（Winning the AI Race: America’s AI Action Plan）&lt;/a&gt;的戰略文件，以保證美國毫無爭議地成為全球 AI 霸主。&lt;/p&gt; 
&lt;p&gt;&lt;img height="595" src="https://static.oschina.net/uploads/space/2025/0724/103055_nOWT_2720166.png" width="521" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0724/103116_zfCg_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;該計劃主要有三大支柱，加速 AI 創新、構建 AI 基礎設施以及主導國際外交與安全，涵蓋 90 多項具體行政命令。其中，廢除限制 AI 創新監管條例，加速發電場、水資源、半導體芯片等基礎設施建設，這對於像 OpenAI、微軟、亞馬遜、谷歌、Meta 等 AI 巨頭來説非常有利。&lt;/p&gt; 
&lt;p&gt;白宮在 28 頁的 AI 行動計劃中特別要求，凡聯邦政府採購的大語言模型必須「客觀、不受自上而下意識形態影響」。此外，該計劃還把中國列為主要競爭對手，希望在技術創新、模型開源、基礎設施等方面領先。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362050</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362050</guid>
      <pubDate>Thu, 17 Jul 2025 02:31:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>蜻蜓 FM 開源 SmartXPlayer 音頻播放組件</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;由蜻蜓 FM 研發的音頻播放組件「SmartXPlayer」近日已正式開源並上線 OpenHarmony 三方庫中心倉。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;根據&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FtWpwyDSCWemr7NCevxiiYw%3Fpoc_token%3DHHyUgWijpdpTgHnwJ0S3aKNInFpLFta7--q5a3AQ" target="_blank"&gt;介紹&lt;/a&gt;，作為一款專為鴻蒙多端場景打造的音頻播放引擎，SmartXPlayer 基於鴻蒙系統分佈式能力和多線程架構，提供高性能、易集成的音頻播放能力支持，助力開發者高效構建更順滑、更智能、更便捷的音頻播放體驗。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="273" src="https://oscimg.oschina.net/oscnet/up-86f4c94cb3161e141e4caf4a5965a5e4178.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;當前，隨着音頻內容和智能設備的普及，傳統播放器在多端適配、分佈式投播、主線程阻塞等方面存在開發難、效率低、體驗差等痛點。在這一背景下，SmartXPlayer 應運而生，以組件化、跨線程、高擴展的技術路徑，有效提升鴻蒙平台音頻應用開發效率與終端播放體驗。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;SmartXPlayer 基於蜻蜓 FM 實際業務場景研發打磨，在多項關鍵能力上具備優異表現：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;span style="color:#000000"&gt;跨線程播放架構，提升系統響應效率&lt;/span&gt;&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;SmartXPlayer 首創子線程播放技術，通過引入 ThreadWorker 機制，播放任務在子線程處理，主線程專注 UI 渲染與狀態管理，將播放性能提升 50%，有效緩解主線程阻塞帶來的卡頓、閃退等問題。&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;span style="color:#000000"&gt;支持分佈式投播與後台播放，適配多端設備&lt;/span&gt;&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;SmartXPlayer 內置的 SXCastPlayer 實現了與本地播放器一致的標準播放接口，開發者無需為投播功能單獨學習新接口。同時，它能實時監聽設備連接狀態變化，當檢測到投播需求時，播放器會自動將內部的播放邏輯從本地播放器切換為 SXCastPlayer，從而實現「本地播放」到「跨設備投播」的無縫銜接。此外，它還具備後台播放與狀態同步能力，實現鴻蒙「全場景互聯」下的流暢音頻體驗。&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;span style="color:#000000"&gt;高度抽象 API，開發門檻低、接入效率高&lt;/span&gt;&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;SmartXPlayer 組件接口設計高度抽象，支持一行代碼實現多端投播，僅需少量代碼即可快速實現初始化與播放控制，開發效率大幅提升。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;據蜻蜓 FM 內部估算，在實際應用中，實現同樣的音頻播放效果，SmartXPlayer 相比傳統方案能夠將開發時長由 2 周縮短至 2-3 天，代碼量減少 60%，維護成本降低 50%，用戶體驗顯著提升。目前該方案已在蜻蜓 FM 鴻蒙版和蜻蜓電台元服務中集成使用, 整體表現優異，並計劃在未來支持更多音頻內容形態與播放場景的適配與擴展。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362049</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362049</guid>
      <pubDate>Thu, 17 Jul 2025 02:25:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>GitHub Spark 發佈公測預覽版，通過自然語言構建全棧應用</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;GitHub 宣佈其新產品 GitHub Spark 已面向 Copilot Pro+訂閲用戶開啓&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.blog%2Fchangelog%2F2025-07-23-github-spark-in-public-preview-for-copilot-pro-subscribers%2F" target="_blank"&gt;公測&lt;/a&gt;，該產品旨在讓開發者通過自然語言在數分鐘內完成從想法到部署的全棧智能應用程序的構建和發佈，無需進行環境設置或配置。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0724/101944_bsPs_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Spark 的核心功能由 Claude Sonnet 4 驅動，允許用戶通過描述想法來構建包含前端和後端的應用。該平台內置了數據存儲、LLM 推理、託管、部署和 GitHub 身份驗證等功能。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-d9febfe928d296469d6ce6186d282669ebe.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;開發者可以在應用中添加由 OpenAI、Meta、DeepSeek、xAI 等公司模型驅動的 AI 功能，而無需管理 API 密鑰。應用可以通過一鍵點擊進行部署，並自動創建一個包含 GitHub Actions 和 Dependabot 的 GitHub 倉庫，確保所有內容保持同步。&lt;/p&gt; 
&lt;p&gt;開發者可以通過自然語言、可視化編輯控件或在集成 GitHub Copilot 代碼補全的編輯器中進行編碼來迭代他們的想法。&lt;/p&gt; 
&lt;p&gt;此外，用戶可以直接從 Spark 中打開一個 codespace，以使用 Copilot agent 模式進行迭代，或將 issue 分配給 Copilot coding agent。Copilot Pro+訂閲用戶可直接訪問 Spark，其使用會消耗 GitHub Copilot 計劃中包含的 premium requests。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362046</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362046</guid>
      <pubDate>Thu, 17 Jul 2025 02:20:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Qwen3 系列模型迎來新第三方部署和價格特惠</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;阿里巴巴的 Qwen3 系列模型近期在多個平台獲得部署並在官方平台開啓了價格特惠。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0724/100552_pcPY_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Cerebras 宣佈推出 Qwen3-235B 模型，實現了每秒 1500 個 token 的推理速度，目前可進行有限制的免費體驗。&lt;/p&gt; 
&lt;p&gt;阿里雲的通義靈碼 IDE 已集成 Qwen3-Coder，並去掉了原有的 DeepSeek 模型。GMI inference cloud 也上線了 Qwen3 Coder 480B A35B Instruct FP8 版本，定價為輸入$1.00/M Tokens，輸出$2.00/M Tokens。&lt;/p&gt; 
&lt;p&gt;阿里雲百鍊平台宣佈對 Qwen3-Coder-Plus 進行為期一個月的限時降價，並進一步對上下文緩存功能進行説明：「上下文緩存的命中概率並不是 100%，即使是上下文完全一致的請求，也存在無法命中的概率，命中概率依據系統判斷而定。」&lt;/p&gt; 
&lt;table style="display:table; text-align:left"&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th style="border-color:rgba(36, 4, 4, 0.4); height:auto; text-align:left"&gt; &lt;p&gt;&lt;span&gt;Token 數量&lt;/span&gt;&lt;/p&gt; &lt;/th&gt; 
   &lt;th style="border-color:rgba(36, 4, 4, 0.4); height:auto; text-align:left"&gt; &lt;p&gt;&lt;span&gt;輸入成本 (每千 Token)&lt;/span&gt;&lt;/p&gt; &lt;/th&gt; 
   &lt;th style="border-color:rgba(36, 4, 4, 0.4); height:auto; text-align:left"&gt; &lt;p&gt;&lt;span&gt;輸出成本 (每千 Token)&lt;/span&gt;&lt;/p&gt; &lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td style="border-bottom-color:rgba(36, 4, 4, 0.4); border-bottom-left-radius:0px; border-bottom-right-radius:0px; border-bottom-style:solid; border-bottom-width:1px; border-left-color:rgba(36, 4, 4, 0.4); border-left-style:solid; border-left-width:1px; border-right-color:rgba(36, 4, 4, 0.4); border-right-style:solid; border-right-width:1px; border-top-color:rgba(36, 4, 4, 0.4); border-top-left-radius:0px; border-top-right-radius:0px; border-top-style:solid; border-top-width:1px; height:auto; text-align:left"&gt;&lt;strong style="color:#1f0c03"&gt;&lt;span&gt;&lt;span&gt;0-32K&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td style="border-bottom-color:rgba(36, 4, 4, 0.4); border-bottom-left-radius:0px; border-bottom-right-radius:0px; border-bottom-style:solid; border-bottom-width:1px; border-left-color:rgba(36, 4, 4, 0.4); border-left-style:solid; border-left-width:1px; border-right-color:rgba(36, 4, 4, 0.4); border-right-style:solid; border-right-width:1px; border-top-color:rgba(36, 4, 4, 0.4); border-top-left-radius:0px; border-top-right-radius:0px; border-top-style:solid; border-top-width:1px; height:auto; text-align:left"&gt; &lt;p&gt;&lt;span&gt;&lt;span&gt;0.004 元&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-bottom-color:rgba(36, 4, 4, 0.4); border-bottom-left-radius:0px; border-bottom-right-radius:0px; border-bottom-style:solid; border-bottom-width:1px; border-left-color:rgba(36, 4, 4, 0.4); border-left-style:solid; border-left-width:1px; border-right-color:rgba(36, 4, 4, 0.4); border-right-style:solid; border-right-width:1px; border-top-color:rgba(36, 4, 4, 0.4); border-top-left-radius:0px; border-top-right-radius:0px; border-top-style:solid; border-top-width:1px; height:auto; text-align:left"&gt; &lt;p&gt;&lt;span&gt;&lt;span&gt;0.016 元&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="border-bottom-color:rgba(36, 4, 4, 0.4); border-bottom-left-radius:0px; border-bottom-right-radius:0px; border-bottom-style:solid; border-bottom-width:1px; border-left-color:rgba(36, 4, 4, 0.4); border-left-style:solid; border-left-width:1px; border-right-color:rgba(36, 4, 4, 0.4); border-right-style:solid; border-right-width:1px; border-top-color:rgba(36, 4, 4, 0.4); border-top-left-radius:0px; border-top-right-radius:0px; border-top-style:solid; border-top-width:1px; height:auto; text-align:left"&gt;&lt;strong style="color:#1f0c03"&gt;&lt;span&gt;&lt;span&gt;32K-128K&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td style="border-bottom-color:rgba(36, 4, 4, 0.4); border-bottom-left-radius:0px; border-bottom-right-radius:0px; border-bottom-style:solid; border-bottom-width:1px; border-left-color:rgba(36, 4, 4, 0.4); border-left-style:solid; border-left-width:1px; border-right-color:rgba(36, 4, 4, 0.4); border-right-style:solid; border-right-width:1px; border-top-color:rgba(36, 4, 4, 0.4); border-top-left-radius:0px; border-top-right-radius:0px; border-top-style:solid; border-top-width:1px; height:auto; text-align:left"&gt; &lt;p&gt;&lt;span&gt;&lt;span&gt;0.0042 元 (原價 0.006 元的&amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;strong style="color:#1f0c03"&gt;&lt;span&gt;&lt;span&gt;7 折&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-bottom-color:rgba(36, 4, 4, 0.4); border-bottom-left-radius:0px; border-bottom-right-radius:0px; border-bottom-style:solid; border-bottom-width:1px; border-left-color:rgba(36, 4, 4, 0.4); border-left-style:solid; border-left-width:1px; border-right-color:rgba(36, 4, 4, 0.4); border-right-style:solid; border-right-width:1px; border-top-color:rgba(36, 4, 4, 0.4); border-top-left-radius:0px; border-top-right-radius:0px; border-top-style:solid; border-top-width:1px; height:auto; text-align:left"&gt; &lt;p&gt;&lt;span&gt;&lt;span&gt;0.0168 元 (原價 0.024 元的&amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;strong style="color:#1f0c03"&gt;&lt;span&gt;&lt;span&gt;7 折&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="border-bottom-color:rgba(36, 4, 4, 0.4); border-bottom-left-radius:0px; border-bottom-right-radius:0px; border-bottom-style:solid; border-bottom-width:1px; border-left-color:rgba(36, 4, 4, 0.4); border-left-style:solid; border-left-width:1px; border-right-color:rgba(36, 4, 4, 0.4); border-right-style:solid; border-right-width:1px; border-top-color:rgba(36, 4, 4, 0.4); border-top-left-radius:0px; border-top-right-radius:0px; border-top-style:solid; border-top-width:1px; height:auto; text-align:left"&gt;&lt;strong style="color:#1f0c03"&gt;&lt;span&gt;&lt;span&gt;128K-256K&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td style="border-bottom-color:rgba(36, 4, 4, 0.4); border-bottom-left-radius:0px; border-bottom-right-radius:0px; border-bottom-style:solid; border-bottom-width:1px; border-left-color:rgba(36, 4, 4, 0.4); border-left-style:solid; border-left-width:1px; border-right-color:rgba(36, 4, 4, 0.4); border-right-style:solid; border-right-width:1px; border-top-color:rgba(36, 4, 4, 0.4); border-top-left-radius:0px; border-top-right-radius:0px; border-top-style:solid; border-top-width:1px; height:auto; text-align:left"&gt; &lt;p&gt;&lt;span&gt;&lt;span&gt;0.005 元 (原價 0.01 元的&amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;strong style="color:#1f0c03"&gt;&lt;span&gt;&lt;span&gt;5 折&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-bottom-color:rgba(36, 4, 4, 0.4); border-bottom-left-radius:0px; border-bottom-right-radius:0px; border-bottom-style:solid; border-bottom-width:1px; border-left-color:rgba(36, 4, 4, 0.4); border-left-style:solid; border-left-width:1px; border-right-color:rgba(36, 4, 4, 0.4); border-right-style:solid; border-right-width:1px; border-top-color:rgba(36, 4, 4, 0.4); border-top-left-radius:0px; border-top-right-radius:0px; border-top-style:solid; border-top-width:1px; height:auto; text-align:left"&gt; &lt;p&gt;&lt;span&gt;&lt;span&gt;0.02 元 (原價 0.04 元的&amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;strong style="color:#1f0c03"&gt;&lt;span&gt;&lt;span&gt;5 折&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="border-bottom-color:rgba(36, 4, 4, 0.4); border-bottom-left-radius:0px; border-bottom-right-radius:0px; border-bottom-style:solid; border-bottom-width:1px; border-left-color:rgba(36, 4, 4, 0.4); border-left-style:solid; border-left-width:1px; border-right-color:rgba(36, 4, 4, 0.4); border-right-style:solid; border-right-width:1px; border-top-color:rgba(36, 4, 4, 0.4); border-top-left-radius:0px; border-top-right-radius:0px; border-top-style:solid; border-top-width:1px; height:auto; text-align:left"&gt;&lt;strong style="color:#1f0c03"&gt;&lt;span&gt;&lt;span&gt;256K-1M&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td style="border-bottom-color:rgba(36, 4, 4, 0.4); border-bottom-left-radius:0px; border-bottom-right-radius:0px; border-bottom-style:solid; border-bottom-width:1px; border-left-color:rgba(36, 4, 4, 0.4); border-left-style:solid; border-left-width:1px; border-right-color:rgba(36, 4, 4, 0.4); border-right-style:solid; border-right-width:1px; border-top-color:rgba(36, 4, 4, 0.4); border-top-left-radius:0px; border-top-right-radius:0px; border-top-style:solid; border-top-width:1px; height:auto; text-align:left"&gt; &lt;p&gt;&lt;span&gt;&lt;span&gt;0.01 元 (原價 0.02 元的&amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;strong style="color:#1f0c03"&gt;&lt;span&gt;&lt;span&gt;5 折&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-bottom-color:rgba(36, 4, 4, 0.4); border-bottom-left-radius:0px; border-bottom-right-radius:0px; border-bottom-style:solid; border-bottom-width:1px; border-left-color:rgba(36, 4, 4, 0.4); border-left-style:solid; border-left-width:1px; border-right-color:rgba(36, 4, 4, 0.4); border-right-style:solid; border-right-width:1px; border-top-color:rgba(36, 4, 4, 0.4); border-top-left-radius:0px; border-top-right-radius:0px; border-top-style:solid; border-top-width:1px; height:auto; text-align:left"&gt; &lt;p&gt;&lt;span&gt;&lt;span&gt;0.1 元 (原價 0.2 元的&amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;strong style="color:#1f0c03"&gt;&lt;span&gt;&lt;span&gt;5 折&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;詳情：https://help.aliyun.com/zh/model-studio/qwen3-coder-plus-price-drop&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362044</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362044</guid>
      <pubDate>Thu, 17 Jul 2025 02:06:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
  </channel>
</rss>
