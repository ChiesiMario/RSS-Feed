<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>開源中國-綜合資訊</title>
        <link>https://www.oschina.net/news/industry</link>
        <atom:link href="http://8.134.148.166:30044/oschina/news/industry" rel="self" type="application/rss+xml"></atom:link>
        <description>開源中國-綜合資訊 - Powered by RSSHub</description>
        <generator>RSSHub</generator>
        <webMaster>contact@rsshub.app (RSSHub)</webMaster>
        <language>en</language>
        <lastBuildDate>Tue, 11 Feb 2025 16:36:30 GMT</lastBuildDate>
        <ttl>5</ttl>
        <item>
            <title>百度文小言（原文心一言）App 接入 DeepSeek-R1 模型</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;iOS 版百度文小言（原文心一言）App 日前迎來了 4.9.0 版本更新，更新描述稱該版本已接入 DeepSeek-R1 模型，優化拍照解題功能。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-5a4769f89c4a201f4028836651b2741d29f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;▲ 百度文小言（原文心一言）App 接入 DeepSeek-R1 模型&lt;/p&gt; 
&lt;p&gt;接入 DeepSeek-R1 模型後，文心一言 App 的拍照解題功能得到了顯著提升。用戶在使用該功能時，可以清晰地看到解題過程中的思考步驟，這與 DeepSeek 特有的思維鏈功能非常相似。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-4cc68ae51d81f06ed72ba3d94755a877e04.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-1944e58c65fe9c878bae4407b0cf63cf2ab.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;有用戶表示，「這極大地提升了用戶的解題體驗。用戶通過拍攝問題，系統將自動識別並給出詳細的解題思路，這對於需要進行學習和複習的用戶來説是個好消息。」&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333181</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333181</guid>
            <pubDate>Fri, 07 Feb 2025 10:42:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>男子用 DeepSeek 買彩票中獎：買 10 元中 5 元</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;今日，詞條#用 DeepSeek 買彩票真中獎了#登上微博熱搜榜第一，引起許多網友熱議。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0211/172819_3xs4_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;據媒體報道，日前，安徽蕪湖一男子發帖稱，自己按照 DeepSeek 推薦的號碼買雙色球，真的中獎了。&lt;/p&gt; 
&lt;p&gt;該男子用 5 組 DeepSeek 推薦的數字下注，&lt;strong&gt;合計 10 元，其中一組數字中了「2+1」。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0211/173130_nyzI_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;他表示，這是第一次買雙色球，也是才接觸 DeepSeek，突發奇想想看看到底準不準，這樣的行為不能「上頭」，自己之後不會再用 DeepSeek 推薦的數字繼續買彩票。&lt;/p&gt; 
&lt;p&gt;據中國福利彩票服務熱線工作人員介紹，&lt;strong&gt;上述情況是中了六等獎，獎金為 5 元。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;對於該男子的做法，有網友表示：「合計 10 元，中了 5 元？有沒有可能沒有 DeepSeek，你買五組也有這個概率呢？我覺得也沒必要神話 DeepSeek。」「隨機概率這麼大，跟它真沒太大關係。」&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333169</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333169</guid>
            <pubDate>Fri, 07 Feb 2025 09:32:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>百度李彥宏：自動駕駛比人開車安全十倍</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;2 月 11 日消息，「世界政府峯會」（World Goverments Summit 2025）今日在阿聯酋迪拜開幕，百度創始人李彥宏今日上午在主論壇上與阿聯酋 AI 部長奧馬爾・蘇丹・奧拉馬（Omar Sultan AI Olama）對談時表示，Robotaxi 可以大大降低交通事故死亡率。從蘿蔔快跑的實際記錄來看，出險率僅為人類駕駛員的 1/14。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-b8b66f69d2ba73a703403b32ebdae56e2f3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;李彥宏表示：「技術進步非常快，自動駕駛比人類司機安全十倍。」&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-74a55024af52c507dd898d356bf590ace0f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;據瞭解，2024 年第二季度，百度的自動駕駛服務蘿蔔快跑供應的自動駕駛訂單約 89.9 萬單，同比增長 26%。截至 2024 年 7 月 28 日，蘿蔔快跑累計為公眾提供的自動駕駛出行服務訂單超過 700 萬單。&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;相關閲讀&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/news/301302&quot; target=&quot;news&quot;&gt;百度旗下的「蘿蔔快跑」無人駕駛出租車武漢街頭撞倒行人&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333160</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333160</guid>
            <pubDate>Fri, 07 Feb 2025 08:51:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>嘉立創集團加入 openKylin，助推社區計算多元化生態繁榮</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;近日，深圳嘉立創科技集團股份有限公司（簡稱「嘉立創集團」），簽署 OpenAtom openKylin（簡稱「openKylin」）社區 CLA（Contributor License Agreement 貢獻者許可協議），正式加入 openKylin 開源社區。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;1527&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-e8345ba8245337a544b99cf88d729976dd5.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;span&gt;嘉立創集團&lt;/span&gt;&lt;/span&gt;&lt;span&gt;是電子及機械產業一站式基礎設施服務提供商，旗下擁有電子及機械產業一站式服務提供商嘉立創科技、大批量 PCB/PCBA 智造企業中信華和國內領先的樣品/小批量電子元器件線上服務商立創商城等三大運營板塊。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;依託產業鏈一站式服務、全流程閉環數字化底座、柔性化智能製造等領先優勢以及百萬級用戶基礎，公司打造了「一站式產業互聯智造模式」，為客戶提供覆蓋從打樣到小批量再到大批量的 PCB 智造、電子元器件商城、SMT、激光鋼網等電子產業鏈和 CNC 機械智造、3D 打印、FA 機械電氣零部件商城等機械產業鏈一體化服務，以及以 EDA、CAM、DFM、CAD 軟件為核心的工業軟件集羣。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;410&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-620266042dadaea8fb8477b013d338fc9ca.jpg&quot; width=&quot;940&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;在加入 openKylin 社區後，嘉立創集團除了在產品適配、市場推廣、技術協同等關鍵業務領域積極作為與開拓進取之外，也將進一步秉持開放合作、互利共贏的發展理念，充分發揮自身資源優勢與產品的優越性，對 openKylin 社區進行全方位的業務支持，助力實現開源生態的繁榮發展，&lt;span&gt;為經濟持續發展、科技再創新高提供源動力。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333148</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333148</guid>
            <pubDate>Fri, 07 Feb 2025 07:55:00 GMT</pubDate>
            <author>來源: 投稿</author>
        </item>
        <item>
            <title>DeepSeek 梁文鋒身家暴漲，有專家預計或超黃仁勳</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;彭博社報道顯示，7 位創業公司創始人和人工智能專家對 DeepSeek 的估值存在巨大分歧，估值區間在 10 億美元到 1550 億美元之間。按照彭博億萬富翁指數中間值估算，DeepSeek 估值約在 20 億至 300 億美元，而持有公司 84% 股份的梁文鋒，身家可能在 16.8 億到 252 億美元之間，有望躋身亞洲最富有的科技大亨之列，甚至問鼎中國首富。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;不同業內人士給出的估值差異極大。波士頓風險投資公司 Glasswing Ventures 創始人魯迪納・塞塞裏認為，按同行公司估值，DeepSeek 最少值 10 億美元；研究工程師 Sebastian Raschka 則覺得，憑藉強大的品牌認知度，其估值應在 20 億到 100 億美元之間，高於 Mistral AI。而 Sweat Free Telecom 創始人查納基亞・拉姆德夫的預測更為樂觀，認為 DeepSeek 估值可達 1550 億美元。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;148&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-d2fbcd3fec3d8b307089121298b01c62f34.webp&quot; width=&quot;700&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;此前 1 月有報道稱，軟銀集團洽談牽頭對 OpenAI 進行最高 400 億美元融資，融資後估值達 3000 億美元。若 DeepSeek 按此估值一半計算，梁文鋒個人財富或達 1260 億美元，有望超過英偉達 CEO 黃仁勳，身家遠超鍾睒睒等富豪，在同領域也將遠超字節跳動創始人張一鳴（2024 年福布斯中國內地富豪榜顯示張一鳴身價 456 億美元，梁文鋒身家或為其 3 倍） 。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;梁文鋒出生於 1985 年，本碩就讀於浙江大學信息與通用工程專業，師從項志宇，研究機器視覺，2010 年畢業。畢業後，他投身量化投資，成立幻方量化，僅 6 年管理規模達千億，成為 「量化四大天王」 之一。2023 年 5 月，梁文鋒決心進軍通用人工智能領域，7 月成立 DeepSeek，被視為量化投身 AI 創業第一人。2024 年 12 月底，DeepSeek 發佈的 DeepSeek-V3 火遍全網。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;不過，由於 DeepSeek 收入、利潤等財務數據保密，外界只能通過對比 OpenAI、Anthropic 等公司估值來推測其價值，這些估值僅供參考。梁文鋒的真實身家究竟幾何，還需時間揭曉。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333142</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333142</guid>
            <pubDate>Fri, 07 Feb 2025 07:17:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>「互聯網之子」 Aaron Swartz 雕像在互聯網檔案館揭幕</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;Aaron Swartz 的大理石雕像上週五在互聯網檔案館的禮堂揭幕，有大約 300 人出席，半身像下方刻有文字&lt;em&gt;「The Internet&#39;s Own Boy（互聯網之子）」&lt;/em&gt;。本週重 312 磅的雕像將先轉移至大廳，直至獲得許可放置在當地公園內。&lt;/p&gt; 
&lt;p&gt;揭幕儀式上，Creative Commons 聯合創始人 Lisa Rein 強調：「崇拜 Aaron 的前提是正確理解他的故事——他並非殉道者，而是為公眾已付費的科學研究成果應自由獲取而戰。」電子前沿基金會（EFF）執行董事 Cindy Cohn 則稱，這座雕像「提醒人們為真理與正義持續鬥爭」。科幻作家 Cory Doctorow 在視頻致辭中暗諷特朗普政府時期的政治環境：「這是一個希望稀缺的時代，但這座雕像應激勵我們讓世界變得更好」。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-f82cf0e74040f33cd165bbe250c47ae3e5c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Aaron Swartz 出生於 1986 年，參與了 RSS 和 Markdown、web.py 等項目的開發，被視為是 Reddit 的聯合創始人。&lt;/p&gt; 
&lt;p&gt;2011 年 1 月他因為在 MIT 下載學術論文而遭到逮捕，面臨最高 35 年的刑期，他拒絕了認罪協議，於 2013 年 1 月 11 日自殺身亡。他在當年被追授進入互聯網名人堂。&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;相關閲讀&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/news/92332/in-memory-of-aron-swartz&quot; target=&quot;news&quot;&gt;紀念 Aaron Swartz：他用生命捍衞了互聯網的開放和自由&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/news/36671/aaron-swartz-kill-himself&quot; target=&quot;news&quot;&gt;web.py 作者 Aaron Swartz 自殺身亡&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333126/aaron-swartz-marble-statue-unveiled-internet-archive</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333126/aaron-swartz-marble-statue-unveiled-internet-archive</guid>
            <pubDate>Fri, 07 Feb 2025 06:33:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>谷歌 DeepMind CEO：DeepSeek 模型是中國最好的作品，但炒作有點誇大</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;谷歌旗下人工智能公司 DeepMind 首席執行官戴米斯·哈薩比斯（Demis Hassabis）在巴黎一場谷歌主辦的活動上，對 Deepseek 的 AI 模型做出了評價。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0211/113748_Ev6o_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;哈薩比斯稱讚 DeepSeek 的模型是令人印象深刻的作品，並表示「我認為這可能是我見過中國最好的作品」&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;哈薩比斯認為，其在短時間內完成的開發和訓練成本控制方面表現出色，然而從技術角度來看，哈薩比斯指出，Deepseek 並沒有展示出非常大的科學進步。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;他表示：「儘管有很多人吹捧，但其實這背後並沒有真正的新的科學進步……它（DeepSeek）在人工智能中使用的是已知的技術。」他補充説，圍繞 DeepSeek 的炒作「有點誇張」。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;他還聲稱，DeepMind 本週發佈的 Gemini 2.0 Flash 模型比 DeepSeek 大模型更為高效。&lt;/p&gt; 
&lt;p&gt;此外，哈薩比斯還談到了通用人工智能（AGI）的前景，他認為 AI 行業正在走向 AGI，且可能在未來 5 年左右實現這一目標。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333095/deepseeks-ai-model-the-best-work-out-of-china</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333095/deepseeks-ai-model-the-best-work-out-of-china</guid>
            <pubDate>Fri, 07 Feb 2025 03:39:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>TIOBE 2 月榜單：Rust 達新高，Mojo 和 Zig 嶄露頭角</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;TIOBE 公佈了 2025&amp;nbsp;年 2 月的&lt;/span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.tiobe.com%2Ftiobe-index%2F&quot; target=&quot;_blank&quot;&gt;編程語言排行榜&lt;/a&gt;&lt;span style=&quot;color:#000000&quot;&gt;。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;64&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-1df184cb150f1180b39eb214604fb7eeef4.png&quot; width=&quot;700&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;自去年 &lt;a href=&quot;https://www.oschina.net/news/296488/tiobe-index-202406&quot;&gt;6 月&lt;/a&gt;成功超越了 C 成為了 TIOBE 指數中新的第二名之後，C++ 便穩定在了這一位置上。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;TIOBE CEO&amp;nbsp;Paul Jansen&amp;nbsp;點評道，「隨着全球對每秒計算能力的需求日益增長，而硬件的發展速度卻未能跟上這一需求，程序的運行速度變得越來越重要。正因如此，在 TIOBE 指數中，那些以速度見長的編程語言逐漸嶄露頭角也就不足為奇了。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;除了&amp;nbsp;C++，Go 也穩居榜單前 10，Rust 則達到 1.47% 的歷史新高。此外，以速度著稱的 Mojo 和 Zig 也分別位列第 51 和第 56 位，正在叩響前 50 名的大門。&lt;/span&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;你可能會好奇，Python 這種慢速語言是如何在這些競爭激烈的語言面前生存下來的。這是因為，除了性能之外，如今還有另一個驅動因素：&lt;strong style=&quot;color:#404040&quot;&gt;學習一門新編程語言的難易程度&lt;/strong&gt;。除了需要處理更多的數據，世界還需要更多的程序員。完全依賴 AI 開發應用程序目前還無法實現，因此對新程序員的需求依然非常高。由於軟件工程專業畢業生的數量遠遠無法滿足市場需求，許多非軟件工程師也開始紛紛加入編程的行列，而他們最喜歡的語言正是 Python。這就是為什麼 Python 依然能夠穩居編程語言的主流地位。&lt;/span&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong style=&quot;color:#333333&quot;&gt;TIOBE 2 月 TOP 20 編程語言&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;404&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-d0707406f2fb35a7fdf61ce778525b8f614.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong style=&quot;color:#333333&quot;&gt;TOP 10 編程語言 TIOBE 指數走勢（2002-2024）&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;226&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-70c51a5ab719ba0927f95909df990227d21.png&quot; width=&quot;700&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong style=&quot;color:#333333&quot;&gt;第 21-50 名編程語言排行&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;417&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-52892925983133de7bb115cd8a0c1d16f89.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;第 51-100 名如下，由於它們之間的數值差異較小，僅以文本形式列出（按字母排序）：&lt;/span&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ABC, ActionScript, Algol, Alice, Apex, APL, AutoLISP, CFML, CHILL, Clipper, CLIPS, Clojure, Crystal, Curl, Elm, Erlang, F#, Forth, Groovy, Hack, Icon, Inform, Io, JScript, LabVIEW, Modula-2, Mojo, MQL5, NATURAL, Nim, OCaml, Occam, OpenCL, OpenEdge ABL, PL/I, Q, Raku, Ring, Scheme, Simulink, Smalltalk, SPARK, SPSS, Stata, SystemVerilog, Vala/Genie, VHDL, Wolfram, X++, Zig&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;TIOBE 編程社區指數（The TIOBE Programming Community index）是一個衡量編程語言受歡迎程度的指標，該指數每月更新一次。評判的依據來自世界範圍內的工程師、課程和第三方供應商，包括流行的搜索引擎，如 Google、必應、雅虎、維基百科、亞馬遜、YouTube 和百度都被用於指數計算。值得注意的是，TIOBE 指數並不代表編程語言的好壞或編寫代碼的多少。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;該指數可以用來檢查你的編程技能是否還能跟上時代的步伐，或者在開始建立一個新的軟件系統時，基於指數對採用何種編程語言做出決策。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.tiobe.com%2Ftiobe-index%2Fprogramminglanguages_definition%2F&quot; target=&quot;_blank&quot;&gt;TIOBE 指數&lt;/a&gt;&lt;span style=&quot;color:#000000&quot;&gt;的定義方式，以及詳細榜單信息&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.tiobe.com%2Ftiobe-index%2F&quot; target=&quot;_blank&quot;&gt;均可查看官網&lt;/a&gt;。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333092/tiobe-index-202502</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333092/tiobe-index-202502</guid>
            <pubDate>Fri, 07 Feb 2025 03:32:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>Sam Altman：AI 成本每年暴跌 10 倍，2035 年人人都有超級大腦</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;OpenAI CEO Sam Altman &lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.samaltman.com%2Fthree-observations&quot; target=&quot;_blank&quot;&gt;更新了個人博客&lt;/a&gt;&lt;/u&gt;，其中他預測 AI 成本每年將暴跌 10 倍，並且到了 2035 年，人人都能擁有超級大腦。 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0211/111413_19Xl_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;文中，Altman 提到自己對 AI 經濟學的三點觀察：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;AI 模型的智能水平大致等於其訓練和運行所使用資源的對數；&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;使用固定級別 AI 的成本大約每 12 個月降低 10 倍，價格下降會極大促進 AI 的使用；&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;線性增長的智能水平所創造的社會經濟價值呈超指數級增長。&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;並且 Altman 總結，如果這三點趨勢繼續保持，AI 對社會的影響將是巨大的。 &amp;nbsp;Altman 還預測，AI Agents 最終可能會像「虛擬同事」一樣與人類協作，在各個領域的知識工作中發揮作用。&lt;/p&gt; 
&lt;p&gt;此外，Altman 還認為，在某些方面，AI 在經濟上的作用可能會類似於晶體管，AI 也能夠大規模推廣，並滲透到經濟的各個角落。 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;最後，Altman 還表示確保 AGI 的廣泛受益，讓 AGI 的好處惠及全社會至關重要。並且他提議持續降低智能計算的成本，讓人人都能負擔得起 AI，按照這一目標，或將在 2035 年人人都能獲得近乎無限的智能支持。&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;附上博客原文：&lt;/p&gt; 
&lt;h3&gt;三點觀察&lt;/h3&gt; 
&lt;p&gt;我們的使命是確保 AGI（通用人工智能）造福全人類。&lt;/p&gt; 
&lt;p&gt;如今，一些接近 AGI 的系統已經開始顯現，因此我們認為理解當前所處的階段至關重要。AGI 的定義較為模糊，但通常指的是一種能夠在人類水平上解決越來越複雜問題的系統，且適用於多個領域。&lt;/p&gt; 
&lt;p&gt;（作者註釋：本文使用「AGI」一詞，我們的目的是清晰表達，並無意改變或重新定義我們與微軟的合作關係，以及避免斷章取義的解讀，我們完全預計將與微軟保持長期合作。）&lt;/p&gt; 
&lt;p&gt;人類天生具有構建工具的能力，並且擁有理解和創造的驅動力，這促使世界不斷進步。每一代人都會在前人發現的基礎上進一步創新，創造出更強大的工具——&lt;strong&gt;從電力到晶體管，再到計算機、互聯網，如今則是 AGI。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;儘管人類的創新曆程並非一帆風順，但從長遠來看，這一進程始終推動着社會發展，使人們的生活在各個方面都得到極大改善。&lt;/p&gt; 
&lt;p&gt;從某種角度來看，AGI 只是人類不斷攀登進步階梯的又一個工具。但從另一個角度來看，它可能標誌着一個真正不同的時代的開始。未來的經濟增長前景令人驚歎，我們甚至可以設想一個世界：所有疾病都能被治癒，我們擁有更多時間陪伴家人，並能夠充分發揮自己的創造潛力。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;或許再過十年，地球上的每個人都能擁有比今天最具影響力的人更強的能力。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;我們持續見證 AI 發展的迅猛進步，以下是關於 AI 經濟學的三點觀察：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;AI 模型的智能水平大致等於其訓練和運行所使用資源的對數。這些資源主要包括訓練計算（compute）、數據和推理計算（inference compute）。目前的趨勢表明，只要投入足夠的資金，就能持續且可預測地提升 AI 能力，而支撐這一趨勢的縮放定律（Scaling Laws）在多個數量級範圍內都被證明是準確的。&lt;/li&gt; 
 &lt;li&gt;使用固定級別 AI 的成本大約每 12 個月降低 10 倍，價格下降會極大促進 AI 的使用。一個明顯的例子是 GPT-4 在 2023 年初的使用成本，相比 GPT-4o 在 2024 年中期，其每個 token 的價格下降了約 150 倍。摩爾定律每 18 個月帶來 2 倍的性能提升，而 AI 成本下降的速度遠超這一趨勢，影響將更加深遠。&lt;/li&gt; 
 &lt;li&gt;線性增長的智能水平所創造的社會經濟價值呈超指數級增長。這一趨勢意味着，對於 AI 的指數級投資在可預見的未來不會停止。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;如果這三點趨勢繼續保持，AI 對社會的影響將是巨大的。&lt;/p&gt; 
&lt;p&gt;目前，&lt;strong&gt;我們已經開始推出 AI Agents，它們最終可能會像「虛擬同事」一樣與人類協作。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;以軟件工程領域的 AI Agent 為例——這是我們認為極為重要的應用方向之一。設想未來的 AI Agent 能夠完成大部分經驗 3-5 年的頂級公司軟件工程師可以完成的任務，但任務時長限制在幾天內。它不會有突破性的創新想法，需要大量的人類監督和指導，在某些方面表現出色，同時在某些意想不到的地方表現較差。&lt;/p&gt; 
&lt;p&gt;儘管如此，它仍可以被視作一名真實但相對初級的虛擬同事。&lt;strong&gt;現在，想象一下如果有 1000 個這樣的 AI Agnet，或者 1000000 個。再進一步，設想這樣的 AI Agnet 被應用到所有知識型工作領域，其影響將難以估量。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在某些方面，AI 在經濟上的作用可能會類似於晶體管——一個重大科學突破，能夠大規模推廣，並滲透到經濟的各個角落。如今，我們不會特別關注晶體管或生產晶體管的公司，但它們的存在讓我們的計算機、電視、汽車、玩具等設備變得更加強大、近乎奇蹟般地運作。&lt;/p&gt; 
&lt;p&gt;世界的變化不會一蹴而就，它從未如此。短期內，生活仍將繼續，2025 年的人們大概率會和 2024 年一樣度過日常——我們仍會相愛、組建家庭、在網上爭論、去大自然中遠足等等。&lt;/p&gt; 
&lt;p&gt;然而，未來的到來將不可忽視，長期來看，社會和經濟的變化將是巨大的。人類將找到新的事物去探索，找到新的方式去互相幫助、去競爭，但這些方式可能與今天的工作模式截然不同。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;在這樣的時代，主動性、意志力和決策能力將變得尤為寶貴。正確地決定要做什麼，並在不斷變化的世界中找到前進的道路，將具有極高的價值。因此，韌性和適應能力將成為關鍵技能。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;AGI 將是史上最強大的槓桿，極大增強人類的主觀能動性，它不會削弱個人的影響力，反而會讓個體的能力比以往任何時候都更強大。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;AGI 的影響不會均勻分佈。某些行業可能變化不大，但科學進步的速度可能比今天快得多，甚至可能超越 AGI 帶來的所有其他變革。&lt;/p&gt; 
&lt;p&gt;長期來看，許多商品的價格將大幅下降（目前，智能成本和能源成本是許多行業的主要限制因素）。與此同時，奢侈品和一些稀缺資源（如土地）的價格可能反而會飆升。&lt;/p&gt; 
&lt;p&gt;從技術角度來看，AGI 的發展道路相對清晰。但如何將 AGI 融入社會，公共政策和社會共識將起到至關重要的作用。&lt;strong&gt;這也是我們不斷儘早、頻繁推出 AI 產品的原因之一——讓社會與技術共同演進，為未來做好準備。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;AI 將滲透到經濟和社會的方方面面，未來，我們會期待一切都變得智能化。面對這一趨勢，許多人認為應該給予個人更多對技術的控制權，比如開放源碼等措施，同時也要接受在安全性與個體賦權之間找到平衡，必然需要做出一些取捨。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;我們始終希望避免魯莽行事，未來在 AGI 安全性方面可能會做出一些不受歡迎的重要決策和限制。但總體而言，隨着 AGI 的逐步實現，我們認為更傾向於個體賦權是正確的方向。否則，我們可能會看到另一條道路。&lt;/p&gt; 
&lt;p&gt;確保 AGI 的廣泛受益，讓 AGI 的好處惠及全社會至關重要。從歷史來看，科技進步通常會改善健康狀況、經濟繁榮等關鍵指標，且長期來看整體趨勢是向好的。但技術本身不會自動帶來更大的平等，如果希望在社會公平方面做得更好，我們可能需要新的思維方式。&lt;/p&gt; 
&lt;p&gt;尤其值得關注的是，資本與勞動力之間的力量平衡可能會被打破，這可能需要及早幹預。&lt;/p&gt; 
&lt;p&gt;我們願意考慮一些聽起來不太尋常的想法，比如給每個人分配一定的「計算預算」（compute budget），讓全球所有人都能充分利用 AI。&lt;strong&gt;當然，也有一種更簡單的方法：持續降低智能計算的成本，讓人人都能負擔得起 AI。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;到 2035 年，每個人都應該能夠調用相當於 2025 年全人類智慧總和的智力資源。所有人都應當獲得近乎無限的智能支持，並自由地發揮想象力&lt;/strong&gt;。目前，世界上仍有大量人才因缺乏資源而無法充分發揮自己的潛力，如果我們改變這一點，全球的創造力將迎來爆發式增長，併為所有人帶來巨大的福祉。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333079/sam-altman-three-observations</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333079/sam-altman-three-observations</guid>
            <pubDate>Fri, 07 Feb 2025 03:14:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>被面試官拷問三個小時，應屆博士無緣 DeepSeek</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbaijiahao.baidu.com%2Fs%3Fid%3D1823558407486179899%26wfr%3Dspider%26for%3Dpc&quot; target=&quot;_blank&quot;&gt;據媒體報道&lt;/a&gt;&lt;/u&gt;，應聘者劉哲回憶起去年 5 月參加 DeepSeek 線上面試的經歷。那時，面試官連續 3 小時的高強度提問讓他倍感壓力。儘管他作為 211、985 高校的應屆博士生，在校期間已嶄露頭角，但面對那些深入且具有挑戰性的問題，他仍感到不小的難度。&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:#222222&quot;&gt;「最開始有一個 coding（編程）環節，會根據應聘者的專業出題，測試結果將決定應聘者是否能進入面試環節。面試持續 3 小時，由兩位面試官分別進行，每位面試官負責 1.5 小時。其中一位面試官會深入考察機器學習的基礎知識，連續提問 1.5 小時，據説所有應聘者都會被問到相同的題目，以便於他們進行比較和排名。接下來的 1.5 小時則是針對項目經驗的討論，他們特別關注應聘者在項目執行過程中的思考方式。」據劉哲所述，面試由 HR 主持，兩位面試官都很年輕，不超過 30 歲，整個面試過程體驗良好，能夠感受到團隊充滿青春活力。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-7ed8b4dc5acec727a286bfd36d21d80307e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;「在我所經歷過的互聯網公司中，&lt;strong&gt;DeepSeek 是唯一一家會根據應聘者的專業背景量身定製編程題目的公司。&lt;/strong&gt;」回顧面試經歷，劉哲這樣描述。在劉哲看來，DeepSeek 的崛起似乎是必然的。他透露，應聘者普遍來自清華、北大等頂尖學府，面試過程嚴謹且要求高，當時招聘並未設定人數上限，明顯感受到公司旨在網絡頂尖智慧人才，只招收天才級別的精英。&lt;/p&gt; 
&lt;p&gt;網絡上也有人在分享面試 DeepSeek 的經歷時表示遇到了出乎意料的問題。例如，面對「DPO 為什麼用 KL 散度,不用交叉熵?機器學習中什麼時候必須用 KL 散度，什麼時候必須用交叉熵,什麼時候兩者可互換」這樣的問題，有網友不禁感嘆：「這還是我能理解的中文嗎？」&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-f13afeea08b0fb9b3fe3418f8dd4256571b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;據知情人透露，有朋友曾參與 DeepSeek 的面試，並直接與創始人對話。總體感受是，公司充滿願景，洋溢着理想主義精神，研究氛圍優於高校實驗室，非常適合對 AI 充滿熱情的研究人員。&lt;/p&gt; 
&lt;p&gt;另外，一些參加過 DeepSeek 面試的人表示，公司不設 KPI 考覈，採取扁平化管理模式，每位核心算法人員都能直接與梁文峯探討問題，不太像傳統公司，更像大學的一個研究團隊。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333075</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333075</guid>
            <pubDate>Fri, 07 Feb 2025 03:04:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>全球開源大模型前十均為阿里通義千問衍生模型</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;近日，全球最大 AI 開源社區 Huggingface 發佈了最新的開源大模型榜單（Open LLM Leaderboard），其中榜單顯示，其排名前十的開源大模型全部是基於阿里通義千問（Qwen）開源模型二次訓練的衍生模型。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-5d2384f1f2035007cae119894ebe4235ed3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;來源：&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fspaces%2Fopen-llm-leaderboard%2Fopen_llm_leaderboard%23%2F&quot; target=&quot;_blank&quot;&gt;https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/&lt;/a&gt;&lt;/u&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;據悉，Open LLM Leaderboard 是目前全球最具權威性的開源大模型榜單，其測試維度涵蓋閲讀理解、邏輯推理、數學計算、事實問答等。&lt;/p&gt; 
&lt;p&gt;而通義千問 Qwen 大模型已經成為全球最大的開源模型族羣。在海內外開源社區中，Qwen 的衍生模型數量已突破 9 萬，超越美國 Meta 公司旗下的 Llama 系列開源模型，位居全球第一。在 Hugging face2024 年的開源模型下載中，Qwen 模型系列中的 Qwen2.5-1.5B-Instruct 的下載量佔總下載量的 26.6%，是全球下載量最高的開源模型。&lt;/p&gt; 
&lt;p&gt;此外，此前爆火的 DeepSeek 公司基於 R1 推理模型蒸餾了 6 個模型開源給社區，其中有 4 個模型來自 Qwen。近期，著名 AI 科學家李飛飛團隊用較少的資源和數據訓練出的 s1 推理模型，同樣以 Qwen 模型為基礎模型。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333071</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333071</guid>
            <pubDate>Fri, 07 Feb 2025 02:54:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>馬斯克欲以 7000 億收購 OpenAI，奧特曼回應：不如讓我們收購 X</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;2025 年 2 月 10 日（路透社）&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.reuters.com%2Fmarkets%2Fdeals%2Felon-musk-led-group-makes-974-billion-bid-control-openai-wsj-reports-2025-02-10%2F&quot; target=&quot;_blank&quot;&gt;報道稱&lt;/a&gt;&lt;/u&gt;，由埃隆·馬斯克（Elon Musk）領銜的財團週一表示，已經提出以 974 億美元（當前約 7,115 億元人民幣）收購 OpenAI 的運營資產。報道指出，這一舉動可能會對蓬勃發展的 AI 行業產生重大影響。數月前，這位億萬富翁曾起訴這家人工智能初創公司，試圖阻止其向營利性企業過渡。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-caff5ae16c7c36c9f1368d8366a9be892c9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;此次收購提議的背景是，OpenAI 首席執行官薩姆・阿爾特曼正在嘗試對公司進行重組，計劃將其非營利董事會與盈利業務分離。然而，目前尚不清楚阿爾特曼和非營利董事會是否已經就過渡價格達成一致。&lt;/p&gt; 
&lt;p&gt;對於這一消息，阿爾特曼在 X 平台上&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Felonmusk%2Fstatus%2F1889062013109703009&quot; target=&quot;_blank&quot;&gt;回應稱&lt;/a&gt;&lt;/u&gt;：「不用了，謝謝，但如果你願意，我們可以以 97.4 億美元的價格收購推特。」&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;516&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0211/105032_Gj7Z_2720166.png&quot; width=&quot;1280&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;馬斯克隨後還單獨發佈了&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Felonmusk%2Fstatus%2F1889070627908145538&quot; target=&quot;_blank&quot;&gt;推文&lt;/a&gt;&lt;/u&gt;，稱奧特曼是「Scam Altman（騙子奧特曼）」。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;1070&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0211/104848_aRfy_2720166.png&quot; width=&quot;1286&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333069</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333069</guid>
            <pubDate>Fri, 07 Feb 2025 02:50:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>字節跳動開源大語言模型應用開發框架 Eino</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;字節跳動技術團隊發文&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FJQREuZyI6ug3cc9Ov7diog&quot; target=&quot;_blank&quot;&gt;宣佈&lt;/a&gt;，基於 Golang 的大模型應用綜合開發框架 Eino 已正式開源，旨在提供簡潔、可擴展、可靠的開發工具。&lt;/p&gt; 
&lt;p&gt;據悉，Eino 基於明確的「組件」定義，提供強大的流程「編排」，覆蓋開發全流程，旨在幫助開發者以最快的速度實現最有深度的大模型應用。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-74d0fd011f9ebf4fd3950e2f6cc1b0f23bf.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;項目地址：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;https://github.com/cloudwego/eino&lt;/li&gt; 
 &lt;li&gt;https://github.com/cloudwego/eino-ext&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;字節跳動技術團隊介紹，Eino 作為旨在覆蓋 devops 全流程的大模型應用開發框架，具有內核穩定、API 簡單易懂、豐富的擴展性、提供開箱即用的配套工具等特點，能夠幫助開發者快速、簡單的上手。&lt;/p&gt; 
&lt;p&gt;Eino 已成為字節跳動內部大模型應用的首選全代碼開發框架，已有包括豆包、抖音、釦子等多條業務線、數百個服務接入使用。字節跳動表示，未來還將以 Eino 開源庫為核心代碼倉庫，堅持內外用一套代碼，與社區共建最優秀的大模型應用開發框架。&lt;/p&gt; 
&lt;p&gt;Eino&amp;nbsp;借鑑了 LangChain 和 LlamaIndex 等開源框架的優勢，並結合前沿研究，提供了一系列豐富的組件抽象，如 ChatModel、Tool、ChatTemplate 等，方便用戶組合開發。通過強大的編排框架（Graph、Chain），Eino 支持類型檢查、流式處理、併發管理等功能，簡化了開發流程。&lt;/p&gt; 
&lt;p&gt;Eino 框架結構圖：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0211/103841_1C2B_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;此外，Eino 提供了流式處理、回調機制、可視化開發工具等功能，幫助開發者高效構建 AI 應用。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333062</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333062</guid>
            <pubDate>Fri, 07 Feb 2025 02:38:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>外交部回應 DeepSeek 引發國際廣泛關注討論</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;在 2 月 10 日外交部例行記者會上，有記者提問稱，日前中國人工智能企業深度求索（DeepSeek）推出性能優越、免費商用的開源大模型，且訓練成本相較同類產品更低，在國際上引起廣泛關注和熱烈討論，請問發言人對此有何評論？&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;外交部發言人郭嘉昆表示：具體的專業問題建議向主管部門瞭解。我想強調的是，當前，人工智能的新技術不斷突破，新業態持續湧現，新應用加快拓展，已經成為新一輪科技革命和產業變革的重要驅動力量。中國積極擁抱智能變革，大力推進人工智能創新發展，重視人工智能安全，支持鼓勵企業自主創新，為全球人工智能發展作出了積極貢獻。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;中方積極推動人工智能普惠發展，幫助發展中國家加強能力建設，主張開源人工智能技術，促進人工智能服務的可及性，實現各國共享智能紅利。同時，我們反對以意識形態劃線，反對泛化國家安全概念、將經貿問題政治化的做法。中方願同各方加強人工智能交流合作，堅持以共商促共享，攜手打造開放包容、互利共贏的發展環境，共同在人工智能的廣闊天地裏深度求索。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333056</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333056</guid>
            <pubDate>Fri, 07 Feb 2025 01:51:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>操作教程丨使用 1Panel 開源面板快速部署 DeepSeek-R1</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;近期，DeepSeek-R1 模型因其在數學推理、代碼生成與自然語言推理等方面的優異表現而受到廣泛關注。作為能夠有效提升生產力的工具，許多個人和企業用戶都希望能在本地部署 DeepSeek-R1 模型。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;通過 1Panel 的應用商店能夠簡單、快速地在本地部署 DeepSeek-R1 模型。本教程將按照安裝 Ollama→安裝 OpenWebUI→安裝並使用 DeepSeek-R1 的順序，為您介紹使用 1Panel 開源面板部署 DeepSeek-R1 模型的具體步驟。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;在部署好 DeepSeek 之後，用戶也可以嘗試將其與 MaxKB 開源知識庫問答系統進行對接，構建一個自己的 Chatbox，也就是一個與 DeepSeek 大模型的智能會話界面。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;strong&gt;&lt;span style=&quot;color:#005eeb&quot;&gt;一、從 1Panel 應用商店安裝 Ollama&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;Ollama 是一個開源的本地大語言模型運行框架，專門為在本地便捷部署和運行大型語言模型而設計。為了能夠正常運行 DeepSeek-R1 模型，需要先在本地安裝 Ollama，本章節將介紹具體步驟。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;首先，從 1Panel 首頁進入「應用商店」，在「AI/大模型」分類下找到 Ollama，點擊「安裝」按鈕進行安裝。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt;
 &lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-3eb857757eb258cc4161d9b4046acdfe2e0.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;/div&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;在安裝詳情頁中，您可以自定義端口並勾選「端口外部訪問」選項，其他設置保持默認，最後點擊「確認」按鈕。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;987&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-0f1f981b2e3061d054c784539d621db3ab6.png&quot; width=&quot;1939&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;Ollama 安裝成功後，在 1Panel 操作界面中返回應用商店的「已安裝」標籤頁，看到 Ollama 已經安裝成功。此時點擊「服務端口:11434」按鈕，瀏覽器自動跳轉新標籤頁，若標籤頁內顯示「Ollama is running」，則表示 Ollama 已成功安裝並正常運行。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;987&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-dd4e30cec3cefda771ea15fb930c61b37ae.jpg&quot; width=&quot;1939&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;987&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-1fa60c450c2a377092f78c7c44a5a1467c0.png&quot; width=&quot;1939&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span style=&quot;color:#005eeb&quot;&gt;二、從 1Panel 應用商店安裝 OpenWebUI&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;由於 Ollama 本身沒有用戶交互界面，為了提升模型的使用體驗，我們需要從 1Panel 的應用商店中安裝 OpenWebUI。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;987&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-5e5456150b638b4dbe716ca3839f814339b.png&quot; width=&quot;979&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;OpenWebUI 是一款高度可擴展、功能豐富、操作便捷的自託管 AI 平台，它提供了一個更直觀的用戶界面，同時增強了安全性和擴展性。OpenWebUI 與 Ollama 相結合，能夠方便地管理和使用本地部署的大型語言模型。本章節將介紹安裝 OpenWebUI 的具體步驟。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;確認 Ollama 正常運行後，返回 1Panel 應用商店，在「AI/大模型」分類中找到 OpenWebUI，點擊「安裝」按鈕進行安裝。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;987&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-3d0545f49855958419fbb4aacff814f0760.jpg&quot; width=&quot;1939&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;在安裝詳情頁中，您需要填寫 Ollama 服務地址和 Secret Key，並勾選「端口外部訪問」選項，其他設置保持默認，最後點擊「確認」按鈕。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;987&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-9ed84ad322dc29653252b58c00a0abe586e.png&quot; width=&quot;1939&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;OpenWebUI 安裝成功後，返回 1Panel 應用商店「已安裝」標籤頁，點擊「服務端口:3000」按鈕，進入 OpenWebUI 控制枱（注意：如果瀏覽器未能顯示 OpenWebUI 控制枱頁面，請在 OpenWebUI 應用顯示的「已安裝」時間超過 5 分鐘以後再進行訪問）。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;987&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-9aa59370c5af4fd4ad89aec078704932e3a.png&quot; width=&quot;1939&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;在 OpenWebUI 控制枱中，依次設置「名稱」、「電子郵箱」和「密碼」，完成設置後，點擊「創建管理員賬號」按鈕。稍等片刻，頁面會彈出更新提示窗口，點擊「確認，開始使用！」按鈕，提示窗口消失後，即可開始使用 OpenWebUI。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;987&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-cfb617497b6a6c3ca0f93ce15df4f43edf8.png&quot; width=&quot;1939&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span style=&quot;color:#005eeb&quot;&gt;三、通過 OpenWebUI 安裝並使用 DeepSeek-R1&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;本章節將介紹如何通過 OpenWebUI 安裝和使用 DeepSeek-R1 模型。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;strong&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;1. 安裝 DeepSeek-R1 模型：&lt;/span&gt;&lt;/strong&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;返回至 OpenWebUI 控制枱頁面，點擊頁面左上角的「選擇一個模型」選項，在搜索框中輸入「deepseek-r1:1.5b」，然後點擊「從 Ollama.com 拉取 deepseek-r1:1.5b」選項。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;987&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-060b281d5425c3d9dd26e454ce843597c57.png&quot; width=&quot;1939&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;2. 使用 DeepSeek-R1 模型：&lt;/span&gt;&lt;/strong&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;模型下載完成後，刷新頁面，確認頁面左上角的模型顯示為「deepseek-r1:1.5b」。然後點擊屏幕中央的輸入框，就可以開始和 DeepSeek-R1 模型對話了。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;922&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-309adcc2cdecb9c8f9caaff84223ca49b0d.png&quot; width=&quot;1608&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;稍等片刻，收到回覆後即可確認 DeepSeek-R1 已通過 1Panel 成功部署到您的服務器。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;987&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-f5748142a990c5a72b8fb2d6db37312c833.png&quot; width=&quot;1939&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span style=&quot;color:#005eeb&quot;&gt;四、使用 GPU 為 DeepSeek- R1 加速&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;使用 GPU 為 DeepSeek-R1 加速可以顯著提升模型的推理速度。在本章節中，我們以 NVIDIA GPU 為例，介紹如何在 1Panel 中為 DeepSeek-R1 配置加速。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;strong&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;重要提示：在啓用 GPU 加速之前，請確保服務器已經安裝 GPU 卡並配置了相關驅動。&lt;/span&gt;&lt;/strong&gt;&lt;br&gt; &lt;span style=&quot;color:#3e3e3e&quot;&gt;進入 1Panel 應用商店的「已安裝」標籤頁，點擊 Ollama 應用下的「參數」按鈕。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;987&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-579e56d39751e426ecc172ba3966317a62a.png&quot; width=&quot;1939&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;點擊「編輯」按鈕進入參數配置頁面的「詳情」界面，勾選「高級設置」選項，並啓用「編輯 compose 文件」選項。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt;
 &lt;img src=&quot;https://oscimg.oschina.net/oscnet//47c9356c33a5e702b5a41a3de4be878b.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;/div&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;此時，在 compose 內容編輯框中，輸入以下與 GPU 相關的代碼：&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;reservations:
       devices:
              -  driver: nvidia
                  count: all
                  capabilities: [gpu]&lt;/code&gt;&lt;/pre&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;輸入完成後，點擊編輯頁面中的「確認」按鈕，Ollama 會自動重建。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;當重建完成後，Ollama 在 1Panel 應用商店中的狀態變更為「已啓動」，此時可以使用 NVIDIA GPU 為 DeepSeek-R1 提供加速。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;987&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-37ea6f8a8900338c209c5bbb3b25bcac4ac.png&quot; width=&quot;1939&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span style=&quot;color:#005eeb&quot;&gt;五、企業如何用好 DeepSeek？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;DeepSeek 部署完成後，該如何讓各個業務部門使用好這個能力超強的大模型呢？&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;這時候，MaxKB 開源知識庫問答系統就可以發揮積極作用了。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;1080&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-b3d7952c60ed617db87088d004d8cf9fa7b.jpg&quot; width=&quot;1920&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;首先，MaxKB 可以為本地部署的 DeepSeek 構建一個 Chatbox，也就是一個智能對話的界面，類似於個人用戶直接與 DeepSeek 進行對話。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;MaxKB 提供的 Chatbox 可以方便地嵌入到企業 OA 系統和業務系統，讓員工使用更加便捷、安全。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;987&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-ba43c99463a01d2e8219fa6a91eb22765b8.png&quot; width=&quot;1939&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;另一方面，企業內部有非常多的私有知識文檔，這些內容是經過長期的積累和不斷修訂形成的，在企業內部可以形成知識庫問答系統為企業的員工、合作伙伴和客戶提供服務。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;904&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-3762219074a17fe370af4ecb448ce1f2a4b.png&quot; width=&quot;947&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;MaxKB 提供開箱即用的 RAG（Retrieval-Augmented Generation，檢索增強生成）技術，能夠結合私有知識庫提升問答效果，有效降低大模型幻覺。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333030</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333030</guid>
            <pubDate>Thu, 06 Feb 2025 14:31:00 GMT</pubDate>
            <author>來源: 投稿</author>
        </item>
        <item>
            <title>Linux 內核新補丁調整 AC 電源插拔行為，向 Windows 看齊以提升硬件兼容性</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;blockquote&gt; 
 &lt;p&gt;AMD 工程師主導優化，解決便攜設備休眠喚醒痛點。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;近日，AMD 工程師 Mario Limonciello 向 Linux 內核&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flore.kernel.org%2Flinux-pm%2F20250208162210.3929473-1-superm1%40kernel.org%2F&quot; target=&quot;_blank&quot;&gt;提交了一系列補丁&lt;/a&gt;&lt;/u&gt;，旨在調整系統在&lt;strong&gt;s2idle（掛起到空閒）&lt;/strong&gt;狀態下的 AC 電源插拔行為，使其更貼近 Windows 11 的邏輯。&lt;/p&gt; 
&lt;p&gt;這一改動主要針對筆記本電腦、手持遊戲設備（如 Steam Deck 同類產品）在休眠時因電源狀態切換導致的兼容性問題，尤其是此前曝光的 Legion Go S（搭載 AMD Ryzen Z2 芯片）的固件級故障。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;為何需要「模仿」Windows？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;當前，Linux 與 Windows 在 s2idle 狀態下的電源行為存在關鍵差異：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;：插入或拔出 AC 電源時，系統會完全喚醒，若後續無用戶操作則重新進入睡眠。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt;：AC 事件僅觸發短暫喚醒後立即重回休眠，可能導致硬件固件因快速狀態切換出現異常（例如某些設備無法正確處理快速進入/退出低功耗模式）。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Limonciello 指出，由於 OEM 廠商通常基於 Windows 進行硬件驗證，Linux 的差異行為易暴露底層固件缺陷。新補丁通過記錄休眠前的電池狀態，並在 AC 事件後對比狀態變化，決定是否徹底喚醒系統，從而減少「兼容性陷阱」。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;技術細節：喚醒機制與能耗監控&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;喚醒邏輯重構&lt;/strong&gt;&lt;br&gt; 補丁在 ACPI 電池驅動中新增&lt;code&gt;suspend_state&lt;/code&gt;字段，休眠時保存當前電源狀態（如是否充電）。若喚醒後檢測到狀態變化（如從充電變為放電），則觸發系統完全喚醒，而非立即休眠。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;能耗統計透明化&lt;/strong&gt;&lt;br&gt; 新增&lt;code&gt;/sys/power/suspend_stats/last_sleep_energy&lt;/code&gt;文件，以&lt;strong&gt;毫安時（mAh）&lt;/strong&gt;為單位記錄上次休眠週期的電池消耗量，方便用戶空間工具分析功耗問題。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;爭議與用戶控制&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;儘管新行為默認啓用，但開發者社區對其適用場景存在分歧。例如，若筆記本合蓋時連接電源，是否應強制喚醒？Limonciello 認為，這與用戶外接擴展塢的場景需求一致，但用戶仍可通過禁用 ACPI 電池設備的&lt;code&gt;power/wakeup&lt;/code&gt;屬性恢復舊邏輯。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;影響與未來展望&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;此次調整尤其利好搭載 AMD 芯片的設備，但惠及所有支持 s2idle 的 x86/ARM 平台。隨着 Linux 在掌機市場的滲透（如 Steam OS 設備），此類優化將顯著提升用戶體驗。此外，補丁的「Windows 兼容性驅動」思路或成為未來硬件支持的新範式，減少廠商因生態差異對 Linux 的適配成本。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;結語&lt;/strong&gt;&lt;br&gt; Linux 在電源管理領域的「向 Windows 學習」，並非妥協，而是以用戶體驗為優先的務實選擇。這一補丁不僅修復了長期存在的兼容性痛點，也為開源生態與 OEM 廠商的協作提供了新思路。未來，類似「求同存異」的優化或成常態，進一步模糊兩大操作系統在硬件支持上的體驗邊界。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333017/linux-patches-ac-plug-s2idle</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333017/linux-patches-ac-plug-s2idle</guid>
            <pubDate>Thu, 06 Feb 2025 11:24:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>圖解系列｜DeepSeek-R1 的出眾推理能力從何而來？</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                    
                                                                                                                                                    &lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;編者按：&lt;/strong&gt; DeepSeek-R1 到底有什麼特別之處？它為什麼能在推理任務上取得如此出色的表現？這背後的訓練方法又蘊含着怎樣的創新？&lt;/p&gt; 
 &lt;p&gt;當我們需要模型處理數學題、編程任務，或是進行邏輯分析時，高質量的推理能力顯得尤為重要。然而，傳統的訓練方法往往需要耗費大量人力物力，這對許多研究團隊和企業來説都是不小的負擔。&lt;/p&gt; 
 &lt;p&gt;今天這篇深度解析 DeepSeek-R1 訓練方法的文章，將展示一個令人耳目一新的解決方案：如何通過創新的強化學習方法，在少量高質量人工標註數據的情況下，打造出一個推理能力出眾的 AI 模型。文章詳細介紹了 DeepSeek 團隊如何通過&quot;自動驗證機制&quot;來訓練模型，這種方法不僅大大降低了對人工標註數據的依賴，還能持續提升模型的推理質量。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;作者 | Jay Alammar&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;編譯 | 嶽揚&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-6fa463c0670c30f6fa2098b0a2cfb8cedbd.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;DeepSeek-R1 代表了人工智能發展的又一重要里程碑。對於機器學習領域的研究人員與開發者羣體而言，這次發佈之所以備受關注，主要有以下兩點：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;首先，這是一款開源權重的模型，並且提供了更小的、經過蒸餾的版本；&lt;/li&gt; 
 &lt;li&gt;其次，它公佈並深入探討了訓練方法，該方法能夠復現類似於 OpenAI O1 的推理模型。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;本文將帶您瞭解這一模型的構建過程。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;目錄&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;01 回顧：大語言模型（LLMs）的訓練方法&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;02 DeepSeek-R1 的訓練步驟&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2.1- 長推理鏈的 SFT 數據&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2.2- 一個過渡性的、擅長推理的高質量大語言模型（但在非推理任務上表現稍遜）&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2.3- 利用大規模強化學習（RL）構建推理模型&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2.3.1- 以推理為導向的大規模強化學習（R1-Zero）&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2.3.2- 利用過渡性推理模型生成 SFT 推理數據&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2.3.3- 常規強化學習訓練階段&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;03 模型架構&lt;/strong&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;01 回顧：大語言模型（LLMs）的訓練方法&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;與大多數現有的大語言模型一樣，DeepSeek-R1 也是逐個生成 token，但其獨特之處在於擅長解決數學和推理問題。這是因為它能夠通過生成一系列思考 tokens 來詳細闡述其思考過程，從而更加深入地處理問題。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-5b41809ec9dc436f27455aa39b8ef58c830.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;下圖摘自書籍《Hands-On Large Language Models》的第 12 章，展示了創建高質量大語言模型的三個主要步驟：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-5960076009014b645e62ad11df7e601f3dd.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;1）&lt;strong&gt;語言建模階段&lt;/strong&gt;，我們利用海量的網絡數據訓練模型預測下一個詞彙，從而得到一個基礎模型。&lt;/p&gt; 
&lt;p&gt;2）&lt;strong&gt;監督式微調階段&lt;/strong&gt;，這一步驟讓模型在執行指令和回答問題時更加得心應手，進而得到一個指令調優的模型或稱為監督式微調/SFT 模型。&lt;/p&gt; 
&lt;p&gt;3）最後是&lt;strong&gt;偏好調優階段&lt;/strong&gt;，這一步驟進一步優化模型的行為，使其更符合人類偏好，最終形成的是你在各種平台和應用中使用的偏好調優後的 LLM。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;02 DeepSeek-R1 的訓練步驟&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;DeepSeek-R1 遵循了這一通用框架。其第一步的具體內容源自於之前關於 DeepSeek-V3 模型的研究論文[1]。R1 使用的是該論文中的基礎模型（並非最終的 DeepSeek-V3 模型），並且同樣經歷了 SFT（監督式微調）和偏好調優階段，但它的獨特之處在於這些階段的具體操作方法。&lt;/p&gt; 
&lt;p&gt;在 R1 的構建過程中，有三個關鍵點值得特別關注。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;2.1 長推理鏈的 SFT 數據&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-71048a70e57a4dd98c33f2c0fb43d5a0d16.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;這些長思維鏈推理的實例數量龐大（總共達到 60 萬個）。如此大規模的實例獲取難度極高，且若要依靠人工標註，成本也將極為昂貴。&lt;/strong&gt; 因此，這些實例的創建過程是我們需要強調的第二個獨特之處。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;2.2 一個過渡性的、擅長推理的高質量 LLM（但在非推理任務上表現稍遜）&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;這些數據是由 R1 的前身，一個專注於推理但尚未命名的姊妹模型所生成的。這個姊妹模型受到了另一個模型 R1-Zero 的啓發（我們將在稍後討論）。它之所以意義重大，並不是因為它是一個非常好用的 LLM，而在於在它的創建過程中，幾乎無需依賴標註數據，僅通過大規模的強化學習，就能培育出一個擅長處理推理問題的模型。&lt;/p&gt; 
&lt;p&gt;接着，這個未命名的推理專家模型的輸出結果，可以用來訓練一個更為多能的模型，它不僅能夠處理推理任務，還能應對其他類型的任務，滿足用戶對大語言模型（LLM）的普遍期待。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-4851fe74d4b6fa9ff29c1036a1790de83f4.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;2.3 利用大規模強化學習（RL）構建推理模型&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;此處分為兩個步驟：&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;2.3.1- 以推理為導向的大規模強化學習（R1-Zero）&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;在此過程中，我們利用強化學習（RL）來構建一個臨時的推理模型。隨後，這個模型被用於生成用於監督式微調（SFT）的推理示例。然而，能夠創建這個模型的關鍵，在於之前的一項實驗，該實驗成功打造了一個名為 DeepSeek-R1-Zero 的早期模型。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-fbae4d4fb50b77fa5484a9d220719bbe4d6.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;R1-Zero 的獨特之處在於，它能夠在沒有經過標註的 SFT 訓練集的情況下，依然在推理任務上表現卓越。它的訓練過程直接從預訓練的基礎模型出發，通過強化學習訓練（跳過了 SFT 階段）。它的表現非常出色，能夠與 O1 模型相媲美。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-fd68b8120aaa63d61a0ca7bb0b7333d62ab.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;這一成就重要重大，因為數據一直是機器學習模型能力的助推器。那麼，這個模型是如何打破這一傳統的呢？這主要歸功於以下兩點：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;1- 現代基礎模型在質量和能力上已經達到了一個臨界點（這個基礎模型是在高達 14.8 萬億的高質量 tokens 上訓練而成的）。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2- 與通用聊天或寫作請求不同，推理問題可以實現自動驗證或標註。&lt;/strong&gt; 可以通過以下這個示例來説明這一點。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;示例：推理問題的自動驗證&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;以下是一個可能出現在 RL 訓練步驟中的提示詞/問題：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;編寫一段 Python 代碼，獲取一個數字列表，返回排序後的列表，並在列表開頭添加數字 42。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;這樣的問題非常適合自動驗證。假設我們將這個問題拋給正在訓練的模型，它會生成：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;使用軟件語法檢查器可以驗證生成的代碼是否為有效的 Python 代碼。&lt;/li&gt; 
 &lt;li&gt;我們可以運行這段 Python 代碼，以檢查其是否能夠成功執行。&lt;/li&gt; 
 &lt;li&gt;其他現代代碼生成 LLM 可以創建單元測試來驗證代碼的行為是否符合預期（它們自身無需具備推理能力）。&lt;/li&gt; 
 &lt;li&gt;我們甚至可以進一步，通過測量代碼的執行時間，讓訓練過程偏好那些性能更優的解決方案，即使其他解決方案也是正確的 Python 程序。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;在訓練步驟中，我們可以向模型提出這樣的問題，並生成多種可能的解決方案。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-f83bf0627d5f3ff8f1fda69f2a0769899e6.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;我們可以不依賴人工幹預，自動進行檢查，發現第一個輸出根本不是代碼。第二個輸出是代碼，但並非 Python 代碼。第三個輸出看似是一個解決方案，卻未能通過單元測試，而第四個輸出則是正確的解決方案。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-a1c61910f7a45c46610b945fcd73cf50a89.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;這些反饋都是可以直接用來優化模型的信號。這一過程當然是在大量示例（以小批量形式）和連續的訓練步驟中完成的。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-c2e60359299a142483ec274c460a0c90dc6.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;這些獎勵信號和模型更新是模型在強化學習訓練過程中不斷進步的關鍵，如下圖所示。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-ce02ceb2d7a3049768b4b796755b83f0f9f.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;與此能力提升相伴的是，模型生成了更長的響應，即使用了更多的思考 tokens 來處理問題。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-4c90ef53271c751b695ad334dddfbb87f40.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;儘管這個過程很有價值，但 R1-Zero 模型在推理問題上的高分表現背後，仍存在一些問題，使其實際可用性未達理想狀態。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;雖然 DeepSeek-R1-Zero 展現出了卓越的推理能力，並自主發展出了出人意料的強大推理行為，但它也遭遇了一些挑戰，比如文本可讀性不佳和語言混雜等問題。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;R1 模型的設計目標是提高可用性。因此，它（DeepSeek-R1-Zero）不僅僅完全依賴於強化學習過程，而是如前文所述，在以下兩個方面發揮作用：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;1- 創建一個過渡性的推理模型，用以生成監督式微調（SFT）的數據點。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2- 訓練 R1 模型，以在推理和非推理問題上取得進步（利用其他類型的驗證器）。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-694a728dfc22ac72182045659f53b114a1b.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;2.3.2- 利用過渡性推理模型生成 SFT 推理數據&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;為了提升過渡性推理模型的實際效用，我們對其進行了監督式微調（SFT）訓練，這一步驟在數千個推理問題示例上進行（部分示例由 R1-Zero 生成並篩選）。在論文中，這些示例被稱為&quot;冷啓動數據&quot;。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;2.3.1. 冷啓動階段&lt;/p&gt; 
 &lt;p&gt;與 DeepSeek-R1-Zero 不同，為了防止基礎模型在強化學習訓練初期出現不穩定的冷啓動問題，對於 DeepSeek-R1，我們構建並收集了少量長思維鏈（CoT）數據對模型進行微調，將其作為初始的強化學習策略模型。為收集這類數據，我們探索了多種方法：使用帶有長 CoT 示例的小樣本提示技術、直接提示模型生成帶有反思和驗證的詳細答案、收集 DeepSeek-R1-Zero 生成的易讀格式輸出，並通過人工標註員對結果進行後處理細化。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;但或許你會問，既然我們已經有了這些數據，為什麼還需要依賴強化學習過程呢？答案在於數據的規模。我們可以獲取的可能只有 5,000 個示例的數據集，而訓練 R1 則需要 600,000 個示例。&lt;/strong&gt; 這個過渡性模型幫助我們縮小了這一差距，並使我們能夠合成生成那些極為重要的數據。&lt;/p&gt; 
&lt;p&gt;對於監督式微調（SFT）這一概念，可能你還不太熟悉，它是一種訓練過程，通過向模型展示形式為提示詞和正確補全的訓練示例來進行。下面這個圖展示了書籍《Hands-On Large Language Models》第 12 章中的一些 SFT 訓練示例：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-e00bd1bb414511cf4a13d9225c9ed6bb7ba.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;2.3.3- 常規強化學習訓練階段&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;這樣，R1 模型不僅在推理任務上表現卓越，還能有效地應對其他非推理類任務。這一過程與我們之前提到的強化學習過程相似，但因為它涵蓋了非推理領域的應用，所以它還引入了一個實用性獎勵模型和安全性獎勵模型（與 Llama 模型有相似之處），用於處理這些應用領域的提示詞。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-cd6bf829caa63d1804a38dcdf71e88e2293.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;03 模型架構&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;與 GPT2[2] 和 GPT3[3] 等同源的早期模型一樣，DeepSeek-R1 也是由 Transformer[4] 解碼器塊堆疊而成，總共包含了 61 個這樣的塊。其中，前三個塊是密集層，而後續的則是採用了混合專家層（MoE）。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-817d8e03a6a9f616d918a3f53eb7e8bdede.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;關於模型的維度大小和其他超參數配置，具體信息如下：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-0acee698853f2545eaf2350f4bae0ca92ea.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;有關模型架構的更多詳細信息，可以在他們之前發表的兩篇論文中找到：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;DeepSeek-V3 Technical Report[1]&lt;/li&gt; 
 &lt;li&gt;DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models[5]&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;&lt;strong&gt;04 Conclusion&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;通過上述內容，相信你現在應該對 DeepSeek-R1 模型有了基本的理解。&lt;/p&gt; 
&lt;p&gt;如果你覺得需要更多基礎知識來理解這篇文章，我建議你獲取一本《Hands-On Large Language Models》[6]或者在線在 O&#39;Reilly[7] 上閲讀，並在 Github[8] 上查看相關內容。&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Thanks for reading!&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Hope you have enjoyed and learned new things from this blog!&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;strong&gt;About the author&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Jay Alammar&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Machine learning R&amp;amp;D. Builder. Writer. Visualizing artificial intelligence &amp;amp; machine learning one concept at a time. @CohereAI.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;END&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;本期互動內容 🍻&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;❓你覺得 AI 模型最難掌握的是哪種推理能力？歡迎在評論區分享你的觀點👇&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;🔗文中鏈接🔗&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;[1]&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2412.19437v1&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/pdf/2412.19437v1&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[2]&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fjalammar.github.io%2Fillustrated-gpt2%2F&quot; target=&quot;_blank&quot;&gt;https://jalammar.github.io/illustrated-gpt2/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[3]&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fjalammar.github.io%2Fhow-gpt3-works-visualizations-animations%2F&quot; target=&quot;_blank&quot;&gt;https://jalammar.github.io/how-gpt3-works-visualizations-animations/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[4]&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fjalammar.github.io%2Fillustrated-transformer%2F&quot; target=&quot;_blank&quot;&gt;https://jalammar.github.io/illustrated-transformer/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[5]&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2401.06066&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/pdf/2401.06066&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[6]&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.llm-book.com%2F&quot; target=&quot;_blank&quot;&gt;https://www.llm-book.com/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[7]&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flearning.oreilly.com%2Flibrary%2Fview%2Fhands-on-large-language%2F9781098150952%2F&quot; target=&quot;_blank&quot;&gt;https://learning.oreilly.com/library/view/hands-on-large-language/9781098150952/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[8]&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FhandsOnLLM%2FHands-On-Large-Language-Models&quot; target=&quot;_blank&quot;&gt;https://github.com/handsOnLLM/Hands-On-Large-Language-Models&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;原文鏈接：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnewsletter.languagemodels.co%2Fp%2Fthe-illustrated-deepseek-r1&quot; target=&quot;_blank&quot;&gt;https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1&lt;/a&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/IDP/blog/17553692</link>
            <guid isPermaLink="false">https://my.oschina.net/IDP/blog/17553692</guid>
            <pubDate>Thu, 06 Feb 2025 10:19:00 GMT</pubDate>
            <author>原創</author>
        </item>
        <item>
            <title>豆包開源視頻生成模型 VideoWorld</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FmXaktIsD3w5BgCJQb6R7xQ&quot; target=&quot;_blank&quot;&gt;據豆包大模型團隊官方公眾號消息&lt;/a&gt;&lt;/u&gt;，在北京交通大學和中國科學技術大學的聯合研究下，由豆包大模型團隊提出的 「VideoWorld」 視頻生成實驗模型近日正式開源。&lt;/p&gt; 
&lt;p&gt;據介紹，不同於 Sora 、DALL-E 、Midjourney 等主流多模態模型，&lt;strong&gt;VideoWorld 在業界首次實現無需依賴語言模型，即可認知世界&lt;/strong&gt;。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;論文鏈接：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2501.09781&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2501.09781&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;代碼鏈接：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fbytedance%2FVideoWorld&quot; target=&quot;_blank&quot;&gt;https://github.com/bytedance/VideoWorld&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;項目主頁：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmaverickren.github.io%2FVideoWorld.github.io&quot; target=&quot;_blank&quot;&gt;https://maverickren.github.io/VideoWorld.github.io&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;「VideoWorld」 通過分析和處理大量視頻數據，實現了複雜的推理、規劃和決策能力。研究團隊的實驗顯示，模型在僅有 300M 參數的情況下，便取得了顯著的效果。與現有依賴語言或標籤數據的模型不同，VideoWorld 能夠獨立進行知識學習，尤其在摺紙、打領結等複雜任務中，能夠提供更加直觀的學習方式。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-fc6aac365dbf1a8e0403b8bb24da8452019.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;為了驗證該模型的有效性，研究團隊搭建了圍棋對戰和機器人模擬操控兩種實驗環境。圍棋作為一項高度策略性遊戲，可以有效評估模型的規則學習和推理能力，而機器人任務則考察模型在控制和規劃方面的表現。在訓練階段，模型通過觀看大量視頻演示數據，逐步建立起對未來畫面的預測能力。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/332996</link>
            <guid isPermaLink="false">https://www.oschina.net/news/332996</guid>
            <pubDate>Thu, 06 Feb 2025 10:00:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>DeepSeek 服務站點大全</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;中國信息通信研究院去年 7 月 11 日發佈國內首個算力互聯公共服務平台，並聯合產業界開展算力互聯網共識共創行動。&lt;/p&gt; 
&lt;p&gt;該算力互聯公共服務平台是推進和管理全國算力互聯互通和算力互聯網體系的綜合服務平台，包括算力標識管理、算力互聯網業務查詢、算力統一大市場、政策和研究、標準體系、開源項目和運行監測等功能。&lt;/p&gt; 
&lt;p&gt;中國信通院今日宣佈，為便利國內 AI 開發者「找調用算力」需求，算力互聯公共服務平台宣佈增設全球雲服務商 DeepSeek 服務能力彙總功能頁面（截至 2 月 5 日已彙集 22 家服務商）。&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;地址：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fstateioc.cn%2Farticle-details%2FVjX&quot; target=&quot;_blank&quot;&gt;https://stateioc.cn/article-details/VjX&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;最後歡迎各位使用 Gitee AI —— Gitee AI 的 Serverless API 為您提供開箱即用的企業級的大模型 API 服務。&lt;/p&gt; 
&lt;p&gt;&lt;u&gt;&lt;em&gt;&lt;a href=&quot;https://ai.gitee.com/serverless-api&quot; target=&quot;_blank&quot;&gt;https://ai.gitee.com/serverless-api&lt;/a&gt;&lt;/em&gt;&lt;/u&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0210/174609_Xr4I_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/332995</link>
            <guid isPermaLink="false">https://www.oschina.net/news/332995</guid>
            <pubDate>Thu, 06 Feb 2025 09:46:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>ai.com 域名現已跳轉至 DeepSeek</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;現在在瀏覽器輸入&amp;nbsp;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fai.com%2F&quot; target=&quot;_blank&quot;&gt;ai.com&lt;/a&gt;，將直接重定向至 DeepSeek 官網 (&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fchat.deepseek.com%2F&quot; target=&quot;_blank&quot;&gt;https://chat.deepseek.com/&lt;/a&gt;)。&lt;/p&gt; 
&lt;p&gt;ai.com 域名的定位被視作前沿 AI 的象徵，此前這一域名曾長期跳轉至 ChatGPT、谷歌 Gemini 以及馬斯克的 xAI 官網。根據 Whois 數據，ai.com 域名註冊於 1993 年，有效期直至 2031 年 5 月，註冊聯繫人來自馬來西亞首都吉隆坡。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;219&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-8ceffc7eb4acfec05d9bcabc263bf478854.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/332978</link>
            <guid isPermaLink="false">https://www.oschina.net/news/332978</guid>
            <pubDate>Thu, 06 Feb 2025 08:30:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
    </channel>
</rss>