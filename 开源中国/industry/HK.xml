<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - industry - 繁體中文（香港）</title>
    <link>https://www.oschina.net/news/industry</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news/industry" rel="self" type="application/rss+xml"/>
    <description>已對該 RSS 進行格式化操作：中英字符之間插入空格、使用直角引號、標點符號修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-hk</language>
    <lastBuildDate>Wed, 11 Jun 2025 07:41:37 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>開源網盤應用 Alist 原開發者稱項目已交由公司運營</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;AList 是一款免費開源、支持多存儲的自建網盤程序 (文件列表程序)，可以輕鬆在 VPS 服務器、NAS、普通電腦 Win、Mac、Linux 上部署。它除了能作為一款自建網盤 (將文件保存在設備硬盤上) 外，最大的特色就是支持「掛載各大主流網盤」。&lt;/p&gt; 
&lt;p&gt;近日，有用户在該項目 GitHub 倉庫提交 issue，反饋官網出現 404 問題，並提出」項目是否被賣了」的疑問。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;img height="1184" src="https://static.oschina.net/uploads/space/2025/0611/150208_kinx_2720166.png" width="822" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FAlistGo%2Falist%2Fissues%2F8649" target="_blank"&gt;https://github.com/AlistGo/alist/issues/8649&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Alist 原開發者 Xhofe 今日在訂閲頻道發佈公告，&lt;strong&gt;稱項目已交由公司運營&lt;/strong&gt;，之後會幫助審查開源版本倉庫的代碼，確保 release 分支由 CI 自動構建。此外&amp;nbsp;main 分支已開啓分支保護，後續所有提交均需經過 PR 審核。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0611/145251_8h2m_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ft.me%2Falist_news%2F85" target="_blank"&gt;https://t.me/alist_news/85&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354817</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354817</guid>
      <pubDate>Wed, 11 Jun 2025 07:06:34 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>豆包大模型 1.6 發佈</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;火山引擎正式發佈了豆包大模型 1.6、豆包·視頻生成模型 Seedance 1.0 pro、豆包·語音播客模型。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0611/143623_6g0S_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;全新發布的豆包大模型 1.6 系列由三個模型組成：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;doubao-seed-1.6：All-in-One 的綜合模型，是國內首個支持 256K 上下文的思考模型，支持深度思考、多模態理解、圖形界面操作等多項能力。支持選擇開啓或關閉深度思考、自適應思考三種方式，其中自適應思考模式可根據提示詞難度自動決定是否開啓思考，提升效果的同時大幅減少 tokens 消耗。&lt;/li&gt; 
 &lt;li&gt;doubao-seed-1.6-thinking：豆包大模型 1.6 系列在深度思考方面的強化版本；在代碼、數學、邏輯推理等基礎能力上進一步提升；支持 256K 上下文。&lt;/li&gt; 
 &lt;li&gt;doubao-seed-1.6-flash：豆包大模型 1.6 系列的極速版本，支持深度思考、多模態理解、256K 上下文；延遲極低，TOPT 僅需 10ms；視覺理解能力比肩友商旗艦模型。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0611/143455_cA7e_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在價格方面，&lt;strong&gt;豆包大模型 1.6 採用統一定價模式，首創按「輸入長度」區間定價&lt;/strong&gt;，在企業使用最多的輸入區間 0-32K 範圍內，豆包大模型 1.6 的價格為輸入 0.8 元/百萬 tokens、輸出 8 元/百萬 tokens，綜合成本比豆包 1.5·深度思考模型、DeepSeek R1 降低 63%。&lt;/p&gt; 
&lt;p&gt;Seedance 1.0 pro 模型每千 tokens 0.015 元，相當於每生成一條 5 秒的 1080P 視頻只需 3.67 元，行業最低。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0611/143521_DcAP_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;豆包·實時語音模型已全量上線火山方舟，對企業客户開放使用。該模型支持自然語言高級指令控制，具備唱歌表演、聲線模仿、方言演繹等多種能力，語氣、用語、思考方式等擬人感大幅提升，能隨時打斷與主動搭話。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354815</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354815</guid>
      <pubDate>Sun, 11 May 2025 06:37:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Cline 提供為期兩週的免費 Grok-3 模型</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;xAI 與 AI 代碼工具開發商 Cline&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Fcline%2Fstatus%2F1932513639015329822"&gt;合作&lt;/a&gt;，為 Cline 用户提供為期兩週的 Grok 3 模型免費訪問權限。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-082d9ece19970284ff4cd71ee2adcb88880.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;用户只需註冊 Cline 賬户，即可在 Cline 的提供商中選擇並免費使用 x-ai/grok-3 模型進行編碼。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0611/142036_91S3_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Cline 是開源 AI 編程 Agent，以 VS Code 插件的形式提供，支持 Plan/Act 雙模式，具有終端執行能力和 Model Context Protocol (MCP) 特性。它能夠分析用户的項目文件結構、源代碼等，幫助用户創建和編輯文件、執行終端命令、使用瀏覽器進行測試等，還可以通過 MCP 協議擴展其功能，添加自定義工具。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354810</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354810</guid>
      <pubDate>Sun, 11 May 2025 06:22:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>3D 大模型公司 VAST 再獲數千萬美元融資</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;3D 大模型公司「VAST」宣佈再次完成數千萬美元的 Pre-A+輪融資，同時正式發佈了全球首個 AI 驅動的一站式 3D 工作台 Tripo Studio，並即將推出全新算法 Tripo 3.0。&lt;/p&gt; 
&lt;p&gt;據稱此次融資將重點投入 Tripo 系列大模型研發及 Tripo Studio 產品及生態平台建設，加速構建「AI+3D」全產業鏈條，打造「基礎模型 + 生態插件 + 原生工作台」的端到端產品體系，從而構建覆蓋專業級（PGC 生產者）、達人級（PUGC 創作者）到大眾級（UGC 用户）的創作者畫像完整梯度。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0611/114227_dFcn_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;據介紹，VAST 成立於 2023 年 3 月，是一家專注於通用 3D 大模型研發的 AI 公司，致力於通過打造大眾級 3D 內容創作工具建立 3D UGC 內容平台，使基於 3D 的空間成為用户體驗升級、內容表達創新和新質生產力提升的核心要素。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0402/185956_RSvr_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;自 2024 年初起，VAST 持續迭代 Tripo 大模型，先後推出 Tripo1.0 至 Tripo2.5 等數十億參數規模的 3D 大模型系列，同時發佈 TripoSR、TripoSG、TripoSF 等廣受全球開源社區認可的 3D 基礎模型，並配套開發了系列 3D 軟件生態插件。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;相關閲讀&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.oschina.net/news/345674/vast-opensource-unirig" target="news"&gt;生成式 3D AI 公司 VAST 最新開源：通用自動骨骼綁定框架 UniRig&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.oschina.net/news/342506" target="news"&gt;生成式 3D AI 公司 VAST 開源基礎 3D 生成模型 TripoSG 和 TripoSF&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354785</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354785</guid>
      <pubDate>Sun, 11 May 2025 03:45:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>百度百舸萬卡集羣的訓練穩定性系統設計和實踐</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;h1&gt;01 AI 訓練穩定性的演進歷程&lt;/h1&gt; 
&lt;p&gt;2012 年 ImageNet 競賽中 AlexNet 的橫空出世，開啓了現代 AI 發展的新紀元。彼時我們不會想到，十年後支撐 AI 訓練的 GPU 集羣會從研究室裏的幾台服務器，發展成需要專門供電系統的萬卡級計算矩陣。在這個算力爆發式增長的過程中，訓練系統的穩定性管理正經歷着從「簡單運維」到「精密工程」的深刻變革。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;1.1 標早期的小模型時代：手動運維的黃金年代&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;2022 年之前的 AI 訓練，更像是手工作坊式的精雕細琢。大多數訓練任務只需十幾塊 GPU，利用 PyTorch 或 TensorFlow 的數據並行功能就能輕鬆應對。記得那時算法工程師們有個共識：如果訓練遇到問題，重啓往往比排查更高效。&lt;/p&gt; 
&lt;p&gt;當時我們構建的監控系統就像汽車儀表盤，只能顯示最基本的任務狀態。當訓練意外中斷時，工程師們會像偵探一樣翻查日誌 —— 如果發現是 GPU 報錯，就聯繫運維同事。運維人員則帶着「NVIDIA 三件套」（nvidia-smi、dcgm、nsys）到機房巡檢，像老中醫把脈般通過温度、功耗等指標判斷硬件狀態。這種工作模式雖簡單，但應對數十卡規模的集羣還算遊刃有餘。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;1.2&lt;/strong&gt; &lt;strong&gt;大模型風暴：從量變到質變的衝擊&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;ChatGPT 的登場如同打開潘多拉魔盒，將 AI 訓練帶入新的紀元。當我們開始部署千卡/萬卡集羣時，才發現原有的運維體系就像用小漁網捕鯨魚 —— 完全無法匹配新需求。&lt;/p&gt; 
&lt;p&gt;讓我們通過百度百舸經歷過的一個真實案例來深入理解這個問題：&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;2024 年初，百度百舸幫助一家 AIGC 創業公司迅速將其訓練規模從百卡擴展到千卡級別。然而在訓練數天後的某個週末凌晨，訓練進程意外發生了 hang 死。由於當時缺乏有效的故障感知和容錯機制，直到第二天算法工程師發現任務超時退出時，已經耽誤了數小時寶貴的訓練時間。更糟糕的是，任務日誌中除了簡單的 timeout 報錯外毫無線索，平台監控也顯示所有訓練節點狀態正常。&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;着急恢復訓練的算法工程師沒有立即上報問題，而是選擇直接重新提交任務。但不幸的是，新任務運行數小時後再次出現相同的超時退出。這時他們才不得不尋求技術支持，但值班工程師面對這種任務 hang 死的問題也缺乏診斷經驗，只能通過二分法慢慢定位。最終發現是某個節點的靜默故障（SDC）導致了訓練進程假死。等問題得到解決時，距離首次故障已經過去將近 30 小時，這意味着損失了價值巨大的千卡算力資源。&lt;/em&gt;&lt;/p&gt; 
&lt;h1&gt;02 百度百舸集羣訓練穩定性全景圖&lt;/h1&gt; 
&lt;p&gt;站在現在的時間點回望，AI 訓練穩定性已從輔助功能演變為核心基礎設施。就像現代建築中的抗震結構，它雖不直接參與空間構成，卻是萬丈高樓得以屹立的關鍵。當行業向着數萬卡集羣邁進時，這套隱形護甲的質量，將直接決定 AI 進化的速度與邊界。&lt;/p&gt; 
&lt;p&gt;在 2024 年百度百舸對訓練過程的生命週期進行了更細緻的拆分，提出了「無效訓練時間」這一關鍵指標，並致力於將其最小化。具體來説：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;任務無效訓練時間 = 故障中斷次數 × 任務故障恢復時長 + 任務常態寫 Ckpt 總時長&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;其中，任務故障恢復時長 = 故障感知召回耗時（自動/人工定位）+ 任務調度耗時 + 任務初始化耗時 + 任務重算時長。&lt;/p&gt; 
&lt;p&gt;通過這個公式可以看出，要降低無效訓練時間，需要「圍繞基礎設施穩定性」、「任務容錯」兩個維度來系統展開，重點解決三個方面的問題：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;提高基礎設施的交付質量。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;提高任務故障容錯的召回率、準確率和時效性。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;優化 checkpoint 機制，減少保存時間和恢復時的重算時間。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;經過容錯架構的整體變革，百度百舸形成了從 「任務負載 —&amp;nbsp;框架 —&amp;nbsp;通信&amp;nbsp;—&amp;nbsp;基礎架構」全鏈路的自動異常感知、診斷、恢復能力，可覆蓋 90%+ 的訓練異常場景，時效性最快可以實現秒級異常感知、分鐘級定位，以及平均 3 分鐘的故障自愈能力。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-5a9c915ac6d0cd3d443262a00768d1aeebb.webp" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h1&gt;03 基礎設施交付質量保障&lt;/h1&gt; 
&lt;p&gt;基礎設施的交付質量保障是穩定性的基礎。&lt;/p&gt; 
&lt;p&gt;CPU 時代，機器的交付前可能僅會跑一些常規的 CPU 計算、網絡的壓力測試，並不會從業務視角去評估基礎架構，機器交付後硬件異常的故障頻率相對較少。有硬件故障時，通常走工單系統人工換機用户相對是可接受的。&lt;/p&gt; 
&lt;p&gt;而 GPU 時代，AI Infra 的交付則需要考慮 CPU、GPU、RDMA 網絡、存儲，甚至機房的功率、温度等各方面因素，遺漏任何一個環節都會成為後續穩定性的隱患。在交付給客户後，機器也可能會由於長時間的高負載運行頻繁出現硬件故障，而 GPU 機器的高昂成本，使客户對節點故障感知、換機的時效性提出了非常高的要求。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-08b3f03a6cfd9c28b1a6a2f175eb081f37f.webp" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;因此百度百舸對 GPU 機器交付前及交付後的穩定性質量進行了系統性管理：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;交付前，百度百舸會對機器進行 200 多項指標檢測，然後進行 48 小時烤機，以及 NCCL-Test 的機內、機間的大環、同號卡通信性能基準測試，端到端的大模型訓練、推理性能基準測試。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;交付後，需要能夠實時的感知節點故障及定期巡檢，並具備分級處理的自愈能力，例如 Error 級別的故障實現自動排水、重啓，Fault 級別故障實現自動換機。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;04 任務容錯的準召率保障&lt;/h1&gt; 
&lt;p&gt;任務層面穩定性最核心的就是做好容錯，能夠讓業務在無論遇到何種故障時都能快速恢復。&lt;/p&gt; 
&lt;p&gt;那麼，首要的工作就是我們能夠準確的識別出異常，然後對故障進行診斷定位，最後能夠自動化的從異常中恢復。&lt;/p&gt; 
&lt;p&gt;因此，任務容錯需要能夠從端側（即每個訓練 worker）探測到進程與環境的各類異常，同時有個中心服務（Master）從任務全局的視角去診斷、定位異常，最終做出相應的決策來使任務能夠快速從異常中恢復。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-bede84ad1e729a350a1750dbfde88615822.webp" alt="圖片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;任務容錯最重要的就是提升故障的召回率與準確率，即如何能夠儘可能的準確識別、定位所有故障。我們將故障分類兩類：顯式故障和隱式故障。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;顯式的故障通常比較容易召回，我們將實踐積累的各種進程異常狀態及各類報錯 pattern 形成專家知識庫，再結合硬件感知服務（HAS Agent）的硬件全鏈路 10 秒級監控能力，可以實現顯式故障的召回率達到 95%+。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;隱式的異常則往往很難輕易的識別，例如訓練進程 hang、慢節點就是典型的隱式故障，需要豐富的經驗積累才能準確的識別出異常。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;下面我們就以最典型的隱式故障場景 —— 訓練進程 hang 死為例，來看下如何能夠做好 hang 自動感知、診斷。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;4.1 訓練****hang 的自動感知&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;訓練任務發⽣ hang 之後，絕⼤多數情況都會以 timeout 的⽅式報錯並退出進程，最常⻅的就是在通信過程中如果發⽣ hang，NCCL 的 watchdog 會中斷通信，並有報如下 timeout 報錯，然後再由 pytorch 的 torchrun 進程感知並中斷訓練過程。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;[E ProcessGroupNCCL.cpp:828] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=15173, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1802710 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:828] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=15173, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1802713 milliseconds before timing out.



&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Pytorch 默認為 10 分鐘 NCCL 通信超時，而 Megatron-LM 為 30 分鐘。在萬卡規模訓練場景中，意味着一萬張卡要至少浪費 30 分鐘才能被發現。這個時效性是不可接受的。而且當 30 分鐘超時後程序會立馬退出，很難有機會進行下一步定位，需要一些時效性更高的感知機制，並且在程序退出前獲取一些有效信息供後續診斷分析。&lt;/p&gt; 
&lt;p&gt;很多公司、實驗室在面對 hang 的問題時，會在採用框架層插樁的方式來 trace 訓練進程，這種方式通常是比較直接且準確的，但是有比較強的侵入性，而且可能還會有一些性能開銷。對於雲廠商來説，需要尋找對用户更透明、更無損的方式來感知、定位 hang 異常。&lt;/p&gt; 
&lt;p&gt;如何感知訓練 hang，以百度百舸的產品設計思路為例，我們可以從以下幾個方向去思考：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;訓練進程 hang 的最直觀表現是什麼？&lt;/p&gt; &lt;p&gt;人工判斷一個任務是否 hang 了，最直接的方式就是看是否所有 worker 的任務日誌一段時間內都不輸出日誌了，所以 hang 自動感知的第一種方法就是採集所有 worker 的日誌，並判斷所有 worker 日誌中最後一行日誌是否為 x 分鐘前的（x 小於 Pytorch 的通信超時時間，例如 8 分鐘），如果是則基本可以判定為 hang。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;任務 hang 時進程有什麼樣的表現？&lt;/p&gt; &lt;p&gt;任務 hang 時，可能進程的調用棧都不在發生變化，進程的調用棧可以通過 py-spy/pystack 等工具進行探測，所以我們可以用此類工具對所有訓練任務進行一個定時採樣，當採集 n 個樣本所有進程棧都沒有變化時，可以判定一次 hang，這種方式通常可以將 hang 感知縮小至 3～5 分鐘。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;任務 hang 時監控指標有哪些變化？&lt;/p&gt; &lt;p&gt;訓練進程中的 CUDA 算子計算、集合通信操作通常都是在毫秒，甚至微秒、納秒內完成的，當任務在正常迭代過程中發生了 hang，我們常遇到的情況是所有 rank 的 RDMA 流量會降到 0，而 GPU 的利用率為 100%、SM 利用率則在很低的水位。如果持續幾分鐘都是這種狀態時，意味着訓練進程已經計算完成，在等着集合通信完成，這種情況下基本可以判定為 hang。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;是否能在通信庫中更快的感知通信 hang？&lt;/p&gt; &lt;p&gt;通常單次集合通信操作都是在 ms 級的，如果一次操作在 30 秒鐘都沒有完成，那就可以判定為通信 hang 死了。百度自研的 BCCL 集合通信庫層可以對每一次集合通信操作都進行打點，來實現通信 hang 感知。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;上述幾種方法，我們可以分別實現一種探針，來抓取相應的特徵到中心端 master 組件進行下一步診斷和容錯決策。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;百度集合通信庫 BCCL 是百度智能雲推出的一款面向大模型訓練場景優化的集合通信庫。&lt;/p&gt; 
 &lt;p&gt;BCCL 基於開源的 NCCL 進行了功能擴展和能力增強，針對大模型訓練場景在可觀測性、故障診斷、穩定性等方面進行優化，進一步提升集合通信庫的可運維能力。相比 NCCL，BCCL 的關鍵特性如下：&lt;/p&gt; 
 &lt;p&gt;*&amp;nbsp;可觀測性：新增集合通信帶寬實時統計能力；&lt;/p&gt; 
 &lt;p&gt;*&amp;nbsp;故障診斷：新增集合通信 hang 時的故障診斷能力；&lt;/p&gt; 
 &lt;p&gt;*&amp;nbsp;穩定性：增強網絡穩定性和故障容錯能力；&lt;/p&gt; 
 &lt;p&gt;*&amp;nbsp;性能優化：提升大模型訓練主流 GPU 芯片的集合通信性能。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;&lt;strong&gt;4.2&lt;/strong&gt; &lt;strong&gt;訓練 hang 的自動診斷&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;有了以上感知手段，我們需要進一步的診斷、定位，來確定是否真的發生了 hang，以及 hang 的具體位置。具體的來講，master 收集到各類 agent 的數據後，會做一些綜合分析：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;是否真的發生了 hang？&lt;/p&gt; &lt;p&gt;感知階段各種探針只能探測到 hang 的一種特徵，並沒有辦法 100% 的確定是否真的 hang 住了，事實上不侵入用户進程是很難做到 100% 確定 hang 的。因此，為了提高 hang 的判定準確率，我們需要將各種特種綜合起來判斷，探針上報到 master 後，由一個 hang 診斷模塊，按照一個時間窗口（例如 5 分鐘），進行綜合判斷。如果在時間窗口內日誌、監控、進程調用棧、通信庫中有 2 條以上都處於不處於活躍狀態時，我們判斷任務真正發生了 hang。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;hang 的具體發生的位置？&lt;/p&gt; &lt;p&gt;確定任務 hang 了之後，我們需要找到 hang 所在的節點來對它進行隔離。因此診斷模塊需要在探針上報的數據中進一步找尋特徵，來確定 hang 發生的位置：&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;BCCL Tracehang 診斷：在感知階段，BCCL 可以在通信庫層面對所有 rank 的通信進行打點。如果有節點一直未完成通信則是發生了 hang。但是此節點可能並非真正發生 hang 的源頭，有可能是在等待其他節點完成通信。診斷模塊可以根據 BCCL 打印的通信組信息，進行交叉判斷，如果某個節點在多個通信組中都未完成通信，那這個節點就是 hang 的源頭。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;RDMA/GPU 指標診斷：上文中我們提到，通信階段發生 hang 之後，所有 rank 的 RDMA 流量都會降到 0，而同時絕大部分 rank 的 GPU 利用率持續為 100%，只有某一兩個 rank 的 GPU 利用率為 0，那這個 rank 很有可能是 hang 的源頭。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;進程調用棧診斷：進程調用棧也可以作為一個 hang 源頭診斷的重要參考。當發生 hang 之後，絕大部分的 rank 都要麼處於 barrier 等待狀態，要麼處於通信等待階段。只有個別的 rank 卡在其他函數上，那麼通過對比分析，可以將調用棧與其他 rank 不同的節點初步判定為 hang 的源頭。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;綜合診斷：上面 3 種特徵為我們提供了 hang 的診斷依據，將 3 者關聯起來分析後，我們基本上可以比較準確的確定一個具體的 hang 的源頭，再結合硬件故障感知的相關信息可以進一步明確根因。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;&lt;strong&gt;4.3&lt;/strong&gt; &lt;strong&gt;基於 eBPF 的隱式故障感知與診斷&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;在複雜的大規模分佈式訓練場景中，傳統用户態監控往往難以捕獲系統內核層面的異常事件。&lt;/p&gt; 
&lt;p&gt;百度百舸基於 eBPF（Extended Berkeley Packet Filter）技術的隱式故障感知體系，能夠在不侵入用户代碼的前提下，對訓練進程的系統調用、網絡通信、CPU 調度等內核態行為以及訓練框架關鍵函數運行時間建立立體觀測能力。&lt;/p&gt; 
&lt;p&gt;eBPF 探針部署原理通過在內核關鍵路徑注入輕量級探針，實現低開銷的系統級行為捕獲。針對訓練場景特點，主要聚焦 4 類事件跟蹤：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;訓練關鍵函數跟蹤：微秒級跟蹤訓練過程中，前向計算、反向計算、集合通信操作等關鍵函數執行耗時，記錄函數間調用關係。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;進程調度阻塞跟蹤：掛鈎 sched_switch 事件，檢測進程在 TASK_UNINTERRUPTIBLE 狀態持續時間，當單次持續超過閾值（如 5 秒）時捕獲調用棧。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;CUDA 運行時 API 監控：通過 uprobe 在 &lt;a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Flibcuda.so" target="_blank"&gt;libcuda.so&lt;/a&gt; 等關鍵庫注入探針，記錄 CUDA API 調用耗時分佈。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;RDMA Verbs 級通信監控：在 ibv_post_send/ibv_poll_cq 等核心通信接口設置觀測點，統計通信時延分佈。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;結合上面 4 類事件，完成以下 2 類數據分析：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;單體異常探測基線與實時數據對比。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;羣體一致性檢測。採用卡間對比算法，當某一 rank 的以下指標偏離集羣中位數超過閾值時判定異常，包括系統調用頻率、進程就緒隊列等待時長、NVLink/RDMA 帶寬利用率等。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;基於以上所述方法，百度百舸針對以下 2 類典型的隱式故障進行診斷：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;訓練 hang 根因定位。通過關聯 eBPF 捕獲的多維度數據進行如下操作：&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;當檢測到某 rank 的 GPU &amp;nbsp;Kernel 執行出現分鐘級空跑（SM 利用率 &amp;gt; 70% 但無有效計算輸出）。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;同時伴隨該節點 RDMA QP 狀態停滯（ibv_poll_cq 無新完成事件）。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;內核調度器顯示進程處於 D 狀態超過閾值。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;性能抖動溯源。基於 eBPF 火焰圖、時序圖等進行分析：&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;抓取發生性能下降時段的 CPU on-cpu/off-cpu 堆棧。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;對比正常時段數據，識別出異常的鎖競爭（futex 調用佔比上升）。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;結合 NUMA 內存訪問統計，定位跨 NUMA 內存訪問導致的 TLB 顛簸問題。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;此類技術已在百度百舸的萬卡規模訓練集羣中驗證，相比單純依賴應用層監控的方案，將隱式故障的平均檢測時間從分鐘級縮短至秒級，診斷準確率提升 40% 以上。&lt;/p&gt; 
&lt;p&gt;通過與既有硬件故障感知服務、BCCL 通信庫監測體系聯動，百度百舸形成了覆蓋從硬件到系統內核再到應用層的立體化診斷能力。&lt;/p&gt; 
&lt;h1&gt;05 任務故障恢復的時效性保障&lt;/h1&gt; 
&lt;p&gt;故障恢復的時效性也是容錯能力的一個重要指標，反映的是任務從故障發生到再次重新進入訓練迭代的時間，恢復效率越高則算力浪費越少。影響到任務恢復效率有 2 個重要因素，一是任務平均中斷時間，二是訓練重算時間。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;5.1&lt;/strong&gt; &lt;strong&gt;多級重啓策略減少故障中斷時間&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;任務發生異常後，上文中我們提到需要經過故障自動感知、診斷和自愈等 3 個環節，那麼減少中斷時間的核心思想，就是儘可能的縮短這 3 個環節的時間，通過多維度的感知、診斷手段可以將故障發現、定位的時效性降低至分鐘級甚至秒級。自愈則需要能夠根據不同的診斷結果進行分級恢復和故障屏蔽的能力：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;單點顯式故障：重調度異常節點（replace），對節點進行集羣級別屏蔽。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;單點隱式故障：重調度異常節點，對節點進行任務級別屏蔽。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;非單點故障：原地重啓嘗試恢復（restart），無法恢復時重新調度所有節點（resubmit）。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;通過多級重啓策略，儘可能避免單點故障引發全部節點的重新調度。在萬卡級別的訓練場景中，百度百舸將大部分訓練異常場景恢復時間從過去的 30min 縮短至現在的 30s 內，成功率到 95%+。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;5.2&lt;/strong&gt; &lt;strong&gt;觸發式 checkpoint 減少訓練重算時間&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;除了上述的多級任務重啓策略外，另一個提高任務故障恢復效率的重要手段就是減少訓練重算時間。在探討具體技術方案之前，我們先來看看目前主流的 checkpoint 保存策略。&lt;/p&gt; 
&lt;p&gt;傳統的 checkpoint 保存通常採用固定間隔策略，比如每隔 N 個 step 或每隔 T 小時保存一次，這種方式實現簡單但缺乏靈活性，可能會產生大量冗餘存儲，同時在故障發生時可能會損失較多訓練進度。&lt;/p&gt; 
&lt;p&gt;而觸發式 checkpoint 則是一種更智能的方案，它根據特定條件或異常事件（如故障、顯存不足、顯式指令等）動態觸發模型狀態保存。其核心目標是通過靈活的控制保存時機，減少不必要的存儲開銷和訓練中斷時間，從而降低因頻繁或冗餘保存導致的重算時間浪費。&lt;/p&gt; 
&lt;p&gt;隨着大模型訓練規模的擴大，還有一種更激進的「零重複 checkpoint」技術，即在每個訓練 step 都保存一次 checkpoint。這種方案的優勢在於可以將重算時間降到最低，確保故障發生時能夠從最近的 step 恢復，幾乎不會損失訓練進度。但其顯著的缺點是存儲開銷巨大，即使採用增量式存儲，仍然需要相當大的存儲空間和 I/O 帶寬。此外，頻繁的 checkpoint 操作也可能影響訓練性能。&lt;/p&gt; 
&lt;p&gt;相比之下，觸發式 checkpoint 走的是一條平衡之路。我們來看下它實現的幾個核心要點：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;集成容錯：訓練進程集成容錯的故障感知與定位機制，在進程退出前自動觸發保存。這種主動感知機制能夠在故障發生的第一時間保存訓練狀態，最大限度減少進度損失。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;高速轉儲：異步 checkpoint 保存機制會將 checkpoint 暫存到共享內存中，再由外部程序轉儲至磁盤。當某個節點異常時，容錯組件會拉起新節點，並在新節點訓練進程啓動前，利用 RDMA 技術實現 checkpoint 快速從故障節點轉儲至新節點，這大大減少了從遠程存儲拉取 checkpoint 的時間。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;冗餘備份：觸發式 checkpoint 也並非完美無缺，例如在節點發生內核 crash 等嚴重故障時，可能無法觸發自動保存。因此，需要通過定期的冗餘備份機制進行兜底，確保 checkpoint 不會完全丟失。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;實踐表明，當觸發式 checkpoint 與異步、增量式的 checkpoint 機制結合使用時，可以在保證數據安全性的同時，顯著提高 checkpoint 保存效率，減少訓練重算時間。&lt;/p&gt; 
&lt;p&gt;相比零重複 checkpoint 的重型方案，觸發式 checkpoint 提供了一個更實用的折中方案，在合理的存儲開銷下實現較好的容錯效果。當然，具體選擇哪種方案，還需要根據實際的訓練規模、硬件條件和可用資源來權衡。&lt;/p&gt; 
&lt;p&gt;隨着分佈式訓練規模的持續增長，相信未來會出現更多創新的 checkpoint 方案，比如基於預測的主動保存策略、多級存儲架構的智能調度等，這些都將為提高大規模訓練的可靠性提供新的可能。&lt;/p&gt; 
&lt;h1&gt;06 業務發展對穩定性的要求&lt;/h1&gt; 
&lt;p&gt;AI 訓練的穩定性管理已經演變為智能時代的精密工程。從最初靠人工重啓解決問題的摸索階段，到如今能自動感知異常、快速恢復的智能系統，每一次進步都映照着算力規模的跨越式發展。&lt;/p&gt; 
&lt;p&gt;讓人不禁思考，在未來十萬卡集羣的算力洪流中，或許會出現更精妙的動態平衡方案：既能像鷹隼般敏鋭捕捉故障徵兆，又能如雁羣遷移般智能調度資源，在秒級恢復與 PB 級存儲成本之間找到新的平衡支點。&lt;/p&gt; 
&lt;p&gt;目前百度百舸支持廠內千卡和萬卡集羣有效訓練時長已經可達 99.5%，為客户大模型的預訓練保駕護航，比如國內第一個數學大模型——九章算術，國內第一個類 Sora 大模型 —— Vidu 等。&lt;/p&gt; 
&lt;p&gt;----------END----------&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;推薦閲讀&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247604282%26idx%3D1%26sn%3Dbf4ca5dcc5420b035888229cb177c562%26scene%3D21%23wechat_redirect" target="_blank"&gt;LLM 增強語義嵌入的模型算法綜述&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247604236%26idx%3D1%26sn%3D1b8ff1181ea3dc12ede0b0e849f009c6%26scene%3D21%23wechat_redirect" target="_blank"&gt;持續推進「人工智能＋」行動，百度智能雲+DeepSeek 為何成為國有企業首選？&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247604214%26idx%3D1%26sn%3D71c43bcfd51b145fc769c15539570307%26scene%3D21%23wechat_redirect" target="_blank"&gt;GPU 雲服務器的軟件系統設計和實踐&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247604202%26idx%3D1%26sn%3D68fea54ea6869f0bf6d7cd67c11943a6%26scene%3D21%23wechat_redirect" target="_blank"&gt;基於 Flink 的配置化實時反作弊系統&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247604182%26idx%3D1%26sn%3D224203a0b523de10d3b6365d9a3a0aa5%26scene%3D21%23wechat_redirect" target="_blank"&gt;百度智能雲 xDeepSeek，最具性價比的 DeepSeek 一體機合集來了！&lt;/a&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/4939618/blog/17935991</link>
      <guid isPermaLink="false">https://my.oschina.net/u/4939618/blog/17935991</guid>
      <pubDate>Sun, 11 May 2025 03:02:00 GMT</pubDate>
      <author>原創</author>
    </item>
    <item>
      <title>Hugging Face 發佈開放權重模型貢獻榜：Qwen 與 DeepSeek 躋身 TOP15</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;Hugging Face 近日&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fspaces%2Fcfahlgren1%2Fmodel-release-heatmap" target="_blank"&gt;發佈&lt;/a&gt;開放權重模型貢獻榜，中國團隊 Qwen 和 DeepSeek 成功入圍前 15 名。該榜單表彰為開源社區提供高質量模型權重的團隊，其模型廣泛應用於學術與產業創新。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="304" src="https://oscimg.oschina.net/oscnet/up-2f6cdc5076cdfa96c95990be765043ef270.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;由阿里巴巴雲智能集團支持的 Qwen 團隊，以 Qwen3 系列模型在指令跟隨、代碼生成等任務中的優異表現受到社區青睞。Qwen2.5-72B 系列位列開源大語言模型前列，其輕量化模型 QwQ-32B 通過強化學習優化，在數學推理和代碼生成中媲美大型模型，大幅降低部署成本。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;DeepSeek 則以低成本、高性能的 R1 系列模型聞名。R1-0528 在 LiveCodeBench 排行榜中超越多個國際競品，僅次於 OpenAI 頂尖模型。其輕量化版本 DeepSeek-R1-0528-Qwen3-8B 通過知識蒸餾技術，單 GPU 即可運行，在 AIME2025 數學測試中擊敗 Google 的 Gemini2.5Flash，展現了在特定領域的競爭優勢。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;Qwen 和 DeepSeek 的入榜反映了中國 AI 團隊在開源生態中的崛起。Hugging Face 負責人表示，兩團隊的貢獻為全球開發者提供了高效資源。NVIDIA 首席執行官黃仁勳也讚揚其性能與成本平衡正在重塑 AI 格局。未來，Qwen 計劃探索多模態技術，DeepSeek 則將推出 R2 模型，持續推動 AI 創新。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354773/model-release-heatmap</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354773/model-release-heatmap</guid>
      <pubDate>Sun, 11 May 2025 02:58:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>🔥 Solon Flow 設計器入門</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#24292e; text-align:start"&gt;探索視頻：&lt;/p&gt; 
&lt;p&gt;&lt;iframe frameborder="0" height="400" scrolling="no" src="https://player.bilibili.com/player.html?isOutside=true&amp;amp;aid=114657104759990&amp;amp;bvid=BV1opT6z5EiJ&amp;amp;cid=30416702034&amp;amp;p=1" style="box-sizing: border-box; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(36, 41, 46); font-family: -apple-system, &amp;quot;system-ui&amp;quot;, &amp;quot;Segoe UI&amp;quot;, Helvetica, Arial, sans-serif, &amp;quot;Apple Color Emoji&amp;quot;, &amp;quot;Segoe UI Emoji&amp;quot;, &amp;quot;Segoe UI Symbol&amp;quot;; text-align: start; background-color: rgb(255, 255, 255);" width="700" referrerpolicy="no-referrer"&gt;&lt;/iframe&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354770</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354770</guid>
      <pubDate>Sun, 11 May 2025 02:51:00 GMT</pubDate>
      <author>來源: 投稿</author>
    </item>
    <item>
      <title>Android 16 正式發佈</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;谷歌發佈了 &lt;u&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.google%2Fproducts%2Fandroid%2Fandroid-16%2F" target="_blank"&gt;Android 16 正式版&lt;/a&gt;&lt;/u&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-625ec525ab94813ebb3e980c0109b784e8a.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-79defc140e2b5a5857ec68c7355fa11d8d0.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;作為今年的第一次大版本升級，本次更新的主要特性包括：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;面向按鍵式導航（三大金剛）的預測性返回手勢&lt;/li&gt; 
 &lt;li&gt;強制通知分組&lt;/li&gt; 
 &lt;li&gt;以進度為中心的通知&lt;/li&gt; 
 &lt;li&gt;面向 Pixel 設備的桌面模式（開發者選項）&lt;/li&gt; 
 &lt;li&gt;低功耗藍牙聽力輔助設備支持&lt;/li&gt; 
 &lt;li&gt;自定義鍵盤快捷方式&lt;/li&gt; 
 &lt;li&gt;HDR 截圖優化&lt;/li&gt; 
 &lt;li&gt;以舊換新模式等&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Android 16 新特性詳細介紹查看：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.android.com%2Fintl%2Fen_us%2Fnew-features-on-android%2F" target="_blank"&gt;https://www.android.com/intl/en_us/new-features-on-android/&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354766/android-16</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354766/android-16</guid>
      <pubDate>Sun, 11 May 2025 02:42:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>OpenAI 推遲開源模型的發佈時間</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;OpenAI 首席執行官山姆·奧特曼宣佈，原計劃於今年初夏發佈的公開權重的開源模型預計&lt;strong&gt;將推遲至夏末發佈&lt;/strong&gt;，而不是 6 月與公眾見面。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-52e03b48d8e7654af9853f14bbc91177053.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;他表示研究團隊做了一些出乎意料且非常令人驚奇的事情，這非常值得等待，但需要更長的時間。&lt;/p&gt; 
&lt;p&gt;今年 3 月底，OpenAI 宣佈將發佈自 GPT-2 以來的首個「開源」語言模型。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;相關閲讀：&lt;a href="https://www.oschina.net/news/346315/open-ai-model-best-opensource-coming-soon" target="news"&gt;OpenAI 正在打造「最強」開源模型，計劃今年初夏發佈&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354761</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354761</guid>
      <pubDate>Sun, 11 May 2025 02:33:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Mistral 推出首個推理模型系列 Magistral</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="margin-left:auto !important; margin-right:auto !important; text-align:start"&gt;&lt;span style="color:#212623"&gt;Mistral&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmistral.ai%2Fnews%2Fmagistral" target="_blank"&gt;宣佈推出&lt;/a&gt;其首個推理模型系列 Magistral，採用 step-by-step&lt;/span&gt;&amp;nbsp;&lt;span style="color:#212623"&gt;的方式，以提高數學和物理等主題的一致性和可靠性。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="margin-left:auto !important; margin-right:auto !important; text-align:start"&gt;&lt;span style="color:#212623"&gt;Magistral 有兩種版本：Magistral Small 和 Magistral Medium。Magistral Small 擁有 240 億個參數，在 Apache 2.0 協議下開源。Magistral Medium 是一款功能更強大的模型，目前已在 Mistral 的 Le Chat 聊天機器人平台、該公司的 API 以及第三方合作伙伴雲平台上提供預覽。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="margin-left:auto !important; margin-right:auto !important; text-align:start"&gt;&lt;img height="295" src="https://oscimg.oschina.net/oscnet/up-05ff3090a9b8126e0803e475b935c4f0aac.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="margin-left:auto !important; margin-right:auto !important; text-align:start"&gt;&lt;span style="color:#212623"&gt;Mistral 在博客文章中寫道：「Magistral 適用於各種企業用例，從結構化計算和程序邏輯到決策樹和基於規則的系統。這些模型針對多步驟邏輯進行了微調，提高了可解釋性，並以用户的語言提供了可追溯的思維過程。」&lt;/span&gt;&lt;/p&gt; 
&lt;p style="margin-left:auto !important; margin-right:auto !important; text-align:start"&gt;&lt;span style="color:#212623"&gt;Mistral 成立於 2023 年，該公司得到了 General Catalyst 等風險投資機構的支持，迄今已籌集超過 11 億歐元（約合 12.4 億美元）。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="margin-left:auto !important; margin-right:auto !important; text-align:start"&gt;&lt;span style="color:#212623"&gt;儘管 Mistral 資源雄厚，但在某些領域，例如推理模型開發，Mistral 仍落後於其他領先的人工智能實驗室。從 Mistral 自身的基準測試來看，Magistral 似乎也並非一款特別有競爭力的版本。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="margin-left:auto !important; margin-right:auto !important; text-align:start"&gt;&lt;span style="color:#212623"&gt;在 GPQA Diamond 和 AIME 測試中，Magistral Medium 的表現不及 Gemini 2.5 Pro 和 Anthropic 的 Claude Opus 4。在流行的編程基準 LiveCodeBench 上，Magistral Medium 也未能超越 Gemini 2.5 Pro。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="margin-left:auto !important; margin-right:auto !important; text-align:start"&gt;&lt;span style="color:#212623"&gt;或許正因如此，Mistral 在其博客文章中大力宣揚 Magistral 的其他優勢。聲稱 Magistral 在 Le Chat 中提供答案的速度是競爭對手的「10 倍」，並且支持多種語言，包括意大利語、阿拉伯語、俄語和簡體中文。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="margin-left:auto !important; margin-right:auto !important; text-align:start"&gt;&lt;span style="color:#212623"&gt;Magistral 的發佈是在 Mistral 推出「vibe coding」客户端 Mistral Code 之後。在此之前的幾周，Mistral&amp;nbsp;推出了幾款專注於編碼的模型，並推出了 Le Chat Enterprise，一項面向企業的聊天機器人服務，提供 AI 代理構建器等工具，並將 Mistral 的模型與 Gmail 和 SharePoint 等第三方服務集成。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354760/mistral-magistral</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354760/mistral-magistral</guid>
      <pubDate>Sun, 11 May 2025 02:31:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>【運維實操指南】2 分鐘定製雷池 WAF 認證頁：從「標準表單」到「視覺升級」全攻略</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="margin-left:0; text-align:left"&gt;&lt;span&gt;在，通用設置 &amp;gt; 防護配置，模塊下，找到 [自定義 HTML] 模塊&lt;/span&gt;&lt;/p&gt; 
&lt;p style="margin-left:0; text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="margin-left:0; text-align:left"&gt;&lt;img height="341" src="https://oscimg.oschina.net/oscnet//30cd12a12b9b01facd09506982aa5867.jpg" width="716" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="margin-left:0; text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="margin-left:0; text-align:left"&gt;&lt;span&gt;就像寫一個普通的 html 頁面一樣，你可以同時寫入 style、script 等標籤, 所以用 css 就能修改中心區域的樣式啦。&lt;/span&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;示例：&lt;/span&gt;&lt;/h2&gt; 
&lt;p style="margin-left:0; text-align:left"&gt;&lt;span&gt;把文末的示例代碼複製到&lt;/span&gt;&lt;/p&gt; 
&lt;p style="margin-left:0; text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="margin-left:0; text-align:left"&gt;&lt;img height="449" src="https://oscimg.oschina.net/oscnet//4116eaeb2f8f24a9d1bf87150294cc81.jpg" width="725" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="margin-left:0; text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;效果圖:&lt;/span&gt;&lt;/h2&gt; 
&lt;p style="margin-left:0; text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="margin-left:0; text-align:left"&gt;&lt;img height="346" src="https://oscimg.oschina.net/oscnet//679bc9d16ccf472845db825ecfd23844.jpg" width="746" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="margin-left:0; text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;lt;script&amp;gt;
  console.log('Im a console.log, which is written in a script tag');
&amp;lt;/script&amp;gt;
&amp;lt;style type="text/css"&amp;gt;
  body {
    background: #395180;
    margin: 0;
  }
  body #slg-box {
    background-color: grey;
    width: 400px;
    height: 100%;
    top: 0;
    left: 0;
    transform: translate(0, 0);
    padding: 100px 20px;
  }
  body #slg-usergroup-username,
  body #slg-usergroup-password {
    background-color: grey;
    color: #fff;
  }
  body #slg-box-title {
    color: #e15ccf;
  }
  body #slg-usergroup-btn {
    color: grey !important;
  }
  body #slg-with-more-title div:nth-child(2) {
    background-color: transparent;
    width: 100%;
    height: 30px;
    line-height: 30px;
    text-align: center;
    border: 1px solid;
  }
  body #slg-with-more-title div:nth-child(1) {
    display: none;
  }
  body #slg-tabs &amp;gt; div {
    fill: green;
  }
  body #slg-usergroup-container input {
    border-style: dashed;
  }
&amp;lt;/style&amp;gt;

&amp;lt;div
  style="
    background-color: grey;
    width: 200px;
    height: 100px;
    text-align: right;
    top: 50%;
    position: relative;
    left: calc(50% + 200px);
    position: relative;
    transform: translate(-50%,-50%);
    border-radius: 10px;
    font-size: 30px;
    line-height: 100px;
    text-align: center;
  "
&amp;gt;
  hello world
&amp;lt;/div&amp;gt;&lt;/code&gt;&lt;/pre&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354756</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354756</guid>
      <pubDate>Sun, 11 May 2025 02:26:00 GMT</pubDate>
      <author>來源: 投稿</author>
    </item>
    <item>
      <title>OpenAI 發佈 o3-pro：更強大，但也更「慢」</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;OpenAI 正式&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2FOpenAI%2Fstatus%2F1932530409684005048" target="_blank"&gt;發佈&lt;/a&gt;了 o3-pro 推理模型，基於 o3 所打造，擁有更強的數學、科學、編程等領域的表現。&lt;/p&gt; 
&lt;p&gt;據介紹，o3-Pro 可，自動調用多種工具，包括可以搜索網頁、分析文件、推理視覺輸入、使用 Python、通過記憶功能個性化回覆等。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;由於調用的工具較多，所以，思考的時間比 o1 Pro、o3 更長。&lt;/strong&gt;o3-pro 與 o3 系列一樣擁有 200K 的上下文窗口和 100K 的輸出，但價格卻比它們暴降 80%。&lt;/p&gt; 
&lt;p&gt;性能表現上：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;o3-pro 在專家評估中，評審人員普遍認為 o3 Pro 在多方面都比 o3 模型更進一步，尤其適合用在科學、教育、編程、商業和寫作這些需要深度輸出的任務中。&lt;/li&gt; 
 &lt;li&gt;在學術評估的基準測試中，o3-pro 的整體表現持續優於 o1-pro 和 o3。&lt;/li&gt; 
 &lt;li&gt;OpenAI 還通過四次嘗試獲取正確答案的方式進行實驗發現，o3-pro 能保持較好的性能表現。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-440f5fd24e55a09735e48e4783972977b21.png" referrerpolicy="no-referrer"&gt; &lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-ffff792d97fc13847cc2f83b51f91107089.png" referrerpolicy="no-referrer"&gt; &lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-79e4c50f3dc3263fc980ed598022fbf89d7.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;目前，o3-pro 已向 Pro 和 Team 用户提供，取代 o1-pro；企業版和教育版用户將在下週獲得使用權限。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0611/102054_daJ8_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;價格方面，o3-pro 輸入為 20 美元/百萬 token，輸出 80 美元/百萬 token；而 OpenAI CEO Sam Altman 昨晚宣佈，o3 降價 80%——因此 o3 價格來到了輸出 2 美元/百萬 token、輸入 8 美元/百萬 token。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354753/openai-o3-pro</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354753/openai-o3-pro</guid>
      <pubDate>Sun, 11 May 2025 02:22:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>KubeCon+CloudNativeCon China 2025 在香港盛大開幕，共繪雲原生未來</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;div&gt; 
 &lt;p&gt;2025 年 6 月 10 日，中國香港 —— 今日，由雲原生計算基金會（CNCF）和 Linux 基金會聯合主辦的全球雲原生計算領域頂尖盛會 KubeCon + CloudNativeCon China 2025 於香港隆重啓幕。來自全球的開發者、技術專家、企業決策者及行業領袖共聚一堂，探索雲原生技術未來藍圖，共推雲計算生態繁榮發展。&lt;/p&gt; 
 &lt;p&gt;盛會首日，彙集了來自 Linux 基金會、CNCF、華為、Akamai、阿里雲、Arm、AWS、Intel、LFOSSA、DaoCloud、F5、Fortinet、ICON、KubeWharf、ManageEngine、SUSE、KubeDB、科大訊飛等頭部企業與組織的技術專家、企業代表及開源領袖。為期兩天的議程將呈現約 100 場主題演講、閃電演講及項目展示，聚焦雲原生與 AI 融合、安全合規、多雲架構、數據處理與存儲等多項前沿技術，為現場觀眾帶來深度技術實踐與戰略洞察。&lt;/p&gt; 
 &lt;p&gt;聚焦核心議程，首日的開幕致辭與主題演講環節亮點頻現，重量級嘉賓相繼登台。&lt;/p&gt; 
 &lt;p&gt;Linux 基金會執行董事 Jim Zemlin 為大會致開場詞。Jim Zemlin 鼓勵科技公司使用開源軟件來幫助創新。他指出，長期以來，有很多公司反對開源，試圖保持專有的地位，最終要麼以低估值被收購，要麼只能退出一些業務領域。為什麼開源在所有的技術創新中的作用如此巨大？Jim Zemlin 表示，答案是因為它在經濟上非常有價值，「我們與哈佛商學院進行了一項研究，如果必須購買所有用來創建技術、產品和服務的開源軟件，那需要花費的成本高達 9 萬億美元，這就是開源如此強大的原因。」&lt;/p&gt; 
 &lt;p&gt;&lt;img height="507" src="https://oscimg.oschina.net/oscnet/up-3148dadc1499f9a5e77b67236866f6c5967.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
 &lt;p&gt;CNCF 首席技術官 Chris Aniszczyk 為大會做社區開幕致詞。Chris Aniszczyk 高度認可中國在雲原生技術領域的創新與貢獻。中國在科技創新，尤其雲原生領域展現重大貢獻，是 CNCF 最早且最強大的生態系統之一，開源貢獻位居全球第二，孕育出如 Volcano、Dragonfly、KubeEdge、OpenYurt 等多個具有全球影響力的項目，彰顯了在邊緣計算、容器調度、分佈式處理等多方面的卓越能力。&lt;/p&gt; 
 &lt;p&gt;&lt;img height="513" src="https://oscimg.oschina.net/oscnet/up-a458d49baccea2f5a94b050de918d9924e6.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
 &lt;p&gt;Odyssey Cloud 聯合創始人 Amit Dsouza 和 Nirmata 社區負責人 Cortney Nickerson 發表《Crossplane Is the Answer! but What Is the Question?》主題演講。二人介紹了 IaC 工具 Crossplane 在平台工程方面的賦能作用，比如通過擴展 Kubernetes API，以聲明式的方式管理基礎設施和應用程序等。此外，Crossplane + ArgoCD + Kyverno 堆棧還可以實現 GitOps 驅動的自動化，確保部署符合組織合規性和安全策略。&lt;/p&gt; 
 &lt;p&gt;&lt;img height="516" src="https://oscimg.oschina.net/oscnet/up-b39291f8c25f9ae68378b06ecbccac514ff.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
 &lt;p&gt;華為首席開源聯絡官，CNCF 董事會成員任旭東發表《邁向人工智能集羣雲》主題演講。任旭東指出，人工智能硬件基礎設施正朝着大型處理器集羣的方向發展，需要我們在構建和管理雲的方式上進行重大變革，而藉助 Linux、Volcano 和 Karmada 等項目，我們可以實現向人工智能集羣雲的演進。&lt;/p&gt; 
 &lt;p&gt;&lt;img height="513" src="https://oscimg.oschina.net/oscnet/up-2981a2c77bb968828df1b6e8df65eed5037.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
 &lt;p&gt;Second State 創始人 Michael Yuan 發表了《針對 GenAI 工作負載優化的 Linux 堆棧》主題演講。Michael Yuan 介紹了 Flatcar 的基礎知識及其對 Wasm 運行時的支持，討論了 WasmEdge 對可移植 AI 模型和推理應用程序的支持，並演示了一個可在 Flatcar 中跨 GPU 和 CPU 運行的 GenAI 應用程序。&lt;/p&gt; 
 &lt;p&gt;&lt;img height="506" src="https://oscimg.oschina.net/oscnet/up-b3c6a0bdb509152e9f34e215bf552e6a93b.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
 &lt;p&gt;華為雲軟件工程師 Xuzheng Chang 和科大訊飛平台架構師 Dong Jiang 發表《利用 Volcano 進行擴展模型訓練：科大訊飛的 Kubernetes 突破》主題演講。據介紹，科大訊飛在大規模模型訓練中，通過利用 Volcano，將 GPU 利用率提升了 40% 以上，並將故障恢復時間縮短了 70%。&lt;/p&gt; 
 &lt;p&gt;&lt;img height="515" src="https://oscimg.oschina.net/oscnet/up-2098f237a664232ed1ddce8a1bd6d078672.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
 &lt;p&gt;香港環球航空學會科技主管、主任助理 Aaron Xu 和 DaoCloud 首席執行官兼創始人 Roby Chen 發表《香港人工智能的未來：從本地創新到全球影響力》主題演講。據介紹，HKGAI V1 的發佈標誌着香港人工智能發展翻開了新的篇章。HKGAI 團隊充分發揮本土互聯和全球佈局的優勢，擁抱開源社區，共同應對從優化高性能計算集羣到探索前沿人工智能模型等諸多挑戰。未來，香港將進一步整合內地和國際資源，深化技術創新和應用拓展，為全球人工智能標準和應用貢獻「香港方案」。&lt;/p&gt; 
 &lt;p&gt;&lt;img height="500" src="https://oscimg.oschina.net/oscnet/up-0b36a5534d137948137e29dbcaa96ffceb9.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
 &lt;p&gt;此外，大會現場還特別設立了項目展廳，展示全球多個頂級開源項目，呈現開源技術生態的最新進展和創新成果。&lt;/p&gt; 
 &lt;p&gt;首日盛況已燃，精彩遠未落幕！在接下來的議程中，與會者將有機會深入技術細節，參與更多深度對話。大會還將帶來近百場分技術演講及特色活動，聚焦微服務治理、可觀測性、安全、平台工程等熱點議題，更多來自全球頂級企業與創新團隊的洞見與實踐將精彩呈現。敬請期待，共同見證雲原生與 AI 融合新紀元的無限可能！&lt;/p&gt; 
 &lt;p&gt;&lt;span style="color:#6425d0"&gt;&lt;strong&gt;感謝贊助商&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;KubeCon + CloudNativeCon China 2025 的成功舉辦，得益於贊助商們的大力支持。在此感謝以下贊助商：&lt;/p&gt; 
 &lt;p&gt;&lt;span style="color:#6425d0"&gt;&lt;strong&gt;關於&lt;/strong&gt;&lt;/span&gt;&lt;span style="color:#6425d0"&gt;&lt;strong&gt;雲原生計算基金會&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;雲原生計算能協助組織利用開源軟件棧，在公共雲、私有云和混合雲等各種雲環境中構建和運行可擴展的應用程序。雲原生計算基金會 (CNCF) 託管包括 Kubernetes、Prometheus 和 Envoy 在內的全球技術基礎設施的關鍵組件。&lt;/p&gt; 
 &lt;p&gt;CNCF 彙集了行業頂尖的開發者、終端用户和供應商，並舉辦世界上最大的開源開發者會議。作為非營利性 Linux 基金會的部分，CNCF 得到了超過 800 個成員的支持，其中包括全球最大的雲計算和軟件公司，以及 200 多個創新型初創企業。有關更多信息，請搜索 CNCF 官網。&lt;/p&gt; 
 &lt;p&gt;&lt;span style="color:#6425d0"&gt;&lt;strong&gt;關於&lt;/strong&gt;&lt;/span&gt;&lt;span style="color:#6425d0"&gt;&lt;strong&gt;Linux 基金會&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;Linux 基金會是全球領先的開源軟件、硬件、標準和數據協作平台。Linux 基金會的項目對全球基礎設施至關重要，涵蓋 Linux、Kubernetes、Node.js、ONAP、PyTorch、RISC-V、SPDX、OpenChain 等。Linux 基金會致力於採納最佳實踐，滿足貢獻者、用户以及解決方案提供者的需求，打造可持續的開放合作模式。有關更多信息，請搜索 Linux 基金會官網。&lt;/p&gt; 
&lt;/div&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354700</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354700</guid>
      <pubDate>Sat, 10 May 2025 14:23:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>iOS 26 新增實時翻譯：基於端側並向第三方開放接口</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-2e75214ed6a7bf091530afec2180ff3869d.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;開發者朋友們大家好：&lt;/p&gt; 
&lt;p&gt;這裏是 &lt;strong&gt;「RTE 開發者日報」&lt;/strong&gt; ，每天和大家一起看新聞、聊八卦。&lt;/p&gt; 
&lt;p&gt;我們的社區編輯團隊會整理分享 RTE（Real-Time Engagement） 領域內「有話題的 &lt;strong&gt;技術&lt;/strong&gt; 」、「有亮點的 &lt;strong&gt;產品&lt;/strong&gt; 」、「有思考的 &lt;strong&gt;文章&lt;/strong&gt; 」、「有態度的 &lt;strong&gt;觀點&lt;/strong&gt; 」、「有看點的 &lt;strong&gt;活動&lt;/strong&gt; 」，但內容僅代表編輯的個人觀點，歡迎大家留言、跟帖、討論。&lt;/p&gt; 
&lt;p&gt;本期編輯：@趙怡嶺，@鮑勃&lt;/p&gt; 
&lt;h2&gt;01 有話題的技術&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;1、Direct3D-S2：影視級 3D 生成模型，僅需 8 塊 GPU 即可訓練，效果超越許多閉源商用模型&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;DreamTech 與南大、復旦和牛津聯合推出的 Direct3D-S2 開源 3D 生成模型，在 HuggingFace 熱榜中表現卓越，僅需 8 塊 GPU 即可訓練，效果超越許多閉源商用模型，達到了影視級精細度。其核心創新 —— 空間稀疏注意力機制（SSA）顯著提升了生成效率和細節表現，解決了傳統 3D 建模面臨的計算壓力和複雜度問題。&lt;/p&gt; 
&lt;p&gt;在 Direct3D-S2 中，DreamTech 團隊提出了一項核心創新——空間稀疏注意力機制（Spatial Sparse Attention， SSA）。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-bfaba50ee60f71dfe7e6d2905c689c38efe.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;這一機制專為解決當前 Diffusion Transformer（DiT）在處理高分辨率 3D 生成時效率低、精細度差的問題而設計，堪稱 3D 生成領域的效率引擎。&lt;/p&gt; 
&lt;p&gt;相關鏈接：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2505.17412" target="_blank"&gt;https://arxiv.org/pdf/2505.17412&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;相關鏈接：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FDreamTechAI%2FDirect3D-S2" target="_blank"&gt;https://github.com/DreamTechAI/Direct3D-S2&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;相關鏈接：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.neural4d.com%2Fresearch%2Fdirect3d-s2%2F" target="_blank"&gt;https://www.neural4d.com/research/direct3d-s2/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;相關鏈接：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fspaces%2Fwushuang98%2FDirect3D-S2-v1.0-demo" target="_blank"&gt;https://huggingface.co/spaces/wushuang98/Direct3D-S2-v1.0-demo&lt;/a&gt; （@新智元、@果比 AI）&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2、Neuralink 和 Grok 合作，腦機芯片為漸凍症患者賦予「發聲」能力&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;近日，馬斯克在 X 上轉發的一則案例顯示：Neuralink 和 Grok 正合作使漸凍症患者重新「發聲」。&lt;/p&gt; 
&lt;p&gt;通過腦機接口技術，一名漸凍症患者成功實現了用意念輸出文字，並藉助 AI 完成語句補全和聲音克隆，最終以接近本人的聲音「説話」。這一突破性進展源於 Neuralink 的腦機芯片植入技術，以及 Grok 強大的自然語言處理能力。&lt;/p&gt; 
&lt;p&gt;具體來説，患者只需通過思考即可移動光標生成文本，Grok 助手則像「讀心術」一樣自動更正並補全文本，最後通過 AI 克隆出患者原本的聲音，讓交流更加自然。&lt;/p&gt; 
&lt;p&gt;馬斯克轉發的帖子原出處 Mario Nawfal 此前介紹，患者 Bradford Smith 因為漸凍症喪失了行動和説話能力，而 Neuralink 使其能夠通過思考來生成文本，Grok 則可以實現「讀心術」式的自動更正，再通過另一個 AI「克隆」的其真實聲音，從而使他「説話」時能夠擁有聽起來就像本人的聲音。&lt;/p&gt; 
&lt;p&gt;今年 5 月，Neuralink 的腦機接口設備 Link 獲得了美國 FDA 的「突破性設備」認證，專門用於幫助嚴重語言障礙患者恢復溝通能力。&lt;/p&gt; 
&lt;p&gt;新聞鏈接：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.ithome.com%2F0%2F859%2F328.htm" target="_blank"&gt;https://www.ithome.com/0/859/328.htm&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;X 鏈接：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2FMarioNawfal%2Fstatus%2F1928406038803558837" target="_blank"&gt;https://x.com/MarioNawfal/status/1928406038803558837&lt;/a&gt; （@IT 之家、@新智訊）&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;3、開源框架 Rowboat：快速構建智能助手，支持 MCP、Agent SDK&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;由 Y Combinator 支持的開源多智能體開發框架 Rowboat 亮相，支持 MCP 服務和 OpenAI Agent SDK。框架由 Agent、Playground 和 Co pilot 三大模塊構成，方便用户快速構建、測試和部署智能助手。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Agent，主要負責處理對話的特定部分，並能依據指令使用工具執行任務。其亮點在於可通過自然語言指令進行配置，能以圖的形式在智能體之間進行編排，還可訪問工具和 RAG。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Playground，這是一個交互式環境，方便用户在構建助手時以對話方式進行測試。它具備實時測試和調試功能，可在界面內檢查工具調用的參數和結果，能與單個智能體或整個助手進行對話。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Copilot，由 AI 驅動的輔助工具，可代用户創建和更新智能體與工具。能感知包括演練場在內的所有組件的上下文，可根據對話和反饋優化智能體，能理解用户以自然語言提出的請求。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;用户可創建多智能體，如信用卡助手，實現任務協同。Rowboat 還提供 HTTP API 和 Python SDK，適應多樣開發場景。目前，Rowboat 在 Github 已經超過 2000 顆星。&lt;/p&gt; 
&lt;p&gt;開源地址：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Frowboatlabs%2Frowboat%3Ftab%3Dreadme-ov-file%25EF%25BC%2588%40AIGC" target="_blank"&gt;https://github.com/rowboatlabs/rowboat?tab=readme-ov-file（@AIGC&lt;/a&gt; 開放社區、@OneThingAI Lab）&lt;/p&gt; 
&lt;h2&gt;02 有亮點的產品&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;1、Apple Intelligence 實時翻譯功能：基於端側、橫框多個應用、向第三方開發者開放&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在 Apple 最新發布的 iOS 26 中，Apple Intelligence 支持實時翻譯功能，這個功能橫跨電話、信息與 Facetime 三個通訊軟件，當你收到外語信息時，系統會自動將其翻譯成你的語言；相關功能已集成到信息、電話等 App 中，能夠實現即時翻譯文本和音頻，從而幫助用户跨越語言障礙。&lt;/p&gt; 
&lt;p&gt;同樣的，你發出的內容也會被實時翻譯成對方的語言，讓跨語言交流變得前所未有的順暢。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-a1581d62c50fe6604340d5a10ba25be1442.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;實時翻譯功能完全基於端側，你的對話內容不會由此流通到任何未經允許的地方。&lt;/p&gt; 
&lt;p&gt;由 Apple Intelligence 驅動的實時翻譯功能將通過 API 接口，向所有第三方開發者開放，開發者可以將實時翻譯功能集成到任何通訊軟件中。&lt;/p&gt; 
&lt;p&gt;過去一年，蘋果在海外推出瞭如 Genmoji、圖樂園等 AI 功能，幫助用户更自由、有趣地表達內容，而外界最為關心的 AI Siri 將什麼時候落地，在今年 WWDC 依舊並沒有給出具體的日期。&lt;/p&gt; 
&lt;p&gt;語言適配方面倒是有所進展，Apple 智能將在今年年底前支持這些語言：丹麥語、荷蘭語、挪威語、葡萄牙語、瑞典語、土耳其語、繁體中文和越南語。&lt;/p&gt; 
&lt;p&gt;蘋果宣佈推出 Foundation Models Framework。這是一項全新的 API，允許第三方開發者調用 Apple Intelligence 核心的大型語言模型（LLM），並將其集成到自家應用中。&lt;/p&gt; 
&lt;p&gt;開發者無需構建自己的 AI 模型，也不必依賴雲端服務，就能在自己的 App 中調用一個功能強大、響應快速、且重視隱私保護的智能助手。更重要的是，不怕斷網，離線也能運行。 （@APPSO、@IT 之家）&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2、Talking Tours：Google 發佈的 AI 導遊，支持實時對話互動&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-32abd937a92b609d665aa4542e702d431b0.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;打開 Talking Tours 頁面，你會看到一張互動地圖，涵蓋全球多個文化地標和自然景觀，分為多個主題：文化機構（博物館、圖書館、劇院）、地標建築、古蹟和自然景觀（森林、洞穴、沙漠、園林、海洋）。&lt;/p&gt; 
&lt;p&gt;點擊地圖上的座標，即可進入對應地點的沉浸式街景視圖。AI 導遊會通過語音講解該地點的背景信息，比如某所博物館的建築風格、歷史典故，甚至細節到展廳裏壁紙的設計靈感。&lt;/p&gt; 
&lt;p&gt;切換畫面後，點擊「take a snapshot」按鈕，AI 會基於新畫面重新生成一段講解，換個角度看，同一地點也可能講出不同的故事。還可以點擊右下角的「🙋」圖標，對 AI 導遊發起提問。&lt;/p&gt; 
&lt;p&gt;體驗鏈接：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fartsandculture.google.com%2Fexperiment%2F8AGlfzgsYmBeIA" target="_blank"&gt;https://artsandculture.google.com/experiment/8AGlfzgsYmBeIA&lt;/a&gt; （@Founder Park）&lt;/p&gt; 
&lt;h2&gt;03 有態度的觀點&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;1、任正非：AI 也許是人類社會最後一次技術革命&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;《人民日報》6 月 10 日頭版刊文消息，近日，在深圳華為總部，圍繞大眾關心的一些熱點話題，人民日報記者一行與華為 CEO 任正非面對面交流。 交流中，任正非透露，在「面對外部封鎖打壓，遇到很多困難」時，自己堅信「不去想困難，幹就完了，一步一步往前走」。&lt;/p&gt; 
&lt;p&gt;面對「人工智能（AI）的未來前景怎麼看」時，任正非表示，「人工智能也許是人類社會最後一次技術革命」。其解釋稱：&lt;/p&gt; 
&lt;p&gt;人工智能發展要經歷數十年、數百年。不要擔心，中國也有很多優勢。任正非還強調，人工智能在技術上的要害，是要有充足的電力、發達的信息網絡。發展人工智能要有電力保障，中國的發電、電網傳輸都是非常好的，通信網絡是世界最發達的，東數西算的理想是可能實現的。&lt;/p&gt; 
&lt;p&gt;另外，任正非還提到了其他優勢：芯片問題其實沒必要擔心，用疊加和集羣等方法，計算結果上與最先進水平是相當的。軟件方面，將來是千百種開源軟件滿足整個社會需要。(@ APPSO)&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2、OpenAI 前首席科學家：AI 會完成我們能做的一切&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;日前，OpenAI 前首席科學家 Ilya Sutskever 返回母校多倫多大學，在接受榮譽博士學位時發表了個人演講。&lt;/p&gt; 
&lt;p&gt;Ilya 開篇就分享了個人心態：接受現實，儘量不去後悔過去，努力改善現狀。接着，他表示，大家都處在一個真正不同尋常的時代——因為 AI 的出現。&lt;/p&gt; 
&lt;p&gt;Ilya 坦言，如今的 AI 已經在很大程度上改變了「學生」的含義，並且遠不止於此。Ilya 表示，AI 能做的事情已經遠超想象，而我們眼下的挑戰是「AI 會如何影響我們的工作和職業」，同時也有更深層次的挑戰——未來 AI 的發展將是前所未有、極其劇烈的。&lt;/p&gt; 
&lt;p&gt;他還強調：「任何我能學到的東西，任何你們中的任何一個人能夠學到的東西，AI 都能學會。那麼，為什麼我這麼確信呢？我們怎麼知道 AI 將來能做這些事情呢？原因是，我們每個人的大腦都是一個生物計算機。我們有大腦，就是因為它是一個生物計算機。那麼，既然人類的生物計算機能做這些事情，為什麼數字計算機、也就是數字大腦不能做同樣的事呢？這就是為什麼我認為 AI 最終能做到所有我們能做到的事情的原因。」&lt;/p&gt; 
&lt;p&gt;對於「當 AI 能做我們所有的工作時，會發生什麼？」這一問題，Ilya 認為十分需要重視。他提醒：「你可能不關心 AI，但 AI 會主動來關心你」。&lt;/p&gt; 
&lt;p&gt;因此，Ilya 建議大家，在 AI 時代下，只要你開始使用 AI，去了解當下最先進的 AI 能做些什麼，你就會逐漸建立起一種直覺。「我認為，通過使用 AI 並觀察當今最先進的 AI 能做什麼，你會形成一種直覺。隨着 AI 在一年、兩年、三年內不斷改進，這種直覺會變得更強烈」。慢慢的，我們能對 AI 的發展有一定的概念，自然也不會再對 AI 產生恐懼，並能夠掌控 AI，激發新技術給我們帶來的力量。&lt;/p&gt; 
&lt;p&gt;最後，Ilya 強調：&lt;/p&gt; 
&lt;p&gt;AI 帶來的挑戰是人類歷史上最大的挑戰。但如果我們應對得當，所獲得的回報也將是人類歷史上最大的回報。&lt;/p&gt; 
&lt;p&gt;演講全程：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fyoutu.be%2FzuZ2zaotrJs%3Ffeature%3Dshared" target="_blank"&gt;https://youtu.be/zuZ2zaotrJs?feature=shared&lt;/a&gt; （@APPSO、@機器之心）&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-fa92df803a9e43d2a75ae183d839112a4a6.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;更多 Voice Agent 學習筆記：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FGe3xHvHjsvm3cNEMxiLIEQ" target="_blank"&gt;實時多模態如何重塑未來交互？我們邀請 Gemini 解鎖了 39 個實時互動新可能丨 Voice Agent 學習筆記&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FSqXLZvq_zwWDcOVKbAb7HQ" target="_blank"&gt;級聯 vs 端到端、全雙工、輪次檢測、方言語種、商業模式…語音 AI 開發者都在關心什麼？丨 Voice Agent 學習筆記&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F7QPgzp8kDR_9iHUa4oFeiA" target="_blank"&gt;a16z 最新報告：AI 數字人應用層即將爆發，或將孕育數十億美金市場丨 Voice Agent 學習筆記&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FUM1qs2IT1S6kJ4sZf_k3uA" target="_blank"&gt;a16z 合夥人：語音交互將成為 AI 應用公司最強大的突破口之一，巨頭們在 B2C 市場已落後太多丨 Voice Agent 學習筆記&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FWI0gE4x-TZG0gdgSV_bVSA" target="_blank"&gt;ElevenLabs 33 億美元估值的秘密：技術驅動+用户導向的「小熊軟糖」團隊丨 Voice Agent 學習筆記&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FSVsgDF8F1hxy3-e5-ntGbw" target="_blank"&gt;端側 AI 時代，每台家居設備都可以是一個 AI Agent 丨 Voice Agent 學習筆記&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F4K5wdUEDxrs1afHZSAIuqg" target="_blank"&gt;世界最炙手可熱的語音 AI 公司，舉辦了一場全球黑客松，冠軍作品你可能已經看過&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FJCYzc1Ig-HFFAN3sTQDYbw" target="_blank"&gt;多模態 AI 怎麼玩？這裏有 18 個腦洞&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FrN9poD_X6SDxRLMsudg_xg" target="_blank"&gt;AI 重塑宗教體驗，語音 Agent 能否成為突破點？&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FeFS1mnAbUpAJdiLSSGWpSA" target="_blank"&gt;對話 TalktoApps 創始人：Voice AI 提高了我五倍的生產力，語音輸入是人機交互的未來&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;寫在最後：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;我們歡迎更多的小夥伴參與 &lt;strong&gt;「RTE 開發者日報」&lt;/strong&gt; 內容的共創，感興趣的朋友請通過開發者社區或公眾號留言聯繫，記得報暗號「共創」。&lt;/p&gt; 
&lt;p&gt;對於任何反饋（包括但不限於內容上、形式上）我們不勝感激、並有小驚喜回饋，例如你希望從日報中看到哪些內容；自己推薦的信源、項目、話題、活動等；或者列舉幾個你喜歡看、平時常看的內容渠道；內容排版或呈現形式上有哪些可以改進的地方等。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-a8e54bdf0d246189e94f7cf3b2e418da213.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;素材來源官方媒體/網絡新聞&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354682</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354682</guid>
      <pubDate>Sat, 10 May 2025 11:42:00 GMT</pubDate>
      <author>來源: 投稿</author>
    </item>
    <item>
      <title>微軟開始測試 Windows 11 的新版「開始」菜單</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;微軟現在&lt;u&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblogs.windows.com%2Fwindows-insider%2F2025%2F06%2F09%2Fannouncing-windows-11-insider-preview-build-26200-5641-dev-channel%2F" target="_blank"&gt;允許&lt;/a&gt;&lt;/u&gt; Windows 11 測試人員試用全新、更大的「開始」菜單，該菜單包含可滾動的界面、新的視圖和更多可自定義功能。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-133f02d0def812a3023b1918667ec4cbb4c.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Windows Insider 團隊解釋説：「我們更新了可滾動的「開始」菜單，讓您可以更輕鬆地啓動應用。」 這個可滾動的「開始」菜單意味着所有應用現在都位於頂層，因此您無需導航到第二個頁面即可找到應用。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-95c628f3783c80a5c0be69c6167b9d9f144.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;更新後的「開始」菜單有兩個新視圖可供選擇&lt;/p&gt; 
&lt;p&gt;您還可以禁用推薦部分，以便查看更多應用，並選擇兩種新視圖：類別視圖和網格視圖。默認類別視圖按類別對應用進行分組，而網格視圖則按字母順序排列，更像傳統的列表視圖。&lt;/p&gt; 
&lt;p&gt;微軟還根據設備或顯示器的屏幕尺寸放大了「開始」菜單。Windows Insider 團隊表示：「在較大的設備上，用户可以在「開始」菜單中看到 8 列固定應用、6 條推薦和 4 列類別。在較小的設備上，你將看到 6 列固定應用、4 條推薦和 3 列類別。」&lt;/p&gt; 
&lt;p&gt;開始菜單上還新增了一個移動設備按鈕，可用於展開或摺疊與開始菜單一起顯示的「Phone Link」界面。微軟還允許 Windows 11 用户選擇顯示哪些鎖屏小部件，允許添加或刪除小部件，並重新排列它們以適應鎖屏。&lt;/p&gt; 
&lt;p&gt;最後，最新的 Dev Channel 版本還包含一個新的 Gamepad 鍵盤更新，可讓您使用控制器通過 PIN 碼登錄 PC。這是微軟改進 Windows 11 在手持遊戲設備（例如最近發佈的 ROG Xbox Ally 設備）上的運行效果的一部分。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354680/windows-11-new-start-menu-testing-dev-channel</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354680/windows-11-new-start-menu-testing-dev-channel</guid>
      <pubDate>Sat, 10 May 2025 11:23:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>美團發佈 AI Coding Agent 工具「NoCode」</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:start"&gt;&lt;span&gt;美團&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FdByPiajMM7fX109GSotLVQ" target="_blank"&gt;上線&lt;/a&gt;了名為「NoCode」的&amp;nbsp;&lt;/span&gt;AI Coding Agent 工具&lt;span&gt;，用户通過自然語言對話即可生成網頁、小程序等應用，並支持實時修改、一鍵部署。&lt;/span&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:start"&gt;NoCode 是一款無需編程背景和經驗，通過自然語言和對話形式，即可快速生成應用的平台。可幫助不同角色以"零代碼"的方式創建個人提效工具、產品原型、可交互頁面等，降低開發門檻，實現創意釋放。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;&lt;span&gt;NoCode&lt;/span&gt;功能亮點&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;自然語言編程&lt;/strong&gt;：使用自然語言描述想法，NoCode 自動解讀並轉化為完整功能，無需編程經驗即可生成可用能力。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;實時預覽效果&lt;/strong&gt;：根據對話內容即時渲染、呈現頁面，可實時查看每次對話後的實際效果。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;局部定位修改&lt;/strong&gt;：使用 Visual Edit 功能，可針對定位內容進行局部修改及完善；同時支持版本間對比、回退，保障每一步都「有跡可循」。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;一鍵部署分享&lt;/strong&gt;：應用完成後，代碼將自動上傳到倉庫，可直接分享鏈接給他人使用，簡化發佈流程。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:start"&gt;&lt;img height="450" src="https://static.oschina.net/uploads/space/2025/0610/184826_5IfE_2720166.png" width="750" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;體驗地址：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnocode.cn%2F" target="_blank"&gt;https://nocode.cn/&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354675/meituan-nocode</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354675/meituan-nocode</guid>
      <pubDate>Sat, 10 May 2025 10:49:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>法國 AI 初創公司 Mistral 將發佈推理模型 Magistral</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.cnbc.com%2F2025%2F06%2F10%2Fmicrosoft-backed-ai-lab-mistral-debuts-reasoning-model-to-rival-openai.html" target="_blank"&gt;根據 CNBC 的報道&lt;/a&gt;，法國 AI 初創公司 Mistral 將推出其首個推理模型 Magistral，加入與 OpenAI、DeepSeek 等全球領先企業的競爭。&lt;/p&gt; 
&lt;p&gt;&lt;img height="898" src="https://static.oschina.net/uploads/space/2025/0610/183614_pyVq_2720166.png" width="2104" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Mistral 首席執行官亞瑟・門施介紹道，Magistral 不僅擅長數學和編碼，還能夠實現歐洲語言的邏輯推理，突破了美國和中國模型的語言侷限性。&lt;/p&gt; 
&lt;p&gt;今年 3 月，Mistral 已發佈 240 億參數的 Mistral Small 3.1 模型，該模型以低成本實現本地運行，部分性能甚至超越 OpenAI 的 GPT-4o mini。5 月，Mistral 進一步推出了 Medium 3 模型，這款中量級模型在保持前沿性能的同時，顯著降低了企業使用成本，每百萬 Token 輸入僅需 0.4 美元。&lt;/p&gt; 
&lt;p&gt;Mistral 通過技術創新，正逐步提升其在全球 AI 市場的競爭力，併為多語言應用場景提供更優解決方案。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354673/mistral-debuts-reasoning-model-to-rival-openai</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354673/mistral-debuts-reasoning-model-to-rival-openai</guid>
      <pubDate>Sat, 10 May 2025 10:37:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>百度網盤、文庫聯合發佈「AI 相機」</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;6 月 10 日，在百度 AI Day 開放日上，百度網盤、文庫聯合發佈行業首個「拍存管一體」的「AI 相機」，具備全模態輸入、處理、輸出的系統化完整交付 AI 能力。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-acb055f53ca9b3a5f087e132e834b1d7f25.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;AI 相機已在百度網盤 App 上線，並已接入百度文庫 App。百度文庫還宣佈多智能體協作能力「GenFlow 超能搭子」全新升級為 2.0 版本，使其成為率先實現全場景滿足、全鏈路覆蓋的多智能體協作應用。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0610/183107_JyHR_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;GenFlow 超能搭子 2.0 依託於文庫、網盤海量的公私域數據和用户記憶庫，可完整交付更懂用户的個性化內容；它可以自主調用各種模型和工具，一次性並行生成多模態、多格式內容；它還支持後鏈路的編輯環節，在內容創作上靈活度更高。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354672</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354672</guid>
      <pubDate>Sat, 10 May 2025 10:31:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>AI 時代的「數據之困」，什麼是 AI-Ready Data</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;人工智能（AI）無疑是當今科技領域最激動人心的變革力量，它橫跨各個行業，展現出重塑未來的巨大潛力。從智能客服到精準醫療，從自動駕駛到個性化推薦，AI 的觸角幾乎無所不至。然而，在這股 AI 浪潮之下，一個普遍的困境也日益凸顯：許多雄心勃勃的 AI 項目在起步後便步履維艱，難以實現預期的投資回報，甚至大量試點項目最終未能成功轉化為生產力。&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;這種「雷聲大，雨點小」的現象，不禁讓人深思：&lt;strong&gt;AI 的理想與現實之間，究竟橫亙着怎樣的鴻溝？&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;追根溯源，這一困境的核心往往直指 AI 的「食糧」——數據。數據是驅動 AI 系統洞察、預測和決策的燃料。然而，企業在將數據應用於 AI 時，普遍面臨着一系列嚴峻挑戰：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;數據質量參差不齊&lt;/strong&gt;：不準確、不完整、標籤錯誤或充滿噪聲的數據是 AI 項目失敗的常見元兇。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;span&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fzhida.zhihu.com%2Fsearch%3Fcontent_id%3D258754464%26content_type%3DArticle%26match_order%3D1%26q%3D%25E6%2595%25B0%25E6%258D%25AE%25E5%25AD%25A4%25E5%25B2%259B%26zhida_source%3Dentity" target="_blank"&gt;數據孤島&lt;/a&gt;&lt;/span&gt;與集成難題&lt;/strong&gt;：數據往往散落在企業內部各個孤立的系統中，格式各異，難以有效整合和統一訪問。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;缺乏標準化與有效治理&lt;/strong&gt;：數據格式不統一、元數據缺失、數據血緣關係不清晰以及&lt;span&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fzhida.zhihu.com%2Fsearch%3Fcontent_id%3D258754464%26content_type%3DArticle%26match_order%3D1%26q%3D%25E6%2595%25B0%25E6%258D%25AE%25E6%25B2%25BB%25E7%2590%2586%26zhida_source%3Dentity" target="_blank"&gt;數據治理&lt;/a&gt;&lt;/span&gt;機制的薄弱，都為 AI 應用埋下了隱患。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;這些普遍存在的數據問題，實際上反映了許多企業在 AI 戰略上的一個深層錯位：即，&lt;strong&gt;對 AI 技術本身抱有極高期望，卻忽視了構建堅實數據基礎的重要性&lt;/strong&gt;。企業紛紛投入巨資採購先進的 AI 工具和算法，但如果供給這些「智能引擎」的是劣質「燃料」，那麼再強大的算法也難以發揮其應有的效能。AI 的雄心壯志與薄弱的數據能力之間形成的巨大反差，正是導致眾多 AI 項目折戟的關鍵。&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;面對 AI 時代的「數據之困」，企業迫切需要一種能夠有效解決上述問題、真正釋放 AI 潛能的數據形態。於是，「AI-ready Data」 的概念應運而生。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;什麼是 AI-ready Data？為何如此重要？&lt;/strong&gt;&lt;/h2&gt; 
&lt;div&gt;
 &lt;img src="https://oscimg.oschina.net/oscnet//be5fce4d40a25157514e64dbd6664171.jpg" width="1024" referrerpolicy="no-referrer"&gt;
&lt;/div&gt; 
&lt;h3&gt;&lt;strong&gt;AI-ready Data：超越數據的「數據」&lt;/strong&gt;&lt;/h3&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;AI-ready Data，顧名思義，是指那些經過精心準備、結構化處理和嚴格驗證，能夠以最佳效能服務於人工智能應用的數據。這類數據使得 AI 算法能夠高效地學習模式、做出準確預測並生成有價值的洞察。它強調的不僅僅是擁有海量數據，更在於數據的質量、結構和相關性，確保數據能夠被 AI 算法高效處理和分析。&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;打個比方，如果説 AI 是一個高性能引擎，那麼 AI-ready Data 就是為其量身定製的、經過提純的高辛烷值燃料，確保引擎能夠以巔峯狀態持續運轉。它不是原始、未經雕琢的「數據礦石」，而是經過精煉、可以直接投入 AI「熔爐」的「高品位原料」。&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;AI-ready Data 不可或缺的價值&lt;/strong&gt;&lt;/h3&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;AI-ready Data 之所以關鍵，在於它能為 AI 的成功應用帶來一系列實實在在的好處。高質量、準備充分的數據是訓練出高精度、高可靠性 AI 模型的基礎，直接決定了模型的準確性和有效性，正所謂「Garbage in, Garbage out」。通過大幅減少數據科學家在數據清洗和整理上耗費的巨量時間，AI-ready Data 能夠顯著加速 AI 項目的落地進程，使團隊更專注於模型創新與優化。它是構建穩健、可擴展 AI 系統，使其能處理複雜任務並大規模有效運作的基石，最終通過驅動更明智決策、提升運營效率、降低成本和增強市場競爭力，為企業創造切實的商業價值。同時，清晰、可溯源且管理良好的數據還有助於企業遵守日益嚴格的數據法規與 AI 倫理規範，為 AI 系統的透明度和問責制提供保障。&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;理解 AI-ready Data 的價值，更要認識到它並非一勞永逸的靜態目標，而是一個持續演進的動態過程，需要隨 AI 發展、業務變化及法規更新不斷調整優化，其及時性、可擴展性和定期刷新的需求都印證了這是一項長期投入。追求 AI-ready Data 的本質，是將數據管理從單純的「收集」提升到戰略性的「策展」與「價值創造」層面，要求企業帶着明確的 AI 應用目標有意識地準備數據，使數據管理從後端支持轉變為驅動創新的核心環節。更深遠地看，實現數據 AI 就緒的努力將催化組織在數據治理、數據素養和跨部門協作等方面的全面成熟，打破數據孤島，提升整體數據能力，從而孕育出惠及企業全局的數據驅動文化，這其中，人的因素和流程優化與技術平台同等重要。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;不同領域的 AI-ready Data 特徵上有什麼區別？&lt;/strong&gt;&lt;/h2&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;儘管 AI-ready Data 的核心原則具有普適性，但在不同的 AI 細分領域，其具體的形態、準備的側重點以及在模型訓練和推理階段的要求，都會呈現出顯著的差異。&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;機器學習中的 AI-ready Data&lt;/strong&gt;&lt;/h3&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;傳統的機器學習是許多企業 AI 應用的起點，其對數據的要求相對成熟和明確。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;形態&lt;/strong&gt;：ML 模型的數據通常是結構化的表格數據，例如 CSV 文件或數據庫中的表，其中每一行代表一個樣本，每一列代表一個特徵。對於監督學習任務，數據中還會包含一個目標列或標籤列，用以指示模型需要預測的結果 。雖然 ML 也可以處理文本、圖像等非結構化數據，但這往往需要通過複雜的特徵工程將其轉換為結構化的數值特徵，才能被傳統 ML 算法有效利用。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;特徵&lt;/strong&gt;：ML 模型的數據通常是結構化的表格數據，例如 CSV 文件或數據庫中的表，其中每一行代表一個樣本，每一列代表一個特徵。對於監督學習任務，數據中還會包含一個目標列或標籤列，用以指示模型需要預測的結果 。雖然 ML 也可以處理文本、圖像等非結構化數據，但這往往需要通過複雜的特徵工程將其轉換為結構化的數值特徵，才能被傳統 ML 算法有效利用。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;示例&lt;/strong&gt;：用於預測客户流失的數據集，可能包含客户的人口統計信息、消費行為、服務使用頻率等特徵；用於垃圾郵件檢測的已標註郵件數據集。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div&gt;
 &lt;img src="https://oscimg.oschina.net/oscnet//3911ebe7c46166407d1fad003e28b19d.jpg" width="600" referrerpolicy="no-referrer"&gt;
&lt;/div&gt; 
&lt;h3&gt;&lt;strong&gt;深度學習中的 AI-ready Data&lt;/strong&gt;&lt;/h3&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;深度學習以其處理複雜模式和大規模數據的能力，在圖像識別、自然語言處理等領域取得了革命性進展，其對數據的需求也更為「貪婪」。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;形態&lt;/strong&gt;：深度學習模型的訓練通常依賴於大規模的非結構化以及多模態數據，如圖像、音頻、文本和視頻。這些數據往往需要進行大量且精準的標註，例如物體檢測任務中的邊界框、圖像分割的掩碼、語音識別的文本轉錄等。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;特徵&lt;/strong&gt;：數據的「量」和「多樣性」是深度學習成功的關鍵。同時，標註的一致性和準確性對模型性能至關重要，高質量的數據集是實現準確語音識別等任務的基礎。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;示例&lt;/strong&gt;：著名的 ImageNet 數據集包含數百萬張標註圖像；LibriSpeech 數據集包含數千小時的轉錄音頻；維基百科的文本轉儲等大型文本語料庫。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div&gt;
 &lt;img src="https://oscimg.oschina.net/oscnet//22383fbb9f6e44994f6c4a867cb18fce.jpg" width="700" referrerpolicy="no-referrer"&gt;
&lt;/div&gt; 
&lt;h3&gt;&lt;strong&gt;&lt;span&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fzhida.zhihu.com%2Fsearch%3Fcontent_id%3D258754464%26content_type%3DArticle%26match_order%3D1%26q%3D%25E7%2594%259F%25E6%2588%2590%25E5%25BC%258FAI%26zhida_source%3Dentity" target="_blank"&gt;生成式 AI&lt;/a&gt;&lt;/span&gt;與 RAG 系統中的 AI-Ready Data&lt;/strong&gt;&lt;/h3&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;具體到生成式 AI 領域，其對 AI-ready data 的需求首先體現在模型預訓練和微調階段。基礎模型的構建依賴於規模宏大、內容多樣甚至多模態的數據集，涵蓋了從公開網頁文本、專業書籍到代碼、圖像和音視頻等廣泛來源。而模型的微調則更側重於特定領域內高質量、高相關性的專業數據集。貫穿始終的是對數據合規性、版權以及潛在偏見的嚴格審視與倫理考量，負責任的數據策略是實現 AI 價值的前提。&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;在眾多生成式 AI 應用中，檢索增強生成（RAG）架構尤為依賴 AI-ready data 的精細化準備。RAG 通過引入外部知識源來提升模型輸出的準確性、時效性和深度，其核心挑戰在於如何將這些外部知識高效、準確地「喂」給 LLM。這一過程的關鍵瓶頸與優化焦點在於數據切片（Chunking）。當前主流的數據切片方法往往顯得「粗糙」。許多系統簡單地採用固定字符數、按句子或段落等規則進行切分，這種方式極易破壞文本原有的語義完整性，可能導致一個完整的邏輯思路或上下文聯繫在切分中斷裂，進而影響大模型對信息的準確理解和答案生成的質量。同時，這些簡單方法常常忽略文檔的內在結構，如章節、標題、列表和表格等，而這些結構本身就承載着重要的語義信息。面對不同類型（如法律合同、技術手冊、研究論文或代碼）和複雜格式的文檔，通用的「一刀切」切片策略往往難以達到理想效果。切片的大小也需精妙平衡：過小則可能上下文不足，難以支撐複雜問答；過大則可能引入過多噪聲，稀釋關鍵信息。此外，多數在數據預處理階段完成的靜態切片，也缺乏對用户動態查詢意圖的靈活適應性。&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;因此，理想的 RAG 數據切片策略應向更智能化、語義驅動的方向演進。其核心目標是&lt;strong&gt;最大程度地保持語義單元的完整性&lt;/strong&gt;，切分點應儘可能選在自然的語義邊界。同時，要充分感知並利用文檔的固有結構信息，如將標題及其對應內容作為一個單元，或整體處理表格及其註釋。為了保持切分後各知識塊之間的上下文連貫，可以採用重疊切片技術，或構建具有內在聯繫的層級式塊結構，並通過元數據明確記錄它們之間的邏輯關係。針對不同內容特性，應採用內容自適應的切片邏輯。至關重要的是，每個切分後的數據塊都應附帶豐富的元數據，如原始文檔出處、章節信息、主題標籤等，這些元數據不僅能提升檢索的精確度，還能為大模型提供更全面的背景知識，從而增強其輸出內容的可信度和可溯源性。&lt;/p&gt; 
&lt;div&gt;
 &lt;img src="https://oscimg.oschina.net/oscnet//f2fb964dde666532d21ce8a16e7310fe.jpg" width="1080" referrerpolicy="no-referrer"&gt;
&lt;/div&gt; 
&lt;h3&gt;&lt;strong&gt;&lt;span&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fzhida.zhihu.com%2Fsearch%3Fcontent_id%3D258754464%26content_type%3DArticle%26match_order%3D1%26q%3DPhysical%2BAI%26zhida_source%3Dentity" target="_blank"&gt;Physical AI&lt;/a&gt;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;中的 AI-ready Data&lt;/strong&gt;&lt;/h3&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;Physicla AI，如機器人和自動駕駛系統，需要在複雜的物理世界中進行感知、決策和行動，其數據需求具有獨特性和挑戰性。&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;strong&gt;訓練數據&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;形態&lt;/strong&gt;：來自多種傳感器的融合數據，包括激光雷達的點雲數據、攝像頭的圖像/視頻流、雷達信號、慣性測量單元數據、GPS 定位信息、觸覺傳感器數據等。此外，還包括機器人的關節狀態、運動軌跡、與環境的交互數據，以及大量來自模擬環境的合成數據。這類數據通常是時間序列數據，需要精確的時間同步。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;特徵&lt;/strong&gt;：要求數據能夠高保真地復現真實世界的物理特性和動態變化，覆蓋多樣化的環境條件（如不同天氣、光照）、複雜的交互場景和罕見的邊緣案例。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;示例&lt;/strong&gt;：自動駕駛領域的&lt;span&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fzhida.zhihu.com%2Fsearch%3Fcontent_id%3D258754464%26content_type%3DArticle%26match_order%3D1%26q%3DWaymo%2BOpen%2BDataset%26zhida_source%3Dentity" target="_blank"&gt;Waymo Open Dataset&lt;/a&gt;&lt;/span&gt;、&lt;span&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fzhida.zhihu.com%2Fsearch%3Fcontent_id%3D258754464%26content_type%3DArticle%26match_order%3D1%26q%3DnuScenes%25E6%2595%25B0%25E6%258D%25AE%25E9%259B%2586%26zhida_source%3Dentity" target="_blank"&gt;nuScenes 數據集&lt;/a&gt;&lt;/span&gt;。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;strong&gt;推理數據&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;形態&lt;/strong&gt;：來自機器人或車輛上搭載的各種傳感器的實時、連續的數據流。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;特徵&lt;/strong&gt;：數據處理的低延遲性對於物理 AI 系統做出及時、安全的決策和行動至關重要。系統還需要對傳感器噪聲、數據丟失或遮擋等情況具有魯棒性。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div&gt;
 &lt;img src="https://oscimg.oschina.net/oscnet//f6effd68cabc43be465afc81f71a66c4.jpg" width="1080" referrerpolicy="no-referrer"&gt;
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;審視這四大 AI 領域對數據的需求演變，可以發現一個清晰的趨勢：AI 模型對數據的「胃口」越來越大，要求的數據集規模日益龐大，多樣性和複雜性也與日俱增。從機器學習對結構化數據的依賴，到深度學習對海量非結構化數據的渴求，再到生成式 AI 對網絡規模多模態數據的吞噬，以及 Physical AI 對高維、多傳感器融合數據的整合，無不體現了這一趨勢。這種趨勢意味着，數據的「AI 就緒」不僅關乎數據本身的質量和形態，也對底層的數據存儲、處理和管理技術平台提出了更高的要求。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;打造 AI 的堅實基礎：通往 AI-ready Data 之路&lt;/strong&gt;&lt;/h2&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;將原始數據轉化為 AI-ready Data，是一項涉及多個步驟的持續性系統工程，而非一蹴而就的任務。這需要隨着 AI 技術、業務需求和數據源的變化而不斷演進和優化，是一個動態的、持續改進的過程。一個典型的數據準備流程始於數據收集與獲取，即從多樣化的內外部來源彙集原始數據，&lt;strong&gt;尤其值得強調的是，在 AI 時代，企業自身積累的、獨特的內部數據是構建差異化競爭優勢和深化護城河的核心戰略資產，對其的有效盤活與利用是首要任務。&lt;/strong&gt;隨後是數據清洗與預處理，旨在識別並修正原始數據中的錯誤、不一致、缺失值和重複項，以提升數據質量。接着進行數據轉換與豐富，將數據轉化為適合 AI 模型的格式，可能包括特徵工程、數據聚合，並通過添加元數據等方式增強數據上下文。對於監督學習任務，準確的數據標註是不可或缺的一環。在數據投入訓練之前，需進行嚴格的數據驗證與質量保證。最後，貫穿整個數據生命週期的是數據治理與安全，要求企業建立清晰的管理政策，確保數據合規、安全。&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;AI-ready Data 並非遙不可及的理想概念，而是成功且可靠的人工智能應用的堅實基石。正如高質量原材料是優質產品的先決條件，高質量的 AI-ready Data 是構建高性能 AI 模型的根本保障，&lt;strong&gt;特別是當這些數據源自企業內部，承載着特定業務洞察和運營經驗時，其轉化為 AI 洞察的能力，將直接賦能企業構建難以複製的競爭壁壘。&lt;/strong&gt;它能夠顯著提升模型的準確性和可靠性，加速 AI 應用的研發部署，並最終驅動商業價值和創新突破。因此，企業應將提升數據就緒水平，尤其是內部數據的「AI 就緒」水平，視為一項戰略要務，而非項目啓動後的被動補救。通往 AI 驅動的創新之路，很大程度上是由對自身獨特數據資產的深度挖掘和高質量準備鋪就的。&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;擁抱 AI-ready Data，意味着正視數據的挑戰，投入必要資源，建立完善的流程和文化，核心目標在於充分釋放企業內部沉澱數據的潛在價值。這無疑是一項艱鉅的任務，但其回報——通過人工智能洞察自身運營、優化決策、創新產品與服務，從而在市場競爭中佔據領先地位——將是無可估量的。生成式 AI 並非短暫趨勢，而是一場深刻的變革，而適配這種變革的數據基礎設施和數據就緒能力，&lt;strong&gt;特別是將企業獨有的內部數據轉化為驅動 AI 的優質燃料的能力，將是企業在這場變革中深化護城河、立於不敗之地的關鍵。&lt;/strong&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354670</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354670</guid>
      <pubDate>Sat, 10 May 2025 10:26:00 GMT</pubDate>
      <author>來源: 投稿</author>
    </item>
    <item>
      <title>RWKV 2025 生態內容徵集大賽 | 5 月投稿作品及評審結果</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;大家好，我們在 2024 年底推出了 「&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FUeKemVw9HnTU6FBL1iM0bg" target="_blank"&gt;RWKV 2025 生態內容徵集大賽&lt;/a&gt;」，公開徵集 RWKV 相關的作品，包括但不限於 RWKV 相關的論文、講解 RWKV 的教程，以及基於 RWKV 的應用等。&lt;/p&gt; 
&lt;p&gt;2025 年 5 月，活動共收到 RWKV 生態作品投稿 &lt;strong&gt;2 份&lt;/strong&gt;，包括 &lt;strong&gt;1 篇論文、1 個教程&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;本文將公佈 2025 年 5 月的活動投稿作品及評審結果。&lt;/p&gt; 
&lt;h2&gt;評審結果&lt;/h2&gt; 
&lt;h3&gt;評審結果省流版&lt;/h3&gt; 
&lt;table&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;th&gt;作品名稱&lt;/th&gt; 
   &lt;th&gt;作品分類&lt;/th&gt; 
   &lt;th&gt;投稿人&lt;/th&gt; 
   &lt;th&gt;初評獎項&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Maximizing Asynchronicity in Event-based Neural Networks&lt;/td&gt; 
   &lt;td&gt;論文&lt;/td&gt; 
   &lt;td&gt;biomems&lt;/td&gt; 
   &lt;td&gt;銀獎（2888 元）&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;RWKV-V7 模型解析與實戰：架構原理、機制剖析及自定義微調模型效果展示&lt;/td&gt; 
   &lt;td&gt;教程&lt;/td&gt; 
   &lt;td&gt;坤&lt;/td&gt; 
   &lt;td&gt;參與獎&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;下面是「&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FUeKemVw9HnTU6FBL1iM0bg" target="_blank"&gt;RWKV 2025 生態內容徵集大賽&lt;/a&gt;」 5 月投稿獲獎的作品介紹。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;論文類&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Maximizing Asynchronicity in Event-based Neural Networks&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;投稿鏈接：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2505.11165" target="_blank"&gt;https://arxiv.org/abs/2505.11165&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;投稿人：biomems&lt;/li&gt; 
   &lt;li&gt;獲獎類型：銀獎（2888 元）&lt;/li&gt; 
   &lt;li&gt;項目介紹：論文提出了一種新的異步到同步框架 EVA，用於實時事件相機數據處理&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;該框架基於 RWKV-6 構建了高效的異步編碼器，實現了逐事件的表示更新，並採用自監督學習方法獲得具有高度泛化能力的事件表示。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img alt="Maximizing Asynchronicity" src="https://oscimg.oschina.net/oscnet/up-3ca302b9d83762576c1d83a5c2add0e09c0.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;教程類&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;RWKV-V7 模型解析與實戰：架構原理、機制剖析及自定義微調模型效果展示&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;投稿鏈接：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F1904346608985944244%3Fshare_code%3D1nLMwML5XPvsB%26utm_psn%3D1904552110802055283" target="_blank"&gt;https://zhuanlan.zhihu.com/p/1904346608985944244?share_code=1nLMwML5XPvsB&amp;amp;utm_psn=1904552110802055283&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;投稿人：坤&lt;/li&gt; 
   &lt;li&gt;獲獎類型：參與獎&lt;/li&gt; 
   &lt;li&gt;項目介紹：從原理解析到微調實踐的全流程教程&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;首先帶領初學者一起初步理解 RWKV 架構，然後使用 RWKV-PEFT 微調倉庫進行了全流程的微調並展示了微調效果，在學習原理的同時，微調屬於自己的 RWKV。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img alt="RWKV-V7 模型解析與實戰" src="https://oscimg.oschina.net/oscnet/up-0c74d7e4f003a4e0322e3aee4c8f3e90ec4.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h2&gt;獎品/獎金髮放規則&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;實物獎品（RWKV 周邊等）&lt;strong&gt;以&lt;/strong&gt;順豐快遞&lt;/strong&gt;方式發出&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;獎金&lt;/strong&gt;以&lt;strong&gt;轉賬或第三方線上平台&lt;/strong&gt;等方式發放&lt;/li&gt; 
 &lt;li&gt;同一投稿作品有&lt;strong&gt;多位作者&lt;/strong&gt;的情況下，由&lt;strong&gt;作品投稿人&lt;/strong&gt;領取獎金，團隊內部&lt;strong&gt;自行協商分配獎金&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;二次投稿與獎項升級&lt;/h2&gt; 
&lt;p&gt;所有投稿作品均會獲得&lt;strong&gt;評審意見&lt;/strong&gt;。請根據評審意見優化你的作品，然後可&lt;strong&gt;再次投稿以升級獎項&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;獎項成功升級時，我們將補發&lt;strong&gt;前後兩個獎金的差價&lt;/strong&gt;。例如投稿作品從鐵獎（888 元）升級到銀獎（2888 元），則補發 2888-888=2000 元獎金。&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;&lt;strong&gt;附活動海報&lt;/strong&gt;，歡迎各位轉發！&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-4e11c3df39730d4d504ca57e04f84ed60f8.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;* 本活動最終解釋權歸元始智能所有。&lt;/strong&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354657</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354657</guid>
      <pubDate>Sat, 10 May 2025 09:42:00 GMT</pubDate>
      <author>來源: 投稿</author>
    </item>
  </channel>
</rss>
