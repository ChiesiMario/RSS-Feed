<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>開源中國-綜合資訊</title>
        <link>https://www.oschina.net/news/industry</link>
        <atom:link href="http://8.134.148.166:30044/oschina/news/industry" rel="self" type="application/rss+xml"></atom:link>
        <description>開源中國-綜合資訊 - Powered by RSSHub</description>
        <generator>RSSHub</generator>
        <webMaster>contact@rsshub.app (RSSHub)</webMaster>
        <language>en</language>
        <lastBuildDate>Tue, 11 Feb 2025 07:36:17 GMT</lastBuildDate>
        <ttl>5</ttl>
        <item>
            <title>DeepSeek 梁文鋒身家暴漲，有專家預計或超黃仁勳</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;彭博社報道顯示，7 位創業公司創始人和人工智能專家對 DeepSeek 的估值存在巨大分歧，估值區間在 10 億美元到 1550 億美元之間。按照彭博億萬富翁指數中間值估算，DeepSeek 估值約在 20 億至 300 億美元，而持有公司 84% 股份的梁文鋒，身家可能在 16.8 億到 252 億美元之間，有望躋身亞洲最富有的科技大亨之列，甚至問鼎中國首富。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;不同業內人士給出的估值差異極大。波士頓風險投資公司 Glasswing Ventures 創始人魯迪納・塞塞裏認為，按同行公司估值，DeepSeek 最少值 10 億美元；研究工程師 Sebastian Raschka 則覺得，憑藉強大的品牌認知度，其估值應在 20 億到 100 億美元之間，高於 Mistral AI。而 Sweat Free Telecom 創始人查納基亞・拉姆德夫的預測更為樂觀，認為 DeepSeek 估值可達 1550 億美元。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;148&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-d2fbcd3fec3d8b307089121298b01c62f34.webp&quot; width=&quot;700&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;此前 1 月有報道稱，軟銀集團洽談牽頭對 OpenAI 進行最高 400 億美元融資，融資後估值達 3000 億美元。若 DeepSeek 按此估值一半計算，梁文鋒個人財富或達 1260 億美元，有望超過英偉達 CEO 黃仁勳，身家遠超鍾睒睒等富豪，在同領域也將遠超字節跳動創始人張一鳴（2024 年福布斯中國內地富豪榜顯示張一鳴身價 456 億美元，梁文鋒身家或為其 3 倍） 。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;梁文鋒出生於 1985 年，本碩就讀於浙江大學信息與通用工程專業，師從項志宇，研究機器視覺，2010 年畢業。畢業後，他投身量化投資，成立幻方量化，僅 6 年管理規模達千億，成為 「量化四大天王」 之一。2023 年 5 月，梁文鋒決心進軍通用人工智能領域，7 月成立 DeepSeek，被視為量化投身 AI 創業第一人。2024 年 12 月底，DeepSeek 發佈的 DeepSeek-V3 火遍全網。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;不過，由於 DeepSeek 收入、利潤等財務數據保密，外界只能通過對比 OpenAI、Anthropic 等公司估值來推測其價值，這些估值僅供參考。梁文鋒的真實身家究竟幾何，還需時間揭曉。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333142</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333142</guid>
            <pubDate>Tue, 11 Feb 2025 07:18:14 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>「互聯網之子」 Aaron Swartz 雕像在互聯網檔案館揭幕</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;Aaron Swartz 的大理石雕像上週五在互聯網檔案館的禮堂揭幕，有大約 300 人出席，半身像下方刻有文字&lt;em&gt;「The Internet&#39;s Own Boy（互聯網之子）」&lt;/em&gt;。本週重 312 磅的雕像將先轉移至大廳，直至獲得許可放置在當地公園內。&lt;/p&gt; 
&lt;p&gt;揭幕儀式上，Creative Commons 聯合創始人 Lisa Rein 強調：「崇拜 Aaron 的前提是正確理解他的故事——他並非殉道者，而是為公眾已付費的科學研究成果應自由獲取而戰。」電子前沿基金會（EFF）執行董事 Cindy Cohn 則稱，這座雕像「提醒人們為真理與正義持續鬥爭」。科幻作家 Cory Doctorow 在視頻致辭中暗諷特朗普政府時期的政治環境：「這是一個希望稀缺的時代，但這座雕像應激勵我們讓世界變得更好」。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-f82cf0e74040f33cd165bbe250c47ae3e5c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Aaron Swartz 出生於 1986 年，參與了 RSS 和 Markdown、web.py 等項目的開發，被視為是 Reddit 的聯合創始人。&lt;/p&gt; 
&lt;p&gt;2011 年 1 月他因為在 MIT 下載學術論文而遭到逮捕，面臨最高 35 年的刑期，他拒絕了認罪協議，於 2013 年 1 月 11 日自殺身亡。他在當年被追授進入互聯網名人堂。&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;相關閲讀&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/news/92332/in-memory-of-aron-swartz&quot; target=&quot;news&quot;&gt;紀念 Aaron Swartz：他用生命捍衞了互聯網的開放和自由&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/news/36671/aaron-swartz-kill-himself&quot; target=&quot;news&quot;&gt;web.py 作者 Aaron Swartz 自殺身亡&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333126/aaron-swartz-marble-statue-unveiled-internet-archive</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333126/aaron-swartz-marble-statue-unveiled-internet-archive</guid>
            <pubDate>Sat, 08 Feb 2025 06:33:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>谷歌 DeepMind CEO：DeepSeek 模型是中國最好的作品，但炒作有點誇大</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;谷歌旗下人工智能公司 DeepMind 首席執行官戴米斯·哈薩比斯（Demis Hassabis）在巴黎一場谷歌主辦的活動上，對 Deepseek 的 AI 模型做出了評價。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0211/113748_Ev6o_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;哈薩比斯稱讚 DeepSeek 的模型是令人印象深刻的作品，並表示「我認為這可能是我見過中國最好的作品」&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;哈薩比斯認為，其在短時間內完成的開發和訓練成本控制方面表現出色，然而從技術角度來看，哈薩比斯指出，Deepseek 並沒有展示出非常大的科學進步。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;他表示：「儘管有很多人吹捧，但其實這背後並沒有真正的新的科學進步……它（DeepSeek）在人工智能中使用的是已知的技術。」他補充説，圍繞 DeepSeek 的炒作「有點誇張」。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;他還聲稱，DeepMind 本週發佈的 Gemini 2.0 Flash 模型比 DeepSeek 大模型更為高效。&lt;/p&gt; 
&lt;p&gt;此外，哈薩比斯還談到了通用人工智能（AGI）的前景，他認為 AI 行業正在走向 AGI，且可能在未來 5 年左右實現這一目標。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333095/deepseeks-ai-model-the-best-work-out-of-china</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333095/deepseeks-ai-model-the-best-work-out-of-china</guid>
            <pubDate>Sat, 08 Feb 2025 03:39:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>TIOBE 2 月榜單：Rust 達新高，Mojo 和 Zig 嶄露頭角</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;TIOBE 公佈了 2025&amp;nbsp;年 2 月的&lt;/span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.tiobe.com%2Ftiobe-index%2F&quot; target=&quot;_blank&quot;&gt;編程語言排行榜&lt;/a&gt;&lt;span style=&quot;color:#000000&quot;&gt;。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;64&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-1df184cb150f1180b39eb214604fb7eeef4.png&quot; width=&quot;700&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;自去年 &lt;a href=&quot;https://www.oschina.net/news/296488/tiobe-index-202406&quot;&gt;6 月&lt;/a&gt;成功超越了 C 成為了 TIOBE 指數中新的第二名之後，C++ 便穩定在了這一位置上。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;TIOBE CEO&amp;nbsp;Paul Jansen&amp;nbsp;點評道，「隨着全球對每秒計算能力的需求日益增長，而硬件的發展速度卻未能跟上這一需求，程序的運行速度變得越來越重要。正因如此，在 TIOBE 指數中，那些以速度見長的編程語言逐漸嶄露頭角也就不足為奇了。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;除了&amp;nbsp;C++，Go 也穩居榜單前 10，Rust 則達到 1.47% 的歷史新高。此外，以速度著稱的 Mojo 和 Zig 也分別位列第 51 和第 56 位，正在叩響前 50 名的大門。&lt;/span&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;你可能會好奇，Python 這種慢速語言是如何在這些競爭激烈的語言面前生存下來的。這是因為，除了性能之外，如今還有另一個驅動因素：&lt;strong style=&quot;color:#404040&quot;&gt;學習一門新編程語言的難易程度&lt;/strong&gt;。除了需要處理更多的數據，世界還需要更多的程序員。完全依賴 AI 開發應用程序目前還無法實現，因此對新程序員的需求依然非常高。由於軟件工程專業畢業生的數量遠遠無法滿足市場需求，許多非軟件工程師也開始紛紛加入編程的行列，而他們最喜歡的語言正是 Python。這就是為什麼 Python 依然能夠穩居編程語言的主流地位。&lt;/span&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong style=&quot;color:#333333&quot;&gt;TIOBE 2 月 TOP 20 編程語言&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;404&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-d0707406f2fb35a7fdf61ce778525b8f614.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong style=&quot;color:#333333&quot;&gt;TOP 10 編程語言 TIOBE 指數走勢（2002-2024）&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;226&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-70c51a5ab719ba0927f95909df990227d21.png&quot; width=&quot;700&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong style=&quot;color:#333333&quot;&gt;第 21-50 名編程語言排行&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;417&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-52892925983133de7bb115cd8a0c1d16f89.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;第 51-100 名如下，由於它們之間的數值差異較小，僅以文本形式列出（按字母排序）：&lt;/span&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ABC, ActionScript, Algol, Alice, Apex, APL, AutoLISP, CFML, CHILL, Clipper, CLIPS, Clojure, Crystal, Curl, Elm, Erlang, F#, Forth, Groovy, Hack, Icon, Inform, Io, JScript, LabVIEW, Modula-2, Mojo, MQL5, NATURAL, Nim, OCaml, Occam, OpenCL, OpenEdge ABL, PL/I, Q, Raku, Ring, Scheme, Simulink, Smalltalk, SPARK, SPSS, Stata, SystemVerilog, Vala/Genie, VHDL, Wolfram, X++, Zig&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;TIOBE 編程社區指數（The TIOBE Programming Community index）是一個衡量編程語言受歡迎程度的指標，該指數每月更新一次。評判的依據來自世界範圍內的工程師、課程和第三方供應商，包括流行的搜索引擎，如 Google、必應、雅虎、維基百科、亞馬遜、YouTube 和百度都被用於指數計算。值得注意的是，TIOBE 指數並不代表編程語言的好壞或編寫代碼的多少。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;該指數可以用來檢查你的編程技能是否還能跟上時代的步伐，或者在開始建立一個新的軟件系統時，基於指數對採用何種編程語言做出決策。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.tiobe.com%2Ftiobe-index%2Fprogramminglanguages_definition%2F&quot; target=&quot;_blank&quot;&gt;TIOBE 指數&lt;/a&gt;&lt;span style=&quot;color:#000000&quot;&gt;的定義方式，以及詳細榜單信息&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.tiobe.com%2Ftiobe-index%2F&quot; target=&quot;_blank&quot;&gt;均可查看官網&lt;/a&gt;。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333092/tiobe-index-202502</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333092/tiobe-index-202502</guid>
            <pubDate>Sat, 08 Feb 2025 03:32:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>Sam Altman：AI 成本每年暴跌 10 倍，2035 年人人都有超級大腦</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;OpenAI CEO Sam Altman &lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.samaltman.com%2Fthree-observations&quot; target=&quot;_blank&quot;&gt;更新了個人博客&lt;/a&gt;&lt;/u&gt;，其中他預測 AI 成本每年將暴跌 10 倍，並且到了 2035 年，人人都能擁有超級大腦。 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0211/111413_19Xl_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;文中，Altman 提到自己對 AI 經濟學的三點觀察：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;AI 模型的智能水平大致等於其訓練和運行所使用資源的對數；&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;使用固定級別 AI 的成本大約每 12 個月降低 10 倍，價格下降會極大促進 AI 的使用；&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;線性增長的智能水平所創造的社會經濟價值呈超指數級增長。&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;並且 Altman 總結，如果這三點趨勢繼續保持，AI 對社會的影響將是巨大的。 &amp;nbsp;Altman 還預測，AI Agents 最終可能會像「虛擬同事」一樣與人類協作，在各個領域的知識工作中發揮作用。&lt;/p&gt; 
&lt;p&gt;此外，Altman 還認為，在某些方面，AI 在經濟上的作用可能會類似於晶體管，AI 也能夠大規模推廣，並滲透到經濟的各個角落。 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;最後，Altman 還表示確保 AGI 的廣泛受益，讓 AGI 的好處惠及全社會至關重要。並且他提議持續降低智能計算的成本，讓人人都能負擔得起 AI，按照這一目標，或將在 2035 年人人都能獲得近乎無限的智能支持。&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;附上博客原文：&lt;/p&gt; 
&lt;h3&gt;三點觀察&lt;/h3&gt; 
&lt;p&gt;我們的使命是確保 AGI（通用人工智能）造福全人類。&lt;/p&gt; 
&lt;p&gt;如今，一些接近 AGI 的系統已經開始顯現，因此我們認為理解當前所處的階段至關重要。AGI 的定義較為模糊，但通常指的是一種能夠在人類水平上解決越來越複雜問題的系統，且適用於多個領域。&lt;/p&gt; 
&lt;p&gt;（作者註釋：本文使用「AGI」一詞，我們的目的是清晰表達，並無意改變或重新定義我們與微軟的合作關係，以及避免斷章取義的解讀，我們完全預計將與微軟保持長期合作。）&lt;/p&gt; 
&lt;p&gt;人類天生具有構建工具的能力，並且擁有理解和創造的驅動力，這促使世界不斷進步。每一代人都會在前人發現的基礎上進一步創新，創造出更強大的工具——&lt;strong&gt;從電力到晶體管，再到計算機、互聯網，如今則是 AGI。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;儘管人類的創新曆程並非一帆風順，但從長遠來看，這一進程始終推動着社會發展，使人們的生活在各個方面都得到極大改善。&lt;/p&gt; 
&lt;p&gt;從某種角度來看，AGI 只是人類不斷攀登進步階梯的又一個工具。但從另一個角度來看，它可能標誌着一個真正不同的時代的開始。未來的經濟增長前景令人驚歎，我們甚至可以設想一個世界：所有疾病都能被治癒，我們擁有更多時間陪伴家人，並能夠充分發揮自己的創造潛力。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;或許再過十年，地球上的每個人都能擁有比今天最具影響力的人更強的能力。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;我們持續見證 AI 發展的迅猛進步，以下是關於 AI 經濟學的三點觀察：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;AI 模型的智能水平大致等於其訓練和運行所使用資源的對數。這些資源主要包括訓練計算（compute）、數據和推理計算（inference compute）。目前的趨勢表明，只要投入足夠的資金，就能持續且可預測地提升 AI 能力，而支撐這一趨勢的縮放定律（Scaling Laws）在多個數量級範圍內都被證明是準確的。&lt;/li&gt; 
 &lt;li&gt;使用固定級別 AI 的成本大約每 12 個月降低 10 倍，價格下降會極大促進 AI 的使用。一個明顯的例子是 GPT-4 在 2023 年初的使用成本，相比 GPT-4o 在 2024 年中期，其每個 token 的價格下降了約 150 倍。摩爾定律每 18 個月帶來 2 倍的性能提升，而 AI 成本下降的速度遠超這一趨勢，影響將更加深遠。&lt;/li&gt; 
 &lt;li&gt;線性增長的智能水平所創造的社會經濟價值呈超指數級增長。這一趨勢意味着，對於 AI 的指數級投資在可預見的未來不會停止。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;如果這三點趨勢繼續保持，AI 對社會的影響將是巨大的。&lt;/p&gt; 
&lt;p&gt;目前，&lt;strong&gt;我們已經開始推出 AI Agents，它們最終可能會像「虛擬同事」一樣與人類協作。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;以軟件工程領域的 AI Agent 為例——這是我們認為極為重要的應用方向之一。設想未來的 AI Agent 能夠完成大部分經驗 3-5 年的頂級公司軟件工程師可以完成的任務，但任務時長限制在幾天內。它不會有突破性的創新想法，需要大量的人類監督和指導，在某些方面表現出色，同時在某些意想不到的地方表現較差。&lt;/p&gt; 
&lt;p&gt;儘管如此，它仍可以被視作一名真實但相對初級的虛擬同事。&lt;strong&gt;現在，想象一下如果有 1000 個這樣的 AI Agnet，或者 1000000 個。再進一步，設想這樣的 AI Agnet 被應用到所有知識型工作領域，其影響將難以估量。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在某些方面，AI 在經濟上的作用可能會類似於晶體管——一個重大科學突破，能夠大規模推廣，並滲透到經濟的各個角落。如今，我們不會特別關注晶體管或生產晶體管的公司，但它們的存在讓我們的計算機、電視、汽車、玩具等設備變得更加強大、近乎奇蹟般地運作。&lt;/p&gt; 
&lt;p&gt;世界的變化不會一蹴而就，它從未如此。短期內，生活仍將繼續，2025 年的人們大概率會和 2024 年一樣度過日常——我們仍會相愛、組建家庭、在網上爭論、去大自然中遠足等等。&lt;/p&gt; 
&lt;p&gt;然而，未來的到來將不可忽視，長期來看，社會和經濟的變化將是巨大的。人類將找到新的事物去探索，找到新的方式去互相幫助、去競爭，但這些方式可能與今天的工作模式截然不同。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;在這樣的時代，主動性、意志力和決策能力將變得尤為寶貴。正確地決定要做什麼，並在不斷變化的世界中找到前進的道路，將具有極高的價值。因此，韌性和適應能力將成為關鍵技能。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;AGI 將是史上最強大的槓桿，極大增強人類的主觀能動性，它不會削弱個人的影響力，反而會讓個體的能力比以往任何時候都更強大。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;AGI 的影響不會均勻分佈。某些行業可能變化不大，但科學進步的速度可能比今天快得多，甚至可能超越 AGI 帶來的所有其他變革。&lt;/p&gt; 
&lt;p&gt;長期來看，許多商品的價格將大幅下降（目前，智能成本和能源成本是許多行業的主要限制因素）。與此同時，奢侈品和一些稀缺資源（如土地）的價格可能反而會飆升。&lt;/p&gt; 
&lt;p&gt;從技術角度來看，AGI 的發展道路相對清晰。但如何將 AGI 融入社會，公共政策和社會共識將起到至關重要的作用。&lt;strong&gt;這也是我們不斷儘早、頻繁推出 AI 產品的原因之一——讓社會與技術共同演進，為未來做好準備。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;AI 將滲透到經濟和社會的方方面面，未來，我們會期待一切都變得智能化。面對這一趨勢，許多人認為應該給予個人更多對技術的控制權，比如開放源碼等措施，同時也要接受在安全性與個體賦權之間找到平衡，必然需要做出一些取捨。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;我們始終希望避免魯莽行事，未來在 AGI 安全性方面可能會做出一些不受歡迎的重要決策和限制。但總體而言，隨着 AGI 的逐步實現，我們認為更傾向於個體賦權是正確的方向。否則，我們可能會看到另一條道路。&lt;/p&gt; 
&lt;p&gt;確保 AGI 的廣泛受益，讓 AGI 的好處惠及全社會至關重要。從歷史來看，科技進步通常會改善健康狀況、經濟繁榮等關鍵指標，且長期來看整體趨勢是向好的。但技術本身不會自動帶來更大的平等，如果希望在社會公平方面做得更好，我們可能需要新的思維方式。&lt;/p&gt; 
&lt;p&gt;尤其值得關注的是，資本與勞動力之間的力量平衡可能會被打破，這可能需要及早幹預。&lt;/p&gt; 
&lt;p&gt;我們願意考慮一些聽起來不太尋常的想法，比如給每個人分配一定的「計算預算」（compute budget），讓全球所有人都能充分利用 AI。&lt;strong&gt;當然，也有一種更簡單的方法：持續降低智能計算的成本，讓人人都能負擔得起 AI。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;到 2035 年，每個人都應該能夠調用相當於 2025 年全人類智慧總和的智力資源。所有人都應當獲得近乎無限的智能支持，並自由地發揮想象力&lt;/strong&gt;。目前，世界上仍有大量人才因缺乏資源而無法充分發揮自己的潛力，如果我們改變這一點，全球的創造力將迎來爆發式增長，併為所有人帶來巨大的福祉。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333079/sam-altman-three-observations</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333079/sam-altman-three-observations</guid>
            <pubDate>Sat, 08 Feb 2025 03:14:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>被面試官拷問三個小時，應屆博士無緣 DeepSeek</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbaijiahao.baidu.com%2Fs%3Fid%3D1823558407486179899%26wfr%3Dspider%26for%3Dpc&quot; target=&quot;_blank&quot;&gt;據媒體報道&lt;/a&gt;&lt;/u&gt;，應聘者劉哲回憶起去年 5 月參加 DeepSeek 線上面試的經歷。那時，面試官連續 3 小時的高強度提問讓他倍感壓力。儘管他作為 211、985 高校的應屆博士生，在校期間已嶄露頭角，但面對那些深入且具有挑戰性的問題，他仍感到不小的難度。&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:#222222&quot;&gt;「最開始有一個 coding（編程）環節，會根據應聘者的專業出題，測試結果將決定應聘者是否能進入面試環節。面試持續 3 小時，由兩位面試官分別進行，每位面試官負責 1.5 小時。其中一位面試官會深入考察機器學習的基礎知識，連續提問 1.5 小時，據説所有應聘者都會被問到相同的題目，以便於他們進行比較和排名。接下來的 1.5 小時則是針對項目經驗的討論，他們特別關注應聘者在項目執行過程中的思考方式。」據劉哲所述，面試由 HR 主持，兩位面試官都很年輕，不超過 30 歲，整個面試過程體驗良好，能夠感受到團隊充滿青春活力。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-7ed8b4dc5acec727a286bfd36d21d80307e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;「在我所經歷過的互聯網公司中，&lt;strong&gt;DeepSeek 是唯一一家會根據應聘者的專業背景量身定製編程題目的公司。&lt;/strong&gt;」回顧面試經歷，劉哲這樣描述。在劉哲看來，DeepSeek 的崛起似乎是必然的。他透露，應聘者普遍來自清華、北大等頂尖學府，面試過程嚴謹且要求高，當時招聘並未設定人數上限，明顯感受到公司旨在網絡頂尖智慧人才，只招收天才級別的精英。&lt;/p&gt; 
&lt;p&gt;網絡上也有人在分享面試 DeepSeek 的經歷時表示遇到了出乎意料的問題。例如，面對「DPO 為什麼用 KL 散度,不用交叉熵?機器學習中什麼時候必須用 KL 散度，什麼時候必須用交叉熵,什麼時候兩者可互換」這樣的問題，有網友不禁感嘆：「這還是我能理解的中文嗎？」&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-f13afeea08b0fb9b3fe3418f8dd4256571b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;據知情人透露，有朋友曾參與 DeepSeek 的面試，並直接與創始人對話。總體感受是，公司充滿願景，洋溢着理想主義精神，研究氛圍優於高校實驗室，非常適合對 AI 充滿熱情的研究人員。&lt;/p&gt; 
&lt;p&gt;另外，一些參加過 DeepSeek 面試的人表示，公司不設 KPI 考核，採取扁平化管理模式，每位核心算法人員都能直接與梁文峯探討問題，不太像傳統公司，更像大學的一個研究團隊。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333075</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333075</guid>
            <pubDate>Sat, 08 Feb 2025 03:04:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>全球開源大模型前十均為阿里通義千問衍生模型</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;近日，全球最大 AI 開源社區 Huggingface 發佈了最新的開源大模型榜單（Open LLM Leaderboard），其中榜單顯示，其排名前十的開源大模型全部是基於阿里通義千問（Qwen）開源模型二次訓練的衍生模型。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-5d2384f1f2035007cae119894ebe4235ed3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;來源：&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fspaces%2Fopen-llm-leaderboard%2Fopen_llm_leaderboard%23%2F&quot; target=&quot;_blank&quot;&gt;https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/&lt;/a&gt;&lt;/u&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;據悉，Open LLM Leaderboard 是目前全球最具權威性的開源大模型榜單，其測試維度涵蓋閲讀理解、邏輯推理、數學計算、事實問答等。&lt;/p&gt; 
&lt;p&gt;而通義千問 Qwen 大模型已經成為全球最大的開源模型族羣。在海內外開源社區中，Qwen 的衍生模型數量已突破 9 萬，超越美國 Meta 公司旗下的 Llama 系列開源模型，位居全球第一。在 Hugging face2024 年的開源模型下載中，Qwen 模型系列中的 Qwen2.5-1.5B-Instruct 的下載量佔總下載量的 26.6%，是全球下載量最高的開源模型。&lt;/p&gt; 
&lt;p&gt;此外，此前爆火的 DeepSeek 公司基於 R1 推理模型蒸餾了 6 個模型開源給社區，其中有 4 個模型來自 Qwen。近期，著名 AI 科學家李飛飛團隊用較少的資源和數據訓練出的 s1 推理模型，同樣以 Qwen 模型為基礎模型。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333071</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333071</guid>
            <pubDate>Sat, 08 Feb 2025 02:54:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>馬斯克欲以 7000 億收購 OpenAI，奧特曼回應：不如讓我們收購 X</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;2025 年 2 月 10 日（路透社）&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.reuters.com%2Fmarkets%2Fdeals%2Felon-musk-led-group-makes-974-billion-bid-control-openai-wsj-reports-2025-02-10%2F&quot; target=&quot;_blank&quot;&gt;報道稱&lt;/a&gt;&lt;/u&gt;，由埃隆·馬斯克（Elon Musk）領銜的財團週一表示，已經提出以 974 億美元（當前約 7,115 億元人民幣）收購 OpenAI 的運營資產。報道指出，這一舉動可能會對蓬勃發展的 AI 行業產生重大影響。數月前，這位億萬富翁曾起訴這家人工智能初創公司，試圖阻止其向營利性企業過渡。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-caff5ae16c7c36c9f1368d8366a9be892c9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;此次收購提議的背景是，OpenAI 首席執行官薩姆・阿爾特曼正在嘗試對公司進行重組，計劃將其非營利董事會與盈利業務分離。然而，目前尚不清楚阿爾特曼和非營利董事會是否已經就過渡價格達成一致。&lt;/p&gt; 
&lt;p&gt;對於這一消息，阿爾特曼在 X 平台上&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Felonmusk%2Fstatus%2F1889062013109703009&quot; target=&quot;_blank&quot;&gt;回應稱&lt;/a&gt;&lt;/u&gt;：「不用了，謝謝，但如果你願意，我們可以以 97.4 億美元的價格收購推特。」&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;516&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0211/105032_Gj7Z_2720166.png&quot; width=&quot;1280&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;馬斯克隨後還單獨發佈了&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Felonmusk%2Fstatus%2F1889070627908145538&quot; target=&quot;_blank&quot;&gt;推文&lt;/a&gt;&lt;/u&gt;，稱奧特曼是「Scam Altman（騙子奧特曼）」。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;1070&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0211/104848_aRfy_2720166.png&quot; width=&quot;1286&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333069</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333069</guid>
            <pubDate>Sat, 08 Feb 2025 02:50:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>字節跳動開源大語言模型應用開發框架 Eino</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;字節跳動技術團隊發文&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FJQREuZyI6ug3cc9Ov7diog&quot; target=&quot;_blank&quot;&gt;宣佈&lt;/a&gt;，基於 Golang 的大模型應用綜合開發框架 Eino 已正式開源，旨在提供簡潔、可擴展、可靠的開發工具。&lt;/p&gt; 
&lt;p&gt;據悉，Eino 基於明確的「組件」定義，提供強大的流程「編排」，覆蓋開發全流程，旨在幫助開發者以最快的速度實現最有深度的大模型應用。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-74d0fd011f9ebf4fd3950e2f6cc1b0f23bf.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;項目地址：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;https://github.com/cloudwego/eino&lt;/li&gt; 
 &lt;li&gt;https://github.com/cloudwego/eino-ext&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;字節跳動技術團隊介紹，Eino 作為旨在覆蓋 devops 全流程的大模型應用開發框架，具有內核穩定、API 簡單易懂、豐富的擴展性、提供開箱即用的配套工具等特點，能夠幫助開發者快速、簡單的上手。&lt;/p&gt; 
&lt;p&gt;Eino 已成為字節跳動內部大模型應用的首選全代碼開發框架，已有包括豆包、抖音、釦子等多條業務線、數百個服務接入使用。字節跳動表示，未來還將以 Eino 開源庫為核心代碼倉庫，堅持內外用一套代碼，與社區共建最優秀的大模型應用開發框架。&lt;/p&gt; 
&lt;p&gt;Eino&amp;nbsp;借鑑了 LangChain 和 LlamaIndex 等開源框架的優勢，並結合前沿研究，提供了一系列豐富的組件抽象，如 ChatModel、Tool、ChatTemplate 等，方便用户組合開發。通過強大的編排框架（Graph、Chain），Eino 支持類型檢查、流式處理、併發管理等功能，簡化了開發流程。&lt;/p&gt; 
&lt;p&gt;Eino 框架結構圖：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0211/103841_1C2B_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;此外，Eino 提供了流式處理、回調機制、可視化開發工具等功能，幫助開發者高效構建 AI 應用。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333062</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333062</guid>
            <pubDate>Sat, 08 Feb 2025 02:38:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>外交部回應 DeepSeek 引發國際廣泛關注討論</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;在 2 月 10 日外交部例行記者會上，有記者提問稱，日前中國人工智能企業深度求索（DeepSeek）推出性能優越、免費商用的開源大模型，且訓練成本相較同類產品更低，在國際上引起廣泛關注和熱烈討論，請問發言人對此有何評論？&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;外交部發言人郭嘉昆表示：具體的專業問題建議向主管部門瞭解。我想強調的是，當前，人工智能的新技術不斷突破，新業態持續湧現，新應用加快拓展，已經成為新一輪科技革命和產業變革的重要驅動力量。中國積極擁抱智能變革，大力推進人工智能創新發展，重視人工智能安全，支持鼓勵企業自主創新，為全球人工智能發展作出了積極貢獻。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;中方積極推動人工智能普惠發展，幫助發展中國家加強能力建設，主張開源人工智能技術，促進人工智能服務的可及性，實現各國共享智能紅利。同時，我們反對以意識形態劃線，反對泛化國家安全概念、將經貿問題政治化的做法。中方願同各方加強人工智能交流合作，堅持以共商促共享，攜手打造開放包容、互利共贏的發展環境，共同在人工智能的廣闊天地裏深度求索。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333056</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333056</guid>
            <pubDate>Sat, 08 Feb 2025 01:51:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>操作教程丨使用 1Panel 開源面板快速部署 DeepSeek-R1</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;近期，DeepSeek-R1 模型因其在數學推理、代碼生成與自然語言推理等方面的優異表現而受到廣泛關注。作為能夠有效提升生產力的工具，許多個人和企業用户都希望能在本地部署 DeepSeek-R1 模型。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;通過 1Panel 的應用商店能夠簡單、快速地在本地部署 DeepSeek-R1 模型。本教程將按照安裝 Ollama→安裝 OpenWebUI→安裝並使用 DeepSeek-R1 的順序，為您介紹使用 1Panel 開源面板部署 DeepSeek-R1 模型的具體步驟。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;在部署好 DeepSeek 之後，用户也可以嘗試將其與 MaxKB 開源知識庫問答系統進行對接，構建一個自己的 Chatbox，也就是一個與 DeepSeek 大模型的智能會話界面。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;strong&gt;&lt;span style=&quot;color:#005eeb&quot;&gt;一、從 1Panel 應用商店安裝 Ollama&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;Ollama 是一個開源的本地大語言模型運行框架，專門為在本地便捷部署和運行大型語言模型而設計。為了能夠正常運行 DeepSeek-R1 模型，需要先在本地安裝 Ollama，本章節將介紹具體步驟。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;首先，從 1Panel 首頁進入「應用商店」，在「AI/大模型」分類下找到 Ollama，點擊「安裝」按鈕進行安裝。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt;
 &lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-3eb857757eb258cc4161d9b4046acdfe2e0.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;/div&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;在安裝詳情頁中，您可以自定義端口並勾選「端口外部訪問」選項，其他設置保持默認，最後點擊「確認」按鈕。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;987&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-0f1f981b2e3061d054c784539d621db3ab6.png&quot; width=&quot;1939&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;Ollama 安裝成功後，在 1Panel 操作界面中返回應用商店的「已安裝」標籤頁，看到 Ollama 已經安裝成功。此時點擊「服務端口:11434」按鈕，瀏覽器自動跳轉新標籤頁，若標籤頁內顯示「Ollama is running」，則表示 Ollama 已成功安裝並正常運行。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;987&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-dd4e30cec3cefda771ea15fb930c61b37ae.jpg&quot; width=&quot;1939&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;987&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-1fa60c450c2a377092f78c7c44a5a1467c0.png&quot; width=&quot;1939&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span style=&quot;color:#005eeb&quot;&gt;二、從 1Panel 應用商店安裝 OpenWebUI&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;由於 Ollama 本身沒有用户交互界面，為了提升模型的使用體驗，我們需要從 1Panel 的應用商店中安裝 OpenWebUI。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;987&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-5e5456150b638b4dbe716ca3839f814339b.png&quot; width=&quot;979&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;OpenWebUI 是一款高度可擴展、功能豐富、操作便捷的自託管 AI 平台，它提供了一個更直觀的用户界面，同時增強了安全性和擴展性。OpenWebUI 與 Ollama 相結合，能夠方便地管理和使用本地部署的大型語言模型。本章節將介紹安裝 OpenWebUI 的具體步驟。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;確認 Ollama 正常運行後，返回 1Panel 應用商店，在「AI/大模型」分類中找到 OpenWebUI，點擊「安裝」按鈕進行安裝。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;987&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-3d0545f49855958419fbb4aacff814f0760.jpg&quot; width=&quot;1939&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;在安裝詳情頁中，您需要填寫 Ollama 服務地址和 Secret Key，並勾選「端口外部訪問」選項，其他設置保持默認，最後點擊「確認」按鈕。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;987&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-9ed84ad322dc29653252b58c00a0abe586e.png&quot; width=&quot;1939&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;OpenWebUI 安裝成功後，返回 1Panel 應用商店「已安裝」標籤頁，點擊「服務端口:3000」按鈕，進入 OpenWebUI 控制枱（注意：如果瀏覽器未能顯示 OpenWebUI 控制枱頁面，請在 OpenWebUI 應用顯示的「已安裝」時間超過 5 分鐘以後再進行訪問）。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;987&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-9aa59370c5af4fd4ad89aec078704932e3a.png&quot; width=&quot;1939&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;在 OpenWebUI 控制枱中，依次設置「名稱」、「電子郵箱」和「密碼」，完成設置後，點擊「創建管理員賬號」按鈕。稍等片刻，頁面會彈出更新提示窗口，點擊「確認，開始使用！」按鈕，提示窗口消失後，即可開始使用 OpenWebUI。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;987&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-cfb617497b6a6c3ca0f93ce15df4f43edf8.png&quot; width=&quot;1939&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span style=&quot;color:#005eeb&quot;&gt;三、通過 OpenWebUI 安裝並使用 DeepSeek-R1&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;本章節將介紹如何通過 OpenWebUI 安裝和使用 DeepSeek-R1 模型。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;strong&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;1. 安裝 DeepSeek-R1 模型：&lt;/span&gt;&lt;/strong&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;返回至 OpenWebUI 控制枱頁面，點擊頁面左上角的「選擇一個模型」選項，在搜索框中輸入「deepseek-r1:1.5b」，然後點擊「從 Ollama.com 拉取 deepseek-r1:1.5b」選項。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;987&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-060b281d5425c3d9dd26e454ce843597c57.png&quot; width=&quot;1939&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;2. 使用 DeepSeek-R1 模型：&lt;/span&gt;&lt;/strong&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;模型下載完成後，刷新頁面，確認頁面左上角的模型顯示為「deepseek-r1:1.5b」。然後點擊屏幕中央的輸入框，就可以開始和 DeepSeek-R1 模型對話了。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;922&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-309adcc2cdecb9c8f9caaff84223ca49b0d.png&quot; width=&quot;1608&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;稍等片刻，收到回覆後即可確認 DeepSeek-R1 已通過 1Panel 成功部署到您的服務器。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;987&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-f5748142a990c5a72b8fb2d6db37312c833.png&quot; width=&quot;1939&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span style=&quot;color:#005eeb&quot;&gt;四、使用 GPU 為 DeepSeek- R1 加速&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;使用 GPU 為 DeepSeek-R1 加速可以顯著提升模型的推理速度。在本章節中，我們以 NVIDIA GPU 為例，介紹如何在 1Panel 中為 DeepSeek-R1 配置加速。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;strong&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;重要提示：在啓用 GPU 加速之前，請確保服務器已經安裝 GPU 卡並配置了相關驅動。&lt;/span&gt;&lt;/strong&gt;&lt;br&gt; &lt;span style=&quot;color:#3e3e3e&quot;&gt;進入 1Panel 應用商店的「已安裝」標籤頁，點擊 Ollama 應用下的「參數」按鈕。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;987&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-579e56d39751e426ecc172ba3966317a62a.png&quot; width=&quot;1939&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;點擊「編輯」按鈕進入參數配置頁面的「詳情」界面，勾選「高級設置」選項，並啓用「編輯 compose 文件」選項。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt;
 &lt;img src=&quot;https://oscimg.oschina.net/oscnet//47c9356c33a5e702b5a41a3de4be878b.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;/div&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;此時，在 compose 內容編輯框中，輸入以下與 GPU 相關的代碼：&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;reservations:
       devices:
              -  driver: nvidia
                  count: all
                  capabilities: [gpu]&lt;/code&gt;&lt;/pre&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;輸入完成後，點擊編輯頁面中的「確認」按鈕，Ollama 會自動重建。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;當重建完成後，Ollama 在 1Panel 應用商店中的狀態變更為「已啓動」，此時可以使用 NVIDIA GPU 為 DeepSeek-R1 提供加速。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;987&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-37ea6f8a8900338c209c5bbb3b25bcac4ac.png&quot; width=&quot;1939&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span style=&quot;color:#005eeb&quot;&gt;五、企業如何用好 DeepSeek？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;DeepSeek 部署完成後，該如何讓各個業務部門使用好這個能力超強的大模型呢？&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;這時候，MaxKB 開源知識庫問答系統就可以發揮積極作用了。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;1080&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-b3d7952c60ed617db87088d004d8cf9fa7b.jpg&quot; width=&quot;1920&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;首先，MaxKB 可以為本地部署的 DeepSeek 構建一個 Chatbox，也就是一個智能對話的界面，類似於個人用户直接與 DeepSeek 進行對話。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;MaxKB 提供的 Chatbox 可以方便地嵌入到企業 OA 系統和業務系統，讓員工使用更加便捷、安全。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#000000; text-align:start&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;987&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-ba43c99463a01d2e8219fa6a91eb22765b8.png&quot; width=&quot;1939&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;另一方面，企業內部有非常多的私有知識文檔，這些內容是經過長期的積累和不斷修訂形成的，在企業內部可以形成知識庫問答系統為企業的員工、合作伙伴和客户提供服務。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;904&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-3762219074a17fe370af4ecb448ce1f2a4b.png&quot; width=&quot;947&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;span style=&quot;color:#3e3e3e&quot;&gt;MaxKB 提供開箱即用的 RAG（Retrieval-Augmented Generation，檢索增強生成）技術，能夠結合私有知識庫提升問答效果，有效降低大模型幻覺。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333030</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333030</guid>
            <pubDate>Fri, 07 Feb 2025 14:31:00 GMT</pubDate>
            <author>來源: 投稿</author>
        </item>
        <item>
            <title>Linux 內核新補丁調整 AC 電源插拔行為，向 Windows 看齊以提升硬件兼容性</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;blockquote&gt; 
 &lt;p&gt;AMD 工程師主導優化，解決便攜設備休眠喚醒痛點。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;近日，AMD 工程師 Mario Limonciello 向 Linux 內核&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flore.kernel.org%2Flinux-pm%2F20250208162210.3929473-1-superm1%40kernel.org%2F&quot; target=&quot;_blank&quot;&gt;提交了一系列補丁&lt;/a&gt;&lt;/u&gt;，旨在調整系統在&lt;strong&gt;s2idle（掛起到空閒）&lt;/strong&gt;狀態下的 AC 電源插拔行為，使其更貼近 Windows 11 的邏輯。&lt;/p&gt; 
&lt;p&gt;這一改動主要針對筆記本電腦、手持遊戲設備（如 Steam Deck 同類產品）在休眠時因電源狀態切換導致的兼容性問題，尤其是此前曝光的 Legion Go S（搭載 AMD Ryzen Z2 芯片）的固件級故障。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;為何需要「模仿」Windows？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;當前，Linux 與 Windows 在 s2idle 狀態下的電源行為存在關鍵差異：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;：插入或拔出 AC 電源時，系統會完全喚醒，若後續無用户操作則重新進入睡眠。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt;：AC 事件僅觸發短暫喚醒後立即重回休眠，可能導致硬件固件因快速狀態切換出現異常（例如某些設備無法正確處理快速進入/退出低功耗模式）。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Limonciello 指出，由於 OEM 廠商通常基於 Windows 進行硬件驗證，Linux 的差異行為易暴露底層固件缺陷。新補丁通過記錄休眠前的電池狀態，並在 AC 事件後對比狀態變化，決定是否徹底喚醒系統，從而減少「兼容性陷阱」。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;技術細節：喚醒機制與能耗監控&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;喚醒邏輯重構&lt;/strong&gt;&lt;br&gt; 補丁在 ACPI 電池驅動中新增&lt;code&gt;suspend_state&lt;/code&gt;字段，休眠時保存當前電源狀態（如是否充電）。若喚醒後檢測到狀態變化（如從充電變為放電），則觸發系統完全喚醒，而非立即休眠。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;能耗統計透明化&lt;/strong&gt;&lt;br&gt; 新增&lt;code&gt;/sys/power/suspend_stats/last_sleep_energy&lt;/code&gt;文件，以&lt;strong&gt;毫安時（mAh）&lt;/strong&gt;為單位記錄上次休眠週期的電池消耗量，方便用户空間工具分析功耗問題。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;爭議與用户控制&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;儘管新行為默認啓用，但開發者社區對其適用場景存在分歧。例如，若筆記本合蓋時連接電源，是否應強制喚醒？Limonciello 認為，這與用户外接擴展塢的場景需求一致，但用户仍可通過禁用 ACPI 電池設備的&lt;code&gt;power/wakeup&lt;/code&gt;屬性恢復舊邏輯。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;影響與未來展望&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;此次調整尤其利好搭載 AMD 芯片的設備，但惠及所有支持 s2idle 的 x86/ARM 平台。隨着 Linux 在掌機市場的滲透（如 Steam OS 設備），此類優化將顯著提升用户體驗。此外，補丁的「Windows 兼容性驅動」思路或成為未來硬件支持的新範式，減少廠商因生態差異對 Linux 的適配成本。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;結語&lt;/strong&gt;&lt;br&gt; Linux 在電源管理領域的「向 Windows 學習」，並非妥協，而是以用户體驗為優先的務實選擇。這一補丁不僅修復了長期存在的兼容性痛點，也為開源生態與 OEM 廠商的協作提供了新思路。未來，類似「求同存異」的優化或成常態，進一步模糊兩大操作系統在硬件支持上的體驗邊界。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/333017/linux-patches-ac-plug-s2idle</link>
            <guid isPermaLink="false">https://www.oschina.net/news/333017/linux-patches-ac-plug-s2idle</guid>
            <pubDate>Fri, 07 Feb 2025 11:24:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>圖解系列｜DeepSeek-R1 的出眾推理能力從何而來？</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                    
                                                                                                                                                    &lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;編者按：&lt;/strong&gt; DeepSeek-R1 到底有什麼特別之處？它為什麼能在推理任務上取得如此出色的表現？這背後的訓練方法又藴含着怎樣的創新？&lt;/p&gt; 
 &lt;p&gt;當我們需要模型處理數學題、編程任務，或是進行邏輯分析時，高質量的推理能力顯得尤為重要。然而，傳統的訓練方法往往需要耗費大量人力物力，這對許多研究團隊和企業來説都是不小的負擔。&lt;/p&gt; 
 &lt;p&gt;今天這篇深度解析 DeepSeek-R1 訓練方法的文章，將展示一個令人耳目一新的解決方案：如何通過創新的強化學習方法，在少量高質量人工標註數據的情況下，打造出一個推理能力出眾的 AI 模型。文章詳細介紹了 DeepSeek 團隊如何通過&quot;自動驗證機制&quot;來訓練模型，這種方法不僅大大降低了對人工標註數據的依賴，還能持續提升模型的推理質量。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;作者 | Jay Alammar&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;編譯 | 嶽揚&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-6fa463c0670c30f6fa2098b0a2cfb8cedbd.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;DeepSeek-R1 代表了人工智能發展的又一重要里程碑。對於機器學習領域的研究人員與開發者羣體而言，這次發佈之所以備受關注，主要有以下兩點：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;首先，這是一款開源權重的模型，並且提供了更小的、經過蒸餾的版本；&lt;/li&gt; 
 &lt;li&gt;其次，它公佈並深入探討了訓練方法，該方法能夠復現類似於 OpenAI O1 的推理模型。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;本文將帶您瞭解這一模型的構建過程。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;目錄&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;01 回顧：大語言模型（LLMs）的訓練方法&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;02 DeepSeek-R1 的訓練步驟&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2.1- 長推理鏈的 SFT 數據&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2.2- 一個過渡性的、擅長推理的高質量大語言模型（但在非推理任務上表現稍遜）&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2.3- 利用大規模強化學習（RL）構建推理模型&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2.3.1- 以推理為導向的大規模強化學習（R1-Zero）&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2.3.2- 利用過渡性推理模型生成 SFT 推理數據&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2.3.3- 常規強化學習訓練階段&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;03 模型架構&lt;/strong&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;01 回顧：大語言模型（LLMs）的訓練方法&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;與大多數現有的大語言模型一樣，DeepSeek-R1 也是逐個生成 token，但其獨特之處在於擅長解決數學和推理問題。這是因為它能夠通過生成一系列思考 tokens 來詳細闡述其思考過程，從而更加深入地處理問題。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-5b41809ec9dc436f27455aa39b8ef58c830.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;下圖摘自書籍《Hands-On Large Language Models》的第 12 章，展示了創建高質量大語言模型的三個主要步驟：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-5960076009014b645e62ad11df7e601f3dd.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;1）&lt;strong&gt;語言建模階段&lt;/strong&gt;，我們利用海量的網絡數據訓練模型預測下一個詞彙，從而得到一個基礎模型。&lt;/p&gt; 
&lt;p&gt;2）&lt;strong&gt;監督式微調階段&lt;/strong&gt;，這一步驟讓模型在執行指令和回答問題時更加得心應手，進而得到一個指令調優的模型或稱為監督式微調/SFT 模型。&lt;/p&gt; 
&lt;p&gt;3）最後是&lt;strong&gt;偏好調優階段&lt;/strong&gt;，這一步驟進一步優化模型的行為，使其更符合人類偏好，最終形成的是你在各種平台和應用中使用的偏好調優後的 LLM。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;02 DeepSeek-R1 的訓練步驟&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;DeepSeek-R1 遵循了這一通用框架。其第一步的具體內容源自於之前關於 DeepSeek-V3 模型的研究論文[1]。R1 使用的是該論文中的基礎模型（並非最終的 DeepSeek-V3 模型），並且同樣經歷了 SFT（監督式微調）和偏好調優階段，但它的獨特之處在於這些階段的具體操作方法。&lt;/p&gt; 
&lt;p&gt;在 R1 的構建過程中，有三個關鍵點值得特別關注。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;2.1 長推理鏈的 SFT 數據&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-71048a70e57a4dd98c33f2c0fb43d5a0d16.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;這些長思維鏈推理的實例數量龐大（總共達到 60 萬個）。如此大規模的實例獲取難度極高，且若要依靠人工標註，成本也將極為昂貴。&lt;/strong&gt; 因此，這些實例的創建過程是我們需要強調的第二個獨特之處。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;2.2 一個過渡性的、擅長推理的高質量 LLM（但在非推理任務上表現稍遜）&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;這些數據是由 R1 的前身，一個專注於推理但尚未命名的姊妹模型所生成的。這個姊妹模型受到了另一個模型 R1-Zero 的啓發（我們將在稍後討論）。它之所以意義重大，並不是因為它是一個非常好用的 LLM，而在於在它的創建過程中，幾乎無需依賴標註數據，僅通過大規模的強化學習，就能培育出一個擅長處理推理問題的模型。&lt;/p&gt; 
&lt;p&gt;接着，這個未命名的推理專家模型的輸出結果，可以用來訓練一個更為多能的模型，它不僅能夠處理推理任務，還能應對其他類型的任務，滿足用户對大語言模型（LLM）的普遍期待。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-4851fe74d4b6fa9ff29c1036a1790de83f4.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;2.3 利用大規模強化學習（RL）構建推理模型&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;此處分為兩個步驟：&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;2.3.1- 以推理為導向的大規模強化學習（R1-Zero）&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;在此過程中，我們利用強化學習（RL）來構建一個臨時的推理模型。隨後，這個模型被用於生成用於監督式微調（SFT）的推理示例。然而，能夠創建這個模型的關鍵，在於之前的一項實驗，該實驗成功打造了一個名為 DeepSeek-R1-Zero 的早期模型。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-fbae4d4fb50b77fa5484a9d220719bbe4d6.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;R1-Zero 的獨特之處在於，它能夠在沒有經過標註的 SFT 訓練集的情況下，依然在推理任務上表現卓越。它的訓練過程直接從預訓練的基礎模型出發，通過強化學習訓練（跳過了 SFT 階段）。它的表現非常出色，能夠與 O1 模型相媲美。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-fd68b8120aaa63d61a0ca7bb0b7333d62ab.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;這一成就重要重大，因為數據一直是機器學習模型能力的助推器。那麼，這個模型是如何打破這一傳統的呢？這主要歸功於以下兩點：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;1- 現代基礎模型在質量和能力上已經達到了一個臨界點（這個基礎模型是在高達 14.8 萬億的高質量 tokens 上訓練而成的）。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2- 與通用聊天或寫作請求不同，推理問題可以實現自動驗證或標註。&lt;/strong&gt; 可以通過以下這個示例來説明這一點。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;示例：推理問題的自動驗證&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;以下是一個可能出現在 RL 訓練步驟中的提示詞/問題：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;編寫一段 Python 代碼，獲取一個數字列表，返回排序後的列表，並在列表開頭添加數字 42。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;這樣的問題非常適合自動驗證。假設我們將這個問題拋給正在訓練的模型，它會生成：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;使用軟件語法檢查器可以驗證生成的代碼是否為有效的 Python 代碼。&lt;/li&gt; 
 &lt;li&gt;我們可以運行這段 Python 代碼，以檢查其是否能夠成功執行。&lt;/li&gt; 
 &lt;li&gt;其他現代代碼生成 LLM 可以創建單元測試來驗證代碼的行為是否符合預期（它們自身無需具備推理能力）。&lt;/li&gt; 
 &lt;li&gt;我們甚至可以進一步，通過測量代碼的執行時間，讓訓練過程偏好那些性能更優的解決方案，即使其他解決方案也是正確的 Python 程序。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;在訓練步驟中，我們可以向模型提出這樣的問題，並生成多種可能的解決方案。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-f83bf0627d5f3ff8f1fda69f2a0769899e6.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;我們可以不依賴人工幹預，自動進行檢查，發現第一個輸出根本不是代碼。第二個輸出是代碼，但並非 Python 代碼。第三個輸出看似是一個解決方案，卻未能通過單元測試，而第四個輸出則是正確的解決方案。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-a1c61910f7a45c46610b945fcd73cf50a89.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;這些反饋都是可以直接用來優化模型的信號。這一過程當然是在大量示例（以小批量形式）和連續的訓練步驟中完成的。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-c2e60359299a142483ec274c460a0c90dc6.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;這些獎勵信號和模型更新是模型在強化學習訓練過程中不斷進步的關鍵，如下圖所示。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-ce02ceb2d7a3049768b4b796755b83f0f9f.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;與此能力提升相伴的是，模型生成了更長的響應，即使用了更多的思考 tokens 來處理問題。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-4c90ef53271c751b695ad334dddfbb87f40.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;儘管這個過程很有價值，但 R1-Zero 模型在推理問題上的高分表現背後，仍存在一些問題，使其實際可用性未達理想狀態。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;雖然 DeepSeek-R1-Zero 展現出了卓越的推理能力，並自主發展出了出人意料的強大推理行為，但它也遭遇了一些挑戰，比如文本可讀性不佳和語言混雜等問題。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;R1 模型的設計目標是提高可用性。因此，它（DeepSeek-R1-Zero）不僅僅完全依賴於強化學習過程，而是如前文所述，在以下兩個方面發揮作用：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;1- 創建一個過渡性的推理模型，用以生成監督式微調（SFT）的數據點。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2- 訓練 R1 模型，以在推理和非推理問題上取得進步（利用其他類型的驗證器）。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-694a728dfc22ac72182045659f53b114a1b.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;2.3.2- 利用過渡性推理模型生成 SFT 推理數據&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;為了提升過渡性推理模型的實際效用，我們對其進行了監督式微調（SFT）訓練，這一步驟在數千個推理問題示例上進行（部分示例由 R1-Zero 生成並篩選）。在論文中，這些示例被稱為&quot;冷啓動數據&quot;。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;2.3.1. 冷啓動階段&lt;/p&gt; 
 &lt;p&gt;與 DeepSeek-R1-Zero 不同，為了防止基礎模型在強化學習訓練初期出現不穩定的冷啓動問題，對於 DeepSeek-R1，我們構建並收集了少量長思維鏈（CoT）數據對模型進行微調，將其作為初始的強化學習策略模型。為收集這類數據，我們探索了多種方法：使用帶有長 CoT 示例的小樣本提示技術、直接提示模型生成帶有反思和驗證的詳細答案、收集 DeepSeek-R1-Zero 生成的易讀格式輸出，並通過人工標註員對結果進行後處理細化。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;但或許你會問，既然我們已經有了這些數據，為什麼還需要依賴強化學習過程呢？答案在於數據的規模。我們可以獲取的可能只有 5,000 個示例的數據集，而訓練 R1 則需要 600,000 個示例。&lt;/strong&gt; 這個過渡性模型幫助我們縮小了這一差距，並使我們能夠合成生成那些極為重要的數據。&lt;/p&gt; 
&lt;p&gt;對於監督式微調（SFT）這一概念，可能你還不太熟悉，它是一種訓練過程，通過向模型展示形式為提示詞和正確補全的訓練示例來進行。下面這個圖展示了書籍《Hands-On Large Language Models》第 12 章中的一些 SFT 訓練示例：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-e00bd1bb414511cf4a13d9225c9ed6bb7ba.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;2.3.3- 常規強化學習訓練階段&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;這樣，R1 模型不僅在推理任務上表現卓越，還能有效地應對其他非推理類任務。這一過程與我們之前提到的強化學習過程相似，但因為它涵蓋了非推理領域的應用，所以它還引入了一個實用性獎勵模型和安全性獎勵模型（與 Llama 模型有相似之處），用於處理這些應用領域的提示詞。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-cd6bf829caa63d1804a38dcdf71e88e2293.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;03 模型架構&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;與 GPT2[2] 和 GPT3[3] 等同源的早期模型一樣，DeepSeek-R1 也是由 Transformer[4] 解碼器塊堆疊而成，總共包含了 61 個這樣的塊。其中，前三個塊是密集層，而後續的則是採用了混合專家層（MoE）。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-817d8e03a6a9f616d918a3f53eb7e8bdede.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;關於模型的維度大小和其他超參數配置，具體信息如下：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-0acee698853f2545eaf2350f4bae0ca92ea.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;有關模型架構的更多詳細信息，可以在他們之前發表的兩篇論文中找到：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;DeepSeek-V3 Technical Report[1]&lt;/li&gt; 
 &lt;li&gt;DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models[5]&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;&lt;strong&gt;04 Conclusion&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;通過上述內容，相信你現在應該對 DeepSeek-R1 模型有了基本的理解。&lt;/p&gt; 
&lt;p&gt;如果你覺得需要更多基礎知識來理解這篇文章，我建議你獲取一本《Hands-On Large Language Models》[6]或者在線在 O&#39;Reilly[7] 上閲讀，並在 Github[8] 上查看相關內容。&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Thanks for reading!&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Hope you have enjoyed and learned new things from this blog!&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;strong&gt;About the author&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Jay Alammar&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Machine learning R&amp;amp;D. Builder. Writer. Visualizing artificial intelligence &amp;amp; machine learning one concept at a time. @CohereAI.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;END&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;本期互動內容 🍻&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;❓你覺得 AI 模型最難掌握的是哪種推理能力？歡迎在評論區分享你的觀點👇&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;🔗文中鏈接🔗&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;[1]&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2412.19437v1&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/pdf/2412.19437v1&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[2]&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fjalammar.github.io%2Fillustrated-gpt2%2F&quot; target=&quot;_blank&quot;&gt;https://jalammar.github.io/illustrated-gpt2/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[3]&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fjalammar.github.io%2Fhow-gpt3-works-visualizations-animations%2F&quot; target=&quot;_blank&quot;&gt;https://jalammar.github.io/how-gpt3-works-visualizations-animations/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[4]&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fjalammar.github.io%2Fillustrated-transformer%2F&quot; target=&quot;_blank&quot;&gt;https://jalammar.github.io/illustrated-transformer/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[5]&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2401.06066&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/pdf/2401.06066&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[6]&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.llm-book.com%2F&quot; target=&quot;_blank&quot;&gt;https://www.llm-book.com/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[7]&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flearning.oreilly.com%2Flibrary%2Fview%2Fhands-on-large-language%2F9781098150952%2F&quot; target=&quot;_blank&quot;&gt;https://learning.oreilly.com/library/view/hands-on-large-language/9781098150952/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[8]&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FhandsOnLLM%2FHands-On-Large-Language-Models&quot; target=&quot;_blank&quot;&gt;https://github.com/handsOnLLM/Hands-On-Large-Language-Models&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;原文鏈接：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnewsletter.languagemodels.co%2Fp%2Fthe-illustrated-deepseek-r1&quot; target=&quot;_blank&quot;&gt;https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1&lt;/a&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/IDP/blog/17553692</link>
            <guid isPermaLink="false">https://my.oschina.net/IDP/blog/17553692</guid>
            <pubDate>Fri, 07 Feb 2025 10:19:00 GMT</pubDate>
            <author>原創</author>
        </item>
        <item>
            <title>豆包開源視頻生成模型 VideoWorld</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FmXaktIsD3w5BgCJQb6R7xQ&quot; target=&quot;_blank&quot;&gt;據豆包大模型團隊官方公眾號消息&lt;/a&gt;&lt;/u&gt;，在北京交通大學和中國科學技術大學的聯合研究下，由豆包大模型團隊提出的 「VideoWorld」 視頻生成實驗模型近日正式開源。&lt;/p&gt; 
&lt;p&gt;據介紹，不同於 Sora 、DALL-E 、Midjourney 等主流多模態模型，&lt;strong&gt;VideoWorld 在業界首次實現無需依賴語言模型，即可認知世界&lt;/strong&gt;。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;論文鏈接：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2501.09781&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2501.09781&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;代碼鏈接：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fbytedance%2FVideoWorld&quot; target=&quot;_blank&quot;&gt;https://github.com/bytedance/VideoWorld&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;項目主頁：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmaverickren.github.io%2FVideoWorld.github.io&quot; target=&quot;_blank&quot;&gt;https://maverickren.github.io/VideoWorld.github.io&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;「VideoWorld」 通過分析和處理大量視頻數據，實現了複雜的推理、規劃和決策能力。研究團隊的實驗顯示，模型在僅有 300M 參數的情況下，便取得了顯著的效果。與現有依賴語言或標籤數據的模型不同，VideoWorld 能夠獨立進行知識學習，尤其在摺紙、打領結等複雜任務中，能夠提供更加直觀的學習方式。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-fc6aac365dbf1a8e0403b8bb24da8452019.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;為了驗證該模型的有效性，研究團隊搭建了圍棋對戰和機器人模擬操控兩種實驗環境。圍棋作為一項高度策略性遊戲，可以有效評估模型的規則學習和推理能力，而機器人任務則考察模型在控制和規劃方面的表現。在訓練階段，模型通過觀看大量視頻演示數據，逐步建立起對未來畫面的預測能力。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/332996</link>
            <guid isPermaLink="false">https://www.oschina.net/news/332996</guid>
            <pubDate>Fri, 07 Feb 2025 10:00:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>DeepSeek 服務站點大全</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;中國信息通信研究院去年 7 月 11 日發佈國內首個算力互聯公共服務平台，並聯合產業界開展算力互聯網共識共創行動。&lt;/p&gt; 
&lt;p&gt;該算力互聯公共服務平台是推進和管理全國算力互聯互通和算力互聯網體系的綜合服務平台，包括算力標識管理、算力互聯網業務查詢、算力統一大市場、政策和研究、標準體系、開源項目和運行監測等功能。&lt;/p&gt; 
&lt;p&gt;中國信通院今日宣佈，為便利國內 AI 開發者「找調用算力」需求，算力互聯公共服務平台宣佈增設全球雲服務商 DeepSeek 服務能力彙總功能頁面（截至 2 月 5 日已彙集 22 家服務商）。&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;地址：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fstateioc.cn%2Farticle-details%2FVjX&quot; target=&quot;_blank&quot;&gt;https://stateioc.cn/article-details/VjX&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;最後歡迎各位使用 Gitee AI —— Gitee AI 的 Serverless API 為您提供開箱即用的企業級的大模型 API 服務。&lt;/p&gt; 
&lt;p&gt;&lt;u&gt;&lt;em&gt;&lt;a href=&quot;https://ai.gitee.com/serverless-api&quot; target=&quot;_blank&quot;&gt;https://ai.gitee.com/serverless-api&lt;/a&gt;&lt;/em&gt;&lt;/u&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0210/174609_Xr4I_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/332995</link>
            <guid isPermaLink="false">https://www.oschina.net/news/332995</guid>
            <pubDate>Fri, 07 Feb 2025 09:46:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>ai.com 域名現已跳轉至 DeepSeek</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;現在在瀏覽器輸入&amp;nbsp;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fai.com%2F&quot; target=&quot;_blank&quot;&gt;ai.com&lt;/a&gt;，將直接重定向至 DeepSeek 官網 (&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fchat.deepseek.com%2F&quot; target=&quot;_blank&quot;&gt;https://chat.deepseek.com/&lt;/a&gt;)。&lt;/p&gt; 
&lt;p&gt;ai.com 域名的定位被視作前沿 AI 的象徵，此前這一域名曾長期跳轉至 ChatGPT、谷歌 Gemini 以及馬斯克的 xAI 官網。根據 Whois 數據，ai.com 域名註冊於 1993 年，有效期直至 2031 年 5 月，註冊聯繫人來自馬來西亞首都吉隆坡。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;219&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-8ceffc7eb4acfec05d9bcabc263bf478854.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/332978</link>
            <guid isPermaLink="false">https://www.oschina.net/news/332978</guid>
            <pubDate>Fri, 07 Feb 2025 08:30:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>消息稱軟銀投資 400 億美元，取代微軟成為 OpenAI 最大金主</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;消息人士向 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.cnbc.com%2F2025%2F02%2F07%2Fsoftbank-set-to-invest-40-billion-in-openai-at-260-billion-valuation-sources-say.html&quot; target=&quot;_blank&quot;&gt;CNBC &lt;/a&gt;透露，軟銀即將完成對 OpenAI 的 400 億美元初始投資，投資前估值為 2600 億美元。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Faber 報道稱軟銀將在未來 12 到 24 個月內支付這筆資金，這意味着 OpenAI 的投資後估值將達到 3000 億美元，第一筆款項最快將於今年春季到賬。軟銀最多可以籌集其中的 100 億美元。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;295&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-34a9ea12a4cc3504f16154eda1fdbbc331a.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;部分資金預計將用於 OpenAI 對 Stargate 的承諾。Stargate 是軟銀、OpenAI 和甲骨文公司的合資企業，由美國現任總統唐納德-特朗普於今年 1 月宣佈成立。該計劃要求向美國的人工智能基礎設施投資數十億美元。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;這一輪融資也意味着軟銀將超越微軟，成為 OpenAI 公司的最大投資者。去年 10 月，私人投資者對 OpenAI 的估值為 1570 億美元。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/332971/softbank-40-billion-openai</link>
            <guid isPermaLink="false">https://www.oschina.net/news/332971/softbank-40-billion-openai</guid>
            <pubDate>Fri, 07 Feb 2025 08:15:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>Yandex 開發並開源 Perforator，每年可為企業節省數十億美元的服務器基礎設施成本</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;ul&gt; 
 &lt;li&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Yandex &lt;/span&gt;&lt;span&gt;推出&lt;/span&gt; &lt;span&gt;Perforator&lt;/span&gt;&lt;span&gt;，這是一款可以識別和評估公司整個代碼庫中效率低下的代碼的工具。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Perforator &lt;/span&gt;&lt;span&gt;幫助開發人員識別最佔資源的代碼部分，並提供詳細的統計數據，以便後續優化。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;該解決方案可以幫助企業每年減少 &lt;/span&gt;&lt;span&gt;20% &lt;/span&gt;&lt;span&gt;的 &lt;/span&gt;&lt;span&gt;CPU &lt;/span&gt;&lt;span&gt;資源使用量。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;通過利，用&lt;/span&gt;&lt;span&gt;Perforator&lt;/span&gt;&lt;span&gt;，企業可以根據公司規模節省數百萬甚至數十億美元的開支，並將資源用於進一步的創新和增長。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Perforator &lt;/span&gt;&lt;span&gt;可通過&lt;/span&gt; &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fyandex%2Fperforator&quot; target=&quot;_blank&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#1155cc&quot;&gt;GitHub&lt;/span&gt;&lt;/span&gt;&lt;/a&gt; &lt;span&gt;免費訪問。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;（上海，&lt;/span&gt;&lt;span&gt;2025&lt;/span&gt;&lt;span&gt;年&lt;/span&gt;&lt;span&gt;2&lt;/span&gt;&lt;span&gt;月&lt;/span&gt;&lt;span&gt;10&lt;/span&gt;&lt;span&gt;日）全球領先的科技公司 &lt;/span&gt;&lt;span&gt;Yandex &lt;/span&gt;&lt;span&gt;開發並開源了&lt;/span&gt;&lt;span&gt; Perforator&lt;/span&gt;&lt;span&gt;，這是一款用於對服務器和應用程序進行持續實時監控和分析的創新工具。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Perforator &lt;/span&gt;&lt;span&gt;幫助開發人員識別最佔資源的代碼部分，並提供詳細的統計數據，以便進行後續優化。通過識別代碼中的低效部分並支持基於配置文件的優化，&lt;/span&gt;&lt;span&gt;Perforator &lt;/span&gt;&lt;span&gt;提供了準確的數據，使企業能夠手動優化其應用程序，根據公司規模，降低基礎設施成本最多可達 &lt;/span&gt;&lt;span&gt;20%&lt;/span&gt;&lt;span&gt;。這每年可能節省數百萬甚至數十億美元。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;「Perforator &lt;/span&gt;&lt;span&gt;幫助企業在不犧牲性能的情況下最大化服務器的使用效率，&lt;/span&gt;&lt;span&gt;」 Yandex &lt;/span&gt;&lt;span&gt;的高級開發人員、&lt;/span&gt;&lt;span&gt;Perforator &lt;/span&gt;&lt;span&gt;團隊負責人&lt;/span&gt;&lt;span&gt; Sergey Skvortsov &lt;/span&gt;&lt;span&gt;表示。&lt;/span&gt;&lt;span&gt;「&lt;/span&gt;&lt;span&gt;企業使用&lt;/span&gt;&lt;span&gt; Perforator &lt;/span&gt;&lt;span&gt;可以優化代碼，減少服務器負載，最終降低能源和設備成本。&lt;/span&gt;&lt;span&gt;」&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;為什麼使用 &lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;Perforator&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;資源優化對於大型數據中心、大型科技公司以及資源有限的小型企業和初創公司至關重要。公司可以利用 &lt;/span&gt;&lt;span&gt;Perforator &lt;/span&gt;&lt;span&gt;優化現有的基礎設施，而無需投資額外的設備，也不犧牲性能。該工具已經在 &lt;/span&gt;&lt;span&gt;Yandex &lt;/span&gt;&lt;span&gt;的許多服務中使用了超過一年，現在可以供全球的公司、開發人員和研究人員使用。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;公司可以將 &lt;/span&gt;&lt;span&gt;Perforator &lt;/span&gt;&lt;span&gt;部署在自己的服務器上，減少對外部雲服務提供商的依賴，同時保持對數據的完全控制。這使得 &lt;/span&gt;&lt;span&gt;Perforator &lt;/span&gt;&lt;span&gt;非常適合那些對數據安全要求嚴格且在封閉基礎設施中運營的組織。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;「Perforator &lt;/span&gt;&lt;span&gt;可以為各種規模的公司帶來益處，從擁有&lt;/span&gt;&lt;span&gt; 10 &lt;/span&gt;&lt;span&gt;至&lt;/span&gt;&lt;span&gt; 100 &lt;/span&gt;&lt;span&gt;台服務器的小型企業，每年節省數百萬美元，到擁有數千台服務器甚至更多的大型企業，每年節省數億美元甚至數十億美元，&lt;/span&gt;&lt;span&gt;」 Sergey Skvortsov &lt;/span&gt;&lt;span&gt;指出。&lt;/span&gt;&lt;span&gt;「&lt;/span&gt;&lt;span&gt;無論公司規模如何，&lt;/span&gt;&lt;span&gt;Perforator &lt;/span&gt;&lt;span&gt;都能幫助您減少基礎設施成本，為進一步的創新和增長釋放更多資源。&lt;/span&gt;&lt;span&gt;」&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Perforator &lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;如何工作&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Perforator &lt;/span&gt;&lt;span&gt;提供了關於服務器資源使用的詳細洞察，並分析代碼對性能的影響，突出了哪些應用程序消耗了最多的系統資源。&lt;/span&gt;&lt;span&gt;Perforator &lt;/span&gt;&lt;span&gt;使用&lt;/span&gt;&lt;span&gt; eBPF &lt;/span&gt;&lt;span&gt;技術在&lt;/span&gt;&lt;span&gt; Linux &lt;/span&gt;&lt;span&gt;內核中運行小程序，既安全又不會拖慢系統速度。&lt;/span&gt;&lt;span&gt;eBPF &lt;/span&gt;&lt;span&gt;能夠在不更改源代碼的情況下，改善監控、安全性和性能優化。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Perforator &lt;/span&gt;&lt;span&gt;支持&lt;/span&gt;&lt;span&gt; C&lt;/span&gt;&lt;span&gt;、&lt;/span&gt;&lt;span&gt;C++&lt;/span&gt;&lt;span&gt;、&lt;/span&gt;&lt;span&gt;Go&lt;/span&gt;&lt;span&gt;、&lt;/span&gt;&lt;span&gt;Rust&lt;/span&gt;&lt;span&gt;、&lt;/span&gt;&lt;span&gt;Python &lt;/span&gt;&lt;span&gt;和 &lt;/span&gt;&lt;span&gt;Java &lt;/span&gt;&lt;span&gt;等原生編程語言。該解決方案通過火焰圖提供深入的分析和數據可視化，使問題診斷變得易於管理。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;287&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-f866d0af24a5577182b973257c103fef72e.png&quot; width=&quot;602&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Perforator &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/em&gt;&lt;em&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;生成的火焰圖示例&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;「Perforator &lt;/span&gt;&lt;span&gt;在&lt;/span&gt;&lt;span&gt; Yandex &lt;/span&gt;&lt;span&gt;的高需求環境中經過了超過一年的實戰測試，提供了廣泛的功能，使其成為一款可靠且多功能的服務器性能監控和優化解決方案，&lt;/span&gt;&lt;span&gt;」 Sergey Skvortsov&lt;/span&gt;&lt;span&gt;補充道。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Perforator &lt;/span&gt;&lt;span&gt;的一個關鍵優勢是支持基於配置文件的優化（&lt;/span&gt;&lt;span&gt;PGO&lt;/span&gt;&lt;span&gt;），它能夠自動將&lt;/span&gt;&lt;span&gt; C++ &lt;/span&gt;&lt;span&gt;程序的速度提高多達 &lt;/span&gt;&lt;span&gt;10%&lt;/span&gt;&lt;span&gt;。此外，&lt;/span&gt;&lt;span&gt;Perforator &lt;/span&gt;&lt;span&gt;設計可以在個別計算機上無縫運行，使其不僅適合大型企業，還能為初創公司和科技愛好者提供便利。更重要的是，&lt;/span&gt;&lt;span&gt;Perforator &lt;/span&gt;&lt;span&gt;為大企業提供了包括&lt;/span&gt;&lt;span&gt; A/B &lt;/span&gt;&lt;span&gt;測試功能在內的重要特性，幫助做出更明智的決策。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;為開發人員和企業提供的開源解決方案&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;將&lt;/span&gt;&lt;span&gt; Perforator &lt;/span&gt;&lt;span&gt;開源的決定體現了&lt;/span&gt;&lt;span&gt; Yandex &lt;/span&gt;&lt;span&gt;致力於促進社區合作開發系統技術的承諾。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;「我們相信，開源諸如此類基礎系統的技術能夠推動全球技術創新」， &lt;/span&gt;&lt;span&gt;Sergey Skvortsov&lt;/span&gt; &lt;span&gt;補充道。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;「&lt;/span&gt;&lt;span&gt;我們的目標是讓我們的技術造福全球，併為開發人員和企業提供價值。此外，技術的開放性使我們能夠與社區共同做出有關配置文件分析基礎設施開發的決策。&lt;/span&gt;&lt;span&gt;」&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;接下來會發生什麼？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Perforator &lt;/span&gt;&lt;span&gt;將在近期增加更多功能，包括與&lt;/span&gt;&lt;span&gt; Python &lt;/span&gt;&lt;span&gt;和&lt;/span&gt;&lt;span&gt; Java &lt;/span&gt;&lt;span&gt;的更好集成以及對事件的更精確分析。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Perforator &lt;/span&gt;&lt;span&gt;的源代碼現已在&lt;/span&gt; &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fyandex%2Fperforator&quot; target=&quot;_blank&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#1155cc&quot;&gt;GitHub&lt;/span&gt;&lt;/span&gt;&lt;/a&gt; &lt;span&gt;上公開，和其他&lt;/span&gt;&lt;span&gt; Yandex &lt;/span&gt;&lt;span&gt;開源解決方案一起提供，如&lt;/span&gt;&lt;span&gt;YaFSDP&lt;/span&gt;&lt;span&gt;，這是一個旨在加速大語言模型訓練的工具。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Perforator &lt;/span&gt;&lt;span&gt;是&lt;/span&gt;&lt;span&gt; Yandex &lt;/span&gt;&lt;span&gt;開源工具系列中的最新成員。您可以在&lt;/span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopensource.yandex%2Fen%2F&quot; target=&quot;_blank&quot;&gt;&lt;span&gt;此頁面&lt;/span&gt;&lt;/a&gt;&lt;span&gt;查看該公司所有的開源項目，包括&lt;/span&gt;&lt;span&gt; YaFSDP&lt;/span&gt;&lt;span&gt;、&lt;/span&gt;&lt;span&gt;AQLM&lt;/span&gt;&lt;span&gt;、&lt;/span&gt;&lt;span&gt;Ytsaurus &lt;/span&gt;&lt;span&gt;等。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/332970</link>
            <guid isPermaLink="false">https://www.oschina.net/news/332970</guid>
            <pubDate>Fri, 07 Feb 2025 08:14:00 GMT</pubDate>
            <author>來源: 投稿</author>
        </item>
        <item>
            <title>GitHub Copilot：Agent 覺醒</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;blockquote&gt; 
 &lt;p&gt;本文翻譯自：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.blog%2Fnews-insights%2Fproduct-news%2Fgithub-copilot-the-agent-awakens%2F&quot; target=&quot;_blank&quot;&gt;https://github.blog/news-insights/product-news/github-copilot-the-agent-awakens/&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-0ad483c840f09803ba7d525bf909fa01465.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;當我們於 2021 年推出 GitHub Copilot 時，我們有一個明確的目標：通過一個幫助開發者編寫更好代碼的 AI 編程助手，讓開發者的生活變得更輕鬆。這個名字反映了我們的信念，即人工智能（AI）不會取代開發者。相反，它始終站在開發者身邊。而且，就像任何優秀的副駕駛一樣，Copilot 也可以獨立飛行：例如，在提供 PR 回覆、自動修復安全漏洞或頭腦風暴如何實現問題解決方案時。&lt;/p&gt; 
&lt;p&gt;今天，我們用更加強大的&lt;strong&gt;代理式人工智能 (agentic AI)&lt;/strong&gt;升級了&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Ffeatures%2Fcopilot%2Fwhats-new%3Futm_source%3Dagent-awakens-announcement%26utm_medium%3Dblogtop%26utm_campaign%3Dagentic-ai&quot; target=&quot;_blank&quot;&gt;GitHub Copilot&lt;/a&gt;——推出&lt;strong&gt;代理模式 (agent mode)&lt;/strong&gt;，併發布 Copilot Edits 的 GA 版本，兩者均已在 VS Code 中上線。&lt;/p&gt; 
&lt;p&gt;我們在模型選擇器中為所有 Copilot 用户增加了 Gemini 2.0 Flash。我們還首次展示了 Copilot 的新自主代理，代號為 Project Padawan。從代碼補全、聊天、多文件編輯到工作空間和代理，Copilot 將人類置於軟件開發這一創造性工作的中心。AI 幫助處理你不想做的事情，這樣你就有更多時間做自己想做的事情。&lt;/p&gt; 
&lt;h2&gt;代理模式 (Agent mode) 進入預覽階段&lt;/h2&gt; 
&lt;p&gt;GitHub Copilot 的新代理模式能夠迭代自己的代碼，識別錯誤並自動修復。它可以建議終端命令並要求您執行它們。它還可以分析運行時錯誤並具有自我修復功能。&lt;/p&gt; 
&lt;p&gt;在代理模式下，Copilot 不僅會迭代自己的輸出，還會迭代輸出結果。它會一直迭代，直到完成所有必要的子任務以完成您的提示。現在，Copilot 不僅能夠執行您請求的任務，還能夠推斷出一些未指定但也是實現主要請求所必需的額外任務。更好的是，它能夠捕捉到自己的錯誤，讓您無需從終端複製/粘貼回聊天中。&lt;/p&gt; 
&lt;p&gt;以下是一個示例，展示了 GitHub Copilot 如何構建一個用於跟蹤馬拉松訓練的 Web 應用程序：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.youtube.com%2Fembed%2Fof--3Fq1M3w%3Ffeature%3Doembed&quot; target=&quot;_blank&quot;&gt;https://www.youtube.com/embed/of--3Fq1M3w?feature=oembed&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;要開始使用，您需要下載 VS Code Insiders，然後為 GitHub Copilot Chat 啓用代理模式設置：&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-4ba42dfaaf982a3e645b4436884e52e9dbd.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;然後，在 Copilot 編輯面板中，從「編輯」切換到模型選擇器旁邊的「Agent」：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0210/113603_Wgac_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;代理模式將改變開發者在使用編輯器時的工作方式；因此，我們將其帶給 Copilot 支持的所有 IDE。我們也知道今天的 Insiders 版本並不完美，並歡迎您在接下來的幾個月裏提供反饋，以便我們改進 VS Code 和底層代理技術。&lt;/p&gt; 
&lt;h2&gt;Copilot Edits 在 VS Code 中已正式 GA&lt;/h2&gt; 
&lt;p&gt;去年 10 月在 GitHub Universe 上宣佈的 Copilot Edits，結合了 Chat 和 Inline Chat 的優點，具有對話流程和能夠在您管理的文件集中進行行內更改的能力。&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fvscode-copilot-release%2Fissues%2F95&quot; target=&quot;_blank&quot;&gt;您之前提供的反饋&lt;/a&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fvscode-copilot-release%2Fissues%2F1098&quot; target=&quot;_blank&quot;&gt;在 GitHub Universe 上&lt;/a&gt;對於將此功能作為 GA 版本發佈到 VS Code 至關重要。感謝！&lt;/p&gt; 
&lt;p&gt;在 Copilot Edits 中，您指定要編輯的一組文件，然後使用自然語言向 GitHub Copilot 提出您所需的內容。Copilot Edits 通過為快速迭代設計的 UI，在您的代碼空間中對多個文件進行行內更改。在審查建議的更改、接受可行的更改並進行後續詢問時，您始終保持在代碼的流程中。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-c40d04de582b53363ee3ddcdea11b12a89a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;在這背後，Copilot Edits 利用雙模型架構來提升編輯效率和準確性。首先，一個基礎語言模型會考慮 Edits 會話的完整上下文來生成初始編輯建議。您可以在以下基礎語言模型中選擇您偏好的一個：OpenAI 的 GPT-4o、o1、o3-mini、Anthropic 的 Claude 3.5 Sonnet，以及現在新增的 Google 的 Gemini 2.0 Flash。為了獲得最佳體驗，我們開發了一個推測性解碼端點，針對快速應用文件中的更改進行了優化。基礎模型提出的編輯建議會被髮送到推測性解碼端點，該端點隨後將在編輯器中直接提出這些更改。&lt;/p&gt; 
&lt;p&gt;Copilot Edits 之所以有效，是因為它將控制權交給了您，從設置正確上下文到接受更改。整個過程是迭代的：當模型出錯時，您可以審查多個文件中的更改，接受好的更改並迭代，直到與 Copilot 一起找到正確的解決方案。接受更改後，您可以運行代碼以驗證更改，並在需要時在 Copilot Edits 中撤銷更改，以回到先前的有效工作狀態。Copilot Edits 位於次級側邊欄（默認位於右側），這樣您在審查建議的更改時可以與主側邊欄中的視圖（如資源管理器、調試或源代碼控制視圖）進行交互。例如，您可以在左側的&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcode.visualstudio.com%2Fdocs%2Feditor%2Ftesting&quot; target=&quot;_blank&quot;&gt;測試視圖中&lt;/a&gt;運行單元測試，同時使用右側的 Copilot Edits 視圖，這樣在每次迭代中，您都可以驗證 Copilot Edits 提出的更改是否通過了您的單元測試。&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcode.visualstudio.com%2Fdocs%2Feditor%2Fvoice&quot; target=&quot;_blank&quot;&gt;通過語音&lt;/a&gt;在使用 Copilot 修改時是一種自然的體驗。只需與 Copilot 對話，就能使互動變得順暢且具有對話性。這幾乎就像是與一位在該領域具有專業知識的同事互動，使用你在現實生活中結對編程時相同的迭代流程。&lt;/p&gt; 
&lt;p&gt;接下來在我們的路線圖上，我們將改進「應用更改」的投機解碼端點性能，支持從 Copilot Chat 過渡到 Copilot Edits，通過保留上下文來實現，向工作集建議文件，並允許您撤銷建議的塊。如果您想成為第一批體驗這些改進的人，請確保使用&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcode.visualstudio.com%2Finsiders%2F&quot; target=&quot;_blank&quot;&gt;VS Code Insiders&lt;/a&gt;和&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmarketplace.visualstudio.com%2Fitems%3FitemName%3DGitHub.copilot-chat&quot; target=&quot;_blank&quot;&gt;GitHub Copilot Chat&lt;/a&gt;擴展的預發佈版本。為了幫助我們改進這個功能，&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fvscode-copilot-release%3Futm_source%3Dagent-awakens-announcement%26utm_medium%3Dblog%26utm_campaign%3Dagentic-ai&quot; target=&quot;_blank&quot;&gt;請在我們的倉庫中提交問題&lt;/a&gt;。&lt;/p&gt; 
&lt;p&gt;除了 VS Code 的 GA 之外，Copilot Edits 現在也在 Visual Studio 2022 中處於預覽階段。&lt;/p&gt; 
&lt;h2&gt;Padawan 項目：GitHub 上的 SWE 代理&lt;/h2&gt; 
&lt;p&gt;我們激動地與大家分享我們自主開發的 SWE（軟件工程師）智能代理，以及我們設想這類代理將如何融入 GitHub 用户體驗。&lt;/p&gt; 
&lt;p&gt;當我們在代號 Project Padawan 的產品今年晚些時候發佈時，您將可以直接將問題分配給 GitHub Copilot，使用任何 GitHub 客户端，並讓它生成經過全面測試的拉取請求。一旦任務完成，Copilot 將指派人類審閲者對 PR 進行審核，並努力解決他們提出的反饋。從某種意義上説，這就像將 Copilot 作為貢獻者引入 GitHub 上的每一個倉庫。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0210/114025_eVHJ_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.youtube.com%2Fembed%2FVWvV2-XwBMM%3Ffeature%3Doembed&quot; target=&quot;_blank&quot;&gt;https://www.youtube.com/embed/VWvV2-XwBMM?feature=oembed&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;該功能背後，Copilot 會為分配給它的每個任務自動啓動一個安全的雲沙盒。然後它異步克隆倉庫，設置環境，分析代碼庫，編輯必要的文件，並構建、測試和檢查代碼。此外，Copilot 還會考慮問題或 PR 中的任何討論，以及倉庫中的任何自定義指令，以便它理解任務的全貌意圖，以及項目的指南和約定。&lt;/p&gt; 
&lt;p&gt;就像我們之前在 Copilot 擴展和 Copilot 模型選擇器中做的那樣，我們也將提供機會將集成到這個 AI 原生工作流程中，並與合作伙伴和客户緊密合作，形成一個緊密的反饋循環。我們相信 Project Padawan 的最終狀態將改變團隊管理關鍵但日常任務的方式，例如修復錯誤或創建和維護自動化測試。因為最終，一切都是關於通過讓他們專注於重要的事情來賦予開發者力量，並讓協作者做其餘的工作。別擔心，我們會保持耐心，所以代理不會「黑化」。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/332928/github-copilot-the-agent-awakens</link>
            <guid isPermaLink="false">https://www.oschina.net/news/332928/github-copilot-the-agent-awakens</guid>
            <pubDate>Fri, 07 Feb 2025 03:40:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>RWKV 社區動態 2025 年 1 月</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;歡迎大家收看《RWKV 社區最新動態》，本期內容收錄了 RWKV 社區 2025 年 1 月的最新動態。&lt;/p&gt; 
&lt;p&gt;只需 3 分鐘，快速瞭解 RWKV 社區 1 月都有哪些新鮮事！&lt;/p&gt; 
&lt;h2&gt;1 月動態省流版（TL;DR）&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;RWKV 學術研究動態&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;新論文： Rate-Aware Learned Speech Compression（RWKV 語音壓縮）&lt;/li&gt; 
   &lt;li&gt;新論文： RWKV-UNet（RWKV 醫學圖像分割）&lt;/li&gt; 
   &lt;li&gt;新論文： FSSC（RWKV 觸覺傳感跨域適應）&lt;/li&gt; 
   &lt;li&gt;新論文： TRP（RWKV 知識圖譜補全）&lt;/li&gt; 
   &lt;li&gt;新論文： TCVADS（RWKV 視頻異常檢測）&lt;/li&gt; 
   &lt;li&gt;新論文： RWKV Voice Dialog System（RWKV 語音對話系統）&lt;/li&gt; 
   &lt;li&gt;新論文： Visualrwkv-Hm（RWKV 視覺語言模型）&lt;/li&gt; 
   &lt;li&gt;新論文： AutoGMM-RWKV（RWKV 無線傳感器網絡安全）&lt;/li&gt; 
   &lt;li&gt;新論文： Revenge of the Fallen?（RWKV 語言理解對比研究）&lt;/li&gt; 
   &lt;li&gt;新論文： Enhancing Transformer RNNs（RWKV 多時間視角增強）&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;RWKV 模型新聞動態&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;新模型： RKWV-7-1.5B&lt;/li&gt; 
   &lt;li&gt;新模型： RKWV-7-0.4B&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;RWKV 社區活動&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;RWKV 創始人閉門會開啓報名&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;RWKV 社區項目動態&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;RWKV Othello&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;RWKV 學術研究動態&lt;/h2&gt; 
&lt;p&gt;RWKV 學術研究包括&lt;strong&gt;基於 RWKV 架構的新論文&lt;/strong&gt;或 &lt;strong&gt;RWKV 社區參加的學術研究&lt;/strong&gt;。&lt;/p&gt; 
&lt;h3&gt;Rate-Aware Learned Speech Compression&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;論文名稱：Rate-Aware Learned Speech Compression&lt;/li&gt; 
 &lt;li&gt;論文鏈接：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2501.11999&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2501.11999&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;發佈日期：2025-01-21&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;這篇論文提出了一種基於通道感知熵模型的學習語音壓縮方案，該方案通過替換傳統的量化器來增強率失真性能。它利用多尺度卷積和 RWKV 混合塊來提高編碼器和解碼器的表示能力。&lt;/p&gt; 
&lt;p&gt;實驗結果表明，與現有編解碼器相比，提出的方法在比特率節省和聲學質量指標方面取得了顯著改善。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-4f7caaf051f652da78952613f6f383bf0d0.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;RWKV-UNet&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;論文名稱：RWKV-UNet: Improving UNet with Long-Range Cooperation for Effective Medical Image Segmentation&lt;/li&gt; 
 &lt;li&gt;論文鏈接：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2501.08458&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2501.08458&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;發佈日期：2025-01-14&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;論文提出 RWKV-UNet，它將 RWKV 結構融入 U-Net 用於醫學圖像分割。通過 IR-RWKV 模塊增強長距離依賴捕獲能力，結合 CCM 模塊改善跳躍連接。&lt;/p&gt; 
&lt;p&gt;實驗表明，RWKV-UNet 在多個數據集上取得 SOTA 性能，平衡了性能和效率。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-ba61005fb424e4a90b230f40c5da963eedd.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;FSSC&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;論文名稱：Reducing Cross-Sensor Domain Gaps in Tactile Sensing via Few-Sample-Driven Style-to-Content Unsupervised Domain Adaptation&lt;/li&gt; 
 &lt;li&gt;論文鏈接：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.mdpi.com%2F1424-8220%2F25%2F1%2F256&quot; target=&quot;_blank&quot;&gt;https://www.mdpi.com/1424-8220/25/1/256&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;發佈日期：2025-01-05&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;這篇論文介紹了 FSSC，一種全新的 few-sample-driven style-to-content 無監督域適應方法。它採用基於 RWKV 架構的設計來應對跨傳感器域適應中的難題，例如傳感器差異導致的域差距等問題。藉助 GLAB 層、FST 模塊等重要組件，它達成了有效減少觸覺傳感跨傳感器域差距的目標。&lt;/p&gt; 
&lt;p&gt;實驗表明，FSSC 在跨傳感器域適應任務的準確性以及對少量樣本的利用效率上均超越了現有的先進方法。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-c9929c53128b28220a6b2be13e7004e05d5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;RWKV Knowledge Graph Completion&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;論文名稱：Efficient Relational Context Perception for Knowledge Graph Completion&lt;/li&gt; 
 &lt;li&gt;論文鏈接：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2501.00397&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2501.00397&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;發佈日期：2024-12-31&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;文提出一種用於知識圖譜補全的新方法，它採用受 Rwkv 啓發的 Triple Receptance Perception (TRP) 架構來解決先前知識圖譜嵌入模型的缺點，如表達能力有限、計算成本高等問題。通過 TRP 中的時間混合和通道混合模塊等關鍵要素，它實現了高效且高質量的知識圖譜補全。&lt;/p&gt; 
&lt;p&gt;實驗表明，該方法在鏈接預測和三元分類任務方面都優於最先進的方法。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-651119224e074229f55231cb42a9b12bc21.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;TCVADS&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;論文名稱：Injecting Explainability and Lightweight Design into Weakly Supervised Video Anomaly Detection Systems&lt;/li&gt; 
 &lt;li&gt;論文鏈接：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2412.20201&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2412.20201&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;發佈日期：2024-12-28&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;論文介紹了一種名為 TCVADS 的視頻異常檢測系統。該系統採用兩個階段的運行模式。在第一階段，系統使用增強的 RWKV 模塊來進行高效的時間序列分析。通過結合知識蒸餾和跨模態學習技術，TCVADS 在性能上優於現有的方法。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-7948ea9998ad0a372ce2ede11942518b7bf.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;RWKV Voice Dialog System&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;論文名稱：Voice dialog system based on RWKV model&lt;/li&gt; 
 &lt;li&gt;論文鏈接：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fieeexplore.ieee.org%2Fabstract%2Fdocument%2F10762107&quot; target=&quot;_blank&quot;&gt;https://ieeexplore.ieee.org/abstract/document/10762107&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;發佈日期：2024-11-28&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;論文提出開發一個面向老年人的智能語音對話系統，採用經 LoRA 微調的 RWKV 模型。實驗結果表明它提高了答案的流暢性和合理性，在老年護理方面有應用潛力，未來工作會對模型進行優化。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-b9cb2625db793016ac552cb831c4bb0a7a0.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;Visualrwkv-Hm&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;論文名稱：Visualrwkv-Hm: Enhancing Linear Visual-Language Models Via Hybrid Mixing&lt;/li&gt; 
 &lt;li&gt;論文鏈接：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fpapers.ssrn.com%2Fsol3%2Fpapers.cfm%3Fabstract_id%3D5028149&quot; target=&quot;_blank&quot;&gt;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5028149&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;發佈日期：2024-11-21&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;論文提出 VisualRWKV-HM，這是一種線性複雜度的視覺語言模型。它基於 RWKV 整合了時間和跨狀態混合。在多個基準測試上達到了 SOTA，在 24K 上下文時比 LLaVA-1.5 等模型效率更高，還展現出強大的可擴展性。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-d60804f11cb6f97d2d77b7d9b04c8202f47.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;AutoGMM-RWKV&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;論文名稱：AutoGMM-RWKV: A Detecting Scheme Based on Attention Mechanisms Against Selective Forwarding Attacks in Wireless Sensor Networks&lt;/li&gt; 
 &lt;li&gt;論文鏈接：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fieeexplore.ieee.org%2Fabstract%2Fdocument%2F10729884&quot; target=&quot;_blank&quot;&gt;https://ieeexplore.ieee.org/abstract/document/10729884&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;發佈日期：2024-10-23&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;論文提出了 AutoGMM-RWKV 用於檢測無線傳感器網絡中的選擇性轉發攻擊。它聚焦於節點單輪轉發率時間序列，通過將自編碼器、高斯混合模型和 K - 均值與 RWKV 相結合，提高了檢測精度。模擬結果顯示誤檢率和漏檢率較低，提供了一個可靠的解決方案。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-ca49954aa5387686e8d7257714795a83744.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;Revenge of the Fallen?&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;論文名稱：Revenge of the Fallen? Recurrent Models Match Transformers at Predicting Human Language Comprehension Metrics&lt;/li&gt; 
 &lt;li&gt;論文鏈接：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2404.19178&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2404.19178&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;發佈日期：2024-08-26&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;論文提出，在語言任務中，Transformer 一直佔據主導地位，但近期 RWKV 等循環模型出現。本文表明像 RWKV 這樣的當代循環模型在模擬人類語言理解方面能夠與 Transformer 相媲美甚至超越它們，開啓了新的研究方向。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-950085a35252d8db50bb7a763699263aacf.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;Enhancing Transformer RNNs with Multiple Temporal Perspectives&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;論文名稱：Enhancing Transformer RNNs with Multiple Temporal Perspectives&lt;/li&gt; 
 &lt;li&gt;論文鏈接：https://arxiv.org/abs/2402.02625&lt;/li&gt; 
 &lt;li&gt;發佈日期：2024-07-11&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;論文提出多時間視角概念以增強循環神經網絡（RNN）。將其應用於 RWKV 模型時，能以極少的參數增加豐富上下文理解。實證結果驗證了其有效性，在基準測試中表現提升且保持線性推理複雜度。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-9a57d07d01cf1ad8f5a65263282fa35c72d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;RWKV 模型動態&lt;/h2&gt; 
&lt;h3&gt;新模型： RKWV-7-1.5B&lt;/h3&gt; 
&lt;p&gt;RWKV-7-World-1.5B-v3 模型於 2025 年 1 月 28 日正式發佈！&lt;/p&gt; 
&lt;p&gt;RWKV-7-1.5B 模型基於 RWKV World v3 數據集（共 3.1T 數據）訓練而來。在英文和多語言評測中，RWKV-7-1.5B 模型的評分對比其他同參數模型處於&lt;strong&gt;絕對領先&lt;/strong&gt;地位。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-56b7f8f2ab08a6d01054853563dd460ec50.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;新模型： RKWV-7-0.4B&lt;/h3&gt; 
&lt;p&gt;RWKV-7-World-0.4B-v2.9 模型於 2025 年 1 月 8 日正式發佈！&lt;/p&gt; 
&lt;p&gt;RWKV-7-World-0.4B 在 world-2.9（從 world-v3 數據集中採樣 2T tokens）數據集上訓練。其英文和多語言能力&lt;strong&gt;顯著超越其他 0.4B 模型&lt;/strong&gt;，且支持全球 100+ 種語言和代碼。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-88a64b43559f9bbfd783387587d05122cf5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;RWKV 社區活動&lt;/h2&gt; 
&lt;p&gt;此版塊包含 &lt;strong&gt;RWKV 官方動態&lt;/strong&gt;，以及 &lt;strong&gt;RWKV 社區舉辦或參加的各類活動&lt;/strong&gt;。&lt;/p&gt; 
&lt;h3&gt;RWKV 創始人閉門會開啓報名&lt;/h3&gt; 
&lt;p&gt;2 月 21 日晚 7 點，將在上海組織 「RWKV-7 與未來趨勢「 的閉門會。&lt;/p&gt; 
&lt;p&gt;RWKV 創始人彭博會線下參加，歡迎 RWKV 開發者、感興趣的業內人士掃碼報名🤝&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-c23a88285bae005e1f5e9a0b05360a9ec7b.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;RWKV 社區項目動態&lt;/h2&gt; 
&lt;h3&gt;RWKV Othello&lt;/h3&gt; 
&lt;p&gt;RWKV 社區成員 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FJellyfish042&quot; target=&quot;_blank&quot;&gt;@Jellyfish042&lt;/a&gt; 基於 RWKV-7 架構開發了 RWKV Othello 項目。&lt;/p&gt; 
&lt;p&gt;項目 GitHub 倉庫： &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FJellyfish042%2FRWKV_Othello&quot; target=&quot;_blank&quot;&gt;https://github.com/Jellyfish042/RWKV_Othello&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;RWKV Othello 項目利用 Othello（也稱為反轉棋或黑白棋）的 CoT 數據訓練了僅 8.8M 參數的 RWKV-7-Othello 模型。&lt;/p&gt; 
&lt;p&gt;RWKV-7-Othello 模型可以和人類或其他模型自動對戰 Othello 遊戲，且在與人類對戰時實現了非常高的勝率。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-5cdd352820faef98513bceb46e59e9a65b4.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/332925</link>
            <guid isPermaLink="false">https://www.oschina.net/news/332925</guid>
            <pubDate>Fri, 07 Feb 2025 03:36:00 GMT</pubDate>
            <author>來源: 投稿</author>
        </item>
    </channel>
</rss>