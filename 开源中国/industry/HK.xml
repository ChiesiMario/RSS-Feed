<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - industry - 繁體中文（香港）</title>
    <link>https://www.oschina.net/news/industry</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news/industry" rel="self" type="application/rss+xml"/>
    <description>已對該 RSS 進行格式化操作：中英字符之間插入空格、使用直角引號、標點符號修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-hk</language>
    <lastBuildDate>Tue, 29 Jul 2025 07:45:01 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>螞蟻 inclusionAI 團隊發佈 Ming-lite-omni v1.5</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;螞蟻集團 inclusionAI 團隊發佈了全面升級版的全模態模型 &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Finclusionai.github.io%2Fzh%2Fblog%2Fming-lite-omni-1_5%2F" target="_blank"&gt;&lt;strong&gt;Ming-Lite-Omni v1.5&lt;/strong&gt;&lt;/a&gt;，基於 &lt;strong&gt;Ling-lite-1.5&lt;/strong&gt; 構建，總參數量為 &lt;strong&gt;203 億&lt;/strong&gt;（其中 MoE 部分活躍參數為 &lt;strong&gt;30 億&lt;/strong&gt;），在圖像-文本理解、文檔理解、視頻理解、語音理解與合成、圖像生成與編輯等全模態能力上顯著提升。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-95012b461af8180f5480bac2f4c85b95949.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Ming-lite-omni v1.5 模型架構如下，主題參考了 Ming-lite-omni v1 版本的結構，區別在於為了增強圖像編輯人物和場景一致性，升級 Vision head 支持參考圖特徵輸入。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-6c48bdf68ea2bcdfc0e8deb440f605297fb.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;關鍵優化&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;增強視頻理解&lt;/strong&gt;：通過 &lt;strong&gt;MRoPE 3D 時空編碼&lt;/strong&gt; 和針對長視頻的 &lt;strong&gt;課程學習策略&lt;/strong&gt;，顯著提升對複雜視覺序列的理解能力 。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;優化多模態生成&lt;/strong&gt;：採用雙分支圖像生成（ID 與場景一致性損失）和新的音頻解碼器及 BPE 編碼，提升生成一致性與感知控制，實現高質量實時語音合成。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;數據全面升級&lt;/strong&gt;：新增結構化文本數據、高質量產品信息及包括方言（如普通話、粵語、四川話等）在內的精細化視覺與語音感知數據。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;性能表現&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;在 &lt;strong&gt;MMVet&lt;/strong&gt;、&lt;strong&gt;MathVista&lt;/strong&gt;、&lt;strong&gt;OCRBench&lt;/strong&gt; 等數據集上表現突出，文檔理解任務（如 &lt;strong&gt;ChartQA&lt;/strong&gt;、&lt;strong&gt;OCRBench&lt;/strong&gt;）取得 10B 以下參數模型中的 &lt;strong&gt;SOTA&lt;/strong&gt; 成績。&lt;/li&gt; 
 &lt;li&gt;視頻理解、語音理解與生成（支持多種方言）及圖像生成（保持人物 ID 一致性編輯）均處於行業領先地位。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;該模型已在 &lt;strong&gt;Hugging Face&lt;/strong&gt; 和 &lt;strong&gt;ModelScope&lt;/strong&gt; 上開放下載，並提供詳細安裝指南、代碼示例和 &lt;strong&gt;Gradio&lt;/strong&gt; 演示。&lt;/p&gt; 
&lt;p&gt;Hugging Face: https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5&lt;br&gt; ModelScope: https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362971</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362971</guid>
      <pubDate>Tue, 29 Jul 2025 07:39:59 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>eBPF 助力 NAS 分鐘級別 Pod 實例溯源</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;span id="OSC_h1_1"&gt;&lt;/span&gt; 
&lt;h1&gt;一、背景&lt;/h1&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;雲存儲 NAS 產品是一個可共享訪問、彈性擴展、高可靠、高性能的分佈式文件系統。 NAS 兼容了 POSIX 文件接口，可支持數千台計算節點共享訪問，可掛載到彈性計算 ECS、容器實例等計算業務上，提供高性能的共享存儲服務。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;鑑於多主機間共享的便利性和高性能， NAS 在得物的算法訓練、應用構建等場景中均成為了基礎支撐。&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img src="https://static001.geekbang.org/infoq/36/36274710c18533ce5fc246ee82e640c2.jpeg" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;在多業務共享的場景中，單個業務流量異常容易引發全局故障。目前，異常發生後需依賴&lt;strong&gt;雲服務廠商 NAS &lt;/strong&gt;的溯源能力，&lt;strong&gt;但只能定位到主機級別，無法識別具體異常服務&lt;/strong&gt;。要定位到服務級別，仍需依賴所有使用方協同排查，並由 SRE 多輪統計分析，&lt;strong&gt;效率低下&lt;/strong&gt;&lt;/span&gt;&lt;span style="color:#6a6a6a"&gt;（若服務實例發生遷移或重建，排查難度進一步增加）&lt;/span&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;&lt;strong&gt;為避免因 NAS 異常或帶寬佔滿導致模型訓練任務受阻&lt;/strong&gt;，因此需構建支持服務級流量監控、快速溯源及 NAS 異常實時感知的能力，以提升問題定位效率並減少業務中斷。&lt;/span&gt;&lt;/p&gt; 
&lt;span id="OSC_h1_2"&gt;&lt;/span&gt; 
&lt;h1&gt;二、流量溯源方案調研和驗證&lt;/h1&gt; 
&lt;span id="OSC_h2_3"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span style="color:#000000"&gt;NAS 工作原理&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;NAS 本地掛載原理&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;在 Linux 平台上，NAS 的產品底層是基於標準網絡文件系統 NFS（Network File System），通過將遠端文件系統掛載到本地，實現用户對遠端文件的透明訪問。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;NFS 協議（主要支持 NFS v3 和 v4，通常以 v3 為主）允許將遠端服務掛載到本地，使用户能夠像訪問本地文件目錄一樣操作遠端文件。文件訪問請求通過 RPC 協議發送到遠端進行處理，其整體流程如下：&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img height="465" src="https://oscimg.oschina.net/oscnet/up-3f3abf4fce2a08639688cf370284cf62cdd.png" width="620" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#6a6a6a"&gt;文件系統訪問時的數據流向示意&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img height="552" src="https://oscimg.oschina.net/oscnet/up-a5b7a533fbca51c726053b4598e79c9786f.jpg" width="507" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#6a6a6a"&gt;Linux 內核中 NFS 文件系統&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;NFS 文件系統讀/寫流程&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;在 Linux NFS 文件系統的實現中，文件操作接口由 nfs_file_operations 結構體定義，其讀取操作對應的函數為:&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;//NFS 文件系統的 VFS 層實現的函數如下所示：
const&amp;nbsp;struct&amp;nbsp;file_operations nfs_file_operations = {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; .llseek &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; = nfs_file_llseek,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; .read_iter &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;= nfs_file_read,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; .write_iter &amp;nbsp; &amp;nbsp; &amp;nbsp; = nfs_file_write,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;// ...
};&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;針對 NFS 文件系統的讀操作涉及到 2 個階段（寫流程類似，只是函數名字有所差異，本文僅以讀取為例介紹）。由於文件讀取涉及到網絡操作因此這兩個階段涉及為異步操作：&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;&lt;u&gt;※ 兩個階段&lt;/u&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;讀取請求階段：&lt;/strong&gt;當應用程序針對 NFS 文件系統發起 read() 讀操作時，內核會在 VFS 層調用 nfs_file_read 函數，然後調用 NFS 層的 nfs_initiate_read 函數，通過 RPC 的 rpc_task_begin 函數將讀請求發送到 NFS Server，至此向 NFS Server 發起的請求工作完成。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;讀響應階段：&lt;/strong&gt;在 NFS Server 返回消息後，會調用 rpc_task_end 和 nfs_page_read_done 等函數，將數據返回到用户空間的應用程序。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img height="415" src="https://oscimg.oschina.net/oscnet/up-fd2c800299c65096f8d6eba7de108c0581f.jpg" width="1280" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;在瞭解 NFS 文件系統的讀流程後，我們回顧一下 NFS Server 為什麼無法區分單機訪問的容器實例或進程實例。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;這是因為 NFS 文件系統的讀寫操作是在內核空間實現的。當容器 A/B 和主機上的進程 C 發起讀請求時，這些請求在進入內核空間後，統一使用主機 IP（如 192.168.1.2）作為客户端 IP 地址。因此，NFS Server 端的統計信息只能定位到主機維度，無法進一步區分主機內具體的容器或進程。&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img height="516" src="https://oscimg.oschina.net/oscnet/up-91366119d285327518f226735b34227dddd.jpg" width="1080" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#6a6a6a"&gt;內核空間實現示意&lt;/span&gt;&lt;/p&gt; 
&lt;span id="OSC_h2_4"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span style="color:#000000"&gt;方案調研和驗證&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;進程對應容器上下文信息關聯&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;內核中進程以 PID 作為唯一編號，與此同時，內核會建立一個 struct task_struct 對象與之關聯，在 struct task_struct 結構會保存進程對應的上下文信息。如實現 PID 信息與用户空間容器上下文的對應（進程 PID 1000 的進程屬於哪個 Pod 哪個 Container 容器實例），我們需基於內核 task_struct 結構獲取到容器相關的信息。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;通過分析內核代碼和資料確認，發現可以通過 task_struct 結構中對應的 cgroup 信息獲取到進程對應的 cgroup_name 的信息，而該信息中包含了容器 ID 信息，例如&lt;strong&gt; docker-2b3b0ba12e92...983.scope &lt;/strong&gt;，完整路徑較長，使用 .... 省略。基於容器 ID 信息，我們可進一步管理到進程所歸屬的 Pod 信息，如 Pod NameSpace 、 Pod Name 、 Container Name 等元信息，最終完成進程 PID 與容器上下文信息元數據關聯。&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;struct&amp;nbsp;task_struct&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;struct&amp;nbsp;css_set&amp;nbsp;__rcu &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;*cgroups;
}


struct&amp;nbsp;css_set&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;struct&amp;nbsp;cgroup_subsys_state&amp;nbsp;*subsys[CGROUP_SUBSYS_COUNT];
}


struct&amp;nbsp;cgroup_subsys_state&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;struct&amp;nbsp;cgroup&amp;nbsp;*cgroup;
}


struct&amp;nbsp;cgroup&amp;nbsp;{
&amp;nbsp;&amp;nbsp;struct&amp;nbsp;kernfs_node&amp;nbsp;*kn; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;/* cgroup kernfs entry */
}


struct&amp;nbsp;kernfs_node&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;const&amp;nbsp;char&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; *name; &amp;nbsp;// docker-2b3b0ba12e92...983.scope
}&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;以某容器進程為例，該進程在 Docker 容器環境中的 cgroup 路徑完整為 /sys/fs/cgroup/cpu/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podefeb3229_4ecb_413a_8715_5300a427db26.slice/docker-2b3b0ba12e925820ac8545f67c8cadee864e5b4033b3d5004d8a3aa742cde2ca.scope 。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;經驗證，我們在內核中讀取 task-&amp;gt;cgroups-&amp;gt;subsys[0]-&amp;gt;kn-&amp;gt;name 的值為 docker-2b3b0ba12e925820ac8545f67c8cadee864e5b4033b3d5004d8a3aa742cde2ca.scope 。&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img src="https://static001.geekbang.org/infoq/92/92735ea140e6e0021584e2c7cc21b0b4.png" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;其中容器 ID 字段為 docker- 與 .scope 間的字段信息，在 Docker 環境中一般取前 12 個字符作為短 ID，如 2b3b0ba12e92 ，可通過 docker 命令進行驗證，結果如下：&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;docker&amp;nbsp;ps -a|grep&amp;nbsp;2b3b0ba
2b3b0ba12e92&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; registry-cn-hangzhou-vpc.ack.aliyuncs.com/acs/pause:3.5&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;NAS 上下文信息關聯&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;NAS 產品的訪問通過掛載命令完成本地文件路徑的掛載。我們可以通過 mount 命令將 NAS 手工掛載到本地文件系統中。&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;mount&amp;nbsp;-t nfs -o vers=3,nolock,proto=tcp,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport \
&amp;nbsp;&amp;nbsp;3f0f3489aa-xxxx.cn-shanghai.nas.aliyuncs.com:/test /mnt/nas&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;執行上述掛載命令成功後，通過 mount 命令則可查詢到類似的掛載記錄：&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;5368 47 0:660 / /mnt/nas rw,relatime shared:1175 \
&amp;nbsp; &amp;nbsp; &amp;nbsp;- nfs 3f0f3489aa-xxxx.cn-shanghai.nas.aliyuncs.com:/test \ &amp;nbsp; &amp;nbsp;
&amp;nbsp; &amp;nbsp; &amp;nbsp;rw,vers=3,rsize=1048576,wsize=1048576,namlen=255,hard,nolock,\
&amp;nbsp; &amp;nbsp; &amp;nbsp;noresvport,proto=tcp,timeo=600,retrans=2,sec=sys, \
&amp;nbsp; &amp;nbsp; &amp;nbsp;mountaddr=192.168.0.91,mountvers=3,mountport=2049,mountproto=tcp,\
&amp;nbsp; &amp;nbsp; &amp;nbsp;local_lock=all,addr=192.168.0.92&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;核心信息分析如下：&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;# 掛載點，父掛載點，掛載設備號 &amp;nbsp; 目錄 &amp;nbsp; &amp;nbsp; 掛載到本機目錄 &amp;nbsp;協議 &amp;nbsp; NAS 地址
5368&amp;nbsp; &amp;nbsp; &amp;nbsp;47&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;0:660&amp;nbsp; &amp;nbsp; &amp;nbsp;/ &amp;nbsp; &amp;nbsp; &amp;nbsp; /mnt/nas &amp;nbsp; &amp;nbsp; nfs &amp;nbsp; &amp;nbsp;3f0f3489aa-xxxx.cn-shanghai.nas.aliyuncs.com:/test
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;maror:minor&amp;nbsp;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;掛載記錄中的&lt;/span&gt;&lt;span style="color:#d92142"&gt;&lt;strong&gt; 0:660 &lt;/strong&gt;&lt;/span&gt;為本地設備編號，格式為 major:minor ， 0 為 major 編號， 660 為 minor 編號，系統主要以 minor 為主。在系統的 NFS 跟蹤點 nfs_initiate_read 的信息中的 dev 字段則為在掛載記錄中的 minor 編號。&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;cat /sys/kernel/debug/tracing/events/nfs/nfs_initiate_read/format
format:
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; field:dev_t dev; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;offset:8; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; size:4; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;signed:0;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;...
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; field:u32 count; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;offset:32; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;size:4; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;signed:0;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;通過用户空間 mount 信息和跟蹤點中 dev_id 信息，則可實現內核空間設備編號與 NAS 詳情的關聯。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;內核空間信息獲取&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;如容器中進程針對掛載到本地的目錄 /mnt/nas 下的文件讀取時，會調用到 nfs_file_read() 和 nfs_initiate_read 函數。通過 nfs_initiate_read 跟蹤點我們可以實現進程容器信息和訪問 NFS 服務器的信息關聯。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;通過編寫 eBPF 程序針對跟蹤點 tracepoint/nfs/nfs_initiate_read 觸發事件進行數據獲取，我們可獲取到訪問進程所對應的 cgroup_name 信息和訪問 NFS Server 在本機的設備 dev_id 編號。&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img height="673" src="https://oscimg.oschina.net/oscnet/up-b7e2eac3eeba85b51ed94f7a84d28a096ea.jpg" width="1280" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#6a6a6a"&gt;獲取 cgroup_name 信息&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;進程容器上下文獲取：&lt;/strong&gt; 通過 cgroup_name 信息，如樣例中的 docker-2b3b0ba12e92...983.scope ，後續可以基於 container_id 查詢到容器對應的 Pod NameSpace 、 Pod Name 和 Container Name 等信息，從而定位到訪問進程關聯的 Pod 信息。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;NAS 上下文信息獲取：&lt;/strong&gt; 通過 dev 信息，樣例中的 660 ，通過掛載到本地的記錄，可以通過 660 查詢到對應的 NAS 產品的地址，比如 3f0f3489aa-xxxx.cn-shanghai.nas.aliyuncs.com 。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;用户空間元信息緩存&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img src="https://static001.geekbang.org/infoq/aa/aa252b079e6ce3cb1e52e02ab5b4052a.png" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;在用户空間中，可以通過解析掛載記錄來獲取 DEV 信息，並將其與 NAS 信息關聯，從而建立以 DevID 為索引的查詢緩存。如此，後續便可以基於內核獲取到 dev_id 進行關聯，進一步補全 NAS 地址及相關詳細信息。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;對於本地容器上下文的信息獲取，最直接的方式是通過 K8s kube-apiserver 通過 list-watch 方法進行訪問。然而，這種方式會在每個節點上啓動一個客户端與 kube-apiserver 通信，顯著增加 K8s 管控面的負擔。因此，我們選擇通過本地容器引擎進行訪問，直接在本地獲取主機的容器詳情。通過解析容器註解中的 Pod 信息，可以建立容器實例緩存。後續在處理指標數據時，則可以通過 container-id 實現信息的關聯與補全。&lt;/span&gt;&lt;/p&gt; 
&lt;span id="OSC_h1_5"&gt;&lt;/span&gt; 
&lt;h1&gt;三、架構設計和實現&lt;/h1&gt; 
&lt;span id="OSC_h2_6"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span style="color:#000000"&gt;整體架構設計&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;內核空間的信息採集採用 Linux eBPF 技術實現，這是一種安全且高效的內核數據採集方式。簡單來説，eBPF 的原理是在內核中基於事件運行用户自定義程序，並通過內置的 map 和 perf 等機制實現用户空間與內核空間之間的雙向數據交換。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;在 NFS 和 RPC 調用事件觸發的基礎上，可以通過編寫內核空間的 eBPF 程序來獲取必要的原始信息。當用户空間程序蒐集到內核指標數據後，會對這些原始信息進行二次處理，並在用户空間的採集程序中補充容器進程信息（如 NameSpace、Pod 和 Container 名稱）以及 NFS 地址信息（包括 NFS 遠端地址）。&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img src="https://static001.geekbang.org/infoq/c6/c687b3f4df8a2ce5d0ab5cd9f287dfd4.png" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;span id="OSC_h2_7"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span style="color:#000000"&gt;內核 eBPF 程序流程&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;以 NFS 文件讀為例，通過編寫 eBPF 程序跟蹤 nfs_initiate_read / rpc_task_begin / rpc_task_end / nfs_page_read_done 等關鍵鏈路上的函數，用於獲取到 NFS 讀取的數據量和延時數據，並將訪問鏈路中的進程上下文等信息保存到內核中的指標緩存中。&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img height="471" src="https://oscimg.oschina.net/oscnet/up-f854a1a8cdf429eff044e715772dc96dfb6.jpg" width="1280" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;如上圖所示， nfs_initate_read 和 rpc_task_begin 發生在同一進程上下文中，而 rpc_task_begin 與 rpc_task_end 是異步操作，儘管兩者不處於同一進程上下文，但可以通過 task_id 進行關聯。同時， page_read_done 和 rpc_task_end 則發生在同一進程上下文中。&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img height="627" src="https://oscimg.oschina.net/oscnet/up-70f989400d6710f021cfa7577375a709030.jpg" width="1280" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;nfs_initiate_read 函數調用觸發的 eBPF 代碼示例如下所示：&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;

SEC("tracepoint/nfs/nfs_initiate_read")
int&amp;nbsp;tp_nfs_init_read(struct&amp;nbsp;trace_event_raw_nfs_initiate_read *ctx)
&amp;nbsp; &amp;nbsp;&amp;nbsp;// 步驟 1 獲取到 nfs 訪問的設備號信息，比如 3f0f3489aa-xxxx.cn-shanghai.nas.aliyuncs.com
&amp;nbsp; &amp;nbsp;&amp;nbsp;// dev_id 則為： 660&amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;dev_t&amp;nbsp;dev_id =&amp;nbsp;BPF_CORE_READ(ctx, dev);
&amp;nbsp; &amp;nbsp; u64 file_id =&amp;nbsp;BPF_CORE_READ(ctx, fileid);
&amp;nbsp; &amp;nbsp; u32 count =&amp;nbsp;BPF_CORE_READ(ctx, count);
&amp;nbsp; &amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;struct&amp;nbsp;task_struct&amp;nbsp;*task = (struct&amp;nbsp;task_struct *)bpf_get_current_task();


&amp;nbsp; &amp;nbsp;&amp;nbsp;// 步驟 2 獲取進程上下文所在的容器 cgroup_name 信息
&amp;nbsp; &amp;nbsp;&amp;nbsp;// docker-2b3b0ba12e925820ac8545f67c8cadee864e5b4033b3d5004d8a3aa742cde2ca.scope
&amp;nbsp; &amp;nbsp;&amp;nbsp;const&amp;nbsp;char&amp;nbsp;*cname =&amp;nbsp;BPF_CORE_READ(task, cgroups, subsys[0], cgroup, kn, name);
&amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(cname)
&amp;nbsp; &amp;nbsp; {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;bpf_core_read_str(&amp;amp;info.container, MAX_PATH_LEN, cname);
&amp;nbsp; &amp;nbsp; }


&amp;nbsp; &amp;nbsp;&amp;nbsp;bpf_map_update_elem(&amp;amp;link_begin, &amp;amp;tid, &amp;amp;info, BPF_ANY);
}


SEC("tracepoint/nfs/nfs_readpage_done")
int&amp;nbsp;tp_nfs_read_done(struct&amp;nbsp;trace_event_raw_nfs_readpage_done *ctx)
{
&amp;nbsp; &amp;nbsp;//... 省略
}


SEC("tracepoint/sunrpc/rpc_task_begin")
int&amp;nbsp;tp_rpc_task_begin(struct&amp;nbsp;trace_event_raw_rpc_task_running *ctx)
{
&amp;nbsp; &amp;nbsp;&amp;nbsp;//... 省略
}


SEC("tracepoint/sunrpc/rpc_task_end")
int&amp;nbsp;tp_rpc_task_done(struct&amp;nbsp;trace_event_raw_rpc_task_running *ctx)
{
&amp;nbsp; &amp;nbsp;//... 省略
}&lt;/code&gt;&lt;/pre&gt; 
&lt;span id="OSC_h2_8"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span style="color:#000000"&gt;用户空間程序架構&lt;/span&gt;&lt;/h2&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img height="606" src="https://oscimg.oschina.net/oscnet/up-b9cf6d19b68dcceaa9f6f459645264baaa8.jpg" width="1280" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;元數據緩存&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;&lt;u&gt;※ NAS 掛載信息緩存&lt;/u&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;通過解析掛載記錄，可以獲取 DEV 信息與 NAS 信息的關聯關係。以下是實現該功能的關鍵代碼詳情：&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;scanner := bufio.NewScanner(mountInfoFile)
count :=&amp;nbsp;0
for&amp;nbsp;scanner.Scan() {
&amp;nbsp; &amp;nbsp; line := scanner.Text()
&amp;nbsp; &amp;nbsp; devID,remoteDir, localDir, NASAddr = parseMountInfo(line)


&amp;nbsp; &amp;nbsp; mountInfo := MountInfo{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;DevID: &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; devID,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;RemoteDir: &amp;nbsp; &amp;nbsp; remoteDir,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;LocalMountDir: localDir,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;NASAddr： NASAddr,
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; mountInfos =&amp;nbsp;append(mountInfos, mountInfo)

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;&lt;u&gt;※ 容器元信息緩存&lt;/u&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;通過 Docker 或 Containerd 客户端，從本地讀取單機的容器實例信息，並將容器的上下文數據保存到本地緩存中，以便後續查詢使用。&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;podInfo := PodInfo{
&amp;nbsp; &amp;nbsp; NameSpace: &amp;nbsp; &amp;nbsp; labels["io.kubernetes.pod.namespace"],
&amp;nbsp; &amp;nbsp; PodName: &amp;nbsp; &amp;nbsp; &amp;nbsp; labels["io.kubernetes.pod.name"],
&amp;nbsp; &amp;nbsp; ContainerName: labels["io.kubernetes.container.name"],
&amp;nbsp; &amp;nbsp; UID: &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; labels["io.kubernetes.pod.uid"],
&amp;nbsp; &amp;nbsp; ContainerID: &amp;nbsp; conShortID,
}&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;數據處置流程&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;用户空間程序的主要任務是持續讀取內核 eBPF 程序生成的指標數據，並對讀取到的原始數據進行處理，提取訪問設備的 dev_id 和 container_id 。隨後，通過查詢已建立的元數據緩存，分別獲取 NAS 信息和容器 Pod 的上下文數據。最終，經過數據合併與處理，生成指標數據緩存供後續使用。&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;func&amp;nbsp;(m *BPFEventMgr)&amp;nbsp;ProcessIOMetric() {
&amp;nbsp; &amp;nbsp;&amp;nbsp;// ...
&amp;nbsp; &amp;nbsp; events := m.ioMetricMap
&amp;nbsp; &amp;nbsp; iter := events.Iterate()


&amp;nbsp; &amp;nbsp;&amp;nbsp;for&amp;nbsp;iter.Next(&amp;amp;nextKey, &amp;amp;event) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;// ① 讀取到的 dev_id 轉化為對應的完整 NAS 信息
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;devId := nextKey.DevId
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;mountInfo, ok := m.mountMgr.Find(int(devId))


&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;// ② 讀取 containerID 格式化並查詢對應的 Pod 上下文信息
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;containerId := getContainerID(nextKey.Container)
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;podInfo, ok = m.criMgr.Find(containerId)
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;// ③ 基於事件信息、NAS 掛載信息和 Pod 上下文信息，生成指標數據緩存&amp;nbsp;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;metricKey, metricValue := formatMetricData(nextKey， mountInfo, podInfo)
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;value, loaded := metricCache.LoadOrStore(metricKey, metricValue)
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp;&amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;// ④ 指標數據緩存，生成最終的 Metrics 指標並更新&amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;var&amp;nbsp;ioMetrics []metric.Counter
&amp;nbsp; &amp;nbsp; metricCache.Range(func(key, value&amp;nbsp;interface{})&amp;nbsp;bool&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;k := key.(metric.IOKey)
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;v := value.(metric.IOValue)


&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;ioMetrics =&amp;nbsp;append(ioMetrics, metric.Counter{"read_count",&amp;nbsp;float64(v.ReadCount),
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;[]string{k.NfsServer, v.NameSpace, v.Pod, v.Container})
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;// ...
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;}
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;return&amp;nbsp;true
&amp;nbsp; &amp;nbsp; })
&amp;nbsp; &amp;nbsp;&amp;nbsp;
&amp;nbsp; &amp;nbsp; m.metricMgr.UpdateIOStat(ioMetrics)
}&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;啓動 Goroutine 處理指標數據：通過啓動一個 Goroutine，循環讀取內核存儲的指標數據，並對數據進行處理和信息補齊，最終生成符合導出格式的 Metrics 指標。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;&lt;u&gt;※ 具體步驟&lt;/u&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;獲取 NAS 信息：&lt;/strong&gt;從讀取的原始數據中提取 dev_id ，並通過 dev_id 查詢掛載的 NAS 信息，例如遠端訪問地址等相關數據。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;查詢 Pod 上下文：&lt;/strong&gt;對 containerID 進行格式化處理，並查詢對應的容器 Pod 上下文信息。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;生成指標數據緩存：&lt;/strong&gt;基於事件數據、NAS 掛載信息和 Pod 上下文信息，生成指標數據緩存。此過程主要包括對相同容器上下文的數據進行合併和累加。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;導出 Metrics 指標：&lt;/strong&gt;根據指標數據緩存，生成最終的 Metrics 指標，並更新到指標管理器。隨後，通過自定義的 Collector 接口對外導出數據。當 Prometheus 拉取數據時，指標會被轉換為最終的 Metrics 格式。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;通過上述步驟，用户空間能夠高效地處理內核 eBPF 程序生成的原始數據，並結合 NAS 掛載信息和容器上下文信息，生成符合 Prometheus 標準的 Metrics 指標，為後續的監控和分析提供了可靠的數據基礎。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;自定義指標導出器&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;在導出指標的場景中，我們需要基於保存在 Go 語言中的 map 結構中的動態數據實時生成，因此需要實現自定義的 Collector 接口。自定義 Collector 接口需要實現元數據描述函數 Describe() 和指標蒐集的函數 Collect() ，其中 Collect() 函數可以併發拉取，因此需要通過加鎖實現線程安全。該接口需要實現以下兩個核心函數：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Describe() ：用於定義指標的元數據描述，向 Prometheus 註冊指標的基本信息。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Collect() ：用於蒐集指標數據，該函數支持併發拉取，因此需要通過加鎖機制確保線程安全。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;type&amp;nbsp;Collector&amp;nbsp;interface&amp;nbsp;{
&amp;nbsp; &amp;nbsp;&amp;nbsp;// 指標的定義描述符
&amp;nbsp; &amp;nbsp; Describe(chan&amp;lt;- *Desc)
&amp;nbsp; &amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;// 並將收集的數據傳遞到 Channel 中返回
&amp;nbsp; &amp;nbsp; Collect(chan&amp;lt;- Metric)
}&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;我們在指標管理器中實現 Collector 接口， 部分實現代碼，如下所示：&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;nfsIOMetric := prometheus.NewDesc(
&amp;nbsp; &amp;nbsp; prometheus.BuildFQName(prometheusNamespace,&amp;nbsp;"",&amp;nbsp;"io_metric"),
&amp;nbsp; &amp;nbsp;&amp;nbsp;"nfs io metrics by cgroup",
&amp;nbsp; &amp;nbsp; []string{"nfs_server",&amp;nbsp;"ns",&amp;nbsp;"pod",&amp;nbsp;"container",&amp;nbsp;"op",&amp;nbsp;"type"},
&amp;nbsp; &amp;nbsp;&amp;nbsp;nil,
)


// Describe and Collect implement prometheus collect interface
func&amp;nbsp;(m *MetricMgr)&amp;nbsp;Describe(ch&amp;nbsp;chan&amp;lt;- *prometheus.Desc) {
&amp;nbsp; &amp;nbsp; ch &amp;lt;- m.nfsIOMetric
}


func&amp;nbsp;(m *MetricMgr)&amp;nbsp;Collect(ch&amp;nbsp;chan&amp;lt;- prometheus.Metric) {
&amp;nbsp; &amp;nbsp;&amp;nbsp;// Note：加鎖保障線程併發安全
&amp;nbsp; &amp;nbsp; m.activeMutex.Lock()
&amp;nbsp; &amp;nbsp;&amp;nbsp;defer&amp;nbsp;m.activeMutex.Unlock()
&amp;nbsp; &amp;nbsp;&amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;for&amp;nbsp;_, v :=&amp;nbsp;range&amp;nbsp;m.ioMetricCounters {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;ch &amp;lt;- prometheus.MustNewConstMetric(m.nfsIOMetric, prometheus.GaugeValue, v.Count, v.Labels...)
&amp;nbsp; &amp;nbsp; }

&lt;/code&gt;&lt;/pre&gt; 
&lt;span id="OSC_h1_9"&gt;&lt;/span&gt; 
&lt;h1&gt;四、總結&lt;/h1&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;當前 NAS 溯源能力已正式上線，以下是主要功能和視圖介紹：&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;&lt;u&gt;※ 單 NAS 實例整體趨勢&lt;/u&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;支持基於環境和 NAS 訪問地址過濾，展示 NAS 產品的讀寫 IOPS 和吞吐趨勢圖。同時，基於內核空間統計的延時數據，提供 P95 讀寫延時指標，用於判斷讀寫延時情況，輔助問題分析和定位。&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img src="https://static001.geekbang.org/infoq/14/14577d6fe8b2876ca7f5138680fc3667.png" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img src="https://static001.geekbang.org/infoq/9c/9c091aeaecc284956b851416de5d313a.png" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;在 NAS 流量溯源方面，我們結合業務場景設計了基於任務和 Pod 實例維度的流量分析視圖：&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;&lt;u&gt;※ 任務維度流量溯源&lt;/u&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;通過聚合具有共同屬性的一組 Pod 實例，展示任務級別的整體流量情況。該視圖支持快速定位任務級別的流量分佈，幫助用户進行流量溯源和多任務錯峯使用的依據。&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img src="https://static001.geekbang.org/infoq/0a/0a504606b05345b52061d2f754c15b51.png" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;&lt;u&gt;※ Pod 實例維度流量溯源&lt;/u&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;以 Pod 為單位進行流量分析和彙總，提供 Pod NameSpace 和 Name 信息，支持快速定位和分析實例級別的流量趨勢，幫助細粒度監控和異常流量的精準定位。&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img src="https://static001.geekbang.org/infoq/90/90765c51493906beaf530c64494abc6a.png" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;在整體能力建設完成後，我們成功構建了 NAS 實例級別的 IOPS、吞吐和讀寫延時數據監控大盤。通過該能力，進一步實現了 NAS 實例的 IOPS 和吞吐可以快速溯源到任務級別和 Pod 實例級別，流量溯源時效從小時級別縮短至分鐘級別，有效提升了異常問題定位與解決的效率。同時，基於任務流量視圖，我們為後續帶寬錯峯複用提供了直觀的數據支持。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;往期回顧&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#586c90"&gt;1.&lt;/span&gt;正品庫拍照 PWA 應用的實現與性能優化｜得物技術&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#586c90"&gt;2.&lt;/span&gt;匯金資損防控體系建設及實踐 | 得物技術&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#586c90"&gt;3.&lt;/span&gt;一致性框架：供應鏈分佈式事務問題解決方案｜得物技術&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#586c90"&gt;4.&lt;/span&gt;得物社區活動：組件化的演進與實踐&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#586c90"&gt;5.&lt;/span&gt;從 CPU 冒煙到絲滑體驗：算法 SRE 性能優化實戰全揭秘｜得物技術&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;文 / 泊明&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;關注得物技術，每週更新技術乾貨&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;要是覺得文章對你有幫助的話，歡迎評論轉發點贊～&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;未經得物技術許可嚴禁轉載，否則依法追究法律責任。&lt;/span&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/5783135/blog/18683994</link>
      <guid isPermaLink="false">https://my.oschina.net/u/5783135/blog/18683994</guid>
      <pubDate>Tue, 29 Jul 2025 07:35:59 GMT</pubDate>
      <author>原創</author>
    </item>
    <item>
      <title>英偉達開源 Llama-3.3-Nemotron-Super-49B-v1.5 模型</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;英偉達發佈了 Llama-3.3-Nemotron-Super-49B-v1.5，這是一款專為推理和 Agentic 任務優化的開源模型，在單個 H100 GPU 上實現高吞吐量。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-1eb8efcbf4188aaa81c53fbda0c23259d86.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;模型介紹&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Llama Nemotron Super v1.5 是 Llama-3.3-Nemotron-Super-49B-V1.5 的簡稱。它是 Llama-3.3-Nemotron-Super-49B-V1 的升級版本（該模型是 Meta 的 Llama-3.3-70B-Instruct 的衍生模型），專為複雜推理和智能體任務設計，支持 128K tokens 的上下文長度。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;模型架構&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Llama Nemotron Super v1.5 採用神經架構搜索（Neural Architecture Search，NAS），使該模型在準確率和效率之間實現了良好的平衡，將吞吐量的提升有效轉化為更低的運行成本。&lt;/p&gt; 
&lt;p&gt;（注：NAS 的目標是通過搜索算法從大量的可能架構中找到最優的神經網絡結構，利用自動化方法替代人工設計神經網絡架構，從而提高模型的性能和效率。）&lt;/p&gt; 
&lt;p&gt;模型經過了多階段後訓練，包括針對數學、代碼、科學和工具調用的監督微調 (SFT)，以及用於聊天對齊的獎勵感知偏好優化 (RPO)、用於推理的帶可驗證獎勵的強化學習 (RLVR) 和用於工具調用能力增強的迭代直接偏好優化 (DPO)。&lt;/p&gt; 
&lt;p&gt;在多個基準測試中，該模型表現出色。例如，在 MATH500 上 pass@1 達到 97.4，在 AIME 2024 上達到 87.5，在 GPQA 上達到 71.97。模型支持 Reasoning On/Off 模式，用户可通過在系統提示中設置 /no_think 來關閉推理模式。官方推薦在推理開啓時使用 temperature=0.6 和 Top P=0.95，在關閉時使用貪心解碼。&lt;/p&gt; 
&lt;p&gt;該模型已準備好用於商業用途，遵循 NVIDIA Open Model License 和 Llama 3.3 社區許可協議。開發者可以通過 NVIDIA build.nvidia.com 或 Hugging Face 下載和試用該模型，並可使用 vLLM（推薦 v0.9.2）進行部署，官方倉庫中提供了支持工具調用的解析器插件。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362966</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362966</guid>
      <pubDate>Tue, 29 Jul 2025 07:26:59 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>GitHub 出現大範圍服務中斷：目前已全部恢復，影響超 8 小時</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;代碼託管平台 GitHub 從 2025 年 7 月 28 日 16:50 UTC（北京時間 7 月 29 日 00:50）起突發&lt;strong&gt;大規模服務中斷&lt;/strong&gt;，受影響服務包括 Git 操作、API 請求、Pull 請求和 Issues 跟蹤等核心功能 。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0729/150643_ZtSu_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;儘管 GitHub 工程團隊&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.githubstatus.com%2Fincidents%2Fs6d4x8c6cvv5" target="_blank"&gt;嘗試了多種修復措施&lt;/a&gt;（如增設服務器容量、調整限流措施），初期效果不佳，直到北京時間 &lt;strong&gt;7 月 29 日 9:23 左右&lt;/strong&gt; 才取得實質性進展。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1416" src="https://static.oschina.net/uploads/space/2025/0729/150606_wHAe_2720166.png" width="1890" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;最終，相關問題已逐步解決，截至目前，API 請求、Pull 請求等服務已全面恢復，整體中斷時間超過 &lt;strong&gt;8 小時&lt;/strong&gt;。GitHub 官方表示正在深入調查具體原因，後續將發佈詳細技術分析報告。&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;https://www.githubstatus.com/history&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362956</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362956</guid>
      <pubDate>Tue, 29 Jul 2025 07:07:59 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Julius AI 完成 1000 萬美元種子輪融資</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        ********************************************************************************************************************
    ********************************************************************************************************************
    ********************************************************************************************************************
    ********************************************************************************************************************
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362957</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362957</guid>
      <pubDate>Tue, 29 Jul 2025 07:07:59 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>中國移動「九天」3.0 發佈，多項核心技術同步開源</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;中國移動發佈了其自主研發的 「九天」基礎大模型 3.0。根據介紹，「九天眾擎語言大模型」實現了架構上的突破性創新，採用可擴展至萬億級的&amp;nbsp;&lt;strong&gt;MoE 架構&lt;/strong&gt;。通過 15T token 的多階段配比預訓練數據與全流程治理體系，其推理能力得到顯著強化。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;該模型還創新構建了 113 域 ×53 能力的二維分級後訓練框架，結合動態強化學習策略，使複雜推理能力提升了&amp;nbsp;&lt;strong&gt;35%&lt;/strong&gt;。測評結果顯示，「九天」語言大模型：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;在&amp;nbsp;&lt;strong&gt;GPQA-Diamond&lt;/strong&gt;&amp;nbsp;評測中，以&amp;nbsp;&lt;strong&gt;77.67 分&lt;/strong&gt;斬獲全球第二，超越 DeepSeekR1 和 Qwen3。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;在&amp;nbsp;&lt;strong&gt;ArenaHard V1.0&lt;/strong&gt;&amp;nbsp;中，以&amp;nbsp;&lt;strong&gt;67.2 分&lt;/strong&gt;位居全球第一。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;在&amp;nbsp;&lt;strong&gt;BFCL V3&lt;/strong&gt;&amp;nbsp;評測中，達到&amp;nbsp;&lt;strong&gt;68 分&lt;/strong&gt;。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;在性能大幅躍升的同時，模型進一步強化了可控生成能力，通過精確流程內置等技術細節，實現了專業場景下的&lt;strong&gt;零幻覺&lt;/strong&gt;，破解了沉浸式角色演繹難題。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;基於最新的語言大模型，中國移動還同步推出了多個專項模型:&lt;/span&gt;&lt;/p&gt; 
&lt;ul style="margin-left:0; margin-right:0"&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;九天代碼大模型：&lt;/strong&gt;採用兩階段持續訓練技術，支持代碼生成、註釋生成、單元測試生成、代碼智能問答等任務，覆蓋 Python、Java、JS、TS、Go、C++ 等 10 餘種主流編程語言。在 EvalPlus、MHPP、LivecodeBenchv6 等多個代碼生成榜單上表現領先。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;九天數學大模型：&lt;/strong&gt;在短思考、長思考模式下均達到業界 SOTA 水平，多項指標超越 Qwen2.5Math、Qwen3、DeepSeek Math、DeepSeek R1-Distill 等同參數量級模型。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;「九天善智多模態大模型」引入複雜時空建模、流匹配圖片視頻漸進式聯合訓練、端到端局部可控注意力機制等創新技術。同時，通過融合多模態理解信息和聯合圖文交織數據訓練，顯著提升了模型對文本指令和輸入條件圖像視頻的感知能力。這意味着模型不僅能生成高質量的圖像視頻，還能進行多輪對話式高可控精確編輯操作，大幅提升了視覺生成的靈活便利性。例如，在圖片生成方面可支持多輪精準局部修改，如修改文字、修改背景、增加元素等。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;模型的圖理解和視頻理解性能也得到了全面提升：&lt;/span&gt;&lt;/p&gt; 
&lt;ul style="margin-left:0; margin-right:0"&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;圖理解方面：&lt;/strong&gt;在 MMStar、HallusionBench 和 OCRBench 等圖理解任務中，九天模型分別獲得了&amp;nbsp;&lt;strong&gt;82.2、64.3 和 94.9 的高分&lt;/strong&gt;，處於業界領先水平。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;視頻理解方面：&lt;/strong&gt;在 Videomme 和 MVbench 兩個任務中均表現領先，超越 Qwen2-VL 和 InternVideo2。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;目前，中國移動已將多項模型及核心技術進行開源：&lt;/span&gt;&lt;/p&gt; 
&lt;ol style="margin-left:0; margin-right:0"&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;開源九天數童結構化數據大模型&lt;/strong&gt;：包括 JT-DA-8B 模型及後續演進版本，支持下載模型權重、微調代碼、推理代碼等。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;開源九天數學大模型&lt;/strong&gt;：包括 JT-Math-8B 系列模型，支持下載模型權重、推理代碼、技術報告。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;開源九天代碼大模型&lt;/strong&gt;：包括 JT-Coder-8B 系列模型，支持下載模型權重、推理代碼、技術報告。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;開源業界首創的結構化數據模型評測數據及 TReB 評測體系&lt;/strong&gt;：涵蓋 6 大任務、34 個能力，包括高質量、全面的數據、推理模式及評價指標，支持下載評測數據集、測試代碼。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;開源 CCR-Bench 行業場景複雜指令遵循評測數據集&lt;/strong&gt;：包含 174 條高質量、多樣化、高難度複雜指令數據，高度模擬健康專家、智能客服、醫療助手等典型工業場景，支持下載數據集。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362949</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362949</guid>
      <pubDate>Tue, 29 Jul 2025 06:45:59 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>特斯拉與三星簽訂 165 億美元 AI 芯片製造協議</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;7 月 28 日，三星電子在提交給監管機構的文件中表示，三星電子與一家全球大型公司簽署了價值 22.8 萬億韓元（注：現匯率約合 1181.72 億元人民幣，約合 165 億美元）的芯片製造協議，但未透露具體客户名稱。&lt;/p&gt; 
&lt;p&gt;據消息人士透露，特斯拉正是這家客户，該公司目前與三星的合同芯片製造部門已有業務往來。&lt;/p&gt; 
&lt;p&gt;有外媒表示，三星電子公司將就新達成的 165 億美元協議，為特斯拉公司生產半導體，這將為其表現不佳的晶圓代工部門提供助力。該合作的合同期 2025 年 7 月 24 日-2033 年 12 月 31 日。&lt;/p&gt; 
&lt;p&gt;對此，特斯拉 CEO 埃隆・馬斯克確認了合作爆料，三星在美國得克薩斯州新建的巨型工廠將專門用於生產特斯拉的下一代 AI6 芯片（注：特斯拉汽車智駕芯片），並稱「其戰略重要性毋庸置疑」。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-99e430011e2ff79c31d84adca382c598c73.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;馬斯克還稱，三星目前正在生產 AI4 芯片。台積電將首先在中國台灣地區生產剛剛完成設計的 AI5 芯片，然後在美國亞利桑那州生產。&lt;/p&gt; 
&lt;p&gt;根據外媒今年 6 月報道，台積電在全球第三方晶圓代工市場的市佔比為 67%，而排名第二的三星則僅佔 11%。另外，有消息人士稱，三星電子 2025 上半年晶圓代工部門獲零獎金。此前，外媒援引供應鏈消息稱，三星已啓動「精選和聚焦」戰略，集中資源提升 2nm 工藝良率，希望通過產量和成本優勢來挑戰台積電。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362947</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362947</guid>
      <pubDate>Thu, 17 Jul 2025 06:41:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>端側原生大模型 SmallThinker 正式開源</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;上海交通大學 IPADS 研究所、上海交通大學人工智能學院聯合初創公司本智激活（Zenergize AI），發佈了開源端側原生大模型 SmallThinker。&lt;/p&gt; 
&lt;p&gt;該系列模型採用為端側算力、內存、存儲特性而原生設計的模型架構，並從零開始預訓練，具體包含兩個尺寸的稀疏模型，分別是 SmallThinker-4B-A0.6B 和 SmallThinker-21B-A3B。&lt;/p&gt; 
&lt;ul&gt; 
&lt;/ul&gt; 
&lt;p&gt;SmallThinker 專為低成本硬件設計，可在百元級國產開發板（如瑞芯微 RK3588）上流暢運行百億參數模型，旨在為資源受限的個人設備帶來強大、私密且低延遲的 AI 能力，無需依賴雲端。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1578" src="https://static.oschina.net/uploads/space/2025/0729/143436_ewWq_2720166.png" width="2044" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;用户可以通過 Transformers（版本需 &amp;gt;= 4.53.3）或 ModelScope 來運行該模型。官方 GitHub 倉庫提供了詳細的設置、模型轉換和運行指南。官方提示，模型使用了稀疏的 lm_head，可能會導致一定的精度損失，但用户可以手動修改代碼禁用此特性。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct&lt;/li&gt; 
 &lt;li&gt;https://github.com/SJTU-IPADS/PowerInfer/tree/main/smallthinker&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362945</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362945</guid>
      <pubDate>Thu, 17 Jul 2025 06:36:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>🔥造物分享：AnyShake Project 開源地震監測系統</title>
      <description/>
      <link>https://www.oschina.net/ai-creation/details/2112</link>
      <guid isPermaLink="false">https://www.oschina.net/ai-creation/details/2112</guid>
      <pubDate>Thu, 17 Jul 2025 06:18:00 GMT</pubDate>
    </item>
    <item>
      <title>谷歌 NotebookLM 即將推出「視頻概覽」功能</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;谷歌 NotebookLM 即將推出一項名為「視頻概覽 (Video Overviews)」的新功能，能以視頻幻燈片形式呈現內容摘要。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0729/141652_Z58B_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Ftestingcatalog%2Fstatus%2F1949120138373914737" target="_blank"&gt;相關爆料稱&lt;/a&gt;，這些視頻概覽將以視頻幻燈片的形式呈現，內容包含文本、圖像和其他視覺元素，並由女性聲音進行旁白解説。&lt;/p&gt; 
&lt;p&gt;谷歌 NotebookLM 功能於去年推出，旨在通過 AI 虛擬主持人根據用户上傳到 NotebookLM 的文檔（如課程閲讀材料或法律摘要）生成播客，幫助用户以另一種方式理解和消化文檔中的信息。&lt;/p&gt; 
&lt;p&gt;用户可以上傳中文 PDF、Google Docs、網頁鏈接或文本，NotebookLM 會生成中文總結或回答基於中文來源的問題。支持高達 50 萬字的單個來源，適合處理長篇中文文檔。NotebookLM 目前支持 130 種語言的輸入來源和聊天功能，包括中文（簡體和繁體）。用户可以上傳中文文檔、網頁鏈接或文本，並以中文與 AI 進行交互。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362937</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362937</guid>
      <pubDate>Thu, 17 Jul 2025 06:18:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>全球首家機器人 6S 店深圳開業</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;全球首家機器人「6S 店」於 7 月 28 日在深圳龍崗星河 WORLD 園區機器人劇場開業，店內集聚數百種機器人及配套零部件。多家企業帶來的產品涵蓋了家庭服務、醫療輔助、工業巡檢、教育陪伴等多個領域。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="358" src="https://oscimg.oschina.net/oscnet/up-5d0777ddd254f3732d7201680e7a9e95e67.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;「機器人 6S 店究竟是什麼？」深圳未來時代機器人有限公司 CEO 兼 6S 店店長林楓解釋稱，其在傳統汽車 4S 店「銷售（Sale）、零配件供應（Sparepart）、售後服務（Service）、信息反饋（Survey）」的基礎上，新增「租賃（Lease）、個性化訂製（Customized）」兩大功能，形成獨特的「6S」模式。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;活動現場數據顯示，已有超 200 家產業鏈上下游企業表達進駐意向，其中人形及服務機器人企業近 50 家，涵蓋從核心零部件研發到場景應用的全產業鏈環節。該店緊扣機器人產業特性，一方面將建立實時數據反饋機制，精準收集用户需求反哺研發；另一方面提供租賃服務，讓用户無需購買即可體驗。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;據介紹，機器人 6S 店內置「機器人零配件超市」，匯聚伺服電機、減速器等核心元器件，能滿足主流機器人維修需求，實現「快速響應、即時維修」。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362936</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362936</guid>
      <pubDate>Thu, 17 Jul 2025 06:12:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>網信辦整治自媒體發佈不實信息，平台需優化 AI 生成內容標識</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;為持續深入整治「自媒體」發佈不實信息亂象，進一步規範「自媒體」信息發佈行為，按照 2025 年「清朗」系列專項行動總體安排，中央網信辦決定自 7 月 24 日起，在全國範圍內啓動為期 2 個月的「清朗·整治‘自媒體’發佈不實信息」專項行動。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="221" src="https://oscimg.oschina.net/oscnet/up-fcdd633462b44ba4d48a4e73cc8356abdca.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;專項行動重點整治四類突出問題：&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;1.惡意蹭炒誤導公眾問題。涉熱點輿情或公眾人物時，假冒當事人、近親屬等，通過賬號名稱、簡介等方式編造身份，蹭炒熱點，混淆視聽。涉重大輿情、突發事件時，假冒知情人士，編造起因進展、傷亡人數等，無中生有，幹擾輿論。發佈財經、軍事、外交等重要領域信息時，虛構所謂「權威報道」「一手數據」「深度揭秘」等信息，胡編亂造，誤導認知。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;2.多種手段歪曲事實問題。利用人工智能生成合成技術，仿冒他人，或編造社會民生等領域虛假信息，欺騙公眾。通過劇情擺拍、拼湊剪輯等方式，編造事件、虛構或誇大情節，引起關注。歪曲解讀關乎公眾利益的政策方針、法規文件，宣揚「即將取消」「重大變動」等不實信息，製造噱頭。對往年社會新聞、政策發佈等舊聞舊事摘頭去尾，掩蓋時間、地點、結果等關鍵要素，惡意炒作。藉助網絡黑灰產等渠道，以刷榜打榜買榜方式，通過熱搜榜單呈現不實信息，操縱榜單。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;3.不做標註以假亂真問題。對涉及國內外時事、公共政策、社會事件等相關信息，未標註或未準確標註信息來源。以「網傳」「網友表示」「來源於互聯網」等方式發佈信息，模糊標註信息來源，發佈無實際依據內容。標註錯誤信息來源，或矩陣賬號互相引用標註，導致公眾無法追溯真實來源。以過小字號、隱蔽位置、進度條遮擋等方式標註，刻意弱化標註標識。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;4.專業領域信息不實問題。不進行專業資質認證，或以虛假認證、過期認證方式，冒用財經專家、醫生、律師等身份。歪曲解讀專業內容，如杜撰或篡改真實案例細節，發佈未經科學驗證或明顯違背科學常識的信息，將不同的歷史人物與事件張冠李戴、篡改史實。借專業知識分享名義，編造同質化文案或虛假故事，藉機引流帶貨。發佈教程，教授通過虛假擺拍、蹭熱引流等方式打造「網紅專家」人設，擾亂傳播秩序。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;中央網信辦要求網站平台建立三大機制：在信息發佈環節強制設置來源標註選項，未標註內容不得進入算法推薦池；細化專業資質認證流程，動態核驗賬號身份與運營業務匹配度;暢通舉報渠道，對首次違規賬號採取提示引導，對惡意編造重點領域信息、仿冒熱點當事人的賬號實施長期禁言或封號。同時，平台需完善負面清單、營利權限管理等制度，對存在突出問題的平台依法採取處罰措施。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;此次行動強調「標本兼治」，既通過技術手段壓縮不實信息生存空間，如優化 AI 生成內容標識功能，又壓實平台主體責任，要求其定期排查隱形變異問題。據網信辦負責人介紹，專項行動將與日常監管形成合力，推動建立「自媒體」行業信用評價體系，引導內容創作者回歸真實、專業的傳播軌道，為公眾營造可信、有序的網絡環境。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362926</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362926</guid>
      <pubDate>Thu, 17 Jul 2025 05:47:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>智象未來亮相 WAIC：多模態智能體，重塑創作的未來版圖</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#000000; text-align:left"&gt;2025 世界人工智能大會（WAIC）期間，智象未來（HiDream.ai）聯合創始人兼首席技術官姚霆發表主題演講，系統闡釋了多模態智能體在內容創作領域的技術突破與商業化實踐。作為聚焦多模態生成的 AI 創新企業，智象未來期待通過探索多模態大模型的有效落地形式， 「讓創作迴歸靈感，讓時間忠於故事」 ，推動內容創作從工具效率提升向生產力革命跨越。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:center"&gt;&lt;img alt="圖 1.jpg" height="500" src="https://oscimg.oschina.net/oscnet//85051c6c86d90e89291a7bfae9e6a83f.jpg" width="750" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;AI 技術的爆發式發展，正從實驗室快速走向產業應用。智象未來始終以「解決真實創作痛點」為導向，在商業化落地中探索出一條「技術築基、場景破局、價值閉環」的路徑。智象未來認為，真正的 AI 商業化不是單點技術的炫耀，而是從模型能力到服務形態，再到最終成果的全鏈路賦能。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;智象未來持續致力於從技術到價值的產品化思路，在這一過程中，智象構建了「MaaS-SaaS-RaaS」的遞進商業化體系。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:center"&gt;&lt;img alt="圖 2.jpg" height="422" src="https://oscimg.oschina.net/oscnet//9b90e81b026c2f731020661813f929b2.jpg" width="750" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;&lt;strong&gt;MaaS&lt;/strong&gt;&lt;strong&gt;（&lt;/strong&gt;&lt;strong&gt;Model as a Service&lt;/strong&gt;&lt;strong&gt;）&lt;/strong&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;是根基。打造百億級多模態基礎模型，支持圖像、視頻、音頻、文本等多模態的生成與理解。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;&lt;strong&gt;SaaS&lt;/strong&gt;&lt;strong&gt;（&lt;/strong&gt;&lt;strong&gt;Service as a Service&lt;/strong&gt;&lt;strong&gt;）&lt;/strong&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;是橋樑。基於基礎模型，開發面向垂直場景的產品，建設個人創作者平台和社區，將技術能力轉化為開箱即用的服務，降低創作門檻。 &amp;nbsp;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;&lt;strong&gt;RaaS&lt;/strong&gt;&lt;strong&gt;（&lt;/strong&gt;&lt;strong&gt;Result as a Service&lt;/strong&gt;&lt;strong&gt;）&lt;/strong&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;是終局。通過商業視頻營銷服務、新媒體創作智能體，直接為客户交付「可落地的成果」，讓 AI 真正成為創作的「生產力工具」而非「技術概念」。 &amp;nbsp;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;這種 「模型支撐服務，服務落地場景」 的邏輯，已在實際應用中驗證：智象多模態生成平台已服務於影視製作、產品營銷、文旅互娛等領域，實現從技術研發到商業價值的閉環。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;&lt;strong&gt;多模態技術突破：從&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;「&lt;/strong&gt;&lt;strong&gt;能生成&lt;/strong&gt;&lt;strong&gt;」&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;到&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;「&lt;/strong&gt;&lt;strong&gt;生成優&lt;/strong&gt;&lt;strong&gt;」&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;技術實力是商業化的底氣。智象多模態模型以「高維理解、精準生成」為核心，構建了覆蓋圖像、視頻、編輯的全棧能力矩陣。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;技術層面，智象多模態基礎模型歷經三次重要迭代，構建起 「理解深、控制準、畫質高」 的核心優勢。模型從 2023 年 8 月的 1.0 版本（擴散模型 DiT，實現多模態對齊），到 2024 年 6 月 2.0 版本（擴散自迴歸模型 DiT+AR，強化時空建模），再到 2024 年 12 月 3.0 版本（MoE 多場景學習，記憶增強），持續突破生成技術瓶頸。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;這些能力轉化為三大核心價值：語義一致性（如 IP 故事活化時保持風格統一）、精準可控性（支持個性化定製與元素自由調整）、影視級畫質（4K 分辨率、長時序穩定輸出），為專業創作提供技術保障。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:center"&gt;&lt;img alt="圖片 3.png" height="421" src="https://oscimg.oschina.net/oscnet//bc0b074c8a15ca0e37dd98a7a08fe241.png" width="750" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;在圖像生成領域，HiDream 系列開源模型表現亮眼，累計下載量超 60 萬次，被 Diffusers 庫、ComfyUI 、Recraft 等主流工具集成。智象多模態全系列模型均在國際權威榜單排名前列。HiDream-I1 全面開源後 24 小時內即登頂 Artificial Analysis 榜單，成為首個問鼎榜首的中國自研模型，Hugging Face 實時排名全球第一，下載量與點贊數持續攀升。此外，智象大模型家族已實現文本、圖像、視頻的聯合建模，其視頻生成產品支持 4K 高清畫質、全局 / 局部可控及劇本多鏡頭生成，被行業專家評價為「重新定義 AIGC 的美學標準」。同時，結合其開源的交互式編輯模型 HiDream-E1，用户通過自然語言指令即可完成圖像生成及編輯，直接降低創作門檻，助力全球開發者與創作者實現「所想即所得」。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;7 月，繼問鼎圖像生成開源模型競技場榜單後，最新開源模型 HiDream E1.1 再次強勢躋身 Artificial Analysis 圖像編輯智能體榜單第一梯隊，作為領先的開源圖像編輯模型，性能全面超越 Flux.1 Kontext 等主流模型，支持自然語言驅動的圖像編輯 —— 用户通過文字指令即可完成背景替換、顏色修改、局部重繪等操作。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:center"&gt;&lt;img alt="圖片 4.png" height="420" src="https://oscimg.oschina.net/oscnet//9dddc0cdea26950d1a5dfe9cd240357f.png" width="750" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;在視頻生成領域，模型支持文生視頻、圖生視頻、首尾幀生成，可精準復刻國漫、吉卜力等風格，實現鏡頭運動與畫面運動的聯合學習。通過擴散自迴歸模型（DiT+AR），我們解決了視頻生成中「時空一致性」難題，讓生成內容更貼近真實物理世界的規律。 &amp;nbsp;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;在創作工具箱層面，AI 口播、視頻模板、運動筆刷、虛擬換衣、圖像超分等功能，形成了「生成-編輯-優化」的完整閉環，滿足從個人創作者到企業客户的全場景需求。 &amp;nbsp;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:center"&gt;&lt;img alt="圖片 5.png" height="422" src="https://oscimg.oschina.net/oscnet//9d53f9a7a31deb70f0db3d482a02fec7.png" width="750" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;&lt;strong&gt;產品形態：&lt;/strong&gt;&lt;strong&gt;agent&lt;/strong&gt;&lt;strong&gt;驅動的&lt;/strong&gt;&lt;strong&gt;「&lt;/strong&gt;&lt;strong&gt;創作革命&lt;/strong&gt;&lt;strong&gt;」&lt;/strong&gt;&lt;strong&gt;，重構內容創作全流程&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;在產品形態上，智象以 「智能體」 為核心形態，構建覆蓋圖像生成、視頻創作、營銷傳播的工具鏈。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;作為面向短視頻二創的智能體，vivago agent 以「多模態輸入、智能拆解、交互式生成」為核心優勢。用户只需提供圖像、視頻、音頻、文本等素材（例如咖啡館的 logo、照片、宣傳語），即可自動分析需求、拆解任務（分鏡設計、劇本生成、素材檢索），調用圖像/視頻生成模型補全內容，並通過智能剪輯工具整合輸出。它不僅能理解「棕色線條勾勒的火焰+波浪 logo」的視覺特徵，還能捕捉「靜謐奢華的吧枱場景」的氛圍，讓短視頻創作從「從零開始」變為「按需生成」。 &amp;nbsp;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;智象未來即將正式發佈長視頻編輯智能體-HiClip。針對長視頻「內容過載、分發低效、回報週期長」的痛點，HiClip 通過多模態語義理解，精準解構內容核心（如提取高光片段、生成音頻摘要），實現「一次創作、全域適配」的二次傳播。無論是影視片段的高光剪輯，還是教育課程的知識點拆解，HiClip 都能讓長視頻內容煥發新的流量生命力。 &amp;nbsp;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;產品化落地實現了創作方面的互補：vivago agent 聚焦短視頻二創，通過模板檢索、智能剪輯、多模態生成，幫助用户快速製作個性化內容，解決傳統模板化創作的同質化問題；HiClip 則針對長視頻 「內容過載、分發低效」 的痛點，以多模態語義理解解構長視頻核心信息，實現高光片段提取、跨平台適配剪輯，激發長視頻二次傳播價值。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;&lt;strong&gt;生態共創：鏈接全產業鏈的價值網絡&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;AI 的價值，在於連接與賦能；技術與產品的落地，離不開生態的協同支撐。目前，智象未來正攜手跨境、互聯網、影視、新媒體、文旅等多領域夥伴，構建覆蓋多領域的生態網絡，形成 「技術-場景-生態」 的共贏格局。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:center"&gt;&lt;img alt="圖片 6.png" height="431" src="https://oscimg.oschina.net/oscnet//a8763d0a57e911cb5a981b70e0c97fde.png" width="750" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;讓每個創作者都能更好釋放創意潛力，是智象的始終堅持。讓 AI 真正 「理解創作、輔助創作」，讓內容產業的生產力革新正加速到來。智象未來期待以多模態智能體為支點，與行業夥伴共同探索「技術為筆，創意為墨」的新可能——讓每個創作者都能聚焦靈感，讓每個故事都能抵達更遠的地方。 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362906</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362906</guid>
      <pubDate>Thu, 17 Jul 2025 04:00:00 GMT</pubDate>
      <author>作者: 開源科技</author>
    </item>
    <item>
      <title>微軟為 Edge 瀏覽器推出新的 Copilot 模式，支持實時分析屏幕內容</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;微軟正在為其 Edge 瀏覽器推出一種名為&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.microsoft.com%2Fzh-cn%2Fedge%2Fai-powered%2Fcopilot-mode" target="_blank"&gt;「Copilot Mode」&lt;/a&gt;的全新實驗性模式，旨在提供一種由 AI 驅動的網頁瀏覽體驗。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1856" src="https://static.oschina.net/uploads/space/2025/0729/113855_QJwL_2720166.png" width="3360" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;該模式包含多項新功能，包括全新的現代化主頁（New Modern Homepage）、快速撰寫（Quick Compose）、簡單的任務切換（Simple Task Handoff）以及語音導航（Voice Navigation）。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-4177c9b14685e66cd78d1978e425461b2a9.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;其中一個核心功能是「Copilot Vision」，它允許 Copilot 「看到」用户的屏幕，即時掃描和分析屏幕上的內容，並實時提供相關的建議和見解。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362895/microsoft-edge-copilot-mode</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362895/microsoft-edge-copilot-mode</guid>
      <pubDate>Thu, 17 Jul 2025 03:39:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>國內首個農業智能大模型上線，每畝地增收可達 200 元</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;中國中化集團推出了國內首個 「農業種植綜合大模型」。這個大模型不僅依託於全國數百座農業技術服務中心的支持，更整合了超過千萬條農業知識資源，為農民提供精準、科學的種植指導。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;該農業種植綜合大模型貫穿了 「耕、種、管、收」 的整個過程，能夠進行高效、可靠的複雜任務處理。通過大模型的應用，農藝師只需在手機或平板電腦上就能實現線上智能決策，線下為農民提供貼身服務。這意味着，農民能夠通過實時監測作物的生長情況、土壤濕度，以及氣象和病蟲害等重要因素，獲取及時的建議，例如 「每畝需要多少肥料、何時澆水」 等信息。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="282" src="https://oscimg.oschina.net/oscnet/up-b29e4fd6f3ee345ed36aa8353bfd5d8e80a.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;許多網友對此表示興奮，甚至戲稱這項技術讓他們想起了小時候的 QQ 農場。科技的進步，不僅為農事決策帶來了高效和便利，還能夠讓農民在增加收入的同時，享受到更多的樂趣和成就感。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;據悉，使用這一大模型後，農事決策的時間可以縮短 75%，每畝地的增收可達 150 到 200 元。這對廣大農民而言，無疑是一個好消息。農業種植綜合大模型的上線，不僅提升了農業生產的智能化水平，也為全國的農業發展注入了新動力。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362892</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362892</guid>
      <pubDate>Thu, 17 Jul 2025 03:37:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>上海 AI 實驗室開源科學多模態大模型『書生』Intern-S1</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;上海人工智能實驗室（上海 AI 實驗室）&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fd7DfDz4yw_5ktewSpjzVDA" target="_blank"&gt;發佈&lt;/a&gt;並開源『書生』科學多模態大模型 Intern-S1，聲稱多模態能力全球開源第一，文本能力比肩國內外一流模型，科學能力全模態達到國際領先，作為融合科學專業能力的基礎模型，Intern-S1 綜合性能為當前開源模型中最優。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;公告介紹稱，Intern-S1 在同一模型內實現了語言和多模態性能的高水平均衡發展，具備「全能高手」的實力；同時，作為「科學明星」，它還富集多學科專業知識，重點強化了科學能力，在化學、材料、地球等多學科專業任務基準上超越了頂尖閉源模型 Grok-4；此外，Intern-S1 還開創了「多任務的通專融合」的新範式，支持大規模多任務強化學習齊頭並進，在保持能力全面的同時實現專業精通。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="319" src="https://oscimg.oschina.net/oscnet/up-e8b2a446e9dc17a212b7fc9fe2fd02a0f70.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="304" src="https://oscimg.oschina.net/oscnet/up-8d4b59325c224883c411d165c948180e543.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="153" src="https://oscimg.oschina.net/oscnet/up-99ef18ed34310abfc6028e4dc8aa28294d1.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;為了更好地適應科學數據，Intern-S1 新增了動態 Tokenizer 和時序信號編碼器，可支持多種複雜科學模態數據，實現了材料科學與化學分子式、生物製藥領域的蛋白質序列、天文巡天中的光變曲線、天體碰撞產生的引力波信號、地震台網記錄的地震波形等多種科學模態的深度融合。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;Intern-S1 還實現了對科學模態數據的深入理解與高效處理，例如，其對化學分子式的壓縮率相比 DeepSeek-R1 提升 70% 以上；在一系列基於科學模態的專業任務上消耗的算力更少，同時性能表現更優。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;更多詳情可&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fd7DfDz4yw_5ktewSpjzVDA" target="_blank"&gt;查看官方公告&lt;/a&gt;。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362877</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362877</guid>
      <pubDate>Thu, 17 Jul 2025 03:12:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>GPT-5 即將發佈！相關參數、功能與展望預測彙總</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;隨着人工智能領域的競爭日益加劇，OpenAI 的下一代大語言模型 GPT-5 備受關注。根據最新信息，GPT-5 預計將於 2025 年年中至晚些時候發佈，具體時間可能在 8 月或更晚。本文綜合網絡信息，整理了關於 GPT-5 的參數、功能及潛在影響的最新動態，為您呈現最全面的預覽。&lt;/span&gt;&lt;/p&gt; 
&lt;h3 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;發佈日期與開發進展&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;根據 OpenAI 首席執行官 Sam Altman 在 2025 年 2 月發佈的路線圖，GPT-5 預計在 2025 年年中推出，具體可能在 8 月或稍晚。Altman 在近期採訪中表示，GPT-5 的發佈將晚於 GPT-4.5（代號 Orion，已於 2025 年 2 月 27 日發佈），並強調其為「前沿模型」，代表重大技術飛躍。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;然而，開發過程中面臨的技術與資源挑戰可能導致進一步延遲。據報道，GPT-5（代號可能為 Orion 或 Arrakis）的訓練成本高達 5 億美元以上，且需要大規模數據中心支持，訓練時間至少 6 個月。OpenAI 內部也經歷了高管離職等動盪，可能影響開發進度。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;img height="241" src="https://oscimg.oschina.net/oscnet/up-44d549d72cc75a7aa62b1b9593f9f177949.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;h3 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;參數規模與技術架構&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;雖然 OpenAI 尚未公開 GPT-5 的具體參數數量，但業界推測其參數規模將顯著超越前代模型。以下是關鍵信息:&lt;/span&gt;&lt;/p&gt; 
&lt;ul style="margin-left:0; margin-right:0"&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;參數規模&lt;/strong&gt;：GPT-4 據估擁有約 1.5 萬億參數，而 GPT-5 可能達到 3 至 50 萬億參數，具體取決於是否採用混合專家模型（MoE）。有報道稱，GPT-5 可能利用 20，000 個 NVIDIA GB200 芯片或 150，000 個 H100 芯片進行訓練，支持高達 80 萬億參數的模型。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;架構創新&lt;/strong&gt;：GPT-5 將整合 GPT 系列與 o 系列（如 o1、o3）的能力，採用統一架構，消除用户在不同任務間切換模型的需求。可能引入圖神經網絡 (GNN) 和增強注意力機制，提升語言處理效率和複雜情境理解能力。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;訓練數據&lt;/strong&gt;：GPT-5 預計使用更大規模的多樣化數據集，包括公開網絡數據和私有企業數據，可能結合合成數據以應對數據短缺問題。然而，合成數據可能引發反饋循環，增加「幻覺」風險。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;核心功能與改進&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;GPT-5 被設計為多模態、統一智能系統，旨在提供更高效、準確的 AI 體驗。以下是其核心功能的預期亮點:&lt;/span&gt;&lt;/p&gt; 
&lt;ul style="margin-left:0; margin-right:0"&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;多模態能力&lt;/strong&gt;：GPT-5 將進一步增強多模態處理能力，支持文本、圖像、語音和視頻輸入輸出。基於 GPT-4o 的語音和圖像處理基礎，GPT-5 可能集成視頻處理功能，例如通過 OpenAI 的 SORA 技術實現文本到視頻的生成。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;高級推理能力&lt;/strong&gt;：OpenAI 強調 GPT-5 將顯著提升鏈式推理（Chain-of-Thought）能力，擅長多步驟邏輯和決策制定。相比 GPT-4o 的快速響應，GPT-5 將更擅長處理複雜問題，如軟件工程中的代碼生成與調試、數學和物理等科學任務。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;上下文窗口擴展&lt;/strong&gt;：GPT-4o 的上下文窗口為 128，000 個 token，而 GPT-5 可能支持高達 500 萬個 token，足以處理整本書籍或大型企業數據，提升長文本處理能力。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;減少幻覺&lt;/strong&gt;：GPT-5 預計將「幻覺」率降至 10% 以下，顯著提高輸出的準確性和可靠性，特別是在科學和編程領域。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;自主 AI 代理&lt;/strong&gt;：GPT-5 可能引入自主 AI 代理功能，能夠執行現實世界的任務，如管理郵件、預訂日程或根據用户偏好完成購物，減少人工幹預。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;Canvas 工作空間&lt;/strong&gt;：基於 GPT-4o 的 Canvas 功能，GPT-5 將提供更強大的交互式工作空間，優化編碼、數學和分步工作流程的體驗。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;行業影響與應用前景&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;GPT-5 的發佈將對多個領域產生深遠影響:&lt;/span&gt;&lt;/p&gt; 
&lt;ul style="margin-left:0; margin-right:0"&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;軟件開發&lt;/strong&gt;：測試者反饋，GPT-5 在複雜軟件項目中的代碼生成和調試能力超越了 Anthropic 的 Claude4Sonnet，可能成為開發者首選工具。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;科學研究&lt;/strong&gt;：在數學、物理和生物學等學科中，GPT-5 的高級推理能力將加速研究進程，支持複雜數據分析和假設驗證。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;商業與生產力&lt;/strong&gt;：通過自主 AI 代理和個性化功能，GPT-5 可優化客户服務、內容創作和日常任務自動化，提升企業效率。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;教育與醫療&lt;/strong&gt;：GPT-5 的多模態能力和上下文理解將革新教育領域的個性化學習，以及醫療領域的患者交互和文檔處理。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;挑戰與倫理考量&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;儘管前景光明，GPT-5 的開發和部署面臨多重挑戰:&lt;/span&gt;&lt;/p&gt; 
&lt;ul style="margin-left:0; margin-right:0"&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;成本與資源&lt;/strong&gt;：訓練成本高昂，數據中心建設週期長，可能限制 OpenAI 的並行開發能力。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;倫理與安全&lt;/strong&gt;：大規模模型可能引發誤用風險，如生成虛假信息或模擬人類行為。OpenAI 正在進行嚴格的安全測試，推遲了部分功能的發佈。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;競爭壓力&lt;/strong&gt;：Anthropic 的 Claude 系列、Google 的 Gemini 和 Meta 的 LLaMA 等競品正在迅速追趕，迫使 OpenAI 在性能與可靠性之間找到平衡。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;用户反饋與社區期待&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;社交媒體上，開發者對 GPT-5 的期待集中在編程能力和推理性能的提升。部分用户在 X 平台上提到，GPT-5 的早期測試版在軟件工程任務中表現優異，超越 Claude Sonnet4。然而，也有用户擔憂其高昂的訂閲成本和潛在的使用限制，類似 Anthropic 對 Claude Code Max 計劃的調整可能引發的不滿。&lt;/span&gt;&lt;/p&gt; 
&lt;h3 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;總結&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;GPT-5 作為 OpenAI 的下一代旗艦模型，預計將通過更大的參數規模、統一的架構和多模態能力，顯著提升 AI 的推理、準確性和實用性。儘管面臨成本、安全和競爭等多重挑戰，其在編程、科研和商業領域的潛力不容忽視。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362874</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362874</guid>
      <pubDate>Thu, 17 Jul 2025 03:01:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Anthropic 將在 8 月底面向 Claude Pro 和 Max 訂閲用户推出新的每週使用限制</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Anthropic &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2FAnthropicAI%2Fstatus%2F1949898502688903593" target="_blank"&gt;宣佈&lt;/a&gt;，由於 Claude Code 的需求空前增長，將從 8 月 28 日起為 Claude Pro 和 Max 訂閲計劃引入新的每週使用量限制。此舉旨在解決因少數極端使用案例和違反服務條款的行為（如賬户共享和轉售）導致的系統容量問題，以確保為所有用户提供更公平、可靠的服務。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0729/105318_5uv3_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;新的限制措施將在現有的每 5 小時重置限制的基礎上，增加一個每 7 天重置的總體每週限制，以及一個針對 Claude Opus 4 的特定每週限制。Anthropic 估計，根據當前使用情況，新規將影響不到 5% 的訂閲用户。&lt;/p&gt; 
&lt;p&gt;公司透露，一些重度用户在後台 24/7 連續運行模型，其中一個案例是在每月 200 美元的套餐上消耗了價值數萬美元的模型用量。新限制旨在緩解此類高成本使用情況。&lt;/p&gt; 
&lt;p&gt;對於受影響的用户，Anthropic 給出了一些預期使用時長的參考：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;大多數 Pro 用户在每週限制內預計可使用 40-80 小時的 Sonnet 4；&lt;/li&gt; 
 &lt;li&gt;大多數 Max 20x 用户則可使用 240-480 小時的 Sonnet 4 和 24-40 小時的 Opus 4。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;對於超出限制的 Max 計劃用户，Anthropic 將提供以標準 API 價格購買額外用量的選項。公司表示仍在探索支持長期使用案例的最佳方式，並歡迎重度用户提供反饋。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362871</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362871</guid>
      <pubDate>Thu, 17 Jul 2025 02:53:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>快手可靈發佈 Kling Lab</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;快手的 Kling AI 團隊&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2FKling_ai%2Fstatus%2F1949760383692255518" target="_blank"&gt;宣佈&lt;/a&gt;推出 Kling Lab，這是一個旨在簡化創作流程、提高效率並促進協作的新工作空間。該產品目前正處於 Beta 測試階段。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0729/104711_dOxb_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;此外，在 2025 世界人工智能大會（WAIC）上，可靈 AI 還披露了最新用户數據，在全球擁有超過 4500 萬創作者，產品自發布以來迭代升級 30 餘次，累計生成超 2 億個視頻和 4 億張圖片。&lt;/p&gt; 
&lt;p&gt;可靈 AI 產品及運營負責人李楊表示，4 月可靈 2.0 發佈以來，服務的 B 端商家數量迎來爆發式增長。截至目前，全球範圍內已有超過兩萬企業客户及開發者接入了可靈 AI 的 API（應用程序編程接口）接口，覆蓋全球 149 個國家和地區。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362870</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362870</guid>
      <pubDate>Thu, 17 Jul 2025 02:48:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>阿里開源通義萬相 Wan2.2</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;阿里&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FiPL7OLQhwYdoFelHt41N6Q" target="_blank"&gt;宣佈&lt;/a&gt;開源視頻生成模型「通義萬相 Wan2.2」，此次共開源文生視頻（Wan2.2-T2V-A14B）、圖生視頻（Wan2.2-I2V-A14B）和統一視頻生成（Wan2.2-IT2V-5B）三款模型。其中文生視頻模型和圖生視頻模型均為業界首個使用 MoE 架構的視頻生成模型，總參數量為 27B，激活參數 14B。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;根據介紹，通義萬相 2.2 率先在視頻生成擴散模型中引入 MoE 架構，有效解決視頻生成處理 Token 過長導致的計算資源消耗大問題。Wan2.2-T2V-A14B、Wan2.2-I2V-A14B 兩款模型均由高噪聲專家模型和低噪專家模型組成，分別負責視頻的整體佈局和細節完善，在同參數規模下，可節省約 50% 的計算資源消耗，在模型能上，通義萬相 2.2 在複雜運動生成、人物交互、美學表達、複雜運動等維度上也取得了顯著提升。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;Wan2.2 還首創了「電影美學控制系統」，光影、色彩、構圖、微表情等能力媲美專業電影水平。例如，用户輸入「黃昏」、「柔光」、「邊緣光」、「暖色調」「中心構圖」等關鍵詞，模型可自動生成金色的落日餘暉的浪漫畫面；使用「冷色調」、「硬光」、「平衡圖」、「低角度」的組合，則可以生成接近科幻片的畫面效果。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" height="213" src="https://oscimg.oschina.net/oscnet/up-9384df8eada31bdbc196286746847bfe546.gif" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img alt="" height="177" src="https://oscimg.oschina.net/oscnet/up-58b00afb9376a3a0972e10d973fdb82ab5f.gif" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img alt="" height="213" src="https://oscimg.oschina.net/oscnet/up-7c4a2896d24586d24ff41491862261a2aeb.gif" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;通義萬相還開源了一款 5B 小尺寸的統一視頻生成模型，單一模型同時支持文生視頻和圖生視頻，可在消費級顯卡部署。該模型採用了高壓縮率 3D VAE 架構，時間與空間壓縮比達到高達 4×16×16，信息壓縮率提升至 64，均實現了開源模型的最高水平，僅需 22G 顯存（單張消費級顯卡）即可在數分鐘內生成 5 秒高清視頻，是目前 24 幀每秒、720P 像素級的生成速度最快的基礎模型。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362867</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362867</guid>
      <pubDate>Thu, 17 Jul 2025 02:13:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
  </channel>
</rss>
