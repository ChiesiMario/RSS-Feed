<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>開源中國-綜合資訊</title>
        <link>https://www.oschina.net/news/industry</link>
        <atom:link href="http://8.134.148.166:30044/oschina/news/industry" rel="self" type="application/rss+xml"></atom:link>
        <description>開源中國-綜合資訊 - Powered by RSSHub</description>
        <generator>RSSHub</generator>
        <webMaster>contact@rsshub.app (RSSHub)</webMaster>
        <language>en</language>
        <lastBuildDate>Mon, 28 Apr 2025 07:37:15 GMT</lastBuildDate>
        <ttl>5</ttl>
        <item>
            <title>2023 年最熱門的 AI 職位——「提示詞工程師」已過時</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;據《華爾街日報》&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.wsj.com%2Farticles%2Fthe-hottest-ai-job-of-2023-is-already-obsolete-1961b054&quot; target=&quot;_blank&quot;&gt;報道&lt;/a&gt;，曾被譽為 2023 年最熱門 AI 職位、年薪可達 20 萬美元的「提示工程師」（Prompt Engineer）正迅速降温。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0428/144406_aPB5_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;根據微軟近期一項覆蓋 31 個國家 31,000 名員工的調查，在未來 12 至 18 個月企業考慮增設的新職位中，提示工程師排名倒數第二，遠低於 AI 訓練師、AI 數據專家和 AI 安全專家等職位。&lt;/p&gt; 
&lt;p&gt;所謂提示詞工程，是指設計、開發、測試和優化用於與生成式 AI 模型交互的文本輸入（即「提示詞」），目標是引導 AI 模型生成準確且符合需求的輸出，發揮 AI 模型的潛力。但是，由於 AI 模型固有的不透明性，這種操作的科學性和有效性始終存在疑問，也有很多人藉此營銷、行騙。&lt;/p&gt; 
&lt;p&gt;報道指出，「提示詞工程師」熱度發生變化的核心原因在於，&lt;strong&gt;現代 AI 模型已能更好地理解用户意圖，甚至在指令不清時主動提問，大大降低了對精心設計提示詞的依賴&lt;/strong&gt;。另一個關鍵因素是企業策略的轉變。相比設立專門職位，許多公司選擇對現有員工提供 AI 工具和培訓，將使用 AI 的能力視為一項基礎技能。&lt;/p&gt; 
&lt;p&gt;招聘市場的實際數據也印證了這一趨勢。儘管在 ChatGPT 面世後，用户對「提示工程師」的搜索量曾短暫飆升，但企業發佈的實際招聘崗位數量始終極少，搜索熱度也已大幅回落並趨於平穩。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/347101</link>
            <guid isPermaLink="false">https://www.oschina.net/news/347101</guid>
            <pubDate>Mon, 28 Apr 2025 06:45:13 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>微軟發佈 2025 工作趨勢：每位員工都將成為 Agent 的 「老闆」</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;微軟近日在其官網發佈了 2025 年工作趨勢指數報告，分析了來自全球 31 個國家和地區的 31，000 家企業。報告結合了 LinkedIn 勞動力市場趨勢、數萬億個 Microsoft365 的生產力信號以及眾多專家的見解，指出 「人機協作」 模式正在重塑企業架構，催生出一種全新的 「前沿公司」。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;251&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-262509e06b372a5030a9fbb7c87f5ba548a.png&quot; width=&quot;700&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style=&quot;color:#242424; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;「前沿公司」 是一種新型的組織形式，主要圍繞智能體（Agent）構建，以適應快速變化的商業環境和技術進步。這種公司的核心特點是將人類智慧與智能體相結合，形成高效的團隊，顯著提高生產力和創新能力，並節省工作時間。在這樣的公司中，智能體可以是各種自動化工具或智能助手，執行從數據處理到複雜決策支持的多種任務。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#242424; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;報告指出，隨着智能體的廣泛應用，員工將逐步成為 「Agent 老闆」，他們不僅需要管理和優化這些智能體，還需具備相應的新技能。這意味着未來每位員工都要像初創公司的 CEO 一樣思考如何利用 AI 來提升工作效率。根據調研顯示，67% 的企業領導者表示他們熟悉 Agent 的概念，而這一比例在員工中僅為 40%。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#242424; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;此外，前沿公司的組織結構也將發生變化，變得更加靈活和以結果為導向。這種新的工作架構會根據業務需求動態調整，靈活組合人類和智能體資源，以實現最佳效果。微軟指出，隨着智能體的加入，未來每位員工都有可能從第一天起就參與到複雜的工作中，甚至一名初級員工也可以藉助 AI 管理整個營銷活動。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#242424; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;在這個新模式中，企業需要關注人機協作的比例，確保資源的高效利用。同時，管理層需要重構職能，以適應這種人機協作的新趨勢。微軟還提到，員工需要從 「工具使用者」 轉變為 「合作伙伴」，通過與智能體的雙向互動激發創新能力。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/347091</link>
            <guid isPermaLink="false">https://www.oschina.net/news/347091</guid>
            <pubDate>Mon, 28 Apr 2025 05:58:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>Hyprnote —— 會議專用 AI 記事本</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                                                                                                                            &lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:#3c3c43&quot;&gt;Hyprnote 專為會議繁忙的人士打造，是一款&lt;/span&gt;&lt;span style=&quot;background-color:#ffffff; color:#1f2328&quot;&gt;適用於連續會議的 AI 記事本。本地優先且可擴展。&lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;錄並轉錄您的會議&lt;/li&gt;
&lt;li&gt;從原始會議記錄中生成&lt;strong&gt;有力的摘要&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;離線&lt;/strong&gt;工作使用&lt;strong&gt;開源模型&lt;/strong&gt;（&lt;em&gt;Whisper&lt;/em&gt;和&lt;em&gt;Llama&lt;/em&gt;）&lt;/li&gt;
&lt;li&gt;高度&lt;a href=&quot;https://docs.hyprnote.com/extensions/&quot;&gt;可擴展&lt;/a&gt;，由&lt;a href=&quot;https://docs.hyprnote.com/plugins/&quot;&gt;插件提供支持&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div&gt;
&lt;div style=&quot;margin-left:auto; margin-right:auto&quot;&gt;
&lt;div style=&quot;margin-right:calc(50% - 678px)&quot;&gt;
&lt;div&gt;
&lt;h2&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;亮點&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h2&gt;
&lt;/div&gt;

&lt;div&gt;
&lt;h3&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;增強你的筆記&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h3&gt;
&lt;/div&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;隨意記下一些東西，Hyprnote 將根據您的備忘錄製作會議記錄。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;img alt=&quot;&quot; height=&quot;391&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0427/152058_SbVr_4252687.gif&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;

&lt;div&gt;
&lt;h3&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;離線和隱私&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h3&gt;
&lt;/div&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Hyprnote 是本地優先的&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;img height=&quot;392&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0427/151933_5xtQ_4252687.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;div&gt;
&lt;h3&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;擴展和插件&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h3&gt;
&lt;/div&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;就像 VSCode 一樣，可以根據你的情況添加或創建擴展。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;img height=&quot;382&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0427/151827_6b1U_4252687.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;div&gt;
&lt;div&gt;
&lt;div&gt;
&lt;div&gt;
&lt;div&gt;
&lt;div&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;例如，&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;a href=&quot;https://docs.hyprnote.com/extensions/transcript.html&quot;&gt;transcript extension&lt;/a&gt;&lt;span style=&quot;color:#1f2328&quot;&gt;&amp;nbsp;&lt;/span&gt;由&amp;nbsp;&lt;a href=&quot;https://docs.hyprnote.com/plugins/listener.html&quot;&gt;listener plugin&lt;/a&gt;&amp;nbsp;提供支持。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div&gt;
&lt;pre&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;background-color:#f6f8fa&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;color:#1f2328&quot;&gt;&lt;span style=&quot;background-color:#f6f8fa&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;color:#6639ba&quot;&gt;useEffect&lt;/span&gt;&lt;/span&gt;&lt;span&gt;(&lt;/span&gt;&lt;span&gt;(&lt;/span&gt;&lt;span&gt;)&lt;/span&gt; &lt;span&gt;&lt;span style=&quot;color:#0550ae&quot;&gt;=&amp;gt;&lt;/span&gt;&lt;/span&gt; &lt;span&gt;{&lt;/span&gt;
  &lt;span&gt;&lt;span style=&quot;color:#cf222e&quot;&gt;const&lt;/span&gt;&lt;/span&gt; &lt;span&gt;channel&lt;/span&gt; &lt;span&gt;&lt;span style=&quot;color:#0550ae&quot;&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span style=&quot;color:#cf222e&quot;&gt;new&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span style=&quot;color:#953800&quot;&gt;Channel&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span style=&quot;color:#0550ae&quot;&gt;&amp;lt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span style=&quot;color:#1f2328&quot;&gt;SessionEvent&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span style=&quot;color:#0550ae&quot;&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;(&lt;/span&gt;&lt;span&gt;)&lt;/span&gt;&lt;span&gt;;&lt;/span&gt;
  &lt;span&gt;listenerCommands&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;&lt;span&gt;&lt;span style=&quot;color:#6639ba&quot;&gt;subscribe&lt;/span&gt;&lt;/span&gt;&lt;span&gt;(&lt;/span&gt;&lt;span&gt;channel&lt;/span&gt;&lt;span&gt;)&lt;/span&gt;&lt;span&gt;;&lt;/span&gt;

  &lt;span&gt;channel&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;&lt;span&gt;&lt;span style=&quot;color:#6639ba&quot;&gt;onmessage&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span style=&quot;color:#0550ae&quot;&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span&gt;(&lt;/span&gt;&lt;span&gt;e&lt;/span&gt;&lt;span&gt;)&lt;/span&gt; &lt;span&gt;&lt;span style=&quot;color:#0550ae&quot;&gt;=&amp;gt;&lt;/span&gt;&lt;/span&gt; &lt;span&gt;{&lt;/span&gt;
    &lt;span&gt;&lt;span style=&quot;color:#cf222e&quot;&gt;if&lt;/span&gt;&lt;/span&gt; &lt;span&gt;(&lt;/span&gt;&lt;span&gt;e&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;&lt;span&gt;&lt;span style=&quot;color:#0550ae&quot;&gt;type&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span style=&quot;color:#0550ae&quot;&gt;===&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span style=&quot;color:#0a3069&quot;&gt;&quot;started&quot;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;)&lt;/span&gt; &lt;span&gt;{&lt;/span&gt;
      &lt;span&gt;&lt;span style=&quot;color:#6639ba&quot;&gt;setIsLive&lt;/span&gt;&lt;/span&gt;&lt;span&gt;(&lt;/span&gt;&lt;span&gt;&lt;span style=&quot;color:#0550ae&quot;&gt;true&lt;/span&gt;&lt;/span&gt;&lt;span&gt;)&lt;/span&gt;&lt;span&gt;;&lt;/span&gt;
    &lt;span&gt;}&lt;/span&gt;
    &lt;span&gt;&lt;span style=&quot;color:#cf222e&quot;&gt;if&lt;/span&gt;&lt;/span&gt; &lt;span&gt;(&lt;/span&gt;&lt;span&gt;e&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;&lt;span&gt;&lt;span style=&quot;color:#0550ae&quot;&gt;type&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span style=&quot;color:#0550ae&quot;&gt;===&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span style=&quot;color:#0a3069&quot;&gt;&quot;stopped&quot;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;)&lt;/span&gt; &lt;span&gt;{&lt;/span&gt;
      &lt;span&gt;&lt;span style=&quot;color:#6639ba&quot;&gt;setIsLive&lt;/span&gt;&lt;/span&gt;&lt;span&gt;(&lt;/span&gt;&lt;span&gt;&lt;span style=&quot;color:#0550ae&quot;&gt;false&lt;/span&gt;&lt;/span&gt;&lt;span&gt;)&lt;/span&gt;&lt;span&gt;;&lt;/span&gt;
    &lt;span&gt;}&lt;/span&gt;
  &lt;span&gt;}&lt;/span&gt;&lt;span&gt;;&lt;/span&gt;

  &lt;span&gt;&lt;span style=&quot;color:#cf222e&quot;&gt;return&lt;/span&gt;&lt;/span&gt; &lt;span&gt;(&lt;/span&gt;&lt;span&gt;)&lt;/span&gt; &lt;span&gt;&lt;span style=&quot;color:#0550ae&quot;&gt;=&amp;gt;&lt;/span&gt;&lt;/span&gt; &lt;span&gt;{&lt;/span&gt;
    &lt;span&gt;listenerCommands&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;&lt;span&gt;&lt;span style=&quot;color:#6639ba&quot;&gt;unsubscribe&lt;/span&gt;&lt;/span&gt;&lt;span&gt;(&lt;/span&gt;&lt;span&gt;channel&lt;/span&gt;&lt;span&gt;)&lt;/span&gt;&lt;span&gt;;&lt;/span&gt;
  &lt;span&gt;}&lt;/span&gt;&lt;span&gt;;&lt;/span&gt;
&lt;span&gt;}&lt;/span&gt;&lt;span&gt;,&lt;/span&gt; &lt;span&gt;[&lt;/span&gt;&lt;span&gt;]&lt;/span&gt;&lt;span&gt;)&lt;/span&gt;&lt;span&gt;;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

                                                                    &lt;/div&gt;
                                                                </description>
            <link>https://www.oschina.net/p/hyprnote</link>
            <guid isPermaLink="false">https://www.oschina.net/p/hyprnote</guid>
            <pubDate>Mon, 28 Apr 2025 05:53:00 GMT</pubDate>
        </item>
        <item>
            <title>騰訊正式開源跨端框架 Kuikly：基於 Kotlin 創建 Android、iOS、鴻蒙、Web、小程序應用</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;騰訊跨端框架&amp;nbsp;Kuikly 正式開源。根據官方介紹，Kuikly 是基於 Kotlin Multiplatform 的 UI 與邏輯全面跨端綜合解決方案，由騰訊大前端領域 Oteam（公司級）推出，目的在於提供一套一碼多端、極致易用、動態靈活的全平台高性能開發框架。&lt;/p&gt; 
&lt;p&gt;Kuikly（Kotlin UI Kit，發音同 quickly）使用 Kotlin 開發了聲明式 UI 框架，映射到系統原生控件做渲染，最終用 KMM（Kotlin Multiplatform Mobile）實現跨端。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-c10e21f658b9ab50513e216dc7c37cfa28e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;雖然是全平台，但目前暫時只開源了 Android 和 iOS，鴻蒙部分 5 月才開源，而 Web 和，小程序暫定是 Q2：&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0428/121851_K8I9_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Kuikly 開源地址：&lt;em&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FTencent-TDS%2FKuiklyUI&quot;&gt;https://github.com/Tencent-TDS/KuiklyUI&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Kuikly 基於 Kotlin MultiPlatform（KMP）技術，它利用了 KMP 邏輯跨平台的能力，並抽象出通用的跨平台 UI 渲染接口，複用平台的 UI 組件，從而達到 UI 跨平台，具有輕量、高性能、可動態化等優點；同時，KuiklyBase 基建同樣支持邏輯跨端。 讓開發者&lt;strong&gt;可以使用 Kotlin 創建 Android、iOS、鴻蒙、Web、小程序應用&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-911df639ea27ac02b452b9a379738d91ddd.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://img.ithome.com/newsuploadfiles/2025/3/c0981983-cece-4d31-9488-e775586c8881.png?x-bce-process=image/format,f_avif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;Kuikly 跨端框架系統要求：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;iOS 12.0 版本及以上&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Android 5.0 版本及以上&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;HarmonyOS Next 5.0.0 (12) 版本及以上&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Kotlin 版本 1.3.10 版本及以上&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;詳情查看文檔：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fkuikly.tds.qq.com%2F%25E7%25AE%2580%25E4%25BB%258B%2Farch.html&quot;&gt;https://kuikly.tds.qq.com/%E7%AE%80%E4%BB%8B/arch.html&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/347077/tencent-tds-kuikly</link>
            <guid isPermaLink="false">https://www.oschina.net/news/347077/tencent-tds-kuikly</guid>
            <pubDate>Mon, 28 Apr 2025 04:22:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>GPUStack v0.6 超重磅更新：vLLM 多機分佈式、昇騰 MindIE 等</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                                                                                                                                        &lt;p&gt;&lt;strong&gt;GPUStack 是一個 100% 開源的模型服務平台&lt;/strong&gt; ，支持 &lt;strong&gt;Linux、Windows 和 macOS&lt;/strong&gt; ，支持 &lt;strong&gt;NVIDIA、AMD、Apple Silicon、昇騰、海光、摩爾線程&lt;/strong&gt; 等 GPU 構建&lt;strong&gt;異構 GPU 集羣&lt;/strong&gt; ，支持 &lt;strong&gt;LLM、多模態、Embedding、Reranker、圖像生成、Speech-to-Text 和 Text-to-Speech&lt;/strong&gt; 模型，支持 &lt;strong&gt;vLLM、MindIE、llama-box&lt;/strong&gt; （&lt;strong&gt;基於 llama.cpp 與 stable-diffusion.cpp&lt;/strong&gt; ）等多種推理引擎與&lt;strong&gt;推理引擎多版本並行&lt;/strong&gt; ，支持&lt;strong&gt;資源自動調度分配、模型故障自動恢復、多機分佈式推理、混合異構推理、推理請求負載均衡、資源與模型監控指標觀測、國產化支持、用户管理與 API 認證授權等各種企業級特性&lt;/strong&gt; ，提供 &lt;strong&gt;OpenAI 兼容 API 無縫接入 Dify、RAGFlow、FastGPT、MaxKB 等各種上層應用框架&lt;/strong&gt;，是企業建設模型服務平台的理想選擇。&lt;/p&gt; 
&lt;p&gt;GPUStack 一直&lt;strong&gt;致力於以最簡單易用的方式，幫助用户快速納管異構 GPU 資源並運行所需的 AI 模型，從而支撐 RAG、AI Agents 以及其他生成式 AI 落地場景&lt;/strong&gt;。為用户打造絕佳的使用體驗是我們始終堅持的目標。最新發布的 v0.6 是迄今為止最重磅的版本，全方位完善了平台的整體功能、性能、穩定性和用户使用體驗。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;GPUStack v0.6 版本的核心更新包括&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;vLLM 多機分佈式推理&lt;/strong&gt;：提供生產級的多機分佈式推理能力，支撐 DeepSeek R1 / V3 等單機 GPU 資源無法運行的超大參數量模型。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;昇騰 MindIE 支持&lt;/strong&gt;：為昇騰 910B 和 310P 用户提供內置的 MindIE 推理引擎支持，以提供最佳的模型推理表現。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;模型兼容性檢測&lt;/strong&gt;：提供對模型是否支持部署的兼容性檢測，目前提供對模型架構支持、操作系統兼容、資源可用性、本地路徑可用性等依賴的實時檢測，後續還會持續加入更多檢測條件，提供更加友好的模型部署體驗。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;模型下載管理&lt;/strong&gt;：支持管理已下載的模型文件、支持以不佔用 GPU 資源分配為前提，發起單機/多機的模型下載任務、支持將本地路徑的模型文件添加到 UI 中進行統一管理。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;模型故障自動恢復&lt;/strong&gt;：支持模型在發生故障時的自動恢復機制。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;端口暴露優化&lt;/strong&gt;：優化需要暴露的端口範圍，API 入口到模型實例的推理請求統一經過代理轉發，不再需要暴露模型實例端口，降低 96% 以上的端口暴露，並支持用户自定義。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;增強國際化支持&lt;/strong&gt;：GPUStack 用户遍佈全球上百個國家和地區，本次 GPUStack 社區用户貢獻了俄語和日語支持，為不同語言的用户提供更加友好的使用體驗，加速推進 GPUStack 的全球化應用。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;UI / UX 全方位優化&lt;/strong&gt;：全方位的 UI / UX 優化，逐幀打磨，打造業界最好用的模型推理平台。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;這一版本總共包含&lt;strong&gt;上百項增強、修復、穩定性改進和用户體驗優化&lt;/strong&gt;，為用户的生產落地提供強大的場景支持。&lt;/p&gt; 
&lt;p&gt;有關 &lt;strong&gt;GPUStack&lt;/strong&gt; 的詳細信息，可以訪問：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;GitHub 倉庫地址: &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fgpustack%2Fgpustack&quot; target=&quot;_blank&quot;&gt;https://github.com/gpustack/gpustack&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;GPUStack 用户文檔: &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.gpustack.ai&quot; target=&quot;_blank&quot;&gt;https://docs.gpustack.ai&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;重點特性介紹&lt;/h2&gt; 
&lt;h3&gt;vLLM 多機分佈式推理&lt;/h3&gt; 
&lt;p&gt;隨着大語言模型的參數規模不斷提升，傳統單機 GPU 資源已難以滿足推理部署的實際需求。為此，GPUStack 在當前版本中正式支持生產級的 vLLM 多機分佈式推理能力。通過跨主機部署，將模型按張量或按層切分，分佈到多個節點運行，從而實現對超大參數模型（如 DeepSeek R1、DeepSeek V3 等）的推理支持。&lt;/p&gt; 
&lt;p&gt;當前，GPUStack 對以下兩類推理引擎提供分佈式支持：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;llama-box：異構分佈式，適用於研發測試環境&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt; • 支持 Linux、Windows 和 macOS 操作系統；&lt;/p&gt; 
&lt;p&gt; • 允許不同操作系統、不同品牌、不同規格的 GPU 混合實現異構分佈式推理；&lt;/p&gt; 
&lt;p&gt; • 可在桌面或輕量服務器上快速構建異構分佈式推理環境；&lt;/p&gt; 
&lt;p&gt; • 更適用於日常研發、模型驗證、兼容性測試等場景。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;vLLM：同構分佈式，面向生產環境&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt; • 支持在多台 Linux 服務器之間進行同構分佈式推理；&lt;/p&gt; 
&lt;p&gt; • 要求參與節點的硬件環境基本一致（如 GPU 型號、數量、顯存）；&lt;/p&gt; 
&lt;p&gt; • 支持張量並行和流水線並行，具備良好的推理吞吐能力；&lt;/p&gt; 
&lt;p&gt; • 適合生產環境下對高併發、低延遲模型服務的部署需求。&lt;/p&gt; 
&lt;p&gt;通過 vLLM 和 llama-box 的分佈式推理能力，GPUStack 能夠覆蓋&lt;strong&gt;從模型研發驗證到大規模生產部署的完整流程&lt;/strong&gt;。在研發階段，用户可使用 llama-box 構建靈活的測試集羣；在生產部署階段，則可通過 vLLM 提供穩定可靠的推理服務能力。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//ec5d9aef8c8a1a7aeacd7175a0c0e600.png&quot; alt=&quot;model-info&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;昇騰 MindIE 支持&lt;/h3&gt; 
&lt;p&gt;在之前版本中，GPUStack 基於 llama-box 推理引擎初步支持了昇騰 910B 和 310P 芯片的模型推理。然而由於算子支持不全及相關生態不夠完善，實際使用中存在較多限制，例如只支持模型的部分量化精度，在性能和穩定性方面也弱於昇騰官方推理引擎 MindIE。&lt;/p&gt; 
&lt;p&gt;為了提升用户在昇騰 NPU 上的模型推理體驗，GPUStack 現已內置集成 MindIE 推理引擎，對 910B 和 310P 提供更加穩定且高性能的模型推理能力。&lt;/p&gt; 
&lt;p&gt;MindIE 是昇騰官方推出的高性能深度學習推理框架，具備運行加速、調試調優與快速部署等多項優勢，目前在昇騰硬件上表現最為出色。得益於其較為成熟的軟硬件協同生態，MindIE 已成為在 NPU 上部署推理模型的主流方案。&lt;/p&gt; 
&lt;p&gt;當前，GPUStack 已完成對 MindIE 引擎的初步集成，相比於 llama-box 引擎，在部分場景可以達到數倍的推理速度提升。未來還將持續優化，並探索對更多推理引擎的支持，例如 vLLM（vLLM-Ascend），以滿足在昇騰平台上的多樣化模型推理需求。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//c0680304502e35923518c9dcf932256b.png&quot; alt=&quot;image-20250415095544399&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;模型兼容性檢測&lt;/h3&gt; 
&lt;p&gt;在過往版本中，用户直接從 Hugging Face 或 ModelScope 搜索任意模型進行部署時，存在一定的失敗可能性。常見原因包括顯存不足、操作系統與推理引擎不兼容、模型架構不被支持、本地路徑配置錯誤等。這些問題不僅浪費時間，還嚴重影響用户體驗。&lt;/p&gt; 
&lt;p&gt;為瞭解決這一痛點，GPUStack 推出了&lt;strong&gt;模型兼容性檢測機制&lt;/strong&gt;。系統會在部署前自動檢測模型與運行環境的匹配情況，涵蓋模型架構與引擎支持、操作系統兼容性、GPU 資源是否充足、本地路徑是否有效等多個關鍵維度。通過這些檢測，潛在問題能夠被提前識別，並提供清晰提示，幫助用户避免不必要的部署失敗。&lt;/p&gt; 
&lt;p&gt;我們設定了三個明確的目標：第一，部署前提供清晰的兼容性提示；第二，在滿足條件的情況下將部署成功率提升至 99% 以上；第三，對於特殊需求場景，允許用户跳過檢測，強制部署，保留靈活性。&lt;/p&gt; 
&lt;p&gt;這項功能特性將持續演進，未來將支持更多檢測項、覆蓋更廣泛的系統環境，不斷完善檢測機制，全面助力用户在不同平台上實現穩定、高效的模型部署。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//fb036e198f11d8e36ec761c749ecca62.png&quot; alt=&quot;image-20250421173053252&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;模型下載管理&lt;/h3&gt; 
&lt;p&gt;在模型部署過程中，模型文件的統一管理與高效分發始終是用户關注的核心問題。以往，模型下載通常依賴於實例啓動時自動觸發，既需佔用 GPU 資源，又常常依賴額外的手動操作才能完成下載；同時，GPUStack 也無法管理用户預先下載到本地路徑的模型文件，導致部署效率低下，管理體驗不佳。&lt;/p&gt; 
&lt;p&gt;為此，GPUStack 引入了&lt;strong&gt;模型文件下載管理&lt;/strong&gt; 模塊：用户可在 UI 中為多個目標主機手動發起模型的下載任務，且&lt;strong&gt;無需佔用 GPU 資源&lt;/strong&gt;。各節點上已下載的模型文件也可在 UI 中統一可視化管理與部署，進一步提升了部署的靈活性與效率。&lt;/p&gt; 
&lt;p&gt;同時，GPUStack 還支持將本地已有的模型文件路徑添加到 UI 中進行統一管理，適配私有部署、離線環境等多種使用場景。通過這一模塊，既解決了用户獨立下載模型文件的需求，也使 GPUStack 能夠更好地支持多機分佈式部署，提升了部署效率與多機協同能力。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//0f873b0ef5ec73c42ae118694f3825ee.png&quot; alt=&quot;image-20250415204131503&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;模型故障自動恢復&lt;/h3&gt; 
&lt;p&gt;在追求高可用性和穩定性的生產環境中，模型推理服務的穩定性至關重要。為了進一步提升這一點，GPUStack 引入了&lt;strong&gt;模型故障自動恢復機制&lt;/strong&gt;！當模型發生故障時，GPUStack 會自動觸發恢復機制，迅速嘗試重新啓動模型，確保服務不中斷。&lt;/p&gt; 
&lt;p&gt;同時，為了避免過於頻繁的無效重啓，GPUStack 採用了&lt;strong&gt;5 分鐘為上限的指數退避延遲機制&lt;/strong&gt;，在故障持續時逐步延遲重啓，避免系統資源的浪費。總體而言，v0.6 版本提供的模型故障自動恢復機制大幅提升了模型服務的容錯能力，讓生產的模型推理更加穩健！&lt;/p&gt; 
&lt;h3&gt;端口暴露優化&lt;/h3&gt; 
&lt;p&gt;在舊版本架構中，每台 Worker 節點需為每個模型實例開放端口訪問，以供 Server 端進行推理請求的轉發。在用户大規模使用時暴露了一些問題：由於大量端口需要映射，容器啓停緩慢，且在啓動時容易發生端口衝突；防火牆配置容易遺漏，導致推理請求轉發異常。此外，也不支持用户自定義端口範圍。&lt;/p&gt; 
&lt;p&gt;為此，我們在 v0.6 版本中重構了端口暴露機制：推理請求從 API 入口到模型實例的鏈路現已通過統一的代理轉發，無需再為每個模型實例開放端口訪問。同時優化了端口分配，將端口暴露範圍壓縮超過 96%，顯著降低部署複雜度和運維風險。同時也支持用户自定義端口配置，使系統能夠靈活適配不同的網絡環境與安全策略，為用户帶來更簡單、穩定的部署體驗。&lt;/p&gt; 
&lt;h3&gt;增強國際化支持&lt;/h3&gt; 
&lt;p&gt;目前 GPUStack 的用户遍佈全球上百個國家和地區，隨着 GPUStack 用户羣體在全球範圍內的持續擴大，我們致力於為不同語言背景的開發者提供一致、便捷的使用體驗。本次 GPUStack 社區用户貢獻了&lt;strong&gt;俄語&lt;/strong&gt; 和&lt;strong&gt;日語&lt;/strong&gt;支持，標誌着 GPUStack 在國際化進程中的又一重要里程碑。&lt;/p&gt; 
&lt;p&gt;通過持續拓展多語言能力，GPUStack 為全球社區用户創造了更加包容與高效的使用體驗。未來，我們將繼續深化本地化支持，為全球用户提供更全面、更優質的服務體驗，加速推動 AI 應用的全球落地與普及。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//8781f64407b5d62f1496d53824097f69.png&quot; alt=&quot;image-20250413233340395&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//8e1e428e259bcf52136144df288263a0.png&quot; alt=&quot;image-20250413233303590&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;全方位的 UI / UX 優化&lt;/h3&gt; 
&lt;p&gt;在本次版本中，我們對 UI / UX 進行了全方位優化，從信息展示到交互細節，幾乎每一處都經過精心打磨，力求帶來更流暢、更易用的使用體驗。過去幾個月收集的每一條用户建議，都是此次優化的重要參考。&lt;/p&gt; 
&lt;p&gt;我們始終堅持一個目標：打造業界最好用的模型推理平台，而 GPUStack 正在持續朝這一目標穩步前進。也正因為有用户的積極反饋，我們才能不斷迭代優化------如果你有任何建議或想法，歡迎隨時向我們提出，我們會認真評估並持續改進。&lt;/p&gt; 
&lt;h2&gt;參與開源&lt;/h2&gt; 
&lt;p&gt;想要了解更多關於 GPUStack 的信息，可以訪問我們的倉庫地址：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fgpustack%2Fgpustack&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;https://github.com/gpustack/gpustack&lt;/strong&gt;&lt;/a&gt;。如果你對 GPUStack 有任何建議，歡迎&lt;strong&gt;提交 GitHub issue&lt;/strong&gt; 。在體驗 &lt;strong&gt;GPUStack&lt;/strong&gt; 或提交 issue 之前，請在我們的 GitHub 倉庫上&lt;strong&gt;點亮 Star&lt;/strong&gt; ⭐️關注我們，也非常歡迎大家一起參與到這個開源項目中！&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;如果覺得對你有幫助，歡迎&lt;strong&gt;點贊&lt;/strong&gt; 、&lt;strong&gt;轉發&lt;/strong&gt; 、&lt;strong&gt;關注&lt;/strong&gt;。&lt;/p&gt; 
&lt;/blockquote&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/gpustack/blog/18260677</link>
            <guid isPermaLink="false">https://my.oschina.net/gpustack/blog/18260677</guid>
            <pubDate>Mon, 28 Apr 2025 03:43:00 GMT</pubDate>
            <author>原創</author>
        </item>
        <item>
            <title>得物業務參數配置中心架構綜述</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                                                                                                                                        &lt;h1&gt;一、背景&lt;/h1&gt; 
&lt;h3&gt;&lt;strong&gt;現狀與痛點&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;在目前互聯網飛速發展的今天，企業對用人的要求越來越高，尤其是後端的開發同學大部分精力都要投入在對複雜需求的處理，以及代碼架構，穩定性的工作中，在對比下，簡單且重複的 CRUD 就顯得更加浪費開發資源。目前 scm 供應鏈管理頁面中，存在約 77% 的標準頁面，這些標準頁面裏，還存在着很多類似的參數配置頁面，就是對某一個模型進行增、刪、改、查、導入、導出進行類似的操作，這種開發工作技術含量較低，而且相對耗費人力。&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;什麼是業務參數配置中心&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;參數配置中心，是一個能夠通過配置的方式，快速生成前端頁面以及配套增、刪、改、查、導入、導出服務的配置平台，它與得物內部低代碼前端頁面平台 wizard 相互集成，參數配置中心提供後台增刪改查服務，wizard 輸出對應的前端頁面代碼，並可以支持用户自定義修改。&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;使用場景&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;針對讀多寫少的簡單的單表的增刪改查；&lt;/li&gt; 
 &lt;li&gt;業務中需要交給運營來修改的複雜 ark 配置（簡單配置除外），可以嘗試使用業務參數配置中心接入，減少人為修改 JSON 可能產生的錯誤，導致系統無法編譯進而產生故障。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;比如如下的 JSON：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;[{&quot;position&quot;:&quot;1&quot;,&quot;red&quot;:2.49,&quot;blue&quot;:2.4,&quot;green&quot;:1},

{&quot;position&quot;:&quot;2&quot;,&quot;red&quot;:2.49,&quot;blue&quot;:2.4,&quot;green&quot;:1},

{&quot;position&quot;:&quot;3&quot;,&quot;red&quot;:2.49,&quot;blue&quot;:2.4,&quot;green&quot;:1},

{&quot;position&quot;:&quot;4&quot;,&quot;red&quot;:2.49,&quot;blue&quot;:2.4,&quot;green&quot;:1},

{&quot;position&quot;:&quot;5&quot;,&quot;red&quot;:2.49,&quot;blue&quot;:2.4,&quot;green&quot;:1},

{&quot;position&quot;:&quot;6&quot;,&quot;red&quot;:2.49,&quot;blue&quot;:2.4,&quot;green&quot;:1},

{&quot;position&quot;:&quot;7&quot;,&quot;red&quot;:2.49,&quot;blue&quot;:2.4,&quot;green&quot;:1},

{&quot;position&quot;:&quot;8&quot;,&quot;red&quot;:2.49,&quot;blue&quot;:2.4,&quot;green&quot;:1}]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;&lt;strong&gt;業務參數配置中心極速體驗&lt;/strong&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;後台服務搭建流程，以及數據錄入&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;數據讀取可以通過參數配置中心的 SDK，輸入自己的業務入參以及自己的業務出參，SDK 會自動根據方案下的參數以及用户的輸入條件，查詢出對應的參數信息：&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-95529e5439a558b65e0757f2e1313155752.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;從上面的快速體驗裏可以看到很多名詞，你一定有會有下面的疑問：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-964c6ee7e450e7cce90376eddb00b05c04f.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h1&gt;二、整體架構與原理&lt;/h1&gt; 
&lt;h3&gt;&lt;strong&gt;實現思路&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;首先我們對這種普通的頁面進行初步剖析：頁面中總體包含搜索條件、靜態展示字段以及操作欄，搜索條件一般是靜態字段的子集，並且操作欄的功能一般都類似，所以為了能夠結構化地構造出這樣的頁面，我們可以將靜態展示字段進行進一步抽象：比如元素、維度、參數、方案、參數實例。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-1adfd37a09e81f9bf0438a2bed718c19a80.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;元素&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;構成頁面的每一個業務字段，統稱元素，因為有些字段是大家常用的（比如倉庫，品牌，一級類目，省份等），它有自己的字段名稱，以及取值範圍。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;維度&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;一條記錄一定有能夠標註其唯一性的信息，可能是一個字段或者是多個字段，在參數中心裏，能確定一條記錄唯一性的所有字段就叫做維度，維度這個概念在參數中心裏很重要，它是不可變的。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;參數&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在業務發展過程裏，可以改變值的字段，就叫參數，也可以説一條記錄裏，除了維度，都可以叫做參數。&lt;/p&gt; 
&lt;p&gt;綜合維度和參數，舉個例子，比如商品信息，商品 ID 就是維度，商品售價、折扣率就是參數。或者醫院掛號系統，科室 ID 就是維度，掛號費，出診時間就是參數。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;方案&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;一個參數方案它管理着一個場景下的業務配置，可以簡單理解一個方案就代表着一個頁面，包含了上述我們説的維度以及參數，並且指定了可以指定哪些字段為搜索條件，哪些是必填字段，哪些字段可以多選。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;參數實例&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;描述好方案並生成頁面後，實際產生的業務配置數據，我們稱之為參數實例。&lt;/p&gt; 
&lt;p&gt;經過剛才對頁面元素的解剖，大家會發現搭建一個這樣的頁面，猶如建房子一樣，維度與參數是最基礎的木料，創建方案就是設計建造的過程，參數實例就是一個個真實的房間，所以業務參數配置中心整體產品思路如下：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-54cf2fc71dbf64169277bd863672f638b30.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;整體架構&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;通過上文的介紹，我們介紹了業務參數配置中心最核心的概念，接下來我們看看整體的架構設計。我們針對這些最核心的概念，來設計實現這些業務功能的架構、核心包含領域模型、領域服務、應用服務以及基礎設施層需要的存儲部件，以及外部可以整合的導入導出框架、日誌框架（外部依賴的框架也可以自己實現）、核心的元素維護、方案維護，存儲設計好之後，我們就需要一個 SDK，可以讓用户訪問到我們的數據。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-f29f79f77a54e147851172489afd9765082.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;系統的實體關係圖如下：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-4876084e664581f0f46677ab3e1b89d59d7.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;通過上文我們可以初步瞭解到整體的架構設計，那麼每一個子模塊我們如何實現？接下來我們分析更加細節的原理。&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;核心原理&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;如何設計存儲的細節是這個系統的一大挑戰，因為既要兼顧頁面的靈活變動，也要兼顧數據整體的一致性不受影響，同時也要兼顧整體數據的查詢性能，下面的小節列出了所有這些核心的挑戰點。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;存儲流程&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;每一個頁面的字段都不一樣，我們是怎麼存儲的？&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-072aa230f23621ba48c937b8f7db995c249.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;從上面的兩個頁面可以看到，因為頁面的字段變化多端，所以我們的思考是，必須採用抽象存儲的方式來應對，核心用一張，大寬表存儲，其中包含很多抽象列，每一個抽象列在不同的方案下，業務含義不同。&lt;/p&gt; 
&lt;p&gt;同時把方案的元數據：維度、參數、以及功能性設置（如每個字段是否可以刪除，是否需要多選）單獨存儲，每個方案下的大寬表裏的抽象列的業務含義，就存儲在這些元數據表中。&lt;/p&gt; 
&lt;p&gt;同時為了應對大批量的查詢，我們引入了 OLAP 的數據庫，對於在應用內部的單點查詢，我們走 MySQL 實現，如果運營後台針對某個字段做大批量查詢，則可以用 OLAP 數據庫來緩解查詢壓力。&lt;/p&gt; 
&lt;p&gt;下面是存儲的整個過程以及舉例：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-68a28fece8de3ef9c82fc7843e8110e2a2e.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;SDK 查詢流程&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;因為在業務參數使用時，各個業務方有自己的業務對象，所以我們在 SDK 中集成了反射的能力，可以避免用户直接感知到底層的抽象存儲，查詢的流程使用上比較簡單，一共分為三步，第一步為自定義 request，第二步自定義 response，第三步調用 SDK 方法獲取參數實例，比如：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;定義 request：&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://my.oschina.net/difrik&quot;&gt;@Data&lt;/a&gt;&lt;/p&gt; &lt;p&gt;public class PinkDeviceCameraConfigRequest implements Serializable {&lt;/p&gt; &lt;pre&gt;&lt;code&gt; */***

  * 配置類型

  */

 private String configType;

 */***

  * 設備編號

  */

 private String deviceNo;
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;}&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;定義 response&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://my.oschina.net/difrik&quot;&gt;@Data&lt;/a&gt;&lt;/p&gt; &lt;p&gt;public class PinkDeviceCameraConfigResponse implements Serializable {&lt;/p&gt; &lt;pre&gt;&lt;code&gt; */***

  * 配置類型

  */

 private String configType;

 */***

  * 設備編號

  */

 private String deviceNo;



     */***

  * 配置明細

  */

 private List&amp;lt;CameraConfigDto&amp;gt; configValueList;



     [@Data](https://my.oschina.net/difrik)

 public static class CameraConfigDto implements Serializable {

     private String position;

     */***

      * 白平衡 (Red)

      */

     private BigDecimal red;

     */***

      * 白平衡 (Blue)

      */

     private BigDecimal blue;

     */***

      * 白平衡 (Green)

      */

     private BigDecimal green;

     */***

      * 亮度 (Brightness)

      */

     private BigDecimal brightness;

     */***

      * 自動曝光時間上限 (us)

      */

     private BigDecimal autoExposureTimeUpperLimit;

     */***

      * 採集幀率

      */

     private BigDecimal acquisitionFrameRate;

     */***

      * 增益自動開關 (us)

      */

     private String gainAuto;

     */***

      * 增益自動上限

      */

     private BigDecimal gainAutoUpperLimit;

     */***

      * 增益自動上限

      */

     private BigDecimal gainAutoLowerLimit;

 }
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;}&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;調用 SDK 的服務方法查詢&lt;/p&gt; &lt;p&gt;PinkDeviceCameraConfigRequest pinkDeviceCameraConfigRequest = new PinkDeviceCameraConfigRequest();&lt;/p&gt; &lt;p&gt;pinkDeviceCameraConfigRequest.setConfigType(&quot;DEVICE_NO&quot;);&lt;/p&gt; &lt;p&gt;pinkDeviceCameraConfigRequest.setDeviceNo(&quot;123@LuSun&quot;);&lt;/p&gt; &lt;p&gt;&lt;em&gt;//&lt;/em&gt; 單個查詢場景&lt;/p&gt; &lt;p&gt;PinkDeviceCameraConfigResponse response =&lt;/p&gt; &lt;pre&gt;&lt;code&gt; paramInstQueryService.getParams(&quot;P80-DEVICE-CAMERA-PARAM-MANAGER&quot;,

      pinkDeviceCameraConfigRequest,

      PinkDeviceCameraConfigResponse.class);
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;//&lt;/em&gt; 批量查詢場景&lt;/p&gt; &lt;p&gt;PageQueryOption pageQueryOption = new PageQueryOption();&lt;/p&gt; &lt;p&gt;pageQueryOption.setPageIndex(1);&lt;/p&gt; &lt;p&gt;pageQueryOption.setPageSize(200);&lt;/p&gt; &lt;p&gt;PageInfo&amp;lt;PinkDeviceCameraConfigResponse&amp;gt; paramsPage =&lt;/p&gt; &lt;pre&gt;&lt;code&gt; paramInstQueryService.getParamsPage(&quot;P80-DEVICE-CAMERA-PARAM-MANAGER&quot;, 

     pinkDeviceCameraConfigRequest, 

     PinkDeviceCameraConfigResponse.class,

     pageQueryOption);
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;獲得結果&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-19953d23e8dcd9b90b57575b8fc6c5533ab.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;整體查詢實現原理如下：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-68822ebba4f5d690e41c18c156dffdad174.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 目前整個服務的性能在 10+ms 左右：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-f86b118721761e37edb0d80428c83fa7355.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;參數優先級實現&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;為什麼會有參數優先級這個功能？&lt;/p&gt; 
&lt;p&gt;比如有一個場景，要維護一個供應鏈系統中的補貨參數：安全庫存，低於這個安全庫存的時候，要通知商家進行補貨，整個供應鏈裏有 100 個倉庫，20 個一級類目，200 個二級類目，2000 個三級類目，涉及到 500 個品牌，要維護每一個商品的安全庫存，你會怎麼實現？&lt;/p&gt; 
&lt;p&gt;你一定不會把 100 倉庫_2000 類目_500 品牌 = 1000000000 種可能全都設置一遍參數，對你來説，重點類目，要單獨詳細配置安全庫存，非重點類目可能只需要管控到一級或者二級類目即可，這樣你所需要的配置會大大減少。那麼參數的決策就需要遵循一定的規則，比如:&lt;/p&gt; 
&lt;p&gt;有倉庫+一級類目+二級類目+三級類目，的安全庫存，優先取；&lt;/p&gt; 
&lt;p&gt;如果取不到，則取倉庫+一級類目+二級類目的安全庫存；&lt;/p&gt; 
&lt;p&gt;再取不到，取倉庫+一級類目的安全庫存。&lt;/p&gt; 
&lt;p&gt;比如：&lt;/p&gt; 
&lt;p&gt;DN 倉，鞋 安全庫存 100&lt;/p&gt; 
&lt;p&gt;DN 倉，鞋-運動鞋，安全庫存 500&lt;/p&gt; 
&lt;p&gt;DN 倉，鞋-運動鞋-籃球鞋，安全庫存 1000&lt;/p&gt; 
&lt;p&gt;那如果一個商品是籃球鞋的話，則會命中安全庫存 1000 的規則，如果是登山鞋的話，只能命中運動鞋的規則取 500，如果是高跟鞋，則只能取 100 的安全庫存。&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;（事實上這種補貨規則要詳細的多，這裏只是方便大家理解需求，並不是真正的參數）&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;也就是説，當用户的入參同時可能命中多條參數的時候，需要通過優先級來判斷應該返回哪個參數。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-92bbffb30b7b6f12d6b8fa4717437f376d8.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;為了加速查詢，系統在設計時添加了兩層緩存：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-129d351a3be37ca0cf02b4472de6fcd50e8.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 當後台數據發生變化時，會將對應的緩存進行失效。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-821ca41cc55e1456a0ad3fb0ba5dc76247f.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;元素多選處理&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;維度多選場景：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-28b192d9ccc63092c15fcc0f12187ddf449.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;參數多選場景：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-e4a1bdf71195372e13f56ab0aa27813e3ce.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;既要保證維度唯一，又要保證能正常搜索，以及展示，如何實現？業務參數配置中心引入了一個&quot;組&quot;的概念，是將同屬於一行的參數實例，歸為一個組，這個組是最小的新建、編輯單位。&lt;/p&gt; 
&lt;p&gt;對於新增流程如下圖所示：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-fd67e7858a3927f9960d4c2d364c0dc4a6a.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;對於修改流程，如下圖所示： &lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-ed29af351bef248291bc76b841bafcedcae.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;元素範圍查詢&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;頁面中的字段，我們統稱為元素，只要是字段，一定有它的取值範圍，我們平衡了用户使用成本以及系統性能，將字段取值類型劃分成了四種：&lt;/p&gt; 
&lt;p&gt;1）枚舉類元素&lt;/p&gt; 
&lt;p&gt;2）dubbo 全量接口元素&lt;/p&gt; 
&lt;p&gt;3）dubbo 單點查詢接口元素&lt;/p&gt; 
&lt;p&gt;4）自定義文本元素&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;枚舉元素由用户手動在頁面創建，一般幾十個以內為佳，創建成本不高，比如經常用到的 &quot;是&quot;，&quot;否&quot;，或者比如單據類型等等。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;dubbo 全量接口元素，一般是幾十到上百個的體量，比如一級類目，倉庫等，地址。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;dubbo 單點查詢接口，一般是幾千到幾萬體量的取值範圍，無法直接在內存裏存儲所有枚舉，比如品牌等。只能通過兩個接口來完成搜索以及數據的展示，比如&quot;品牌 ID &amp;gt;品牌名稱&quot;接口，和 &quot;品牌名稱-&amp;gt;品牌 ID&quot; 接口。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;自定義文本，非枚舉類字段，可以選擇使用自定義文本來承接。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;比如以下是可以通過 dubbo 接口全量獲取配置的元素：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-265ea951a69d044a0e77e39825e029716fc.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;與 dubbo 全量接口的錄入類似，單點搜索接口與全量接口不同的點在於，單點接口需要保留一個變量，給系統查詢時調用，比如&quot;通過品牌 ID 查詢品牌名稱&quot; 和 &quot;通過品牌名稱查詢品牌 ID&quot; ，需要留給系統調用的入參，用#{var}代替。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-154171e3bd51e9ec195984b8fc5f2408f3a.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;當然，有時元素的範圍並不是隻取決於它自己，可能也取決於同頁面裏其他元素的取值，比如説有一個質量原因的字段，當一級類目為鞋時，取值為 A、B、C，為服裝時為 D、E、F，這是元素範圍在設置時，就需要將對應的元素入參維護到其中，比如：&lt;/p&gt; 
&lt;p&gt;| 接口入參類型 | 接口入參取值 | | --- | --- | | com.d.s.q.s.d.r.ConfigRequest | {&quot;ruleVersion&quot;:#{ruleVersion},&quot;spuId&quot;:#{spuId}} |&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;導入導出&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;以下是導入處理流程：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-72582bd979727a7d8718ed80cfe71b8973e.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;為了照顧使用人員的體驗，再多數導入場景時，我們的導入文件都用的是文案，而不是後台存儲的數值，比如導入的字段包含類目時，導入文件輸入的是鞋、服裝、美妝等文案，而不是 2、3、4 這樣存儲在後台的數值，那麼勢必這裏就會有將文案轉換成數值的過程，這其中就用到了 2.3.5 章節中提到的元素範圍查詢使用的接口，當然，對於需要其他元素作為入參的元素，我們默認每個元素左邊的元素都可以作為當前元素的入參。&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;業務參數配置中心不適合做什麼？&lt;/strong&gt;&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;有極為複雜的 UI 交互&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;較為複雜的校驗邏輯（長期計劃支持）&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;高頻寫入場景&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;應用查詢參數時以非&quot;=&quot;條件匹配&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h1&gt;三、總結與展望&lt;/h1&gt; 
&lt;p&gt;本文簡要描述了業務參數配置中心的設計思路，參數配置中心配套生成增、刪、改、查、導入、導出服務，並且結合前端低代碼平台自動生成前端代碼，平台目前業務參數中心已經有 40+個場景接入節省了大量的工作人日，能夠讓研發人員，擺脱低效的 CRUD，更專注於自己內部業務邏輯的開發。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;對於目前系統的未來規劃：&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;持續增加 SDK 的查詢靈活性：包括不限於批量代參數優先級對數據進行查詢、通過 SDK 分頁查詢全量參數、對系統字段吐出方便業務方使用；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;持續增加對方案定義的靈活性：支持更多的元素範圍的定義，比如 HTTP 等調用方式；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;持續增加對元數據定義的靈活性：部分元數據的取值可能需要同頁面中的另一個元素的取值來決定，所以在取值渲染時，可以保留給其他元素的佔位符，進而隨着頁面的動態變動，後台取值也可以動態變動。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;&lt;strong&gt;往期回顧&lt;/strong&gt;&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzkxNTE3ODU0NA%3D%3D%26mid%3D2247539092%26idx%3D1%26sn%3D6fc02ccebc5c838f143d5128691a635b%26scene%3D21%23wechat_redirect&quot; target=&quot;_blank&quot;&gt;得物增長兑換商城的構架演進&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzkxNTE3ODU0NA%3D%3D%26mid%3D2247539014%26idx%3D1%26sn%3D90a168b730490ae84a0917863ad3e077%26scene%3D21%23wechat_redirect&quot; target=&quot;_blank&quot;&gt;得物自研 DGraph4.0 推薦核心引擎升級之路&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzkxNTE3ODU0NA%3D%3D%26mid%3D2247538986%26idx%3D1%26sn%3Db6b82a790a3c696bce27704472e799b2%26scene%3D21%23wechat_redirect&quot; target=&quot;_blank&quot;&gt;大語言模型的訓練後量化算法綜述 | 得物技術&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzkxNTE3ODU0NA%3D%3D%26mid%3D2247538473%26idx%3D1%26sn%3D0a83895ef8dcd555e9926151a989b663%26scene%3D21%23wechat_redirect&quot; target=&quot;_blank&quot;&gt;如何合理規劃 Elasticsearch 的索引｜得物技術&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzkxNTE3ODU0NA%3D%3D%26mid%3D2247538394%26idx%3D1%26sn%3D51f91adc969a03f7c8baa31f6cc39c67%26scene%3D21%23wechat_redirect&quot; target=&quot;_blank&quot;&gt;DPP 推薦引擎架構升級演進之路｜得物技術&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;文 / sakuta&lt;/h4&gt; 
&lt;p&gt;關注得物技術，每週新技術乾貨&lt;/p&gt; 
&lt;p&gt;要是覺得文章對你有幫助的話，歡迎評論轉發點贊～&lt;/p&gt; 
&lt;p&gt;未經得物技術許可嚴禁轉載，否則依法追究法律責任。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/u/5783135/blog/18230829</link>
            <guid isPermaLink="false">https://my.oschina.net/u/5783135/blog/18230829</guid>
            <pubDate>Mon, 28 Apr 2025 03:24:00 GMT</pubDate>
            <author>原創</author>
        </item>
        <item>
            <title>Anthropic 向逆向工程 Claude Code 的開發者發送刪除通知</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftechcrunch.com%2F2025%2F04%2F25%2Fanthropic-sent-a-takedown-notice-to-a-dev-trying-to-reverse-engineer-its-coding-tool%2F&quot; target=&quot;_blank&quot;&gt;TechCrunch 報道稱&lt;/a&gt;&lt;/u&gt;，在 Anthropic 的 Claude Code 和 OpenAI 的 Codex CLI 兩款「智能體」式 AI 編程工具的較量中，後者獲得了更多開發者的青睞。部分原因在於，Anthropic 向一位試圖逆向工程 Claude Code 的開發者發出了刪除通知，而 Claude Code 的使用許可要比 Codex CLI 更加嚴格。&lt;/p&gt; 
&lt;p&gt;Claude Code 和 Codex CLI 都是讓開發者能夠利用雲端的 AI 模型來完成各種編程任務的工具，功能相似。兩家公司幾乎在同一時期發佈了這兩款工具，爭奪開發者的關注。&lt;/p&gt; 
&lt;p&gt;Codex CLI 的源代碼採用 Apache 2.0 許可證，允許分發和商業使用。相比之下，Claude Code 則依賴於 Anthropic 的商業許可證，限制了「在未獲得公司明確許可的情況下對其進行修改」的方式。&lt;/p&gt; 
&lt;p&gt;另外，Anthropic 對 Claude Code 的源代碼進行了「混淆」，意味着其源代碼並不容易獲得。當有開發者通過反混淆手段將代碼發佈到 GitHub&amp;nbsp;時，Anthropic &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fgithub%2Fdmca%2Fblob%2Fmaster%2F2025%2F03%2F2025-03-10-anthropic.md&quot; target=&quot;_blank&quot;&gt;提出了 DMCA 投訴&lt;/a&gt; ——&amp;nbsp;這是一份要求刪除代碼的版權通知。&lt;/p&gt; 
&lt;p&gt;社交媒體上的開發者們對 Anthropic 此舉&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2FtheLance%2Fstatus%2F1914458771679486389&quot; target=&quot;_blank&quot;&gt;非常不滿意&lt;/a&gt;，認為這種做法遠不如 OpenAI 發佈 Codex CLI 時的開放態度。在 Codex CLI 發佈後的短短一週內，OpenAI 就將幾十條開發者建議納入了工具的代碼庫，其中包括一個讓 Codex CLI 能調用來自其他競爭者（包括 Anthropic）的 AI 模型的功能。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;2820&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0428/105716_o6ne_2720166.png&quot; width=&quot;1289&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Anthropic 尚未對此事作出回應。Claude Code 仍處於測試階段，並且存在一些 bug。而在未來，Anthropic 有望以寬鬆的許可證發佈源代碼。公司對源代碼進行混淆的原因多種多樣，其中之一便是出於「安全」考慮。&lt;/p&gt; 
&lt;p&gt;對於 OpenAI 來説，這多少是一次公關上的勝利，因為最近幾個月，OpenAI 一直迴避開源發佈，轉而推出專有、封閉的產品。這可能標誌着實驗室方法的一個更廣泛的轉變；OpenAI 首席執行官 Sam Altman 今年早些時候表示，他認為公司在開源問題上一直站在「歷史錯誤的一邊」。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/347063/anthropic-sent-a-takedown-notice-to-a-dev</link>
            <guid isPermaLink="false">https://www.oschina.net/news/347063/anthropic-sent-a-takedown-notice-to-a-dev</guid>
            <pubDate>Mon, 28 Apr 2025 02:58:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>Transformers 作者：未來互聯網將演變為 AI Agent 網絡</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;日前，Transformers 合著者 Illia Polosukhin 接受了 a16z 的專題採訪，並在交流中分享了自己對於 AI、Agent 等方面的觀點。&lt;/p&gt; 
&lt;p&gt;開篇，Illia 就分享了自己對現有 AI Agent 的看法。他表示，據團隊觀察，大量用户對需要複雜規劃的場景特別感興趣。但這種局面在未來將會反過來：AI 助理將會主動提出方案給用户，用户也僅需要做出方向性選擇即可。對於這種 AI 何時面世，Illia 預測在未來一年內，就會出現首批成熟應用的場景。&lt;/p&gt; 
&lt;p&gt;對於「死亡互聯網理論」，Illia 則坦言：雖然開放網絡正在消亡，但並非網絡上的機器人數量過多，而是因為平台容易被垃圾信息攻陷。對此他認為智能 Agent 能夠為人類進行信息把關，未來 AI 助手也會成為互聯網「垃圾分揀員」：能夠為用户提供上下文鏈接，如實指出錯誤信息並揭露事實真相。&lt;/p&gt; 
&lt;p&gt;另外，主持人問及「未來將會有多少 AI Agent？與人類的數量比例又是如何？」時，Illia 則表示，未來每個人都會擁有屬於自己的 AI 助手，而 AI 助手的背後可能運行着數十個子 Agent 項目，因此這會構建起一個龐大的 Agent 網絡，並且每個人都將如同獲得一套「按需助理系統」。&lt;/p&gt; 
&lt;p&gt;主持人還特別向&amp;nbsp;Transformers 作者問起了對 DeepSeek 的看法：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Robert:&lt;/p&gt; 
 &lt;p&gt;您如何看待 DeepSeek 最新發布的高性能開源模型？相比其他選項，它不僅表現優異且成本更低，更特別的是由中國對沖基金以開源方式推出。&lt;/p&gt; 
 &lt;p&gt;Illia:&lt;/p&gt; 
 &lt;p&gt;首先這確實是激動人心的突破。他們在有限硬件上實現大規模高性能模型訓練的工程能力令人驚豔，證明優秀工程實踐能大幅降低成本。中國模型訓練成本正在快速下降，但最關鍵的創新在於：他們提出了一種極其簡單的強化學習方法——這個方法具有普適性，無論是 10 億還是 70 億參數模型都能快速獲得優異效果。&lt;/p&gt; 
 &lt;p&gt;這種「階躍式創新」讓我想起 Transformer 的誕生——原理簡單、開箱即用、人人可復現。&lt;/p&gt; 
 &lt;p&gt;坦白講，這類基礎方法論本應自由傳播 (畢竟只是公式或原理)，但必須承認 DeepSeek 團隊極其專業，他們憑藉後發優勢規避了許多早期問題。現在更重要的機遇在於：藉助可驗證計算技術，我們可以訓練用户或社區擁有的模型——確切知道訓練數據來源。&lt;/p&gt; 
 &lt;p&gt;當前所有開源模型都只公開參數，無人知曉訓練數據構成，即便公佈也無法驗證真偽。&lt;/p&gt; 
 &lt;p&gt;區塊鏈領域現在有機會聯合訓練一個「加密透明」的開源模型：所有人都能驗證數據輸入、訓練過程及潛在偏差，確保沒有隱藏後門或惡意代碼。這樣的模型才能真正成為 AI 時代可信賴的基礎設施。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;原文：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FhDgE_7fIb-ps4xSOuced_A&quot; target=&quot;_blank&quot;&gt;https://mp.weixin.qq.com/s/hDgE_7fIb-ps4xSOuced_A&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/347060</link>
            <guid isPermaLink="false">https://www.oschina.net/news/347060</guid>
            <pubDate>Mon, 28 Apr 2025 02:31:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>馬斯克旗下 xAI 擬融資 200 億美元</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.bloomberg.com%2Fnews%2Farticles%2F2025-04-26%2Felon-musk-s-xai-holdings-is-in-discussions-to-raise-20-billion&quot; target=&quot;_blank&quot;&gt;彭博社援引知情人士透露&lt;/a&gt;&lt;/u&gt;，馬斯克旗下 xAI 目前正與投資者洽談，計劃籌集大約 200 億美元資金，用於其新合併的人工智能初創公司和社交媒體業務。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0428/102209_yrAz_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;數據提供商 PitchBook 的數據顯示，&lt;strong&gt;如果成功，這筆交易將成為歷史上第二大創業公司融資&lt;/strong&gt;，僅次於今年早些時候 OpenAI 的 400 億美元融資。據知情人士透露，憑藉此輪洽談中的融資，xAI 的估值超過 1200 億美元。&lt;/p&gt; 
&lt;p&gt;值得一提的是，該輪融資可能有助於償還馬斯克在將 X 前身 ——Twitter 私有化後所承擔的一部分債務。知情人士透露，上述債務一直對 X 構成財務壓力。此前彭博社報道指出，僅在今年 3 月，X 就支付了約 2 億美元的債務服務費用，截止 2024 年底，其年度利息支出將超過 13 億美元。&lt;/p&gt; 
&lt;p&gt;據瞭解，儘管談判仍處於初期階段，但 xAI 目標是未來幾個月內籌集資金。知情人士表示，融資規模可能會超過最初的 200 億美元，具體金額和條款尚未確定。&lt;/p&gt; 
&lt;p&gt;報道指出，這一大規模融資凸顯了投資者對人工智能公司日益增長的興趣，同時也顯示了馬斯克作為商業巨頭和政治影響力人物的地位。儘管特斯拉的市值有所下滑，但馬斯克的其他企業仍在蓬勃發展，例如馬斯克的火箭公司 SpaceX，於去年一次私募交易中被估值為 3500 億美元，成為歷史上最有價值的初創公司。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/347056/xai-holdings-is-in-discussions-to-raise-20-billion</link>
            <guid isPermaLink="false">https://www.oschina.net/news/347056/xai-holdings-is-in-discussions-to-raise-20-billion</guid>
            <pubDate>Mon, 28 Apr 2025 02:22:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>字節跳動推出 QuaDMix：大型語言模型預訓練數據質量與多樣性的統一框架</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;字節跳動近日宣佈推出其全新的數據選擇框架 QuaDMix，旨在提升大型語言模型（LLM）預訓練的效率和泛化能力。眾所周知，模型的訓練效果受基礎數據集的質量和多樣性影響很大。然而，傳統的數據篩選方法往往將質量和多樣性視為兩個獨立的目標，先進行質量過濾，再進行領域平衡。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;320&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-3b5dc2134907308fa5f27fb6e2823c7d4cf.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#242424; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;這種逐步優化的方式忽略了質量與多樣性之間的複雜相互關係。優質數據集往往存在領域偏差，而多樣化的數據集可能會降低質量。因此，在固定的訓練預算下，如何同時優化這兩個維度以最大化模型性能，成為了一個亟待解決的難題。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#242424; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;QuaDMix 框架的主要運作分為三個階段：特徵提取、質量聚合和質量 - 多樣性感知採樣。在初始階段，每個文檔都會被標註領域標籤和多項質量評分。通過歸一化和合並這些評分，生成一個綜合質量分數。接着，系統通過基於 sigmoid 的函數採樣文檔，優先考慮高質量樣本，並通過參數化控制確保領域平衡。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#242424; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;為了優化模型，QuaDMix 在不同參數設置下訓練了數千個代理模型。通過這些代理實驗訓練的迴歸模型可以預測性能結果，從而識別出最佳採樣配置。這種方法使得在高維參數空間中進行結構化探索成為可能，從而更好地將數據選擇與下游任務對接。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#242424; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;實驗結果顯示，QuaDMix 在 RefinedWeb 數據集上進行的驗證實驗中，與多種基線模型相比，平均得分達到了 39.5%。這些基線模型包括隨機選擇、Fineweb-edu、AskLLM、DCLM 等。實驗結果表明，聯合優化策略在整體表現上始終優於單獨關注質量或多樣性的方法。此外，經過優化的數據混合更能提升特定下游任務的性能。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#242424; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;QuaDMix 為大型語言模型的預訓練數據選擇提供了一個系統化的解決方案，解決了長期以來同時優化數據質量與多樣性的挑戰。通過結合質量聚合和領域感知採樣，QuaDMix 建立了一種可擴展的方法論，提升了 LLM 預訓練的效率。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/347054</link>
            <guid isPermaLink="false">https://www.oschina.net/news/347054</guid>
            <pubDate>Mon, 28 Apr 2025 02:11:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>月暗開源 Kimi-Audio，單一框架執行多種語音任務；照片秒變可對話數字人，LemonAI 推出 Slice Live 丨日報</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                                                                                                                                        &lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-42daa67a23199a5e38b5a98abb2517572ae.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;開發者朋友們大家好：&lt;/p&gt; 
&lt;p&gt;這裏是 &lt;strong&gt;「RTE 開發者日報」&lt;/strong&gt; ，每天和大家一起看新聞、聊八卦。我們的社區編輯團隊會整理分享 RTE（Real-Time Engagement） 領域內「有話題的 &lt;strong&gt;技術&lt;/strong&gt; 」、「有亮點的 &lt;strong&gt;產品&lt;/strong&gt; 」、「有思考的 &lt;strong&gt;文章&lt;/strong&gt; 」、「有態度的 &lt;strong&gt;觀點&lt;/strong&gt; 」、「有看點的 &lt;strong&gt;活動&lt;/strong&gt; 」，但內容僅代表編輯的個人觀點，歡迎大家留言、跟帖、討論。&lt;/p&gt; 
&lt;p&gt;本期編輯：@趙怡嶺、&lt;a href=&quot;https://my.oschina.net/u/862736&quot;&gt;@鮑勃&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;01.有話題的技術&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;1、百度推出文心 4.5 Turbo 和深度思考模型 X1 Turbo&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-041ced382bdd171c05e0c36d4df884fc4c9.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;4 月 25 日，在面向開發者的 Create 大會重磅推出兩款全新模型：文心 4.5 Turbo 和深度思考模型 X1 Turbo。&lt;/p&gt; 
&lt;p&gt;兩款模型主打多模態、強推理和低成本。百度旗下新搜索智能助手文小言也宣佈全面接入，免費向用户開放，即日起用户打開文小言 APP 即可使用。&lt;/p&gt; 
&lt;p&gt;文心大模型 4.5 Turbo 進一步強化了多模態能力。在多個基準測試集中，文心 4.5 Turbo 多模態能力已與 GPT-4.1 持平，甚至在部分維度優於 GPT-4o。&lt;/p&gt; 
&lt;p&gt;而文心大模型 X1 Turbo 則在 4.5 Turbo 的基礎上進行了「深度思考」升級。無論是問答能力、內容創作、邏輯推理，還是工具調用、多模態處理，X1 Turbo 均實現全方位增強，整體表現領先於 DeepSeek R1 和最新版本 V3。(@APPSO)&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2、GPT-4o 模型再次升級&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-768e3d496ec29c08bc8237c168b4e93a021.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;4 月 5 日，OpenAI 稱對 GPT 4o 模型進行了升級。&lt;/p&gt; 
&lt;p&gt;OpenAI CEO Sam Altman 發文宣佈 GPT-4o 迎來能力改進，具體如下：&lt;/p&gt; 
&lt;p&gt;新升級的 GPT 4o 模型個性化更強，優化了模型保存「記憶」的時機，並增強其在 STEM 領域的問題解決能力，還對其響應方式進行了細微的調整，使其更加主動，能夠更好地引導對話走向富有成效的結果，同時對回覆的細節進行了微調，讓 GPT-4o 在各種任務中的表現更直觀、更易用，（&lt;a href=&quot;https://my.oschina.net/u/104417&quot;&gt;@ai&lt;/a&gt; 寒武紀、APPSO）&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;3、總體性能第一：月之暗面開源全新音頻基礎模型 Kimi-Audio，橫掃十多項基準測試&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-a5ea5773cd4c3850cedde7d0db1f67f2923.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;4 月 26 日，Kimi 發佈了新的開源項目 ------ 一個全新的通用音頻基礎模型 Kimi-Audio，支持語音識別、音頻理解、音頻轉文本、語音對話等多種任務，在十多個音頻基準測試中實現了最先進的 （SOTA） 性能。結果顯示，Kimi-Audio 總體性能排名第一，幾乎沒有明顯短板。&lt;/p&gt; 
&lt;p&gt;Kimi-Audio 採用了集成式架構設計，包括三個核心組件 ------ 音頻分詞器（Audio Tokenizer）、音頻大模型（Audio LLM）、音頻去分詞器（Audio Detokenizer）。&lt;/p&gt; 
&lt;p&gt;這一架構使 Kimi-Audio 能夠在單一模型框架下，流暢地處理從語音識別、理解到語音對話等多種音頻語言任務。同時，音頻分詞器還提取連續的聲學向量，以增強感知能力。（@機器之心）&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;4、Cognition Labs 推出 DeepWiki 項目，可為 GitHub 倉庫提供 AI 驅動的實時交互式文檔&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-52eb7173b2ff5edf181b86039e133aa6f11.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;（圖片來源：deepwiki 官網）&lt;/p&gt; 
&lt;p&gt;對於開源項目，這項服務完全免費，甚至無需註冊。訪問 deepwiki.com，探索已經收錄的熱門開源項目的 Wiki，或者把正在瀏覽的任何 GitHub 倉庫 URL 中的 github.com 替換成 deepwiki.com，即可無縫跳轉到該倉庫的 DeepWiki 頁面。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;對話式文檔： 直接向代碼庫「提問」，DeepWiki 會嘗試理解問題並給出文檔級的解答&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;深度研究 （Deep Research）： 對於複雜問題，可以開啓此功能，讓 AI Agent 進行更深入的分析和回答&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;按需索引： 如果關注的公開倉庫還沒被收錄，可以請求 DeepWiki 索引&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;私有倉庫支持： 對於私有倉庫，可以通過註冊 Devin 賬户（devin.ai）來獲得服務&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;輕鬆分享： 生成的 Wiki 頁面和問答結果都可以通過鏈接分享，方便團隊成員保持信息同步（@AI 寒武紀）&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;5、Adobe 發佈商用級 AI 圖像生成模型 Firefly Image 4 系列&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Adobe 更新發布了 Firefly Image 4 和 Firefly Image 4 Ultra 兩大 AI 圖像生成模型，支持最高 2K 分辨率輸出。&lt;/p&gt; 
&lt;p&gt;這兩款模型均基於 Adobe Stock 等授權內容以及公共領域數據訓練，如侵犯版權，可以讓 Adobe 賠償。（@三花 AI）&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;6、MLX-Audio: 蘋果芯片上的高效語音合成模型庫，提供 TTS REST API&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;MLX-Audio 是一個基於 Apple MLX 框架構建的文本轉語音 （TTS） 和語音轉語音 （STS） 庫，專為 Apple Silicon 芯片優化，提供出色的語音合成性能。&lt;/p&gt; 
&lt;p&gt;核心特性：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;蘋果芯片加速： 在 M 系列芯片上實現快速推理；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;多語言支持： 支持多種語言；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;語音定製： 提供豐富的語音定製選項；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;語速調節： 0.5x 到 2.0x 的語速調節範圍；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;可視化交互： 具有 3D 音頻可視化的交互式網頁界面；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;REST API: 提供用於 TTS 生成的 REST API；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;性能優化： 支持量化以優化性能；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;文件快速訪問： 通過 Finder/資源管理器集成直接訪問輸出文件。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;支持模型：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Kokoro: 多語言 TTS 模型，支持多種語言和語音風格。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;CSM （Conversational Speech Model） : Sesame 的對話語音模型，支持文本轉語音和使用參考音頻樣本進行聲音定製。(@GitHub)&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;02.有亮點的產品&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;1、AceditAI 面試教練：實時轉錄、問題檢測和個性化回覆等功能&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Acedit 是一款 Chrome 瀏覽器插件，作為你的 AI 面試教練：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;智能練習：&lt;/strong&gt; 上傳職位描述和簡歷，Acedit 即可生成個性化的練習問答，並通過 AI 模擬面試助你充分準備。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;實時 AI 建議：&lt;/strong&gt; 在 Google Meet、Zoom、Teams 等在線面試平台，Acedit 能讀取面試問題，並結合你的簡歷、領英資料等信息，提供實時 AI 生成的答案建議。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;定製求職信：&lt;/strong&gt; 內置 AI 工具，輕鬆生成個性化求職信。(@ProductHunt)&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2、LemonAI 推出 Slice Live：照片秒變實時數字人&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Lemon Slice Live 是一款實時音視頻 AI 數字人模型，讓你體驗前所未有的視頻聊天。基於擴散變換模型 （DiT） 技術，它能將任何角色圖像立即轉化為支持 10 多種語言的交互式視頻通話。無需訓練或設置特定角色模型，上傳一張照片即可與任意角色流暢對話，兼容寫實、卡通、繪畫等多種風格，支持高達 25 FPS 的實時渲染。（@三花 AI、LemonAI 官網）&lt;/p&gt; 
&lt;h2&gt;03.有態度的觀點&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;1、Anthropic 研究員：從理論上講 AI 有可能產生意識&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;日前，Anthropic 研究員 Kyle Fish 受公司邀請做了一期訪談節目，其中他在節目中表示，理論上講 AI 是可能產生意識的。&lt;/p&gt; 
&lt;p&gt;Kyle Fish 認為，雖然當前 AI 的整體系統與人類大腦在功能和結構上存在差異，但如果能夠以足夠高的保真度，去模擬人腦，其中包括模擬神經遞質分子的作用，那麼從理論上講，AI 有可能產生意識。&lt;/p&gt; 
&lt;p&gt;他還進一步表示，如果將大腦中的神經元逐個被替換成芯片，在替換過程中保持個體的行為和功能的不變，那麼替換完成後，個體的意識體驗可能不會發生太大變化。&lt;/p&gt; 
&lt;p&gt;值得一提的是，Anthropic 為了探索模型更深層次的體驗與潛在意識，啓動了一項研究計劃，旨在調查 AI 模型是否能夠有潛在的偏好和痛苦跡象，並且去判斷這是否符合道德。(@APPSO)&lt;/p&gt; 
&lt;h2&gt;04.有看點的活動&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;1、腦機接口智能技術應用挑戰賽正式開啓報名！( 04.26-05.28)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-b36b85455f3a09c836074daef9eb7d38a3a.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;（圖片來源：智姬）&lt;/p&gt; 
&lt;p&gt;腦機接口智能技術應用挑戰賽（AI-Based BCI Tech Competition）是由中關村領智青年人才自主創新發展中心聯合姬械機科技集團發起的，以腦與智能（Brain and Al）為主題方向的人工智能腦接口（Al-based BCl）前沿創新技術與應用競賽。&lt;/p&gt; 
&lt;p&gt;通過本次技術比賽為腦機科技創新者提供系統性技術支持與創新資源對接，重點推進腦機接口技術問題的解決，同時實現腦機接，口的行業應用示範與產業化落地創新探索。&lt;/p&gt; 
&lt;p&gt;賽題發佈與比賽報名 ：04/26 - 05/28&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;參賽團隊報名審核 ：05/28 - 06/08（截止報名） 比賽形式：（1）線下自主賽題解答； （2） 線上提交賽題答案；（3）現場場答辯分享；&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;一等獎 1 名獎金 30 萬 （第一名） ；&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;二等獎 2 名獎金 15 萬 （第二名、第三名） ；&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;三等獎 5 名獎金 8 萬 （第四名、第五名、第六名、第七名、第八名） 。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;目前官方已發佈&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FbvxYvC8jiE4xc56QP1ihig&quot; target=&quot;_blank&quot;&gt;相關賽題簡介&lt;/a&gt;：基於不同的通道腦機，完成與之相關的技術題、應用題。（@智姬）&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-6fd190df182060c949e714b0be523d5431a.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;更多 Voice Agent 學習筆記：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FSqXLZvq_zwWDcOVKbAb7HQ&quot; target=&quot;_blank&quot;&gt;級聯 vs 端到端、全雙工、輪次檢測、方言語種、商業模式...語音 AI 開發者都在關心什麼？丨 Voice Agent 學習筆記&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F7QPgzp8kDR_9iHUa4oFeiA&quot; target=&quot;_blank&quot;&gt;a16z 最新報告：AI 數字人應用層即將爆發，或將孕育數十億美金市場丨 Voice Agent 學習筆記&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FUM1qs2IT1S6kJ4sZf_k3uA&quot; target=&quot;_blank&quot;&gt;a16z 合夥人：語音交互將成為 AI 應用公司最強大的突破口之一，巨頭們在 B2C 市場已落後太多丨 Voice Agent 學習筆記&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FWI0gE4x-TZG0gdgSV_bVSA&quot; target=&quot;_blank&quot;&gt;ElevenLabs 33 億美元估值的秘密：技術驅動+用户導向的「小熊軟糖」團隊丨 Voice Agent 學習筆記&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FSVsgDF8F1hxy3-e5-ntGbw&quot; target=&quot;_blank&quot;&gt;端側 AI 時代，每台家居設備都可以是一個 AI Agent 丨 Voice Agent 學習筆記&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F4K5wdUEDxrs1afHZSAIuqg&quot; target=&quot;_blank&quot;&gt;世界最炙手可熱的語音 AI 公司，舉辦了一場全球黑客松，冠軍作品你可能已經看過&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FJCYzc1Ig-HFFAN3sTQDYbw&quot; target=&quot;_blank&quot;&gt;多模態 AI 怎麼玩？這裏有 18 個腦洞&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FrN9poD_X6SDxRLMsudg_xg&quot; target=&quot;_blank&quot;&gt;AI 重塑宗教體驗，語音 Agent 能否成為突破點？&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FeFS1mnAbUpAJdiLSSGWpSA&quot; target=&quot;_blank&quot;&gt;對話 TalktoApps 創始人：Voice AI 提高了我五倍的生產力，語音輸入是人機交互的未來&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fr2z1bilamX6YWTg90F8xYA&quot; target=&quot;_blank&quot;&gt;a16z 最新語音 AI 報告：語音將成為關鍵切入點，但非最終產品本身（含最新圖譜）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;寫在最後：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;我們歡迎更多的小夥伴參與 &lt;strong&gt;「RTE 開發者日報」&lt;/strong&gt; 內容的共創，感興趣的朋友請通過開發者社區或公眾號留言聯繫，記得報暗號「共創」。&lt;/p&gt; 
&lt;p&gt;對於任何反饋（包括但不限於內容上、形式上）我們不勝感激、並有小驚喜回饋，例如你希望從日報中看到哪些內容；自己推薦的信源、項目、話題、活動等；或者列舉幾個你喜歡看、平時常看的內容渠道；內容排版或呈現形式上有哪些可以改進的地方等。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-b097a16f69dc64b4f3a6805bc0066e50a2f.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;素材來源官方媒體/網絡&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/agora/blog/18255332</link>
            <guid isPermaLink="false">https://my.oschina.net/agora/blog/18255332</guid>
            <pubDate>Sun, 27 Apr 2025 12:10:00 GMT</pubDate>
            <author>原創</author>
        </item>
        <item>
            <title>谷歌認為自己是唯一能運營 Chrome 的公司，如若轉手，將「萬劫不復」</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;在美國司法部對谷歌在搜索引擎市場的非法壟斷案中，谷歌 Chrome 瀏覽器總經理 Parisa Tabriz &lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ffortune.com%2Farticle%2Fgoogle-chrome-suffer-if-forced-to-sell-parisa-tabriz%2F&quot; target=&quot;_blank&quot;&gt;表示&lt;/a&gt;&lt;/u&gt;，將谷歌與 Chrome 「剝離」是不可能的，並補充説，她認為「Chrome 不可能在其他地方被複制」。&lt;/p&gt; 
&lt;p&gt;Tabriz 強調造就 Chrome 瀏覽器今日成功的基石，源於 17 年來與谷歌其他部門的緊密協作。&lt;/p&gt; 
&lt;p&gt;Tabriz 表示，谷歌 Chrome 是 Chrome 團隊、谷歌以及向公司的開源 Chromium 項目提交技術貢獻的公司「17 年合作」的結果，該項目的開源代碼也被用於其他幾個谷歌項目，如 Android 操作系統。「谷歌在 Chromium 上投入了數億美元」，Tabri 説到，並表示其他公司「目前並沒有以任何有意義的方式做出貢獻。」&lt;/p&gt; 
&lt;p&gt;專家 James Mickens 認為，將 Chrome 從谷歌內部基礎設施進行剝離在技術上是「feasible」（可行的），並不會破壞其功能。他指出，谷歌仍有動力繼續為開源項目 Chromium 貢獻技術。&lt;/p&gt; 
&lt;p&gt;然而，Tabriz 反駁稱，&lt;strong&gt;谷歌自 2015 年以來貢獻了 Chromium 超過 90% 的代碼&lt;/strong&gt;，其他公司幾乎沒有實質性投入。&lt;/p&gt; 
&lt;p&gt;Tabriz 透露，谷歌正積極將 AI 技術融入 Chrome。用户目前可通過擴展程序使用 OpenAI 的 ChatGPT 和 Perplexity AI，或調整設置以便於使用其他 AI 模型搜索，不過 Gemini 被設為默認 AI 助手。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/346989/google-chrome-suffer-if-forced-to-sell-parisa-tabriz</link>
            <guid isPermaLink="false">https://www.oschina.net/news/346989/google-chrome-suffer-if-forced-to-sell-parisa-tabriz</guid>
            <pubDate>Sun, 27 Apr 2025 11:40:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>OpenAI 改進 GPT-4o 模型，帶來更強的智能和個性</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;本月初，OpenAI 發佈了多個新的 AI 模型。面向開發者的 GPT-4.1 模型引入了對 100 萬個 Token 上下文窗口的支持，並在指令遵循、編碼和智能方面進行了改進。o3 和 o4-mini 推理模型在多個 AI 基準測試中取得了最佳結果。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-e4d30e4eac108c004154d6855d6c524ec1d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;即使在發佈這些新模型之後，OpenAI 仍在持續更新 GPT-4o 模型。&lt;/p&gt; 
&lt;p&gt;今年 3 月，OpenAI 對 GPT-4o 進行了增強，使其更加直觀、更具創造力、更具協作性，並具有更好的指令遵循性、更強大的編碼能力以及更清晰的溝通風格。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;我們今天更新了 GPT-4o！智力和個性都得到了提升。&lt;/p&gt; 
 &lt;p&gt;— 薩姆·奧爾特曼 (@sama)&amp;nbsp;2025 年 4 月 25 日&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;今天，OpenAI CEO 奧特曼&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Fsama%2Fstatus%2F1915902652703248679&quot; target=&quot;_blank&quot;&gt;宣佈&lt;/a&gt;對 GPT-4o 模型進行再次更新，重點提升了智能和個性。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;優化 GPT-4o 保存記憶的時間長度並增強 STEM 的問題解決能力；&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;還對 GPT-4o 響應方式進行了細微的更改，使其更加主動，更好地引導對話取得富有成效的結果。&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0427/190903_uoAz_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;此增強版本目前僅通過 ChatGPT 體驗提供，開發者尚無法通過 API 訪問。&lt;/p&gt; 
&lt;p&gt;OpenAI 聲稱，該模型現在展現出了更好的「氛圍」、格式、對用户需求的直覺以及其他定性增強。然而，由於改進更難以量化，他們並未分享此版本的最新基準。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-9f13b1ef859a79511ac123aa8987b2341b7.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;艾丹·麥克勞克林 (Aidan McLaughlin) 目前在 OpenAI 負責模型設計和能力開發，他在 Twitter 上表示，此次 GPT-4o 更新是 OpenAI 迄今為止為主要 4o 系列發佈的最快的更新，這表明發佈速度正在加快。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;太喜歡這個模型了！簡直太有意思了！&lt;/p&gt; 
 &lt;p&gt;如果你有什麼反饋，歡迎留言！&lt;/p&gt; 
 &lt;p&gt;- Aidan McLaughlin (@aidan_mclau)&amp;nbsp;2025 年 4 月 25 日&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;雖然基準衡量了人工智能模型的核心能力，但「氛圍」等現實世界方面的改進表明 OpenAI 越來越關注整體用户體驗和交互風格。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/346982/openai-updated-gpt-4o</link>
            <guid isPermaLink="false">https://www.oschina.net/news/346982/openai-updated-gpt-4o</guid>
            <pubDate>Sun, 27 Apr 2025 11:10:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>模力方舟百模破浪 —— 北京經開區推進 AI 開源開放生態共創</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;4 月 27 日下午，在北京人工智能產業生態創新發佈會上，開源人工智能社區「模力方舟」正式發佈，「開源人工智能應用創新大賽」也同步啓動，經開區將圍繞建設全域人工智能之城，助力共建國內 AI 開源開放生態。&lt;/p&gt; 
&lt;p&gt;模力方舟依託開源中國 17 年生態構建，積累超 1800 萬開發者、2000 餘所高校、36 萬家企業，以絕對中立平台面向開發者提供從開源模型、訓練數據集、國產算力底座到模型在線微調測試的全流程支持，降低大模型開發門檻，提高開發效率，以深厚的開源和開發者服務底藴，致力於成為 AI 時代的重要創新引擎與生態共建平台。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0427/182702_pFoc_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://ai.gitee.com/&quot;&gt;https://ai.gitee.com&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;「模力方舟」旨在對標國外開源社區 Hugging Face，於 2024 年 1 月上線，算力供給方面與沐曦 MetaX、華為昇騰、天數智芯、摩爾線程等國產 GPU 企業合作，目前已積累約 16000 個開源模型及約 10000 個高質量數據集，覆蓋自然語言處理、計算機視覺等主流 AI 領域，註冊用户數超 100 萬。&lt;/p&gt; 
&lt;p&gt;建設「模力方舟」有哪些資源基礎？未來又將如何推進？北京經開區有關負責人介紹：「此前，經開區與北京市聯動投資牽引國內頭部開源社區企業——開源中國總部落地經開區。基於開源中國在傳統開源領域的生態積累，圍繞人工智能開源大模型和國產算力底座軟硬一體化適配，我們啓動了人工智能開源社區‘模力方舟’建設工作。」&lt;/p&gt; 
&lt;p&gt;為助力「模力方舟」建設成為具有國際影響力的開源人工智能社區，經開區將支持開源中國從夯實平台能力、引導資源匯聚、打造國際品牌、政策保駕護航四個方面展開工作。具體來説，充分依託公有云的彈性拓展特性與私有云的安全可控優勢，精心構建起具備高併發處理能力、能夠實現低延時響應的算力基礎設施體系；整合自然語言處理、計算機視覺、語音識別等多個 AI 核心領域的國內外前沿模型，以及圖像數據集、文字數據集、各類先進算法；圍繞「一賽一會」這一核心策略持續擴大影響力，舉辦開源人工智能大賽及開源峯會等品牌活動，推動具有高成長性的開源 AI 創業項目在開源社區形成落地集聚；對優質開源項目、優秀商業化應用、開源生態活動給予一定資金支持等。&lt;/p&gt; 
&lt;p&gt;值得一提的是，在本次發佈會上，「一賽一會」核心策略中的「開源人工智能應用創新大賽」正式啓動。由開源中國聯合戰略合作伙伴，華為昇騰、商湯科技、智譜（Z.ai）、沐曦 MetaX、天數智芯、睿思芯科、希姆計算等國內領先人工智能企業共同發起。他們將為賽事提供核心算力支持、先進 AI 模型、優質數據資源以及行業生態聯動支持。這是一場全國性的人工智能賽事，定位國家級影響力賽會，通過競賽展示開源與人工智能前沿技術創新和商業化應用，推動產業化落地，為項目實踐應用提供發展平台。大賽設專業組和青少年組兩個組別：專業組賽道涵蓋 AI 醫療、AI 金融、AI 智能製造、視覺呈現與感知、具身智能與機器人、AI 教育與智能教學解決方案等七大方向，青少年組賽道包括創新場景實踐應用、AI 算法設計與優化等四大方向。&lt;/p&gt; 
&lt;p&gt;此外，入選團隊將在半決賽中與 2025 GOTC 全球開源技術峯會深度聯動，為參賽團隊打造國際化的展示舞台。優秀項目可在峯會分論壇演講，演示對抗和大咖點評，全程媒體直播，讓創新成果獲得最大曝光。&lt;/p&gt; 
&lt;p&gt;與此同時，為更好地促進參賽團隊創新成果落地，大賽主辦方與經開區政府積極聯動，將設立一、二、三等獎及多項單項獎項，為獲獎團隊提供獎金和配套政策支持，同時協調優質辦公空間及算力資源的方式支持優秀項目孵化落地。&lt;/p&gt; 
&lt;p&gt;本次大賽旨在打造一項全國性、具有國家級影響力的人工智能賽事，目標是通過賽事展示技術創新與商業化應用，推動產業落地，聚焦人工智能的前沿技術，同時也為項目的實際應用提供發展平台。&lt;/p&gt; 
&lt;p&gt;目前，大賽已面向全國範圍的參賽者開放報名，參賽團隊可通過報名通道提供成熟的技術方案、商業化路徑與應用場景等作品。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/346975</link>
            <guid isPermaLink="false">https://www.oschina.net/news/346975</guid>
            <pubDate>Sun, 27 Apr 2025 10:27:00 GMT</pubDate>
            <author>來源: 投稿</author>
        </item>
        <item>
            <title>谷歌在壟斷審判中被曝向三星支付鉅款預裝 Gemini 應用</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p style=&quot;color:#242424; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span&gt;彭博社報道稱，正在進行的谷歌反壟斷審判本週的證詞顯示，谷歌每月向三星支付「鉅額資金」，以在其設備上預裝其 Gemini 人工智能應用程序。這一信息正值法官阿米特·梅塔 (Amit Mehta) 已裁定谷歌的搜索引擎構成非法壟斷之後，目前谷歌的律師正與美國司法部就潛在的處罰力度展開辯論。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#242424; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;img height=&quot;283&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-3bc9b596b7ba040b8f52188c972b5dfbbfc.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#242424; margin-left:0; margin-right:0; text-align:left&quot;&gt;谷歌平台和設備合作副總裁彼得·菲茨傑拉德週一作證稱，谷歌與三星之間的這筆付款協議始於今年 1 月份。值得注意的是，這筆交易啓動於谷歌被認定違反反壟斷法之後，而此前谷歌被判定壟斷的部分原因正是其與蘋果、三星等公司類似的搜索默認合作協議。作為合作的一部分，三星在 1 月份推出的 Galaxy S25 系列手機中，將 Gemini 設置為長按電源鍵時的默認 AI 助手，而三星自家的 Bixby 助手則被置於次要地位。&lt;/p&gt; 
&lt;p style=&quot;color:#242424; margin-left:0; margin-right:0; text-align:left&quot;&gt;據《The Information》報道，菲茨傑拉德在證詞中提及，包括 Perplexity 和微軟在內的其他公司也曾向三星推銷在其設備上預裝人工智能助手應用的協議。然而，美國司法部律師指出，谷歌提交的試圖修改與手機製造商協議的信函實際上是在庭審前夕，即上週才發出的，暗示這些舉動可能是應對庭審壓力。此外，《The Information》報道稱，當天提交的谷歌內部幻燈片似乎顯示，谷歌「正在考慮更具限制性的分銷協議，要求合作伙伴在谷歌搜索和 Chrome 瀏覽器之外預裝 Gemini」。&lt;/p&gt; 
&lt;p style=&quot;color:#242424; margin-left:0; margin-right:0; text-align:left&quot;&gt;關於支付細節，彭博社報道稱，菲茨傑拉德表示，與三星的 Gemini 協議為期兩年，除了固定的月費外，谷歌還將向三星支付一定比例的 Gemini 應用訂閲收入。彭博社援引美國司法部律師戴維·達爾奎斯特（David Dahlquist）的話稱，這筆固定的月費是一筆「鉅款」，但具體數額尚未公開。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/346972</link>
            <guid isPermaLink="false">https://www.oschina.net/news/346972</guid>
            <pubDate>Sun, 27 Apr 2025 10:02:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>崑崙萬維 2024 年營收 56.6 億，研發費用 15.4 億元</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;崑崙萬維日前&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FEAtGxplLxQkmhQtvMNqjew&quot; target=&quot;_blank&quot;&gt;發佈&lt;/a&gt;2024 年度財報，公司實現營業總收入 56.6 億元，同比增長 15.2%。整體毛利率達 73.6%。全年研發費用為 15.4 億元，同比增長 59.5%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;財報顯示，公司 AGI 與 AIGC 業務商業化取得重要進展，其中，AI 社交報告期內單月最高收入突破 100 萬美元，成為海外收入增長速度最快的中國 AI 應用之一；截止 2025 年 3 月底，AI 音樂年化流水收入 ARR 達到約 1,200 萬美金（月流水收入約 100 萬美金）。短劇平台 DramaWave 年化流水收入 ARR 達到約 1.2 億美金（月流水收入約 1000 萬美金），進一步加速 AI 商業化進程。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;img height=&quot;321&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-17907b25e9045ba880f14778875f3df05f4.png&quot; width=&quot;700&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;海外信息分發與元宇宙平台 Opera 繼續保持高速增長，2024 年實現營業收入 4.8 億美元，同比增長 21.1%；海外社交網絡和短劇平台業務實現營業收入 12.5 億元，同比增長 28.5%，綜合推動公司海外業務收入規模上升至 51.5 億元，同比增長 21.9%，佔總收入比重達 91.0%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;報告期內，「天工」峯值日活躍用户超過 100 萬，峯值月活躍用户突破 1,000 萬。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/346955</link>
            <guid isPermaLink="false">https://www.oschina.net/news/346955</guid>
            <pubDate>Sun, 27 Apr 2025 08:56:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>我國將加快建立人工智能知識產權保護規則</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;目前，我國人工智能領域呈現良好的發展勢頭，根據世界知識產權組織報告，中國已經成為全球人工智能專利的最大擁有國，在全球的佔比達到 60%。下一步，我國將持續推進人工智能相關知識產權制度創新，加快建立人工智能、大數據等新領域新業態知識產權保護規則；建設人工智能領域專利池，促進更多人工智能領域專利從實驗室走向產業鏈。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;人工智能是新一輪科技革命和產業變革的重要驅動力量。國家知識產權局局長申長雨表示，近年來，國家知識產權局積極回應人工智能新領域、新業態、新模式發展的需要，深入推進人工智能領域知識產權制度創新，為人工智能技術發展和產業發展提供有力的制度供給。包括及時修改完善《專利審查指南》，發佈《人工智能相關發明專利申請指引》，積極回應和解決了有關人工智能專利申請主體、保護客體、審查標準等熱點問題。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;「聚焦人工智能重點領域，為相關專利申請提供快速審查服務，嚴厲打擊搶注‘DeepSeek’等相關商標申請行為，持續強化知識產權保護，護航人工智能領域科技創新。與此同時，加快人工智能技術在知識產權領域的運用，推動知識產權工作數字化轉型和智能化升級，不斷提升知識產權治理效能。」申長雨説。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;據介紹，下一步，國家知識產權局將積極推動知識產權與人工智能共生演進、雙向賦能、融合發展。其中，將持續推進人工智能相關知識產權制度創新，加快建立人工智能、大數據等新領域新業態知識產權保護規則，為發展人工智能技術提供更加有力的法治保障。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;同時，提高人工智能知識產權保護和運用水平，健全知識產權支撐關鍵核心技術攻關工作體系，加大人工智能領域專利申請按需審查服務力度，做好相關發明專利分析預警和導航服務，指導建設人工智能領域專利池，深入實施專利轉化運用專項行動，促進更多人工智能領域專利從實驗室走向產業鏈，賦能相關產業發展。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;此外，積極參與人工智能領域知識產權全球治理，推動完善相關國際規則和標準，促進全球人工智能產業發展，讓人工智能技術更好造福全人類。（經濟參考報，記者，汪子旭）&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/346940</link>
            <guid isPermaLink="false">https://www.oschina.net/news/346940</guid>
            <pubDate>Sun, 27 Apr 2025 08:07:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>「思考更長時間」 而非 「模型更大」 是提升模型在複雜軟件工程任務中表現的有效途徑</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                                                                                                                                        &lt;p&gt;作者：明巍/臨城/水德&lt;/p&gt; 
&lt;p&gt;還在為部署動輒數百 GB 顯存的龐大模型而煩惱嗎？還在擔心私有代碼庫的安全和成本問題嗎？通義靈碼團隊最新研究《Thinking Longer, Not Larger: Enhancing Software Engineering Agents via Scaling Test-Time Compute》探索瞭如何通過擴展測試時計算（Test-Time Compute Scaling, TTS），讓個人可部署的開源大模型（如僅需單卡運行的 32B 模型），達到與頂級閉源模型（如 DeepSeek R1, OpenAI o1）相媲美的代碼推理和問題解決能力。&lt;/p&gt; 
&lt;h3&gt;核心亮點：&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;性能飛躍：32B 模型在結合了兩種 Test Time Scaling 策略後，在 SWE-bench Verified 基準上，成功解決了 46.0% 的真實 GitHub Issue，與 DeepSeek R1 和 OpenAI o1 等更大規模的業界領先模型表現相當；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;實證 TTS 現象：內部 TTS (Internal TTS) 通過高質量、多階段的合成開發軌跡進行訓練，讓模型學會深度思考，模型在面對更有挑戰的問題時，會動態地分配更多計算資源（輸出更多 Token），這驗證了&quot;思考更長時間&quot;確實能提升模型解決複雜任務的能力。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;最優開發過程搜索：在軟件開發的關鍵決策點（如倉庫理解、故障定位）進行幹預，利用過程獎勵模型和結果獎勵模型指導搜索，以更優的計算效率找到最佳解決方案。同時利用更大的推理 budget 會產生更優的性能。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;方法：內外兼修的 Test-time Scaling 策略&lt;/h3&gt; 
&lt;p&gt;我們提出了一個統一的測試時計算（TTS）擴展框架，包含兩種互補策略：&lt;/p&gt; 
&lt;h4&gt;1. 內部 TTS (Internal Test-Time Scaling)： 內化深度思考能力&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;高質量軌跡合成 (High-Quality Trajectory Synthesis)：&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;數據源&lt;/strong&gt; ： 從 GitHub 上篩選，超過 1000 星標，的高質量倉庫，收集真實的 &lt;code&gt;&amp;lt;issue, pull-request, codebase&amp;gt;&lt;/code&gt; 三元組數據。&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;初始過濾&lt;/strong&gt;： 應用啓發式規則過濾數據，例如，保留描述足夠詳細的 issue (≥20 字符, ≤3 超鏈接)，以及修改量適中 (1-5 個代碼文件, 非純測試文件修改) 的 PR。&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;環境構建與驗證&lt;/strong&gt; ： 利用 &lt;code&gt;ExecutionAgent &lt;/code&gt;嘗試為每個倉庫自動構建可執行的測試環境，確保後續能夠進行真實的補丁驗證。無法成功構建或運行環境的倉庫被排除，最終形成包含約 9000 個 issue 和 300 個倉庫，的高質量數據集。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;軌跡引導與增強 (Trajectory Bootstrapping)：&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;基礎框架&lt;/strong&gt; ： 基於開源的 &lt;code&gt;SWE-SynInfer &lt;/code&gt;框架（包含倉庫理解、故障定位、補丁生成三個階段），增加了補丁驗證 (Patch Verification) 階段，形成 &lt;code&gt;SWE-SynInfer+&lt;/code&gt; 框架。在此階段，模型需生成復現代碼來自動驗證補丁有效性，並在失敗時進行迭代優化。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;引導模型&lt;/strong&gt;： 使用開源推理模型 DeepSeek R1 作為教師模型，在其多次內部推理迭代和優化的能力下，生成詳盡的、包含多輪思考與修正的，長思維鏈（Long CoT）軌跡。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;開發上下文的拒絕採樣 (Development-Contextualized Rejection Sampling)：&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;多維質量把關&lt;/strong&gt; ： 對生成的軌跡進行嚴格的多維度驗證和過濾： 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;倉庫理解準確性&lt;/strong&gt;： 檢查模型識別的待修改文件是否與開發者實際修改的文件一致。&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;故障定位準確性&lt;/strong&gt;： 確認模型生成的補丁是否作用於開發者實際修改的代碼位置（類、函數、代碼塊）。&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Issue 復現代碼有效性&lt;/strong&gt;： 驗證生成的復現代碼能否在原始代碼上觸發問題，在應用開發者補丁後問題消失。&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;補丁正確性&lt;/strong&gt;： 應用模型補丁後，運行其生成的復現代碼和倉庫原有的單元測試，檢查問題是否解決且無新問題引入。&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;複雜性過濾&lt;/strong&gt;： 篩除掉基礎模型（Qwen2.5 Coder 32B）無需複雜推理就能一次性解決的簡單問題，確保訓練數據能有效激發模型的深度推理潛力。&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;保留有效中間步驟&lt;/strong&gt;： 如果一個軌跡的補丁驗證失敗，但之前的倉庫理解、故障定位等步驟是正確的，保留這些正確的中間步驟數據，避免浪費有價值的推理過程信息。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;推理式訓練 (Reasoning Training)：&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;學習目標&lt;/strong&gt;： 採用標準的監督學習，優化模型生成正確推理動作（包括思考過程和最終行動）的條件概率。損失函數同時計算軌跡中每個步驟的 &quot;思考（think）&quot;（規劃、反思、修正等）和 &quot;回答（answer）&quot;（最終輸出的 API 調用、代碼補丁等）部分，促使模型學習完整的決策過程。&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;歷史信息剪枝&lt;/strong&gt; ： 為提高多輪推理效率，借鑑 DeepSeek R1 的機制，在生成第 &lt;code&gt;i&lt;/code&gt; 步時，歷史上下文中只保留第 &lt;code&gt;i-1&lt;/code&gt; 步的 &lt;code&gt;answer&lt;/code&gt; 部分，捨棄 &lt;code&gt;think&lt;/code&gt; 部分，減少冗餘信息。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;2. 外部 TTS (External Test-Time Scaling): 優化決策搜索路徑&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;基於開發流程的搜索策略 (Development-Process-Based Search, Dev-Search)：&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;核心思想： 軟件工程任務是長鏈條決策過程，中間步驟的錯誤會嚴重影響最終結果。我們摒棄僅在終點驗證或對每一步都進行低效驗證的做法，選擇在，三個關鍵決策階段（倉庫理解、故障定位、補丁生成）集中進行搜索和評估，以高效利用計算預算。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;過程獎勵模型 (Process Reward Model, PRM) 引導：&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;訓練目標： 訓練 PRM（基於基礎模型微調）來判斷中間輸出的正確性（二元分類任務）。例如，判斷識別的文件是否正確，定位的代碼位置是否準確。&lt;/li&gt; 
   &lt;li&gt;引導方式： 在每個階段生成 N 個候選輸出，使用 PRM 對其打分，保留 Top-k 的高分候選進入下一階段，實現輕量級的、有指導的 Beam Search，有效剪枝低潛力路徑。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;執行驗證與結果獎勵模型 (Outcome Reward Model, ORM) 排序：&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;補丁驗證： 在補丁生成階段，利用模型生成的復現代碼和倉庫自帶的迴歸測試，對候選補丁進行執行驗證，確保其有效性且不破壞原有功能。&lt;/li&gt; 
   &lt;li&gt;最終排序： 對於通過執行驗證的多個候選補丁（可能存在多個），使用 ORM 進行最終排序。ORM 基於 DPO 進行訓練，學習偏好&quot;通過所有測試&quot;的補丁優於&quot;未通過測試&quot;的補丁。重要的是，ORM 僅需 Issue 描述和候選補丁作為輸入，不依賴中間推理步驟，易於集成。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;* 所有模型均基於開源的 Qwen2.5 Coder 32B 模型進行訓練，該模型可在消費級顯卡上進行部署。&lt;/p&gt; 
&lt;h3&gt;實驗評估：&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;1. 整體性能 SOTA：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;結果&lt;/strong&gt; ：訓練的 SWE-Reasoner 32B 模型結合了內部和外部 TTS (Unified TTC, budget=8) 後，達到了 &lt;strong&gt;46.0%&lt;/strong&gt; 的 Issue 解決率。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;對比&lt;/strong&gt; ：在 &lt;strong&gt;≤100B 參數量級&lt;/strong&gt; 的模型中處於領先地位，超越瞭如 DeepSeek R1 671B (41.20%) 等更大的開源模型，並且接近業界頂尖的閉源模型 &lt;strong&gt;Claude 3.5 Sonnet v2 (46.20%) 和 OpenAI o1 (45.60%)&lt;/strong&gt; 。（詳見圖 1 和表 1）&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;泛化性與獨特性&lt;/strong&gt; ： 該模型在 SWE-bench 覆蓋的 &lt;strong&gt;12 個不同 Python 倉庫&lt;/strong&gt; 上均表現出魯棒的性能，在多數倉庫上媲美或超越 DeepSeek R1 671B（詳見圖 2）。此外，通過與其他模型的解決實例對比，我們的方法能夠&lt;strong&gt;獨立解決 17 個&lt;/strong&gt; 其他模型無法解決的 Issue，展現了獨特的解題能力。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-7db136b0910f0176b97a3c5b1acf6fb954a.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;圖 1: 在 SWE-Bench Verified 上，對具有擴展測試時間計算的較小 LLM 與較大模型的性能進行比較&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-72f7e7d2804b299d8dfa3b233a7800bde83.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;表 1: 與不同模型和框架在 SWE-bench Verified 基準上的性能比較。&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-34e04a33bd527f603d32925d74cd5399fbd.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;圖 2: 針對不同倉庫的 issue 解決率比較&lt;/em&gt;&lt;/p&gt; 
&lt;ol start=&quot;2&quot;&gt; 
 &lt;li&gt;內部 TTS 研究分析：&lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt;不同難度性能優勢：Long CoT 訓練相比 Short CoT 訓練在解決更難 issue 上提升明顯（基於社區解決頻率劃分的 Level 5，效果提升約 6 倍，詳見圖 3）。&lt;/li&gt; 
 &lt;li&gt;Test-Time Scaling 現象：Reasoning 模型在解決更難的問題上會嘗試輸出更多 token，有明顯的 test-time scaling 現象（SWE-Reasoner 和 OpenAI o1），Claude 3.5 Sonnet 也有這個 TTS 現象，但是整體輸出 token 較少。而 Short CoT 模型則沒有這種明顯的自適應計算行為（詳見圖 4）。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-3c30dd96e1f621562e4abaabc3b0de14eaf.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;圖 3: 在不同難度的 SWE-bench Verified 上的不同模型的解決率&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-4621ed1a8486d72ecc1601d79134bff04f0.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;圖 4: 在不同難度的 SWE-bench Verified 上的不同模型的平均輸出 tokens 比較&lt;/em&gt;&lt;/p&gt; 
&lt;ol start=&quot;3&quot;&gt; 
 &lt;li&gt;外部 TTS 研究分析：&lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Dev-Search 策略優勢&lt;/strong&gt; ，在控制相同推理預算（Rollout 次數 1, 2, 4, 8）的條件下，我們提出的 Dev-Search 策略&lt;strong&gt;始終優於&lt;/strong&gt;僅依賴執行驗證 (Exec)、執行驗證+ORM (ORM_Exec) 或投票 (Voting) 的基線方法。這證明瞭在關鍵開發流程中進行幹預和指導能帶來更優的搜索效率。（詳見圖 5）&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;預算與性能關係 (TTS)&lt;/strong&gt; ： 增加推理預算（Generation Budget）通常能帶來性能的提升，再次驗證了&lt;strong&gt;外部 TTS 的有效性&lt;/strong&gt; 。預算的增加對於解決&lt;strong&gt;簡單和中等難度&lt;/strong&gt;（Level 1-4）的問題提升尤為明顯。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;高難度任務瓶頸&lt;/strong&gt; ： 對於&lt;strong&gt;最高難度&lt;/strong&gt; （Level 5）的問題，過高的推理預算反而可能導致性能輕微下降。這暗示對於極其複雜的任務，僅靠外部搜索擴展可能已觸及模型&lt;strong&gt;內在推理能力&lt;/strong&gt;的瓶頸，需要內部 TTS（想得更深）的共同作用或更強的基礎模型能力。（詳見圖 6）&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-434beb18c7bb820d3f490ae790cab2d9350.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;圖 5: 不同搜索方式在相同 budget 下的性能比較&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-468c4c7e44d51a148124f91360ce28d2836.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;圖 6: 在不同難度的 SWE-bench Verified 上使用不同 budget 的能力比較&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;結論&lt;/h3&gt; 
&lt;p&gt;本研究成功展示了通過統一的測試時計算（TTS）擴展框架，可以顯著增強個人可部署的開源 SWE Agent 的代碼推理和問題解決能力。我們證明瞭&quot;思考更長時間&quot;（增加推理計算）而非&quot;模型更大&quot;（增加參數）是提升模型在複雜軟件工程任務中表現的有效途徑。這項工作為在資源受限環境下（如私有部署）使用和發展高性能 SWE Agent 開闢了新的可能性。&lt;/p&gt; 
&lt;h3&gt;展望與思考：更智能更自適應的 SWE Agent&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;自適應計算&lt;/strong&gt; ： 未來可以研究如何讓模型根據任務難度&lt;strong&gt;動態、自適應地調整計算資源的投入&lt;/strong&gt;，實現效率與效果的最佳平衡。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;環境與驗證&lt;/strong&gt;： 提升自動化測試環境構建和解決方案驗證的魯棒性與規模，是進一步利用強化學習 (RL) 釋放 SWE Agent 潛力的關鍵。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;任務泛化&lt;/strong&gt;： 將此 TTS 框架應用到更廣泛的軟件工程任務中，如測試用例生成和代碼重構等。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;🔎 &lt;strong&gt;詳細方案請參考論文：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;arxiv📄: &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2503.23803&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2503.23803&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Github🌟: &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fyingweima2022%2FSWE-Reasoner&quot; target=&quot;_blank&quot;&gt;https://github.com/yingweima2022/SWE-Reasoner&lt;/a&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/u/3874284/blog/18219052</link>
            <guid isPermaLink="false">https://my.oschina.net/u/3874284/blog/18219052</guid>
            <pubDate>Sun, 27 Apr 2025 07:54:00 GMT</pubDate>
            <author>原創</author>
        </item>
        <item>
            <title>Graphiti —— 為 AI 代理構建實時知識圖譜</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                                                                                                                            &lt;p style=&quot;text-align:start&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;color:#1f2328&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;background-color:#ffffff&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Graphiti 是一個用於構建和查詢時序感知知識圖譜的框架，專為在動態環境中運行的 AI 代理量身定製。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:start&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;color:#1f2328&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;background-color:#ffffff&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;與傳統的檢索增強生成 (RAG) 方法不同，Graphiti 持續將用户交互、結構化和非結構化企業數據以及外部信息集成到一個連貫且可查詢的圖中。該框架支持增量數據更新、高效檢索和精確的歷史查詢，無需完全重新計算圖譜，因此非常適合開發交互式、情境感知的 AI 應用程序。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:start&quot;&gt;&lt;span style=&quot;background-color:#ffffff; color:#1f2328&quot;&gt;Graphiti 專門為解決動態和頻繁更新的數據集的挑戰而設計，使其特別適合需要實時交互和精確歷史查詢的應用程序。&lt;/span&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:start&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;color:#1f2328&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;background-color:#ffffff&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;使用 Graphiti 可以：&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;集成並維護動態用户交互和業務數據。&lt;/li&gt;
&lt;li&gt;促進代理基於狀態的推理和任務自動化。&lt;/li&gt;
&lt;li&gt;使用語義、關鍵字和基於圖形的搜索方法查詢複雜、不斷變化的數據。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;優點：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;實時增量更新：&lt;/strong&gt;立即集成新的數據事件，無需批量重新計算。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;雙時間數據模型：&lt;/strong&gt;明確跟蹤事件發生和攝取時間，允許準確的時間點查詢。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;高效混合檢索：&lt;/strong&gt;結合語義嵌入、關鍵字（BM25）和圖遍歷，實現低延遲查詢，而無需依賴 LLM 摘要。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自定義實體定義：&lt;/strong&gt;通過簡單的 Pydantic 模型靈活地創建本體並支持開發人員定義的實體。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;可擴展性：&lt;/strong&gt;通過並行處理有效管理大型數據集，適用於企業環境。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;281&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0427/152503_famB_4252687.gif&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

                                                                    &lt;/div&gt;
                                                                </description>
            <link>https://www.oschina.net/p/graphiti</link>
            <guid isPermaLink="false">https://www.oschina.net/p/graphiti</guid>
            <pubDate>Sun, 27 Apr 2025 07:28:00 GMT</pubDate>
        </item>
        <item>
            <title>手機版 QQ 支持微信小程序</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;QQ 手機端最新版本新增了對微信小程序的支持。&lt;/p&gt; 
&lt;p&gt;經實測，在最新版本的 QQ 手機端下，用户在首頁下拉或在搜索處即可喚醒「QQ 轉微信小程序」的相關入口。首次從 QQ 進入微信小程序會跳轉至微信進行賬號授權，隨後即可在 QQ 處直接喚醒相關小程序。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0427/142420_lJxN_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;980&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0427/142621_Zc2r_2720166.png&quot; width=&quot;746&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0427/142400_VcHG_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;值得一提的是，QQ 處的小程序內登錄的賬號，會與授權過信息的微信保持一致。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/346915</link>
            <guid isPermaLink="false">https://www.oschina.net/news/346915</guid>
            <pubDate>Sun, 27 Apr 2025 06:27:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
    </channel>
</rss>