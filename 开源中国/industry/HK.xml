<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - industry - 繁體中文（香港）</title>
    <link>https://www.oschina.net/news/industry</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news/industry" rel="self" type="application/rss+xml"/>
    <description>已對該 RSS 進行格式化操作：中英字符之間插入空格、使用直角引號、標點符號修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-hk</language>
    <lastBuildDate>Mon, 08 Sep 2025 12:41:01 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>李彥宏頒發「百度最高獎」：心流團隊獲 100 萬美元獎勵</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;今日，百度創始人李彥宏在內部活動上為技術團隊頒發「百度最高獎」，獲獎團隊得到 100 萬美元獎勵，合人民幣超 700 萬元。「百度最高獎」已歷經 15 屆，語音識別、深度學習平台、大模型等大量 AI 技術均曾獲獎，獎金總金額將近 4 億元人民幣。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-322339010570111171fc427256170f32b22.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;據瞭解，「百度最高獎」於 2010 年 7 月設立，鼓勵「小團隊做出大事業」，是百度公司最高級別的獎項，給予每個獲獎團隊 100 萬美元獎勵。獎項評選需滿足三項條件：項目意義重大；成果遠超預期；團隊足夠小，必須是小於等於 10 人。&lt;/p&gt; 
&lt;p&gt;本次百度最高獎的獲獎團隊為「心流」團隊。據介紹，「心流」團隊率先實現了端到端的多模態內容理解與序列生成技術。李彥宏在頒獎時表示，到今天，模型發展已經非常接近臨界點，很快就會有各種有價值的應用被創造出來，「我們生活在一個非常令人興奮、非常令人期待的環境當中」。&lt;/p&gt; 
&lt;p&gt;李彥宏稱，百度搜索已有近 70% 結果含有 AI 生成內容，且通過「百看」帶來富媒體形式，是全球所有的搜索引擎當中改造最激進的，這也代表搜索引擎的未來。&lt;/p&gt; 
&lt;p&gt;同時，百度慧博星數字人已達到「以假亂真」的地步，「很多人看不出是數字人還是真人」；百度蘿蔔快跑已覆蓋全球 16 座城市，代表着最新一代的無人駕駛技術。&lt;/p&gt; 
&lt;p&gt;頒獎典禮現場，李彥宏在談及 AI 發展時指出，「AI 大模型發展到今天，其實已接近了臨界點，很快就會有各種各樣非常有價值的應用能夠創造出來，我們正生活在一個非常令人興奮、非常令人期待的市場環境當中。」&lt;/p&gt; 
&lt;p&gt;「我們所從事的每一項工作都代表着未來，我也希望大家和我一起去期待，去迎接、去奮鬥出一個創新在 C 位的社會。」李彥宏表示。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370987</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370987</guid>
      <pubDate>Sun, 07 Sep 2025 11:28:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>「AI 教父」辛頓竟然被前女友竟用 ChatGPT 提分手</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;近日，被譽為「AI 教父」 的 Geoffrey Hinton 在接受採訪時&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.ft.com%2Fcontent%2F31feb335-4945-475e-baaa-3b880d9cf8ce" target="_blank"&gt;透露&lt;/a&gt;，他的前女友曾用 ChatGPT 給他發送分手信息。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1776" src="https://static.oschina.net/uploads/space/2025/0908/192243_YVS9_2720166.jpg" width="1280" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Hinton 作為人工智能領域的先驅，其在 1980 年代的工作為機器學習和人工神經網絡奠定了基礎，去年還獲得了諾貝爾物理學獎。&lt;/p&gt; 
&lt;p&gt;這位 AI 領域的權威人士卻未能預料到自己會被 AI 工具所「傷害」，他的前女友用 ChatGPT 告訴他他有多糟糕，讓他非常驚訝。「她用聊天機器人説出我的缺點，再傳給我。」不過辛頓自認沒有聊天機器人説的那麼糟，所以也沒有太難過。&lt;/p&gt; 
&lt;p&gt;事實上，讓像 ChatGPT 這樣的聊天機器人撰寫分手短信等似乎並不是什麼新鮮事，畢竟越來越多的人就一系列問題向 AI 諮詢。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370985</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370985</guid>
      <pubDate>Sun, 07 Sep 2025 11:23:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Agent Client Protocol —— 代碼編輯器與 Agent 的通信協議</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                            &lt;p&gt;Agent Client Protocol (ACP) 是用於連接代碼編輯器和 Agent 的協議，對代碼編輯器（用於查看和編輯源代碼的交互式程序）與編碼 Agent（使用生成式 AI 自主修改代碼的程序）之間的通信進行了標準化。&lt;/p&gt;

&lt;p&gt;這一協議讓開發者可以在編輯器中自由接入任意第三方智能體（Agent），無需依賴官方內置工具。其理念類似於語言服務器協議（LSP），通過解耦編輯器與 Agent 的交互方式，提供更靈活的擴展能力。&lt;/p&gt;

&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-2f621ad18024ec580d997b820ea9139346e.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

&lt;p&gt;ACP 協議已經以 Apache 開源許可證發佈，任何開發者都可基於它集成自己的 AI Agent。&lt;/p&gt;

                                                                    &lt;/div&gt;
                                                                </description>
      <link>https://www.oschina.net/p/agent-client-protocol</link>
      <guid isPermaLink="false">https://www.oschina.net/p/agent-client-protocol</guid>
      <pubDate>Sun, 07 Sep 2025 11:18:00 GMT</pubDate>
    </item>
    <item>
      <title>商湯日日新為 Claude API 用户提供「搬家」大禮包</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;9 月 5 日，Anthropic 宣佈將禁止中資控股超過 50% 的公司使用 Claude 服務，並限制企業通過海外雲服務、第三方平台等方式間接使用。&lt;/p&gt; 
&lt;p&gt;即日起，商湯日日新大模型 SenseNova 將為 Claude 用户提供「搬家」服務，幫助客户繼續享受高質量的模型能力和服務。&lt;/p&gt; 
&lt;p&gt;相關模型詳情可訪問 platform.sensenova.cn 註冊。&lt;/p&gt; 
&lt;p&gt;商湯將為從 Claude 遷移到「日日新」的新用户贈送 5000 萬 Tokens 體驗包；同時為用户提供專屬搬家顧問，提供遷移系列培訓，讓新用户入駐新家舒適順利。相關模型詳情可訪問 platform.sensenova.cn 註冊。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0908/185753_qsE0_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;商湯還提供最新交互模型——日日新 SenseNova V6.5 Omni API 的免費接入測試。用户也可在應用商店下載「商量 APP」免費體驗！&lt;/p&gt; 
&lt;p&gt;另外，針對用户對高質量的編程和 Agent 工具的需求，商湯小浣熊還將提供 300,000 元會員權益，所有用户均可掃描文末二維碼領取。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370978</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370978</guid>
      <pubDate>Sun, 07 Sep 2025 10:58:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>英偉達推出通用深度研究（UDR）系統</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;英偉達&lt;span&gt;最新&lt;/span&gt;發佈另外一個通用深度研究（UDR）系統，目前仍處於原型階段。該系統不僅可以與任何大語言模型 (LLM) 兼容，更為用户提供了高度定製的深度研究策略，徹底改變了以往研究智能體的工作方式。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;根據英偉達的&lt;span&gt;最新&lt;/span&gt;論文，UDR 系統的核心優勢在於其極強的靈活性。過去，深度研究智能體往往依賴硬編碼的方式，用户只能使用固定的工具和策略進行研究，無法進行個性化調整。而 UDR 系統的推出，意味着用户可以隨心所欲地創建、編輯和優化自己的研究策略，甚至無需進行額外的模型訓練。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="351" src="https://oscimg.oschina.net/oscnet/up-7461c3da8a2ceb3b17c20d0c5f84c7d2fba.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;UDR 系統配備了一個用户友好的界面，方便用户輸入研究提示，隨時更新進度並查看最終報告。與傳統的對話式 LLM 不同，UDR 能夠在研究過程中持續向用户反饋進展，極大提升了研究效率。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;值得一提的是，UDR 系統在設計上將研究邏輯與語言模型解耦，使開發者能夠靈活選擇&lt;span&gt;最先&lt;/span&gt;進的 AI 模型，並將其與量身定製的研究方案結合使用。這種創新的組合方式，讓用户能夠創造出更強大、更具適應性的深度研究工具。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;儘管 UDR 系統具有諸多優點，但仍存在一些需要改進的地方。系統的準確性依賴於底層 AI 模型生成代碼的質量，同時用户設計的研究策略必須合理可行，否則可能導致生成的報告質量低下。此外，當前版本在執行過程中缺乏用户幹預的能力，所有決策都需在研究開始前預設，限制了靈活性。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;研究人員已提出了進一步的改進方案，包括提供可修改的策略庫和更靈活的用户控制功能。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370976</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370976</guid>
      <pubDate>Sun, 07 Sep 2025 10:39:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>rainfrog - 命令行數據庫工具</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                            &lt;p&gt;&lt;span style="background-color:#ffffff; color:#1f2328"&gt;Rainfrog 的目標是提供一個輕量級的、基於終端的數據庫交互工具。該項目目前處於測試階段。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;特性：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;通過類似 vim 的鍵綁定和鼠標控制實現高效導航&lt;/li&gt;
&lt;li&gt;具有關鍵字高亮顯示、會話歷史記錄和收藏夾的查詢編輯器&lt;/li&gt;
&lt;li&gt;快速複製數據、過濾表以及在模式之間切換&lt;/li&gt;
&lt;li&gt;查看錶元數據和屬性的快捷方式&lt;/li&gt;
&lt;li&gt;跨平台（macOS、linux、windows、android 通過 termux）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img alt="" height="278" src="https://static.oschina.net/uploads/space/2025/0905/115915_L0YY_4252687.gif" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

                                                                    &lt;/div&gt;
                                                                </description>
      <link>https://www.oschina.net/p/rainfrog</link>
      <guid isPermaLink="false">https://www.oschina.net/p/rainfrog</guid>
      <pubDate>Sun, 07 Sep 2025 10:11:00 GMT</pubDate>
    </item>
    <item>
      <title>騰訊混元翻譯模型 Hunyuan-MT-7B 登頂開源熱榜</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;騰訊混元&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F19W9SEUxq7nuYQvVJz8faA" target="_blank"&gt;宣佈&lt;/a&gt;，混元翻譯模型 Hunyuan-MT-7B 登頂 Hugging Face 模型趨勢榜第一位。官方表示，該模型和混元世界模型家族最新成員 HunyunWorld-Voyager 一起，拿下前三中的兩席。&lt;/p&gt; 
&lt;p&gt;Hunyuan-MT-7B 於 9 月 1 日開源，其總參數量僅 7B，支持 33 個語種、5 種民漢語言/方言互譯，是一個能力全面的輕量級翻譯模型。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1506" src="https://static.oschina.net/uploads/space/2025/0908/175938_Dkn8_2720166.png" width="1216" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在 8 月底結束的國際計算語言學協會（ACL）WMT2025 比賽中，Hunyuan-MT-7B（參賽名稱：Shy-hunyuan-MT）拿下了全部 31 個語種比賽中的 30 個第 1 名，處於絕對領先地位。&lt;/p&gt; 
&lt;p&gt;這 31 個語種除了中文、英語、日語等常見語種，也包含捷克語、馬拉地語、愛沙尼亞語、冰島語等小語種。&lt;/p&gt; 
&lt;p&gt;騰訊混元表示，在業界常用的翻譯能力測評數據集 Flores200 上，騰訊混元 Hunyuan-MT-7B 模型也有卓越的效果表現，明顯領先於同尺寸模型，與超大尺寸模型效果對比也不遜色。&lt;/p&gt; 
&lt;p&gt;針對翻譯場景，騰訊混元提出了一個完整的翻譯模型訓練範式，覆蓋從預訓練、到 CPT 再到監督調參、翻譯強化和集成強化全鏈條，使得模型的翻譯效果達到業界最優。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370969</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370969</guid>
      <pubDate>Sun, 07 Sep 2025 10:01:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>上海發佈 AI 廣告扶持政策：最高 500 萬補貼大模型</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span&gt;上海市近日發佈了《上海市支持人工智能賦能廣告業創新發展的若干措施》，旨在通過一系列具體的扶持政策，推動人工智能技術在廣告行業的深度應用和發展。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;strong&gt;核心扶持措施概覽&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;新政策的核心在於「AI+數字廣告」生產要素的強化支持，具體措施包括:&lt;/p&gt; 
&lt;ul style="margin-left:0; margin-right:0"&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;strong&gt;大模型私有化部署補貼:&lt;/strong&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;對於採用第三方大模型進行私有化部署，並將其應用於廣告垂類領域的數字廣告企業，上海市將提供&lt;span&gt;最高&lt;/span&gt;可達核定合同額&lt;strong&gt;50%&lt;/strong&gt;，&lt;span&gt;最高&lt;/span&gt;&lt;strong&gt;500 萬元&lt;/strong&gt;的補貼。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;strong&gt;語料研發與應用補貼:&lt;/strong&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;鼓勵企業購買非關聯方的語料進行廣告垂類應用和「智能體」等研發。對於此類投入，企業可獲得&lt;span&gt;最高&lt;/span&gt;核定合同額&lt;strong&gt;30%&lt;/strong&gt;，&lt;span&gt;最高&lt;/span&gt;&lt;strong&gt;500 萬元&lt;/strong&gt;的補貼。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;strong&gt;算力租用支持:&lt;/strong&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;此外，有條件的區政府還將對租用算力的數字廣告企業提供支持，按實際投入的&lt;strong&gt;30%&lt;strong&gt;比例，給予單個主體年度&lt;span&gt;最高&lt;/span&gt;&lt;/strong&gt;2000 萬元&lt;/strong&gt;的支持。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;這一系列政策的出台，不僅體現了上海市搶佔「AI+廣告」產業制高點的決心，也旨在通過真金白銀的投入，降低企業在技術研發和部署上的成本，激發市場的創新活力。通過支持大模型私有化部署、語料研發和算力投入，上海正着力打造一個集技術、數據和算力於一體的完整 AI 廣告生態系統。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;這些措施預計將吸引更多 AI 技術公司和傳統廣告企業在上海落地和發展，加速人工智能在廣告創意、內容生成、精準投放等環節的深度融合，從而推動整個廣告行業的數字化和智能化轉型。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370959</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370959</guid>
      <pubDate>Sun, 07 Sep 2025 09:25:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>HuggingFace 開源 FinePDFs 與 FineVision 數據集</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Hugging Face 開源了兩個大規模數據集 FinePDFs 和 FineVision，前者是目前最大的公開 PDF 語料庫，後者則專為視覺-語言模型訓練設計，旨在顯著提升開源模型的能力。&lt;/p&gt; 
&lt;p&gt;https://huggingface.co/datasets/HuggingFaceFW/finepdfs&lt;br&gt; https://huggingface.co/datasets/HuggingFaceM4/FineVision&lt;/p&gt; 
&lt;p&gt;FinePDFs 是目前最大的公開 PDF 語料庫，完全由 PDF 文件構建，包含約 3 萬億 tokens，覆蓋 4.75 億份文檔、1733 種語言，數據量 3.65TB。&lt;/p&gt; 
&lt;p&gt;語料來自 105 個 CommonCrawl 快照（2013 夏—2025 年 2 月），經 datatrove 庫去重、過濾與 PII 匿名化，採用 ODC-By 1.0 許可證。文檔平均長度接近 HTML 數據集的兩倍，長於 10 萬，字符的樣本顯著，可用於提升開源 LLM 的長上下文能力。&lt;/p&gt; 
&lt;p&gt;數據集已按語言-腳本對劃分，978 種語言超 100 萬 tokens，66 種語言超 10 億 tokens。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-7cbae8687f50206187cf62b7ba1d65da7be.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;FineVision 面向視覺-語言模型訓練，整合 200 餘個來源，含 1730 萬張圖像、2430 萬樣本、8890 萬輪對話、95 億回答 tokens，支持 GUI 導航、指向、計數等新能力。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-945829421e543e2f159fb676f6f537bbadb.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;官方稱在 10 項基準上帶來 20% 以上提升，可顯著增強開源 VLM 性能。數據已轉為 Parquet，總量約 4.48 TB，支持流式加載。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370951</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370951</guid>
      <pubDate>Sun, 07 Sep 2025 09:15:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>宇樹科技衝刺 IPO 將影響機器人產業格局</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;近日，國內機器人領域頭部企業宇樹科技宣佈，預計在 2025 年 10 月份至 12 月份期間向證券交易所提交 IPO 申請文件。這一消息在科技界和資本市場引發了廣泛關注。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;作為人形機器人商業化落地的標杆企業，宇樹科技衝刺 IPO，有望成為影響機器人產業格局的關鍵節點。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;首先，宇樹科技衝刺 IPO，有望向市場證明其技術商業化的可行性。公司 2024 年營收突破 10 億元，且連續 4 年實現盈利，其中，2024 年四足機器人貢獻了 65% 的收入，驗證了消費級場景的變現能力。若成功上市，通過完整披露研發數據、客户結構及成本模型，宇樹科技將進一步證明其技術護城河並非只是「實驗室成果」。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;機器人企業不僅要注重技術研發，還要重視商業化落地。通過拓展應用場景，開發滿足市場需求的產品，實現技術的商業價值轉化，才能獲得穩定的收入，增強資本吸引力。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;其次，宇樹科技衝刺 IPO，將成為機器人產業鏈價值重估的催化劑，持續推動上游精密製造、中游系統集成、下游場景運營的全鏈條資本化，形成「技術—資本—產業」正循環，從而進一步優化產業鏈。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;目前，宇樹科技已經實現電機、減速器、控制器等核心部件全棧自研，國產化率超 90%。業內預計，宇樹科技或將募資重點投向高扭矩密度電機、輕量化材料等領域，以打破機器人規模化應用的成本瓶頸。上市後，宇樹科技勢必會通過融資擴大產能，相關供應鏈企業有望迎來訂單放量的黃金機遇，上下游協同的良性生態有望加速形成。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;最後，宇樹科技選擇 IPO，本質上是資本效率與技術週期的精準匹配。2025 年 6 月份，宇樹科技完成 C 輪融資，投後估值已達 120 億元。該輪融資由中國移動旗下基金、騰訊、錦秋基金、阿里巴巴、螞蟻集團和吉利資本共同領投。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;宇樹科技衝刺 IPO，是機器人產業加速資本化的縮影。相信在資本市場與機器人產業的「雙向奔赴」中，中國機器人企業將大幅提升競爭力。（證券日報）&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370948</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370948</guid>
      <pubDate>Sun, 07 Sep 2025 09:05:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>未來可能有高達 50% 的入門級工作將被 AI 取代</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;隨着人工智能（AI）的迅速發展，許多公司正在經歷前所未有的變革。曾經的職場成功故事，如 Hewlett Packard Enterprise 的首席執行官安東尼・內裏 (Antonio Neri) 從客服代理晉升為 CEO，正在逐漸被 AI 的興起所取代。分析師預測，未來可能有高達 50% 的入門級工作將被 AI 取代，這意味着許多剛剛步入職場的大學畢業生將面臨前所未有的挑戰。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在一項針對公共科技公司和成長中的風險投資企業的研究中，數據顯示，從 2019 年到 2024 年，具有不到一年工作經驗的求職者的就業機會下降了 50%。這一趨勢影響到了銷售、市場營銷、工程、招聘、運營、設計、財務和法律等各個核心職能。這種變化不僅影響了求職者，也讓企業面臨重新培養人才的壓力。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;儘管如此，行業專家指出，這種失去入門級崗位的情況可能促使組織內部的人才培養模式發生改變。隨着公司的結構變得更加扁平化，入門級崗位可能會轉變為更高要求的技能角色，要求求職者在進入職場前具備更多的能力。雖然對於即將畢業的學生來説，這意味着他們需要自行掌握這些技能，但也可能成為他們在競爭激烈的求職市場中脱穎而出的優勢。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;各大高校也在積極調整課程，旨在為學生提供與 AI 相關的技能培訓。雖然技術進步可能在短期內對就業率產生影響，但歷史上技術革新在長期內並未導致大規模的失業。專家認為，當前大學畢業生面臨的挑戰，可能在未來幾年內影響他們的職業發展。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;然而，儘管有許多未知數，許多經濟學家認為 AI 對勞動市場的長期影響仍然具有高度的不確定性，企業和社會將需要時間來適應這一變化。隨着技術的不斷進步和 AI 的普及，職場的未來可能會迎來全新的模式，而不僅僅是對現有職場階梯的替代。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370944</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370944</guid>
      <pubDate>Sun, 07 Sep 2025 08:47:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>英偉達收購 AI 編程初創公司 Solver</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.theinformation.com%2Fbriefings%2Fnvidia-acquires-coding-startup-solver" target="_blank"&gt;據 The Information 報道&lt;/a&gt;，英偉達最近完成了對 AI 編程公司 Solver 的收購，進一步強化其在 AI 全棧生態的佈局。&lt;/p&gt; 
&lt;p&gt;Solver 成立於 2022 年，前身為 Laredo Labs，專注於 AI Coding Agent，其產品能通過自然語言指令管理完整代碼庫（如生成、測試、修復代碼），而非僅代碼補全。公司曾獲 800 萬美元融資，創始團隊包括前 Siri 和三星 Viv Labs 核心成員。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0908/161953_FkOn_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Solver 的技術突破在於學習「超過一億個軟件項目的開發歷史」，理解代碼演進邏輯，可執行復雜任務（如重構模塊、修復系統性漏洞）。其 API 支持多語言（Python、JavaScript 等），並與主流開發工具無縫集成。&lt;/p&gt; 
&lt;p&gt;此次收購是英偉達 2024-2025 年系列收購的關鍵一環，旨在構建「硬件+軟件+雲服務」全棧生態。Solver 將整合至英偉達開發者工具鏈（如 CUDA、NVIDIA AI Enterprise），降低 AI 應用開發門檻，反向驅動 GPU 需求增長。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370938/nvidia-acquires-coding-startup-solver</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370938/nvidia-acquires-coding-startup-solver</guid>
      <pubDate>Sun, 07 Sep 2025 08:20:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>OpenAI 重組 ChatGPT 「模型行為團隊」</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;OpenAI 內部郵件確認，原「模型行為團隊」（Model Behavior）整體併入「後訓練團隊」（Post Training），直接向該團隊負責人 Max Schwarzer 彙報。此舉旨在把 AI 個性、安全與用户體驗研究更深地嵌入核心模型開發流程，為 GPT-5 後續版本提供更快的迭代支持。&lt;/p&gt; 
&lt;p&gt;該團隊原有 14 人，長期負責減少諂媚、平衡政治偏見、定義聊天語氣等「人格化」工作。與此同時，模型行為團隊創始負責人 Joanne Jang 宣佈轉崗，啓動新項目 OAI Labs，探索超越傳統聊天窗口的人機協作界面。Jang 稱，新實驗室將「讓 AI 成為思考、創作、遊戲、學習和連接的工具」。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0908/160802_ml0W_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0908/160900_9Cth_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;業內分析指出，此次重組反映出 OpenAI 對「模型性格」商業化影響的重視：用户反饋 GPT-5「過於冷淡」或「過度迎合」後，公司已臨時開放舊模型訪問權限，並加速個性調優。同期發表的 OpenAI 研究論文也警告，行業慣用的「應試型」評估可能鼓勵模型幻覺，未來需在評分機制中引入「不確定性誠實」指標。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370934</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370934</guid>
      <pubDate>Sun, 07 Sep 2025 08:10:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Tilde AI 發佈開源 TildeOpen LLM</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;Latvian 語言技術公司 Tilde 發佈了 TildeOpen LLM，這是一個開源的基礎大語言模型（LLM），旨在支持歐洲語言，特別是那些較少被代表的國家和地區語言。這一舉措標誌着歐盟在語言公平和數字主權方面邁出了重要的一步。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="290" src="https://oscimg.oschina.net/oscnet/up-a3afc0c462ebfde5158ba6a9fda49510c9d.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;TildeOpen LLM 是一個擁有 300 億參數的稠密解碼器模型，採用了 CC-BY-4.0 的寬鬆許可證，能夠支持從拉脱維亞語、立陶宛語到烏克蘭語、土耳其語等多種語言。該模型的訓練是在歐洲的&lt;span&gt;超級&lt;/span&gt;計算機 LUMI（芬蘭）和 JUPITER 上進行的，使用了歐盟委員會的大型人工智能大獎挑戰賽所提供的 200 萬 GPU 小時的計算資源。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在技術細節方面，TildeOpen LLM 通過受 EleutherAI 啓發的 GPT-NeoX 腳本進行訓練，共進行了 45 萬次更新，使用了約 2 萬億個令牌。其訓練過程包含三階段採樣：首先在語言間均勻分佈，其次是對高數據量語言的自然分佈進行增強，最後再進行均勻的掃查以確保平衡。模型的超參數包括 60 層、嵌入維度 6144、48 個注意力頭、8192-token 的上下文窗口，以及使用 SwiGLU 激活、RoPE 位置編碼和 RMSNorm 層規範化。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在語言公平和數據主權方面，傳統的主流模型往往側重於英語和其他主要語言，導致在處理波羅的海、斯拉夫及其他較小的歐洲語言時表現不佳，常常出現語法錯誤和奇怪的措辭。而 TildeOpen 通過引入 「公平的標記器」，使得不同語言的文本以相似方式進行表示，從而減少標記數量，提高較少代表語言的推理效率。此外，組織可以選擇在本地數據中心或符合歐盟要求的安全雲中自我託管，確保遵循 GDPR 及其他數據保護法規，從而解決了與美國或亞洲託管模型相關的主權問題。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;TildeOpen 作為基礎模型，預計會推出更多專門化版本，例如經過指令調優的翻譯模型，這將進一步增強其功能。拉脱維亞通過 Tilde 的努力，期望在全球科技領域佔據一席之地，同時致力於保護語言多樣性。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370933</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370933</guid>
      <pubDate>Sun, 07 Sep 2025 08:08:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>合理選擇任務調度的路由策略，可以幫助降本 50%</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                                        &lt;p&gt;作者：黃曉萌（學仁）&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" height="225" src="https://oscimg.oschina.net/oscnet/up-95c58fbed31f7c7be0c1d4fb9dcd324f094.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h2_1"&gt;&lt;/span&gt; 
&lt;h2&gt;概述&lt;/h2&gt; 
&lt;p&gt;有許多的業務場景需要用到短週期的任務，比如：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;訂單異步處理：每分鐘掃描超時未支付的訂單進行訂單處理。&lt;/li&gt; 
 &lt;li&gt;風險監控：每分鐘掃描 metrics 指標，發現異常進行報警。&lt;/li&gt; 
 &lt;li&gt;數據同步：每天晚上同步庫存、門店信息。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;任務調度系統負責管理這些短週期的任務，通過用户設置的調度時間，週期性的把任務分發給執行器執行。每次任務要分發給哪個執行器執行，就是由路由策略決定的。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-bfa179d2d832985259fd56c4cb3164f3350.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;不同任務處理不同的業務邏輯，有些執行時間長，有些執行時間短，有些消耗資源多，有些消耗資源少。如果選擇的路由策略不合適，可能會導致集羣中執行器負載分配不平均，資源利用率上不去，成本上升，甚至產生穩定性故障。&lt;/p&gt; 
&lt;span id="OSC_h2_2"&gt;&lt;/span&gt; 
&lt;h2&gt;輪詢（Round Robin）&lt;/h2&gt; 
&lt;p&gt;輪詢（Round Robin）路由策略是一種簡單且常見的負載均衡方法，其核心原理是按照順序將請求或任務依次分發到後端節點上，從而確保任務的平均分佈，避免資源過度集中在某一節點上。具體實現方式通常是維護一個計數器，記錄當前分配的節點索引。分發請求時，該索引遞增並對節點總數取模，從而實現循環分配。在任務調度系統中，分為任務級別輪詢和應用級別輪詢。&lt;/p&gt; 
&lt;span id="OSC_h3_3"&gt;&lt;/span&gt; 
&lt;h3&gt;任務級別輪詢&lt;/h3&gt; 
&lt;p&gt;代表產品是 XXLJOB &lt;strong&gt;[&lt;/strong&gt; &lt;strong&gt;1]&lt;/strong&gt; ，為每個任務都維護了一個計數器。比如有 ABC 三個執行器。每個任務調度的時候都會按照 A-&amp;gt;B-&amp;gt;C-&amp;gt;A 這個順序輪詢機器。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;如果所有任務的調度時間都一致（比如每小時執行一次），會導致所有任務每次執行都落在同一台後端節點上，負載嚴重不均衡。為瞭解決這個問題，XXL-JOB 初始化每個任務計數器的時候，做了隨機，可以大大降低該問題的概率。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;pre style="margin-left:0; margin-right:0; text-align:justify"&gt;&lt;code&gt;&lt;span&gt;AtomicInteger count = routeCountEachJob.&lt;span style="color:#ca7d37"&gt;get&lt;/span&gt;(jobId);&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;&lt;span style="color:#ca7d37"&gt;if&lt;/span&gt;&amp;nbsp;(count ==&amp;nbsp;&lt;span style="color:#0e9ce5"&gt;null&lt;/span&gt;&amp;nbsp;|| count.&lt;span style="color:#ca7d37"&gt;get&lt;/span&gt;() &amp;gt;&amp;nbsp;&lt;span style="color:#0e9ce5"&gt;1000000&lt;/span&gt;) {&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp;&lt;em&gt;// 初始化時主動 Random 一次，緩解首次壓力&lt;/em&gt;&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp; count =&amp;nbsp;&lt;span style="color:#ca7d37"&gt;new&lt;/span&gt;&amp;nbsp;AtomicInteger(&lt;span style="color:#ca7d37"&gt;new&lt;/span&gt;&amp;nbsp;Random().nextInt(&lt;span style="color:#0e9ce5"&gt;100&lt;/span&gt;));&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;}&amp;nbsp;&lt;span style="color:#ca7d37"&gt;else&lt;/span&gt;&amp;nbsp;{&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp;&lt;em&gt;// count++&lt;/em&gt;&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp; count.addAndGet(&lt;span style="color:#0e9ce5"&gt;1&lt;/span&gt;);&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;如果任務的調度頻率不一致，因為每個任務都按照自己計數器輪詢，也有可能在某個週期，大部分任務都調度到同一個執行器上，導致負載不均衡。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="OSC_h3_4"&gt;&lt;/span&gt; 
&lt;h3&gt;應用級別輪詢&lt;/h3&gt; 
&lt;p&gt;整個應用下所有任務共享同一個計數器，每個任務調度的時候，都會讓計數器+1。該算法可以保證所有執行器接收到的任務次數是平均的。如果所有任務負載和執行時間差不多，是負載均衡的，但是如果有大任務和小任務並存，情況又不一樣了。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-4d9dff721251ff1fbb2639b630c2867d23c.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;如上圖所示，&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;有 ABC 三個執行器，job1~job6 一共 6 個任務需要依次調度，其中 job1 和 job4 是大任務。&lt;/li&gt; 
 &lt;li&gt;job1 調度到 A 節點，job2 調度到 B 節點，job3 調度到 C 節點，job4 調度到 A 節點，job5 調度到 B 節點，job6 調度到 C 節點。&lt;/li&gt; 
 &lt;li&gt;job1 和 job4 這 2 個大任務，每次都調度到 A 節點，導致 A 節點負載比其他節點高。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;span id="OSC_h3_5"&gt;&lt;/span&gt; 
&lt;h3&gt;階段總結&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;如果所有任務負載和執行時間差不多，建議選擇應用級別輪詢。&lt;/li&gt; 
 &lt;li&gt;如果有大任務和小任務存在，這兩種算法都有可能導致負載不均衡，建議給大任務配置任務級輪詢，防止每次都落到同一台節點上。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="OSC_h2_6"&gt;&lt;/span&gt; 
&lt;h2&gt;隨機&lt;/h2&gt; 
&lt;p&gt;隨機路由策略是一種簡單的負載均衡算法，它通過隨機選擇一個後端服務器來處理每個請求，來達到負載均衡的目的。在任務調度系統中，任務每次調度的時候，隨機選一個執行器執行。&lt;/p&gt; 
&lt;p&gt;隨機路由策略由於算法完全依賴隨機數生成器，負載均衡全憑運氣，如果拉長時間區間（比如看一整天的調度情況）看可能是負載均衡的，但是可能存在短時間負載不均的問題（某些服務器在一定時間段內被選中的概率較高）。&lt;/p&gt; 
&lt;span id="OSC_h2_7"&gt;&lt;/span&gt; 
&lt;h2&gt;最近最少使用（LFU）&lt;/h2&gt; 
&lt;p&gt;最近最少使用（LFU，Least Frequently Used）是一種基於訪問頻率的緩存淘汰算法，主要用於內存管理和緩存系統中。其實現機制是為每個數據項維護一個訪問計數器，數據被訪問時計數器加 1，當需要淘汰數據時，選擇計數器值最小的數據項。&lt;/p&gt; 
&lt;p&gt;在任務調度系統中，可以統計執行器的調度次數，優先選擇最近使用次數最少的執行器進行任務調度，從而達到負載均衡的目的。如果所有任務都配置成 LFU 路由策略，該算法最終使用效果，和輪詢算法是差不多的，算法還複雜，沒有太大必要。如果集羣中的任務配置了多種路由策略，不同執行器調度次數不一樣，出現了負載不均衡的情況，給新任務配置 LFU 算法，一定能調度到調度次數最少的執行器上，才能真正發揮它的作用。&lt;/p&gt; 
&lt;p&gt;開源 XXLJOB 具體實現上，使用的是任務級別的 LFU，最終使用效果和任務級別輪詢一致。&lt;/p&gt; 
&lt;span id="OSC_h2_8"&gt;&lt;/span&gt; 
&lt;h2&gt;最近最久未使用（LRU）&lt;/h2&gt; 
&lt;p&gt;最近最久未使用 (LRU,Least Recently Used) 是一種基於訪問時間的緩存淘汰算法，主要用於管理有限緩存空間內的內存數據，當緩存已滿時，依據數據最近使用時間，優先移除最近最久未使用的數據。&lt;/p&gt; 
&lt;p&gt;在任務調度系統中，可以統計執行器的調度時間，優先選擇最久未調度的執行器進行任務調度，從而達到負載均衡的目的。因為每次調度的時候，也會更新執行器調度次數，所以該算法最終使用效果，和 LFU 是差不多的。&lt;/p&gt; 
&lt;p&gt;開源 XXLJOB 具體實現上，使用的是任務級別的 LRU，最終使用效果和任務級別輪詢一致。&lt;/p&gt; 
&lt;span id="OSC_h2_9"&gt;&lt;/span&gt; 
&lt;h2&gt;一致性哈希&lt;/h2&gt; 
&lt;p&gt;有些業務場景，需要任務每次執行在固定的機器上，比如執行器上有緩存，可以較少下游數據加載、加快任務處理速度。最直接的想法就是使用哈希算法，通過任務 ID（JobId）和執行器數量取 mod，把任務調度到固定的機器上。但是如果某個執行器掛掉了，或者執行器擴容的時候，執行器數量發生了變更，會導致所有任務重新哈希到了不同的機器上，所有緩存全部失效，可能會導致後端流量一下子突增。&lt;/p&gt; 
&lt;p&gt;XXLJOB 提供了一致性哈希路由算法，可以保證執行器掛掉或者擴容的時候，大部分任務調度的執行器不變。&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;一致性哈希算法將 2^32 劃分為一個邏輯環，其中每個執行器節點根據哈希值被映射到環上的某個位置。任務通過 JobId 做哈希也映射到環上，然後順時針找到最近的執行器，即為目標執行器。如下圖所示，job1 固定調度到執行器 A，job2 和 job3 固定調度到執行器 B，job4 固定調度到執行器 C：&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-0ad01a71277a9f227ced3b876fa4a2360dd.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;如果添加一個執行器 D，通過哈希值映射到了 job2 和 job3 中。如下圖所示，job2 會調度到執行器 D 上，其他任務調度的機器保持不變：&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-19e2de7321e73d275cb09c27e427f7a4793.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;如果執行器 C 突然掛掉了，如下圖所示，job4 會調度到執行器 A 上，其他任務調度的機器保持不變：&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-49f2dd8255ad7f43faeb56e5e0a37506e9f.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;如果執行器節點不多，直接映射到哈希環上的時候，有可能無法平均分佈，導致任務分配不均。為瞭解決這個問題，可以引入虛擬節點（XXLJOB 引入了 100 個虛擬節點），將虛擬節點平均分佈在哈希環上，然後物理節點映射虛擬節點。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-0af27b26f0bd8673df6d331804136865584.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;如上圖所示，一共有 3 個執行器 ABC，每個執行器分配 4 個虛擬節點，保證所有虛擬節點平均分佈在哈希環上，這樣所有任務調度就基本上負載均衡了。&lt;/p&gt; 
&lt;span id="OSC_h2_10"&gt;&lt;/span&gt; 
&lt;h2&gt;負載最低路由策略&lt;/h2&gt; 
&lt;p&gt;上面提到的路由策略都沒法解決一個問題，就是應用下同時存在大任務和小任務，導致執行器負載不均衡。那麼我們是否可以採集所有執行器的負載，每次調度的時候按照負載最低優先調度呢？確實有些調度系統是這樣做的，代表產品是 Kubernets &lt;strong&gt;[&lt;/strong&gt; &lt;strong&gt;2]&lt;/strong&gt; 。Kubernetes 使用 kube-scheduler 進行調度，本質上是把 Pod 調度到合適的 Node 上，默認策略就是調度到負載最低的 Node 上。在容器服務中，每個 Pod 都會預製佔用 cpu 和內存，kube-scheduler 每調度一個 Pod，就能實時更新所有 Node 的負載，該算法沒有問題。&lt;/p&gt; 
&lt;p&gt;但是在傳統的任務調度系統中（比如 XXLJOB），一般都是通過線程運行任務的，沒法提前知道每個任務會佔用多少資源。任務調度到執行器上，也不是馬上導致執行節點負載上升，通常會有滯後性。甚至有些邏輯複雜的任務，可能好幾分鐘後才會有大量的 IO 操作，導致該執行節點好幾分鐘後才能明顯負載上升。舉個例子，有 AB 兩個執行節點：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;A 節點上當前有 1 個任務在運行，A 節點負載 20%，B 節點當前沒有任務運行，負載 0%。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-be10f131d25815e10c703a6b56b4c79afaf.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;這個時候有 job2~job5 一共 4 個任務要調度，都選擇了當時負載最低的執行器 B。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-4c2a38d0bbec16b75d051f38e62fe5263d3.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;4 個任務都發送到執行器 B 後，過了一會，執行器 B 負載上升到 100%，執行器 A 還是 5%，導致負載不均衡。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;span id="OSC_h2_11"&gt;&lt;/span&gt; 
&lt;h2&gt;任務權重路由策略&lt;/h2&gt; 
&lt;p&gt;有沒有辦法可以像 kube-scheduler 一樣，調度的時候就預算每個執行節點的負載呢？因為定時任務都是週期性運行的，每次執行的代碼或者腳本是固定的，通過業務邏輯或者歷史執行時間，我們其實是知道哪些是大任務哪些是小任務的。每次任務調度的時候，我們只要知道當前各個執行器上運行了多少個大任務多少個小任務，就能把當前這個任務分發到最合適的執行器上。&lt;/p&gt; 
&lt;p&gt;MSE-XXLJOB &lt;strong&gt;[&lt;/strong&gt; &lt;strong&gt;3]&lt;/strong&gt; 設計並實現了任務權重路由策略，每個任務都可以用户自定義權重（int 值），交互流程如下：&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-0d9160df2b04280c0c7c1f1887c088ecc37.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;如上圖所示，&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Scheduler 調度器開始調度任務。&lt;/li&gt; 
 &lt;li&gt;RouteManger 負責路由策略，如果是任務權重路由策略，去 WorkerManger 裏尋找當前權重最小的執行器，並更新該執行器的權重（+當前任務的權重）。&lt;/li&gt; 
 &lt;li&gt;RouteManger 把任務分發給權重最小的執行器執行任務。&lt;/li&gt; 
 &lt;li&gt;執行器運行完任務，更新 WorkerManger，把該執行器的權重減去該任務的權重。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;下面以一個詳細的例子來説明。當前有兩個執行器，有 ABCD 4 個任務需要調度，其中 A 是大任務，按照歷史經驗 cpu 會佔用 50%，BCD 是小任務，每個會佔用 cpu 20%。將 A 任務權重設置為 5，BCD 設置為 2。初始化執行器 1 和執行器 2 的權重都是 0。&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;A 任務調度到執行器 1，執行器 1 的權重變為 5，執行器 2 的權重還是 0。B 任務選擇任務權重最小的機器，調度到執行器 2，執行器 2 的權重變為 2。C 任務選擇任務權重最小的機器，調度到執行器 2，執行器 2 的權重變為 4。D 任務選擇任務權重最小的機器，調度到執行器 2，執行器 2 的權重變為 6。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-294a2873a5488022b9503489236d079c705.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;這個時候，又有個小任務 E 要調度，權重也是 2，選擇當前權重最小的機器 1，則機器 1 的權重變為了 7，如下圖&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-0cb6bdad28fca8ad7ad85a7041fd24a8586.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;小任務 B 和 C 執行很快就跑完了，這個時候執行器 2 的權重減去 4，變為了 2，如下圖：&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-a8fbdbf6a298620ffc183e8eb4853863b44.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;這個時候又來個大任務 F，權重是 5，選擇權重最小的機器 2，機器 2 的任務權重變為了 7，如下圖：&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-d431902415007920bbd5594f3d235dfe1c6.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ol start="5"&gt; 
 &lt;li&gt;可以看到該算法，將每個任務實際消耗的資源映射成任務權重，可以實時統計每個執行器的權重，提前規劃不同權重的任務分配到哪個執行器上去執行，達到全局最優解。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;span id="OSC_h2_12"&gt;&lt;/span&gt; 
&lt;h2&gt;總結&lt;/h2&gt; 
&lt;p&gt;在真實生產環境下，不同的任務處理不同的業務邏輯，如果選錯路由策略，可能會導致集羣中大部分執行節點負載不到 10%，但是個別節點負載會衝高到 100%，雖然平均負載不高，但是也無法減少規格，可能還需要增大規格防止出穩定性問題。但是如果選對了路由策略，集羣所有節點負載均衡，就可以減少節點規格，成本降低 50% 以上。下面以一個表格詳細介紹不同路由算法的場景：&lt;/p&gt; 
&lt;p&gt;&lt;img height="352" src="https://oscimg.oschina.net/oscnet/up-135f88cb4ca465c1c8dfa02b9b2f2522ea3.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;相關鏈接：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;[1] XXLJOB&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fxuxueli%2Fxxl-job" target="_blank"&gt;https://github.com/xuxueli/xxl-job&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;[2] Kubernets&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.csdn.net%2Fjy02268879%2Farticle%2Fdetails%2F148855464" target="_blank"&gt;https://blog.csdn.net/jy02268879/article/details/148855464&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;[3] MSE-XXLJOB&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhelp.aliyun.com%2Fzh%2Fschedulerx%2Fschedulerx-xxljob%2Fgetting-started%2Fcreate-and-deploy-a-xxljob-job-in-a-container-in-10-minutes" target="_blank"&gt;https://help.aliyun.com/zh/schedulerx/schedulerx-xxljob/getting-started/create-and-deploy-a-xxljob-job-in-a-container-in-10-minutes&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/3874284/blog/18689928</link>
      <guid isPermaLink="false">https://my.oschina.net/u/3874284/blog/18689928</guid>
      <pubDate>Sun, 07 Sep 2025 07:55:00 GMT</pubDate>
      <author>原創</author>
    </item>
    <item>
      <title>OpenBMB 發佈並開源 MiniCPM 4.1-8B</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;OpenBMB&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FOJq_nfcFPIGTEeExQpT9GA" target="_blank"&gt;正式推出並開源&lt;/a&gt;&amp;nbsp;MiniCPM4.1-8B，這是首個開源的混合推理大語言模型，該模型通過系統性創新實現了端側極致效率，支持深度推理模式與非推理模式一鍵切換。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0908/155239_yTBq_2720166.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;該系列在 8B 參數規模下通過模型架構、訓練數據、訓練算法和推理系統四個維度的系統性創新，實現端側極致效率。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;模型亮點&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;首個原生稀疏架構的深思考模型，通過可訓練稀疏注意力創新，代碼、數學推理等任務的推理速度比同尺寸開源模型快 3 倍以上&lt;/li&gt; 
 &lt;li&gt;知識、推理、編程、指令遵循等 15 個評測基準，取得綜合平均分同尺寸模型第一&lt;/li&gt; 
 &lt;li&gt;支持高效雙頻換擋：長文本用稀疏，短文本用稠密&lt;/li&gt; 
 &lt;li&gt;端側友好，在 128K 長文本場景下，MiniCPM 4.1 相較於 Qwen3-8B 僅需 25% 的緩存存儲空間&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img height="1707" src="https://static.oschina.net/uploads/space/2025/0908/155247_ogz0_2720166.jpg" width="1280" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="1707" src="https://static.oschina.net/uploads/space/2025/0908/155318_mGYS_2720166.jpg" width="1280" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;MiniCPM4.1-8B 採用 InfLLM v2 可訓練稀疏注意力機制，在 128K 長文本場景下每個 token 僅與不到 5% 的 token 計算相關性，顯著降低長文本計算開銷；原生支持 65,536 token 上下文，通過 LongRoPE 可擴展至 131,072 token。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1707" src="https://static.oschina.net/uploads/space/2025/0908/155352_SHVT_2720166.jpg" width="1280" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;目前，模型已在 Github、Hugging Face、魔搭社區開源&lt;/p&gt; 
&lt;p&gt;🔗Github：https://github.com/OpenBMB/MiniCPM&lt;br&gt; 🔗Hugging Face:&amp;nbsp;https://huggingface.co/openbmb/MiniCPM4.1-8B&lt;br&gt; 🔗ModelScope:https://modelscope.cn/models/OpenBMB/MiniCPM4.1-8B&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370929</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370929</guid>
      <pubDate>Sun, 07 Sep 2025 07:55:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>光刻機巨頭 ASML 領投 Mistral AI 17 億歐元 C 輪融資</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;2025 年 9 月 8 日，荷蘭光刻機巨頭阿斯麥（ASML）&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fa16z.com%2Fannouncement%2Finvesting-in-mistral%2F" target="_blank"&gt;宣佈&lt;/a&gt;領投法國 AI 初創公司 Mistral AI 的 17 億歐元 C 輪融資，注資 13 億歐元（約 15 億美元），成為其最大股東並獲董事會席位。融資後 Mistral AI 估值達 100 億歐元（約 117 億美元），成為歐洲估值最高的人工智能企業。&lt;/p&gt; 
&lt;p&gt;&lt;img height="659" src="https://static.oschina.net/uploads/space/2025/0908/152256_Qedv_2720166.png" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;作為先進芯片製造設備的關鍵供應商，ASML 的這筆投資旨在強化歐洲科技主權，減少歐洲對美國和中國 AI 模型的依賴。Mistral AI 自 2023 年成立以來發展迅速，被視為法國乃至歐洲 AI 領域的領軍者，競爭對手包括美國的 OpenAI 和谷歌等科技巨頭。 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-fa260b31072a0d42e879961b65f0267b980.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;通過此次合作，ASML 不僅能借助 Mistral AI 的數據分析和 AI 能力提升自身設備性能，還有望助力歐洲在全球科技競爭中脱穎而出。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370920</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370920</guid>
      <pubDate>Sun, 07 Sep 2025 07:23:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>MiniMax 啓動期權增發獎勵</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;MiniMax 已啓動期權增發事宜，根據員工對公司的貢獻程度不同，激勵在幾十萬美金到幾百萬美金不等；涵蓋算法、工程等全序列核心貢獻員工，以鼓勵員工大膽追求 AGI。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;據瞭解，本次期權發放來自於全員會口頭通知，不僅模型算法，產品、市場、增長、職能等崗位都在其中（包含正職和實習生）；後續還會繼續對突出貢獻者進行即時期權激勵。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="221" src="https://oscimg.oschina.net/oscnet/up-6c4759bf41e6427dd7c2ba90a2bac56ab55.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;官網資料顯示，MiniMax 成立於 2022 年初，以「與所有人共創智能」為使命，致力於推動人工智能科技前沿發展，實現通用人工智能 (AGI）。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;目前，MiniMax 自主研發了一系列多模態通用大模型，包括 MiniMax M1、Hailuo-02、Speech-02 和 Music-01，具備超長上下文處理能力，能夠理解、生成並整合包括文本、音頻、圖像、視頻和音樂在內的多種模態。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;並基於這些自研模型面向全球推出一系列 AI 原生產品，包括 MiniMax、海螺 AI、MiniMax Audio、星野等，以及面向企業和開發者的開放平台。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;數據表明，MiniMax 的自研多模態模型及 AI 原生應用已累計為來自超過 200 個國家及地區的逾 1.57 億名個人用户，以及來自超過 90 個國家及地區的 50,000 餘名企業客户以及開發者提供服務。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370916</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370916</guid>
      <pubDate>Sun, 07 Sep 2025 07:08:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>谷歌即將推出 「AI 模式」 作為默認搜索體驗</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;谷歌搜索正面臨一次重大的變革，「AI 模式」 即將成為默認選項。這一模式基於谷歌現有的 「AI 概述」 功能，與 ChatGPT 等其他生成式 AI 類似，允許用户在初次搜索後與 AI 繼續對話。這一轉變意味着谷歌將從一個通向網絡的信息門户，轉變為一個以生成式 AI 為中心的封閉平台。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="357" src="https://oscimg.oschina.net/oscnet/up-f7a6f986c397021bf4f6ca86279c89daf48.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;谷歌產品經理洛根・基爾帕特里克（Logan Kilpatrick）在社交平台 X 上暗示，AI 模式很快將成為新的搜索體驗。例如，訪問 google.com/ai 現在會自動跳轉至 AI 模式，而當用户詢問這一模式是否會成為默認設置時，他回應道 「很快會的 :)」。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;目前，AI 模式已在包括美國在內的 180 多個國家推出，但尚未完全取代傳統搜索。在&lt;span&gt;最新&lt;/span&gt;的更新中，谷歌為 AI 模式增加了新的代理功能，使用户能夠直接通過聊天界面預訂本地服務或購買活動門票。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;與此同時，谷歌的法律團隊在一場關於其廣告技術部門的訴訟中表示，「開放網絡已經處於快速衰退之中」。他們聲稱，拆分谷歌的廣告業務只會使情況變得更糟，尤其是廣告資金正逐漸從傳統的開放網站展示廣告轉向連接電視、零售媒體以及谷歌所稱的 「廣受歡迎的發佈平台，如 AI 聊天機器人，它們可以有效地貨幣化展示內容」。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;谷歌指出，強制拆分將加速這一轉變，擠壓那些依賴廣告收入的出版商的資源。該公司正在反對訴訟中要求出售其廣告交易所、將其拍賣邏輯開源以及放棄 50% 淨收入等要求。其他提議則要求制定新的行為規則和開放 API，以限制谷歌的市場力量並增加競爭。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;值得注意的是，谷歌在使用開放網絡衰退這一論點來為自己辯護的同時，也在推出 AI 模式並將搜索流量引向自身平台，這無疑加劇了它所警告的趨勢。最終，谷歌正利用其在這一問題上所扮演的角色，來抵制對其業務拆分的要求，這種循環邏輯在谷歌高管關於 AI 搜索如何影響開放網絡的矛盾言論中也屢見不鮮。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370913</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370913</guid>
      <pubDate>Sun, 07 Sep 2025 06:56:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Linus 對 Git 提交信息中「Link:」標籤被濫用表達不滿</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span&gt;9 月 6 日，知名 Linux 之父 Linus Torvalds 公開表達了對 Git 提交信息中「Link:」標籤濫用的不滿。他指出，近期在 Linux 內核的 Git 提交和補丁中頻繁出現的「Link:」標籤，往往沒有提供實際價值，反而浪費了開發者和維護者的時間。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;Torvalds 在一次 block 模塊的 pull request 評審中直接批評道，&lt;strong&gt;許多「Link:」標籤鏈接內容僅僅是與當前補丁內容重複的信息，並未為評審工作提供任何額外解釋或背景&lt;/strong&gt;。他強調，如果「Link:」標籤無法帶來新的、有意義的信息，比如指向相關的 bug 報告、討論串、或多補丁系列的封面信件等，最好不要添加這些無用鏈接。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="434" src="https://static.oschina.net/uploads/space/2025/0908/145316_V8JA_2720166.png" width="1148" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;https://lore.kernel.org/all/CAHk-=wjamixjqNwrr4+UEAwitMOd6Y8-_9p4oUZdcjrv7fsayQ@mail.gmail.com/&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;雖然 Torvalds 表示，在多補丁系列場景下，需要追溯到系列補丁的 cover letter 時，「Link:」標籤確實能發揮重要作用，但他呼籲大家不要在普通情況下機械地添加該標籤。為此，他打算更加嚴格審核包含無意義「Link:」標籤的合併請求，如今甚至有可能直接拒絕這類請求。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;他還建議未來如果要引入自動化輔助，不妨將 AI 等技術用於檢測相關討論，從而在需要時自動添加有實際參考價值的標籤。他直言：「我喜歡有用的鏈接，但 99% 的鏈接都只是浪費時間的垃圾。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;Torvalds 呼籲 Linux 開發者們在今後的補丁提交中，務必確保「Link:」標籤切實為代碼評審和問題追蹤帶來幫助，否則應避免使用。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;strong&gt;全文如下：&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;該死的，這次提交明明帶着那個充滿希望的"鏈接:"參數，我本以為它能解釋這個毫無意義的提交為何存在，但一如既往地，那個鏈接只是浪費我的時間——指向的又是那些老掉牙的信息。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;我原本期待它能指向某個錯誤報告之類的東西，解釋為什麼我的最初反應是錯誤的。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;別再搞這種垃圾了。別再添加浪費人時間的無用鏈接參數。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;只有當鏈接包含*額外*信息時才添加。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;該死，我真的討厭這些無用鏈接。我喜歡看到*有價值*的鏈接，但實際看到的 99% 都指向愚蠢無用的垃圾，而且*只會*浪費我的時間。又一次。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;所以我不打算合併這個請求，光是看到它就讓我惱火。若真要我合併，請給出實質解釋而非無用鏈接。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;沒錯，我正處於暴躁狀態。感覺我的主要工作——實際上唯一的工作——就是處理合併請求，因此我極度厭惡這些自動添加的垃圾內容，它們只會讓我的工作更難做。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;「所以我希望至少有某種方式能阻止這種機械、無腦的使用——在理想情況下，還可以有一種更有用、自動添加鏈接的模式。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;例如，我覺得對於多提交系列的封面信件來説，鏈接到補丁系列的提交記錄可能更有用——而且也不會那麼煩人——因為它會被加入到合併信息（merge message）中，而不是每一個具體提交中。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;因為如果有人真的在查看合併信息，他們很可能是在查找更宏觀的背景信息——或在處理某個合併衝突——此時我認為最初的提交可能就更相關。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;當然，大多數人實際上並不會在合併時使用封面信件，他們只是把補丁作為一個系列應用，所以其實也沒那麼煩人，因為它根本不會存在於 git 歷史記錄裏 ;)&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;總之，‘阻止無腦使用’可能就只是加個大大的警告，提醒大家：這個鏈接可能只會帶來煩人的負擔。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;相比之下，一個‘完美’的模式可能是實現某種自動化——‘除非真的有實際討論發生’。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;不過我覺得這種模式可能太複雜，除非真的有人願意探索用 AI，因為他們的工作描述就是‘尋找 AI 的實際用途’。在如今的科技世界中，我相信確實有人這樣定位自己的崗位。唉。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;舉例來説，既然‘b4’最終會瀏覽補丁的下游帖子以便自動添加 acked-by 等信息，我確實覺得理論上可以建立這樣一個啓發式模型：‘某個補丁有活躍的郵件討論，所以值得加上鍊接’。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;坦率地講，就算這種討論最終沒有什麼實質內容，我想只要這個鏈接至少指向某個帖子（而不是那些已經收集起來的 acked-by 郵件），我也會覺得沒那麼煩人，相比那種只是指向單封郵件、沒人回覆過的鏈接。至少這樣我會覺得多少有點實際內容。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;當然，一如既往，我也意識到有些人認為補丁提交郵件之後，以後或許會有更多郵件回覆。但實際上，這種情況很少見，因為後續測試中的問題往往會創建新郵件，而不是回覆原始郵件（而且即便有人真去回覆了原始郵件，我們也可以很容易根據提交查找郵件，反過來也可以查到）。」&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370911</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370911</guid>
      <pubDate>Sun, 07 Sep 2025 06:54:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
  </channel>
</rss>
