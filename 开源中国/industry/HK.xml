<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - industry - 繁體中文（香港）</title>
    <link>https://www.oschina.net/news/industry</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news/industry" rel="self" type="application/rss+xml"/>
    <description>已對該 RSS 進行格式化操作：中英字符之間插入空格、使用直角引號、標點符號修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-hk</language>
    <lastBuildDate>Thu, 03 Jul 2025 07:45:23 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>騰訊提醒開發者可將微信小程序遷移至 QQ 客户端</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;騰訊 QQ 小程序開發者平台發文，提醒 QQ 客户端將全面接入微信小程序，&lt;strong&gt;開發者可以將微信小程序遷移至 QQ 以取代原有的舊版 QQ 小程序&lt;/strong&gt;，開發者當前已上線的舊版 QQ 小程序仍可正常使用和更新，不過官方稱「強烈建議儘早遷移，以便獲得更完整的接口支持，同時享受 QQ + 微信的雙端流量紅利」。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-e508f98702091d79c43aa333caf718d83a9.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;遷移前，QQ 小程序引擎實際上對微信小程序的大多數接口進行了兼容，原本只需要在微信小程序原有代碼基礎上做一些簡單判斷（主要是登錄方面）就可以分別提交兩個平台。&lt;/p&gt; 
&lt;p&gt;而在遷移後，QQ 端運行的就是微信小程序，體驗比 QQ 小程序會好一些。開發者需要通過 QQ 提供的插件（qq-wxmini-plugin）區分運行環境、處理登錄邏輯。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;來源：https://m.ithome.com/html/864991.htm&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;相關閲讀：&lt;a href="https://www.oschina.net/news/346915" target="news"&gt;手機版 QQ 支持微信小程序&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358586</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358586</guid>
      <pubDate>Thu, 03 Jul 2025 07:39:20 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>多模態才是智能應用爆發的關鍵？</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                                        &lt;p&gt;此前，快手發佈 2025 年一季度財報時，一個數字引發關注：成立僅兩年的 AI 業務線「可靈 AI」單季度貢獻營收 1.5 億元，同比增長 320%。而可靈 AI 正是一個多模態應用的典型產品，涉及到語言、視頻、音頻等交互。&lt;/p&gt; 
&lt;p&gt;前不久，在 OSCHINA 和小度教育技術負責人丁小晶的&lt;a href="https://my.oschina.net/u/4489239/blog/18426743" rel="nofollow"&gt;對話&lt;/a&gt;中。丁小晶表示，多模態技術非常重要，甚至可以説，沒有多模態技術效果的快速提升，教育行業不可能如此迅猛發展。比如 AI 作業批改和 AI 講題答疑方向的應用，完全靠純文本大模型是無法滿足需求的，非常依賴對大模型的圖片理解能力。還比如超擬人 AI 老師，語音情感大模型就起來非常關鍵的作用。&lt;/p&gt; 
&lt;p&gt;百度最新發布的發佈文心快碼 Comate AI IDE 產品，其中也提到了多模態能力的增強，比如支持 Figma 設計稿一鍵轉換為高可用代碼，能實現圖層的精準還原。百度工程效能部前端研發經理楊經緯告訴開源中國，無論是從自然語言、圖片還是設計稿生成代碼，最終都是為了能更加接近人類工程的意圖，因為人類去描述自己想要實現的想法的方式與形態是多種多樣的，也就對應了研發過程中的多模態形式。&lt;/p&gt; 
&lt;p style="text-align:center"&gt;&lt;img height="210" src="https://oscimg.oschina.net/oscnet/up-db06f16dbd4e854566d762bff8c3dfe1e5f.png" width="800" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;人類從不會只用一種感官認知世界。人工智能也勢必不能僅有一種交互途徑。&lt;/p&gt; 
&lt;p&gt;我們聞到咖啡香氣的瞬間，腦海裏會立刻浮現深褐色液體與白瓷杯的畫面；聽到「貓」這個詞時，腦海中自動補全毛茸茸的觸感和呼嚕聲。這種多模態信息融合，正是人類智能的底層邏輯。而單一模態交換的 AI 模型的信息處理能力有限，例如文本生成模型難以理解圖像語義，無法根據文字生成圖像，視頻生成工具則無法同步解析聲音與畫面邏輯。這種時候，就需要多模態模型或是能力的配合。&lt;/p&gt; 
&lt;span id="OSC_h2_1"&gt;&lt;/span&gt; 
&lt;h2&gt;多模態，比文本慢一步&lt;/h2&gt; 
&lt;p&gt;智源研究院院長王仲遠不久前公開&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.msn.cn%2Fzh-cn%2F%25E6%258A%2580%25E6%259C%25AF%2F%25E6%258A%2580%25E6%259C%25AF%25E5%2585%25AC%25E5%258F%25B8%2F%25E8%2581%259A%25E7%2584%25A6%25E5%25A4%259A%25E6%25A8%25A1%25E6%2580%2581-chatgpt%25E6%2597%25B6%25E5%2588%25BB%25E6%259C%25AA%25E5%2588%25B0-2025%25E5%25A4%25A7%25E6%25A8%25A1%25E5%259E%258B-%25E5%258F%2598%25E6%2585%25A2-%25E4%25BA%2586%25E5%2590%2597%2Far-AA1GjaHk%3Focid%3DBingNewsSerp" rel="nofollow" target="_blank"&gt;指出&lt;/a&gt;，當前多模態大模型的學習路徑，尤其是多模態理解模型，通常是先將語言模型訓練到很強的程度，再學習其他模態信息。在這個過程中，模型的能力可能會出現下降。&lt;/p&gt; 
&lt;p&gt;比單一模態更難的是，多模態模型還需解決一個核心問題：如何將圖像、文本、音頻等異構數據在語義層面對齊並融合。&lt;/p&gt; 
&lt;p&gt;文本、圖像、聲音等模態的數據結構天然異構——文本是離散符號序列，圖像是連續像素矩陣，音頻是時間序列信號。比如要讓模型理解「貓」的文本描述與貓的圖片、叫聲之間的關聯，需構建跨模態的共享語義空間。&lt;/p&gt; 
&lt;p&gt;早期，有研究嘗試通過數據級拼接，將圖像像素和文本特徵直接拼接，實現跨模態融合，但由於圖像和文本的時空特性差異較大，導致特徵對齊困難，最終效果不佳。直到對比學習和注意力機制的出現，才實現跨模態語義映射。比如 OpenAI 2021 年推出的一種基於對比學習只的多模態預訓練模型 CLIP，它通過大規模的圖像和文本數據進行訓練，使得模型能夠理解圖像內容和相關文本之間的語義關係。CLIP 的核心貢獻在於它打破了傳統的固定類別標籤範式，通過對比學習的方式，將圖像和文本映射到同一個向量空間中，從而實現跨模態的檢索和分類。但是 CLIP 模型的訓練數據規模龐大，據 OpenAI 披露，其使用了約 4 億圖像-文本對進行訓練，訓練成本高達數千 GPU 日，遠超 GPT-3 等純文本模型。&lt;/p&gt; 
&lt;p style="text-align:center"&gt;&lt;img height="304" src="https://oscimg.oschina.net/oscnet/up-4ad6b286433edebde043654fd53af191e30.png" width="800" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="text-align:center"&gt;&lt;span style="color:#8f959e"&gt;&lt;em&gt;CLIP 模型方法概述 &lt;/em&gt;&lt;/span&gt;&lt;span style="color:#8f959e"&gt;&lt;u&gt;&lt;em&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2103.00020" rel="nofollow" target="_blank"&gt;Learning Transferable Visual Models From Natural Language Supervision&lt;/a&gt;&lt;/em&gt;&lt;/u&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;多模態融合需處理高維數據，如 4K 視頻的像素量是文本的百萬倍，傳統 Transformer 的二次方計算複雜度成為致命短板。對此，業界也有一些解決方式，比如此前 Mamba 架構通過狀態空間模型 SSM 將計算複雜度降至線性，2025 年擴展動態融合模塊&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F2985554863" rel="nofollow" target="_blank"&gt; FusionMamba&lt;/a&gt;，在其中實現多模態特徵高效交互，推理速度提升 3 倍。&lt;/p&gt; 
&lt;p&gt;不僅如此，相較於文本的資料庫和數據集，高質量多模態數據集也更加稀缺，收集難度更大。比如醫療影像、工業質檢的報告中的缺陷描述等，就需專家級別的標註人員。&lt;/p&gt; 
&lt;span id="OSC_h2_2"&gt;&lt;/span&gt; 
&lt;h2&gt;落地需求更多&lt;/h2&gt; 
&lt;p&gt;雖然技術上還有諸多難點，但是多模態能力正在逐步提升，並且帶來非常可觀的價值和效果。&lt;/p&gt; 
&lt;p&gt;比如，從圖片或者是 Figma 設計稿直接生成代碼可以幫助許多開發者或是產品經理完成一些開發工作。這項能力此前在一些低代碼或是輔助編程工具中也存在，但往往是通過 Figma DSL 進行設計稿解析，通過節點虛擬化技術實現像素級還原，其不足在於不一定適配當前項目，比如轉了一套 Vue 框架的代碼，就無法在 React 框架項目中使用。&lt;/p&gt; 
&lt;p&gt;楊經緯介紹，此次文心快碼 Comate AI IDE 的發佈以及相關功能更新後，通過大模型能力增強了 Figma to Code 和當前項目的融合度。首先在 IDE 裏進行操作，天然就可以理解用户當前環境和本地優勢，而 IDE 內智能體 Zulu 的接入，會更深入到本地項目中瞭解當前的框架、能力、代碼風格等，再結合 Image to Code 的能力，可以實現較高的還原度，並且適配當前的項目。&lt;/p&gt; 
&lt;p&gt;而根據一些公開信息顯示，可靈 AI 的多模態技術，支持通過圖片、文字、聲音甚至手繪軌跡等輸入生成視頻。在上半年的 2.0 模型的迭代中，可靈 AI 也發佈了 AI 視頻生成的全新交互理念 Multi-modal Visual Language（MVL），讓用户能夠結合圖像參考、視頻片段等多模態信息，將腦海中包含身份、外觀、風格、場景、動作、表情、運鏡在內的多維度複雜創意，直接高效地傳達給 AI。MVL 由 TXT（Pure Text，語義骨架）和 MMW（Multi-modal-document as a Word，多模態描述子）組成，能從視頻生成設定的基礎方向以及精細控制這兩個層面。此外，其技術也結合了類 Sora 的 DiT 結構和 Flow 擴散模型，提升在物理模擬和細節上的表現。&lt;/p&gt; 
&lt;p&gt;基於這些技術特徵。商業化層面，截至今年 6 月，可靈 AI 已為超過 1 萬家企業客户提供 API 服務，覆蓋廣告營銷、影視動畫等領域，企業客户續費率較高。&lt;/p&gt; 
&lt;p&gt;此外，一些傳統行業或場景也在結合多模態能力，實現與 AI 的加速融合。比如迪瑞醫療近期採用的多模態 AI 大模型算法技術為臨牀診斷帶來了重要的技術革新，結合多種檢測結果和患者的多維信息，如尿常規、血常規、生化和化學發光免疫，以及患者的個人背景、臨牀表現、現病史與既往病史等，進行全面分析。&lt;/p&gt; 
&lt;p&gt;這種跨學科的信息整合使得診斷提示更加精準，對於減少漏診、誤診的概率具有顯著的作用，並進一步提升了醫療診療的整體效率。大洋彼岸，斯坦福醫學院的科研團隊研發出了一種名為&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzkxMTM0OTQzNQ%3D%3D%26mid%3D2247486194%26idx%3D1%26sn%3D5ac605d67ca7019b3b2e524d65b0f88e%26chksm%3Dc0eed67e545679711993370e69032cc62e9d4fc0c6ff3e283c43854fd93355eae8a07b4fcb02%23rd" rel="nofollow" target="_blank"&gt; MUSK 的 AI 模型&lt;/a&gt;，將視覺數據，如病理圖像和文本數據的病歷和臨牀記錄相結合，為癌症治療帶來了新的可能。MUSK 模型不僅提高了預測癌症患者預後和治療反應的準確性，而且通過分析數千個數據點，更準確地確定了哪些療法對個體患者最有效。&lt;/p&gt; 
&lt;p style="text-align:center"&gt;&lt;img height="285" src="https://oscimg.oschina.net/oscnet/up-196e0ee8b1058ba8ee70698e626a846fe72.png" width="800" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="text-align:center"&gt;&lt;span style="background-color:#f2f3f5"&gt;&lt;em&gt;視覺問答測試，圖片來源於網絡&lt;/em&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;在金融領域。江蘇銀行通過本地化部署微調 DeepSeek-VL2 多模態模型、輕量 DeepSeek-R1 推理模型，分別運用於智能合同質檢和自動化估值對賬場景中，通過對海量金融數據的挖掘與分析，重塑金融服務模式，實現金融語義理解準確率與業務效率雙突破。具體而言，DeepSeek-VL2 多模態模型採用了最新的 Transformer 架構，結合多層次的特徵融合機制，有效提升了金融合同、賬單等複雜文本與圖像信息的理解能力。模型在智能合同質檢場景中表現出色，準確率較傳統方法提升了 15% 以上，顯著降低了人工審核成本。同時，輕量化的 DeepSeek-R1 推理模型則在自動化估值與對賬場景中展現出極佳的實時響應能力，推理速度提升了 30%，為金融業務流程的自動化提供了堅實支撐。&lt;/p&gt; 
&lt;span id="OSC_h2_3"&gt;&lt;/span&gt; 
&lt;h2&gt;新的基礎設施&lt;/h2&gt; 
&lt;p&gt;應用邊界在不斷拓寬的同時，多模態模型的能力也在成長。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;而隨着應用場景的深化，模型架構也在同步進化，從基礎感知邁向複雜推理成為必然趨勢。OpenAI 在 2025 年 4 月發佈了多模態模型 O3 和 O4-mini，實現了「用圖像思考」的突破性能力。這些模型不僅能夠識別圖像內容，還能將圖像信息整合進推理思維鏈，支持多步推理和因果分析，比如夠處理模糊、倒置或複雜的圖像輸入，並給出合理的推理結果。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;其背後的關鍵技術包括分層注意力機制，將圖像分解為局部細節、全局關係和時序邏輯三層結構，從而提升對圖像內容的理解能力；動態工具鏈調用，在推理過程中，模型可以自主選擇 Python 分析、知識圖譜檢索、圖像生成等工具輔助決策，以及安全約束模塊，通過對抗訓練減少模型的幻覺輸出。&lt;/p&gt; 
&lt;p&gt;就在本月，中國科學院自動化研究所等單位的科研人員&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fkw.beijing.gov.cn%2Fxwdt%2Fkcyx%2Fxwdtkjqy%2F202506%2Ft20250611_4111006.html" rel="nofollow" target="_blank"&gt;首次證實&lt;/a&gt;，多模態大語言模型在訓練過程中自己學會了「理解」事物，而且這種理解方式和人類非常像。&lt;/p&gt; 
&lt;p&gt;科研人員借鑑人腦認知的原理，設計了一個巧妙的實驗：讓大模型和人類玩「找不同」遊戲。實驗人員會給出三個物品概念（選自 1854 種常見物品），要求選出最不搭的那個。通過分析高達 470 萬次的判斷數據，科研人員繪製出了大模型的「思維導圖」——「概念地圖」。通過實驗證實多模態大模型具備類人「概念理解」能力。研究團隊設計「找不同」遊戲，基於 470 萬次判斷數據繪製大模型「概念地圖」，提煉 66 個理解維度（如物體功能、文化意義），發現其與人腦神經活動高度一致，證明多模態模型比純文本模型更接近人類思維模式。&lt;/p&gt; 
&lt;p&gt;據谷歌雲在 2024 年年底發佈的&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnews.qq.com%2Frain%2Fa%2F20241219A07AW200" rel="nofollow" target="_blank"&gt;《2025 年人工智能商業趨勢報告》&lt;/a&gt;，預測到 2025 年，多模態 AI 將成為企業採用 AI 的主要驅動力。這種技術通過整合圖像、視頻、音頻和文本等多種數據源，使 AI 能夠以前所未有的準確性從更廣泛的上下文源中學習，提供更精確、定製化的輸出，創造自然直觀的體驗。報告預計，全球多模態 AI 市場規模將在 2025 年達到 24 億美元，到 2037 年底達到 989 億美元。&lt;/p&gt; 
&lt;p&gt;2025 進度已經過半，我們也能看到市面上許多多模態技術和產品的進展，而這場變革的終極圖景，或許正是讓 AI 真正成為理解世界、服務人類的「多模態智能夥伴」。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/4489239/blog/18679654</link>
      <guid isPermaLink="false">https://my.oschina.net/u/4489239/blog/18679654</guid>
      <pubDate>Thu, 03 Jul 2025 07:32:20 GMT</pubDate>
      <author>原創</author>
    </item>
    <item>
      <title>Windows 11 記事本正式支持 Markdown</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;5 月底，預覽體驗計劃中的 Windows 11 記事本迎來史詩級更新：&lt;a href="https://www.oschina.net/news/353253/windows-notepad-markdown"&gt;支持 Markdown 格式&lt;/a&gt;。現在普通用户也可以使用這個版本了，只需要在應用商店中更新記事本即可使用。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-af88d0da29ab2cf4246999e5b5025bf874c.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;目前支持：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;粗體&lt;/li&gt; 
 &lt;li&gt;斜體&lt;/li&gt; 
 &lt;li&gt;鏈接&lt;/li&gt; 
 &lt;li&gt;序號&lt;/li&gt; 
 &lt;li&gt;標題（H1～H5）&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;如下面的屏幕截圖所示，您可以點擊新的「H1」圖標，然後選擇您喜歡的標題：標題、副標題、章節，甚至是小節。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-387e4d297dcd5ac663e49554b55e2f51091.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;接下來，我們可以看到項目符號和數字列表按鈕，以及用於加粗或應用斜體的選項，但最吸引我注意的是超鏈接支持。現在，您可以使用 Ctrl + K 鍵盤快捷鍵（該快捷鍵在 Word 中也使用）插入帶有錨文本的鏈接，並在默認瀏覽器中打開。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-79d07dc4e8307d47c79e780de309830f227.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;此外，您可以點擊屏幕底部的「格式化視圖」按鈕切換到 Markdown 語法（原始）視圖。&lt;/p&gt; 
&lt;p&gt;與「格式」視圖不同，語法視圖允許您將井號轉換為標題、使用星號強調、用反引號包裹代碼等等。語法視圖類似於在後端使用 Markdown 編輯器，但它不會更改輸出。這取決於您在記事本中使用 Markdown 的方式。&lt;/p&gt; 
&lt;p&gt;在我們的測試中，Windows 最新版本觀察到記事本默認啓用 Markdown，但您有兩個選擇。您可以單擊清理格式按鈕，返回原始記事本體驗，而無需禁用 Markdown 支持。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-78b526954a85441998f9ce8f508e77c65f8.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;或者，您可以打開「設置」，向下滾動一點，找到一個名為「格式化」的新選項。關閉「格式化」後，記事本的經典體驗將恢復。您將不再看到格式化欄，Windows 也不會提示您使用它。&lt;/p&gt; 
&lt;p&gt;測試中，我們還注意到微軟在記事本中實現了非常輕量級的 Markdown 功能，它不會讓您的電腦運行速度變慢。&lt;/p&gt; 
&lt;p&gt;有些人可能會認為，給記事本添加太多功能違背了它作為純文本編輯器的初衷。這種觀點很有道理，但只要 Markdown 之類的功能是可選的，我並不介意。如果我需要它們，可以在「設置」中打開；如果不需要，也可以再次關閉。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;相關閲讀：&lt;a href="https://www.oschina.net/news/353253/windows-notepad-markdown" target="news"&gt;Windows 記事本支持 Markdown&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358574/windows-11-notepad-markdown</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358574/windows-11-notepad-markdown</guid>
      <pubDate>Thu, 03 Jul 2025 07:10:20 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>馬斯克 xAI 獲 100 億美元融資引關注，亞馬遜/微美全息佈局 AI 大模型應用加速落地</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;7 月 3 日消息，摩根士丹利在海外社交媒體 X 上發文，稱埃隆·馬斯克旗下 xAI 已完成 50 億美元（約合人民幣 358 億）債務融資及另外 50 億美元（約合人民幣 358 億）戰略股權融資。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;div&gt; 
   &lt;div&gt; 
    &lt;div&gt; 
     &lt;div&gt; 
      &lt;div&gt;
       &lt;img height="268" src="https://oscimg.oschina.net/oscnet//9981b9dae3d71c1206ff0c355aa797ad.png" width="300" referrerpolicy="no-referrer"&gt; 
       &lt;div&gt;
        &amp;nbsp;
       &lt;/div&gt; 
       &lt;div&gt;
        &amp;nbsp;
       &lt;/div&gt; 
      &lt;/div&gt; 
     &lt;/div&gt; 
    &lt;/div&gt; 
   &lt;/div&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;strong&gt;&lt;span&gt;獲新融資，xAI 估值破萬億&lt;/span&gt;&lt;/strong&gt;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;除債務融資外，xAI 還就約 200 億美元股權融資進行談判，這將使公司估值超過 1200 億美元。該交易獲得超額認購，參與方包括多家全球知名債務投資者，所募資金將用於開發 AI 解決方案，包括建設數據中心及 xAI 旗艦平台 Grok 更新升級。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;據悉，xAI 是馬斯克於 2023 年 7 月創辦，公司員工大多來自 OpenAI、谷歌 DeepMind、微軟、特斯拉等巨頭。硅谷競爭進入白熱化階段，為了讓它能和 ChatGPT 直接競爭，馬斯克將最近兩年的大部分精力用 xA，他們已經正在研發 Grok 4 人工智能模型。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;div&gt; 
   &lt;div&gt; 
    &lt;div&gt; 
     &lt;div&gt; 
      &lt;div&gt;
       &lt;img src="https://oscimg.oschina.net/oscnet//1be2c9519deef1028c410402dbd8bceb.png" width="719" referrerpolicy="no-referrer"&gt; 
       &lt;div&gt;
        &amp;nbsp;
       &lt;/div&gt; 
       &lt;div&gt;
        &amp;nbsp;
       &lt;/div&gt; 
      &lt;/div&gt; 
     &lt;/div&gt; 
    &lt;/div&gt; 
   &lt;/div&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;strong&gt;&lt;span&gt;亞馬遜發佈新 AI 大模型&lt;/span&gt;&lt;/strong&gt;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;與此同時，全球電商、雲計算巨頭亞馬遜（AMZN.US）在官網宣佈，在機器人技術與 AI 領域的兩個重要里程碑：推出新的 AI 基礎大模型 Deep Fleet，部署的機器人數量突破 100 萬大關。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;div&gt; 
   &lt;div&gt; 
    &lt;div&gt; 
     &lt;div&gt; 
      &lt;div&gt;
       &lt;img src="https://oscimg.oschina.net/oscnet//c99015b5cf3475a9ddb496174bee193d.png" width="657" referrerpolicy="no-referrer"&gt; 
       &lt;div&gt;
        &amp;nbsp;
       &lt;/div&gt; 
      &lt;/div&gt; 
     &lt;/div&gt; 
    &lt;/div&gt; 
   &lt;/div&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;近日，百度（BIDU.US）正式開源文心大模型 4.5 系列模型，涵蓋 47B、3B 激活參數的混合專家（MoE）模型，與 0.3B 參數的稠密型模型等 10 款模型，並實現預訓練權重和推理代碼的完全開源。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;目前，文心大模型 4.5 開源系列已可在飛槳星河社區、HuggingFace 等平台下載部署使用，同時開源模型 API 服務也可在百度智能雲千帆大模型平台使用。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;另據外媒消息人士透露稱，蘋果（AAPL.US）公司正考慮放棄其自研內部模型，並使用 Anthropic 或 OpenAI 的人工智能（AI）技術來驅動新版 Siri。知情人士透露，蘋果已與這兩家公司進行了接觸，討論將它們的大語言模型（LLM）部署在蘋果自有云基礎設施上進行測試。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;div&gt; 
   &lt;div&gt; 
    &lt;div&gt; 
     &lt;div&gt; 
      &lt;div&gt;
       &lt;img src="https://oscimg.oschina.net/oscnet//be1a66f75bfecfe9519cc3a7a5153d00.png" width="548" referrerpolicy="no-referrer"&gt;
      &lt;/div&gt; 
     &lt;/div&gt; 
    &lt;/div&gt; 
   &lt;/div&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;strong&gt;&lt;span&gt;微美全息 AI 產業邁入應用新階段&lt;/span&gt;&lt;/strong&gt;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;與此同時，面對洶湧的 AI 浪潮，資料顯示，AI 視覺創新廠商微美全息（WIMI.US）憑藉多年來的技術積累，旗下人工智能大模型形成了差異化競爭優勢，推動大模型向普惠性融合創新基礎設施轉化，自主創新生態也不斷完善，加速 AI 技術向生產端滲透。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;此外，微美全息通過開源模式推動基座大模型創新，持續推進多模態大模型研發，支持語音、圖像、視頻等多類型數據融合分析，並計劃推出更高精度的推理模型，促進「技術-數據-場景」循環迭代，降低中小企業使用門檻，為智能製造、智慧城市等場景提供廣泛支持。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;strong&gt;&lt;span&gt;結語&lt;/span&gt;&lt;/strong&gt;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;不可否認，為了能讓 xAI 追趕上 OpenAI，馬斯克可謂是付出了所有。而乘風 AI 熱潮，全球大模型百花齊放，並且國產 DeepSeek 實現彎道超車，打破海外算力封鎖，奠定了國產 AI 公司後來居上的基石，推動大模型技術的研究與創新發展，加速推進人工智能在千行百業的應用與價值創造。隨着這些創新技術的應用推廣，AI 技術或將改變整個行業的格局。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
&lt;/div&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358569</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358569</guid>
      <pubDate>Thu, 03 Jul 2025 06:56:20 GMT</pubDate>
      <author>來源: 資訊</author>
    </item>
    <item>
      <title>抖音內容技術團隊開源 ContentV：有限算力下高效訓練視頻生成模型的新路徑</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                                        &lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//adfb650ff16d8feb12d600710037b8a4.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;div class="ckeditor-html5-video" style="text-align:center"&gt; 
  &lt;video controls="controls" controlslist="nodownload" src="https://www.bilibili.com/video/BV1jC3azYEaS/?spm_id_from=333.1387.upload.video_card.click&amp;amp;vd_source=c09f0713b2507369924e94f4fec6c133"&gt;
    &amp;nbsp; 
  &lt;/video&gt; 
 &lt;/div&gt; 
 &lt;div class="ckeditor-html5-video" style="text-align:center"&gt; 
  &lt;video controls="controls" controlslist="nodownload" src="https://www.bilibili.com/video/BV1jC3azYEuW/?spm_id_from=333.1387.upload.video_card.click&amp;amp;vd_source=c09f0713b2507369924e94f4fec6c133"&gt;
    &amp;nbsp; 
  &lt;/video&gt; 
 &lt;/div&gt; 抖音內容技術團隊開源了 ContentV，一種面向視頻生成任務的高效訓練方案。該方案在多項技術優化的基礎上，使用 256 塊顯卡，在約 4 周內完成了一個 8B 參數模型的訓練。儘管資源有限，ContentV 在多個評估維度上取得了與現有主流方案相近的生成效果。該工作探索了在有限算力條件下訓練視頻生成模型的可行路徑。目前，推理代碼與模型權重已對外開放。 
&lt;/div&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;項目主頁&lt;/strong&gt;：https://contentv.github.io&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;技術報告&lt;/strong&gt;：https://arxiv.org/abs/2506.05343&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;代碼倉庫&lt;/strong&gt;：https://github.com/bytedance/ContentV&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;模型權重&lt;/strong&gt;：https://huggingface.co/ByteDance/ContentV-8B&lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="OSC_h2_1"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;核心亮點&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;極簡設計&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;CogVideoX、HunyuanVideo 和 Wan2.1 等一系列優秀的開源工作表明，視頻生成的關鍵並不在於架構上的特殊設計，而在於如何高效利用有限的數據資源，並有效對齊人類偏好。&lt;/p&gt; 
&lt;p&gt;為驗證 ContentV 方案的通用性，本次開源的版本在擴散模型部分採用了經典的文生圖模型 Stable Diffusion 3.5 Large。為了適配視頻模態，模型在結構上僅做了以下兩項必要調整：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;將原始圖像 VAE 替換為 Wan2.1 中使用的 3D-VAE；&lt;/li&gt; 
 &lt;li&gt;將 2D 位置編碼升級為 3D 版本。在具體編碼方式上，團隊對比了傳統的絕對位置編碼與主流的旋轉位置編碼。評估結果顯示，兩者在客觀指標和主觀感受上差異較小，因此保留了計算更高效的絕對位置編碼方案。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//a970cfacc9f335795a1cb051ba654811.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="text-align:center"&gt;ContentV 模型結構‌&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;多階段漸進訓練策略&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;上述的最小化結構改動，在解鎖了視頻生成能力的同時，也最大限度地保留了原模型的圖像生成能力。實驗證明，在新的 VAE 和位置編碼的適配階段，沿用 Flow Matching 的訓練方式，僅需 1000 步左右的微調，就能基本還原模型的圖片生成能力，大幅節省圖片預訓練階段的訓練成本。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//da7aab81f63561b7c0d8679eceba1022.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="text-align:center"&gt;VAE 適配過程‌&lt;/p&gt; 
&lt;p&gt;在視頻生成的預訓練階段，為加速收斂實現高效訓練，研究團隊設計了一套從「低清短片」到「高清長片」的多階段漸進式訓練流程，逐步引導模型學習時間維度與空間維度上的動態表徵，從而提升視頻的連續性、動態表現力和畫面細節。&lt;/p&gt; 
&lt;p&gt;此外，實驗證明，在推理階段引入非線性採樣步長機制（Flow Shift）能夠顯著提升視頻的整體生成質量。通過多組對比實驗，團隊最終確定了最優的採樣策略，進一步優化了生成效果。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;輕量級 RLHF 強化訓練&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//25bf5c0471707e1d93343f8649c7f46a.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//aaf7366e824c8a20dc80a43af6d4e872.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="text-align:center"&gt;RLHF 顯著提升畫面質感‌&lt;/p&gt; 
&lt;p&gt;在後訓練階段，除了使用高質量數據集進行微調外，通過 RLHF 或 DPO 等對齊人類偏好的監督訓練，也能顯著提升視頻生成質量。然而，這類方法通常依賴大量人工標註，用於訓練獎勵模型或直接監督擴散模型。同時，相較於圖像，視頻的序列長度顯著增加了 RLHF 和 DPO 的訓練資源需求。&lt;/p&gt; 
&lt;p&gt;為此，ContentV 研究團隊提出了一種輕量級的 RLHF 訓練方案，旨在不依賴人工標註的前提下，低成本提升視頻質量：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;利用開源的圖像獎勵模型對生成視頻的單幀進行監督。相較於視頻場景，目前圖像獎勵模型的訓練數據更易獲取，且在實際效果中表現更佳。實驗證明，由於 MM DiT 採用全局注意力機制，僅優化單幀即可帶動整體視頻質量的提升；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;將監督範圍限制在生成視頻的前 1 秒，相較於對完整視頻進行監督，可大幅減少訓練資源的消耗，同時獲得相近的質量提升效果。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;採用上述策略後，在無需人工標註的情況下，僅使用少量訓練資源，便可顯著提升畫面質量。RLHF 微調後，模型在視覺質量（VQ）指標上的表現大幅提升，評估勝率高達 89.38%。&lt;/p&gt; 
&lt;span id="OSC_h2_2"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;效果對比&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;在 VBench 這一主流視頻生成評測基準上，ContentV（8B）取得了 85.14 的綜合得分，表現優於多個現有的商業閉源模型，包括 Sora、Kling 1.6 和 Gen-3 等。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//2e5d2e9c6fb2a651ef95db56aa008420.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="text-align:center"&gt;VBench 榜單 (按照 Overall 分數降序排列)‌&lt;/p&gt; 
&lt;p&gt;為更貼近真實用户偏好，研究團隊圍繞感知質量、指令跟隨、物理一致性和視覺效果四個維度開展了人類偏好評估。結果顯示，ContentV 在整體表現上與 CogVideoX-5B、HunyuanVideo-13B 和 Wan2.1-14B 等主流開源模型相比具有一定優勢。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//547c06ccbfa684f90c21d1432e0c6786.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="text-align:center"&gt;人類偏好評估指標‌&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/6210722/blog/18683305</link>
      <guid isPermaLink="false">https://my.oschina.net/u/6210722/blog/18683305</guid>
      <pubDate>Thu, 03 Jul 2025 06:53:20 GMT</pubDate>
      <author>原創</author>
    </item>
    <item>
      <title>Bilibili 開源動漫視頻生成模型 AniSora V3 版</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;Bilibili（B 站）宣佈其開源動漫視頻生成模型&lt;strong&gt;AniSora&lt;/strong&gt;迎來重大更新，正式發佈&lt;strong&gt;AniSora V3&lt;/strong&gt;。作為 Index-AniSora 項目的一部分，V3 版本在原有基礎上進一步優化了生成質量、動作流暢度和風格多樣性，為動漫、漫畫及 VTuber 內容創作者提供了更強大的工具。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="292" src="https://oscimg.oschina.net/oscnet/up-27ad0a0400878a1e24e9451fd20c7a87882.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;AniSora V3 基於 Bilibili 此前開源的 CogVideoX-5B 和 Wan2.1-14B 模型，結合&lt;strong&gt;強化學習與人類反饋（RLHF）&lt;/strong&gt;框架，顯著提升了生成視頻的視覺質量和動作一致性。其支持一鍵生成多種風格的動漫視頻鏡頭，包括番劇片段、國創動畫、漫畫視頻改編、VTuber 內容）等。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;核心升級包括：&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;ul style="margin-left:0; margin-right:0"&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;時空掩碼模塊（Spatiotemporal Mask Module）優化&lt;/strong&gt;：V3 版本增強了時空控制能力，支持更復雜的動畫任務，如精細的角色表情控制、動態鏡頭移動和局部圖像引導生成。例如，提示「五位女孩在鏡頭放大時起舞，左手上舉至頭頂再下放至膝蓋」能生成流暢的舞蹈動畫，鏡頭與角色動作同步自然。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;數據集擴展&lt;/strong&gt;：V3 繼續依託超過 1000 萬高質量動漫視頻片段（從 100 萬原始視頻中提取）進行訓練，新增數據清洗流水線，確保生成內容的風格一致性和細節豐富度。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;硬件優化&lt;/strong&gt;：V3 新增對華為 Ascend910B NPU 的原生支持，完全基於國產芯片訓練，推理速度提升約 20%，生成 4 秒高清視頻僅需 2-3 分鐘。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;多任務學習&lt;/strong&gt;：V3 強化了多任務處理能力，支持從單幀圖像生成視頻、關鍵幀插值到唇部同步等功能，特別適合漫畫改編和 VTuber 內容創作。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;在最新基準測試中，AniSora V3 在&lt;strong&gt;VBench&lt;/strong&gt;和雙盲主觀測試中，角色一致性和動作流暢度均達到業界頂尖水平（SOTA），尤其在複雜動作 (如違反物理規律的誇張動漫動作) 上表現突出。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;Bilibili 強調，AniSora 是「對動漫世界的開源禮物」，鼓勵社區協作優化模型。用户需填寫申請表併發送至指定郵箱（如 yangsiqian@bilibili.com）以獲取 V2.0 權重和完整數據集訪問權限。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;V3 還引入了首個針對動漫視頻生成的&lt;strong&gt;RLHF 框架&lt;/strong&gt;，通過 AnimeReward 和 GAPO 等工具對模型進行微調，確保輸出更符合人類審美和動漫風格需求。社區開發者已開始基於 V3 開發定製化插件，例如增強特定動漫風格（如吉卜力風）的生成效果。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;AniSora V3 支持多種動漫風格，包括日本動漫、國產原創動畫、漫畫改編、VTuber 內容及惡搞動畫（鬼畜動畫），覆蓋 90% 的動漫視頻應用場景。 具體應用包括：&lt;/span&gt;&lt;/p&gt; 
&lt;ul style="margin-left:0; margin-right:0"&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;單圖轉視頻&lt;/strong&gt;：用户上傳一張高質量動漫圖像，配合文本提示（如「角色在向前行駛的車中揮手，頭髮隨風擺動」），即可生成動態視頻，保持角色細節和風格一致。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;漫畫改編&lt;/strong&gt;：從漫畫幀生成帶唇部同步和動作的動畫，適合快速製作預告片或短篇動畫。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;VTuber 與遊戲&lt;/strong&gt;：支持實時生成角色動畫，助力獨立創作者和遊戲開發者快速測試角色動作。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;高分辨率輸出&lt;/strong&gt;：生成視頻支持高達 1080p，確保在社交媒體、流媒體平台上的專業呈現。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;AIbase 測試顯示，V3 在生成複雜場景（如多角色交互、動態背景）時，相比 V2 減少了約 15% 的偽影問題，生成時間縮短至平均 2.5 分鐘 (4 秒視頻)。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;相比 OpenAI 的 Sora 或 Kling 等通用視頻生成模型，AniSora V3 專注於動漫領域。 與字節跳動的 EX-4D 相比，AniSora V3 更專注於 2D/2.5D 動漫風格，而非 4D 多視角生成。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358565</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358565</guid>
      <pubDate>Thu, 03 Jul 2025 06:47:20 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>智譜 AI 開源通用視覺推理模型 GLM-4.1V-Thinking</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;智譜 AI 於 7 月 2 日發佈了 GLM-4.1V-Thinking 系列通用視覺推理模型，並宣佈獲得來自浦東創投集團和張江集團的 10 億元聯合戰略投資。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/143332_18Al_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;同時，公司推出了全新生態平台「Agent 應用空間」，並啓動「Agents 開拓者計劃」，投入數億資金扶持 AI Agents 創業團隊。&lt;/p&gt; 
&lt;p&gt;為慶祝模型發佈，智譜大模型開放平台為用户提供新模型 Flash 版 1 億的「高併發版」Tokens，同時，該模型可通過 API 免費使用。&lt;/p&gt; 
&lt;p&gt;此次率先開源的是 GLM-4.1V-9B-Thinking，一個 9B 參數量的多模態模型，對應官方平台的 GLM-4.1V-Thinking-Flash。該模型旨在提升模型的深度思考與複雜推理能力。該模型在多項基準測試中表現卓越，其性能在 18 項任務上持平甚至超過了參數量為其 8 倍的 Qwen-2.5-VL-72B 和 GPT-4o 等主流視覺語言模型。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/143242_aYUB_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;模型具備強大的多模態能力，能夠解析長達 2 小時的視頻、進行數學與科學推理、看圖編寫網頁，並具備 GUI Agent 能力，可識別並操作手機、電腦等屏幕界面元素，完成用户指令。例如，在解析足球比賽時，模型能理解球員位置和戰術特點。&lt;/p&gt; 
&lt;p&gt;GLM-4.1V-Thinking 模型架構由視覺編碼器、MLP 適配器和語言解碼器組成，其卓越性能得益於引入了「課程採樣強化學習」（Reinforcement Learning with Curriculum Sampling）策略，通過由易到難的訓練任務安排，高效提升了模型在 STEM 解題、智能體任務、文檔圖表理解等多個領域的推理能力。&lt;/p&gt; 
&lt;p&gt;目前，GLM-4.1V-9B-Thinking 模型已在 GitHub、魔搭社區及 Hugging Face 上開源。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;開源列表&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;文檔：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbigmodel.cn%2Fdev%2Fhowuse%2Fvisual-reasoning-model%2Fglm-4.1v-thinking" target="_blank"&gt;https://bigmodel.cn/dev/howuse/visual-reasoning-model/glm-4.1v-thinking&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Github：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FTHUDM%2FGLM-4.1V-Thinking" target="_blank"&gt;https://github.com/THUDM/GLM-4.1V-Thinking&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ModelScope：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmodelscope.cn%2Fcollections%2FGLM-41V-35d24b6def9f49" target="_blank"&gt;https://modelscope.cn/collections/GLM-41V-35d24b6def9f49&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Hugging Face：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fcollections%2FTHUDM%2Fglm-41v-thinking-6862bbfc44593a8601c2578d" target="_blank"&gt;https://huggingface.co/collections/THUDM/glm-41v-thinking-6862bbfc44593a8601c2578d&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;HuggingFace 體驗鏈接：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fspaces%2FTHUDM%2FGLM-4.1V-9B-Thinking-Demo" target="_blank"&gt;https://huggingface.co/spaces/THUDM/GLM-4.1V-9B-Thinking-Demo&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358562/glm-4-1-v-thinking</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358562/glm-4-1-v-thinking</guid>
      <pubDate>Sun, 11 May 2025 06:32:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>開源中國聯合發起「全球數字友好開源社區」，共建開放協同新生態</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;2025 年 7 月 2 日，2025 全球數字經濟大會在北京國家會議中心隆重開幕。本次大會經國務院批准，由北京市人民政府、國家互聯網信息辦公室、國家數據局、新華通訊社與聯合國開發計劃署共同主辦，聚焦「建設數字友好城市」主題，來自全球多國政府機構、國際組織、城市代表、科研院所和科技企業代表齊聚一堂，圍繞數字技術賦能城市發展的路徑與合作機制深入交流。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/120249_MSqc_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在大會首場主論壇「數字友好城市建對話」階段，&lt;strong&gt;北京市經開區工委副書記、管委會副主任王磊指出北京正在加快打造以「模力方舟國際開源社區」為代表的 AI 開放創新平台集羣&lt;/strong&gt;，匯聚全球 AI 開發者資源，支撐企業間協同與城市間互信，推動開源力量深度融入全球數字治理體系。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/120301_ICF0_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;隨後，主論壇迎來了重點環節之一——&lt;strong&gt;「全球數字友好開源社區」正式啓動&lt;/strong&gt;。該社區由開源中國、統信軟件、平凱星辰等十八家中外企業、聯盟和機構共同發起，旨在打造面向全球的數字化開放協同平台。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;開源中國作為發起單位之一，研發副總裁李彥成代表公司出席啓動儀式，並與各方共同見證社區成立&lt;/strong&gt;。開源社區已成為推動全球數字協作與技術創新的重要力量。從早期由開發者驅動的協作模式，到如今以城市、企業、場景多元聯動為特徵的深度共建，開源正在加速融入數字城市治理的底層邏輯。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/120314_bEZv_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;開源已成為推動全球數字協作與技術創新的重要力量。從早期由開發者社區推動的技術共享，到如今以城市、企業、場景多元聯動為特徵的深度協作，開源的發展路徑正不斷拓展邊界。在聯合國「數字促進可持續發展」倡議框架下，「全球數字友好開源社區」應運而生，標誌着開源協作正在進入以城市友好關係為紐帶、以產業聯合體為主體的新階段。&lt;/p&gt; 
&lt;p&gt;作為國家重點開源基礎設施平台之一，開源中國·Gitee 始終致力於建設安全、自主、可控的開源生態。&lt;strong&gt;此次參與社區聯合發起，是開源中國積極服務國家數字戰略、深度參與國際開源共建進程的重要舉措&lt;/strong&gt;。依託自身在開源治理、企業級協同平台、人工智能服務平台等方向的長期積累，開源中國將與生態夥伴一道，共同推動開源協作從「項目共建」走向「城市共建」，為打造全球數字友好生態注入持續動能。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/120331_bONF_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在數字技術高速演進、人工智能重塑產業格局的時代背景下，構建開放、包容、可持續的全球協作機制顯得尤為重要。開源中國將繼續秉持開放精神，深度參與社區建設，攜手全球夥伴共建共享，為推動構建人類數字命運共同體貢獻更多開源力量。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358536</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358536</guid>
      <pubDate>Sun, 11 May 2025 04:03:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>馬斯克旗下人工智能公司 xAI 完成 100 億美元融資</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;馬斯克旗下人工智能公司 xAI 近日&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.cnbc.com%2F2025%2F07%2F01%2Felon-musk-xai-raises-10-billion-in-debt-and-equity.html" target="_blank"&gt;完成了 100 億美元融資&lt;/a&gt;，其中包括 50 億美元債務融資和 50 億美元戰略股權投資。這筆資金將用於開發 AI 解決方案、建設數據中心，並推動其旗艦 AI 助手 Grok 的進一步發展。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0703/114408_7iP8_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;除已完成的融資外，xAI 仍在洽談約 200 億美元的股權融資。若成功，其估值可能飆升至 1200 億至 2000 億美元，成為全球最具價值的 AI 公司之一。&lt;/p&gt; 
&lt;p&gt;然而，知情人士透露，xAI 的運營成本極高——2025 年預計將消耗 130 億美元，相當於每月燒錢超 10 億美元。目前的大規模融資僅能勉強跟上其鉅額開支，未來仍需持續輸血以維持技術研發和市場擴張。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358529/elon-musk-xai-raises-10-billion-in-debt-and-equity</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358529/elon-musk-xai-raises-10-billion-in-debt-and-equity</guid>
      <pubDate>Sun, 11 May 2025 03:44:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>搜索數據建設系列之數據架構重構</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                                        &lt;span id="OSC_h1_1"&gt;&lt;/span&gt; 
&lt;h1&gt;導讀&lt;/h1&gt; 
&lt;p&gt;主要概述百度搜索業務數據建設的創新實踐，重點圍繞寬表模型設計、計算引擎優化和新一代業務服務交付模式（圖靈 3.0 開發模式）三大方向，解決了傳統數倉在搜索場景下面臨的諸多挑戰，實現了搜索數據建設的高效、穩定、低成本；為百度搜索業務敏捷迭代奠定夯實基礎。&lt;/p&gt; 
&lt;span id="OSC_h1_2"&gt;&lt;/span&gt; 
&lt;h1&gt;&lt;strong&gt;名詞解釋&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;TDS（Turing Data Studio）&lt;/strong&gt;&lt;/strong&gt;： 是基於圖靈（百度內部數據分析平台）的數據建設解決方案，提供，數據開發、數倉管理、監控運維、資源管理等一站式服務的數據開發平台。詳情參見：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247599508%26idx%3D1%26sn%3D19094522609ca58528295a8ccbb061bd%26chksm%3Dc03f75e8f748fcfe192c0c8817ceee53c0e1f57dcbafd16e22ee371889f4dbf9e0b813b700fa%26token%3D1515601853%26lang%3Dzh_CN%26scene%3D21%23wechat_redirect" target="_blank"&gt;百度 MEG 數據開發治理平台-TDS&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;TDA（Turing Data Analysis）&lt;/strong&gt;&lt;/strong&gt;：是一款可視化 BI 產品，旨在幫助用户輕鬆上手及使用，進行拖拽式可視化數據分析及儀表盤建設。產品模塊包括儀表盤、數據集、可視化分析及智能分析模塊。詳情參見：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247584876%26idx%3D1%26sn%3D7bf459415ef8d51685d4e1c335b0603e%26chksm%3Dc03fbc10f7483506eb7206b02265f9010ddfa434359d7fa975a32d54c1b4886e734051ff9067%26scene%3D21%23wechat_redirect" target="_blank"&gt;百度一站式數據自助分析平台（TDA）建設&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;TDE（Turing Data Engine ）&lt;/strong&gt;&lt;/strong&gt;：是基於圖靈生態的計算引擎，包含 Spark 計算引擎和 ClickHouse。詳情參見：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247581388%26idx%3D1%26sn%3De71a4f3c4ca283ac6e8fe51d0a45a02b%26scene%3D21%23wechat_redirect" target="_blank"&gt;揭秘百度數倉融合計算引擎&lt;/a&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247601378%26idx%3D1%26sn%3D9234aeef05c3813cfb34d9a064261984%26chksm%3Dc03f7c9ef748f5886cc17147d8dfc8bd62ce062de822444ec592914d5ca6e0ab224cfc963e3e%26token%3D286675411%26lang%3Dzh_CN%26scene%3D21%23wechat_redirect" target="_blank"&gt;ClickHouse 在百度 MEG 數據中台的落地和優化&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;UPI（Udw-API）&lt;/strong&gt;&lt;/strong&gt;：百度內部編程訪問接口；以 Map/Reduce 計算框架為主，適用於計算邏輯複雜，以及多種數據源混合計算的例行離線數據挖掘業務&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;數倉融合計算引擎&lt;/strong&gt;&lt;/strong&gt;：是百度自主研發，基於 spark 自研的 adhoc 服務。提供數據查詢分析，具有簡單易用、超大規模支持、成本極低等特點，能實現 T 級數據秒級查詢，也適用於例行生產的 ETL 場景。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;函谷&lt;/strong&gt;&lt;/strong&gt;：圖靈核心模塊，作為圖靈查詢的 gateway，完成圖靈查詢的接收、分發、提交執行、查詢進展輪詢、結果獲取等一系列功能。&lt;/p&gt; 
&lt;span id="OSC_h1_3"&gt;&lt;/span&gt; 
&lt;h1&gt;01 背景與問題&lt;/h1&gt; 
&lt;span id="OSC_h2_4"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;1.1 背景&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;在當今互聯網產品發展日新月異、業務迭代迅猛的時代；跨業務分析的需求日益增長，這種變化既為企業創造了敏捷決策、精準運營的新機遇，也帶來數據割裂、價值釋放滯後等嚴峻挑戰。特別是大型互聯網企業，往往構建有複雜的多業務、多模塊、多線條體系，每日持續產出海量的數據信息。這些數據的服務對象正逐步從數據研發人員擴展至更為廣泛的產品相關人員，如何高效開展數據獲取工作，打破數據孤島現象，充分挖掘並釋放數據驅動業務的潛力，已成為業界廣泛關注和討論的焦點議題。針對該問題，業界傳統數倉常採用的是經典分層模型的數倉架構，從 ODS（Operational Data Store）&amp;gt;DWD（Data Warehouse Detail）&amp;gt;DWS（Data Warehouse Summary）&amp;gt;ADS（Application Data Store）逐層建模，但我們會發現，從傳統固化開發的角度來看，傳統經典數倉模型是比較有優勢的。然而，面對當下數據需求靈活多變的時代，其侷限性也日益凸顯。如下圖&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-939fa5fcdf0baaf3f9376861ee1c7269bb5.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h2_5"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;1.2 搜索場景下的困境與挑戰&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;搜索作為百度的核心支柱業務，涵蓋通用搜索、智能搜索、阿拉丁與垂類等多元化、多模態的搜索產品，具有&lt;strong&gt;&lt;strong&gt;快速迭代、模塊多元化且複雜&lt;/strong&gt;&lt;/strong&gt;的特性，搜索數據更是複雜多樣，整體數倉規模達到數百 PB 以上。&lt;/p&gt; 
&lt;p&gt;隨着搜索業務各個模塊之間的聯繫日益緊密，交叉分析的需求也在不斷增長。使用人員對數據獲取的便捷性提出了更高的要求，其中涵蓋了數據分析師、策略、業務產品經理、運營、評估等多類角色。他們的訴求期望能夠跨越複雜的數據架構壁壘，以更加&lt;strong&gt;&lt;strong&gt;高效、直觀、快速&lt;/strong&gt;&lt;/strong&gt;的方式獲取到所需數據。&lt;/p&gt; 
&lt;p&gt;而傳統的搜索數倉建設體系，無論從建模角度還是技術框架上，都與現階段用户訴求背道而馳。&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;建模角度：多層的傳統分層建模。往往會出現（大表數據量大、查詢慢、存儲冗餘、口徑不統一）等問題，影響業務分析效率，從而達不到數據驅動業務的效果。數據開發側作為需求的被動承接方，根據業務側提出的數據需求進行數據開發與交付，存在需求交付週期長、人力成本高等問題。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;技術框架角度：搜索數倉過去大多是採用&lt;strong&gt;&lt;strong&gt;UPI&lt;/strong&gt;&lt;/strong&gt;框架（以 C++ MR 計算框架為主）進行 ETL 處理。由於該框架技術陳舊，往往會出現以下問題影響數倉整體時效、穩定。從而使業務部門感知需求支持遲緩、數據產出延遲及數據質量低等一系列問題。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;容易出現服務不穩定。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;處理能力薄弱：處理不了特殊字符，從而導致數據丟失或任務失敗等。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;只能通過物理機遠程執行的方式提交，有單節點風險。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;無法 Writer 將數據寫到 Parquet 文件，需要進行特定腳本 ETLServer 框架進行轉換。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;思考&lt;/strong&gt;&lt;/strong&gt;：如何更好的滿足用户角色需求，進一步降低數據獲取的使用門檻？&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;破局&lt;/strong&gt;&lt;/strong&gt;：擁抱變化，以用户訴求為核心出發點。 探索更適合用户的 &lt;strong&gt;&lt;strong&gt;體系化建模&lt;/strong&gt;&lt;/strong&gt; 來進行實質、有效的數據管理。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;體系化建模：&lt;/strong&gt;以業務產品需求驅動模型設計，以模型設計驅動和約束開發實施，防止因模型設計與業務產品割裂、開發實施缺少約束帶來的無序、「煙囱式」開發。在機制上形成模型設計與開發實施的有效協同。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;切入點&lt;/strong&gt;&lt;/strong&gt;：以規範「基礎數據建設」，消除因「煙囱式」開發給業務帶來的困擾和技術上的浪費。&lt;/p&gt; 
&lt;p&gt;由此我們探索出一套新的建模體系：&lt;strong&gt;大寬表+數據集&lt;/strong&gt;：其核心點就是基於寬表，將之前的"需求-交付"解耦為以數據集為中心的建設，從而提升搜索內業務數據分析效率與分析深度，更好助力業務決策。以下將從寬表建模、計算引擎架構優化、新型業務交付模式等方向為大家介紹搜索數據團隊業務實踐。&lt;/p&gt; 
&lt;span id="OSC_h1_6"&gt;&lt;/span&gt; 
&lt;h1&gt;02 搜索建模思路與技術方案&lt;/h1&gt; 
&lt;span id="OSC_h2_7"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;2.1 建模模型&lt;/strong&gt;&lt;/h2&gt; 
&lt;span id="OSC_h3_8"&gt;&lt;/span&gt; 
&lt;h3&gt;2.1.1 思路&lt;/h3&gt; 
&lt;p&gt;基於搜索產品功能特性與差異化業務場景，我們對日誌數據進行主題化的分類。在每個主題域內，結合業務運營的具體環節特徵，構建具備高整合度的寬表模型。在模型構建過程中，保持 ODS（操作數據存儲）層與 DWD（明細數據層）的表結構粒度一致，確保數據的一致性與連貫性。所構建的寬表不僅完整涵蓋下游業務所需的全部字段，包括業務明細表中的基礎數據，還整合了各層級的維度屬性與計算指標。通過這種方式，形成一個全面、統一的數據底座，為上層業務的多維分析、指標監控及決策支持提供堅實的數據支撐，有效滿足多樣化的業務分析需求。&lt;/p&gt; 
&lt;span id="OSC_h4_9"&gt;&lt;/span&gt; 
&lt;h4&gt;2.1.1.1 舉例&lt;/h4&gt; 
&lt;p&gt;以展點主題為例，從歷史的模型表粒度和模型層級來分析：ODS 與 DWD、DWS 錶行為、檢索、點擊各個主題在同層模型或者跨模型之間都存在字段、口徑的冗餘，如下圖&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-a9f6f8e899639d2fe381e3fd22248b8c407.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h4_10"&gt;&lt;/span&gt; 
&lt;h4&gt;2.1.1.2 思路分析過程&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;核心思想過程：展點主題下明確粒度，豐富維度&amp;amp;指標，建設寬表模型。&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;將展點主題下各層之間的事實表複雜嵌套字段打平後與各個維度表、指標等進行 join 生成寬表，寬表的列最終分為公共屬性、展點行為屬性、業務屬性和指標屬性。&lt;/p&gt; 
&lt;p&gt;消除：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;數倉層間：字段存儲冗餘問題&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;數倉層內：口徑不一致問題&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-75e2f39732709336398f3ea0e2f605afc62.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-cc8563c18a8af68b1e8ce82a384dea8aa25.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-c837b8c7763edc5c4290cd758752771d0ce.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h3_11"&gt;&lt;/span&gt; 
&lt;h3&gt;2.1.2 建模核心思想&lt;/h3&gt; 
&lt;p&gt;基於思路分析過程，總結出一套核心建模理論，核心思想如下圖&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-59b8209db3232d786eb0920bd402783d568.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;構建搜索系統性數據建模：根據產品功能和業務不同，按照不同主題構建寬表。從而達到節約存儲、精簡表數量、口徑更清晰的目標。&lt;/p&gt; 
&lt;span id="OSC_h3_12"&gt;&lt;/span&gt; 
&lt;h3&gt;2.1.3 整體模型架構&lt;/h3&gt; 
&lt;p&gt;基於核心建模思想理論得到整體的模型架構，如下圖&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-65a11aadcf98271e6926e75e6f4de59f9cf.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;採用 Parquet 列式存儲，可支持寬表數百、千列，超多字段，再經過按列的高效壓縮（bucket sort 後，壓縮率更高），降低了數倉整體存儲空間，提高了 IO 效率，起到了降低上層應用延遲的效果。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;將各層之間的表複雜嵌套字段打平後與各個維度表、指標等進行 join 生成百列寬表，寬表的列最終分為公共屬性、業務維度屬性和指標屬性，便於業務分析，實現快速迭代。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="OSC_h2_13"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;2.2 計算引擎&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;為了保證數據生產穩定、準確性。我們對計算引擎的選擇做了升級，採用傳統 Spark 結合&lt;strong&gt;&lt;strong&gt;數倉融合計算引擎&lt;/strong&gt;&lt;/strong&gt;對搜索數倉 ETL 進行重構。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-a8f5a69d8cba77d36bc869cdaa8f603c4e6.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h3_14"&gt;&lt;/span&gt; 
&lt;h3&gt;2.2.1 從架構&amp;amp;處理流程上&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;C++ MR&lt;/strong&gt;&lt;/strong&gt; ：多進程，每個任務獨立運行，必須經過 Map-Shuffle-Reduce，然後中間結果寫磁盤。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;Spark&lt;/strong&gt;&lt;/strong&gt; ：多線程，任務在 Executor 內以線程執行。基於 DAG，可以在內存中緩存數據，減少 IO。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Spark 框架，相較於 MR 框架優勢在於&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;基於內存計算，處理速度快。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;支持多種計算模式，功能豐富，適合迭代處理數據。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;提供了高級的 API，開發效率高。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;基於平台提交，有效避免單節點計算風險。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;且在有 shuffle 情況下計算表現更好（MR 在 Shuffle 時默認進行排序，spark 對 shuffle 有優化，只有在部分場景才需要排序），在具體業務實踐中：同耗時的情況下，Spark 計算資源相較於 MR 節省 20% 左右。&lt;/p&gt; 
&lt;span id="OSC_h3_15"&gt;&lt;/span&gt; 
&lt;h3&gt;2.2.2 ETLServer 到數倉融合引擎轉變&lt;/h3&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-91ec2c9be50fe59678b383ecdee2fe2439a.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;各主題寬表模型的計算通過&lt;strong&gt;&lt;strong&gt;數倉融合計算引擎&lt;/strong&gt;&lt;/strong&gt;（通過 Spark Application Context 常駐方式做到資源有效複用；省去了啓動 Driver 的時間實現任務的快速啓動，來提升任務執行時間）可直接 Writer 將數據寫到 Parquet 文件，文件無需進行多次腳本 server 轉換。&lt;/p&gt; 
&lt;p&gt;在具體業務實踐中，各主題計算耗時由之前 40min 縮短至 10min（減少了 30min），實現數倉快速產出。&lt;/p&gt; 
&lt;span id="OSC_h2_16"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;2.3 新數據模型及架構下的挑戰與解決方案&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;任何數倉模型架構不會存在一個絕對完美的、涵蓋所有方面的解決方案。寬表設計僅是當前環境數倉模型的最優解，它依然面臨着諸多不容忽視的挑戰。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-d897150d0eb4ebe4cbfe141af1e656db0a7.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h3_17"&gt;&lt;/span&gt; 
&lt;h3&gt;2.3.1 挑戰 1 解決方案&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;strong&gt;列式存儲&amp;amp;讀取：&lt;/strong&gt;&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;寬表採用了 Parquet 列式存儲，以及 ZSTD 高效壓縮算法。結合數倉融合引擎，達到 Data Skipping（即讀的越少、計算越快）的效果，僅需讀取查詢涉及的分區及列，減少了磁盤 I/O 和內存傳輸的數據量來提升查詢效率，通過 Sql 分析服務發現熱點複雜字段，主動引導業務建設物化列，命中後查詢性能提升 80%。&lt;/p&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;strong&gt;複雜嵌套字段打平&lt;/strong&gt;&lt;/strong&gt;：&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;業務常用核心指標以及高頻字段口徑下沉寬表。雖然行數變多了，但是避免了 explode，get_json_object、array、map 等複雜字段獲取的耗時操作，查詢性能相較於之前提升了 2.1 倍。&lt;/p&gt; 
&lt;span id="OSC_h3_18"&gt;&lt;/span&gt; 
&lt;h3&gt;2.3.2 挑戰 2 解決方案&lt;/h3&gt; 
&lt;p&gt;搜索數據升級到了湖倉一體架構，藉助&lt;strong&gt;&lt;strong&gt;Iceberg Merge Into&lt;/strong&gt;&lt;/strong&gt;功能，實現高效回溯方式：對錶數據進行行級別的更新或刪除。相比 insert overwrite 操作更加高效，減少了不必要的數據移動和存儲浪費。&lt;/p&gt; 
&lt;p&gt;通過單一原子操作實現了複雜的數據整合需求。相比傳統的先刪除再插入的方式，&lt;strong&gt;&lt;strong&gt;Merge Into&lt;/strong&gt;&lt;/strong&gt;提供了更好的性能和一致性保證，其原理是通過重寫包含需要刪除和更新行數據所在的 date files。Merge Into 可以使用一個查詢結果數據來更新目標表的數據，其語法類似 join 關聯方式，根據指定的匹配條件對匹配的行數據進行相應的操作&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Merge Into&lt;/strong&gt;&lt;/strong&gt;基本語法&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-d2bd76ce88c3a899a2b4c7e9030565f332e.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;回溯原理流程如下圖&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-65989e3d43c224d06d336056c29f796c598.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;1. 關聯匹配&lt;/strong&gt;&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;目標表和源表根據指定 key 進行 join 操作。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;2. 條件判斷&lt;/strong&gt;&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;若 Key 匹配&lt;/strong&gt;&lt;/strong&gt;：根據源表操作類型，對目標表中的記錄執行相應的操作（更新或刪除）。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;若 Key 不匹配&lt;/strong&gt;&lt;/strong&gt;：執行 Insert 操作，將源表數據插入目標表。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;3. 原子性操作&lt;/strong&gt;&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;整個流程是事務性的，確保數據一致性。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;以下是特定回溯場景下 hive 與 iceberg 不同方式的回溯耗時對比，可以看的出來用 merge into 代替 insert overwrite 進行回溯，回溯更新效率整體可提高&lt;strong&gt;&lt;strong&gt;54% 左右。&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-83ae59dc2b2014d6f454f8ccb8112fdcaa4.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h3_19"&gt;&lt;/span&gt; 
&lt;h3&gt;2.3.3 挑戰 3 解決方案&lt;/h3&gt; 
&lt;span id="OSC_h4_20"&gt;&lt;/span&gt; 
&lt;h4&gt;2.3.3.1 重排序、高效壓縮&lt;/h4&gt; 
&lt;p&gt;開發 ATO 優化器 (通過任務依次執行重排序、壓縮等一系列 Rules，實現分區優化和數據重分佈)，高效率壓縮，解決存儲成本，存儲節約 20%。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-0748d5efaac1312246563e0bce0ba04e1c3.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;（1）壓縮編碼&lt;/p&gt; 
&lt;p&gt;數倉表字段元信息採集：通過任務對圖靈寬表表進行字段元信息採集，分析數據分佈情況，獲取重排序字段。&lt;/p&gt; 
&lt;p&gt;具體做法：通過 RLE、Delta 等縮碼方式來提升數據壓縮效率；數據重複度越高、連續性越好（有序）的場景，壓縮效率會越高，RLE、Delta 編碼原理如下。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-e0a7287f220ad886011cb4c0b407334ac22.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;（2） 壓縮格式&lt;/p&gt; 
&lt;p&gt;使用 ZSTD 壓縮格式和更大的壓縮 level，在不影響查詢性能的情況下，更大的壓縮 level 能進一步提高壓縮率，level=9 時在壓縮效率和耗時上最為平衡，讀寫耗時和壓縮率對比效果如下。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-9f7b43c93732b26daa2a8d7c38e2da18252.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;（3） Page Size&lt;/p&gt; 
&lt;p&gt;針對 Parquet 文件格式特性進行深入挖掘 ，對 Parquet page size 進行分頁擴容，將 Page Size 從 1MB 增大至 5MB，讓更多相似的數據集中到同一個數據頁中，充分利用編碼的壓縮特性，進一步減少各個數據頁之間存在的相似數據。在 ZSTD 的基礎上，能進一步提升壓縮效果，效果如下&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-e1a9ccfc36925c7c5a6ba0e90ecb3ae1a78.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h4_21"&gt;&lt;/span&gt; 
&lt;h4&gt;2.3.3.2 歷史裁剪，管理有效字段&lt;/h4&gt; 
&lt;p&gt;開發了一套半自動化的通用裁剪模式，通過採集日常任務代碼，sql parser 模塊解析出無用字段信息（尤其是大 json 大 map 類型擴展字段的無用字段）自動化實現了裁剪。減少了 &lt;strong&gt;&lt;strong&gt;50%&lt;/strong&gt;&lt;/strong&gt; 的回溯任務計算資源消耗，將人力投入從 5 人/天降低到 0.5 人/天。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-59334d514c3341fa8f02863a2b78ccdc7ab.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;字段頻率統計模塊&lt;/strong&gt;&lt;/strong&gt;：通過對函谷 SQL 數據庫和 TDS 平台 No SQL 任務的物理執行計劃進行解析，實現對寬表 SQL 任務和非 SQL 任務的字段訪問頻率的自動化統計。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;裁剪字段抽取模塊&lt;/strong&gt;&lt;/strong&gt;：基於字段訪問頻率，每月抽取冷温字段，形成可視化的字段訪問頻率報表，生成裁剪 SQL。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;**冷温字段告警模塊：**通過對比前一個月和本月冷温字段列表，生成當月新增冷温字段列表，然後向產品研發團隊和數據 RD 團隊發出告警，確認需要動態調整的裁剪字段；引入冷温字段告警模塊，成功實現了裁剪字段的動態調整。最後，通過滾動裁剪模塊自動裁剪 395 天前的數據，進一步降低人力/資源的消耗。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;滾動裁剪模塊&lt;/strong&gt;&lt;/strong&gt;：自動化滾動裁剪，裁剪寬表中 395 天前的數據。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;基於業務實踐證明：&lt;strong&gt;&lt;strong&gt;寬表數倉模型&lt;/strong&gt;&lt;/strong&gt;與&lt;strong&gt;&lt;strong&gt;數倉融合計算引擎&lt;/strong&gt;&lt;/strong&gt;的結合，比傳統數倉模型，更適合，面向服務於快速迭代的驅動型業務，主要體現在&lt;/p&gt; 
&lt;p&gt;1. 查詢性能巨大提升帶來快速響應支持業務需求：&lt;/p&gt; 
&lt;p&gt;簡單查詢場景 ：Adhoc 查詢場景，耗時在數十秒級別，相比於普通 Spark 性能提升 5 倍。&lt;/p&gt; 
&lt;p&gt;複雜場景：業務常用複雜字段拆分打平，避免數組、map 等複雜字段耗時操作、查詢性能提升 2.1 倍。&lt;/p&gt; 
&lt;p&gt;2.口徑封裝下沉：封裝業務核心口徑，解決業務長期數據源多、口徑不一致帶來的數據準確性問題，省去不必要的溝通，使用更加簡便。&lt;/p&gt; 
&lt;p&gt;3.減少冗餘存儲：相較於經典傳統數倉同主題模型下存儲降低 30% 左右。&lt;/p&gt; 
&lt;span id="OSC_h1_22"&gt;&lt;/span&gt; 
&lt;h1&gt;03 基於建模與技術框架初步整合&amp;nbsp;&lt;strong&gt;&lt;strong&gt;探討圖靈 3.0 生態新一代業務服務交付模式&lt;/strong&gt;&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;隨着搜索數倉模型&amp;amp;計算引擎架構的重構和技術棧統一，搜索數倉定義逐步清晰化、數倉個數大幅度降低，整體趨向更加緊湊、高效以及收斂的態勢。在此基礎上，為了助力數據迭代效率和分析效率進一步提升，在業務線基礎數倉及應用層數據建設上，百度 MEG 內部開發了圖靈 3.0 生態系統（即，數倉合理建設，數據分析需求儘可能收斂到 TDA 平台，配套數據集建設完善），包括 Turing Data Engine(TDE) 計算引擎、Turing Data Studio(TDS) 數據開發治理平台和 Turing Data Analysis(TDA) 可視化 BI 產品。依託圖靈 3.0 生態，我們進而形成了一套新的開發範式—— 圖靈 3.0 新開發模式，用來提升搜索內業務數據分析效率與分析深度，如下圖（階段三）所示&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-d20b1a79b0fbe50ef85eb7c922eab6fc5bd.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h2_23"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;3.1 &lt;strong&gt;&lt;strong&gt;階段一到階段二&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;如之前所述：由於搜索數倉早期查詢性能不理想，為了提升業務分析效率建設了大量的業務表。從而導致數據冗餘、數據鏈路穩定性差、效率低、指標口徑不一致等一系列問題。搜索數據團隊通過數倉模型（將多層數據模型控制在 1-2 層）以及計算引擎架構升級重構、湖倉一體、高效壓縮、裁剪等一系列措施解決了這些問題。數據建設更加完善規範化，搜索數倉表的數量由過去的數百張減少至 20 張左右，時效性大幅提升，全數據鏈路全流程提速 4H+，數據穩定性及運維成本降低 30%。&lt;/p&gt; 
&lt;span id="OSC_h2_24"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;3.2 階段二到階段三&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;隨着圖靈 3.0 生態系統（包括 TDA、TDS、TDE）及搜索數倉模型的日益完善，內部提出了，以數據集為核心來構建數據應用層，將數據開發側與業務側的依賴關係從之前的"需求-交付"解耦為以數據集為中心的建設，實現數據集&amp;lt;-&amp;gt;可視化分析&amp;lt;-&amp;gt;儀表盤的數據分析閉環，解決業務常用維度、指標長週期的查詢分析需求 ——&amp;gt; 圖靈 3.0 新開發模式。&lt;/p&gt; 
&lt;p&gt;圖靈 3.0 新開發模式核心思想在於數據集的建設，我們將不再僅僅只是根據業務需求來定製開發數據報表，而是構建一個靈活、可擴展的數據集。使業務側能夠自主地根據需求從中提取、分析和可視化數據，以滿足不斷變化的業務需求。&lt;/p&gt; 
&lt;p&gt;那麼，在數據集建模實踐中，如何才能合理構建一個靈活、可擴展且高質量的數據集？是數據研發對數據集建模關鍵核心，也是最大的挑戰。&lt;/p&gt; 
&lt;span id="OSC_h3_25"&gt;&lt;/span&gt; 
&lt;h3&gt;3.2.1 數據集建模合理度挑戰&lt;/h3&gt; 
&lt;p&gt;1. 為了滿足業務需求的多樣性與廣泛性，並支持更多的維度和指標。我們往往會傾向於在單個數據集中不斷疊加新的維度和指標，這種做法雖然表面上看起來方便快捷，但實際上卻導致了數據集行數的急劇增加，進而對聚合查詢的性能造成了不利影響&lt;/p&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;為了確保查詢的高效性，同時兼顧更多維度與指標的業務需求。我們往往的做法，就是建立更多的數據集，以空間換時間去滿足查詢性能。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;顯然，這些做法之間存在着明顯的矛盾，具體如下圖。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-cd8fad4be3fd6dfa2887117d9f41ac20272.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h3_26"&gt;&lt;/span&gt; 
&lt;h3&gt;3.2.2 解決方案&lt;/h3&gt; 
&lt;p&gt;為了更好地找到平衡點，搜索數據團隊採取了以下解決措施：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;明確邊界&lt;/strong&gt;&lt;/strong&gt;：分主題建設對應數據集，單主題內，數據集儘量做到合併統一，以達到更高的集成度與一致性。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;明確粒度&lt;/strong&gt;&lt;/strong&gt;：從業務場景需求出發，單主題內數據集建設前明確數據集最小粒度 ，確保數據最小粒度既能滿足主題分析的精度要求，又避免因過度細化或粗放導致的分析效能損耗，為後續數據集的結構化構建與高效奠定基礎。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;深度性能優化&lt;/strong&gt;&lt;/strong&gt;：充分利用了 TDE-ClickHouse 強大基礎引擎，例如在處理高基數去重計數字段時，創新性地採用 NoMerge 技術來替代傳統的 COUNT(DISTINCT) 方法，降低了聚合層的計算負擔，實現了查詢性能 5 至 10 倍的提升，極大地優化了數據處理速度。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;span id="OSC_h2_27"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;3.3 新模式帶來的改變&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;img alt="圖片" src="https://oscimg.oschina.net/oscnet/up-11ffa0f8e33c9d7a46ccb8a729f3b0031ce.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;△ 圖靈 3.0 的數據開發新模式&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;強化主動能力，業務自助效率顯著提升&lt;/strong&gt;&lt;/strong&gt;：相較於以往被動式的一對一需求定製化開發模式，數據研發工作已從單純響應被動需求轉變為主動規劃構建數據集。圖靈 3.0 新開發模式下，實現數據集&amp;lt;-&amp;gt;可視化分析&amp;lt;-&amp;gt;儀表盤的數據分析閉環（滿足 90% 查詢；其餘 10% 長尾交給 Adhoc 查詢），業務人員對日常通用需求的分析工作轉移到數據集自助查詢與分析上（根據數據集自助創建可視化數據報表）。可視化分析佔比、&lt;strong&gt;&lt;strong&gt;業務自助率提高至 90%，數據研發日常需求量減少 80%。&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;非核心常用維度指標查詢性能顯著提升&lt;/strong&gt;&lt;/strong&gt;：非核心常用維度指標由以往業務提需，查表或單獨建設報表來獲取數據的方式，轉變為通過數據集自助下鑽、拖拉拽自由組合常用維度指標，實現可視化分析的方式。藉助 TDE-ClickHouse 強大基礎引擎能力：可視化分析效率大幅提升，&lt;strong&gt;&lt;strong&gt;從小時、分鐘級的數據分析效率，提升至秒級分析&lt;/strong&gt;&lt;/strong&gt;。單次查詢數據週期由&lt;strong&gt;&lt;strong&gt;1 周內，提升至 1 年內（&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;秒級完成查詢&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;）&lt;/strong&gt;&lt;/strong&gt;，真正做到即需即查即用。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;血緣管理規範化，運維效率顯著提升&lt;/strong&gt;&lt;/strong&gt;：數據血緣更加完整流程化，數倉-數據集，血緣在 TDS 完成閉環，數據集內字段血緣在 TDA 完成閉環，以數據集為紐帶串聯整個數據流全過程，&lt;strong&gt;&lt;strong&gt;數據鏈路運維效率提升 2-3 倍&lt;/strong&gt;&lt;/strong&gt;。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;目前，該模式已經廣泛應用於搜索各業務數據運營人員早報、週報等多種業務彙報場景。得益於該模式，搜索產品線下&lt;strong&gt;&lt;strong&gt;儀表盤周均查詢（PV）高達 1.7W 次&lt;/strong&gt;&lt;/strong&gt;左右，&lt;strong&gt;&lt;strong&gt;可視化分析周均 0.93W 次左右 ，每週&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;超過 400 多名用户參與 TDA 搜索數據分析工作&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;。&lt;strong&gt;&lt;strong&gt;更重要的是，需求的交付週期實現了顯著縮短，由&lt;/strong&gt;&lt;/strong&gt;以往的單/雙週縮短至按天交付&lt;/strong&gt;&lt;/strong&gt;；甚至在某些情況下，業務人員能夠直接自助獲取所需數據。在處理重點項目時，該模式也能確保業務團隊在第一時間獲取到 P0 級別的關鍵數據。這種方式的轉變不僅能夠減輕數據開發團隊的工作負擔——人力成本由原先的&lt;strong&gt;&lt;strong&gt;3 人鋭減至 1 人&lt;/strong&gt;&lt;/strong&gt;，還能提高業務側的數據使用效率和自主性，使得團隊得以從繁瑣的「取數」與「跑數」任務中解放出來，將更多的精力投入到數倉模型的優化、技術框架的探索與治理等更具戰略價值的工作中去。&lt;/p&gt; 
&lt;span id="OSC_h1_28"&gt;&lt;/span&gt; 
&lt;h1&gt;04 總結與展望&lt;/h1&gt; 
&lt;p&gt;數據建模領域正經歷從「技術驅動」向「價值驅動」的深刻轉型，更加強調的是敏捷性、可解釋性和業務對齊。儘管當前的技術工具愈發強大，但成功的關鍵依舊在於跟業務的緊密協作與一個清晰明確的治理框架。&lt;/p&gt; 
&lt;p&gt;搜索業務，作為百度內部最核心且最為複雜的板塊，涵蓋了多個至關重要的產品線。近年來，搜索數據團隊始終致力於運用前沿技術來不斷優化和完善數倉體系的建設，以堅實的基礎數倉架構支撐起數據質量飛躍提升，從而高效賦能業務，帶來可量化、可感知的業務成效與用户體驗升級。&lt;/p&gt; 
&lt;p&gt;展望未來，隨着 AI 代理和邊緣計算等技術的蓬勃發展，數據建模有望朝着自適應與嵌入式方向進一步進化。搜索數據側還將在以下關鍵方向與大家進行深入探討、交流和學習：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;通用數據流解決方案&lt;/strong&gt;&lt;/strong&gt;：構建事件規則引擎等通用數據流處理工具，簡化數據處理流程，提高數據處理效率與靈活性。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;日誌埋點技術（含無埋點）&lt;/strong&gt;&lt;/strong&gt;：探索高效的自動化埋點機制，提升數據採集的全面性與準確性，為業務洞察提供堅實的數據基礎。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;寬表模型框架抽象層&lt;/strong&gt;&lt;/strong&gt;：探索更為高效、靈活的模型統一抽象方法層。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;AI 大模型時代下的高效開發模式&lt;/strong&gt;&lt;/strong&gt;：探索如何通過利用大模型技術，來優化代碼質量、數據鏈路等，打造更加高效、可靠的數據開發與運維體系。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;我們期待之後再次與大家見面探討這些議題，共同推動數據領域的創新與發展。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/4939618/blog/18683272</link>
      <guid isPermaLink="false">https://my.oschina.net/u/4939618/blog/18683272</guid>
      <pubDate>Sun, 11 May 2025 03:38:00 GMT</pubDate>
      <author>原創</author>
    </item>
    <item>
      <title>AI 編程軟件 Cursor 開發商聘請兩位 Anthropic 前高管</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;AI 編程軟件 Cursor 開發商 Anysphere 聘請了兩位來自 Anthropic 的前高管，分別擔任首席架構師兼工程主管和產品負責人。&lt;/p&gt; 
&lt;p&gt;Boris Cherny，曾是 Claude Code 的開發負責人，將擔任 Anysphere 的首席架構師兼工程主管。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0703/113518_qNjw_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Boris Cherny 於 2024 年 9 月加入 Anthropic，入職還不到一年，此前在 Meta 任職首席軟件工程師、 Instagram 的服務器架構和開發基礎設施主管， 以及 Meta 的代碼質量主管，畢業於美國加州大學聖迭戈分校。&lt;/p&gt; 
&lt;p&gt;Cat Wu，曾是 Claude Code 的產品經理，將擔任產品負責人。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0703/113527_RWRC_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Cat Wu 全名 Catherine Wu，2024 年 8 月加入 Anthropic，擅長構建高可靠、可解釋、可控制的人工智能系統，本科畢業於普林斯頓大學，專業計算機科學，加入 Anthropic 之前有多段不同領域工作實習經歷，最長兩年，比如在谷歌實習任職軟件工程師，在 J.P. 摩根實習任職交易員，在 Alexandr Wang 公司 Scale AI 作為作為產品經理任職兩年。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358526</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358526</guid>
      <pubDate>Sun, 11 May 2025 03:37:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>富士康推出首款 AI 推理大模型 「FoxBrain」，商標申請已提交</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;近日，鴻海精密工業股份有限公司（也就是大家熟悉的富士康）在國家知識產權局商標局提交了 「FoxBrain」 商標註冊申請。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;這款 AI 推理大模型不僅是富士康的首次嘗試，更是台灣省首個該類型的 AI 模型。根據公開資料顯示，該商標的國際分類為科學儀器，目前正處於 「等待實質審查」 的狀態。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;img height="363" src="https://oscimg.oschina.net/oscnet/up-853c20df5c0da43f4e610f96af33d644262.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;「FoxBrain」 是鴻海研究院重磅推出的 AI 推理大模型，涵蓋數據分析、數學推理、代碼生成等多個功能。富士康聲稱，FoxBrain 的初始版本基於 Meta 的 Llama3.1 模型進行開發，使用了 120 塊英偉達 H100GPU 進行了為期一個月的訓練。這一模型特別針對繁體中文進行了優化，儘管其性能相較於其他模型，如 DeepSeek，可能稍顯不足。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;值得一提的是，富士康並非台灣省唯一在 AI 領域發力的公司。早前，聯發科也推出了 Llama-Breeze2 系列 AI 模型，這些模型雖然定位為 「輕量級」，但同樣主打繁體中文處理能力。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358520</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358520</guid>
      <pubDate>Sun, 11 May 2025 03:16:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Grok 4 將提供面向編程的「Code」版本</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2FAiBattle_%2Fstatus%2F1940139539525419512" target="_blank"&gt;博主「AiBattle」爆料稱&lt;/a&gt;，其在 xAI 控制枱的源代碼中發現了 2 個 Grok 4 模型的相關信息。&lt;/p&gt; 
&lt;p&gt;據悉，&lt;strong&gt;本次 Grok 4 將擁有標準版和麪向編程的 Code 版&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Grok 4：xAI 最出色、最新的旗艦模型，在自然語言、數學和推理方面表現優秀。&lt;/li&gt; 
 &lt;li&gt;Grok 4 Code：專為編程而生，能夠諮詢代碼相關問題，或者將 Grok 4 Code 嵌入代碼編輯器中。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-9bcf46eea400a93a91e4827ee44b5545da0.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-3a6e92d6992d4ffe25867b6b3a69cdcbc4f.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;另據爆料，Grok 4 的權限已經部分開通，可通過 API 訪問。目前，Grok 4 支持文本模態以及視覺、圖像生成等功能，其他功能也即將推出。&lt;/p&gt; 
&lt;p&gt;馬斯克近日也透露，Grok 4 計劃在今年 7 月 4 日之後發佈；並且新模型將嘗試從第一性原理出發進行推理，也就是將物理學的方法應用到思維過程中。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0630/185329_AUaz_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;相關閲讀：&lt;a href="https://www.oschina.net/news/358033" target="news"&gt;Grok 4 將於 7 月 4 日後發佈&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358516</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358516</guid>
      <pubDate>Sun, 11 May 2025 03:05:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>字節跳動開源 4D 視頻生成框架 EX-4D</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;&lt;span style="background-color:#ffffff"&gt;字節跳動旗下 PICO-MR 團隊正式開源了 EX-4D，一款突破性的 4D 視頻生成框架；能夠從單一視角（單目）視頻生成高質量、多視角的 4D 視頻序列 (3D 空間+時間維度)。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;傳統視頻生成技術在多視角生成方面面臨兩大挑戰:一是需要昂貴的多視角相機和數據集進行訓練;二是難以處理遮擋區域，導致生成的視頻在極端視角下出現物體穿幫或細節失真。EX-4D 通過創新的深度密閉網格（DW-Mesh）表示和輕量級適配架構，成功解決了這些問題。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;DW-Mesh 是 EX-4D 的核心技術，它通過構建全密閉網格結構，記錄場景中的可見和隱形面片，無需多視角監督即可統一處理複雜場景拓撲。結合預訓練深度預測模型，EX-4D 將單幀像素投影到 3D 空間，形成網格頂點，並根據幾何關係精準標記遮擋區域。這種方法確保了生成視頻在極端視角（如±90°）下仍能保持物理一致性和細節完整性。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;此外，EX-4D 引入了兩種模擬 mask 生成策略——渲染 mask 和跟蹤 mask，通過模擬視角移動和幀間一致性，破解了多視角訓練數據的稀缺難題。這些策略使 EX-4D 僅憑單目視頻即可「腦補」全視角數據，極大降低了數據採集成本。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;性能測試結果表明，EX-4D 在 FID（弗雷歇距離）、FVD(弗雷歇視頻距離) 和 VBench 等行業標準指標上全面超越現有開源方法。尤其在極端視角 (如接近 90°) 的生成任務中，EX-4D 的性能優勢尤為明顯，生成的視頻在物體細節和遮擋邏輯上表現更為真實。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;img height="251" src="https://oscimg.oschina.net/oscnet/up-dbd1b11be587afb0f59e10fdb2c7026588a.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0px; margin-right:0px"&gt;&lt;span style="color:#000000"&gt;在一項由 50 位志願者參與的主觀評估中，70.7% 的參與者認為 EX-4D 在極端視角下的物理一致性遠超其他開源方法。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;EX-4D 基於預訓練的 WAN-2.1 模型，結合 LoRA-based Adapter 架構，在保持計算效率的同時，融入了 DW-Mesh 的幾何先驗信息，確保生成視頻的幾何一致性和幀間連貫性。這種輕量級設計使得 EX-4D 在資源受限的環境下也能高效運行，適合廣泛的開發場景。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;span style="background-color:#ffffff"&gt;字節跳動 PICO-MR 團隊負責人表示，EX-4D 是團隊在 3D 重建與 4D 場景生成領域多年研究的結晶，未來將繼續優化模型性能，探索更廣泛的應用場景。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358512</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358512</guid>
      <pubDate>Sun, 11 May 2025 02:52:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>DeepEval —— 開源 LLM 評估框架</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                            &lt;p style="text-align:start"&gt;&lt;span&gt;&lt;span&gt;&lt;span style="color:#1f2328"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style="background-color:#ffffff"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;DeepEval&amp;nbsp;&lt;/strong&gt;是一個簡單易用的開源 LLM 評估框架，用於評估和測試大型語言模型系統。它與 Pytest 類似，但專門用於對 LLM 輸出進行單元測試。DeepEval 結合了最新研究成果，基於 G-Eval、幻覺、答案相關性、RAGAS 等指標來評估 LLM 輸出，它使用 LLM 和其他各種在&lt;strong&gt;本地&lt;/strong&gt;運行的 NLP 模型進行評估。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p style="text-align:start"&gt;&lt;span&gt;&lt;span&gt;&lt;span style="color:#1f2328"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style="background-color:#ffffff"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;無論你的 LLM 應用程序是 RAG &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="background-color:#ffffff; color:#1f2328"&gt;pipelines&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span style="color:#1f2328"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style="background-color:#ffffff"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;、聊天機器人、AI 代理，還是通過 LangChain 或 LlamaIndex 實現，DeepEval 都能滿足你的需求。藉助它，你可以輕鬆確定最佳模型、提示和架構，以改進你的 RAG 管道和代理工作流，防止 &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;prompt drifting&lt;span&gt;&lt;span&gt;&lt;span style="color:#1f2328"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style="background-color:#ffffff"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;，甚至可以自信地從 OpenAI 過渡到託管你自己的 Deepseek R1。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p style="text-align:start"&gt;&amp;nbsp;&lt;/p&gt;

&lt;ul style="margin-left:0; margin-right:0"&gt;
&lt;li&gt;以類似於 Pytest 的方式輕鬆地「單元測試」 LLM 輸出。&lt;/li&gt;
&lt;li&gt;即插即用 30 多個 LLM 評估指標，其中大多數都有研究支持。&lt;/li&gt;
&lt;li&gt;支持端到端和組件級評估。&lt;/li&gt;
&lt;li&gt;對 RAG、代理、聊天機器人以及幾乎任何用例的評估。&lt;/li&gt;
&lt;li&gt;使用最先進的進化技術生成合成數據集。&lt;/li&gt;
&lt;li&gt;指標易於定製並涵蓋所有用例。&lt;/li&gt;
&lt;li&gt;紅隊，安全掃描 LLM 應用程序是否存在安全漏洞。&lt;/li&gt;
&lt;/ul&gt;

&lt;p style="margin-left:0; margin-right:0; text-align:start"&gt;&lt;span&gt;&lt;span&gt;&lt;span style="color:#1c1e21"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;此外，DeepEval 還有一個雲平台&lt;a href="https://app.confident-ai.com/" target="_blank"&gt;Confident AI&lt;/a&gt;，允許團隊使用 DeepEval 在雲端進行&lt;strong&gt;評估、迴歸測試、紅隊和監控 LLM 應用程序。&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt="" height="282" src="https://static.oschina.net/uploads/space/2025/0617/152602_UCxK_4252687.gif" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

                                                                    &lt;/div&gt;
                                                                </description>
      <link>https://www.oschina.net/p/deepeval</link>
      <guid isPermaLink="false">https://www.oschina.net/p/deepeval</guid>
      <pubDate>Sat, 10 May 2025 10:44:00 GMT</pubDate>
    </item>
    <item>
      <title>設計協作平台 Figma 遞交首次公開募股（IPO）申請</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Figma 昨日正式提交了首次公開募股（IPO）申請，計劃在美國紐約證券交易所（NYSE）上市，股票代碼為「FIG」。&lt;/p&gt; 
&lt;p&gt;Figma 成立於 2016 年，主要在網絡上提供界面設計協作服務，同時也推出了 macOS / Windows 平台桌面客户端。該公司的產品線除了最早推出的設計工具 Figma Design 外，還包括在線協作白板 FigJam、演示文稿協作工具 Figma Slides、繪圖工具 Figma Draw、設計自動化軟件 Dev Mode、網站設計工具 Figma Sites，以及用於構建社交平台的 Figma Buzz 等。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0417/183511_QrFp_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Figma 公司曾計劃在 2022 年以 200 億美元&lt;a href="https://www.oschina.net/news/210475/adobe-to-acquire-figma"&gt;出售給 Adobe&lt;/a&gt;，但由於歐盟和英國監管機構擔心該交易會影響市場競爭，相應計劃最終被叫停，迫使 Adobe 在當年底向 Figma 支付了 10 億美元的解約費用。 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-bf6c1fe7a671d0abebcca4cc84a6f1c39da.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;參考 Figma 提交的 S-1 申請文件，今年第一季度，公司擁有 1300 萬月活躍用户（其中三分之二用户並非專業設計師），公司已獲得了 95% 的《財富》500 強企業和 78% 的《福布斯》全球 2000 強企業的青睞。 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;而在營收方面，Figma 在 2024 年實現了 7.49 億美元（現匯率約合 53.65 億元人民幣）營收，同比增長 48%，但全年仍有 7.3 億美元（現匯率約合 52.29 億元人民幣）虧損。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358402</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358402</guid>
      <pubDate>Sat, 10 May 2025 08:54:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>騰訊回應微信「Al 搜索」泄露個人隱私：僅整合公開信息，不會碰用户隱私</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;近日，網友在社交平台吐槽被微信新推出的「AI 搜索」功能強行開盒。&lt;/p&gt; 
&lt;p&gt;該網友發現，當微信推文中出現本人姓名時，名字會變成藍色超鏈接，點擊人名即可一鍵瀏覽公眾號 A1 強制生成的「個人簡歷」及所有涉及該姓名的推文。不少網友在嘗試了該功能後表示「確實可以根據名字查到很多個人資料」，引發隱私安全方面的擔憂。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-ef93511d7c54a97e53e3f1cfb985f5f4df0.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;對此，騰訊方面今日回應媒體稱，為了豐富用户搜索體驗，微信搜索此前通過接入 DeepSeek 和混元等大模型推出 AI 搜索。AI 搜索僅整合公眾號及互聯網其他公開信息，不會使用用户隱私信息。根據用户近期的相關反饋，微信搜索將進一步優化 AI 搜索的使用體驗。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358391</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358391</guid>
      <pubDate>Sat, 10 May 2025 08:03:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>TDMQ RabbitMQ Serverless 版限流機制深度解析與實踐指南</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                                        &lt;h2&gt;導語&lt;/h2&gt; 
&lt;p&gt;分佈式集羣限流是保障雲服務高可用性的核心技術手段，其意義不僅在於防止系統過載，更是構建彈性架構、優化資源效率、實現業務可持續性的關鍵策略。未來，隨着邊緣計算和 Serverless 的普及，限流技術將進一步與底層基礎設施深度融合，成為構建下一代高可用架構的核心基石。&lt;/p&gt; 
&lt;p&gt;騰訊雲 TDMQ RabbitMQ Serverless 版作為一款極致彈性、高性能且高可靠的消息中間件，通過提供穩定低延遲的消息服務，助力企業實現系統異步解耦並高效應對海量消息堆積。然而，在高併發、大流量的實際業務中，如何科學分配資源、規避系統過載風險，已成為保障服務穩定性的關鍵。為此，騰訊雲 TDMQ RabbitMQ Serverless 版引入了集羣級別的分佈式限流機制，通過動態調控集羣的發送與消費速率，確保集羣在高負載下仍能穩定運行。&lt;/p&gt; 
&lt;p&gt;本文將深度剖析騰訊雲 TDMQ RabbitMQ Serverless 版的限流機制，涵蓋限流策略設計、觸發機制及底層實現邏輯。通過真實場景案例解析與實踐指南，系統講解如何通過客户端優化來降低限流影響，同時幫助客户精準掌握集羣限流相關服務端配置技巧，有效規避因流控策略不當引發的業務中斷風險，全面提升高併發場景下的系統穩定性與可靠性。&lt;/p&gt; 
&lt;h2&gt;概要設計&lt;/h2&gt; 
&lt;h3&gt;分佈式限流的必要性&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;資源瓶頸的不可預測性&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在分佈式系統中，單節點流量可能因負載均衡策略（如 Round-Robin）不均導致傾斜。例如，某台服務器因硬件故障觸發重試風暴，流量突增 300%，若無全局視角的限流，可能引發級聯雪崩。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;長尾延遲的放大效應&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;當某服務節點響應延遲升高（如磁盤刷寫延遲增大），後續請求堆積導致線程池耗盡，觸發上游重試，形成惡性循環。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;突發流量衝擊&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;秒殺活動、熱點新聞等場景下，流量可能在毫秒級陡增數十倍。例如，某電商平台大促期間，訂單服務 QPS 從 5k 飆升至 80k，若未通過分佈式限流攔截異常流量，核心計算資源將被瞬間打滿，導致服務不可用。&lt;/p&gt; 
&lt;h3&gt;TDMQ RabbitMQ Serverless 版限流規則&lt;/h3&gt; 
&lt;p&gt;騰訊雲 TDMQ RabbitMQ Serverless 版為超大規模、低時延、高可用性要求的在線業務提供專業級消息服務。客户端通過 RabbitMQ SDK 與 TDMQ RabbitMQ Serverless 版集羣建立長連接，實現高效的消息收發操作，同時動態佔用集羣的計算、存儲及網絡帶寬等關鍵資源。在此背景下，為確保消息服務的高性能與穩定性，在應對高併發、大流量場景時，必須對集羣的負載水位進行精細化管理。 基於集羣的資源配置上限，服務端支持動態調控客户端的每秒消息發送與消費能力（TPS），確保系統在高負載下依然保持穩定運行。&lt;/p&gt; 
&lt;p&gt;為實現資源隔離與靈活適配的雙重目標，系統對發送消息與消費消息的 TPS 配額進行獨立分配，並支持用户按需配置配額比例，從而實現精細化資源管理與業務場景的精準匹配（默認配額比例為 1 : 1 也即 50%）。業務可以根據實際的收發比例進行調整，可調整的收發消息比例範圍在 20%-80%（服務端支持動態調整該區間）之間。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-7e99eeeb65ba2619e29a1109b273abf1cd8.jpg" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;TDMQ RabbitMQ Serverless 版限流行為&lt;/h3&gt; 
&lt;p&gt;騰訊雲 TDMQ RabbitMQ Serverless 版採用 Fail-Fast 限流機制，即當客户端請求速率觸及預設上限時，服務端會即時返回錯誤響應。在響應時間敏感的在線業務場景中，該機制可使客户端實時感知限流事件並主動介入處理，從而有效避免因資源競爭導致的端到端時延長尾，保障業務連續性與系統穩定性。&lt;/p&gt; 
&lt;p&gt;以 1000TPS 規格的基礎集羣為例（假設收發 TPS 比例為 1:1 也即 50%），客户端視角下的限流行為：&lt;/p&gt; 
&lt;p&gt;| &lt;strong&gt;説明&lt;/strong&gt; | &lt;strong&gt;發送消息限流&lt;/strong&gt; | &lt;strong&gt;消費消息限流&lt;/strong&gt; | | ------------ | ------------ | ------------ | | 觸發限流情景 | 所有連接該集羣的發送客户端每秒最多可發送 TPS 總和為 500 條，發送速率達到限制後，超限的發送請求會失敗。 | 所有連接該集羣的消費客户端每秒最多可消費 TPS 總和為 500 條，消費速率達到限制後，消息的消費延遲會增加。 | | 觸發限流時 SDK 日誌關鍵詞 | com.rabbitmq.client.AlreadyClosedException: channel is already closed due to channel error; protocol method: #method&amp;lt;channel.close&amp;gt;(reply-code=530, reply-text=[requestId: 3143682333552716694] Action: pub rate limited by cluster on instance:xxx reason:PublishMessage, class-id=60, method-id=40) | 消費超過閾值以後，客户端使用 BasicGet 拉取消息時，會出現：com.rabbitmq.client.AlreadyClosedException: channel is already closed due to channel error; protocol method: #method&amp;lt;channel.close&amp;gt;(reply-code=530, reply-text=[requestId: 31436823332424324] Action: BasicGet rate limited by cluster rate limiter vhost: xxx. queue: xxx.當客户端使用 BasicConsume 消費消息時，服務端會抑制向客户端 DeliverMessage 的速率，客户端不會感知到明顯的 Channel 斷開的錯誤，整體表現為類似 AMQP 協議消費者 QOS 的行為，會抑制推送到消費者消息的速率，此時消費延遲會增加，可以通過調整限流比例或者增大購買的 TPS 來解決。消費的總 TPS 主要由 BasicGet 和 DeliverMessage 的調用 TPS 次數共享。 | | 觸發限流時 SDK 重試機制 | 客户端 SDK 業務側需要處理連接斷開的行為，需要對發送錯誤被限流的消息重新建立 Channel 連接然後進行發送重試。 | 客户端 SDK 業務消費側會感知到延遲增加。若使用拉取 BasicGet 拉取消息，會感知到 Channel 連接斷開，需要業務上主動重試。 |&lt;/p&gt; 
&lt;h2&gt;詳細設計與實現&lt;/h2&gt; 
&lt;h3&gt;架構設計&lt;/h3&gt; 
&lt;p&gt;騰訊雲 TDMQ RabbitMQ Serverless 版採用雙模式限流架構，兼顧節點級保護與集羣級協同：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;單機限流（Node-Level Throttling）&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;用於節點級資源保護，通過限制 CPU、內存、線程等關鍵資源的使用，防止單節點因過載導致服務不可用。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;分佈式限流（Cluster-Level Throttling）&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;基於集羣全局視角，通過多節點流量協同管理，保護共享存儲資源（如 Broker）及後端系統穩定性。該模式通過使用分佈式限流系統實現。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-cab8876ba86ab2c10b491074cac992a20d0.jpg" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;限流實現&lt;/h3&gt; 
&lt;p&gt;騰訊雲 TDMQ RabbitMQ Serverless 版通過在計算層 （TDMQ RabbitMQ Serverless 版集羣）接入分佈式限流系統實現集羣級讀寫流量控制，其核心機制是：TDMQ RabbitMQ Serverless 版集羣節點在處理 BasicPublish / BasicGet / DeliverMessage 請求前，需通過集成的限流 SDK 向限流 Server 異步上報與申請 Token。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;生產端限流&lt;/strong&gt; ：若 BasicPublish 申請失敗，則立即拒絕生產消息請求並返回錯誤。 &lt;strong&gt;消費端限流&lt;/strong&gt;：若 BasicGet 申請失敗，則立即拒絕拉取消息請求並返回錯誤。若 DeliverMessage 申請失敗，則抑制推送到消費者的消息速率，實現消費端不斷連接的流控，此時類似於 RabbitMQ 開源的實現，此時該消費者處於 Flow 狀態。&lt;/p&gt; 
&lt;p&gt;TDMQ RabbitMQ Serverless 版集羣內部集成限流 SDK，該 SDK 提供 Token 申請 API，並負責與限流 Server 通信，通過這種集中式 Token 管理實現對核心存儲層 (底座 Broker 集羣) 的保護。&lt;/p&gt; 
&lt;h3&gt;限流實現難點一：如何平衡性能與精度&lt;/h3&gt; 
&lt;p&gt;使用 TDMQ RabbitMQ Serverless 版的各類在線業務通常對時延比較敏感，如果計算層節點處理每次讀寫請求都執行一次 Limiter RPC 調用（SDK -&amp;gt; Server）的話，雖然 Limiter Server 內部處理耗時幾乎可以忽略，但兩次 RPC 的網絡 IO 耗時對消息端到端時延的影響是不能忽視的。&lt;/p&gt; 
&lt;p&gt;實際上從服務端的角度看， TDMQ RabbitMQ Serverless 版執行限流的主要目的是防止核心存儲層過載，而非追求 100% 精準的流量控制，即 SDK 與 Server 之間的強同步並不是必須的。因此，為了在限流性能和限流精度之間取得平衡，Limiter 採用了一種【先消費後結算】的 Token 管理機制，Token 申請過程在限流 SDK 內部閉環，SDK 會週期性（週期大概在 50ms 以內，上報週期越短，限流越敏感）地向限流 Server 異步上報 Token 使用量並同步配額。&lt;/p&gt; 
&lt;p&gt;騰訊雲 TDMQ RabbitMQ Serverless 版的限流機制通過以下四大核心特性，在保障系統穩定性的同時實現高性能與低時延的平衡：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;內存級處理，主鏈路零幹擾&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;執行機制&lt;/strong&gt;：限流判斷為純內存操作，不涉及外部 RPC 調用，確保消息處理流程完全不受阻塞。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;性能優勢&lt;/strong&gt;：主鏈路延遲無感知，適用於對響應時間要求嚴苛的在線業務場景。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;先消費後結算，消除誤限流風險&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;設計原理&lt;/strong&gt; ：採用異步 Token 核銷機制，客户端可先執行操作，限流 SDK 後續異步週期性同步配額消耗。 &lt;strong&gt;效果保障&lt;/strong&gt;：杜絕因限流判斷延遲導致的正常請求被誤拒，確保業務連續性。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;短暫超限容忍，資源緩衝池兜底&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;場景説明&lt;/strong&gt; ：在流量毛刺等突發場景中，可能出現瞬時配額超限，由於先消費後結算的機制導致。 &lt;strong&gt;容錯機制&lt;/strong&gt;：通過服務端資源預留 Buffer 吸收流量波動，避免因短暫超限觸發系統風險。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;彈性容錯設計，弱耦合架構保障可用性&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;故障降級策略&lt;/strong&gt; ：當限流 Server 服務異常時，系統自動切換至單機 Sentinel 組件實現基礎單機限流功能。 &lt;strong&gt;依賴特性&lt;/strong&gt;：對限流 Server 服務實現弱耦合架構，可以通過隨時動態降級來避免限流 Server 服務異常導致的服務異常，確保分佈式限流服務的高可用。&lt;/p&gt; 
&lt;h3&gt;限流實現難點二：如何平滑限流毛刺&lt;/h3&gt; 
&lt;p&gt;騰訊雲 TDMQ RabbitMQ Serverless 版採用 TPS 作為集羣規格單位，用於衡量集羣的吞吐能力。例如，1000TPS 表示集羣每秒可處理 1000 條 TPS（即綜合生產、消費等操作的加權計算）。在分佈式限流系統中，這一規格對應每秒分配 1000 個 Token，其中 "一秒"即為默認的限流計數週期，用於動態控制流量配額。&lt;/p&gt; 
&lt;p&gt;在使用限流服務的實際運維中發現：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;短週期（如 1 秒）&lt;/strong&gt;： 優勢：對流量波動敏感，可快速響應潛在過載風險； 缺陷：易因短暫毛刺誤觸限流，影響正常業務波動場景。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;長週期（如 10 秒）&lt;/strong&gt;： 優勢：容忍毛刺，降低誤控率； 缺陷：服務端資源需承受更高瞬時衝擊風險。&lt;/p&gt; 
&lt;p&gt;為平衡流量控制精度與用户體驗，騰訊雲 TDMQ RabbitMQ Serverless 版將默認限流計數週期從 1 秒調整為 10 秒。這樣既降低了用户因毛刺導致的限流困擾，又通過利用少量的服務器預留資源 Buffer 來承載瞬時流量衝擊，為高併發場景下的消息處理提供了可靠的支撐。&lt;/p&gt; 
&lt;h3&gt;限流實現難點三：如何實現消費端限流&lt;/h3&gt; 
&lt;p&gt;騰訊雲 TDMQ RabbitMQ Serverless 版集羣使用 AMQP 協議與客户端交互，然而 AMQP 協議中並沒有定義很好的處理 Fail-Fast 限流錯誤的幀，因此在發送消息被限流的情況下，只能通過關閉 Channel 連接來通知到客户端，此時客户端會收到相應的 AlreadyClosedException 異常，然後業務需要通過重試來解決當前時間窗口內消息發送被限流的問題。&lt;/p&gt; 
&lt;p&gt;而在消費端限流的情況下，分為兩種情況，AMQP 協議中支持兩種消費模式，BasicGet（拉模式) 和 BasicConsume（推模式）。 此時對消費端的限流就需要考慮消費的連續性和延遲。針對 BasicGet 模式，是客户端發起的主動同步拉取消息的命令，此時客户端每一次拉取消息是可以直接感知到是否被限流的，更好的方式是通過關閉連接來讓客户端感知到限流，從而讓業務上通過重試來解決拉取當前時間窗口內消息消費被限流的問題。&lt;/p&gt; 
&lt;p&gt;但是針對 BasicConsume（推模式）, 同時也是 AMQP 客户端最普遍的使用方式，考慮到客户端開啓一個長連接監聽相應隊列上的消息，此時如果因為限流粗暴地關閉 Channel 連接, 此時的客户端往往不能實時感知到連接 Channel 斷開，增加了客户端業務上處理的複雜度，同時消費側重建 Channel 連接也會讓消費流量充滿毛刺和消費延遲增加。因此騰訊雲 TDMQ RabbitMQ Serverless 版在推模式下使用消費抑制的方式來實現消費端限流，當消費 TPS 超過閾值時，會減少推送到客户端的頻率，保證了在連接 Channel 不斷開的情況下，消費流量的平穩，儘量減少因為限流導致的消費延遲。&lt;/p&gt; 
&lt;h2&gt;客户端實踐教程&lt;/h2&gt; 
&lt;h3&gt;規劃集羣限流負載&lt;/h3&gt; 
&lt;p&gt;騰訊雲 TDMQ RabbitMQ Serverless 版的限流機制旨在保障服務穩定性與可靠性。防止在集羣高負載時出現服務響應長尾毛刺，最終導致請求成功率下降，業務受損等問題。因此，在接入時建議：客户側需要提前規劃集羣負載，依據當前規模和未來趨勢預測來充分評估業務 TPS， 如果業務流量具有波動特性，應以峯值 TPS 為準，根據相應的評估後的 TPS 購買相應規格的實例集羣。&lt;/p&gt; 
&lt;h3&gt;限流相關告警配置&lt;/h3&gt; 
&lt;p&gt;騰訊雲 TDMQ RabbitMQ Serverless 版默認接入了騰訊雲監控的能力，可以利用騰訊雲 TDMQ RabbitMQ Serverless 版控制枱的監控告警能力實現對集羣負載的實時觀測，提前發現 TPS 水位風險並及時操作升配來避免觸發限流導致業務受損。告警策略建議：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;發送和消費 TPS 水位超過容量的 70% 時觸發告警，提醒進行升配評估。&lt;/li&gt; 
 &lt;li&gt;出現發送限流時觸發告警，警告業務發送消息可能失敗風險。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;客户端限流異常處理&lt;/h3&gt; 
&lt;p&gt;業務代碼通過 RabbitMQ SDK 發送消息時，需要捕獲包括限流錯誤在內的異常，並保存必要的上下文信息，以便人工介入恢復業務。當騰訊雲 TDMQ RabbitMQ Serverless 版實例的 TPS 流量峯值超過騰訊雲控制枱所購買實例的 TPS 規格上限時，業務側生產消費流量會被限流。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;限流後的行為如下&lt;/strong&gt;：&lt;/p&gt; 
&lt;p&gt;騰訊雲 TDMQ RabbitMQ Serverless 版服務端會返回錯誤碼信息。&lt;/p&gt; 
&lt;p&gt;騰訊雲 TDMQ RabbitMQ Serverless 版服務端關閉當前請求的 Channel 連接，代碼中可以捕獲異常並重新打開 Channel 連接，具體請參見錯誤碼處理示例代碼章節。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;錯誤碼信息&lt;/strong&gt;： 錯誤碼：reply-code=530&lt;/p&gt; 
&lt;p&gt;錯誤信息：reply-text=[requestId: 3143682333552716694] Action: pub rate limited by cluster on instance:xxx reason:PublishMessage, class-id=60, method-id=40)&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;錯誤堆棧&lt;/strong&gt;：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Suppressed: com.rabbitmq.client.AlreadyClosedException: channel is already closed due to channel error; protocol method: #method&amp;lt;channel.close&amp;gt;(reply-code=530, reply-text=[requestId: 3143682333552716694] Action: pub rate limited by cluster on instance:amqp-autotest reason:PublishMessage, class-id=60, method-id=40)
at com.rabbitmq.client.impl.AMQChannel.processShutdownSignal(AMQChannel.java:437)
at com.rabbitmq.client.impl.ChannelN.startProcessShutdownSignal(ChannelN.java:295)
at com.rabbitmq.client.impl.ChannelN.close(ChannelN.java:624)
at com.rabbitmq.client.impl.ChannelN.close(ChannelN.java:557)
at com.rabbitmq.client.impl.ChannelN.close(ChannelN.java:550)
at com.rabbitmq.client.impl.recovery.AutorecoveringChannel.lambda$close$0(AutorecoveringChannel.java:74)
at com.rabbitmq.client.impl.recovery.AutorecoveringChannel.executeAndClean(AutorecoveringChannel.java:102)
at com.rabbitmq.client.impl.recovery.AutorecoveringChannel.close(AutorecoveringChannel.java:74)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;RabbitMQ AMQP Java SDK 業界使用的比較廣泛，因此使用該 SDK 作為示例，RabbitMQ AMQP Java SDK 不會對限流錯誤進行自動重試，因此業務代碼需要捕獲異常並進行處理，示例代碼如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;private static final int MAX_RETRIES = 5; // 最大重試次數
private static final long WAIT_TIME_MS = 2000; // 每次重試的等待時間（以毫秒為單位）
private void doAnythingWithReopenChannels(Connection connection, Channel channel) {
    try {
        // ......
        // 在當前通道 channel 下執行的任何操作
        // 例如消息發送、消費等
        // ......
    } catch (AlreadyClosedException e) {
        String message = e.getMessage();
        if (isChannelClosed(message)) {
            // 如果通道已經關閉，關閉並重新創建通道
            channel = createChannelWithRetry(connection); 
            // 在重連後可以繼續執行其它操作
            // ......
        } else {
            throw e;
        }
    }
}
private Channel createChannelWithRetry(Connection connection) {
    for (int attempt = 1; attempt &amp;lt;= MAX_RETRIES; attempt++) {
        try {
            return connection.createChannel();
        } catch (Exception e) {
            System.err.println("Failed to create channel. Attempt " + attempt + " of " + MAX_RETRIES);
            // 檢查錯誤, 若仍是被限流導致的關閉錯誤，則可以等待後繼續重試
            // 也可移除本部分重試邏輯
            if (attempt &amp;lt; MAX_RETRIES) {
                try {
                    Thread.sleep(WAIT_TIME_MS);
                } catch (InterruptedException ie) {
                    Thread.currentThread().interrupt(); // 還原中斷狀態
                }
            } else {
                throw new RuntimeException("Exceeded maximum retries to create channel", e);
            }
        }
    }
    throw new RuntimeException("This line should never be reached"); // 理論上不會到達這裏
}
private boolean isChannelClosed(String errorMsg) {
    // 判斷是否包含 channel.close 報錯，該報錯代表通道已關閉。
    // 可能涵蓋 530，541 等錯誤信息。
    if (errorMsg != null &amp;amp;&amp;amp; errorMsg.contains("channel.close")) {
        System.out.println("[ChannelClosed] Error details: " + errorMsg);
        return true;
    }
    return false;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;總結&lt;/h2&gt; 
&lt;p&gt;騰訊雲 TDMQ RabbitMQ Serverless 版通過分佈式限流架構為在線業務提供高可用的消息服務保障。在分佈式限流模式下，服務端通過集中式 Token 管理系統（限流 Limiter）動態分配資源配額，防止因突發流量衝擊導致核心存儲層（底座 Broker 集羣）過載來提高系統穩定性，同時採用 【先消費後結算】的異步處理模式，客户端在限流 SDK 內部閉環完成 Token 申請，避免阻塞主鏈路，確保限流調用接口低延時，限流 SDK 週期性同步 Token 消耗數據至限流 Server，最終平衡了限流精度與調用限流服務的性能開銷。同時針對消費端的限流可以實現不斷 Channel 連接，實現了消費端在限流毛刺與消費延遲之間的雙重保證，此外，面對限流 Server 服務不可用的情況，系統能夠自動動態降級為單機限流模式，確保客户端請求的連續性，保持對限流服務的弱依賴設計來實現系統解耦。&lt;/p&gt; 
&lt;p&gt;在實際應用與運維中，同時也需要客户業務方的配合，在接入騰訊雲 TDMQ RabbitMQ Serverless 版服務時，業務方客户需要根據業務規模和未來趨勢合理規劃集羣，預留足夠的 TPS 配額以應對突發流量，防患於未然。同時騰訊雲 TDMQ RabbitMQ Serverless 版提供了服務端完善的監控和告警，支持客户通過監控告警能力實時訂閲集羣負載，提前發現 TPS 水位風險並及時進行升配等操作。在客户端業務代碼層面，需要捕獲限流異常並處理，保證代碼的健壯性，同時保存必要的上下文信息，以便人工查看相關日誌最終介入來恢復業務。&lt;/p&gt; 
&lt;p&gt;通過本文對騰訊雲 TDMQ RabbitMQ Serverless 版的限流機制的介紹與實踐教程，相信讀者對騰訊雲 TDMQ RabbitMQ Serverless 版的限流機制有了更深入的理解，並能夠在實際項目中靈活應用，最終為業務的高併發、大流量場景提供穩定的支持。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/4587289/blog/18638288</link>
      <guid isPermaLink="false">https://my.oschina.net/u/4587289/blog/18638288</guid>
      <pubDate>Sat, 10 May 2025 07:34:00 GMT</pubDate>
      <author>原創</author>
    </item>
    <item>
      <title>百度搜索迎來十年來最大改版：AI 智能框、百看、AI 助手全面進化</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;在近日的百度 AI Day 開放日上，百度搜索宣佈進行了其十年來最大規模的改版，此次革新涵蓋了搜索框、搜索結果頁以及整個搜索生態。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="262" src="https://oscimg.oschina.net/oscnet/up-98d15d20c6065a79b817c094d1a37c4066c.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;升級後的百度搜索框被命名為「智能框」，顯著增強了其輸入能力，現在可支持超過千字的文本輸入。同時，拍照、語音、視頻等多種輸入方式也得到全面加強，並能直接調取 AI 寫作、AI 作圖等創作工具，極大地豐富了用户與搜索的交互方式。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;「百看」功能也在此次改版中實現了全面升級，不僅支持圖文、音視頻的混合內容輸出，還創新性地接入了智能體和真人服務等功能，旨在提供更豐富、多元的信息呈現形式。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;「AI 助手」的升級是本次改版的另一大亮點，新增了視頻通話功能，並顯著提升了多模態輸入、富媒體輸出、一站式工作台及深度搜索能力，使其成為更全面的 AI 輔助工具。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;此外，智能創作能力也得到大幅提升，用户現在只需一句話即可生成三分鐘的創意視頻，並支持分鏡編輯和自定義畫面內容。截至目前，百度搜索開放平台已成功接入 1.8 萬多個優質 MCP（多媒體內容提供商），使其成為國內最大的 AI 生態系統。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;值得一提的是，此次百度搜索還接入了商業研發團隊自研的視頻生成模型 MuseSteamer 及其創作平台「繪想」。據瞭解，MuseSteamer 是全球首個實現中文音視頻一體化生成的視頻模型。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/358379</link>
      <guid isPermaLink="false">https://www.oschina.net/news/358379</guid>
      <pubDate>Sat, 10 May 2025 07:20:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>向量檢索算法：從哈希、樹到量化與圖</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                                        &lt;p&gt;向量檢索這門技術，其發展由來已久，可以追溯到上世紀六七十年代。1975 年發表的 KD 樹算法，就是早期經典的高維數據檢索算法之一。&lt;strong&gt;然而，此後近四十年間，向量檢索長期處於冷門狀態，並沒有特別多的應用需要它。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;直到 2015 年，ImageNet 圖片分類數據集及何愷明教授的 ResNet 等突破性論文引爆了深度學習，使得模型在多個任務上超越人類。推薦系統和搜索引擎快速成為向量檢索技術主要落地場景，向量引擎也由此開始大規模應用。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;大模型爆發又掀起第二輪熱潮：&lt;/strong&gt;基於向量檢索的 RAG 架構，已成為解決模型幻覺、實現知識實時更新的關鍵技術，推動其在多模態、企業知識庫等場景爆發式應用。&lt;/p&gt; 
&lt;p&gt;不久前，&lt;strong&gt;開源中國直播欄目《數智漫談》邀請到了傅聰博士，分享了向量檢索技術的發展情況。&lt;/strong&gt;傅聰於浙江大學計算機博士畢業，曾赴美國南加州大學訪問研究，其主導發明的 NSG、SSG、PSP、MAG 等高性能檢索算法，已落地為千億級向量檢索系統，成為工業界大規模檢索的標杆方案。目前，傅聰博士在 shopee(新加坡) 擔任資深算法專家，專注於 AI 大規模應用落地方面的研究。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;微信掃碼，觀看直播回放：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="189" src="https://oscimg.oschina.net/oscnet/up-a3d943da38c62d33dda46a1b30db2454488.png" width="200" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;本文根據直播整理，介紹四種向量檢索算法類型：哈希算法、樹算法、量化算法、圖算法。其中，哈希算法、樹算法慢慢淡出了歷史舞台，量化算法、圖算法是當前主流。&lt;/p&gt; 
&lt;span id="OSC_h4_1"&gt;&lt;/span&gt; 
&lt;h4&gt;哈希算法：向量檢索的古早形態，已逐漸被淘汰&lt;/h4&gt; 
&lt;p&gt;向量檢索的最初形態——哈希算法，是一種歷史悠久的向量索引方式。&lt;/p&gt; 
&lt;p&gt;從事計算機相關行業的朋友，對哈希表應該都不陌生。哈希的本質是什麼？就是把數據通過一個哈希映射函數，映射到哈希桶（buckets）裏。目標是讓每個桶裏的數據分佈儘可能均勻，這樣就能通過高效的二進制方式快速檢索數據。&lt;/p&gt; 
&lt;p style="text-align:center"&gt;&lt;img height="330" src="https://oscimg.oschina.net/oscnet/up-3ebf5adc615560ea36947f573932442be26.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;向量哈希函數與我們常規理解的哈希算法，核心區別在於，哈希函數本身是專門為向量設計的。大家無需深究其具體實現，只需知道這類函數能將向量相對均勻地分散到各個哈希桶中，從而實現高效檢索。&lt;/p&gt; 
&lt;p&gt;那麼，為什麼哈希算法沒有在當下各領域大規模應用呢？主要原因有兩個：一是哈希表索引結構本身非常龐大；二是其效率低且精度差。&lt;/p&gt; 
&lt;p&gt;我們可以以構建一個線上工業級併發系統為例來説明。通常來説，系統性能的核心目標通常包括 QPS（即單機處理用户請求的最大併發能力）和 recall（召回精度）。在特定的延遲要求下，評估向量檢索算法的關鍵指標是——在此延遲約束內能達到多高的召回精度。而哈希算法最大的問題恰恰是其精度不足，因此才逐漸被淘汰。&lt;/p&gt; 
&lt;span id="OSC_h4_2"&gt;&lt;/span&gt; 
&lt;h4&gt;&lt;strong&gt;樹算法：單棵樹精度不夠，曇花一現&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;歷史上與哈希算法一樣曇花一現的，還有樹算法。它的本質就是分治思想。設想一個龐大複雜的高維向量空間，目標是通過方法將其逐層切分，最終把整個空間分割成許多小格子（也就是葉子節點），並讓每個葉子節點包含的向量數量儘量接近。&lt;/p&gt; 
&lt;p style="text-align:center"&gt;&lt;img height="391" src="https://oscimg.oschina.net/oscnet/up-f1128fc60ac26c9adde0404801cc8352ec4.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;這樣就能借助樹結構高效的索引邏輯進行快速查詢。入門數據庫時學到的那些索引，很多其實就是這種樹結構，只是切分方式不同。&lt;/p&gt; 
&lt;p&gt;這種樹索引有個明顯特點：體積很大。 別光看它是一棵樹就覺得體積小。問題在於，單棵樹的檢索精度通常不夠好。所以實際應用中，要構建多棵樹。&lt;/p&gt; 
&lt;p&gt;比如上個時代，有名的開源檢索庫 FLANN (Fast Library for Approximate Nearest Neighbors)，它在計算機視覺領域用得挺多，就不會只建一棵樹，而是構建多棵隨機樹進行並行查詢，以此來提升效果。代價就是索引會膨脹得厲害。&lt;/p&gt; 
&lt;p&gt;相比哈希算法，它的精度確實高一些，但通常還是達不到在線產品系統的要求。關鍵問題在於：即使勉強達到某個精度（可能還不達標），它的檢索速度也依然非常慢。&lt;/p&gt; 
&lt;span id="OSC_h4_3"&gt;&lt;/span&gt; 
&lt;h4&gt;量化算法：低損耗，空間切分類算法的性能天花板&lt;/h4&gt; 
&lt;p&gt;量化算法，和前兩類算法（哈希和樹算法）其實比較相似，可以把它們理解為空間切分類算法。這類算法有一個共同的缺陷，就是索引效率較低。&lt;/p&gt; 
&lt;p&gt;但量化算法另闢蹊徑，它用一種低損耗的方式實現了較好的空間切分。這裏有個比較經典的例子，就是 Meta 的 FAISS 庫（前身由 Facebook 開發），它最早集成的核心算法 IVFPQ（Product Quantization）代表了十年前的主流方案。PQ 的核心邏輯是通過聚類來切分空間，這和前兩類算法的思路不太一樣。&lt;/p&gt; 
&lt;p&gt;它具體是怎麼做的呢？在切分好的空間裏，選取中心點（圖中紅點）作為區域代表點。用户的查詢向量只需要和這些中心點計算距離。這個距離近似等價於查詢向量與該區域內所有原始數據點（藍點）的距離比較。這樣就能快速篩選出一部分近鄰候選向量，最後再用真實距離進行精確比較。&lt;/p&gt; 
&lt;p style="text-align:center"&gt;&lt;img height="233" src="https://oscimg.oschina.net/oscnet/up-15d3f09345246ba6a850f9370c91c0d7c3f.png" width="500" referrerpolicy="no-referrer"&gt;&lt;br&gt; （圖片來源於，知乎用户 @vegetabledog）&lt;/p&gt; 
&lt;p&gt;這種算法的優勢很明顯：索引結構非常小（內存佔比低），檢索速度足夠快，而且精度也比哈希和樹算法更高。量化算法發展到這裏，可以説空間切分類算法的性能天花板已經顯現了。&lt;/p&gt; 
&lt;p&gt;也正是在這個階段，創新的圖檢索算法開始興起。&lt;/p&gt; 
&lt;span id="OSC_h4_4"&gt;&lt;/span&gt; 
&lt;h4&gt;圖算法：精度高、速度快，當前被普遍採用&lt;/h4&gt; 
&lt;p&gt;圖檢索算法可以説和前面三種算法（哈希、樹、量化）是完全不同的思路。&lt;/p&gt; 
&lt;p&gt;前面那些算法的核心是「分而治之」，像切割領土、築牆建院一樣去切割空間。而圖算法的核心思想恰恰相反，它是要打通區塊之間的壁壘——不是去切分空間，而是在空間的點與點之間構建四通八達的「高速公路」，這就是圖算法的設計理念。&lt;/p&gt; 
&lt;p&gt;圖算法的特點是，它的索引會比量化算法大很多。但從當前各種基準測試的表現來看，它能在保持極快速度的同時，以高精度碾壓量化類算法。&lt;/p&gt; 
&lt;p&gt;那麼圖算法是怎麼檢索的呢？&lt;/p&gt; 
&lt;p&gt;假設我們已經構建好一個圖結構，圖中每個點只連接空間中的少量鄰近點。比如起始點是黑色點 M，目標查詢點是左下角的黑色點 P。從任意起點 M 逼近目標 P 的過程，本質上是一個貪婪式的圖上「遊走」：每一步都在當前點的鄰近中尋找離 P 最近的那個點跳轉過去，然後迭代重複這個過程。這樣一步步跳轉，最終就能逼近目標區域（紅色點即為最近的候選結果）。&lt;/p&gt; 
&lt;p style="text-align:center"&gt;&lt;img height="331" src="https://oscimg.oschina.net/oscnet/up-bb7e20fa086e16efab68928e5adbe9301ff.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;想要讓圖算法發展得更好——精度更高、速度更快——關鍵在於兩點：第一，路徑越短越好（跳轉次數少則速度快）；第二，鄰近越少越好（每個點計算量小則算力需求低）。這意味着理想的圖結構應該是足夠稀疏，並且點與點之間的連通路徑儘可能短。&lt;/p&gt; 
&lt;span id="OSC_h4_5"&gt;&lt;/span&gt; 
&lt;h4&gt;選型：量化算法還是圖算法？&lt;/h4&gt; 
&lt;p&gt;現在開源項目或 Demo 中，最常用的就是量化算法和圖算法。那要怎麼選型、找到適用場景？根據長期工作經驗總結如下：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;如果數據總量較小，在線精度要求不高，但對內存或 GPU 顯存有嚴格限制，就比較適合用量化算法。因為量化是用聚類中心代表其他點，這個過程必然有信息損失——數據量越小損失越小，數據量越大精度下降越明顯。&lt;/li&gt; 
 &lt;li&gt;如果數據總量非常大，同時要求高精度、低延遲，並且有充足的內存資源（目前內存也相對便宜了），那麼圖算法是更好的選擇。實際上，當前無論是開源項目還是閉源產品，主流的向量檢索方案普遍都在使用圖算法。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="text-align:center"&gt;&lt;img height="167" src="https://oscimg.oschina.net/oscnet/up-dc8e9e8b16cadc5a2d42d658c4914e8ae48.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;量化算法有一個典型場景，就是推薦系統中的 GPU 端向量召回。&lt;/strong&gt;例如在電商平台，需要構建一個約 2000-3000 萬規模的精品商品庫，通過向量表徵模型（如生成 64 維或 128 維向量）實時召回個性化商品作為推薦候選集。&lt;/p&gt; 
&lt;p&gt;這些向量表徵模型部署在 GPU 上進行推理，因此生成的向量數據會直接駐留在 GPU 顯存中。我們希望向量檢索也能在顯存內完成，避免向 CPU 傳輸數據。然而，模型本身可能已佔用 10 GB 以上的顯存，剩餘可用顯存往往僅剩 2-3GB。&lt;/p&gt; 
&lt;p&gt;在這種顯存受限的場景下，量化類檢索算法就顯得尤為合適。類似地，如果大模型應用中的知識庫規模有限，且需要端到端 GPU 處理，量化算法也是理想選擇。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;至於圖算法的典型場景，可以千億級視頻版權檢測為例。&lt;/strong&gt;抖音、快手等短視頻平台，抖音、快手這類平台每日面臨海量視頻上傳，必須在極短時間內完成版權篩查以避免法律風險，&lt;/p&gt; 
&lt;p&gt;其核心流程可分為三步：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;特徵提取：針對版權庫視頻，提取關鍵幀的畫面特徵並壓縮為高維向量（如 CLIP 生成 1024 維向量），確保內容相似的視頻片段在向量空間中距離相近；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;索引構建：將向量存入支持 ANN（近似最近鄰）檢索的專用數據庫（如 Milvus/FAISS），構建千億級向量索引；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;實時檢索：新視頻上傳時，實時提取其特徵向量，在毫秒級延遲內從向量庫中檢索相似內容，返回相似度超過閾值（如 &amp;gt;95%）的潛在侵權片段。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;面對這種超大規模數據+高併發+高精度的需求，圖檢索算法，可以説是目前最有效的解決方案。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;小結：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;縱觀向量檢索的發展，從早期的哈希、樹算法，到當前主流的量化、圖算法，技術的迭代始終圍繞着效率、精度與資源消耗的平衡。哈希與樹算法受限於精度或效率瓶頸，逐漸淡出主流視野；量化算法憑藉其低內存消耗的特性，在資源受限場景（如 GPU 顯存下的中小規模召回）找到了穩固位置；而圖算法則以其高精度、低延遲的優勢，成為應對千億級海量數據、高併發在線檢索的首選。&lt;/p&gt; 
&lt;p&gt;技術迭代一直在進步。傅聰博士主導發明的基於圖的 NSG 算法，已成為當前主流方案之一，但仍然在實踐中不斷演進——目前，他在 Shopee（新加坡） 帶領團隊已將其迭代至第三代 PSP 與第四代 MAG 算法，不斷突破着大規模向量檢索的性能邊界。&lt;br&gt; &amp;nbsp;&lt;/p&gt; 
&lt;hr&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span&gt;&lt;span style="background-color:#ffffff"&gt;&lt;span&gt;&lt;strong&gt;&lt;strong&gt;&lt;span&gt;&lt;span style="background-color:#ffffff"&gt;&lt;span&gt;&lt;span style="color:#27ae60"&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;【數智漫談】&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span&gt;&lt;span style="background-color:#ffffff"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;OSCHINA 視頻號直播暢聊欄目【數智漫談】，每期一個技術話題，三五位專家圍坐，各抒己見，暢聊開源。給大家帶來最新的行業前沿、最熱門的技術話題、最有趣的開源項目、最犀利的思想交鋒。如果你手上也有新點子、好項目，想要跟同行交流分享，歡迎聯繫我們，講壇隨時開放～&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:center"&gt;&lt;img height="537" src="https://oscimg.oschina.net/oscnet/up-4dd54c1b0b817689ceefa15aa66d79cfae8.png" width="400" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/3859945/blog/18683168</link>
      <guid isPermaLink="false">https://my.oschina.net/u/3859945/blog/18683168</guid>
      <pubDate>Sat, 10 May 2025 07:17:00 GMT</pubDate>
      <author>原創</author>
    </item>
  </channel>
</rss>
