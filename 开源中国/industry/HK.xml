<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - industry - 繁體中文（香港）</title>
    <link>https://www.oschina.net/news/industry</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news/industry" rel="self" type="application/rss+xml"/>
    <description>已對該 RSS 進行格式化操作：中英字符之間插入空格、使用直角引號、標點符號修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-hk</language>
    <lastBuildDate>Sat, 28 Jun 2025 12:42:53 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>關鍵領域軟件工廠的安全中樞：Gitee Scan 全面升級供應鏈檢測能力</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;blockquote&gt; 
 &lt;p&gt;作者：Gitee DevSecOps 團隊，李穎萍，李文博&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;隨着軟件供應鏈安全體系在互聯網、金融等領域逐步成熟，關鍵領域正加速邁向以 MLOps、軟件工廠為核心的新型研發生態。在這一過程中，面對代碼安全、依賴合規、系統可信等多重挑戰，傳統人工審查模式已難以滿足國家級高安全性要求。&lt;/p&gt; 
&lt;p&gt;Gitee Scan&amp;nbsp;&lt;strong&gt;集成 SAST（靜態應用安全測試）、DAST（動態應用安全測試）、SBOM（軟件物料清單）等關鍵技術&lt;/strong&gt;，面向關鍵領域企業提供自動化安全檢測能力，為軟件供應鏈構建「可視、可控、可追溯」的防線。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;關鍵領域軟件安全合規的挑戰&lt;/h2&gt; 
&lt;p&gt;關鍵領域軟件系統需滿足比通用商業軟件更為嚴苛的安全標準，挑戰主要包括：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;高級威脅對抗能力不足：如供應鏈攻擊、零日漏洞利用、側信道攻擊等，要求具備防篡改、防逆向的設計能力。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;依賴來源可信難保障：缺乏有效 SBOM 工具與流程，組件溯源難、更新慢。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;研發環境隔離性強：普遍採用物理內網與封閉開發工具鏈，難以對接現代 DevSecOps 流程。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;安全環節後置：採用瀑布式模型，無法實現安全「左移」，風險發現延遲。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;開發工具不兼容：現有工具難以集成自動化 CI/CD 流程。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;生命週期極長：系統生命週期長達 10～30 年，對漏洞管理與文檔留存要求極高。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;法規合規負擔重：需同時滿足 GJB5000A、GJB8114、ISO 27001、NIST SP800 等多套標準體系。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;過程留痕難：人工文檔負擔重，難以滿足 DevSecOps 強調的「基礎設施即代碼」理念。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;Gitee Scan：構建關鍵領域軟件工廠的質量中樞&lt;/h2&gt; 
&lt;p&gt;作為 Gitee DevSecOps 平台中質量保障的核心組件，Gitee Scan 擔綱軟件工廠中的質量車間角色，貫穿從代碼提交到漏洞修復的全過程，構建覆蓋 SAST、DAST、SBOM 的安全掃描閉環。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0627/192102_QtGg_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;核心技術能力&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Gitee Scan 採用自主研發的 BCA 掃描引擎&lt;/strong&gt;，源自獨創代碼執行鏈分析技術（已申請專利），結合 AST 靜態分析、控制流/數據流建模與指紋匹配算法，實現高精度漏洞識別。&lt;/p&gt; 
&lt;p&gt;目前，已上線的有 BCA–Kotlin、BCA-Java、BCA-OC、BCA-Cobol、BCA-SQL、BCA-C/C++，覆蓋 CWE、OWASP Top 10、GJB8114 等主流規則體系。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0627/192119_G2qO_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;高性能架構設計&lt;/h3&gt; 
&lt;p&gt;支持分佈式部署與高併發掃描，結合權限隔離與多租户機制，實現平台級彈性伸縮與私有部署能力。&lt;/p&gt; 
&lt;h3&gt;安全閉環能力&lt;/h3&gt; 
&lt;p&gt;與 Gitee Team 集成，提供掃描-跟蹤-整改-審計全流程聯動，支持問題歸屬與整改責任劃分，留痕可追溯，滿足關鍵領域內審要求。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;多維掃描能力構建安全基礎設施&lt;/h2&gt; 
&lt;h3&gt;SAST（靜態應用安全測試）&lt;/h3&gt; 
&lt;p&gt;基於自研 BCA 引擎，解析代碼結構與執行路徑，識別高危漏洞（如注入類、緩衝區溢出、硬編碼憑據等），並對超長函數、圈複雜度、重複代碼等可維護性指標進行分析。誤報率控制在 10% 以下，處於行業領先水平。&lt;/p&gt; 
&lt;h3&gt;SBOM 分析能力&lt;/h3&gt; 
&lt;p&gt;自動生成軟件物料清單（SBOM），支持對開源組件、第三方依賴、內部模塊的全量溯源，標註許可證信息與風險等級，助力組件可信評估。&lt;/p&gt; 
&lt;h3&gt;DAST（動態應用安全測試）&lt;/h3&gt; 
&lt;p&gt;結合模擬輸入與接口探測，自動發現服務層漏洞，提升運行時風險發現覆蓋面。與 SAST 形成靜動結合檢測體系。&lt;/p&gt; 
&lt;h2&gt;未來展望：AI 驅動的智能安全助手&lt;/h2&gt; 
&lt;p&gt;同時，Gitee Scan 正在探索 AI 技術在安全能力中的深度融合，推進「自動判斷 + 智能修復」能力落地：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;誤報識別輔助：結合大模型能力分析掃描結果上下文，自動判斷漏洞是否為誤報，減輕研發人員人工判斷負擔。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0627/192200_BOHD_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;個性化修復建議：基於代碼上下文與歷史缺陷庫，生成高質量修復建議，提升研發效率與合規響應速度。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0627/192208_A5pp_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2 style="margin-left:0; margin-right:0; text-align:left"&gt;Gitee DevSecOps 的現代化研發生態&lt;/h2&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;Gitee DevSecOps 是一站式國產化研發與交付平台，集成了代碼託管（Code）、項目協作（Team）、持續集成（CI）、持續部署（CD）、代碼安全（Scan）、數據洞察（Insight）等多項能力，致力於打造具備全生命週期管控能力的現代軟件工廠。&lt;/p&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0523/174619_MpFL_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;&lt;em&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgitee.cn%2Ffactory" target="_blank"&gt;https://gitee.cn/factory&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;平台設計充分考慮關鍵領域行業對安全性、可控性、合規性的極高要求，具備以下核心特徵：&lt;/p&gt; 
&lt;ul style="list-style-type:disc; margin-left:0; margin-right:0"&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;國產化適配與私有化部署能力：全面兼容國產操作系統與基礎設施，支持靈活部署於內網環境，保障數據主權；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;全流程 DevSecOps 管控體系：代碼從提交、審核、構建、掃描、部署到發佈全流程可視、可追溯、安全可控；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;模塊化產品結構：各能力模塊（如 Code、Team、Repo、Pipe、Scan、Insight 等）可靈活組合、漸進集成，適配多樣化團隊與流程要求；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;深度可觀測與度量體系：內置研發效能度量與數據洞察引擎，支撐管理者宏觀掌控項目態勢與交付健康度。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0516/162046_MD15_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;在多個國家級重大項目與關鍵領域單位落地實踐中，Gitee DevSecOps 已成為構建「自主、可控、高效、安全」的軟件工程體系的重要基石。&lt;/p&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-825957ffbed1798ea7b6a37079fd6c99d18.gif" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357644</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357644</guid>
      <pubDate>Sat, 10 May 2025 11:22:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>中國首個海洋領域開源大模發佈</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;中國首個海洋領域開源大模型 OceanGPT(滄淵) 於日前在浙江杭州發佈。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;該大模型由海洋精準感知技術全國重點實驗室 (浙江大學) 牽頭研發，具備基礎的海洋專業知識問答，以及聲吶圖像、海洋觀測圖等海洋特色多模態數據的自然語言解讀能力。其採用的領域知識增強「慢思考」推理機制，相較現有通用大模型能有效降低幻覺式錯誤。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="334" src="https://oscimg.oschina.net/oscnet/up-7aaa94e1643eaae4cb8270cd536a75a4b94.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;此外，模型還適用於海洋機器人操控與水下具身智能等關鍵場景。滄淵研發團隊負責人、浙江大學計算機學院教授陳華鈞介紹稱，「依託大模型的代碼自動生成能力，只需輸入一句自然語言指令，OceanGPT 即可生成對應的機器人操控代碼，實現下發、部署和任務執行。未來，我們希望即使是非專業人士，也能通過語音指令驅動水下機器人完成複雜任務。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;其表示，OceanGPT 還集成了 MCP 大模型協議，旨在實現大模型驅動的多機器人協同協作。後續 OceanGPT 還可直接部署于海洋機器人上的端側大模型，藉助端側大模型的推理能力進一步提升海洋裝備的自主作業能力和作業效率。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;據悉，目前 OceanGPT 已在浙江大學海鷹系列水下機器人平台上完成技術驗證，實測表明該模型將原本依賴人工編寫的機器人代碼編寫效率從「小時級」提升至「秒級」。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357639</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357639</guid>
      <pubDate>Sat, 10 May 2025 10:40:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Embabel Agent —— Spring 之父出品的 JVM 的 Agent 框架</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                        
                                                                                    &lt;p&gt;&lt;span style="background-color:#ffffff; color:#1f2328"&gt;Embabel (Em-BAY-bel) 是一個在 JVM 上編寫 agentic flows&lt;/span&gt;&amp;nbsp;&lt;span style="background-color:#ffffff; color:#1f2328"&gt;的框架，它將 LLM 觸發的交互與代碼和領域模型無縫融合。支持智能路徑查找，以達到目標。它使用 Kotlin 編寫，但提供了 Java 的自然使用模型。它出自 Spring 的創始人之手。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style="background-color:#ffffff; color:#1f2328"&gt;&lt;img alt="" height="304" src="https://static.oschina.net/uploads/space/2025/0627/155828_hK0o_4252687.jpg" width="200" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Actions：&lt;/strong&gt;Steps an agent takes&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Goals：&lt;/strong&gt;agent 試圖實現的目標&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Conditions：&lt;/strong&gt;執行操作或確定目標是否已達成之前需要評估的條件。每次執行操作後都會重新評估條件。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Domain model：&lt;/strong&gt;支撐流程並告知動作、目標和條件的對象。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Plan&lt;/strong&gt;&lt;strong&gt;：&lt;/strong&gt;為實現目標而採取的一系列行動。計劃由系統而非程序員動態制定。系統會在每次行動完成後重新制定計劃，使其能夠適應新的信息並觀察前一次行動的效果。這實際上是一個&lt;a href="https://en.wikipedia.org/wiki/OODA_loop"&gt;OODA 循環&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p style="text-align:start"&gt;&lt;span&gt;&lt;span&gt;&lt;span style="color:#1f2328"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style="background-color:#ffffff"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;這些概念鞏固了與其他 agent 框架的區別：&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;複雜的規劃。&lt;/strong&gt;通過引入真正的規劃步驟，使用非 LLM AI 算法，超越有限狀態機或嵌套順序執行。這使得系統能夠通過以新的順序組合已知步驟來執行未編程的任務，並做出有關並行化和其他運行時行為的決策。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;卓越的可擴展性和重用性&lt;/strong&gt;：由於動態規劃，添加更多的域對象、動作、目標和條件可以擴展系統的功能，&lt;em&gt;而無需編輯 FSM 定義&lt;/em&gt;或現有代碼。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;強類型和麪向對象的優勢&lt;/strong&gt;：操作、目標和條件由領域模型（可包含行為）定義。所有內容均為強類型，提示符和手動編寫的代碼可以清晰交互。告別 &lt;span style="background-color:#ffffff; color:#1f2328"&gt;magic maps&lt;/span&gt;。享受全面的重構支持。&lt;/li&gt;
&lt;/ul&gt;

&lt;p style="text-align:start"&gt;&lt;span&gt;&lt;span&gt;&lt;span style="color:#1f2328"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style="background-color:#ffffff"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;其他好處：&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;平台抽象&lt;/strong&gt;：編程模型和平台內部之間的明確分離允許在本地運行，同時可能在生產中提供更高的 QoS，而無需更改應用程序代碼。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;專為 LLM 混合設計&lt;/strong&gt;：輕鬆構建混合 LLM 的應用程序，確保提供最具成本效益且功能強大的解決方案。這使得系統能夠利用不同模型的優勢來執行不同的任務。特別是，它有助於使用本地模型執行點任務。這對於成本和隱私至關重要。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;基於 Spring 和 JVM 構建，&lt;/strong&gt;可輕鬆訪問現有企業功能和能力。例如：
&lt;ul&gt;
&lt;li&gt;Spring 可以注入和管理代理，包括使用 Spring AOP 來裝飾功能。&lt;/li&gt;
&lt;li&gt;提供強大的持久性和事務管理解決方案。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;專為可測試性&lt;/strong&gt;而設計。單元測試和代理端到端測試都非常簡單。&lt;/li&gt;
&lt;/ul&gt;

                                                                    &lt;/div&gt;
                                                                </description>
      <link>https://www.oschina.net/p/embabel-agent</link>
      <guid isPermaLink="false">https://www.oschina.net/p/embabel-agent</guid>
      <pubDate>Sat, 10 May 2025 09:24:00 GMT</pubDate>
    </item>
    <item>
      <title>Black Forest 開源 FLUX.1 Kontex 模型，使用文本即可實現一鍵 PS</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;知名開源平台 Black Forest 開源了文生圖模型 FLUX.1-Kontext 的開發者版本，該模型讓用户通過自然語言就能實現一鍵 P 圖。&lt;/p&gt; 
&lt;p&gt;Black Forest 公佈的測試數據顯示，FLUX.1-Kontext 在人類偏好評估、指令編輯、文本插入與編輯、樣式參考等評估基準中，超過了 OpenAI 發佈的最新文生圖模型 GPT-image-1，成為目前最強開源文生圖模型之一。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1070" src="https://static.oschina.net/uploads/space/2025/0627/164257_fGbo_2720166.png" width="1280" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;FLUX.1 Kontext 專注於&lt;strong&gt;圖像編輯任務&lt;/strong&gt;：包括迭代編輯、角色保持、局部與全局精細控制。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;可以非常準確地「重繪」圖片中的局部或全圖，比如：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;把帽子加到人物頭上&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;改變背景風景&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;把原圖中的狗換成貓，人物保持原樣&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;多次修改也不會「跑偏」或者失真&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;跟很多流行工具（如 ComfyUI）無縫結合，方便使用&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0627/164220_OOmD_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;模型下載：https://huggingface.co/black-forest-labs/FLUX.1-Kontext-dev&lt;/p&gt; 
&lt;p&gt;技術報告：https://arxiv.org/abs/2506.15742&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357607/flux-1-kontext-dev</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357607/flux-1-kontext-dev</guid>
      <pubDate>Sat, 10 May 2025 08:44:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>騰訊開源輕量級混元-A13B 模型</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;騰訊&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FmWfUrWz7bc7f9RhnOltQOA" target="_blank"&gt;宣佈&lt;/a&gt;開源混元大模型家族的新成員——混元-A13B 模型。該模型採用基於專家混合（MoE）架構，總參數規模達 800 億，激活參數為 130 億。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;公告稱，該模型在保持頂尖開源模型效果的同時，大幅降低了推理延遲與計算開銷。對個人開發者和中小企業來説，極端條件下僅需 1 張中低端 GPU 卡即可部署。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在性能表現上，混元-A13B 模型在數學、科學和邏輯推理任務中展現出領先效果。例如，在數學推理測試中，模型能夠準確完成小數比較並展現分步解析能力。對於時下熱門的智能體（Agent）應用，模型可調用工具，高效生成出行攻略、數據文件分析等複雜指令響應。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="330" src="https://oscimg.oschina.net/oscnet/up-6dda73013829ccd4efe52c7929dd54e70c9.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;根據介紹，預訓練中，混元-A13B 模型使用了 20 萬億高質量網絡詞元語料庫，提升了模型推理能力的上限；完善了 MoE 架構的 Scaling Law&amp;nbsp;（即規模定律）理論體系，為 MoE 架構設計提供了可量化的工程化指導，提升了模型預訓練效果。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;用户可以按需選擇思考模式，&lt;strong&gt;快思考模式&lt;/strong&gt;提供簡潔、高效的輸出，適合追求速度和最小計算開銷的簡單任務；&lt;strong&gt;慢思考模式&lt;/strong&gt;涉及更深、更全面的推理步驟。這優化了計算資源分配，兼顧效率和準確性。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;此外，混元還開源了兩個新數據集。其中，ArtifactsBench 主要用於代碼評估，構建了一個包含 1825 個任務的新基準；C3-Bench 針對 Agent 場景模型評估，設計了 1024 條測試數據，以發現模型能力的不足。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357604</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357604</guid>
      <pubDate>Sat, 10 May 2025 08:18:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>開源 | MeiGen-MultiTalk：基於單張照片實現多人互動演繹</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;blockquote&gt; 
 &lt;p&gt;近日，美團推出了音頻驅動的多人對話視頻生成框架 MultiTalk，並在 GitHub 上開源，首創 L-RoPE 綁定技術，通過標籤旋轉位置編碼精準解決多音頻流與人物錯位難題。該框架創新性地採用局部參數訓練+多任務學習策略，在保留複雜動作指令跟隨能力的同時，實現自適應動態人物定位。只需輸入多人音頻流、參考圖像和文本提示，即可生成口型精準同步、肢體自然的交互視頻，可支持影視製作、直播電商等場景的工具升級。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-1f49ae4f4fb275deb2b5f21c7d98b147cc3.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;如果給你一張圖片，再給你一段語音，怎麼能讓它們完美融合在一起，讓圖片中人物自然説話和做動作，甚至多人之間還能互動起來呢？近日，美團視覺智能團隊在 GitHub 上開源了一款產品&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmeigen-ai.github.io%2Fmulti-talk%2F" target="_blank"&gt;MeiGen-MultiTalk&lt;/a&gt;，它就非常巧妙地解決了這個問題。先上視頻，看一下它實力如何：&lt;/p&gt; 
&lt;p&gt;1.輸入圖像+對話語音&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//7ad2d3b67c25b4c879c9baeab11830b8.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;2.使用 MultiTalk 生成視頻&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fuser-attachments%2Fassets%2F94ce6060-4591-4179-833b-b0e7da3bd31c" target="_blank"&gt;點擊查看視頻&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;注：圖像和音頻均由 AI 生成。&lt;/p&gt; 
&lt;p&gt;還有下面這部《Smile》短片中所有鏡頭，也都是由 MeiGen-MultiTalk 合成的，是不是很驚豔？&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fuser-attachments%2Fassets%2Fb9a7ef25-6068-4296-ab41-1aa94e9b9545" target="_blank"&gt;點擊查看視頻&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;注：每個鏡頭首幀圖像和音頻來源《Smile》- Morgan Wallen&lt;/p&gt; 
&lt;p&gt;不僅僅是這種風格，還有很多其他很多類型的融合，讓小貓説話，給動畫片配音，甚至還讓雙人對唱飈高音，它也表現的相當不錯。感興趣的同學，可移步到&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmeigen-ai.github.io%2Fmulti-talk%2F" target="_blank"&gt;項目主頁&lt;/a&gt;進行查看。或者查看美團技術團隊微信公眾號的推文：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FEJHbR0ShT53BhMTUceY9pg" target="_blank"&gt;開源 | MeiGen-MultiTalk：基於單張照片實現多人互動演繹&lt;/a&gt;。展示完畢，接下來就是最重要的部分，上鍊接！&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;項目主頁&lt;/strong&gt; ：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmeigen-ai.github.io%2Fmulti-talk%2F" target="_blank"&gt;https://meigen-ai.github.io/multi-talk/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;開源代碼&lt;/strong&gt; ：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FMeiGen-AI%2FMultiTalk" target="_blank"&gt;https://github.com/MeiGen-AI/MultiTalk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;技術報告&lt;/strong&gt; ：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2505.22647" target="_blank"&gt;https://arxiv.org/abs/2505.22647&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;一、引言：超越"會説話的頭"------AI 人像視頻的下一個前沿&lt;/h2&gt; 
&lt;p&gt;當前，人工智能在視覺內容生成領域取得了令人矚目的進展，尤其是在音頻驅動的人像視頻方面。無論是"會説話的頭"還是"會説話的身體"技術，都已能夠從音頻信號生成與面部動作高度同步、視覺質量令人滿意的視頻。這些技術在模擬單人講話方面表現出色，例如在虛擬主播或數字替身等應用中展現出逼真的效果。&lt;/p&gt; 
&lt;p&gt;然而，現有方法在處理更復雜的場景時，其侷限性也日益凸顯，面對多人對話視頻生成時面臨三大挑戰：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;多音頻流輸入適配&lt;/strong&gt;：如何區分並綁定不同人物的音頻信號？&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;動態人物定位&lt;/strong&gt;：當人物在畫面中移動時，如何精準定位其運動區域？&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;指令遵循能力&lt;/strong&gt;：如何讓生成的視頻嚴格遵循文本描述的複雜動作（如大幅肢體動作）？&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;這些挑戰促使研究人員思考，AI 人像視頻的下一個前沿究竟在哪裏。從最初僅關注面部表情的"會説話的頭"，到能夠模擬全身動作的"會説話的身體"，再到如今 MultiTalk 所提出的"多人物對話視頻生成"，這清晰地揭示了 AI 人像視頻領域從關注局部細節到全身動作，再到模擬複雜社會互動的演進趨勢。這種演進不僅僅是技術能力的簡單提升，更體現了對真實世界複雜性模擬需求的增長，以及 AI 在內容創作中扮演更高級角色的潛力。用户對 AI 生成內容的"真實感"和"複雜性"要求越來越高，簡單的"動起來"已不足夠，現在需要 AI 能夠"自然地互動"並"理解和執行復雜指令"。&lt;/p&gt; 
&lt;h2&gt;二、MultiTalk 的框架圖：如何實現 AI 對話視頻生成&lt;/h2&gt; 
&lt;p&gt;MultiTalk 實現音頻驅動的多人物對話視頻生成的技術框架，如下圖 2 所示：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//1692b1d06eb9109b7420450994f1fcf1.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;2.1 基礎模型結構：DiT 與 3D VAE&lt;/h3&gt; 
&lt;p&gt;MultiTalk 以 DiT（Diffusion-in-Transformer）為基礎的視頻擴散模型作為其核心骨架。DiT 模型因其在圖像和視頻生成方面的卓越性能而備受關注，它用 Transformer 結構替代了傳統的 U-Net，能夠更好地捕捉長距離依賴關係。&lt;/p&gt; 
&lt;p&gt;為了高效處理視頻數據，MultiTalk 集成了 3D 變分自編碼器（VAE）。3D VAE 能夠對視頻數據在空間和時間維度上進行壓縮，將高維原始視頻數據編碼成更緊湊的潛在表示。這種壓縮大大降低了後續擴散模型的計算負擔，同時保留了關鍵的視覺信息。&lt;/p&gt; 
&lt;p&gt;首先，使用文本編碼器，將用户輸入的文本提示（例如"一個男人和女人正在舞台上唱歌"）轉化為文本條件嵌入，指導視頻內容的生成。其次，通過 CLIP 圖像編碼器提取的全局上下文信息也被注入到 DiT 模型中。這些圖像上下文與文本條件通過解耦的交叉注意力機制協同作用，為生成視頻提供視覺和語義指導，確保生成內容與參考圖像和文本提示保持一致。&lt;/p&gt; 
&lt;h3&gt;2.2 讓 AI"説話"：單人音頻集成&lt;/h3&gt; 
&lt;p&gt;基礎的圖像到視頻（I2V）擴散模型通常不原生支持音頻輸入。為了讓模型能夠"説話"，MultiTalk 在每個 DiT 塊的文本交叉注意力層之後，添加了新的層，這些層包含層歸一化和音頻交叉注意力機制，專門用於處理和整合音頻條件。&lt;/p&gt; 
&lt;p&gt;在音頻嵌入的提取與上下文整合方面，MultiTalk 採用了 Wav2Vec，這是一種廣泛使用的音頻特徵提取器，能夠將音頻波形轉換為高維的音頻嵌入。在音頻驅動的人體視頻中，當前時刻的動作不僅受當前音頻幀影響，也受前後音頻幀的影響。因此，MultiTalk 遵循現有方法，將與當前幀相鄰的音頻嵌入進行拼接（通過上下文長度 k 參數控制），形成更具時間上下文信息的音頻嵌入，以更好地捕捉語音的動態變化。&lt;/p&gt; 
&lt;p&gt;一個重要的挑戰是，由於 3D VAE 對視頻數據進行了時間壓縮，視頻潛在空間的幀長度通常比原始音頻嵌入的幀長度短，這使得兩者之間無法直接進行幀對幀的交叉注意力計算。為瞭解決這種時序長度不匹配的問題，MultiTalk 使用了一個音頻適配器。該適配器通過一系列操作對音頻嵌入進行壓縮和對齊：首先將輸入音頻嵌入分割為初始幀和後續幀；然後對後續幀進行下采樣；接着分別通過多個 MLP 層編碼初始幀和下采樣後的後續幀；將編碼後的特徵拼接起來；最後，再次通過 MLP 層對拼接後的特徵進行編碼，從而獲得與視頻潛在空間幀長度匹配的壓縮音頻條件。音頻適配器解決了視頻和音頻數據固有的時間粒度不匹配問題，確保了信息流的順暢，使得不同模態的數據能夠高效地在同一框架內進行交互。&lt;/p&gt; 
&lt;h3&gt;2.3 核心挑戰：當多重聲音讓 AI"困惑"&lt;/h3&gt; 
&lt;p&gt;與單人視頻相比，多人物對話視頻生成帶來了多重複雜性，這些是現有方法無法解決的。首先，對話場景中，音頻信號來自多個人物，模型需要能夠同時、獨立地處理這些不同的音頻流，這是"多流音頻輸入處理"的挑戰。其次，也是最核心的挑戰之一，是"音頻與人物的精確綁定"。必須確保視頻中的每個人物只由其對應的音頻流驅動，以防止唇形同步錯誤地出現在所有人物身上，導致不自然的"齊聲説話"現象，這在真實對話中是極不自然的。最後，生成視頻中的人物是動態的，他們的位置和姿態會隨着對話和動作而變化。這要求模型具備一種"自適應方法"，能夠精確追蹤每個人物在視頻幀中的運動區域，以便將音頻準確地映射到正確的視覺區域。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//8563dcaa19a0ce3dd0f9b554af583220.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在探索多流音頻注入方案時，MultiTalk 嘗試了多種直覺性的方法，如上圖 3 所示。但多數都未能有效解決音頻與人物的綁定問題，這凸顯了問題本身的複雜性，並非簡單的拼接或分割就能解決。最初的嘗試包括：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;直接拼接多流音頻嵌入&lt;/strong&gt;： 將多流音頻的嵌入直接拼接起來，然後與視頻潛在空間進行交叉注意力計算。然而，這種方法未能將拼接後的多流音頻與視頻中對應的特定人物區域綁定，導致混亂的同步。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;分別計算後相加&lt;/strong&gt;： 分別計算每個音頻流與視頻潛在空間的交叉注意力結果，然後將這些結果相加。然而，這種方法同樣未能解決綁定問題，模型無法區分哪個音頻應該驅動哪個人物。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;分割視頻潛在空間（左右區域）&lt;/strong&gt;： 考慮到視頻中人物通常位於左右兩側，MultiTalk 嘗試將視頻潛在空間簡單地分割成左右兩部分，並讓每個部分與對應的音頻流計算注意力。雖然這種方法在一定程度上成功綁定了多流音頻到不同人物，但其泛化能力極其有限。它僅適用於人物動作範圍很小的視頻；一旦人物出現大範圍移動或交叉，這種簡單的空間分割就會導致音頻綁定失敗。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;這些傳統方法失敗的根本原因在於它們缺乏自適應的對動態主體進行定位的能力。直接拼接、簡單相加或基於固定空間位置的分割，無法讓模型理解哪個音頻流應該對應視頻中哪個動態變化的人物。缺乏這種深層的"人物感知"和"語義綁定"機制，導致了"錯誤綁定"------所有人都同步説話，這在對話場景中是極不自然的，嚴重影響了生成視頻的真實感和可用性。&lt;/p&gt; 
&lt;h3&gt;2.4 讓 AI"交談"：L-ROPE 實現無縫多人物綁定&lt;/h3&gt; 
&lt;p&gt;為瞭解決這個問題，MultiTalk 提出了 L-ROPE。在應用 L-ROPE 進行音頻綁定之前，MultiTalk 首先需要解決一個基礎問題：如何在視頻中動態地識別並追蹤每個人物的位置。給定包含多個人物的參考圖像，模型首先識別出每個人物的掩碼區域以及背景掩碼。在 DiT 模型中，視頻的第一幀通常作為參考圖像。MultiTalk 利用"參考圖像到視頻的自注意力圖"。如圖 4a），通過計算視頻潛在空間中每個 Token 與參考圖像中每個人物掩碼的平均相似度，模型能夠得到一個相似度矩陣。利用這個相似度矩陣，模型可以自適應地確定視頻中每個潛在 Token 屬於哪個人物或背景，從而實現了對每個人物的動態定位和追蹤。&lt;/p&gt; 
&lt;p&gt;Label Rotary Position Embedding （L-ROPE）是 MultiTalk 的核心創新，它基於 ROPE（Rotary Position Embedding）的思想。ROPE 是一種在大型語言模型（LLMs）和視頻擴散模型中廣泛使用的相對位置編碼技術，以其在捕捉 Token 間關係和處理時空信息方面的卓越能力而聞名。L-ROPE 的創新之處在於，它將"類別標籤"融入到位置編碼中，從而在 DiT 塊的音頻交叉注意力層中，實現了多流音頻與多個人物的精準綁定。&lt;/p&gt; 
&lt;p&gt;在標籤分配策略上，視頻潛在空間包多個類別，比如多個人物和背景的區域。MultiTalk 為每個人物分配了一個特定的數值範圍作為標籤（例如，第一個人物的視覺標籤範圍是{0-4}，第二個人物是{20-24}）。視頻潛在空間中每個 Token 的最終標籤，是根據其與對應人物掩碼的相似度，通過歸一化函數在這個範圍內計算得出的。背景區域則被賦予一個靜態標籤，以確保它不與任何音頻流關聯，避免背景元素被音頻驅動。對於多流音頻嵌入，MultiTalk 首先將它們拼接起來，然後為每個音頻流分配一個靜態的、唯一的標籤。為了與視頻中的人物綁定，這些音頻標籤被精心選擇，與對應人物的視覺標籤範圍"接近"或"匹配"（例如，第一個音頻流標籤為 2，第二個音頻流標籤為 22）。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//368e33e0bdb9af15aa2b5d480b2b1de5.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;L-ROPE 的特點在於它將"類別信息"（哪個像素屬於哪個人物類或背景類）巧妙地融入了"位置編碼"中。傳統的 ROPE 處理的是純粹的時空位置信息，而 L-ROPE 則更進一步，將"類別"信息編碼進去。它使得模型能夠區分場景中的不同個體。在音頻交叉注意力機制中，Q（來自視頻潛在空間）和 K（來自多流音頻嵌入）都經過 L-ROPE 處理。通過這種帶有語義標籤的旋轉，當視頻潛在空間中某個區域（例如，對應人物 1 的區域）的標籤與音頻 1 的標籤"匹配"時，它們之間的注意力權重就會被有效激活，從而強制模型將音頻 1 的驅動作用集中到人物 1 身上，解決了不正確的綁定問題，如圖 4c)。這種策略能夠有效激活音頻交叉注意力圖中的特定區域，從而確保音頻與對應人物的唇形和動作精確同步。&lt;/p&gt; 
&lt;p&gt;為了驗證 L-ROPE 的有效性，論文進行了一項消融研究，重點關注標籤範圍的選擇。實驗結果（如下表 3 所示）表明，即使為不同人物選擇不同的標籤範圍，所產生的性能指標接近。這説明 L-ROPE 對具體的標籤範圍變化不敏感。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//3180443fcdf3b17a4c8b2edea7406776.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;2.5 訓練策略&lt;/h3&gt; 
&lt;p&gt;MultiTalk 框架採用了多項訓練策略，這些策略共同確保了模型在多人物場景下的高性能、精確的音頻同步以及指令遵循能力。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;1. 兩階段訓練：循序漸進的技能提升&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;MultiTalk 的訓練過程被劃分為兩個階段，旨在逐步增強模型的音頻處理和唇形同步能力。第一階段的主要目標是開發模型對單人視頻的強大能力，此階段模型使用單人説話視頻數據集進行訓練。在模型掌握了單人視頻能力之後，進入第二階段。第二階段使用專門收集的包含雙流音頻的訓練數據，以促進模型學習多人物視頻和交互。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2. 部分參數訓練：精準調優，避免退化&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;這是 MultiTalk 訓練中的一個關鍵策略。在整個訓練過程中，研究者僅更新音頻交叉注意力層和音頻適配器中的網絡參數，而凍結了所有其他基礎模型的網絡參數。論文發現表明，在計算資源和數據量有限的情況下，如果進行全參數訓練，會導致模型指令遵循能力的顯著下降（特別是對於複雜的動作和人物交互），甚至可能引起生成視頻中手部和物體變形等視覺偽影。相反，通過僅訓練與音頻輸入直接相關的特定層，MultiTalk 能夠很好地保留基礎模型原有的強大指令遵循能力，並避免了上述視覺退化問題。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;3. 多任務訓練：豐富場景理解，強化指令遵循&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;MultiTalk 採用了多任務混合訓練範式，將模型訓練任務分為音頻+圖像到視頻（AI2V）訓練和圖像到視頻（I2V）訓練。儘管任務不同，但它們共享相同的網絡參數。在 AI2V 任務中，模型同時使用參考圖像和音頻作為條件輸入，專注於學習音頻驅動的唇形同步和動作生成。在 I2V 任務中，音頻條件被移除（通過將音頻嵌入置零）。I2V 任務使用的訓練數據是獨特的，主要包含大量多事件視頻。這些視頻涵蓋了人物、物體和場景之間複雜的交互，例如人物拿起杯子、與環境互動等。這種多事件數據集對於確保模型能夠準確理解和執行文本提示中描述的複雜動作和交互至關重要。論文指出，如果僅使用説話的頭和身體數據進行 AI2V 訓練，網絡的指令遵循能力會顯著削弱。然而，通過將 I2V 訓練納入多任務範式，模型能夠有效地保留其強大的指令遵循能力，從而生成更符合用户意圖的視頻，如下圖 5 所示。這種策略體現了泛化與魯棒性，即通過多任務訓練，在保持特定任務能力的同時，增強模型的通用理解和指令遵循能力。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//77370c360f865826c0a5aeb1f35060db.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;2.6 長視頻生成&lt;/h3&gt; 
&lt;p&gt;儘管 MultiTalk 模型能夠生成高質量的短視頻（例如 3-5 秒），但這對於實際應用場景（如製作電影片段、直播內容）來説遠遠不夠，因為這些場景通常需要持續更長的視頻。為了突破單次生成長度的限制，MultiTalk 引入了一種基於自迴歸（Autoregressive）的方法來生成長視頻。將之前生成視頻的末尾部分作為條件，來生成新的視頻片段，從而實現時間上的連續性和擴展。&lt;/p&gt; 
&lt;p&gt;在具體的實現機制上，傳統的圖像到視頻（I2V）模型通常只使用視頻的第一幀作為生成後續幀的條件。MultiTalk 在此基礎上進行了關鍵改進。在生成新的視頻片段時，它不再僅僅依賴第一幀，而是將先前已生成視頻的最後 5 幀作為額外的條件輸入到當前的推理步驟中。這使得模型能夠"記住"並延續之前的動作和場景狀態。這些作為條件的 5 幀視頻，首先會通過 3D VAE 進行壓縮，將其轉化為更緊湊的 2 幀潛在噪聲表示。隨後，為了匹配 DiT 模型的輸入格式，新的視頻幀（除了從歷史信息得來的 2 幀潛在噪聲）會用零填充。這些填充的幀、來自歷史信息的潛在噪聲以及一個視頻掩碼被拼接在一起，形成完整的輸入。最終，這個包含歷史上下文信息的輸入被送入 DiT 模型進行推理，生成新的視頻片段。下面視頻展示了生成結果的流暢性。&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;輸入圖像+對話語音&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//d68a049c134922b777999b4d82e04f34.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;使用 MultiTalk 生成視頻&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fuser-attachments%2Fassets%2Fec77e7e3-f03b-41ac-810b-49fd8d70968a" target="_blank"&gt;點擊查看視頻&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;注：圖像和音頻源於《破產姐妹》。&lt;/p&gt; 
&lt;h2&gt;三、MultiTalk 實戰：性能表現&lt;/h2&gt; 
&lt;p&gt;MultiTalk 的性能通過廣泛的實驗進行了驗證，包括與現有最先進方法的定量和定性比較，充分展示了其在多人物對話視頻生成方面的能力。&lt;/p&gt; 
&lt;p&gt;在數據集與評估指標方面，MultiTalk 的訓練數據集在第一階段使用了約 2K 小時的單人説話視頻，用於學習基礎的音頻驅動視頻能力；第二階段則使用了 100 小時的雙人對話視頻，用於專門訓練多人物交互和綁定。MultiTalk 在三類不同的測試數據集上進行了評估：説話的頭數據集（HDTF 和 CelebV-HQ ）、説話的身體數據集（EMTDT ）以及雙人説話身體數據集（MTHM）。評估採用了行業內通用的多維度指標：FID (Frechet Inception Distance) 和 FVD (Fréchet Video Distance) 用於評估生成數據質量；E-FID (Expression-FID) 用於評估生成視頻中面部表情的表現力；Sync-C 和 Sync-D 用於精確測量生成視頻中唇部動作與音頻的同步程度。&lt;/p&gt; 
&lt;p&gt;在定量評估中，MultiTalk 在説話的頭和説話的身體生成任務上，與 AniPortrait、VExpress、EchoMimic、Hallo3、Sonic、Fantasy Talking 等多個最先進的方法進行了對比。結果顯示，MultiTalk 在大多數指標上超越了這些方法，尤其在唇形同步（Sync-C, Sync-D）和視頻質量（FID, FVD）方面表現出卓越性能。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//3e0ee05f747d8947c63d6637507dc3ee.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;此外，我們還專門探討了多流音頻訓練是否會導致單人視頻性能下降的問題（具體可以參考&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2505.22647" target="_blank"&gt;論文&lt;/a&gt;）。實驗結果（表 1 和表 2 中"MultiTalk-single"與"MultiTalk-multiple"的對比）顯示，MultiTalk 的多人視頻模型在單人數據集上表現與單人視頻模型相當。這表明，MultiTalk 在引入多人物處理能力時，並未犧牲原有的單人視頻性能，實現了能力的無損疊加。&lt;/p&gt; 
&lt;p&gt;在定性評估中，MultiTalk 取得了不錯的效果，如下圖 6 所示。其顯著優勢之一是強大的指令遵循能力。當提供複雜的文本提示（例如"一個男人合上筆記本電腦並放在桌上"、"一個女人戴着耳機坐在桌旁，然後她拿起耳機"）時，MultiTalk 能夠成功生成精確響應這些指令的視頻，而其他同類方法則難以做到，往往出現動作不符或物體變形。MultiTalk 生成的視頻中，視覺偽影（如手部或物體扭曲）顯著減少，整體視覺質量更高，畫面更自然真實。作為首個專門針對多人物生成任務設計的方法，MultiTalk 在處理複雜的交互場景時表現出色。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//f3b1a346abe9f61b3fb9bcce7710c261.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;與簡單的"視頻拼接"方法（即將左右人物視頻分別生成再拼接）相比（如下圖 7 所示），MultiTalk 能夠有效處理人物間的互動，避免了拼接方法中常見的左右片段不一致性問題，使得多人物對話和互動更加流暢自然。論文還通過可視化自注意力圖，直觀地展示了 MultiTalk 能夠自適應地識別視頻中特定人物的定位，這進一步證明瞭 L-ROPE 方法在實現精確音頻綁定方面的有效性。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//e2edf1964be1420e0dcd5b658a2c58cb.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h2&gt;四、總結與展望&lt;/h2&gt; 
&lt;p&gt;MultiTalk 提出一種音頻驅動多人物對話視頻生成方案，其核心突破在於其創新的 L-ROPE 方法，它通過結合自適應人物定位和帶有類別信息的標籤編碼，有效解決了多流音頻的注入和人物綁定這一難題。此外，其精心設計的部分參數訓練和多任務訓練策略，確保了模型在有限資源下依然能夠保持強大的指令遵循能力和高質量的視覺輸出。&lt;/p&gt; 
&lt;p&gt;MultiTalk 的誕生，預示着其在多角色電影製作、虛擬直播、遊戲開發、教育內容創作等領域具有廣闊的應用前景。我們深信，未來它將極大地降低多角色視頻的製作門檻，使個性化、交互式內容創作變得更加高效和便捷。儘管仍存在真實音頻與合成音頻的性能差距等侷限，但 MultiTalk 為未來的研究指明瞭方向。我們期待 MultiTalk 及其後續研究能夠進一步推動 AI 在模擬和創造複雜人機交互方面的能力，使數字世界中的人物更加栩栩如生。&lt;/p&gt; 
&lt;p&gt;現在，MultiTalk 已經在 GitHub 上&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FMeiGen-AI%2FMultiTalk" target="_blank"&gt;開源&lt;/a&gt;，歡迎更多的同學加入我們，一起共建。&lt;/p&gt; 
&lt;h2&gt;五、關於美團視覺智能部&lt;/h2&gt; 
&lt;p&gt;美團視覺智能部圍繞豐富的本地生活電商場景，建設從基礎通用到細分領域的視覺技術能力，包括：視覺生成大模型、多模交互虛擬人，助力營銷創意生產和商家低成本直播；文檔、商品、安全多模態大模型，助力商家開店經營、平台商品治理和違規賬號治理；人臉識別、文字識別、細粒度圖像分析、高性能檢測分割、街景理解，成為公司基礎設施能力。曾開源行業最大規模&lt;a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2F123.57.42.89%2FFoodProject.html" target="_blank"&gt;食品圖像數據集 Food2K&lt;/a&gt;被全球各地區上百家機構使用，&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fmeituan%2FYOLOv6" target="_blank"&gt;目標檢測框架 YOLOV6&lt;/a&gt;榮登 2023 年度世界開源貢獻榜，獲得 10+項國際競賽冠軍，上百項發明專利，60+篇頂會頂刊論文。曾與國內多家知名科研機構合作，多次獲得省部級科技進步獎項。&lt;/p&gt; 
&lt;p&gt;| 關注「美團技術團隊」微信公眾號，在公眾號菜單欄對話框回覆【2024 年貨】、【2023 年貨】、【2022 年貨】、【2021 年貨】、【2020 年貨】、【2019 年貨】、【2018 年貨】、【2017 年貨】等關鍵詞，可查看美團技術團隊歷年技術文章合集。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//2b7e833c2041e319a96878b7c8c571f0.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;| 本文系美團技術團隊出品，著作權歸屬美團。歡迎出於分享和交流等非商業目的轉載或使用本文內容，敬請註明 "內容轉載自美團技術團隊"。本文未經許可，不得進行商業性轉載或者使用。任何商用行為，請發送郵件至 &lt;a href="https://www.oschina.net/action/GoToLink?url=mailto%3Atech%40meituan.com" target="_blank"&gt;tech@meituan.com&lt;/a&gt; 申請授權。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/meituantech/blog/18638557</link>
      <guid isPermaLink="false">https://my.oschina.net/meituantech/blog/18638557</guid>
      <pubDate>Sat, 10 May 2025 07:30:00 GMT</pubDate>
      <author>原創</author>
    </item>
    <item>
      <title>沒人喜歡寫 README？Gitee：現在你不用寫了</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;README 是開源項目的第一張臉，它決定着項目的專業度，也直接影響用户能否快速上手。但你我都知道，寫代碼有意思，寫文檔太難了。尤其在項目初期，README 不是空白，就是隨便應付兩句草草了事。&lt;/p&gt; 
&lt;p&gt;現在，Gitee 幫你解決這個老大難問題——&lt;strong&gt;全新上線的「AI README 生成與優化」，能一鍵生成結構清晰、內容專業的項目文檔&lt;/strong&gt;，還能自動優化現有 README，讓你的項目門面煥然一新！&lt;/p&gt; 
&lt;h2&gt;核心亮點&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;自動識別項目類型&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;深度分析依賴、配置文件與代碼結構，精準提取關鍵信息。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;社區最佳實踐模板&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;內置項目簡介、快速開始、使用指南、貢獻規範、許可證等完整模塊。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;一鍵生成 &amp;amp; 智能優化&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;適配新舊倉庫：沒有 README 時快速生成，有 README 時自動補全、重排結構、提升可讀性。&lt;/p&gt; 
&lt;h2&gt;上手指南&lt;/h2&gt; 
&lt;p&gt;該功能已向所有 Gitee 社區版&amp;amp;企業版開源倉庫開放，新老項目均可直接體驗。&lt;/p&gt; 
&lt;h3&gt;進入倉庫主頁&lt;/h3&gt; 
&lt;p&gt;Gitee 會自動檢測是否存在 README。若缺失，將顯示「AI 生成 README」按鈕。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0627/150448_MUT2_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;啓動 AI 生成&lt;/h3&gt; 
&lt;p&gt;點擊「AI 生成 README」按鈕後，馬建倉 AI 助手將：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;🔍 分析倉庫內容&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;📊 摘取關鍵信息&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;📝 生成專業 README&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;📤 將變更以輕量級 PR 提交&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0627/150509_02hr_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;審核併合並 PR&lt;/h3&gt; 
&lt;p&gt;檢查自動生成的內容，確認無誤後合併即可。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0627/150522_Aq2Q_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0627/150531_yTAj_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;完成展示&lt;/h3&gt; 
&lt;p&gt;合併後，新 README 會立即呈現在倉庫首頁。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0627/150541_gohB_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;如何優化現有 README？&lt;/h3&gt; 
&lt;p&gt;對於已有 README 文件的項目，也可使用 AI 優化功能：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;補全缺失板塊（貢獻指南、許可證等）&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;重排段落結構、統一格式&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;強化示例代碼與項目特色描述&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0627/150552_701l_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h2&gt;智能開發，就在 Gitee&lt;/h2&gt; 
&lt;p&gt;AI README 生成與優化功能，是 Gitee 在智能開發領域邁出的又一步。它回應了開發者在文檔撰寫上的真實痛點，&lt;strong&gt;將重複性工作交由 AI 自動完成&lt;/strong&gt;，讓開發者能夠專注於更有價值的創新；同時通過標準化模板與智能潤色機制，&lt;strong&gt;幫助項目迅速建立專業形象，提升傳播效果&lt;/strong&gt;；清晰、完善的文檔結構，也&lt;strong&gt;為協作與社區建設提供了堅實基礎&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;未來，Gitee 將持續拓展智能工具鏈能力，打造更開放、更高效的開發環境，讓 AI 真正融入開發全流程，服務每一位開發者與每一個項目。&lt;/p&gt; 
&lt;p&gt;歡迎體驗：&lt;a href="https://gitee.com/"&gt;https://gitee.com/&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357583</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357583</guid>
      <pubDate>Sat, 10 May 2025 07:06:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>谷歌發佈 Gemma 3n，專為移動設備打造的全新 AI 模型</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Google&lt;u&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdevelopers.googleblog.com%2Fen%2Fintroducing-gemma-3n-developer-guide%2F" target="_blank"&gt;宣佈&lt;/a&gt;&lt;/u&gt;推出 Gemma 3n，這是其下一代的開放 AI 模型，與我們之前看到的相比有了顯著的提升。繼上個月在 Google I/O 大會上進行預覽後，完整版現已發佈，可直接在移動硬件上運行。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-3a9a3ee99131ac4d39d56d44480a911816f.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;與 Gemini 的不同之處在於，Gemma 是為開發者下載和修改而設計的，而 Gemini 是 Google 的封閉式專有模型。&lt;/p&gt; 
&lt;p&gt;該模型現在可以原生處理圖像、音頻和視頻等輸入並生成文本，這比僅僅基於文本的模型有了很大的飛躍。它甚至可以在內存僅為 2GB 的硬件上運行，並且據稱在編碼和推理等任務上表現更佳。以下是 Google 列出的所有改進：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;多模式設計： Gemma 3n 原生支持圖像、音頻、視頻和文本輸入和文本輸出。&lt;/li&gt; 
 &lt;li&gt;專為設備端優化： Gemma 3n 型號以效率為設計重點，提供兩種基於有效參數的尺寸：E2B 和 E4B。雖然它們的原始參數數量分別為 5B 和 8B，但架構創新使其運行內存佔用與傳統的 2B 和 4B 型號相當，僅需 2GB (E2B) 和 3GB (E4B) 內存即可運行。&lt;/li&gt; 
 &lt;li&gt;突破性的架構： Gemma 3n 的核心是新穎的組件，例如用於計算靈活性的 MatFormer 架構、用於提高內存效率的每層嵌入 (PLE) 以及針對設備用例優化的新型音頻和基於 MobileNet-v5 的視覺編碼器。&lt;/li&gt; 
 &lt;li&gt;增強質量： Gemma 3n 在多語言（支持 140 種文本語言和 35 種語言的多模式理解）、數學、編碼和推理方面實現了質量改進。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Gemma 3n 高效的核心是 Google 稱之為 MatFormer 的新架構。Google 用俄羅斯套娃的比喻來描述它&lt;strong&gt;：一個較大的模型裏麪包含一個較小的、功能齊全的版本&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;這使得單個模型能夠以不同的規模運行不同的任務。至於基準測試，更大的 E4B 模型是第一個在 10B 參數下突破 LMArena 1300 分的模型。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-429797bc92f4d938d5e5f8d3ab6c9e9af2b.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;該模型的音頻功能現在支持設備上的語音轉文本和翻譯，並使用能夠精細處理語音的編碼器。視覺方面則由名為 MobileNet-V5 的全新編碼器提供支持，該編碼器比其前代產品速度更快、效率更高。它能夠在 Google Pixel 設備上以高達 60FPS 的速度處理視頻。&lt;/p&gt; 
&lt;p&gt;如果您有興趣，可以立即開始使用，因為這些模型可以通過 Hugging Face 和 Kaggle 等熟悉的平台獲得，您甚至可以直接在 Google AI Studio 中對它們進行試驗：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Faistudio.google.com%2Fprompts%2Fnew_chat%3Fmodel%3Dgemma-3n-e4b-it" target="_blank"&gt;https://aistudio.google.com/prompts/new_chat?model=gemma-3n-e4b-it&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;u&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdevelopers.googleblog.com%2Fen%2Fintroducing-gemma-3n-developer-guide%2F" target="_blank"&gt;更多詳情請參閲官方公告帖&lt;/a&gt;&lt;/u&gt;。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357582/google-gemma-3n</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357582/google-gemma-3n</guid>
      <pubDate>Sat, 10 May 2025 06:54:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>我國自主研發首套航空運輸大模型發佈</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;我國自主研發的首套航空運輸大模型天牧」低空大模型日前在南京發佈，該大模型可以作為空中交通指揮專家，同時具備智能問答、輔助決策等核心能力，其研發在低空智能管理領域創下多項技術首發成果，實現了多項關鍵技術的突破。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;「天牧」低空大模型屬於低空飛行智慧大腦「天行」中樞的系列產品，該系列產品可以作為空中交通指揮專家，解決低空飛行中的空情監控、資源調度等問題。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;中國電科首席專家丁一波介紹稱，「我們最主要的功能可以歸納為管、協、服三大能力。管理就是對航空器的登記註冊和飛行管理工作。我們的協作主要是在相應的飛行活動過程中着力解決有效協同問題。我們的服務主要是提供情報的服務、氣象的服務和我們各種飛行中的數據服務。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="367" src="https://oscimg.oschina.net/oscnet/up-be2d6109d171ae5f30cc313a4110597193e.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在「天牧」低空大模型的加持下，「天行」系統通過對超千萬條低空運行規則的學習訓練，在高算力集羣支撐下實現了對複雜場景自主查詢效率提升 50% 的技術跨越，首次實現了「AI 驅動」的低空管服模式。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;除了「天行」及「天牧」低空大模型，此次大會上還發布了聚焦低空安全的「天衞」系統以及聚焦飛行器產品研發的「天工」等系列產品。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357561</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357561</guid>
      <pubDate>Sat, 10 May 2025 05:42:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>美團在 AI 投入超百億，由於 GPU 價格昂貴</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F-Cj0BqaRie7Jk8WNA7QRjA" target="_blank"&gt;根據《科創板日報》的報道&lt;/a&gt;，美團核心本地商業 CEO 王莆中 6 月 26 日透露，美團在 AI 上的投入非常大，每年的投入超過百億元。&lt;/p&gt; 
&lt;p&gt;王莆中特別指出，由於圖形處理器（GPU）等硬件設施價格昂貴，AI 技術的研發成本居高不下。然而，他認為這種投入是必要的，因為只有建立起堅實的 AI 基礎設施，並不斷推進大模型的研究開發，才能讓過去十幾年間積累的寶貴數據資源煥發出新的活力，從而更好地服務於用户。&lt;/p&gt; 
&lt;p&gt;根據最新的財務報告，美團在 2024 年度的研發總支出達到了 211 億元。王莆中預測，到 2030 年，中國服務零售行業的線上化率將從當前的 9% 提升至 25%，市場規模將達到 7 萬億元人民幣。屆時，預計將有 300 個品牌能夠開設至少一千家店鋪，成為行業的領軍者。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;相關閲讀&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/news/354675/meituan-nocode" target="news"&gt;美團發佈 AI&amp;nbsp;Coding Agent 工具「NoCode」&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/news/354604" target="news"&gt;美團王興詳解 AI 佈局：No Code 平台免費開放，1680 個應用已上線&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/news/352148" target="news"&gt;美團：過去一季度內 52% 代碼由 AI 生成&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357551</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357551</guid>
      <pubDate>Sat, 10 May 2025 04:03:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Ubuntu 開發商 Canonical 去年營收近 3 億美元</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;十年前， Ubuntu Linux 背後的公司 Canonical 的營收約為 8100 萬美元（2014 年），員工人數約為 337 人。當時，他們的 Linux 桌面業務仍在 OEM/ODM 預裝系統、企業桌面環境以及利潤豐厚的服務器/雲領域站穩腳跟。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Canonical 最近提交了 2024 年年度報告，目前其營收已接近 3 億美元，員工人數超過 1100 人&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;幾天前，Canonical 向其總部所在地英國公司註冊局提交了截至 2024 年 12 月 31 日的年度報告。這份報告為公眾提供了一些關於這家 Ubuntu Linux 背後公司的健康狀況和整體增長情況的有趣見解。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="image.webp" src="https://static.oschina.net/uploads/img/202506/27112815_bbMf.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Canonical 2024 年年終財報顯示，公司營收為 2.92 億美元，較 2023 年的 2.51 億美元大幅增長。而前一年，也就是 2022 年，他們的營收為 2.05 億美元。他們的毛利率也從前一年的 80% 提升至 83%。2024 年，&lt;/p&gt; 
&lt;p&gt;Canonical 招聘了 100 多名新員工，公司平均員工人數從 1034 人增長至 1175 人。2022 年，他們的平均員工人數為 858 人。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="image.webp" src="https://static.oschina.net/uploads/img/202506/27112816_FIDc.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在其 2.91 億美元的營收中，他們報告的毛利潤為 2.58 億美元，營業利潤為 1550 萬美元。1550 萬美元的營業利潤高於 2023 年的 1120 萬美元，比 Canonical 早期的情況要好得多，那時他們通常每年都處於虧損狀態，並且依靠 Ubuntu 創始人 Mark Shuttleworth 的資金維持運營。隨着 Ubuntu Linux 的發展，Canonical 已經穩固地站穩了腳跟，到目前為止，這種情況已經持續了很多年。&lt;/p&gt; 
&lt;p&gt;已經有一段時間沒有聽到任何關於 Canonical 可能進行 IPO 的傳聞/談論了……上一次是在 2022 年，據此前報道他們計劃在 2023 年進行 IPO，但後來就沒了下文。&lt;/p&gt; 
&lt;p&gt;無論如何，至少他們的財務狀況依然強勁，並且繼續朝着正確的方向前進，為明年 Ubuntu 26.04 LTS 的重大發布大步邁進。 感興趣的朋友可以通過&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ffind-and-update.company-information.service.gov.uk%2Fcompany%2F06870835%2Ffiling-history%2FMzQ2OTM2OTYwNWFkaXF6a2N4%2Fdocument%3Fformat%3Dpdf%26download%3D0" target="_blank"&gt;英國公司註冊處 (UK Companies House)&lt;/a&gt;獲取完整的 Canonical 2024 年年度報告。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357540/canonical-2024-annual-report</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357540/canonical-2024-annual-report</guid>
      <pubDate>Sat, 10 May 2025 03:28:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>DeepSeek R2 推遲發佈：因 H20 算力短缺、以及梁文鋒對其性能尚不滿意</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.theinformation.com%2Farticles%2Fdeepseeks-progress-stalled-u-s-export-controls" target="_blank"&gt;根據《The Information》的報道&lt;/a&gt;，DeepSeek 工程師在過去幾個月一直致力於完善 R2 模型，但梁文鋒對 R2 現在的性能還不滿意，工程師團隊仍在全力優化和打磨，發佈時間待定。梁文峯要求模型達到更出色的結果才批准發佈。&lt;/p&gt; 
&lt;p&gt;此外，由於美國出口管制導致中國市場英偉達服務器芯片（H20）短缺，R2 的大規模普及可能面臨困難。&lt;/p&gt; 
&lt;p&gt;目前，大多數使用 DeepSeek R1 模型的中國雲客户仍依賴 H20 芯片。報道指出，如果 DeepSeek 即將推出的 R2 模型其性能超過目前市面上的開放替代模型，預計使用量將激增，&lt;strong&gt;超出中國雲平台的處理能力&lt;/strong&gt;。因為他們需要先進的英偉達芯片來運行 AI 模型。&lt;/p&gt; 
&lt;p&gt;DeepSeek 已向部分中國雲公司提供了 R2 的技術規範，以指導其託管和分發模型的計劃，但尚未公佈具體的發佈日期。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;相關閲讀&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/news/352701/deepseek-r1-0528-release-notes" target="news"&gt;DeepSeek-R1-0528 更新：思考更深，推理更強&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/news/352460/deepseek-r1-0528" target="news"&gt;DeepSeek R1 模型完成小版本試升級，邏輯理解能力提升&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357535/deepseeks-progress-stalled-u-s-export-controls</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357535/deepseeks-progress-stalled-u-s-export-controls</guid>
      <pubDate>Sat, 10 May 2025 03:16:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Nacos 3.0 架構全景解讀，AI 時代服務註冊中心的演進</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;p&gt;作者：楊翊（席翁），柳遵飛（翼嚴），羅鑫（子葵）&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Nacos&lt;/strong&gt; &lt;code&gt;/nɑ:kəʊs/&lt;/code&gt;是 Dynamic &lt;strong&gt;Na&lt;/strong&gt; ming and &lt;strong&gt;Co&lt;/strong&gt; nfiguration &lt;strong&gt;S&lt;/strong&gt; ervice 的首字母簡稱，隨着 Nacos 3.0 的發佈，定位由&lt;code&gt;"更易於構建雲原生應用的動態服務發現、配置管理和服務管理平台"&lt;/code&gt;升級至&lt;code&gt;" 一個易於構建 AI Agent 應用的動態服務發現、配置管理和 AI 智能體管理平台"&lt;/code&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-334a4cbf68666f5998460020dbedad24d97.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Nacos 從 2018 年 7 月宣佈開源以來，已經走過了第六個年頭，在這六年裏，備受廣大開源用户歡迎，收穫許多社區大獎。Nacos 在社區共同的建設下不斷成長，逐步開始幫助用户解決實際問題，助力企業數字化轉型，目前已經廣泛使用在國內的公司中，根據微服務領域調查問卷，Nacos 在註冊配置中心領域已經成為&lt;strong&gt;國內首選&lt;/strong&gt; ，佔有 &lt;strong&gt;50%+國內市場&lt;/strong&gt; 份額，被&lt;strong&gt;各行各業的頭部企業&lt;/strong&gt;廣泛使用。在此期間，Nacos 的部署包下載量突破 300w 次，官網每年訪問用户數超過 90w 人，被國內各主流雲廠商託管服務。&lt;/p&gt; 
&lt;p&gt;隨着 AI 時代到來以及 Nacos 3.0 版本的正式發佈，Nacos 未來的演進目標以及架構也會隨之升級。本文會對比 Nacos 3.0 與 Nacos 2.0 的架構異同，對 Nacos 3.0 的主要功能原理進行介紹。&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Nacos 2.0 架構回顧&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr&gt; 
&lt;p&gt;Nacos 2.0 的架構主要聚焦對&lt;code&gt;性能&lt;/code&gt;和&lt;code&gt;可擴展性&lt;/code&gt;進行優化和提升。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-92aef5263493adf4644595a964f278a1c2f.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;對於&lt;code&gt;性能&lt;/code&gt;升級，Nacos 2.0 通過將通信模型從 HTTP 升級至 gRPC，從短連接模型升級到長連接模型，使得 Nacos 的通信吞吐量中極大提升；同時配合數據存儲和數據結構模型的升級，進一步減少核心操作所涉及的步驟和鏈路，最終實現性能的 &lt;strong&gt;10 倍提升&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;關於&lt;code&gt;可擴展性&lt;/code&gt;升級，Nacos 2.0 通過將一些具有個性化需求的通用能力進行抽象，進行插件化改造的方式，允許 Nacos 用户和運維人員能夠開發自定義插件，適配個人或企業的個性化需求。&lt;/p&gt; 
&lt;p&gt;雖然 Nacos 2.0 在&lt;code&gt;性能&lt;/code&gt;和&lt;code&gt;可擴展性&lt;/code&gt;實現了一些突破，但仍然還存在一些挑戰。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-37c34e83fbdfb2c9872dfe9451498e525a9.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;其中一個主要的挑戰就是 Nacos 的安全風險。比如：Nacos 2.0 中所有的 HTTP API 均使用 8848 端口， 這其中及包含了 1.X 客户端使用的 API，也包含了運維人員以及控制枱的 API。不同類型的 API， 對於權限的需求其實是不同的，對於網絡訪問的連通性要求也是不同的。使用單端口並且使用唯一的鑑權開關，導致了網絡的訪問控制，以及鑑權控制都不是很靈活。許多用户為了方便使用，將此端口暴露在辦公網甚至公網環境，同時未開啓鑑權，這就造成了安全風險。&lt;/p&gt; 
&lt;p&gt;另一個問題就是默認命名空間的使用，Nacos 最初的版本中定義了命名空間作為數據資源的強隔離屬性，不同命名空間之間的服務和配置不能互相發現和獲取；但在最初版本中因為歷史原因，註冊中心和配置中心對於默認命名空間的處理方式有一定的不統一，這導致了許多用户在使用默認命名空間時經常配置錯誤或者出現疑惑；並且在 Nacos 2.0 提供各種插件能力之後，許多插件實現時需要額外工作進行適配，嚴重阻礙了插件的開發以及插件的穩定性。&lt;/p&gt; 
&lt;p&gt;隨着 AI 時代來臨，AI Agent 應用的部署形態在之前雲原生可彈性可伸縮的基礎上，要求更加輕量，更加彈性，例如 FC 場景；在這種要求下，我們需要考慮 Nacos 之前的服務發現和配置管理的能力是否還能承載 AI Agent 的應用的部署。同時，隨着越來越多的 AI Agent 的應用貫穿業務全線，Nacos 能否幫助更好地管理 AI Agent 的應用，是 Nacos 在當前的挑戰，同時也是新的機遇。&lt;/p&gt; 
&lt;p&gt;為了應對這些挑戰以及機遇，Nacos 3.0 架構也做了對應的升級。目標是在 AI 時代成為更安全的 Registry。設計理念也由之前的&lt;code&gt;一個更易於構建雲原生應用的動態服務發現、配置管理和服務管理平台&lt;/code&gt;升級為&lt;code&gt;一個易於構建 AI Agent 應用的動態服務發現、配置管理和 AI 智能體管理平台&lt;/code&gt;。&lt;/p&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Nacos 3.0 架構&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr&gt; 
&lt;h3&gt;2.1 Nacos 3.0 整體架構解析&lt;/h3&gt; 
&lt;p&gt;Nacos 3.0 升級後的整體架構仍然以一致性協議、通信模塊、其他模塊等通用功能模塊為基座，承載出註冊中心、配置中心、AI Registry、協議增強等功能；同時通過各類多語言 SDK，橋接各個生態組件。架構的左右兩側，分別是 Nacos 的插件以及 Nacos 的一些拓展組件，它們一起構成了 Nacos 3.0 的整體架構。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-36bacdaa811ec267912daebb99f3bff0018.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;我們來重點關注 Nacos 3.0 的新增能力，即圖中綠色和棕色的部分。&lt;/p&gt; 
&lt;p&gt;這其中既包括對原本註冊配置中心的增強功能，即模糊訂閲，也包括了對 AI 相關能力的實現和規劃，如 MCP 和管理，MCP Router，動態 Prompt 及 A2A 協議支持；同時也通過支持 xDS 協議及 Nacos Controller 繼續加強和探索 Mesh 生態。&lt;/p&gt; 
&lt;h3&gt;2.2 Nacos 3.0 AI Registry 架構&lt;/h3&gt; 
&lt;p&gt;瞭解完 Nacos 3.0 的整體架構，接下來我們來看 Nacos AI 中心（AI Registry）的架構設計。作為 Nacos 3.0 規劃中最重要的能力，Nacos AI Registry 的架構被分為 3 個層次，分別是&lt;code&gt;模型層、``工具層&lt;/code&gt;和&lt;code&gt;應用層&lt;/code&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-84c758dce0cbc64de279f716cb03fd3cd8d.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在&lt;code&gt;模型層&lt;/code&gt;中，主要通過對 AI 模型中一些常用的動態參數，比如 Prompt、學習率、聯網參數等進行管理，採取複用在雲原生應用中配置動態管理和分發能力的方式，幫助 AI 智能體在模型層進行快速調整及試錯。&lt;/p&gt; 
&lt;p&gt;模型層之上是&lt;code&gt;工具層&lt;/code&gt;，工具層主要幫助 LLM 模型和提供數據的 MCP 工具之間進行自動的發現、註冊以及檢索等能力，複用在雲原生應用中服務的動態註冊、管理、發現的能力，幫助 AI 智能體應用快速及便捷地發現 MCP 工具，同時快速過濾無關工具，減少 Token 損耗。&lt;/p&gt; 
&lt;p&gt;最頂層是 Agent 的&lt;code&gt;應用層&lt;/code&gt;，實現 AI 應用與 AI 應用之間的發現與協作。目前規劃是通過支持 A2A 等社區標準協議，同時配合 Spring AI Alibaba 等 AIAgent 應用框架，幫助 AIAgent 應用便捷的自動註冊自身 AI 應用，同時發現其他 AI 應用，並能夠像雲原生應用一樣，進行任務的分發以及結論的構成。&lt;/p&gt; 
&lt;p&gt;如果從功能視角出發，Nacos AI Registry 又可被分為針對大模型 LLM 的&lt;code&gt;模型動態配置調優，&lt;/code&gt;針對 AI 應用平台的&lt;code&gt;應用開發管理&lt;/code&gt;以及針對 AI Agent 應用的&lt;code&gt;運行時能力增強&lt;/code&gt;。Nacos 希望通過不同的功能點，幫助 AI 應用像微服務雲原生應用一樣，能動態的調整 Prompt，學習率等參數，無需重新發布，從而幫助 AI 應用簡化開發，調試過程中的繁瑣操作，提高 AI 應用的開發和運行效率。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-dd07e5e90a95088b6e84dc992e28fb225fe.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-d4273e4083d0c6f838fde1393f16872b30f.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;2.3 Nacos 3.0 安全架構&lt;/h3&gt; 
&lt;p&gt;Nacos 2.0 中面臨的一個主要的風險就是 Nacos 所有的 HTTP OpenAPI 均通過統一的端口進行暴露，同時使用了統一的鑑權開關，這使得使用者必須在便捷性和安全性中作出取捨，導致在許多部署的環境中可能存在安全風險。&lt;/p&gt; 
&lt;p&gt;Nacos 3.0 為瞭解決這個問題，從 Nacos 的部署架構上作出演進，&lt;code&gt;獨立控制枱部署&lt;/code&gt;，&lt;code&gt;拆分鑑權開關&lt;/code&gt;，&lt;code&gt;分類 API &lt;/code&gt;並&lt;code&gt;默認開啓控制枱及管控類 API 的鑑權。&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-3fc2b32b2e404eafbf929142ad4585d3c3b.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;同時配合&lt;code&gt;配置加密插件，``TLS 傳輸&lt;/code&gt;，來實現 Nacos 3.0 的零信任安全架構。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-f835e58de73cbab938bc6e5d6ed9849472a.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;除了針對 Nacos 自身的安全零信任架構外，Nacos 3.0 還將與 Druid，Spring AI Alibaba/Spring Cloud Alibaba 等開源社區，及 KMS 等安全雲產品合作，提供面向應用側的數據源運行期動態輪轉方案。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-f6a73d965abc4e2b27e2a584508f18ea32d.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在這套解決方案中，數據源的憑據始終由 KMS 等憑據託管平台和系統保存，全程無人工傳遞和配置的過程。用户可以設置定期進行憑據的自動輪轉，或在懷疑密鑰泄漏時手動觸發憑據輪轉；觸發後會通過 Nacos 動態無損的將新的加密憑據通知到 Druid 或 Spring AI Alibaba/Spring Cloud Alibaba，進行憑據的動態刷新和無損替換。這種方式極大降低了憑據泄漏的可能性，同時顯著提高了安全性及出現安全風險時的收斂恢復速度。&lt;/p&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Nacos MCP Registry&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr&gt; 
&lt;h3&gt;3.1 Nacos MCP Registry 架構&lt;/h3&gt; 
&lt;p&gt;Nacos 3.0 最主要的能力升級就是作為 MCP Registry，支持了 MCP 服務的管理能力。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-88c91519a2722fe1b60a505a553c7428a19.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Nacos MCP Registry 支持三類 MCP 服務的註冊方式：&lt;/p&gt; 
&lt;p&gt;第一類是將存量 HTTP 或 RPC 的服務，通過聲明自動轉化為 MCP 服務，配合 Higress 的協議轉換能力，實現 0 代碼改造成 MCP 服務協議，如何將存量 API 轉化為 MCP 服務，詳情可參見文檔【1】。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-3f1b620f54932854273dacb77d9d5d1d201.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;第二類就是新構建的 MCP 服務註冊， 配合 Spring AI 等 AI Agent 應用框架和 Nacos-MCP 的 sdk，能夠做到像微服務一樣自動註冊到 Nacos 中進行統一的管理和維護，如何通過 Spring AI 或 Nacos-MCP 的 sdk 進行 MCP 服務的自動註冊與發現，請參見文檔【2】。&lt;/p&gt; 
&lt;p&gt;第三類就是已經構建好的或其他供應商提供的 MCP 服務，可以導入到 Nacos 中，進行其描述、工具列表、工具 Schema 等內容的動態修改和維護，讓調試 MCP 服務變得更加簡單。&lt;/p&gt; 
&lt;h3&gt;3.2 Nacos MCP Router&lt;/h3&gt; 
&lt;p&gt;Nacos 3.0 支持用户通過 3 種方式發佈 MCP 服務，並對 MCP 服務的元數據和版本進行管理，但如果最終不能將這些元數據和版本信息進行實際的使用，這些信息就沒有意義。&lt;/p&gt; 
&lt;p&gt;因此 Nacos 3.0 提供 Nacos MCP Router 幫助終端使用者無需實際感知 MCP 服務列表，即可自動發現和使用需要的 MCP 服務。&lt;/p&gt; 
&lt;p&gt;Nacos MCP Router 提供兩種工作模式，&lt;code&gt;動態路由&lt;/code&gt;和&lt;code&gt;動態代理&lt;/code&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-054053f13b5e4c1b1712330bffd28c95d5d.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;動態路由模式將會根據 LLM 所提供的關鍵字信息，對註冊在 Nacos 中的 MCP 服務進行相關性過濾和篩選，選擇出與關鍵字相關的 MCP 服務進行實際的使用，從而減少對 LLM 上下文的消耗，實現&lt;code&gt;路由 &lt;/code&gt;MCP 服務的能力。&lt;/p&gt; 
&lt;p&gt;而代理模式能夠進行 MCP 協議的轉換，將 &lt;code&gt;stdio &lt;/code&gt;和 &lt;code&gt;sse &lt;/code&gt;類型的 MCP 服務，代理成 &lt;code&gt;streamable &lt;/code&gt;類型的 MCP 服務。代理模式下的 Nacos MCP Router 不根據關鍵字進行篩選，僅是將註冊在 Nacos 中的 &lt;code&gt;stdio &lt;/code&gt;和 &lt;code&gt;sse &lt;/code&gt;類型的 MCP 服務，轉化成 &lt;code&gt;streamable &lt;/code&gt;類型，同時應用用户在 Nacos 上修改和編輯的 Tool 描述信息，將轉化後的 MCP 服務列表，返回給 LLM 供其使用。&lt;/p&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;Nacos 3.0 RoadMap&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr&gt; 
&lt;p&gt;Nacos 3.0 的目標是成為&lt;code&gt;全面擁抱 AI 時代的服務、配置、AI Registry 平台&lt;/code&gt;，因此 Nacos3.0 的 RoadMap 將會逐步實現 AI Registry 的能力，從當前的 MCP 管理，拓展到 Prompt 管理，Agent 的自動註冊發現，再到 LLM 模型的參數管理和託管；同時進一步加強註冊配置中心的能力和更多相關領域協議的支持（如 DNS，Mesh）。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-445ba3a6100745072fa880c31873dc11d3d.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Nacos 也希望有更多的社區貢獻者加入進 Nacos 社區，幫助 Nacos 更快更好的完善和實現 Nacos3.0。&lt;/p&gt; 
&lt;ol start="5"&gt; 
 &lt;li&gt;歡迎加入 Nacos 社區&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr&gt; 
&lt;p&gt;Nacos 致力於幫助您發現、配置和管理微服務。Nacos 提供了一組簡單易用的特性集，幫助您快速實現動態服務發現、服務配置、服務元數據及 AI 管理。&lt;/p&gt; 
&lt;p&gt;Nacos 幫助用户更敏捷和容易地構建、交付和管理雲原生 AI 應用的平台。 Nacos 是構建以"服務"為中心的現代應用架構 (例如微服務範式、雲原生範式、AI 原生範式) 的服務基礎設施。&lt;/p&gt; 
&lt;p&gt;Nacos 3.0 還有很多待完成的功能及大量待探索和開發的領域，歡迎大家掃碼加入 Nacos 社區羣及 Nacos MCP 社區討論羣，參與 Nacos 社區的貢獻和討論，在 Nacos 社區一起搭把手，讓你的代碼和能力有機會能在各行各業領域內進行釋放能量，期待認識你和你一起共建 Nacos 社區；&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;"Nacos 相信一切都是服務，每個服務節點被構想為一個星球，每個服務都是一個星系；Nacos 致力於幫助這些服務建立連接賦予智能，助力每個有面向星辰的夢想能夠透過雲層，飛在雲上，更好的鏈接整片星空。"&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Nacos 官網：&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnacos.io%2F" target="_blank"&gt;https://nacos.io/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Nacos 倉庫地址：&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Falibaba%2Fnacos" target="_blank"&gt;https://github.com/alibaba/nacos&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;"Nacos 社區羣 5"羣的釘釘羣號：&lt;/p&gt; 
&lt;p&gt;120960003144&lt;/p&gt; 
&lt;p&gt;"Nacos MCP 社區討論羣"羣的釘釘羣號：&lt;/p&gt; 
&lt;p&gt;97760026913&lt;/p&gt; 
&lt;ol start="6"&gt; 
 &lt;li&gt;更多瞭解 Nacos 3.0&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr&gt; 
&lt;p&gt;6 月 6 日，Nacos 在上海舉辦了開源開發者沙龍 MeetUp 活動，此次是 Nacos 社區成員今年首次線下分享最新的能力和實踐，並邀請了 Spring AI Alibaba 和 Higress 一起分享一站式的開源解決方案。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-fecce2cf0706af578321cb7f3935150e704.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;有需要 MeetUp 的 PPT 或希望回看 MeetUp 活動視頻的同學，歡迎加入本文末尾的羣中獲取。&lt;/p&gt; 
&lt;p&gt;同時如果對 Nacos 3.0 的架構，運行原理，最佳實踐等內容感興趣的同學，歡迎閲讀 Nacos 3.0 更多相關文章：&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzUzNzYxNjAzMg%3D%3D%26mid%3D2247574941%26idx%3D1%26sn%3D18ea079839f6c0e5563f0e0b3bd2e80d%26scene%3D21%23wechat_redirect" target="_blank"&gt;《0 代碼改造實現應用運行時數據庫密碼無損輪轉》&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzUzNzYxNjAzMg%3D%3D%26mid%3D2247574770%26idx%3D1%26sn%3Df975121e36eb751a7659a66f42cbac04%26scene%3D21%23wechat_redirect" target="_blank"&gt;《Nacos MCP Router 新版發佈：支持 Docker 遠程部署，MCP 的多協議 stido、SSE、Streamable 互相轉換》&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzUzNzYxNjAzMg%3D%3D%26mid%3D2247574635%26idx%3D1%26sn%3D383a5c81ec298585f660b687a5dd4b12%26scene%3D21%23wechat_redirect" target="_blank"&gt;《企業生產環境中，實現 MCP 服務的統一管理和智能路由的實踐》&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU4NzU0MDIzOQ%3D%3D%26mid%3D2247519792%26idx%3D1%26sn%3D4c1c6491fb2d1f32f8370057be15e6ba%26scene%3D21%23wechat_redirect" target="_blank"&gt;《Nacos 3.0 正式發佈：MCP Registry、安全零信任、鏈接更多生態》&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;【1】存量 API 轉換 MCP 手冊&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnacos.io%2Fdocs%2Flatest%2Fmanual%2Fuser%2Fai%2Fapi-to-mcp%2F%3Fspm%3D5238cd80.2ef5001f.0.0.3f613b7ciRMNL5" target="_blank"&gt;https://nacos.io/docs/latest/manual/user/ai/api-to-mcp/?spm=5238cd80.2ef5001f.0.0.3f613b7ciRMNL5&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;【2】MCP Server 自動註冊手冊&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnacos.io%2Fdocs%2Flatest%2Fmanual%2Fuser%2Fai%2Fmcp-auto-register%2F%3Fspm%3D5238cd80.2ef5001f.0.0.3f613b7ciRMNL5" target="_blank"&gt;https://nacos.io/docs/latest/manual/user/ai/mcp-auto-register/?spm=5238cd80.2ef5001f.0.0.3f613b7ciRMNL5&lt;/a&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/3874284/blog/18638349</link>
      <guid isPermaLink="false">https://my.oschina.net/u/3874284/blog/18638349</guid>
      <pubDate>Sat, 10 May 2025 03:14:00 GMT</pubDate>
      <author>原創</author>
    </item>
    <item>
      <title>雷軍：一些人説小米自研芯片產品賣不動，瞎扯</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;在小米人車家全生態發佈會後，雷軍進行了分享表示，做玄戒 O1 的時候，小米完全沒有想到 O1 做的這麼好。「我真的沒想到，所以整個 O1 的芯片總量定的就不夠。規劃是做了 4 款產品」。&lt;/p&gt; 
&lt;p&gt;&lt;img height="305" src="https://oscimg.oschina.net/oscnet/up-b6b4bb5a4c2a474139aff036b5980fbc097.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;雷軍還特別強調，「特別感謝朱丹領軍的整個芯片團隊為小米做出的巨大貢獻，我自己用的也是玄戒的手機，現在體驗特別好」。&lt;/p&gt; 
&lt;p&gt;雷軍透露，第二代玄戒芯片會考慮在車上應用。「第一代主要是驗證技術，技術好到我無法相信」。&lt;/p&gt; 
&lt;p&gt;「我們這幾款手機和平板備貨都很少，我也看到一些説我們賣不動，瞎扯」，他説，「主要是三四年前，你能知道芯片做的有這麼好？肯定不知道。我們下一步肯定會芯片上車，我們全部自研了四合一的域控制器，我們就是為了掌握這個技術，為將來小米芯片上車做好準備」。（新浪科技）&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357527</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357527</guid>
      <pubDate>Sat, 10 May 2025 02:52:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>阿里巴巴 2025 財年收入 9963 億元</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;6 月 26 日晚，阿里巴巴集團發佈 2025 財年年報顯示，2025 財年阿里巴巴集團收入達 9963.47 億元，淨利潤同比增長 77% 至 1259.76 億元，展現出強勁的盈利能力。在 AI 需求的推動下，阿里雲財年收入突破雙位數增長，AI 相關產品收入連續七個季度實現三位數同比增長。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在 AI 領域，過去一年阿里發佈並開源多款模型，覆蓋全尺寸、全模態、多場景。4 月最新發布的阿里通義 Qwen3（簡稱「千問 3」）大模型，開源僅一個月全球累計下載量突破 1250 萬。截至 4 月底，阿里通義已開源 200 餘款模型，全球下載量超過 3 億次，千問系列衍生模型數量超 10 萬個，成為全球最大的開源模型家族。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;阿里雲加速 AI 產品國際化，截至 2025 年 3 月 31 日，為全球 34 個地區提供雲計算服務。以通義大模型為底座，淘寶天貓、1688、阿里國際站、夸克、釘釘、高德、飛豬、閒魚等阿里多業務 AI 升級加速。其中，阿里 AI 旗艦應用夸克用户規模同比迅速增長，截至 2025 財年末，月活躍用户數已突破 2 億；2025 年 3 月，釘釘的平均付費周活躍用户數達 4200 萬，目前釘釘是國內最大的效率辦公類 App。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在致股東信中，阿里巴巴表示，「阿里的基因裏沒有守成，只有創造。今天的阿里巴巴，正在以創業者的姿態，開啓面向 AI 時代的全新徵程。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="373" src="https://oscimg.oschina.net/oscnet/up-074b55c1ba3ac1d7195b638a6c1bada394d.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;此外，阿里巴巴合夥人名單相比 2024 財年年報披露時發生變化，總數從 26 人減少至 17 人，戴珊、方永新、彭蕾、宋潔、孫利軍、武衞、俞永福、張勇、朱順炎等 9 人退出合夥人之列。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357517</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357517</guid>
      <pubDate>Sat, 10 May 2025 02:19:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>快手開源多模態大模型 Kwai Keye-VL</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;快手宣佈並開源其最新自研的多模態大語言模型 Kwai Keye-VL。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;根據介紹，Kwai Keye-VL 以 Qwen3-8B 語言模型為基礎，引入了基於開源 SigLIP 初始化的 VisionEncoder，能夠深度融合並處理文本、圖像、視頻等多模態信息，憑藉其創新的自適應交互機制與動態推理能力，旨在為用户提供更智能、全面的多模態交互體驗。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;Kwai Keye-VL 支持動態分辨率輸入，按原始比例將圖像切分為 14x14 &amp;nbsp;patch 序列，由一個 MLP 層將視覺 Token 進行映射與合併。模型採用 3D RoPE （旋轉位置編碼）統一處理文本、圖像和視頻，並通過位置編碼與時間戳對齊，精準捕捉視頻時序變化。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="366" src="https://oscimg.oschina.net/oscnet/up-f9bb9b208e03575669510048f8ff6cabc1e.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="138" src="https://oscimg.oschina.net/oscnet/up-a5530c104b198c517ed650e3e67740584c5.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在視覺理解與邏輯推理能力方面，Kwai Keye-VL 的綜合感知能力媲美同規模頂尖模型，並在複雜推理任務中展現出顯著優勢。尤其是邏輯推理方面，Kwai Keye-VL 在最新的 2025 年高考全國數學卷中取得了 140 分的成績。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="314" src="https://oscimg.oschina.net/oscnet/up-d0c30a1de0375792399c2797d5e075fce36.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;為突破公開數據集的數據污染、語言覆蓋侷限及任務單一性等問題，快手構建了內部評測集 KC-MMBench。結果顯示：該模型在 VideoMME 等權威公開 Benchmark 中以 67.4 分超越 Qwen2.5-VL-7B（62.7）與 InternVL-3-8B（65.5）；在內部短視頻場景評測中優勢進一步擴大，綜合得分領先 SOTA 模型超 10%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="296" src="https://oscimg.oschina.net/oscnet/up-d5f95b60cd23280d98fa13ffda02bd537e7.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;更多詳情可&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F2JRGYhB_VDPecXMjp3gZsQ" target="_blank"&gt;查看官方公告&lt;/a&gt;。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357515</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357515</guid>
      <pubDate>Sat, 10 May 2025 02:12:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Gartner：超 40% 的 AI 智能體項目活不過兩年</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;研究諮詢公司 Gartner 最新發布的一份報告&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.gartner.com%2Fen%2Fnewsroom%2Fpress-releases%2F2025-06-25-gartner-predicts-over-40-percent-of-agentic-ai-projects-will-be-canceled-by-end-of-2027" target="_blank"&gt;指出&lt;/a&gt;，預計到 2027 年底，超過 40% 的 AI 智能體項目將被取消，原因是成本不斷上升和商業價值不明確。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;Gartner 高級總監分析師 Anushree Verma 表示：「目前大多數 AI 智能體項目都處於早期實驗或概念驗證階段，這些項目大多受到炒作的驅動，並且經常被誤用。這可能會讓企業忽視大規模部署 AI 智能體的實際成本和複雜性，從而阻礙項目投入生產。他們需要撥開炒作的迷霧，謹慎地制定戰略決策，確定在何處以及如何應用這項新興技術。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="281" src="https://oscimg.oschina.net/oscnet/up-4f7c22926062ebd6122d8165e030afd0db5.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;Gartner 基於 3412 名受訪者的調查結果顯示，19% 的人表示其組織已對 AI 智能體項進行了大量投資，42% 的人進行了保守投資，8% 的人沒有投資，其餘 31% 的人採取觀望態度或不確定。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;許多供應商通過「洗牌」來炒作，即對現有產品（例如 AI 助手、機器人流程自動化 (RPA) 和聊天機器人）進行品牌重塑，而這些產品本身並不具備實質性的智能體功能。Gartner 估計，在數千家 AI 智能體供應商中，只有大約 130 家是有真材實料的。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;「大多數 AI 智能體方案缺乏顯著的價值或投資回報率 (ROI)，因為目前的模型還不夠成熟，無法自主實現複雜的業務目標或持續遵循細微的指令。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;Gartner 預測，到 2028 年，至少 15% 的日常工作決策將通過 AI 智能體自主做出，而 2024 年這一比例為 0%。此外，到 2028 年，33% 的企業軟件應用程序將包含 AI 智能體，而 2024 年這一比例還不到 1%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在早期階段，Gartner 建議僅在能夠帶來明確價值或投資回報率 (ROI) 的情況下才應採用 AI 智能體。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;「為了從 AI 智能體中獲得真正的價值，組織必須專注於企業生產力，而不僅僅是增強單個任務。他們可以先在需要決策時使用 AI 智能體，在日常工作流程中實現自動化，並在簡單檢索時使用助手。這關乎通過成本、質量、速度和規模來推動業務價值。」&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357455/gartner-over-40-percent-agentic-ai-projects-cancel-2027</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357455/gartner-over-40-percent-agentic-ai-projects-cancel-2027</guid>
      <pubDate>Fri, 09 May 2025 10:38:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>LowCodeEngine —— 企業級低代碼技術體系</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                        
                                                                                    &lt;p&gt;&lt;span style="background-color:#ffffff; color:#1f2328"&gt;一套面向擴展設計的企業級低代碼技術體系。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style="background-color:#ffffff; color:#1f2328"&gt;&lt;img alt="" height="276" src="https://static.oschina.net/uploads/space/2025/0619/160054_sOPF_4252687.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt;

&lt;div style="text-align:start"&gt;
&lt;h2&gt;特性&lt;/h2&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;提煉自企業級低代碼平台的面向擴展設計的內核引擎，奉行最小內核，最強生態的設計理念&lt;/li&gt;
&lt;li&gt;開箱即用的高質量生態元素，包括，物料體系、設置器、插件，等&lt;/li&gt;
&lt;li&gt;完善的工具鏈，支持，物料體系、設置器、插件，等生態元素的全鏈路研發週期&lt;/li&gt;
&lt;li&gt;強大的擴展能力，已支撐 100+ 個各種類型低代碼平台&lt;/li&gt;
&lt;li&gt;使用 TypeScript 開發，提供完整的類型定義文件&lt;/li&gt;
&lt;/ul&gt;

&lt;div style="text-align:start"&gt;
&lt;h2&gt;兼容環境&lt;/h2&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;現代瀏覽器（Chrome &amp;gt;= 80, Edge &amp;gt;= 80, last 2 safari versions, last 2 firefox versions）&lt;/li&gt;
&lt;/ul&gt;

&lt;div style="text-align:start"&gt;
&lt;h2&gt;引擎協議&lt;/h2&gt;
&lt;/div&gt;

&lt;p style="color:#1f2328; text-align:start"&gt;引擎完整實現了《低代碼引擎搭建協議規範》和《低代碼引擎物料協議規範》，協議棧是低代碼領域的物料能否流通的關鍵部分。&lt;/p&gt;

&lt;p&gt;&lt;img height="277" src="https://static.oschina.net/uploads/space/2025/0619/160028_gMUx_4252687.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

                                                                    &lt;/div&gt;
                                                                </description>
      <link>https://www.oschina.net/p/lowcode-engine</link>
      <guid isPermaLink="false">https://www.oschina.net/p/lowcode-engine</guid>
      <pubDate>Fri, 09 May 2025 09:49:00 GMT</pubDate>
    </item>
    <item>
      <title>Mozilla 終止維護開源語音轉文本引擎項目「DeepSpeech」</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;DeepSpeech 是 Mozilla 開發的一款開源語音轉文本引擎，基於百度 2014 年發表的研究論文《Deep Speech: Scaling up end-to-end speech recognition》所提出的端到端語音識別方法開發。&lt;/p&gt; 
&lt;p&gt;從&amp;nbsp;DeepSpeech 的倉庫動態來看，Mozilla 已於上週將項目倉庫歸檔，並表示停止維護。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0626/155240_73ji_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;作為一款端到端自動語音識別（ASR）引擎，DeepSpeech 即使在 Raspberry Pi SBC 和其他低功耗系統上運行時，也能提供出色的實時通信性能。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-762647632f4a326522c5f510328561b4af1.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;遺憾的是，&lt;span&gt;近年來 DeepSpeech 項目的活躍度持續降低，其&lt;/span&gt;最後一個標記版本是 2020 年 12 月發佈的 0.9.3。&lt;/p&gt; 
&lt;p&gt;DeepSpeech&lt;span&gt;&amp;nbsp;GitHub 倉庫已經有近 4 年沒有任何 commit，社區貢獻和更新頻率都不盡如人意，這使得項目的進一步發展受到限制，因此 Mozilla 選擇終止該項目。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357400/mozilla-deepspeech-discontinued</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357400/mozilla-deepspeech-discontinued</guid>
      <pubDate>Fri, 09 May 2025 08:04:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>Cursor 如何保障「代碼索引」的安全、高效</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;編者按：&lt;/strong&gt; AI 編程工具如何迅速檢索海量代碼庫，並精準定位到最相關的代碼片段？這個看似不可能完成的任務，卻是決定現代 AI 編程工具用户體驗的關鍵技術挑戰。&lt;/p&gt; 
 &lt;p&gt;我們今天為大家帶來的這篇文章，作者的觀點是：Cursor 通過巧妙運用默克爾樹數據結構，實現了對大型代碼庫的快速索引和高效增量更新，這正是其能夠提供精準 AI 輔助編程服務的技術基礎。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;作者 | Engineer's Codex&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;編譯 | 嶽揚&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Cursor ------ 這家最近宣佈斬獲 3 億美元年營收的熱門 AI 開發工具 ------ 正是利用默克爾樹（Merkle trees）實現對代碼的快速索引。本篇文章將為你詳細介紹其運作原理。&lt;/p&gt; 
&lt;p&gt;在深入瞭解 Cursor 的具體實現方法之前，我們先來瞭解一下默克爾樹的基本概念。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;01 默克爾樹的簡單解釋&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;默克爾樹（Merkle tree）是一種樹狀數據結構，其每個"葉"節點都標註了對應數據塊的加密哈希值，而每個非葉節點則存儲其子節點哈希值組合後的新哈希值。這種層級結構通過比較哈希值，能有效地偵測任何層級的數據變動。&lt;/p&gt; 
&lt;p&gt;通俗理解，它就像是數據的指紋系統：&lt;/p&gt; 
&lt;p&gt;1）每份數據（例如文件）都擁有自己獨一無二的指紋（哈希值）&lt;/p&gt; 
&lt;p&gt;2）成對的指紋被組合在一起，生成一個新的指紋&lt;/p&gt; 
&lt;p&gt;3）此過程層層遞進，直至形成唯一的主指紋（根哈希）&lt;/p&gt; 
&lt;p&gt;根哈希（root hash）概括了所有底層數據塊的指紋信息，相當於對整個數據集做了一次加密公證。只要根哈希不變，就能證明原始數據分毫未改。此機制的精妙之處在於：&lt;strong&gt;任何一個數據塊發生變化，都將牽一髮而動全身 ------ 改變其上所有層級的指紋，最終徹底改變根哈希值。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-54b3916d0a8c97b04ff5e18eb75930572e1.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;02 Cursor 如何利用默克爾樹實現代碼庫索引功能&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;默克爾樹 (Merkle trees) 是 Cursor 代碼庫索引功能的核心組件。根據 Cursor 創始人發佈的帖子[1]和 Cursor 的安全文檔[2]，其工作流程如下：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-434fca0d5d62079cc39b5f46a8de5f6cdc9.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;2.1 步驟 1：代碼分塊與預處理&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;Cursor 首先在本地對代碼庫文件進行分塊處理，將代碼分割成具有語義含義的片段。此步驟是後續操作的必要前提。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;2.2 步驟 2：默克爾樹的構建與同步&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;啓用代碼庫索引功能後，Cursor 會掃描編輯器當前打開的文件夾，併為所有有效文件計算哈希值組成的默克爾樹。隨後，該默克爾樹會與 Cursor 的服務器同步，其安全文檔[2]詳細描述了此過程。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;2.3 步驟 3：生成嵌入向量&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;將代碼分塊併發送至 Cursor 服務器後，將使用 OpenAI 的嵌入 API 或自研的嵌入模型（我未能驗證 Cursor 具體採用的是哪種方法）生成嵌入向量 (embeddings)。這些向量表徵能夠捕捉代碼片段的語義信息。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;2.4 步驟 4：存儲與索引&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;生成的嵌入向量，連同起始/結束行號及文件路徑等元數據，會被存儲在一個遠程向量數據庫（Turbopuffer）中。為兼顧路徑篩選功能與隱私保護，Cursor 會為每個向量附加經過混淆處理的相對文件路徑。Cursor 創始人曾明確表示[1]："我們的數據庫中不會存儲任何代碼。請求處理完畢立即銷燬存儲的代碼數據。"&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;2.5 步驟 5：基於默克爾樹的定期更新&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;每隔 10 分鐘，Cursor 就會檢測哈希值的變化情況，利用默克爾樹精準定位哪些文件發生了變動。如 Cursor 的安全文檔[2]所述，只需上傳所定位到的發生變動的文件，從而大幅降低帶寬消耗。&lt;strong&gt;默克爾樹結構的最大價值正體現於此 ------ 它能實現高效的增量更新。&lt;/strong&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;03 代碼分塊策略&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;代碼庫索引的有效性很大程度上取決於代碼的分塊方式。儘管我先前的説明未深入探討代碼分塊方法，但這篇關於構建類 Cursor 代碼庫功能的博客[3]揭示了一些技術細節：&lt;/p&gt; 
&lt;p&gt;簡單的分塊方式（按字符/按單詞/按行）往往會遺漏語義邊界 ------ 導致嵌入向量質量下降。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;儘管可根據固定的 token 數分割代碼，但這種方式可能導致函數或類等代碼塊被強制截斷。&lt;/li&gt; 
 &lt;li&gt;更有效的方案是使用能夠理解代碼結構的智能分割器，例如遞歸文本分割器（recursive text splitters），它使用高級分隔符（如類定義和函數聲明）在恰當的語義邊界處進行精準切分。&lt;/li&gt; 
 &lt;li&gt;一個更優雅的解決方案是根據代碼的抽象語法樹（AST）結構來分割代碼。通過深度優先遍歷 AST，將代碼分割成符合 token 數量限制的子樹結構。為避免產生過多的碎片化分塊，系統會在滿足 token 限制的前提下，將同級語法節點合併為更大的代碼塊。此類 AST 解析工作可藉助 tree-sitter[4] 等工具實現，其支持絕大多數主流編程語言。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;&lt;strong&gt;04 嵌入向量在推理階段的應用&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;在瞭解 Cursor 如何創建和存儲代碼嵌入向量後，一個自然而然的問題就出現了：這些嵌入向量在生成之後究竟是如何使用的？&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;4.1 語義搜索（Semantic Search）與上下文檢索（Context Retrieval）&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;當你使用 Cursor 的 AI 功能（例如通過 &lt;a href="https://my.oschina.net/u/135318"&gt;@Codebase&lt;/a&gt; 或 ⌘ Enter 詢問代碼庫相關問題時），將觸發以下流程：&lt;/p&gt; 
&lt;p&gt;1）將查詢轉換為向量：Cursor 會為您的提問或當前代碼上下文生成對應的嵌入向量。&lt;/p&gt; 
&lt;p&gt;2）向量相似性搜索：該查詢向量被髮送至 Turbopuffer（Cursor 的向量數據庫），通過最近鄰搜索找出與查詢語義相似的代碼塊。&lt;/p&gt; 
&lt;p&gt;3）訪問本地文件：Cursor 客户端接收到的檢索結果包含經過混淆處理的文件路徑和最相關代碼塊的行號範圍。實際代碼內容始終保留在用户本地設備，僅在需要時從本地讀取。&lt;/p&gt; 
&lt;p&gt;4）上下文整合：客户端從用户本地文件讀取這些相關代碼塊，並將其作為上下文與您的問題一併發送至服務器供大語言模型處理。&lt;/p&gt; 
&lt;p&gt;5）生成響應：此時大語言模型已獲取代碼庫中的相關上下文，可據此提供精準回答或生成符合場景的代碼補全。&lt;/p&gt; 
&lt;p&gt;這種由嵌入向量驅動的檢索機制支持以下功能：&lt;/p&gt; 
&lt;p&gt;1）根據上下文生成代碼：在編寫新代碼時，Cursor 可參考現有代碼庫中的相似實現，保持代碼模式與代碼風格的一致性。&lt;/p&gt; 
&lt;p&gt;2）代碼庫智能問答：可以獲取基於代碼庫中實際代碼的精準解答，而非通用回覆。&lt;/p&gt; 
&lt;p&gt;3）智能代碼補全：代碼補全建議會融合項目的特定約定與特定模式。&lt;/p&gt; 
&lt;p&gt;4）智能重構輔助：重構代碼時，系統可自動識別代碼庫中所有需要同步修改的關聯代碼段。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;05 Cursor 為何選擇默克爾樹&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;這些設計細節多與安全有關，具體可參閲 Cursor 的安全文檔[2]。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;5.1 高效的增量更新&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;默克爾樹使 Cursor 能精準定位自上次同步後變更的文件。因此，無需重新上傳整個代碼庫，僅需上傳修改過的特定文件。對於大型代碼庫來説這一點非常重要 ------ 重新索引所有文件會消耗過多的帶寬和處理時間。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;5.2 數據完整性驗證&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;默克爾樹結構讓 Cursor 能高效驗證所索引的文件與服務器上存儲的文件是否一致。分層的哈希結構可輕鬆檢測傳輸過程中的數據異常或數據損壞。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;5.3 緩存優化&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;Cursor 將嵌入向量（embeddings）存儲在以代碼塊（chunk）哈希值為索引的緩存中，使得重複索引相同代碼庫時速度大幅提升。這對多人協作開發同一代碼庫的團隊尤為有利。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;5.4 隱私保護型索引&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;為保護文件路徑中的敏感信息，Cursor 採用路徑混淆技術：通過用 "/" 和 "." 為分隔符切割路徑，並用存儲在客户端的密鑰加密每一段。雖然這樣做會暴露部分目錄結構，但能隱藏絕大多數敏感細節。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;5.5 集成 Git 版本歷史&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;在 Git 倉庫中啓用代碼庫索引時，Cursor 還會索引 Git 的版本歷史記錄。它會存儲 commit 的 SHA 值、父提交信息（parent information）及混淆後的文件名。為實現同 Git 倉庫且同團隊用户間的數據結構共享，用於混淆文件名的秘鑰來自最近 commit 內容的哈希值。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;06 嵌入模型的選擇與技術考量&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;嵌入模型的選擇直接影響代碼搜索與理解的質量。&lt;/strong&gt; 部分系統採用開源模型（如 all-MiniLM-L6-v2[5]），而 Cursor 可能使用 OpenAI 的嵌入模型或針對代碼場景進行優化的定製模型。對於專用的代碼嵌入模型，微軟的 unixcoder-base[6] 或 Voyage AI 的 voyage-code-2[7] 等模型對代碼的語義理解效果顯著。&lt;/p&gt; 
&lt;p&gt;由於嵌入模型存在 token 容量限制，使得該技術的實現難度大幅增加。以 OpenAI 的 text-embedding-3-small[8] 為例，其 token 上限為 8192。有效的分塊策略能在保留語義的前提下不超出該限制。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;07 握手同步流程&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;Cursor 默克爾樹實現的核心在於同步時的握手機制。根據應用日誌顯示：在初始化代碼庫索引時，Cursor 會創建一個"merkle client"並與服務器進行"初始化握手流程"（詳見 GitHub Issue #2209[9] 與 Issue #981[10]），該握手流程涉及向服務器發送本地計算的默克爾樹的根哈希值。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;握手流程使服務器能精準判定需同步的代碼範圍。&lt;/strong&gt; 如握手日誌所示（參照 GitHub Issue #2209[11]），Cursor 會計算代碼庫的初始哈希值，並將其發送至服務器進行驗證。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;08 技術實現挑戰&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;雖然默克爾樹方案有許多優勢，但其實現過程仍存在一些技術難點。&lt;/strong&gt; Cursor 的索引服務常因瞬時流量過載，導致大量請求失敗。如安全文檔所述[2]，用户可能觀察到向 repo42.cursor.sh 發送的網絡流量比預期要高 ------ 這正是由於文件需多次重傳才能被完全索引。&lt;/p&gt; 
&lt;p&gt;另一項挑戰與嵌入向量的安全性有關。學術研究表明，特定條件下存在逆向解析嵌入向量的可能性。雖然當前的攻擊手段通常需同時滿足：1) 擁有嵌入模型的訪問權限 2) 僅對短文本有效。但若攻擊者獲取 Cursor 向量數據庫的訪問權限，仍存在從存儲的嵌入向量中提取代碼庫信息的風險。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;END&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;本期互動內容 🍻&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;❓Cursor 通過路徑混淆和本地哈希計算保護隱私，但同步時仍需上傳部分數據。在團隊協作場景下，你更傾向於完全本地化的方案，還是接受有限數據上傳以換取更強的 AI 輔助？為什麼？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;文中鏈接&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;[1]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fforum.cursor.com%2Ft%2Fcodebase-indexing%2F36" target="_blank"&gt;https://forum.cursor.com/t/codebase-indexing/36&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[2]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.cursor.com%2Fen%2Fsecurity" target="_blank"&gt;https://www.cursor.com/en/security&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[3]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.lancedb.com%2Frag-codebase-1%2F" target="_blank"&gt;https://blog.lancedb.com/rag-codebase-1/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[4]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftree-sitter.github.io%2Ftree-sitter%2F" target="_blank"&gt;https://tree-sitter.github.io/tree-sitter/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[5]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fsentence-transformers%2Fall-MiniLM-L6-v2" target="_blank"&gt;https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[6]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fmicrosoft%2Funixcoder-base" target="_blank"&gt;https://huggingface.co/microsoft/unixcoder-base&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[7]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.voyageai.com%2Fembeddings%2Fmodels%2F" target="_blank"&gt;https://docs.voyageai.com/embeddings/models/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[8]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fplatform.openai.com%2Fdocs%2Fguides%2Fembeddings" target="_blank"&gt;https://platform.openai.com/docs/guides/embeddings&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[9]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fgetcursor%2Fcursor%2Fissues%2F2209" target="_blank"&gt;https://github.com/getcursor/cursor/issues/2209&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[10]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fgetcursor%2Fcursor%2Fissues%2F981" target="_blank"&gt;https://github.com/getcursor/cursor/issues/981&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[11]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fgetcursor%2Fcursor%2Fissues%2F2209" target="_blank"&gt;https://github.com/getcursor/cursor/issues/2209&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;本文經原作者授權，由 Baihai IDP 編譯。如需轉載譯文，請聯繫獲取授權。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;原文鏈接：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fread.engineerscodex.com%2Fp%2Fhow-cursor-indexes-codebases-fast" target="_blank"&gt;https://read.engineerscodex.com/p/how-cursor-indexes-codebases-fast&lt;/a&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/IDP/blog/18638294</link>
      <guid isPermaLink="false">https://my.oschina.net/IDP/blog/18638294</guid>
      <pubDate>Fri, 09 May 2025 08:01:00 GMT</pubDate>
      <author>原創</author>
    </item>
  </channel>
</rss>
