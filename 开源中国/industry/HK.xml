<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - industry - 繁體中文（香港）</title>
    <link>https://www.oschina.net/news/industry</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news/industry" rel="self" type="application/rss+xml"/>
    <description>已對該 RSS 進行格式化操作：中英字符之間插入空格、使用直角引號、標點符號修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-hk</language>
    <lastBuildDate>Tue, 16 Sep 2025 12:40:40 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>北京擁有全球最密集重大科技基礎設施、最大科研人才隊伍</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbaijiahao.baidu.com%2Fs%3Fid%3D1843300877733716689%26wfr%3Dspider%26for%3Dpc" target="_blank"&gt;根據《北京日報》的報道&lt;/a&gt;，北京市發改委主任楊秀玲介紹，五年來，北京率先整合設立教育科技人才工作領導小組，着力提升創新體系整體效能，努力打造新質生產力的重要發動機。&lt;/p&gt; 
&lt;p&gt;現在的北京，已經擁有全球最密集的重大科技基礎設施、最多人次的高被引科學家數量、最大規模的科研人才隊伍、最具競爭力的開放創新生態。&lt;/p&gt; 
&lt;p&gt;這五年，中關村世界領先科技園區加快建設，「三城一區」主平台作用進一步凸顯，國家實驗室高質量在軌運行，全國重點實驗室 145 家、佔總量的近三成，全社會研發投入強度保持在 6% 左右、位居全球創新城市前列，入選中國科學十大進展的成果數量佔全國半數以上。&lt;/p&gt; 
&lt;p&gt;這五年，北京搶抓產業變革機遇，打造「人工智能第一城」，累計備案上線大模型 158 款、全國佔比約三成，豆包、智譜、kimi 等標杆模型性能穩居全球第一梯隊；&lt;/p&gt; 
&lt;p&gt;商業航天產業培育壯大，擁有全國一半的核心研發單位、上市企業和獨角獸企業，「朱雀三號」可重複使用運載火箭順利完成一級動力系統試車；&lt;/p&gt; 
&lt;p&gt;近 200 種機器創新產品在 130 多種場景實現應用落地，機器人技術逐漸走出實驗室、走進百姓生活；&lt;/p&gt; 
&lt;p&gt;系統佈局未來產業，實現 6G 超寬帶光電融合集成系統、「夸父」量子計算雲平台等技術突破。&lt;/p&gt; 
&lt;p&gt;「五年來，我們堅持以經濟體制改革為牽引，強化重點領域和關鍵環節改革攻堅，市場環境更加公平、企業經營更有活力。」楊秀玲説，要素配置更加高效，北京證券交易所開市；民營經濟高質量發展，世界 500 強榜單北京上榜民營企業 6 家、居全國城市首位；「北京服務」更加貼心，迭代出台改革措施 1700 餘項，非現場監管覆蓋率近 70%，經營主體 2021-2024 年年均增長 6.2%、總量達到 268.6 萬户。&lt;/p&gt; 
&lt;p&gt;五年來，本市大力推進高水平對外開放，主動為國家開放發展試製度，累計實施突破性政策 140 餘項，全市貨物貿易進出口規模連續三年超過 3.6 萬億，服務貿易 2021-2024 年年均增速達 9.4%。新落户國際組織 37 家，新設外資企業超 8000 家，綜保區由 1 個擴大到 4 個。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/372601</link>
      <guid isPermaLink="false">https://www.oschina.net/news/372601</guid>
      <pubDate>Sun, 14 Sep 2025 11:38:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>TinyLisp — 99 行 C 代碼實現的完整 Lisp 解釋器</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                        
                                                                                    &lt;p&gt;TinyLisp 是用 99 行 C 代碼實現的完整 Lisp 解釋器，包含了 21 個內置函數、垃圾回收機制和 REPL 交互環境，甚至還能在 1980 年代的掌上電腦上運行，只需一行命令即可編譯運行。&lt;/p&gt;

&lt;p&gt;主要特性&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;支持函數式編程、閉包、宏等高級特性&lt;/li&gt;
&lt;li&gt;內置簡單垃圾回收機制和 REPL 環境&lt;/li&gt;
&lt;li&gt;配有詳細技術文章解釋實現原理&lt;/li&gt;
&lt;li&gt;多個優化版本適應不同性能需求&lt;/li&gt;
&lt;li&gt;能在 Sharp PC-G850 等古董設備上運行&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-5b51d9dbe46e01a62723005a08a00a27c4f.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-f8e89c8e5f5f4ec1b4d38c18fdc1a45b06b.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

                                                                    &lt;/div&gt;
                                                                </description>
      <link>https://www.oschina.net/p/tinylisp</link>
      <guid isPermaLink="false">https://www.oschina.net/p/tinylisp</guid>
      <pubDate>Sun, 14 Sep 2025 10:57:00 GMT</pubDate>
    </item>
    <item>
      <title>清華聯手上海 AI Lab 發佈開源 SimpleVLA-RL 框架</title>
      <description/>
      <link>https://www.oschina.net/news/372593</link>
      <guid isPermaLink="false">https://www.oschina.net/news/372593</guid>
      <pubDate>Sun, 14 Sep 2025 10:45:00 GMT</pubDate>
    </item>
    <item>
      <title>阿里上榜全球創新人才最佳僱主：AI 原生應用 Accio 備受關注</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;全球知名商業媒體《快公司》發佈 2025 年度「創新人才最佳僱主」榜單，阿里巴巴成排名最高的中國科技公司。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="283" src="https://oscimg.oschina.net/oscnet/up-816fc66a4d8be8e5abdbe06b3898f9b8316.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;《快公司》「創新人才最佳僱主」榜旨在表彰重視員工創新併為團隊創造前瞻性工作環境的企業。《快公司》稱，阿里此次上榜緣於對 AI 原生應用 Accio 的創新探索。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;據悉，Accio 是全球首個貿易領域的 AI 原生應用，由出海平台阿里國際站推出，能幫中小企業自動化地完成全球採購流程，被稱為「第一個會做生意的 AI Agent」。上線 9 個月來，Accio 的海外企業用户數快速突破 200 萬，創新性的體驗深受中小企業歡迎。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/372592</link>
      <guid isPermaLink="false">https://www.oschina.net/news/372592</guid>
      <pubDate>Sun, 14 Sep 2025 10:29:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>崑崙萬維旗下 AI 音樂創作平台 Mureka 「Agent Studio」</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span&gt;崑崙萬維旗下 AI 音樂創作平台 Mureka&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FLNjbrW6yNoKFmnPnywxiVg" target="_blank"&gt;上線&lt;/a&gt;了全新功能——&lt;strong&gt;Agent Studio。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="544" src="https://static.oschina.net/uploads/space/2025/0916/182234_7QJo_2720166.jpg" width="1280" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;該功能通過直觀的方式讓音樂創作變得觸手可及，用户只需簡單描述想法，AI 就能自動生成歌詞和音樂。&lt;/p&gt; 
&lt;p&gt;Agent Studio 包含多個創作場景，如專輯製作、熱點寫歌等，為用户提供多樣化的音樂體驗。&lt;/p&gt; 
&lt;p&gt;-&amp;nbsp;Make Album（創作專輯）：不僅是一首歌，還能基於任意概念生成一整張專輯&lt;br&gt; -&amp;nbsp;Tarot Tunes（塔羅音愈）：問一個問題，AI 用歌來回答，像抽音樂塔羅&lt;br&gt; -&amp;nbsp;Buzz Tracks（熱點寫歌）：把熱點新聞、流行梗做成歌，抓住當下&lt;br&gt; -&amp;nbsp;Diss Tracks（Diss 製造機）：寫一首犀利的 Diss 歌，battle 隨時開局&lt;br&gt; -&amp;nbsp;Gift a Song（以歌致禮）：專門為朋友、戀人、家人寫的禮物歌&lt;br&gt; -&amp;nbsp;Spicy Song（撩人情歌）：大膽、曖昧，適合戀人之間的「私密歌」&lt;/p&gt; 
&lt;p&gt;&lt;img height="602" src="https://static.oschina.net/uploads/space/2025/0916/182334_t0iW_2720166.png" width="1116" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;體驗 Mureka Agent Studio：&lt;em&gt;www.mureka.ai&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/372591</link>
      <guid isPermaLink="false">https://www.oschina.net/news/372591</guid>
      <pubDate>Sun, 14 Sep 2025 10:24:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>騰訊：AI 能力全面開放，全面適配主流國產芯片</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;9 月 16 日，在 2025 騰訊全球數字生態大會主峯會上，騰訊公佈多項 AI 技術和產品最新進展，&lt;strong&gt;並宣佈通過騰訊雲全面開放騰訊 AI 落地能力及優勢場景&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0916/181313_pacO_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;面對各界關注的算力問題，騰訊集團副總裁、騰訊雲總裁邱躍鵬宣佈，目前騰訊雲已經全面適配主流的國產芯片，並積極參與和回饋開源社區。&lt;/p&gt; 
&lt;p&gt;與此同時，軟硬件協同全棧優化是騰訊雲的長期戰略投入，通過異構計算平台的軟件能力，整合不同類型的芯片對外提供高性價比的 Al 算力。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/372588</link>
      <guid isPermaLink="false">https://www.oschina.net/news/372588</guid>
      <pubDate>Sun, 14 Sep 2025 10:15:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>騰訊混元 3D 3.0 模型發佈</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;騰訊&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FXzJIt8glOd82pVs_YXjf6w" target="_blank"&gt;宣佈&lt;/a&gt;推出混元 3D 3.0 模型，其建模精度較前代提升 3 倍，可實現超高清 3D 內容創作，人物面部輪廓、紋理細節等呈現真人手辦級效果。該模型採用首創的 3D-DiT 分級雕刻技術，通過「先搭結構後雕細節」的邏輯，優化複雜幾何結構處理與紋理貼合度。&lt;/p&gt; 
&lt;p&gt;&lt;img height="607" src="https://static.oschina.net/uploads/space/2025/0916/180622_SG6I_2720166.jpg" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;混元 3D 3.0 已集成至混元 3D AI 創作引擎供用户免費使用，同步上線騰訊雲 API，為遊戲、影視、電商等行業提供專業 3D 內容生產能力。同期，支持七大核心製作環節的混元 3D Studio 專業工作台開啓內測，多條件控制的混元 3D Omni 模型亦將於近期開源。&lt;/p&gt; 
&lt;p&gt;數據顯示，騰訊混元 3D 系列模型在社區下載量已超 260 萬，拓竹科技、創想三維等 3D 打印廠商已應用該技術實現效率提升。&lt;/p&gt; 
&lt;p&gt;體驗：&lt;em&gt;https://3d.hunyuan.tencent.com/&lt;/em&gt;（混元 3D 創作引擎提供每日 20 次免費生成額度）&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/372585</link>
      <guid isPermaLink="false">https://www.oschina.net/news/372585</guid>
      <pubDate>Sun, 14 Sep 2025 10:06:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>MoonBit 正式加入 WebAssembly Component Model 官方文檔</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;p&gt;&lt;img src="https://picx.zhimg.com/80/v2-07327fbdaa6f7471b79521f6d668e7fd_720w.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;我們非常高興地宣佈， &lt;strong&gt;MoonBit 已正式收錄在 WebAssembly Component Model 的官方文檔中&lt;/strong&gt; 。這不僅是對 MoonBit 技術路線的一次肯定，也讓我們有機會和 Rust、Go、C# 等語言一起，出現在開發者查閲組件模型的入口頁面中。&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;一、&lt;/em&gt; &lt;strong&gt;關於 WebAssembly Component Model&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;WebAssembly Component Model 是 Wasm 生態正在推進的一個核心標準，它的目標是讓不同語言編寫的組件能夠無縫組合、運行和分發。這個模型提供了一套跨語言通用的接口規則，讓開發者 &lt;strong&gt;不需要關心組件背後的語言實現&lt;/strong&gt; ，就能把它們拼裝在一起，從而逐步成為 Wasm 世界的「通用接口層」。&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;二、&lt;/em&gt; &lt;strong&gt;關於 MoonBit&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;MoonBit 作為新一代 AI 原生編程語言，已經在 Wasm 後端上做了大量探索，包括 &lt;strong&gt;更小的代碼體積&lt;/strong&gt; 、 &lt;strong&gt;更高的運行效率&lt;/strong&gt; ，以及對 &lt;strong&gt;現代編譯器架構的支持&lt;/strong&gt; 。被收錄到 WebAssembly Component Model 文檔中，意味着語言以及工具鏈已經具備了進入跨語言互操作生態的能力，也得到了 Wasm 社區的認可。&lt;/p&gt; 
&lt;p&gt;接下來，我們會繼續投入在：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;多後端支持&lt;/strong&gt; ：Wasm、JavaScript、Native、RISC-V 等；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;組件模型集成&lt;/strong&gt; ：提供更完善的接口定義與跨語言調用能力；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;AI 驅動工具鏈&lt;/strong&gt; ：讓組件的構建、運行、調試過程更高效。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;MoonBit 相信：AI 時代需要新的編程語言和開發者平台，而我們正與全球開發者一起，共同建設這樣的未來。&lt;/p&gt; 
&lt;p&gt;感謝大家一路以來的陪伴與信任。讓我們繼續努力，把 MoonBit 打造成 AI 時代最具活力與創造力的編程平台！&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://picx.zhimg.com/80/v2-31065b175c3b993e841a5738ec875a7f_720w.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;👉 如果你也在關注 Wasm Component Model，歡迎嘗試用 MoonBit 來構建你的第一個組件，並和我們分享體驗！&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcomponent-model.bytecodealliance.org%2F" target="_blank"&gt;https://component-model.bytecodealliance.org/&lt;/a&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/7007853/blog/18692079</link>
      <guid isPermaLink="false">https://my.oschina.net/u/7007853/blog/18692079</guid>
      <pubDate>Sun, 14 Sep 2025 10:02:00 GMT</pubDate>
      <author>原創</author>
    </item>
    <item>
      <title>命令行 AI 編程工具 Codex CLI 已集成全新 GPT-5-Codex 模型</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;OpenAI 為旗下命令行工具 Codex CLI 發佈了&amp;nbsp;0.36.0，新版本集成了強大的&lt;a href="https://www.oschina.net/news/372453" target="_blank"&gt; GPT-5-Codex 模型&lt;/a&gt;，顯著提升了代碼生成速度、推理深度與質量。此次更新還引入了通過 AGENTS.md 文件精細控制其行為的能力。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0916/103155_65FN_2720166.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Codex CLI 是一個輕量級的 AI 編程助手，採用 TypeScript 和 Node.js 編寫，可以直接在用户的終端命令行運行，旨在充分發揮 AI 模型強大的推理能力，連接本地代碼環境，甚至支持處理截圖或草圖進行多模態編程。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Codex CLI 0.36.0 版本覆蓋了會話恢復、統一執行、安全與測試加固、簡化登錄流程、JSON-RPC 與 MCP 接口擴展，以及終端交互優化等多個方面。&lt;/p&gt; 
&lt;p&gt;此外，實驗性地引入了自動上下文壓縮、沙盒範圍擴展和 Azure Responses API 臨時解決方案。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/372581</link>
      <guid isPermaLink="false">https://www.oschina.net/news/372581</guid>
      <pubDate>Sun, 14 Sep 2025 09:52:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>2025 大模型服務性能排行榜：PPIO 吞吐測試排名第一</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#0e0e0e; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;清華大學攜手中國軟件評測中心聯合發佈了《2025 大模型服務性能排行榜》，&lt;strong&gt;PPIO 在 DeepSeek-R1-0528 的吞吐測試中排名第一&lt;/strong&gt;。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#0e0e0e; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;該榜單從延遲、吞吐、可靠性等關鍵指標切入，由專業團隊通過長週期、高頻率、多時段的數據評測，直觀呈現不同 MaaS 供應商的服務表現。而且，平台以匿名用户身份對 MaaS（Model as a Service）平台開展產品端到端的性能測評，從評測主體與流程上雙重保障了客觀公正性。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#0e0e0e; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;本次評測覆蓋多個代表性模型，包括 DeepSeek-R1-0528、DeepSeek-V3.1、Kimi-K2-Instruct 等。PPIO 在 20 餘家 MaaS 供應商中表現突出。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#0e0e0e; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;其中，在 DeepSeek-R1-0528 的吞吐測試中，PPIO 以&amp;nbsp;&lt;strong&gt;45.17 tokens/s&lt;/strong&gt;&amp;nbsp;的成績&lt;strong&gt;位列第一&lt;/strong&gt;。在 DeepSeek V3.1、Kimi-K2-Instruct 等模型測試中，PPIO 在吞吐與延遲性能上也取得了前五名的成績。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#0e0e0e; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;img height="422" src="https://oscimg.oschina.net/oscnet/up-c2218b9a340f30f9637b049f56c84eaa7a9.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#0e0e0e; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;strong&gt;吞吐 (Throughput)&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;與&lt;strong&gt;延遲 (Latency)&lt;/strong&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;與，是評測模型表現的兩個重要維度。吞吐（Throughput）衡量在單位時間內可處理的 Token 數量，高吞吐代表平台能支撐更多用户同時使用，尤其適合大規模應用場景；延遲（Latency）衡量用户從輸入到獲得首個回覆所需的時間，低延遲意味着更快的響應速度，直接影響用户體驗。&lt;/p&gt; 
&lt;p style="color:#0e0e0e; margin-left:0; margin-right:0; text-align:justify"&gt;PPIO 在這兩個維度都表現優秀，不僅能提供流暢的實時交互，也能在高併發和大規模任務處理中保持穩定。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/372579</link>
      <guid isPermaLink="false">https://www.oschina.net/news/372579</guid>
      <pubDate>Sun, 14 Sep 2025 09:40:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>上海 AI Lab 推出 Lumina-DiMOO</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;上海人工智能實驗室與多所知名高校合作近日推出了新一代多模態生成與理解模型 ——Lumina-DiMOO。該模型以 「全方位擴散大語言模型」 命名，旨在推動多模態 AI 技術的發展。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;Lumina-DiMOO 採用了創新的 「全離散擴散架構」，突破了傳統模型在文本與圖像處理上的侷限，提供了更為高效的解決方案。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="296" src="https://oscimg.oschina.net/oscnet/up-8e5b57a18c865ea2e56d87cfee874f973bb.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;多模態 AI 的核心在於如何將不同類型的數據有效整合。Lumina-DiMOO 通過將文本、圖像和音頻等數據映射到一個共享的高維 「語義空間」，使不同模態的數據能夠實現更好的理解和生成。這種方法的成功依賴於強大的對比學習技術，讓模型可以識別和對齊各類數據之間的關係。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在模型設計上，Lumina-DiMOO 的 「全離散擴散建模」 將所有數據視作可被逐步 「去噪」 和 「生成」 的對象。這種處理方式不僅簡化了模型結構，還顯著提升了生成質量和效率。與以往的多模態模型不同，Lumina-DiMOO 兼顧了速度與準確性，在圖像生成任務中只需少量步驟即可獲得高質量結果。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;此外，Lumina-DiMOO 在應用場景上具有廣泛的適用性。無論是文本到圖像生成、圖像理解，還是主題驅動生成，模型都能表現出色。同時，它還具備較強的圖像分析能力，能夠識別圖片中的細節和氛圍，為用户提供深入的理解。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/372572</link>
      <guid isPermaLink="false">https://www.oschina.net/news/372572</guid>
      <pubDate>Sun, 14 Sep 2025 09:11:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>DeepMind 發佈 VaultGemma 模型，具備差分隱私能力</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;谷歌 DeepMind 最近推出了一款名為 VaultGemma 的新型語言模型，這一創新的技術專注於用户隱私的保護。VaultGemma 不僅是開源的，而且是目前規模&lt;span&gt;最大&lt;/span&gt;的具備差分隱私能力的語言模型，參數數量達到了驚人的 10 億。這項技術的發佈，標誌着人工智能領域在保護用户數據隱私方面的重大進步。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;傳統的大語言模型在訓練過程中可能會不小心記住一些敏感信息，比如姓名、地址和機密文檔等。為了應對這一挑戰，VaultGemma 引入了差分隱私技術，通過在訓練過程中添加可控的隨機噪聲，確保模型的輸出無法與特定的訓練樣本關聯。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;這意味着，即使 VaultGemma 曾接觸過機密文件，從統計學上也無法還原其內容。谷歌的初步測試結果顯示，VaultGemma 確實沒有泄露或復現任何訓練數據，進一步提升了用户的信任感。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="257" src="https://oscimg.oschina.net/oscnet/up-279006f903387dbb56f6de856478ba712a4.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在技術架構方面，VaultGemma 是基於 Google 的 Gemma2 架構，採用了僅解碼器的 Transformer 設計，包含 26 層，並使用了多查詢注意力機制。一個關鍵的設計選擇是將序列長度限制為 1024 個 Token，這樣有助於管理私有訓練所需的高密集計算。開發團隊還藉助一種新穎的 「差分隱私縮放定律」，為計算能力、隱私預算和模型效用之間的平衡提供了框架。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;儘管 VaultGemma 的性能與五年前的普通語言模型相當，在生成能力上略顯保守，但它在保護隱私方面提供了更強的保障。谷歌的研究人員表示，他們將在 Hugging Face 和 Kaggle 上以開源許可證公開 VaultGemma 及其相關代碼庫，讓更多人能夠輕鬆訪問這一私有 AI 技術。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/372565</link>
      <guid isPermaLink="false">https://www.oschina.net/news/372565</guid>
      <pubDate>Sun, 14 Sep 2025 08:43:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>阿里通義實驗室發佈端到端語音識別大模型 FunAudio-ASR</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;阿里巴巴通義實驗室發佈了其端到端語音識別大模型 FunAudio-ASR。該模型通過創新的 Context 模塊，成功將高噪聲場景下的幻覺率從 78.5% 大幅降低至 10.7%，降幅接近 70%。&lt;/p&gt; 
&lt;p&gt;模型使用了數千萬小時的音頻數據進行訓練，並融合了大語言模型的語義能力，在遠場、嘈雜、多説話人等複雜條件下，其性能已超越 Seed-ASR、KimiAudio-8B 等主流系統。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0916/160315_13vU_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;FunAudio-ASR 提供了輕量化版本 FunAudio-ASR-nano，在保持較高識別準確率的同時，具備更低的推理成本，適合對資源敏感的部署環境。&lt;/p&gt; 
&lt;p&gt;兩個版本均支持低延遲流式識別、中英無縫切換以及用户自定義熱詞功能。目前，該技術已在釘釘的 「AI 聽記」、視頻會議以及 DingTalk A1 硬件中落地應用。其 API 也已在阿里雲百鍊平台上線。&lt;/p&gt; 
&lt;p&gt;體驗地址：&lt;em&gt;https://modelscope.cn/studios/iic/FunAudio-ASR&lt;/em&gt;&lt;br&gt; 技術報告：&lt;em&gt;https://github.com/FunAudioLLM/FunAudioLLM.github.io/blob/master/pdf/FunAudio-ASR.pdf&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/372559</link>
      <guid isPermaLink="false">https://www.oschina.net/news/372559</guid>
      <pubDate>Sun, 14 Sep 2025 08:05:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>喜報 | GPTBots 與極光推送雙雙榮獲「金靈光杯」重磅獎項！</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;近日，第二屆「金靈光杯」中國互聯網創新大賽頒獎儀式在雄安新區盛大舉行。&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;「&lt;strong&gt;金靈光杯」大賽由中國互聯網協會（&lt;/strong&gt;&lt;strong&gt;Internet Society of China&lt;/strong&gt;&lt;strong&gt;）主辦。作為由國內從事互聯網行業的網絡運營商、服務提供商、設備製造商、系統集成商以及科研、教育機構等&lt;/strong&gt;&lt;strong&gt;700&lt;/strong&gt;&lt;strong&gt;多家會員單位組成的全國性、行業性、非營利性社會組織，中國互聯網協會旨在搭建行業交流平台、推動產業健康發展，其主辦的評選活動在業內享有極高的權威性和公信力。&lt;/strong&gt;本屆大賽旨在發掘和表彰年度最具技術突破、商業價值和社會影響力的創新產品與企業，其評選結果被視為行業發展的風向標。&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;本屆大賽競爭尤為激烈，吸引了超過 1000 家單位申報，提交項目多達 1500 餘個。經過由多名院士及行業頂尖專家組成的評審委員會全方位、深層次的剖析與評估。&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;中國領先的客户互動和營銷科技服務商極光（Aurora Mobile，納斯達克股票代碼：JG），憑藉在人工智能和客户互動領域的持續創新與深厚積累，旗下兩大核心產品——企業級 AI 智能體平台 GPTBots 與市場領先的極光推送（JPush）在 2025 年（第二屆）「金靈光杯」中國互聯網創新大賽中表現突出。GPTBots 憑藉「AI 賦能寵物陪伴，打造寵物情感互動新模式」榮獲&lt;strong&gt;「&lt;/strong&gt;&lt;strong&gt;AI+&lt;/strong&gt;&lt;strong&gt;創新應用」專題賽二等獎&lt;/strong&gt;，極光推送（JPush）則在&lt;strong&gt;「信息通信安全專題賽—網絡和數據安全方向」中獲得優秀獎。&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:center"&gt;&lt;img src="https://oscimg.oschina.net/oscnet//266f8e7be3b4b936d588ab74642e04fb.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:center"&gt;&lt;img src="https://oscimg.oschina.net/oscnet//aaa91f493ef707c7bbd39b2b84bed578.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:center"&gt;&lt;img src="https://oscimg.oschina.net/oscnet//34933ad54e536970960f1848fa6cd935.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;此次獲獎，不僅彰顯了行業對極光技術實力與創新能力的高度認可，也再次印證了其在 AI 時代「成熟業務+創新業務」雙輪驅動戰略的巨大成功。&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;&lt;strong&gt;GPTBots&lt;/strong&gt;&lt;strong&gt;：&lt;/strong&gt;&lt;strong&gt;AI&lt;/strong&gt;&lt;strong&gt;落地應用的破局者，榮獲&lt;/strong&gt;&lt;strong&gt;「AI+&lt;/strong&gt;&lt;strong&gt;創新應用&lt;/strong&gt;&lt;strong&gt;」&lt;/strong&gt;&lt;strong&gt;專題賽二等獎&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;作為極光在 AIGC 時代的戰略級產品，GPTBots 自推出以來，便致力於解決企業「AI 落地難」的核心痛點。它是一個強大的企業級 AI 智能體平台，通過連接大型語言模型（LLM）與企業數據和服務，幫助企業無需複雜的代碼開發，即可快速構建專屬的 AI 專家團隊（Multi-Agent）。&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:center"&gt;&lt;img src="https://oscimg.oschina.net/oscnet//d25f1c3493cb27897cd9f196eebdacc4.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;&lt;strong&gt;在本次「金靈光杯」的評選中，評委會對&lt;/strong&gt;&lt;strong&gt;GPTBots&lt;/strong&gt;&lt;strong&gt;的以下幾點給予了高度評價：&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;&lt;strong&gt;前瞻的&lt;/strong&gt;&lt;strong&gt;Multi-Agent&lt;/strong&gt;&lt;strong&gt;架構：&lt;/strong&gt;GPTBots 允許企業根據業務需求，靈活構建由多個 AI 智能體組成的「虛擬團隊」，協同完成如市場分析、銷售線索挖掘、客户服務等複雜任務，極大提升了業務自動化水平和決策效率。&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;&lt;strong&gt;強大的企業級能力：&lt;/strong&gt;平台提供豐富的官方工具、插件和知識庫集成能力，支持私有化部署，確保了企業數據安全與業務流程的深度融合。&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;&lt;strong&gt;顯著的商業價值：&lt;/strong&gt;從提升 SEO 效率、降低客服成本，到優化金融風控策略，GPTBots 已在電商、金融、教育、醫療等多個行業展現出「降本-增效」的巨大潛力，為企業數字化轉型提供了強大的新引擎。&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:center"&gt;&lt;img src="https://oscimg.oschina.net/oscnet//a183b084c55a0c021632296267dbc386.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;&lt;strong&gt;極光推送（&lt;/strong&gt;&lt;strong&gt;JPush&lt;/strong&gt;&lt;strong&gt;）：十年行業基石，榮獲&lt;/strong&gt;&lt;strong&gt;「&lt;/strong&gt;&lt;strong&gt;信息通信安全專題賽&lt;/strong&gt;&lt;strong&gt;—&lt;/strong&gt;&lt;strong&gt;網絡和數據安全方向優秀獎&lt;/strong&gt;&lt;strong&gt;」&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;作為國內移動消息推送服務的開創者與市場份額遙遙領先的領導者，極光推送（JPush）此次再獲殊榮，充分證明瞭其在行業中不可動搖的標杆地位。成立十餘年來，極光推送（JPush）憑藉其穩定、高效、安全、全球化的服務，為數以百萬計的 App 提供了可靠的客户觸達能力。&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:center"&gt;&lt;img src="https://oscimg.oschina.net/oscnet//7f5bf5e9707fa367270f6df73a079092.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;在技術日新月異的今天，極光推送（JPush）始終保持創新，不僅在推送速度、到達率和穩定性上持續優化，更前瞻性地與 AI 技術結合，為開發者和運營者提供更智能、更精準的用户互動解決方案。&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;&lt;strong&gt;雙輪驅動：&lt;/strong&gt;&lt;strong&gt;AI&lt;/strong&gt;&lt;strong&gt;賦能，共築企業增長新未來&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;此次 GPTBots 與極光推送（JPush）的雙雙獲獎，是極光綜合實力的最好體現。這不僅是兩項獨立技術的勝利，更是兩大業務戰略協同的成果。&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;&lt;strong&gt;數據與場景的滋養：&lt;/strong&gt;極光推送（JPush）海量的客户互動數據和豐富的應用場景，為 GPTBots 的 AI 模型訓練和應用落地提供了最寶貴的「養料」。&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;&lt;strong&gt;智能與效率的賦能：&lt;/strong&gt;GPTBots 強大的 AI 能力，正反向賦能推送業務。例如，企業可通過構建「智能推送策略 Agent」，實現千人千面的精準營銷和自動化運營，極大提升用户參與度與商業轉化效果。&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;再次感謝「金靈光杯」大賽評委會及廣大客户與合作伙伴的認可。這份榮譽是里程碑，更是新起點。未來，極光將繼續深耕客户互動與人工智能領域，堅持以技術創新驅動產品進化，將 GPTBots 和極光推送打造為企業在數字化與智能化浪潮中最值得信賴的夥伴，助力千行百業實現可持續增長。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/372558</link>
      <guid isPermaLink="false">https://www.oschina.net/news/372558</guid>
      <pubDate>Sun, 14 Sep 2025 08:03:00 GMT</pubDate>
      <author>作者: 開源科技</author>
    </item>
    <item>
      <title>Bing 在搜索結果頁面添加 Edge 與 Chrome 的「記分牌」對比表</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;本週，當你在 Bing 上搜索「Chrome」時，會親眼見證微軟新版 Edge 廣告的有趣一幕——微軟專門製作了一張詳細的對比表，每當用户嘗試下載安裝 Chrome 時，該表會在 Bing 的顯眼處展示，直接將 Edge 和 Chrome 進行對比。&lt;/p&gt; 
&lt;p&gt;當用户在 Bing 上搜索「Chrome」，Bing 會提示「一切上網所需就在這裏」，顯然指的就是 Microsoft Edge。廣告下方的簡短描述寫道：「Microsoft Edge 基於與 Chrome 相同的技術，但擁有微軟的信任加持。」&lt;/p&gt; 
&lt;p&gt;事實上 Bing 過去就常常以橫幅推廣 Edge，但此次最吸引人注意的，是新增了「記分牌」對比表，明確將 Edge 列為「贏家」。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-fac1f1040d745a70ff32e06328e5979a8f8.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;廣告截圖顯示，該對比表卡片懸浮在搜索結果前方，左側藍色高亮的是 Edge 標誌，而 Chrome 則被「灰掉」。微軟用色彩和佈局突出強調：Edge 可獲得獎勵積分、內置 VPN、AI 個性化，且為「微軟推薦」，更安全，專為 Windows 11 設計。&lt;/p&gt; 
&lt;p&gt;表格中，每一項 Edge 都打了勾號，而 Chrome 全是叉號。&lt;/p&gt; 
&lt;p&gt;這一卡片出現在 Google Chrome 官方下載鏈接上方，用户首先看到的就是微軟的「推薦理由」，下方還有「瞭解更多 Edge 功能」按鈕，引導用户進一步瞭解 Edge。&lt;/p&gt; 
&lt;p&gt;如果用户繼續下滑並訪問 Google 官網，會遇到微軟 Edge 的彈窗廣告，提醒繼續使用 Edge。如果仍選擇無視微軟推薦、堅持下載安裝 Chrome，Google 網站頁面頂部還會出現 Edge 橫幅廣告，將 Chrome 下載選項下移。這是微軟最後一搏，試圖説服用户轉向 Edge。&lt;/p&gt; 
&lt;p&gt;值得注意的是，這並不是微軟首次用激進手段推薦自家產品，但此次方式堪稱更進一步。&lt;/p&gt; 
&lt;p&gt;不僅如此，早前在 Bing 上搜索 ChatGPT 或 Gemini 時也會彈出 Copilot 相關廣告。如今，微軟更是直接設計出「記分牌」，在用户搜索 Chrome 時宣稱 Edge 為「勝者」。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/372549</link>
      <guid isPermaLink="false">https://www.oschina.net/news/372549</guid>
      <pubDate>Sun, 14 Sep 2025 07:47:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>​Meta AI 發佈 MobileLLM-R1：輕量級邊緣推理模型</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;Meta AI 近日推出了 MobileLLM-R1，這是一系列輕量級邊緣推理模型，目前已在 Hugging Face 上發佈。該系列模型參數範圍從 140M 到 950M，專注於高效的數學、編碼和科學推理，且在不足 10 億的參數規模下實現了優秀的性能表現。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="365" src="https://oscimg.oschina.net/oscnet/up-30b7f801e9b13bb0046a53750505a0aa2ec.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;MobileLLM-R1 的&lt;span&gt;最大&lt;/span&gt;模型為 MobileLLM-R1-950M，採用了一系列架構優化設計:包括 22 層 Transformer 結構、24 個注意力頭和 6 個分組 KV 頭。模型的嵌入維度為 1536，隱藏層維度為 6144。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;此外，模型還採用了分組查詢注意力（GQA）來減少計算和內存需求，塊級權重共享技術降低了參數數量而不顯著增加延遲，SwiGLU 激活函數提升了小模型的表示能力。模型支持 4K 的上下文長度和 32K 的後訓練模型。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在訓練效率方面，MobileLLM-R1 的表現同樣引人注目。該模型總共在約 4.2 萬億個 token 上進行訓練，相較於 Qwen3 的 0.6B 模型訓練的 36 萬億 token，MobileLLM-R1 僅使用了約 11.7% 的數據便達到了或超越了 Qwen3 的準確率。同時，模型在數學、編碼和推理數據集上進行了監督微調，從而降低了訓練成本和資源需求。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在各項基準測試中，MobileLLM-R1-950M 的表現優異:在 MATH500 數據集上，其準確率比 OLMo-1.24B 高出約 5 倍，且比 SmolLM2-1.7B 高出約 2 倍。在 GSM8K、AIME 和 LiveCodeBench 等推理和編碼任務上，MobileLLM-R1 甚至與 Qwen3-0.6B 相匹配或超越，儘管所使用的 token 數量遠少於後者。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;不過，MobileLLM-R1 的聚焦也帶來了侷限性。雖然在數學、編碼和結構化推理方面表現強勁，但在一般對話、常識推理和創造性任務上，MobileLLM-R1 的表現較大型模型有所不足。此外，模型在生產環境中的使用受到 FAIR NC（非商業）許可證的限制，較長的上下文 (32K) 也提高了推理時的 KV 緩存和內存需求。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/372545</link>
      <guid isPermaLink="false">https://www.oschina.net/news/372545</guid>
      <pubDate>Sun, 14 Sep 2025 07:29:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>開發者必看：隱語框架的分層拆解和使用</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;blockquote&gt; 
 &lt;p&gt;打開鏈接點亮社區 Star，照亮技術的前進之路。 &lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-edf8393baa8c76890c2e321dca39a57a078.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;每一個點贊，都是社區技術大佬前進的動力&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Github 地址： &lt;em&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fsecretflow%2Fsecretflow" target="_blank"&gt;https://github.com/secretflow/secretflow&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-f1f5ac60c4e44a644832b0e9d03aac9f579.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h2_1"&gt;&lt;/span&gt; 
&lt;h2&gt;一、"隱語"架構設計全貌&lt;/h2&gt; 
&lt;span id="OSC_h3_2"&gt;&lt;/span&gt; 
&lt;h3&gt;1.隱語框架設計思想&lt;/h3&gt; 
&lt;p&gt;隱私計算是一個新興的跨學科領域，涉及&lt;strong&gt;密碼學、機器學習、數據庫、硬件&lt;/strong&gt;等多個領域。根據過去幾年的實踐經驗，我們發現&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;隱私計算技術方向多樣，&lt;/strong&gt;不同場景下有其各自更為合適的技術解決方案&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;隱私計算學習曲線很高&lt;/strong&gt;，非隱私計算背景的用户使用困難&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;隱私計算涉及領域眾多，&lt;/strong&gt;需要領域專家共同協作&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-66d2fb38d008b759553f3f5a5253b809349.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;隱語的設計目標&lt;/strong&gt;是使得數據科學家和機器學習開發者可以非常容易地使用隱私計算技術進行數據分析和機器學習建模，而無需瞭解底層技術細節。&lt;/p&gt; 
&lt;p&gt;為達到這個目標，**隱語提供了一層設備抽象，**將多方安全計算 (MPC)、同態加密 (HE) 和可信執行環境 (TEE) 等隱私計算技術抽象為密文設備， 將單方計算抽象為明文設備。&lt;/p&gt; 
&lt;p&gt;基於這層抽象，數據分析和機器學習工作流可以表示為一張計算圖，&lt;strong&gt;其中節點表示某個設備上的計算，邊表示設備之間的數據流動，不同類型設備之間的數據流動會自動進行協議轉換&lt;/strong&gt;。在這一點上，隱語借鑑了主流的深度學習框架，後者將神經網絡表示為一張由設備上的算子和設備間的張量流動構成的計算圖。&lt;/p&gt; 
&lt;p&gt;隱語框架圍繞開放這一核心思想，提供了不同層次的設計抽象，希望為不同類型的開發者都提供良好的開發體驗。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;在設備層，隱語提供了良好的設備接口和協議接口，支持更多的設備和協議插拔式的接入&lt;/strong&gt;，我們希望與密碼學、可信硬件、硬件加速等領域專家通力合作，不斷擴展密態計算的類型和功能，不斷提升協議的安全性和計算性能。&lt;/p&gt; 
&lt;p&gt;同時，隱語提供了良好的設備接口，第三方隱私計算協議可作為設備插拔式接入。&lt;strong&gt;在算法層，為機器學習提供了靈活的編程接口，算法開發者可以很容易定義自己的算法。&lt;/strong&gt;&lt;/p&gt; 
&lt;span id="OSC_h3_3"&gt;&lt;/span&gt; 
&lt;h3&gt;2.架構分層總覽&lt;/h3&gt; 
&lt;p&gt;隱語總體架構自底向上一共分為五層：&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-41a68e7f0ca01ba4db1273f9f2d109f05cd.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;資源管理層：&lt;/strong&gt; 主要承擔了兩方面的職責。第一是面向業務交付團隊，可以屏蔽不同機構底層基礎設施的差異，降低業務交付團隊的部署運維成本。另一方面，通過對不同機構的資源進行集中式管理，構建出一個高效協作的數據協同網絡。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;明密文計算設備與原語層：&lt;/strong&gt; 提供了統一的可編程設備抽象，將多方安全計算 (MPC)、同態加密 (HE)、可信硬件 (TEE) 等隱私計算技術抽象為密態設備，將單方本地計算抽象為明文設備。同時，提供了一些不適合作為設備抽象的基礎算法，如差分隱私 (DP)、安全聚合 (Secure Aggregation) 等。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;明密文混合調度層：&lt;/strong&gt; 提供了統一的設備調度抽象，將上層算法描述為一張有向無環圖，其中節點表示某個設備上的計算，邊表示設備之間的數據流動，即邏輯計算圖。邏輯計算圖由分佈式框架進一步拆分並調度至物理節點。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;AI &amp;amp; BI 隱私算法層：&lt;/strong&gt; 這一層的目的是屏蔽掉隱私計算技術細節，但保留隱私計算的概念，其目的是降低隱私計算算法的開發門檻，提升開發效率。&lt;/p&gt; 
&lt;p&gt;有隱私計算算法開發訴求的同學，可以根據自身場景和業務的特點，設計出一些特化的隱私計算算法，來滿足自身業務和場景對安全性、計算性能和計算精度的平衡。&lt;/p&gt; 
&lt;p&gt;在這一層上，隱語本身也會提供一些通用的算法能力，比如 MPC 的 LR/XGB/NN，聯邦學習算法，SQL 能力等。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;用户界面層：&lt;/strong&gt; 隱語的目標並不是做一個端到端的產品，而是為了讓不同的業務都能夠通過快速集成隱語而具備全面的隱私計算能力。因此我們會在最上層去提供一層比較薄的產品 API，以及一些 SDK，去降低業務方集成隱語的成本。&lt;/p&gt; 
&lt;span id="OSC_h3_4"&gt;&lt;/span&gt; 
&lt;h3&gt;&lt;strong&gt;3.架構細節拆解&lt;/strong&gt;&lt;/h3&gt; 
&lt;span id="OSC_h4_5"&gt;&lt;/span&gt; 
&lt;h4&gt;&lt;strong&gt;設備與原語層&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;隱語的設備分為物理設備和邏輯設備，其中，物理設備是隱私計算各個參與方的物理機器，邏輯設備則由一個或多個物理設備構成。&lt;/p&gt; 
&lt;p&gt;邏輯設備支持一組，特定的計算算子 (Device Ops)，有自己特定的數據表示 (Device Object)。&lt;strong&gt;邏輯設備分為明文和密文兩種類型，前者執行單方本地計算，後者執行多方參與的隱私計算。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;邏輯設備的運行時負責內存管理、數據傳輸、算子調度等職責，運行在一個或多個物理設備上。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;邏輯設備和物理設備不是一對一的關係，一個物理設備可能同時屬於多個邏輯設備&lt;/strong&gt;。在同一組物理設備上，可以根據不同的隱私協議和參與組合虛擬出不同的邏輯設備。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-814224da9f6ff77d2740d3b16167d0fbe4c.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;下表是隱語目前暫定支持的設備列表：&lt;/p&gt; 
&lt;table border="1" cellpadding="1" cellspacing="1" style="width:500px"&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;strong&gt;設備&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;strong&gt;類型&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;strong&gt;運行時&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;strong&gt;算子&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;strong&gt;協議&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;strong&gt;前端&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;strong&gt;狀態&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;strong&gt;PYU&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;明文&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;Python Interpreter&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;—&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;—&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;Python&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;Release&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;strong&gt;SPU&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;密文&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;SPU VM&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;PSI, XLA HLO&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;SPDZ-2k, ABY3&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;JAX, TensorFlow, PyTorch&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;Alpha&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;strong&gt;HEU&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;密文&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;HEU Runtime&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;Add, XLA HLO&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;Paillier, OU, TFHE&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;Numpy, JAX&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#333333"&gt;Alpha&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;strong&gt;TEE&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;密文&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;TEE Runtime&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;XLA HLO&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;Intel SGX&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;JAX, TensorFlow, PyTorch&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;WIP&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;span id="OSC_h4_6"&gt;&lt;/span&gt; 
&lt;h4&gt;&lt;strong&gt;可編程性&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;邏輯設備具備可編程性，即用户可以在設備上自定義計算邏輯，每個設備對用户提供了協議無關的編程接口。**在一個設備上，用户可以定義從簡單的矩陣運算， 到完整的深度模型訓練。**當然，這一切取決於設備提供的計算能力。&lt;/p&gt; 
&lt;p&gt;對於明文設備 PYU，它的前端為 python，用户可以通過&lt;code&gt;@device&lt;/code&gt;將一段預定義 python 函數調度至其上執行。&lt;/p&gt; 
&lt;p&gt;對於密文設備 SPU、HEU、TEE，它們的前端可以是任何支持 XLA 的框架， 如 JAX, TensorFlow,PyTorch 等。同樣的，用户也可以通過&lt;code&gt;@device&lt;/code&gt;將基於這些前端自定義的函數調度至指定的設備執行。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;import jax.numpy as jnp

dev = Device()  # maybe PYU, SPU, HEU, TEE


@device(dev)
def selu(x, alpha=1.67, lmbda=1.05):
    return lmbda * jnp.where(x &amp;gt; 0, x, alpha * jnp.exp(x) - alpha)


res = selu(x)  # res is a DeviceObject
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;用户自定義函數首先轉換成&lt;strong&gt;XLA HLO Computation&lt;/strong&gt;，由 XLA 進行設備無關的代碼優化和分析，併發往後端設備。後端設備進一步執行代碼優化和分析，並生成最終，的可執行代碼。&lt;/p&gt; 
&lt;p&gt;可執行代碼或由設備的虛擬機解釋執行 (SPU, HEU)，或由硬件直接執行 (TEE)。使用 XLA HLO 作為 IR，使得我們可以複用 XLA 前端和設備無關，代碼優化，同時使得後端實現更加簡潔乾淨。&lt;/p&gt; 
&lt;p&gt;對於密文設備（半同態）HEU，它僅支持一組有限的計算，因此提供了一組預定義算子如&lt;code&gt;__add__&lt;/code&gt;,&lt;code&gt;__mul__&lt;/code&gt;等，用户不能通過&lt;code&gt;@device&lt;/code&gt;進行自定義編程。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;x, y = HEUObject(), PYUObject()
z = x + y  # add
z = x * y  # mul
z = x @ y  # matmul
&lt;/code&gt;&lt;/pre&gt; 
&lt;span id="OSC_h4_7"&gt;&lt;/span&gt; 
&lt;h4&gt;&lt;strong&gt;協議轉換&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;用户在邏輯設備上進行編程，構建邏輯計算圖，**其節點表示設備上的一段函數或算子，邊表示設備對象的流動。**邏輯計算圖被設備進一步分割為子圖，兩個子圖間的，邊表示跨設備的對象流動，此時需要進行協議轉換。設備對象的&lt;code&gt;DeviceObject.to&lt;/code&gt;接口用於轉換至目標設備對象，任何新增的設備都應該提供相應的轉換函數並，插入對象轉換表中。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;下表是各個邏輯設備對象的轉換表：&lt;/strong&gt;&lt;/p&gt; 
&lt;table border="1" cellpadding="1" cellspacing="1" style="width:500px"&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&amp;nbsp;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;PYU&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;SPU&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;HEU&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;TEE&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;PYU&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&amp;nbsp;&lt;/td&gt; 
   &lt;td&gt;share&lt;/td&gt; 
   &lt;td&gt;encrypt&lt;/td&gt; 
   &lt;td&gt;encrypt&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;SPU&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;reconstruct&lt;/td&gt; 
   &lt;td&gt;&amp;nbsp;&lt;/td&gt; 
   &lt;td&gt;encrypt+add&lt;/td&gt; 
   &lt;td&gt;reconstruct+encrypt&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;HEU&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;decrypt&lt;/td&gt; 
   &lt;td&gt;minus+decrypt&lt;/td&gt; 
   &lt;td&gt;&amp;nbsp;&lt;/td&gt; 
   &lt;td&gt;decrypt+encrypt&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;TEE&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;decrypt&lt;/td&gt; 
   &lt;td&gt;decrypt+share&lt;/td&gt; 
   &lt;td&gt;decrypt+encrypt&lt;/td&gt; 
   &lt;td&gt;&amp;nbsp;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;span id="OSC_h4_8"&gt;&lt;/span&gt; 
&lt;h4&gt;&lt;strong&gt;分佈式引擎&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;用户基於設備構建了一張邏輯計算圖，那麼我們如何執行這張計算圖？由於邏輯設備映射到一個或多個物理設備，&lt;strong&gt;因此我們需要將邏輯設備上的算子正確調度到其對應的物理設備，同時處理好這些物理設備間的數據傳輸關係&lt;/strong&gt;。毫無疑問，我們需要一個分佈式圖執行引擎來解決這些問題。&lt;/p&gt; 
&lt;p&gt;那麼我們需要一個怎樣的分佈式圖執行引擎？以下是隱語對它的要求&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;細粒度的異構計算：在一張邏輯計算圖中，具有不同粒度的計算任務，既有簡單的數據處理（秒級），也有複雜的多方訓練（幾個小時至幾十小時）。同時，物理節點具有不同的硬件環境，CPU, GPU, TEE, FPGA 等。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;靈活的計算模型：在水平、垂直場景下，針對數據處理和模型訓練等不同工作流，支持多種並行模型，如數據並行、模型並行、混合並行。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;動態執行：在聯邦學習場景下，不同機構的數據規模、帶寬延遲、機器性能可能有較大差異，這導致同步模式的效率受限於最慢的工作節點。因此，我們希望支持，異步訓練模式，這要求圖執行引擎具有動態執行能力。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-2c0fb0a2006a01dc6e88397c7510e42537d.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;隱語針對隱私計算場景，已經對框架進行了一些安全加固工作：&lt;strong&gt;通過身份認證、代碼預裝、代碼存證等手段對框架做了整體加固&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;未來，還將探索沙箱隔離、訪問控制、靜態圖等機制以進一步提升安全水位。在環境適配方面，為了適配跨機構網絡通信的特點，推進了 GCS gRPC 通信、域名支持、弱網斷線處理等相關功能的開發。&lt;/p&gt; 
&lt;span id="OSC_h4_9"&gt;&lt;/span&gt; 
&lt;h4&gt;&lt;strong&gt;AI &amp;amp; BI 隱私算法&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;這一層的目的是其目的是降低隱私計算算法的開發門檻，提升開發效率。有隱私計算算法開發訴求的同學，可以根據自身場景和業務的特點，設計出一些特化的隱私計算算法，來&lt;strong&gt;滿足自身業務和場景對安全性、計算性能和計算精度的平衡&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;在這一層上，隱語本身也會提供一些通用的算法能力，比如 MPC 的 LR/XGB/NN，聯邦學習算法，SQL 能力等。&lt;/p&gt; 
&lt;span id="OSC_h2_10"&gt;&lt;/span&gt; 
&lt;h2&gt;二、"隱語"框架的使用&lt;/h2&gt; 
&lt;p&gt;使用隱語構建隱私計算算法 &lt;strong&gt;邏輯設備抽象為算法開發者提供了極大的靈活性，&lt;/strong&gt; 他們可以像積木一樣自由組合這些設備，在設備上自定義計算，從而構建自己的隱私計算算法。&lt;/p&gt; 
&lt;p&gt;接下來，我們通過一個具體的算法來展示隱語框架的通用編程能力。聯邦學習算法聯邦機器學習又名聯邦學習、聯合學習、聯盟學習，是一種機器學習框架，能有效幫助多個機構在滿足用户隱私保護、數據安全和政府法規的要求下，進行數據使用和機器學習建模。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-ac805372dc1d72a4712d5e495a399b0eba4.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;聯邦學習的算法流程如上圖所示，大致分為以下四個步驟：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;機構節點在本地進行多輪訓練，得到模型參數&lt;/li&gt; 
 &lt;li&gt;機構節點使用加密協議，將模型參數上傳至聚合節點&lt;/li&gt; 
 &lt;li&gt;聚合節點使用加密協議，對模型參數進行聚合，得到全局模型&lt;/li&gt; 
 &lt;li&gt;機構節點從聚合節點獲取最新的全局模型，進入下一輪訓練&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;節點本地訓練&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;機構節點運行在機構本地，隱語提供了一個邏輯設備&lt;code&gt;PYU&lt;/code&gt;，執行本地的明文計算。&lt;/p&gt; 
&lt;p&gt;下面的&lt;code&gt;BaseTFModel&lt;/code&gt;定義了本地模型訓練邏輯，用户可以選擇自己喜好的機器學習框架，如&lt;code&gt;TensorFlow, PyTorch&lt;/code&gt;等。&lt;/p&gt; 
&lt;p&gt;隱語提供了&lt;code&gt;@proxy&lt;/code&gt;裝飾器，對一個普通的類進行了初始設置，以便後續在邏輯設備上對其實例化。&lt;code&gt;@proxy(PYUObject)&lt;/code&gt;表明該類需要在 PYU 設備上實例化。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@proxy(PYUObject)
class BaseTFModel:
    def train_step(self, weights, cur_steps, train_steps) -&amp;gt; Tuple[np.ndarray, int]:
    self.model.set_weights(weights)
        num_sample = 0
        for _ in range(train_steps):
            x, y = next(self.train_set)
            num_sample += x.shape[0]
            self.model.fit(x, y)

        return self.model.get_weights(), num_sample
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;模型安全聚合&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;模型聚合對各個機構節點的模型參數進行加權平均，如下面&lt;code&gt;_average&lt;/code&gt;所示。隱語邏輯設備的最大特點在於可編程性，用户可以將一段函數調度到多種設備執行，以達到使用不同隱私計算技術的目的。&lt;/p&gt; 
&lt;p&gt;目前，&lt;code&gt;DeviceAggregator&lt;/code&gt;可以支持 PYU 明文聚合，也可以支持 SPU MPC 協議聚合，後續我們還將支持 TEE, HEU 等多種密文設備。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@dataclass
class DeviceAggregator(Aggregator):
    device: Union[PYU, SPU]

    def average(self, data: List[DeviceObject], axis=0, weights=None):
        # 2. 機構節點使用加密協議，將模型參數上傳至聚合節點
        data = [d.to(self.device) for d in data]
        if isinstance(weights, (list, tuple)):
            weights = [w.to(self.device) if isinstance(w, DeviceObject) else w for w in weights]

        def _average(data, axis, weights):
            return [jnp.average(element, axis=axis, weights=weights) for element in zip(*data)]

        # 3. 聚合節點使用加密協議，對模型參數進行聚合，得到全局模型
        return self.device(_average, static_argnames='axis')(data, axis=axis, weights=weights)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;訓練流程整合&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;有了節點本地訓練、模型安全聚合，我們就可以將其整合起來形成完整的訓練流程。首先，我們在每個 PYU 設備（代表機構節點）創建 BaseTFModel 實例。&lt;/p&gt; 
&lt;p&gt;同時，初始化聚合器，可以是 PYU, SPU, TEE, Secure Aggregation。然後，按照上述描述的聯邦學習算法流程進行迭代訓練。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;class FedTFModel:
    def __init__(self, device_list: List[PYU] = [], model: Callable[[], tf.keras.Model] = None, aggregator=None):
        # 在每個機構節點 (PYU) 創建一個 BaseTFModel 實例
        self._workers = {device: BaseTFModel(
            model, device=device) for device in device_list}
        # 聚合器，可以是 PYU, SPUPPU, TEE, Secure Aggregation
    self._aggregator = aggregator

    def fit(self, x: Union[HDataFrame, FedNdarray], y: Union[HDataFrame, FedNdarray], batch_size=32, epochs=1, verbose='auto',
            callbacks=None, validation_data=None, shuffle=True,
            class_weight=None, sample_weight=None, validation_freq=1, aggregate_freq=1):
    self.handle_data(train_x, train_y, batch_size=batch_size,
                     shuffle=shuffle, epochs=epochs)

    # 初始化模型參數
    current_weights = {
        device: worker.get_weights() for device, worker in self._workers.items()}

    for epoch in range(epochs):
        for step in range(0, self.steps_per_epoch, aggregate_freq):
            weights, sample_nums = [], []
            for device, worker in self._workers.items():
                # 1. 機構節點在本地進行多輪訓練，得到模型參數
                weight, sample_num = worker.train_step(current_weights[device], epoch*self.steps_per_epoch+step, aggregate_freq)
                weights.append(weight)
                sample_nums.append(sample_num)
            # 模型參數聚合，可以是：PYU, SPU, TEE, Secure Aggregation
            current_weight = self._aggregator.average(
                weights, weights=sample_nums)
            # 4. 機構節點從聚合節點獲取最新的全局模型，進入下一輪訓練
            current_weights = {device: current_weight.to(device) for device, worker in self._workers.items()}
&lt;/code&gt;&lt;/pre&gt; 
&lt;span id="OSC_h4_11"&gt;&lt;/span&gt; 
&lt;h4&gt;&lt;strong&gt;更多算法：&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;通過以上聯邦學習算法的例子，我們展示了隱語作為隱私計算框架的可編程性、可擴展性。&lt;/p&gt; 
&lt;p&gt;期待您基於隱語探索更多&lt;strong&gt;有趣的用法&lt;/strong&gt;！更多詳情請參考我們的教程和實現。&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fsecretflow.readthedocs.io%2Fzh%2Flatest%2Ftutorial%2Findex.html%23" target="_blank"&gt;https://secretflow.readthedocs.io/zh/latest/tutorial/index.html#&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;在 SPU 進行 PSI 對齊，邏輯迴歸、神經網絡訓練&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;使用 SPU HEU 的組合構建 HESS-LR, HESS-XGB 算法&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;橫向聯邦學習，在 PYU 進行本地訓練，使用 SPU、TEE、Secure Aggregation 進行梯度、權重聚合&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;縱向拆分學習，將一個模型拆分至多個 PYU，使用 PYU 聚合隱層，使用差分隱私保護前向隱層和反向梯度&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-edf8393baa8c76890c2e321dca39a57a078.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;打開鏈接即可點亮社區 Star，照亮技術的前進之路。&lt;/p&gt; 
&lt;p&gt;Github 地址：&lt;em&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fsecretflow%2Fsecretflow" target="_blank"&gt;https://github.com/secretflow/secretflow&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;本文由隱語社區統一發布，歡迎大家點 &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fsecretflow%2Fsecretflow" target="_blank"&gt;Star&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/5915128/blog/18691893</link>
      <guid isPermaLink="false">https://my.oschina.net/u/5915128/blog/18691893</guid>
      <pubDate>Sun, 14 Sep 2025 07:10:00 GMT</pubDate>
      <author>原創</author>
    </item>
    <item>
      <title>OpenAI 發佈最大規模 ChatGPT 普通用户使用報告</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;OpenAI 經濟研究團隊與哈佛大學經濟學家 David Deming 合作完成了名為&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopenai.com%2Findex%2Fhow-people-are-using-chatgpt%2F" target="_blank"&gt;&lt;em&gt;《How people are using ChatGPT》&lt;/em&gt;&lt;/a&gt;的研究報告，據稱這是有史以來最大規模的一次關於普通用户（consumer users）如何使用 ChatGPT 的調查。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0916/151027_d0un_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;樣本包括 150 萬條對話，結合了 ChatGPT 在 2025 年中期的用户活動數據（當時每週有約 7 億活躍用户）。&lt;/p&gt; 
&lt;p&gt;下面是該研究的一些發現：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;誰在使用 ChatGPT？&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;性別差距正在縮小。早期用户中，名字可分辨為「女性傾向」的佔比曾比現在少；到了 2025 年 7 月，這一比例上升到超過一半。&lt;/li&gt; 
 &lt;li&gt;在低收入和中等收入國家的採用率增長尤其快。到 2025 年 5 月，最低收入國家的增長率是最高收入國家的 4 倍以上。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;人們用 ChatGPT 做什麼？&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;大部分對話是關於處理日常任務（practical guidance）、獲取信息（seeking information）和寫作（writing）。寫作是工作中最常見的任務類型。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;互動類型可以分為三類：「Asking」（提問）、「Doing」（執行任務／創造輸出）、「Expressing」（表達）。在這些中：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;「Asking」（約 49%）— 用户尋求建議、信息等；&lt;/li&gt; 
   &lt;li&gt;「Doing」（約 40%）— 包括起草文本、策劃、編程等實作型任務，其中約三分之一是用於工作用途。&lt;/li&gt; 
   &lt;li&gt;「Expressing」（約 11%）— 包括情感表達、反思、娛樂或個人探索等。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;使用形式與演變？&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;大約 30% 的使用與工作有關，約 70% 與非工作（個人生活）有關。兩者都在增長，説明 ChatGPT 不僅是生產力工具，也在個人日常生活中創造價值。&lt;/li&gt; 
 &lt;li&gt;一個關鍵的價值是決策支持（decision support）：ChatGPT 在知識密集型職業中能夠幫助改善判斷和效率。&lt;/li&gt; 
 &lt;li&gt;使用隨着時間加深：隨着模型能力的提升和用户探索出新的用途，人們使用頻次與深入程度都在上升。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;研究發現，ChatGPT 的用户基礎越來越廣泛，使用者的社會／經濟背景差距在縮小。 它不僅在工作場景中提高效率，也在日常生活中提供幫助；因此其經濟價值不僅體現在對 GDP 的直接貢獻，也體現在人們日常生活中未被傳統經濟統計完全捕捉的價值。 OpenAI 從中得出的理念是，人工智能的訪問應當被視為基本權利，讓更多人能利用它釋放潛力，塑造自己的未來。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/372540/how-people-are-using-chatgpt</link>
      <guid isPermaLink="false">https://www.oschina.net/news/372540/how-people-are-using-chatgpt</guid>
      <pubDate>Sun, 14 Sep 2025 07:08:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>騰訊跨端開源框架 Kuikly 適配「液態玻璃」</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Kuikly 是基於 Kotlin Multiplatform 的 UI 與邏輯全面跨端綜合解決方案，由騰訊大前端領域 Oteam（公司級）推出，旨在提供一套一碼多端、極致易用、動態靈活的全平台高性能開發框架。&lt;/p&gt; 
&lt;p&gt;目前支持如下平台：Android、iOS、鴻蒙、Web（beta）和小程序（beta）。&lt;/p&gt; 
&lt;p&gt;Kuikly 團隊介紹稱，項目已完成對「液態玻璃」的首階段適配，並對外開源發佈。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0916/143023_KFsf_2720166.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="640" src="https://static.oschina.net/uploads/space/2025/0916/143104_iaPI_2720166.jpg" width="295" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;為了適配「液態玻璃」，Kuikly 沒有引入新的獨立組件，而是為現有組件提供了簡潔的視圖屬性擴展。例如，開發者只需通過一行&amp;nbsp;&lt;code&gt;glassEffectIOS()&lt;/code&gt;代碼，即可為任意容器視圖啓用液態玻璃效果。&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-javascript"&gt;View {
    attr {
        glassEffectIOS() // iOS 平台將自動添加液態玻璃效果
    }
    // ... 其他子視圖
}&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;團隊表示，Kuikly 的適配工作並非簡單的 UI 改造，而是充分利用原生提供的基礎能力，在框架渲染層和 DSL 驅動層兩方面進行擴展，旨在為開發者提供一套便捷、低成本的適配方案。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/372524</link>
      <guid isPermaLink="false">https://www.oschina.net/news/372524</guid>
      <pubDate>Sun, 14 Sep 2025 06:32:00 GMT</pubDate>
      <author>來源: OSCHINA</author>
    </item>
    <item>
      <title>邏輯智能開源語音大模型框架 LLaSO</title>
      <description/>
      <link>https://www.oschina.net/news/372521</link>
      <guid isPermaLink="false">https://www.oschina.net/news/372521</guid>
      <pubDate>Sun, 14 Sep 2025 06:26:00 GMT</pubDate>
    </item>
  </channel>
</rss>
