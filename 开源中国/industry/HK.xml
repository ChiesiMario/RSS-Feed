<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>開源中國-綜合資訊</title>
        <link>https://www.oschina.net/news/industry</link>
        <atom:link href="http://8.134.148.166:30044/oschina/news/industry" rel="self" type="application/rss+xml"></atom:link>
        <description>開源中國-綜合資訊 - Powered by RSSHub</description>
        <generator>RSSHub</generator>
        <webMaster>contact@rsshub.app (RSSHub)</webMaster>
        <language>en</language>
        <lastBuildDate>Wed, 02 Apr 2025 07:37:05 GMT</lastBuildDate>
        <ttl>5</ttl>
        <item>
            <title>蘋果六面屏全玻璃設計 iPhone 專利曝光</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;科技媒體 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.patentlyapple.com%2F2025%2F04%2Fapple-won-a-patent-for-all-glass-device-embodiments-for-apple-watch-mac-an-iphone-that-allows-imagery-on-both-front-and-ba.html&quot; target=&quot;_blank&quot;&gt;Patently Apple&lt;/a&gt; 發文稱，蘋果公司最近新獲得了一項適用於 Apple Watch、Mac 和 iPhone 的全玻璃設備專利，正面和背面均可作為顯示屏。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;蘋果在玻璃設備領域的佈局可以追溯到 2014 年，此次獲得的專利具有里程碑意義，首次系統地定義了多面玻璃外殼的技術方案。其他專利可以查看：&lt;/span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.patentlyapple.com%2F2023%2F03%2Fapple-wins-a-patent-for-fused-glass-device-housings-for-ipad-an-apple-tv-imac-and-more.html&quot; target=&quot;_blank&quot;&gt;01&lt;/a&gt;&lt;span style=&quot;color:#000000&quot;&gt;、&lt;/span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.patentlyapple.com%2Fpatently-apple%2F2020%2F09%2Fapple-won-two-macbook-patents-emphasizing-the-expanded-use-of-a-glass-body-keyboard-areas.html&quot; target=&quot;_blank&quot;&gt;02&lt;/a&gt;&lt;span style=&quot;color:#000000&quot;&gt;、&lt;/span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.patentlyapple.com%2F2025%2F03%2Fapple-files-for-additional-protections-to-their-inventive-all-glass-imac-patent.html&quot; target=&quot;_blank&quot;&gt;03&lt;/a&gt;&lt;span style=&quot;color:#000000&quot;&gt;、&lt;/span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.patentlyapple.com%2F2025%2F03%2Fapple-has-invented-a-new-apple-watch-design-with-a-glass-shell-that-provides-side-touch-controls-with-various-control-interf.html&quot; target=&quot;_blank&quot;&gt;04&lt;/a&gt;&lt;span style=&quot;background-color:#ffffff&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;&amp;nbsp;(用於 Apple Watch)。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;strong&gt;最新專利附圖如下：&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;圖 26A-26C 展示了一種六面透明稜鏡外殼，每一面都可以獨立顯示內容。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;355&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-eeee8a0fbe11838830bc1709d0ca95e2057.jpg&quot; width=&quot;300&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;圖&amp;nbsp;53C-B 展示了當手機翻轉時，用户界面根據確定的方向（相對於用户或絕對方向）重新定位後的 iPhone。51A-51B 描繪了 iPhone 的另一個示例，其中正面顯示屏上的圖像擴展到了玻璃 iPhone 的背面。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;389&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-d4a79e481179558d03b335c501ed6f62198.jpg&quot; width=&quot;300&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;圖 4E 是 Mac Pro Tower 概念的透視圖，其外殼呈八稜柱形狀。外殼可以由玻璃製成，並且可以沿所有或基本上所有表面透明。圖 57 則展示了未來可能出現的 Apple Watch，其中包括完全或基本完全由玻璃形成的外殼。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;378&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-bd9c6d6d406781d60a5e5649ff261ae0a18.jpg&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;蘋果方面指出，在某些情況下，iPhone 的側面將能夠根據基於力量的輸入而變形和/或偏轉。例如，用户可以通過擠壓外殼和/或按壓側面來降低或提高音樂或內容的音量。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/342450/apple-patent-for-all-glass-device</link>
            <guid isPermaLink="false">https://www.oschina.net/news/342450/apple-patent-for-all-glass-device</guid>
            <pubDate>Wed, 02 Apr 2025 07:34:01 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>軟件工程的 13 條法則</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;&lt;strong&gt;1、帕金森定律&lt;/strong&gt;：工作會膨脹以填滿可用的時間。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-b6e9c01e7725c6b53b7800033fffdaf1f3f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2、霍夫斯塔特定律&lt;/strong&gt;：事情總是比你預期的要長，即使你已經考慮了霍夫斯塔特定律。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-da20eb6f35d695eb16fe4ab9caf02c0853f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;3、布魯克斯定律&lt;/strong&gt;：向一個已經延期的軟件項目增加人力只會讓它更加延期。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-a79493a976c16e52b6258ff68e4a4e02d15.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;4、康威定律（及逆康威定律）&lt;/strong&gt;：組織做的設計往往是其內部溝通結構的複製品。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-5adf2d562a47cf3c6774f10f54aa750762f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;5、坎寧安定律&lt;/strong&gt;：在互聯網上獲得正確答案的最佳方式不是提問，而是發佈一個錯誤答案。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-efe2126647446248be7d4dabb9ac5d836d4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;6、斯特金定律&lt;/strong&gt;：90% 的東西都是垃圾。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-e6656de38fcc5ded0050bdf5338d5cc75db.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;7、扎温斯基定律&lt;/strong&gt;：每個程序都試圖擴展，直到能夠讀取郵件。那些無法如此擴展的程序會被能夠做到的程序所取代。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-f46d91cd7b15ab18aa92eaa1a48735af5a2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;8、海勒姆定律&lt;/strong&gt;：當 API 的用户數量足夠多時，你在合約中承諾什麼並不重要：系統的所有可觀察行為都會被某些人所依賴。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-8a490e790d81821fcbd5d0d918259ed53a4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;9、普賴斯定律&lt;/strong&gt;：在任何羣體中，50% 的工作是由其總人數的平方根數的人完成的。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-1bf79eb2323574b5fde92c83b8359da2890.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;10、林格爾曼效應&lt;/strong&gt;：羣體中個體成員的生產力隨着羣體規模的增大而逐漸降低的趨勢。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-8a7f61f02eacee4f997dc6f1bc203adbda8.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;11、古德哈特定律&lt;/strong&gt;：當一項指標成為目標時，它就不再是一個好的指標。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-072a453ebe8a6968cb3d158389ec9b00ceb.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;12、吉爾布定律&lt;/strong&gt;：任何你需要量化的東西，都可以通過某種方式進行測量，這總比完全不測量要好。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-5bce1dfd39463accf41c3d4b96efc2c72e5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;13、墨菲定律&lt;/strong&gt;：可能出錯的事就一定會出錯。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-717d819db99a5b3d2d98fd38a349c465f44.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;來源：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnewsletter.manager.dev%2Fp%2Fthe-13-software-engineering-laws&quot; target=&quot;_blank&quot;&gt;https://newsletter.manager.dev/p/the-13-software-engineering-laws&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/342448/the-13-software-engineering-laws</link>
            <guid isPermaLink="false">https://www.oschina.net/news/342448/the-13-software-engineering-laws</guid>
            <pubDate>Wed, 02 Apr 2025 07:23:01 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>「DeepSeek-V3 技術解析」：無輔助損失函數的負載均衡</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                                                                                                                                        &lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;編者按：&lt;/strong&gt; 在混合專家模型（MoE）的實踐中，負載不均衡儼然已成為制約模型性能提升的關鍵瓶頸之一。傳統的均衡策略往往需要引入複雜的輔助損失函數，不僅增加了訓練的複雜度，還可能幹擾模型的核心學習目標。工程師們在提升模型效率的道路上，一直苦苦追尋着一個優雅而高效的平衡解決方案。&lt;/p&gt; 
 &lt;p&gt;DeepSeek 團隊的這項研究，為這一長期困擾業界的技術難題提供了令人耳目一新的解決思路：通過在門控分數中直接添加專家層面的偏置項，在絕大部分不引入額外損失函數的情況下，實現了模型訓練過程中的自適應負載均衡。更令人驚歎的是，這一方法不僅保持了模型的因果關係，還顯著提升了訓練的穩定性和最終性能。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;作者 | Shirley Li&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;編譯 | 嶽揚&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;這是 DeepSeek-V3 系列文章的第三篇，我們將探討 DeepSeek 模型[1, 2, 3]中與混合專家模型（MoE）相關的另一項關鍵架構突破：無輔助損失函數的負載均衡（Auxiliary-Loss-Free Load Balancing）[5]。&lt;/p&gt; 
&lt;p&gt;在本文，我們將深入解析 DeepSeek 如何解決 MoE 的隱藏瓶頸------負載均衡，同時還通過消除梯度幹擾和嚴格遵循因果關係約束，提升了訓練和推理效率，為後續專家模型的優化方向提供了標杆。&lt;/p&gt; 
&lt;p&gt;本文目錄：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;技術背景：介紹混合專家模型（MoE）的基本原理，解釋負載均衡的重要性，回顧之前的工作，包括輔助損失函數（auxiliary loss）方法和專家選擇（Expert Choice）策略。&lt;/li&gt; 
 &lt;li&gt;DeepSeek 的無輔助損失函數的負載均衡：解析其運作原理&lt;/li&gt; 
 &lt;li&gt;性能評估：討論無輔助損失函數的負載均衡的性能表現&lt;/li&gt; 
 &lt;li&gt;總結&lt;/li&gt; 
 &lt;li&gt;參考文獻&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;「DeepSeek-V3 技術解析」系列其他文章：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://my.oschina.net/IDP/blog/17943880&quot;&gt;「DeepSeek-V3 技術解析」：多頭潛在注意力機制（MLA）&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://my.oschina.net/IDP/blog/17961367&quot;&gt;「DeepSeek-V3 技術解析」：DeepSeekMoE&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;&lt;strong&gt;01 技術背景&lt;/strong&gt;&lt;/h1&gt; 
&lt;h2&gt;&lt;strong&gt;1.1&lt;/strong&gt; &lt;strong&gt;MoE (Mixture-of-Experts) in Transformers&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;MoE（Mixture-of-Experts，混合專家模型）在 Transformer 中的實現方式通常為：每隔若干 Transformer 層，將其中的 FFN（前饋網絡）替換為多個 FFN（每個 FFN 充當一個&quot;專家&quot;）。當 input token 進入該層時，通過門控操作（Gating）選擇 Top-K 個專家，並將 input token 只路由至這些被選中的 FFN，從而僅激活對應的專家網絡。&lt;/p&gt; 
&lt;p&gt;下圖展示了這一過程：左側標準 Transformer 層中的 FFN 子層被替換為右側的 MoE 層。&lt;/p&gt; 
&lt;p&gt;如需更詳細的 MoE 技術解析，可參考&lt;a href=&quot;https://my.oschina.net/IDP/blog/17961367&quot;&gt;「DeepSeek-V3 技術解析」：DeepSeekMoE&lt;/a&gt;（文中通過餐廳類比直觀解釋了 MoE 原理）。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-19ae4d0c8392f0c6fb1b110f9ebae146cb4.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;圖 1. Transformer 中的 MoE 層（紅框內）示意圖。圖片改編自文獻 [6]。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;1.2 負載均衡及其重要性&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;在&lt;a href=&quot;https://my.oschina.net/IDP/blog/17961367&quot;&gt;本系列上一篇文章&lt;/a&gt;進行的餐廳類比中，我們通過一個能夠提供多種菜系菜品的餐廳解釋了 MoE 的概念：餐廳的每位廚師都是專家，主廚（Head Chef）的工作類似於門控操作，將每道菜品分配給具備對應技能的特定廚師。&lt;/p&gt; 
&lt;p&gt;為確保該系統高效運作，需滿足以下條件：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;每位專業廚師必須精通自身菜系所需技能（例如餃子廚師必須會包餃子），同時所有廚師需能共同處理所有菜品。&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;主廚需充分了解所有專業廚師的專長，並能高效分配訂單。&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;在 MoE 中，前者對應 expert specialization 與 knowledge sharing 的權衡（我們已在&lt;a href=&quot;https://my.oschina.net/IDP/blog/17961367&quot;&gt;介紹 DeepSeekMoE 的這篇文章中&lt;/a&gt;進行了詳細討論），後者則體現了負載均衡的重要性 ------ 這也是本文的核心主題。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;為何負載均衡如此重要？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;原因在於，當負載不均衡發生時，MoE 無法有效運作。&lt;/strong&gt; 最常見的問題是路由崩潰（Route Collapse）：少數專家接收大部分 input token，而其他專家的利用率極低。&lt;/p&gt; 
&lt;p&gt;因此，大部分計算由超負荷工作的專家承擔，而這些專家通常分佈在多個 GPU 核心上，因此會導致硬件資源浪費。&lt;/p&gt; 
&lt;p&gt;由於梯度衝突（gradient conflict），路由奔潰也會導致訓練不穩定。超負荷工作的專家接收更多 input token，他們積累的梯度也會更大，學習速度也會比工作負荷不足的專家快得多；因此，兩者的梯度在幅值和方向上均可能發生偏離，導致訓練難以收斂。&lt;/p&gt; 
&lt;p&gt;最後，MoE 中的負載不均衡也會導致性能低下和泛化效果不佳，因為工作負荷不足的專家會因為訓練 tokens 不足，而難以學習有效知識。&lt;/p&gt; 
&lt;p&gt;由於負載均衡技術對 MoE 至關重要，因此研究者們針對這一問題提出了多種解決方案。其中，最常用的策略是為負載均衡添加輔助損失函數（Auxiliary Loss）和專家選擇（Expert Choice）。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;1.3 帶輔助損失函數的負載均衡&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;一種常見的改善負載均衡的策略是在模型訓練的目標函數基礎上引入輔助損失函數。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-5e9dccf967a447833e952d9a7a9ddee3d5f.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;圖 2. 用於強化負載均衡的輔助損失函數示例。圖片編輯自文獻[5]。&lt;/p&gt; 
&lt;p&gt;上圖展示了一個輔助損失函數的示例，其中&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;N 是專家數量，T 是 token 數量，K 是每個 input token 激活的專家數量。&lt;/li&gt; 
 &lt;li&gt;s_{i, t} 是門控機制的輸出，通過 Softmax 歸一化到 [0, 1] 區間，表示第 t 個 token 選擇第 i 個專家的概率。向量 u_t 是第 t 個 token 的輸入隱藏狀態，而 e_i 是第 i 個專家的&quot;質心&quot;，可以看作歷史上路由到第 i 個專家的 token 嵌入平均值。因此，s_{i, t} 度量的是當前輸入與第 i 位專家歷史接收 token 的平均值的接近程度。&lt;/li&gt; 
 &lt;li&gt;因此，P_i 可視為整個輸入序列選擇第 i 個專家的平均概率。&lt;/li&gt; 
 &lt;li&gt;f_i 表示被路由到第 i 個專家的 token 比例。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;需要注意的是，f_i 是不可微分的，因此最小化上述損失函數實際上轉化為了最小化 s_{i, t}。同時由於 f_i 依賴於 s_{i, t}，對 s_{i, t} 的調整也會影響 f_i，從而實現對各專家負載分配的調節。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;然而，用這種輔助損失函數來均衡負載需要付出一定代價，因為其梯度可能會干擾語言建模目標（language modeling objective）的梯度，導致模型性能下降，在極端不平衡情況下（工作負荷過大的專家的 f_i 和 P_i 都變得極大時）尤其明顯。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;因此，採用這種方法進行負載均衡需要謹慎設置輔助損失函數的權重。為更清晰地説明這一點，文獻[5]的作者進行了一個實驗，用不同 alpha 值訓練模型，結果如下圖所示，其中縱軸表示困惑度指標下的模型性能，橫軸表示 MaxVio（衡量負載不平衡程度的指標，MaxVio 值越高表示負載越不平衡，i 表示第 i 個專家）：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-270603a291bd57664c068ed046f529e917f.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;圖 3. 輔助損失函數控制訓練中負載均衡與模型性能的權衡困境。圖片引自文獻[5]。&lt;/p&gt; 
&lt;p&gt;如圖所示，當 alpha 過小時（alpha=0），MaxVio 保持高位，説明輔助損失函數未能有效實現負載均衡目標。另一方面，當 alpha 過大時（alpha=0.01），模型最終會產生更高的困惑度。&lt;/p&gt; 
&lt;p&gt;綜上，輔助損失函數控制的負載均衡是把雙刃劍：若 alpha 未經仔細調校，可能損害模型性能。實際 LLM 訓練中，由於資源限制，alpha 的調校過程充滿挑戰，這進一步增加了優化難度。&lt;/p&gt; 
&lt;p&gt;上圖同時展示了本文提出的無損失函數方法在相同 Perplexity-MaxVio 座標系下的表現，該方法同時實現了低困惑度和低 MaxVio，證明瞭無損失函數方法的有效性。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;1.4 專家選擇（Expert Choice）策略&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;在此需提及的另一項前期工作是 Expert Choice [7]，它提出了一種簡單高效的負載均衡方法，將路由策略從&quot;token choice&quot;切換為&quot;expert choice&quot;。&lt;/p&gt; 
&lt;p&gt;具體而言，MoE 路由中的門控分數通常通過對 affinity matrix（譯者注：二維矩陣，用於量化 input token 與各個專家之間的匹配程度。）應用 Softmax 計算得出，如圖 2 所示。傳統路由方法從 token 維度應用 Softmax 為每個 token 選擇專家，因此這些方法被稱為&quot;token choice&quot;。問題在於，該機制下我們無法控制每個專家接收的 token 數量，最終導致負載不均衡問題。&lt;/p&gt; 
&lt;p&gt;而 Expert Choice 方法則從專家維度應用 Softmax，為每個專家選擇被路由的 token。通過這種設計，每個專家接收的 token 數量能夠天然達到完美均衡，因此無需依賴輔助損失函數實現負載均衡。在文獻[7]中，這種方法同時展現出更優的模型性能和更快的訓練速度。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;然而，Expert Choice 這種方法也存在侷限 ------ 未來 token 泄露問題。由於每個專家需要先查看所有 token 的路由分數才能決定處理哪些 token，這違反了因果關係（causality），在文本生成、機器翻譯等自迴歸任務中可能引發嚴重問題。&lt;/strong&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;02 DeepSeek 的無輔助損失函數的負載均衡&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;為在不引入 gradient inference（譯者注：此處或為作者筆誤？應當為&quot;gradient interference（梯度幹擾）&quot;，多個損失函數（或多個優化目標）的梯度方向發生衝突。） 的情況下解決負載均衡問題，DeepSeek 提出了一種名為&quot; Loss-Free Balancing&quot;的新技術，通過直接調整門控分數 s_{i,t} 實現。&lt;/p&gt; 
&lt;p&gt;如前文所述，當我們最小化圖 2 所示的輔助損失函數時，最終會通過調整 s_{i,t} 來實現最小化 P_i。&lt;/p&gt; 
&lt;p&gt;因此，若能直接調整 s_{i,t}，理論上應能達到與施加輔助損失函數相似的效果。&lt;/p&gt; 
&lt;p&gt;為此，我們在每個專家的門控分數上添加了專家層面的偏置項，如下圖所示。需注意 b_i 並不用於最終門控分數的計算（後文也將説明該偏置項也是不可微分的），而是用於 TopK 選擇專家時：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-d4c0f9d7daaae22acf77ebd4afdd50c0c9e.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;圖 4. 在門控分數中引入偏置項 b_i。圖片引自文獻[5]&lt;/p&gt; 
&lt;p&gt;該偏置項 b_i 的計算方式非常直觀，如下圖所示：首先獲取分配給各專家的 token 數量平均值及所有專家的理論全局均值，然後計算給各專家分配的 token 數量與理論全局均值的差值，偏置項由該差值（或誤差）的符號乘以固定更新率（fixed update rate）決定（該更新率為可調超參數）。後續章節我們將對該超參數的影響進行更多實驗分析。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-648298535bc37e35360bb990b243896d409.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;圖 5. DeepSeek 無損失函數的負載均衡算法。圖片引自文獻[5]&lt;/p&gt; 
&lt;p&gt;現可通過下表總結不同負載均衡方法的優勢與侷限：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-625befddfe0736a68e0a4968075dc2d51bf.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;圖 6. 不同負載均衡方法對比。圖片引自文獻[5]&lt;/p&gt; 
&lt;p&gt;圖 3 已展示該方法在模型性能與負載均衡間取得了更好的權衡，但仍有多方面需要驗證。下一章節我們將深入分析實驗結果。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;03 Evaluation&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;有三個關鍵問題需要回答：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;DeepSeek 所提出的方法能否在性能和負載均衡之間實現更好的權衡？&lt;/li&gt; 
 &lt;li&gt;圖 5 中更新率 u 有什麼影響？&lt;/li&gt; 
 &lt;li&gt;我們能否進一步優化偏置項更新規則（鑑於其如此之簡單）？&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;strong&gt;3.1 性能 vs. 負載均衡&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;為回答第一個問題，作者在 1B 和 3B 模型上進行了實驗，比較 loss-controlled 負載均衡和 loss-free 負載均衡的困惑度（Perplexity）和 MaxVio，結果如下圖所示：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-c2829c426107ea67be1110636b5d167242f.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;圖 7. loss-controlled 負載均衡和 loss-free 負載均衡的對比。圖片來自文獻[5]。&lt;/p&gt; 
&lt;p&gt;上述結果與我們在圖 3 中看到的結果類似：所提方法同時實現了更低的困惑度和更低的 MaxVio。&lt;/p&gt; 
&lt;p&gt;除了評估最終的 checkpoint 外，作者還展示了訓練過程中的 MaxVio 曲線，以便更全面地理解該方法在整個訓練過程中的表現，如下圖所示：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-20f534fb208e8a1f89dae2e5032a07ffead.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;圖 8. 訓練過程中的 MaxVio 曲線。圖片來自文獻[5]。&lt;/p&gt; 
&lt;p&gt;如圖中所示，在 1B 和 3B 的模型配置下，loss-free 方法在整個訓練過程中都展現出更優的負載均衡能力，體現了該方法的穩定性。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;3.2 超參數的影響（更新率）&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;如圖 5 所示，所提方法引入了一個新的超參數 u（稱為更新率（update rate）），該超參數如何影響 loss-free 方法的有效性？具體而言，我們需要理解 loss-free 方法對超參數 u 的取值是敏感還是不敏感，以及如何選擇一個最優值來最大化該方法的效果。&lt;/p&gt; 
&lt;p&gt;如前文所述，在門控分數中添加偏置項的概念類似於繞過損失函數的反向傳播，直接對門控分數進行梯度更新。在這種情況下，更新率 u 的作用與梯度更新中的步長（step size）類似。由此可推測其影響也相似：&lt;strong&gt;過小的更新率可能會導致收斂速度緩慢。過大的更新率可能導致不穩定和引發波動。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在原文中，作者對更新率進行了實驗測試（取值從 1e-4 到 1e-2），結果如下圖所示：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-239d0c5e83e1699cf19a5179be50163fe48.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;圖 9. 更新率（update rate）對訓練負載均衡的影響。圖片來自文獻[5]。&lt;/p&gt; 
&lt;p&gt;與預期一致，當 u 過小時（如 1e-4），MaxVio 下降速度較慢；而過大的 u（如 1e-2）則因波動性增強，導致訓練過程中 MaxVio 持續偏高。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;3.3 其他偏置項更新規則&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;為回答第三個問題，研究者嘗試了多種備選策略，並將它們與 DeepSeek 提出的版本進行對比：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;策略變體 1&lt;/strong&gt;：使用 e_i 的數值（而不僅僅是符號）計算偏置項，即從 b_i = b_i +u∗sign(e_i) 改為 b_i = b_i +u∗e_i。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;策略變體 2&lt;/strong&gt;：使用乘法偏置項而非加法偏置項。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;其中策略變體 2 可以更正式地描述如下：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-856884cefb7dacdd99e05414c42d5b08cbb.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;實驗表明，策略變體 1 能帶來略優的負載均衡效果，但未提升模型性能：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-842152dbdd0880f205ec969a7535dc64e13.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;圖 10. 策略變體 1 的性能表現。圖片來自文獻[5]。&lt;/p&gt; 
&lt;p&gt;而策略變體 2 甚至顯示出略差的模型性能：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-71a03da108b196cc374602ba52875add0a5.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;圖 11. 策略變體 2 的性能表現。圖片來自文獻[5]。&lt;/p&gt; 
&lt;p&gt;以上所有結果均表明，最簡單的策略反而是最佳選擇。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;04 Summary&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;在本文中，我們解釋了 DeepSeek 模型的核心架構創新之一 ------ DeepSeekMoE 中使用的無輔助損失函數的負載均衡方法。&lt;/p&gt; 
&lt;p&gt;本文首先介紹了混合專家模型（MoE）的基本原理，強調了負載均衡的重要性，並回顧了先前的解決方案（包括 auxiliary loss 方法和 Expert Choice 機制）。接着，本文闡釋了 DeepSeek 的無損失函數的負載均衡方法及其性能表現。&lt;/p&gt; 
&lt;p&gt;DeepSeek 的無損失函數方法在保持因果關係的同時避免了引入梯度幹擾，其有效性已通過原論文的實證結果得到驗證。&lt;/p&gt; 
&lt;p&gt;感謝您花時間閲讀本文！&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;參考文獻&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;[1] DeepSeek（&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.deepseek.com%2F%EF%BC%89&quot; target=&quot;_blank&quot;&gt;https://www.deepseek.com/）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[2] DeepSeek-V3 Technical Report（&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fdeepseek-ai%2FDeepSeek-V3%2Fblob%2Fmain%2FDeepSeek_V3.pdf%EF%BC%89&quot; target=&quot;_blank&quot;&gt;https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[3] DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model（&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2405.04434%EF%BC%89&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2405.04434）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[4] DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models（&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2401.06066%EF%BC%89&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2401.06066）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[5] Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts（&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2408.15664%EF%BC%89&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2408.15664）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[6] GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding（&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2006.16668%EF%BC%89&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2006.16668）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[7] Mixture-of-Experts with Expert Choice Routing（&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2202.09368%EF%BC%89&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2202.09368）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Thanks for reading!&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Hope you have enjoyed and learned new things from this blog!&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;strong&gt;About the author&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Shirley Li&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;I am a Machine Learning Engineer working on building multi-modality models to solve real-world problems.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;END&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;本期互動內容 🍻&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;❓&lt;strong&gt;在實際工程中，您認為負載均衡對模型性能的影響有多大？除了本文提到的技術路徑，您還瞭解哪些有效的負載均衡方案？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;原文鏈接：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fai.gopubby.com%2Fdeepseek-v3-explained-3-auxiliary-loss-free-load-balancing-4beeb734ab1f&quot; target=&quot;_blank&quot;&gt;https://ai.gopubby.com/deepseek-v3-explained-3-auxiliary-loss-free-load-balancing-4beeb734ab1f&lt;/a&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/IDP/blog/18057719</link>
            <guid isPermaLink="false">https://my.oschina.net/IDP/blog/18057719</guid>
            <pubDate>Wed, 02 Apr 2025 07:03:01 GMT</pubDate>
            <author>原創</author>
        </item>
        <item>
            <title>Sam Altman：OpenAI 新產品將因產能問題延遲推出</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;OpenAI 首席執行官 Sam Altman 在 X 發帖透露示，該公司新產品將因產能不足的問題延遲推出。&lt;/span&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;「我們正在控制局面，但你應該預料到 OpenAI 的新版本可能會被推遲，可能會出現問題，而且由於我們面臨容量挑戰，服務有時會很慢。我們正在提升效率，以確保各項工作順利進行。」&lt;/span&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img height=&quot;171&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-5b77f2974426a3588db4062d73b328abd76.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;近日，OpenAI 的新圖像生成功能因能重現吉卜力工作室手繪動畫等風格的表現而備受關注， 同時也引發了一些爭議。上週末，Altman 在 X 的帖子中表示 ，自推出以來，該公司「一直未能跟上進度」，員工們加班加點，甚至週末加班，以「保持服務正常運行」。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Altman 聲稱，週一僅一個小時，ChatGPT 就增加了 100 萬名新用户。ChatGPT 目前擁有 5 億周活躍用户和 2000 萬付費用户，其 2024 年底的用户數量和付費用户數量分別為 3 億和 1550 萬。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;因此為了緩解容量問題，OpenAI 推遲了面向 ChatGPT 免費用户的圖像生成工具的發佈時間，並暫時禁用了面向 Sora 新用户的視頻生成功能。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/342431/sam-altman-openais-capacity-issues-product-delays</link>
            <guid isPermaLink="false">https://www.oschina.net/news/342431/sam-altman-openais-capacity-issues-product-delays</guid>
            <pubDate>Sun, 23 Mar 2025 06:28:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>微信公佈視頻號算法推薦原理</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;「微信珊瑚安全」發文公佈了視頻號算法推薦原理，&lt;strong&gt;稱視頻號主要依靠社交好友關係來進行推薦&lt;/strong&gt;，若推薦的視頻不符合預期，可關閉個性化推薦功能。&lt;/p&gt; 
&lt;p&gt;官方稱，視頻號平台將持續優化算法信息公示方式，用通俗化的語言來解釋算法的基本原理、運行機制等情況，方便用户查詢和理解。&lt;/p&gt; 
&lt;p&gt;據官方公佈的信息，微信視頻號主要依靠社交好友關係來進行推薦，在功能層面，視頻號平台設計了「朋友❤️」的選項標籤，並以顯著提示的方式展現給用户，讓用户第一時間瞭解到好友推薦了什麼內容。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;對於較多朋友推薦的內容，視頻號平台會提升曝光排序&lt;/strong&gt;，並增加了「朋友今天都在看」和多位好友推薦的提醒，用户可以更快、更直接地看到相關視頻，從而擴展自己的信息獲取半徑。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0402/141552_HHSN_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;官方稱，通過這一推薦機制，用户能直觀看到多位朋友推薦的內容，無需將時間過多消耗在刷視頻中。&lt;/p&gt; 
&lt;p&gt;一圖讀懂視頻號算法推薦：&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;3965&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-0d7ada75a7f7e7e5cf72facf18d42527d08.jpg&quot; width=&quot;1280&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;原文：&lt;em&gt;&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FWADcBzT4n9dSPXbxsOcKuQ&quot; target=&quot;_blank&quot;&gt;https://mp.weixin.qq.com/s/WADcBzT4n9dSPXbxsOcKuQ&lt;/a&gt;&lt;/u&gt;&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/342428</link>
            <guid isPermaLink="false">https://www.oschina.net/news/342428</guid>
            <pubDate>Sun, 23 Mar 2025 06:16:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>CNCF 發佈 2025 年 Dapr 狀態報告</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;2025 年 Dapr 狀態報告現已&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.cncf.io%2Fannouncements%2F2025%2F04%2F01%2Fcloud-native-computing-foundation-releases-2025-state-of-dapr-report-highlighting-adoption-trends-and-ai-innovations%2F&quot; target=&quot;_blank&quot;&gt;發佈&lt;/a&gt;，提供了有關項目加速採用、開發者生產力影響和在 AI 驅動應用中擴展角色的新見解。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;報告指出，接近一半的受訪者表示正在生產環境中運行 Dapr 應用，這一比例較往年顯著增加。隨着組織尋求可擴展的雲原生架構，Dapr 在提高開發者生產力和 AI 應用中的作用驅動了廣泛採用。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;360&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-def120cb3998f0c9eacfa97e880529e27ae.webp&quot; width=&quot;300&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;一些亮點內容包括：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;目前有超過 40,000 家企業利用 Dapr 實現微服務、工作流和雲可移植性&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;72% 的受訪者現在將 Dapr 用於關鍵任務應用程序&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;96% 的受訪者表示節省了時間，60% 稱可節省 30% 或更多的開發時間&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;84% 的受訪者預計他們的 Dapr 使用量將在未來一年內增長。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Dapr（分佈式應用運行時）是一個便攜的運行時，使任何開發者都能輕鬆構建在雲和邊緣運行的韌性分佈式應用。它提供了用於通信、狀態和工作流的集成 API，幫助構建適合生產的應用。該項目於 2019 年首次推出，在 2021 年獲 CNCF 接受，並在 2024 年達到畢業狀態。&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/342424/2025-state-of-dapr-report</link>
            <guid isPermaLink="false">https://www.oschina.net/news/342424/2025-state-of-dapr-report</guid>
            <pubDate>Sun, 23 Mar 2025 06:00:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>賺麻了！​ChatGPT 付費用户激增至 2000 萬​，年化營收增長 30%</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p style=&quot;color:#242424; margin-left:0; margin-right:0; text-align:left&quot;&gt;OpenAI 的 ChatGPT 正在經歷一個快速增長的階段。根據 The Information&lt;span&gt;最新&lt;/span&gt;報道，ChatGPT 的付費用户數量在短短三個月內突破了 2000 萬，較去年年底的 1550 萬增加了近 30%。這一增長表明，越來越多的用户願意為這個能夠撰寫代碼、文章、提供健康建議和理財規劃的人工智能付費。&lt;/p&gt; 
&lt;p style=&quot;color:#242424; margin-left:0; margin-right:0; text-align:left&quot;&gt;根據估算，ChatGPT 目前每月的營收達到至少 4.15 億美元，年化營收可達 50 億美元，較去年年底的月收入 3.33 億美元、年化 40 億美元增長了近 30%。&amp;nbsp;&lt;/p&gt; 
&lt;p style=&quot;color:#242424; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;img height=&quot;244&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-6b476aa45e8eb822c5db023631270331c01.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#242424; margin-left:0; margin-right:0; text-align:left&quot;&gt;除了基礎的 20 美元 / 月的 ChatGPT Plus 訂閲外，OpenAI 還提供高達 200 美元 / 月的企業級 Pro 套餐，進一步推動了收入的增長。此外，OpenAI 的營收還來自於大量開發者和平台通過 API 調用其模型的費用，預計 API 業務在 2023 年將為公司帶來約 20 億美元的收入。若這一增長趨勢持續，OpenAI 在 2025 年可能實現高達 127 億美元的營收，遠超 2024 年的 40 億美元。&lt;/p&gt; 
&lt;p style=&quot;color:#242424; margin-left:0; margin-right:0; text-align:left&quot;&gt;儘管用户數量快速增加，但付費用户的比例卻有所下降。從三個月前的 5% 降至當前的 4%。目前，OpenAI 每週活躍用户已達到 5 億，較去年年底的 3.5 億增長了 43%。&lt;/p&gt; 
&lt;p style=&quot;color:#242424; margin-left:0; margin-right:0; text-align:left&quot;&gt;OpenAI 表示，為了維持這部分用户並支持不斷增加的免費用户，公司需要大量資金。為此，OpenAI 計劃以 2600 億美元的估值融資 400 億美元。儘管目前公司仍處於虧損狀態，OpenAI 預計距離盈利還有至少五年。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/342406</link>
            <guid isPermaLink="false">https://www.oschina.net/news/342406</guid>
            <pubDate>Sun, 23 Mar 2025 03:35:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>中國首款全自研高性能 RISC-V 服務器芯片發佈</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;睿思芯科近日&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FJgBOwvgYbIou4mv5ruwpHA&quot;&gt;宣佈&lt;/a&gt;&lt;/u&gt;推出新一代高性能&lt;strong&gt;靈羽處理器&lt;/strong&gt;，這是中國首款全自研高性能 RISC-V 服務器芯片，在算力、能效和接口配置等方面均達到國際主流水平，滿足高性能計算、全閃存儲與 DeepSeek 等開源大語言模型的應用場景。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0402/112935_NMk2_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;睿思芯科創始人、董事會主席及 CEO 譚章熹博士介紹了靈羽處理器性能及架構層面的關鍵突破，稱其在單核性能和多核並行性能上均實現大幅提升，較現有主流服務器處理器具備顯著優勢。&lt;/p&gt; 
&lt;p&gt;靈羽處理器基於睿思芯科全自研的 CPU 核心 IP 與片上網絡 IP，實現了先進亂序執行、高速數據通路與 Mesh 互聯結構，同時通過軟硬件結合的設計 - 工藝協同優化，在產品工程、EDA 工具鏈、物理設計與晶圓製造流程中實現創新，顯著提升運算中的能效比以及優化總體擁有成本（TCO）。&lt;/p&gt; 
&lt;p&gt;據介紹，靈羽處理器支持 DDR5 高速內存，支持 PCIe 5.0 標準及 CXL 2.0 協議，並支持高達 8 路互聯。同時，靈羽處理器具備企業級 RAS 特性，滿足 RISC-V 服務器標準。&lt;/p&gt; 
&lt;p&gt;睿思芯科官網&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Frivai-ic.com.cn%2Fabout%23introduce&quot;&gt;顯示&lt;/a&gt;&lt;/u&gt;，睿思芯科成立於 2018 年底，是一家提供 RISC-V 高端核心處理器解決方案的公司，創始團隊來自於加州大學伯克利分校 RISC-V 原創項目組。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/342405</link>
            <guid isPermaLink="false">https://www.oschina.net/news/342405</guid>
            <pubDate>Sun, 23 Mar 2025 03:32:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>開源 Java 工具 - Hutool 至大家的一封信</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;各位朋友及廣大&lt;/span&gt;Hutool&lt;span&gt;的用户：&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 近期大家可能注意到，&lt;/span&gt;Hutool&lt;span&gt;的項目在&lt;/span&gt;Gitee&lt;span&gt;等平台做了遷移，項目的地址從&lt;/span&gt;Dromara&lt;span&gt;組織遷移到了&lt;/span&gt;Bugotech&lt;span&gt;，這一操作短暫引起了一些熱議，因此特意在此給大家做了個解釋，同時也説明下&lt;/span&gt;Hutool&lt;span&gt;在未來的發展規劃。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; Hutool&lt;span&gt;於&lt;/span&gt;2013&lt;span&gt;年第一次開源，&lt;/span&gt;2014&lt;span&gt;年&lt;/span&gt;5&lt;span&gt;月&lt;/span&gt;28&lt;span&gt;日發佈了第一個版本，至今已經持續維護了&lt;/span&gt;12&lt;span&gt;個年頭，共計發佈&lt;/span&gt;301&lt;span&gt;個版本，從一個小小的工具方法集合，發展為覆蓋&lt;/span&gt;Java&lt;span&gt;大量常用&lt;/span&gt;API&lt;span&gt;封裝的龐大工具集。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 隨着用户不斷積累增多，&lt;/span&gt;Hutool&lt;span&gt;也陸續建立了&lt;/span&gt;7&lt;span&gt;個&lt;/span&gt;2000&lt;span&gt;人大羣，&lt;/span&gt;2&lt;span&gt;個微信羣，這些羣我們通過「嚴格」的管理，讓廣大用户精準快速的解決了問題，同時通過城市標註，也促成了找工作、交朋友的好氛圍。在&lt;/span&gt;Github&lt;span&gt;和&lt;/span&gt;Gitee&lt;span&gt;平台，&lt;/span&gt;Hutool&lt;span&gt;處理接近&lt;/span&gt;7000&lt;span&gt;個&lt;/span&gt;issue&lt;span&gt;和&lt;/span&gt;2000&lt;span&gt;餘&lt;/span&gt;PR&lt;span&gt;，我們也是採用快速解決的方式，第一時間解決用户的問題和需求。好的技術氛圍以及快速響應，我想這也是&lt;/span&gt;Hutool&lt;span&gt;廣受歡迎的原因。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 不過隨着項目擴大，面臨的很多問題也暴露出來了。首先是&lt;/span&gt;QQ&lt;span&gt;羣的管理，在&lt;/span&gt;2024&lt;span&gt;年，&lt;/span&gt;Hutool4&lt;span&gt;羣突然被封，經過瞭解後才知道是我們沒有及時看羣聊記錄，有用户在羣裏吵架舉報被封。這也暴露出我們創建的這些「烏託邦」並非完美，而後我們不得不隨時關注羣裏動態，極大的分散了精力。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 第二個問題來自於企業用户的認可程度，作為一個自發團隊維護的開源項目，很多用户反饋在其所在企業禁止使用，換位思考一下，我們也非常理解企業的擔憂，畢竟。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 第三個問題是如何良性發展。我們既希望&lt;/span&gt;Hutool&lt;span&gt;工具一如既往的為大家提供純粹的幫助，也希望圍繞廣大的用户做一些新的嘗試。比如前期我們賣&lt;/span&gt;T&lt;span&gt;恤、鼠標墊，算是失敗了，用户説你還不如賣牛肉乾，哈哈。後來我們的團隊成員還嘗試做了腳手架、導航頁等功能，不過由於精力問題，也草草收場。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;基於以上原因，我們決定採用公司化的方式來繼續維護&lt;/span&gt;Hutool&lt;span&gt;，那之後有什麼變化呢？&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 所有代碼層面的維護、更新均無變化，我們依舊保持高效的更新。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 主頁上，只是去掉了一些團隊信息，變更一下備案（從個人備案變更為企業備案），後續可能改版豐富內容。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;文檔方面，依舊免費開放，保證永不閉源。後期的主要變化就是穿插一些商業產品的介紹（我想大家不會介意），後續版本的文檔我們會加快豐富和補充。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; QQ&lt;span&gt;和微信羣方面，有專屬「&lt;strong&gt;客服&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;MM&lt;/strong&gt;&lt;span&gt;」管理解答大家的問題，平時也會發布一些行業新聞什麼的，陪大家閒聊。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 那&lt;/span&gt;Hutool&lt;span&gt;接下來的規劃是什麼呢？&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 1&lt;span&gt;、&lt;/span&gt;Hutool-5.x&lt;span&gt;依舊以&lt;/span&gt;bug&lt;span&gt;修復為主，不再添加新特性，重要的説三遍，穩定穩定穩定！（像極了&lt;/span&gt;JDK8&lt;span&gt;）&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 2&lt;span&gt;、&lt;/span&gt;Hutool-6.x&lt;span&gt;因為一直處於&lt;/span&gt;milestone&lt;span&gt;版本（測試版本），供大家嚐鮮新的&lt;/span&gt;API&lt;span&gt;，並發現問題，因此一直未正式&lt;/span&gt;release&lt;span&gt;。而在開發當中，由於還是基於&lt;/span&gt;JDK8&lt;span&gt;編譯，導致&lt;/span&gt;JDK11+&lt;span&gt;（尤其&lt;/span&gt;JDK17&lt;span&gt;）很多功能特性無法兼容，比如&lt;/span&gt;Jakarta&lt;span&gt;很多包變更後根本不支持&lt;/span&gt;JDK8&lt;span&gt;，&lt;/span&gt;Spring&lt;span&gt;也無法做到同時兼容，因此很有可能在更新幾個&lt;/span&gt;Milestone&lt;span&gt;後停止更新。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 3&lt;span&gt;、&lt;/span&gt;Hutool-7.x&lt;span&gt;，是滴，它要來了，終於下定決心從&lt;/span&gt;JDK17&lt;span&gt;開始支持，這樣就可以輕裝上陣，拋掉很多兼容性代碼（比如在&lt;/span&gt;6.x&lt;span&gt;中為支持新特性，不得不用反射方式調用），接下來，就是老項目繼續使用&lt;/span&gt;Hutool-5.x&lt;span&gt;，新項目使用&lt;/span&gt;7.x&lt;span&gt;。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 我們説，&lt;/span&gt;Hutool&lt;span&gt;的意義遠不是代碼本身，而是一種思維方式，一種交流方式。我們通過開源中的代碼聚到一起，碰撞思維的火花，尋找志同道合的朋友，找到一起成功的夥伴，也找到自我價值的體現。我相信，因為&lt;/span&gt;Hutool&lt;span&gt;的存在，未來會有無限可能！&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:justify&quot;&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0px; margin-right:0px; text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;Hutool&lt;span&gt;團隊敬上&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0px; margin-right:0px; text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;2025&lt;span&gt;年&lt;/span&gt;4&lt;span&gt;月&lt;/span&gt;2&lt;span&gt;日&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/342402</link>
            <guid isPermaLink="false">https://www.oschina.net/news/342402</guid>
            <pubDate>Sun, 23 Mar 2025 03:29:00 GMT</pubDate>
            <author>來源: 投稿</author>
        </item>
        <item>
            <title>微軟重新設計 BSOD：Windows 11 將用「黑屏死機」取代「藍屏死機」</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;微軟宣佈，它將重新設計 Windows 11 的藍屏死機（BSOD）錯誤信息。新的設計摒棄了傳統的藍色背景、悲傷表情和二維碼，轉而採用了一個更簡化的屏幕，其外觀與 Windows 進行更新時看到的黑色屏幕非常相似。&lt;/p&gt; 
&lt;p&gt;目前尚不清楚微軟發佈最終版本更新時，這個新的 BSOD 是否會保持為黑色屏幕。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-f5d78b86ec91a0dda404f20261de42df560.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;微軟在一篇關於這一變化的 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblogs.windows.com%2Fwindows-insider%2F2025%2F03%2F28%2Fannouncing-windows-11-insider-preview-build-26120-3653-beta-channel%2F&quot; target=&quot;_blank&quot;&gt;博客文章&lt;/a&gt; 中解釋道，「我們正在預覽一個新的、更精簡的用户界面，用於處理意外重啓，這更好地符合 Windows 11 的設計原則，並支持我們儘快讓用户恢復生產力的目標，我們簡化了您的體驗，同時保留了屏幕上的技術信息。」&lt;/p&gt; 
&lt;p&gt;Windows 內部測試者可以在 Beta、Dev 和 Canary 通道的測試版本中嘗試新的藍屏死機 (BSOD)，但在最終作為黑色或藍色屏幕發佈之前，這些測試版本中它會以綠色屏幕的形式出現。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-b72bb5e17541e78eef82f333a668ab28952.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;這是自微軟在 Windows 8 中在屏幕上添加悲傷表情符號以來，BSOD 的第一次重大變化。這個新設計包括了 BSOD 錯誤或故障驅動程序，並簡單地聲明「您的設備遇到了問題，需要重新啓動」。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;up-51d3ac36bc416abe53fe1c0517c774a1824.png&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-51d3ac36bc416abe53fe1c0517c774a1824.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;微軟曾在 2021 年將 Windows 11 的測試版本中的 BSOD 轉換為黑色屏幕，但公司隨後又恢復了自 Windows 8 以來一直在使用的藍色屏幕。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/342400/windows-bsod-black-new-design</link>
            <guid isPermaLink="false">https://www.oschina.net/news/342400/windows-bsod-black-new-design</guid>
            <pubDate>Sun, 23 Mar 2025 03:17:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>Dolphin 開源：支持東方 40 語種+中國 22 方言的 SOTA 語音大模型</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;海天瑞聲&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fw4wY8ZgJsJRMpcHSl7M6wQ&quot; target=&quot;_blank&quot;&gt;宣佈&lt;/a&gt;攜手清華大學電子工程系語音與音頻技術實驗室，共同推出了 Dolphin —— 一款專為東方語言設計的語音大模型。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;根據介紹，Dolphin 的訓練數據集整合了海天瑞聲的專有數據和多個開源數據集，總時長超過 20 萬小時，涵蓋 40 個東方語種。其中，海天瑞聲數據集包含 137,712 小時的音頻，覆蓋 38 個東方語種。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;strong&gt;核心亮點&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;支持東方 40 個語種的語音識別，中文語種支持 22 方言（含普通話）；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;訓練數據總時長 21.2 萬小時：其中海天瑞聲高質量專有數據 13.8 萬小時，開源數據 7.4 萬小時；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;在 3 個測試集（海天瑞聲、Fleurs、CommonVoice）下，與 Whisper 同等尺寸模型相比：&lt;/span&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;base 版本平均 WER 降低 63.1%；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;small 版本平均 WER 降低 68.2%；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;medium 版本平均 WER 降低 67.7%；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;large 版本平均 WER 降低 60.6%&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;base 與 small 版本模型與推理代碼全面開源；Dolphin 開源的 small 版本與 Whisper large v3 相比，平均 WER 降低 54.1%。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;strong&gt;模型結構&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#3e3e3e; margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;Dolphin 網絡結構基於 CTC-Attention 架構，E-Branchformer 編碼器和 Transformer 解碼器，並引入了 4 倍下采樣層，以實現高效的大規模多語言語音識別模型的訓練。CTC-Attention 架構結合了 CTC 的序列建模能力和注意力機制的上下文捕捉能力，能夠有效提升模型的識別準確性和效率。E-Branchformer 編碼器採用並行分支結構，能夠更有效地捕捉輸入語音信號的局部和全局依賴關係，為模型提供了更豐富的特徵表示。解碼器部分則採用了在序列到序列任務中表現出色的 Transformer，能夠生成高質量的文本輸出。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#3e3e3e; margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;為了進一步提高訓練效率和性能，項目團隊在模型中引入了 4 倍下采樣層。這一層可以減少輸入特徵的序列長度，從而加速計算過程，同時保留關鍵的語音信息，確保模型的識別效果不受影響。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#3e3e3e; margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;img height=&quot;310&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-b9c613bad8be71cfd5a56a8e1a86fdb9e43.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#3e3e3e; margin-left:0; margin-right:0; text-align:justify&quot;&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;strong&gt;多任務格式&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#3e3e3e; margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;Dolphin 借鑑了 Whisper 和 OWSM 的創新設計方法，但專注於 ASR 進行了若干關鍵修改。Dolphin 不支持翻譯任務，並且去掉了 previous text 及其相關標記的使用，這簡化了輸入格式並減少了潛在的複雜性。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#3e3e3e; margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;Dolphin 引入了兩級語種標籤系統，以便更好地處理語言和地區的多樣性。第一個標籤指定語種（例如&amp;lt;zh&amp;gt;、&amp;lt;ja&amp;gt;），第二個標籤指定地區（例如&amp;lt;CN&amp;gt;、&amp;lt;JP&amp;gt;）。這種分層方法使模型能夠捕捉同一種語言內不同方言和口音之間的差異，以及同一地區內不同語言之間的相似性，從而提高了模型區分密切相關的方言的能力，並通過在語言和地區之間建立聯繫增強了其泛化能力。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#3e3e3e; margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;img height=&quot;155&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-3f0a949e5180d3d4ddd5c382835e9e43dc3.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;測試結果表明，Dolphin 在多種語言上的詞錯誤率（WER）顯著低於現有開源模型。例如，在海天瑞聲數據集上，Dolphin base 模型的平均 WER 為 31.5%，small 模型為 24.5%，medium 模型為 22.2%；在 CommonVoice 數據集上，Dolphin base 模型的平均 WER 為 37.2%，small 模型為 27.4%，medium 模型為 25.0%。即使與 Whisper large-v3 模型相比，Dolphin 在模型規模更小的情況下，性能也更為出色。以中文為例，Dolphin 中模型的 WER 僅為 9.2%，而 Whisper large-v3 模型為 27.9%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;119&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-06462426d56c9716d0df14b1e58a21e0632.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;170&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-11828934de37eba21c5586716cf6aea6c7b.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/342397</link>
            <guid isPermaLink="false">https://www.oschina.net/news/342397</guid>
            <pubDate>Sun, 23 Mar 2025 03:07:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>Linus 口吐芬芳：怒斥英特爾工程師提交的代碼是「令人作嘔的一坨」</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;Linux 6.15 的開源圖形驅動程序更新集已合併，但 Linux 創始人 Linus Torvalds 對這次合併請求並不十分滿意。&lt;/p&gt; 
&lt;p&gt;特別是，他對作為完整內核構建一部分構建的某些新的&quot;hdrtest&quot;測試代碼以及它留下的「垃圾」感到不滿，並認為這段代碼「應該死去」，至少對於非 DRM 驅動程序開發者來説是這樣。&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:#424242&quot;&gt;根據他們在郵件列表的討論，這些代碼是由英特爾工程師 Jani Nikula 提交的&amp;nbsp;&lt;/span&gt;DRM 驅動相關代碼。DRM 是 Linux 內核管理 GPU 渲染的核心子系統，負責硬件加速、視頻播放等圖形處理任務。&lt;/p&gt; 
&lt;p&gt;在週五夜晚，Linus Torvalds 在郵件列表上&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flore.kernel.org%2Fdri-devel%2F174321011387.3019715.1646591159826097779.pr-tracker-bot%40kernel.org%2FT%2F%23t&quot; target=&quot;_blank&quot;&gt;發表評論&lt;/a&gt;：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;「哼。我已經完成了合併，解決了（瑣碎的）衝突，但注意到這最終包含了令人厭惡的「hdrtest」垃圾，&lt;/p&gt; 
 &lt;p&gt;(a) 它減慢了構建速度，因為它是為常規 allmodconfig 構建，而不是作為一些簡單的東西供你們按需運行&lt;/p&gt; 
 &lt;p&gt;(b) 還會在包含目錄中留下隨機的 &#39;hdrtest&#39; 廢物&lt;/p&gt; 
 &lt;p&gt;人們已經分別對此進行了抱怨。它絕對不應該以這種破損的形式出現在我面前。&lt;/p&gt; 
 &lt;p&gt;為什麼這個測試會被當作構建的常規部分來執行？&lt;/p&gt; 
 &lt;p&gt;該死的，我們不應該為依賴項添加隨機的廢物文件，這會讓源代碼樹變得一團糟。&lt;/p&gt; 
 &lt;p&gt;我注意到它仍然存在的原因是 &quot;git status&quot; 會提醒那些愚蠢的垃圾代碼沒有被忽略。&lt;/p&gt; 
 &lt;p&gt;但更重要的是，這些垃圾代碼還會破壞文件名補全功能！所以，將它們添加到 gitignore 中實際上並不能解決問題，它只會讓我更快地注意到。&lt;/p&gt; 
 &lt;p&gt;這東西需要 &lt;em&gt;消失&lt;/em&gt;。&lt;/p&gt; 
 &lt;p&gt;如果你想做那個 hdrtest 的事情，就把它作為你 &lt;em&gt;自己的&lt;/em&gt; 檢查的一部分來做。不要讓其他人看到那個&lt;strong&gt;令人厭惡的一坨 (disgusting turds)&lt;/strong&gt;，並在他們的樹中留下這些廢物。&lt;/p&gt; 
 &lt;p&gt;我現在就通過將其標記為損壞來禁用它。你們可以自己決定怎麼做，但強迫別人看到這些內容並不是解決問題的辦法。&lt;/p&gt; 
 &lt;p&gt;我建議你們 *不要* 將這部分內容納入 Kconfig 設置和常規構建中，而應該是一個 &lt;em&gt;你們&lt;/em&gt; 可以在測試中運行的部分（即像「make drm-hdrtest」那樣，而不是作為常規構建的一部分）。&lt;/p&gt; 
 &lt;p&gt;Linus&lt;/p&gt; 
 &lt;p&gt;&lt;img height=&quot;1638&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0402/105324_cR4r_2720166.png&quot; width=&quot;1156&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;這裏的「hdr」是指 C 頭文件。新的「hdrtest」代碼是為 Intel Xe 內核驅動編寫的，目的是嘗試確保 DRM 頭文件是自包含的，並且能夠通過內核文檔測試。對包含的 DRM 頭文件進行基本的維護檢查，以確保它們都處於良好狀態。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-23fbc2b19b2b3a820bd285e7b3693fa80ed.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:#424242&quot;&gt;後續&lt;/span&gt;英特爾工程師 Jani Nikula&amp;nbsp;&lt;span style=&quot;background-color:#ffffff; color:#424242&quot;&gt;在郵件列表中進行了回應，承諾將測試文件移至.hdrtest 子目錄，並通過 kconfig 選項隔離額外檢查項。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/342394/linux-6-15-hdrtest-turd</link>
            <guid isPermaLink="false">https://www.oschina.net/news/342394/linux-6-15-hdrtest-turd</guid>
            <pubDate>Sun, 23 Mar 2025 02:50:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>預測技術在美團彈性伸縮場景的探索與應用</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                                                                                                                                        &lt;blockquote&gt; 
 &lt;p&gt;管理企業大規模服務的彈性伸縮場景中，往往會面臨着兩個挑戰：第一個挑戰是精準的負載預測，由於應用實例的啓動需要一定預熱時間，被動響應式伸縮會在一段時間內影響服務質量；第二個挑戰是高效的資源分配，即在保障服務質量的同時控制資源成本。為瞭解決這些挑戰，美團基礎技術部與中國人民大學信息學院柴雲鵬教授團隊展開了&quot;預測技術在彈性伸縮場景的應用&quot;科研合作，相關論文《&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdl.acm.org%2Fdoi%2F10.1145%2F3589334.3645330&quot; target=&quot;_blank&quot;&gt;PASS: Predictive Auto-Scaling System for Large-scale Enterprise Web Applications&lt;/a&gt;》在具有國際影響力的會議 The Web Conference 2024（CCF-A 類會議）上作為 Research Full Paper 發表。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-4bbfa4d899326f563c39328ce707f4be8b1.jpg&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;1 背景&lt;/h2&gt; 
&lt;p&gt;在管理企業大規模服務彈性伸縮的場景下，Web 應用的負載時序數據分析和預測至關重要。然而，由於應用的週期性特徵和負載的複雜性，尋找一種能夠適應所有應用的預測模型成為了一項挑戰。首先，應用的負載數據往往具有周期性，這就需要在進行分析和預測時，需要考慮到這種週期性的影響。其次，由於業務特徵的差異，預測算法可能在不同的應用下表現出差異化的效果，不存在&quot;one-size-fits-all&quot;的情況。例如，經測試發現，雖然在線模型的預測效果大多數時候好於離線模型，但在一些規律的突增情況下，在線模型經常存在滯後性的問題。這些問題都需要在對應用負載時序數據分析和預測時，進行深入的研究和探討。&lt;/p&gt; 
&lt;p&gt;除了準確的負載預測，還需要高效的彈性伸縮策略。目前，業界常用的彈性伸縮策略有基於規則的閾值法、基於控制論的目標追蹤和排隊論。然而，我們發現這些方法並不能有效地保障 QoS（Quality of Service，服務質量），尤其是 QoS 包含對尾延遲的要求時，其背後原因是這些策略的性能模型並不準確。閾值法和目標追蹤的性能模型是&quot;當 QPS 或 CPU 利用率在給定的範圍內時 QoS 達標&quot;。但這些閾值通常是根據人工經驗設置的，並不總是準確。排隊論根據統計模型推導在給定的負載和資源下業務的延遲，但它依賴的理論假設（例如用户請求到來的分佈）並不總在現實中成立，使其估算的延遲和真實值有偏差。我們在測試中也發現了排隊論估算的延遲會略低於真實延遲，因而導致出現服務質量不達標。此外，基於 AI 的彈性伸縮方法例如強化學習並不一定適用企業的生產環境，因為 AI 模型缺乏一定的可解釋性，且可能會在線上做出有 QoS 違例風險的行為。&lt;/p&gt; 
&lt;h2&gt;2 探索分析&lt;/h2&gt; 
&lt;h3&gt;2.1 預測實驗探索&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;發現 1&lt;/strong&gt;：由於預測算法的固有限制，沒有一種單一的算法能夠在所有類別的時序數據中都提供最佳的預測效果。多樣化的預測算法對於美團豐富的業務流量預測更有效。&lt;/p&gt; 
&lt;p&gt;我們對美團的服務流量數據進行週期檢測（計算 ACF 自相關係數），發現 92.80% 的應用表現出強週期性（相關係數大於 0.8），4.55% 顯示出弱週期性（相關係數介於 0.5~0.8 之間），只有 2.65% 的應用沒有表現出明顯的週期性（相關係數小於 0.5）。因此，對於美團內的大多數服務，可以使用模型可靠地預測 QPS 流量數據。&lt;/p&gt; 
&lt;p&gt;我們使用 225 個美團真實服務流量數據對業界常用的預測算法進行了測試。實驗結果表明，預測效果最好的算法並不是單一的，而是取決於業務流量的具體特徵。如圖 1(a) 和 (b) 所示，我們使用三個代表性服務的流量數據來説明週期因子算法（Seasonal index）和 patchTST 模型的預測效果。 在圖 1(a) 中，對於服務 1，週期因子算法顯著優於 patchTST，而在圖 1(b) 中，對於服務 2，patchTST 卻又比周期因子預測效果更好。這一實驗説明瞭最優預測算法的動態多樣性，我們需要針對業務流量特徵選擇最合適的預測算法。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//3024fe4dc9e5d02d0ae37f5c55344098.png&quot; alt=&quot;圖 1 各種預測算法的準確性和魯棒性對比&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;發現 2&lt;/strong&gt;：在線預測提供了較高的平均預測準確度，但在&quot;突變特徵&quot;表現不佳。 相比之下，離線預測可以捕獲&quot;突變特徵&quot;，但存在明顯的&quot;振幅偏差&quot;問題。&lt;/p&gt; 
&lt;p&gt;在線預測模型實時地獲取最新的數據輸入，輸出未來一小段時間（如：15 分鐘）的預測數據。 離線預測模型無需實時數據輸入，直接預測未來較長一段時間（如：1 天）的預測數據。 由於缺乏最新實時數據的輸入，離線模型往往無法達到和在線模型相同的預測準確性。但在一些有規律的突增時間點，離線模型能夠捕捉到突變的週期性特徵，從而取得&quot;及時&quot;的預測效果。例如，在週期性&quot;突變特徵&quot;的情況下，在線模型可能會表現出預測滯後（圖 1(c) 中的黑色圓圈突出顯示）。 這種滯後效應會持續一段時間，我們稱之為&quot;髒區間&quot;。&lt;/p&gt; 
&lt;p&gt;相反，離線模型有效地捕獲這些信息，從而能夠及時進行預測。然而，離線模型存在着&quot;振幅偏差&quot;的問題，即特徵的形狀被準確地表示，但其絕對值存在差異（見圖 1(d)）。&quot;突變特徵&quot;在美團服務中尤其普遍，這是由於美團許多業務都存在午、晚流量高峯。雖然&quot;突變特徵&quot;只佔實際時間序列數據的一小部分，但這些特徵的精確預測對於在生產環境中維持 QoS 至關重要。&lt;/p&gt; 
&lt;h3&gt;2.2 伸縮方法分析&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;發現 3&lt;/strong&gt;：雲平台廣泛使用的彈性伸縮方法並不能有效保障 QoS，特別是當 QoS 要求對尾延遲的保障時。 與平均響應時間 RT 相比，當 QoS 要求為 TP999 尾延遲時，表 1 中幾種方法的 QoS 保障率都顯著降低。然而，大多數業務應用都需要關於尾延遲的保障，這使得彈性伸縮方法效率並不高，要麼接受 QoS 違例，要麼多冗餘資源。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;發現 4&lt;/strong&gt;：這些彈性伸縮方法的性能模型不夠準確。基於閾值的規則和目標跟蹤背後的性能模型實際上是&quot;當 QPS 或 CPU 利用率在一定範圍內時 QoS 達標&quot;。 而這個範圍的參數是根據人工經驗確定的。表 1 的結果證明它並不完全有效。另外，排隊論因為過於依賴理論假設而不夠準確，導致其估計的 RT 低於真實值，從而出現 QoS 違例。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//33c8571e501f85c1fac1c60a9b6e862c.png&quot; alt=&quot;表 1 三種常見彈性伸縮方法的 QoS 保障率和資源成本&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;目前雲平台的常見彈性伸縮方法有以下幾種：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;閾值法&lt;/strong&gt;：基於一系列包含閾值的規則來進行彈性伸縮。以 CPU 資源利用率為例，當資源利用率超過上限閾值時，增加資源，反之，當利用率低於下閾值時，減少資源。閾值參數通常憑經驗設置。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;目標追蹤&lt;/strong&gt;：一種基於反饋的控制論方法，將某個指標（例如 CPU 資源利用率）維持在特定範圍內。當實際資源利用率不在設定範圍內時，系統會根據當前狀態自動計算需要伸縮的實例數量。例如，假設 CPU 和流量呈線性相關的前提下，當前 10 個實例的平均 CPU 資源利用率為 80%，目標值為 50%，則實例需要擴容為 80%*10/50%=16 個實例。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;排隊論&lt;/strong&gt;：根據排隊論模型計算性能指標（例如延遲）並進行相應的伸縮以確保 QoS 達標。常見的𝑀/𝑀/𝑠模型表示請求到達和處理的時間呈指數分佈，共有𝑠服務器並行處理。阿里巴巴的彈性伸縮框架 AHPA 使用排隊論作為性能模型。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;強化學習&lt;/strong&gt;：彈性伸縮模塊充當代理並與環境交互，在每次執行伸縮操作後接收獎勵反饋。它通過反覆試驗建立狀態（當前監控指標）和動作（伸縮多少實例）之間的映射模型。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;大部分雲平台使用的彈性伸縮方法都是簡單或直接的，比如：閾值法、目標跟蹤和排隊論。強化學習和其他人工智能相關方法使用較少，因為它們可能會影響在線業務的性能。 我們使用若干具有代表性的後端服務測試了常用彈性伸縮方法的 QoS 保障率。由於我們的主要目的是驗證 QoS 保障率，因此我們使用了階梯式增加的工作負載。QoS 保障率和資源成本的結果如表 1 所示。QoS 保障率是用 QoS 保障時長除以總時長計算得出的。資源成本由實例數隨時間（分鐘）的積分得到。&lt;/p&gt; 
&lt;h2&gt;3 技術方案&lt;/h2&gt; 
&lt;p&gt;為瞭解決這些問題，我們提出了 PASS（Predictive Auto-Scaling System），這是一種為企業大規模在線 Web 應用定製的預測彈性伸縮系統。為了保障預測框架準確性和魯棒性，我們根據每個應用的特徵，動態匹配和校準其適合的預測算法，有效應對了業務負載的多樣性。我們進一步建立了基於在線歷史日誌的性能模型以保障多樣化的 QoS，可解釋的同時不會對在線業務產生負面影響。 除了基於負載預測和性能模型的主動伸縮以外，我們還設計了響應式的兜底策略，以及時應對因不準確的預測或意外事件導致的服務質量違例。在美團廣泛的業務應用和真實負載下，相對於表 1 中其它方法，PASS 表現更優於 SOTA，以更少的資源成本實現更高的負載預測準確度和更低的 QoS 違例。&lt;/p&gt; 
&lt;p&gt;PASS 的整體架構如圖 2 所示。ELPA（Ensemble Learning-based Prediction Algorithm）實時準確地預測業務負載的 QPS 時序數據。通過查詢基於歷史日誌構建的性能模型，PASS 在不違反 QoS 的前提下，預測伸縮到業務所需的實例數量。此外，PASS 還持續監控 QoS 指標，如果發現由於預測不準導致的 QoS 違例，PASS 會基於當前真實的延遲校正預測結果，並伸縮到適當數量的實例，以快速消除 QoS 違例。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//61fbe22e46122909502ea446715b12e5.png&quot; alt=&quot;圖 2 PASS 整體架構，圖中上半部分的黑線表示離線步驟，下半部分中的紅線表示在線過程&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;3.1 ELPA 預測模型&lt;/h3&gt; 
&lt;p&gt;我們提出了 ELPA（Ensemble Learning-based Prediction Algorithm）預測算法框架（如圖 3 所示）。對於每一類業務的實時流量，ELPA 採用一組相應的在線和離線模型來提供預測服務。我們首先從一系列不同的在線模型中選擇一個能夠為當前時間序列數據提供最準確預測的模型。然後，為了更好地預測&quot;突變特徵&quot;，我們使用一個表現最好的離線預測模型代替在線模型。此外，我們使用振幅調整來改善離線預測時可能出現的&quot;振幅偏差&quot;問題，從而進一步增強預測性能。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//a5687f66d210c1b1dea023ffe9a4b9d4.png&quot; alt=&quot;圖 3 預測算法框架&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//a34d503f83aac0ff074dac8b77d1afbd.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;3.2 性能模型設計&lt;/h3&gt; 
&lt;p&gt;我們提出了基於日誌的性能模型（Log-based Performance Model）。主要包括：性能模型構建與模型訓練校準兩部分。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;性能模型構建&lt;/strong&gt;：Algorithm1 説明瞭如何基於歷史日誌（不需要對應用進行畫像）構建性能模型。 我們從監控系統獲取服務的日誌，包括 QPS、實例數、QoS 指標等信息。 輸入的 QoS 由服務設置，當 QoS 發生變化時需要重新建立性能模型。我們首先按實例數量聚合輸入日誌並遍歷所有日誌（第 1-9 行）。 第 4 行按照&quot;cap&quot;的粒度（例如最大 QPS 的千分之一）聚合 QPS，以減少數據數量和算法開銷。 第 5-8 行統計 QoS 違例的發生次數。由於系統故障等因素可能導致某些記錄不準確，因此在評估某個 QPS 的保障率時，我們不僅計算當前的 QPS 記錄，還綜合考慮所有較低的 QPS 記錄（第 10-19 行）。第 20 行的排序規則是優先考慮大於給定閾值𝛿的 QoS 保證率，然後按 QPS 降序排序。𝛿可以根據業務的需求進行調整（默認為 0.99），衡量對 QoS 違例的容忍度。𝛿越接近 1，容忍度越低。最後，我們將排序後的第一個 QPS 指定為當前實例數可以處理的最大流量（第 21 行）。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//ec7e3255dedbe18d3dafe20203fc0215.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;模型訓練校準&lt;/strong&gt;：直接根據監控日誌構建的性能模型表可能不準確。當應用程序設置的彈性伸縮參數不合理時（例如水平擴容步長較大或資源冗餘過多），可能會導致某些實例數沒有監控日誌，或者數據量極少。這可能會導致最初構建的表中條目丟失或不準確。 為瞭解決這個問題，我們首先保證原表數據保持非嚴格單調增長，空的或較低的 QPS 被之前較高的 QPS 替換。 然後，對於實例數量增加但 QPS 不變的部分，我們根據相鄰 QPS 計算斜率並更新它們。 例如，初始化的性能模型映射為{5 : 30, 7 : 20, 8 : 60}，使用實例 5 的 QPS 替換實例 6 和 7 後，則變為{5 : 30,6 : 30,7 : 30,8 : 60}。 然後，根據實例 5 和 8 之間的 QPS 差異，我們更新實例 6 和 7 的 QPS，得到{5 : 30,6 : 40,7 : 50,8 : 60}。 此外，我們在每天低峯時段使用最新的監控日誌定期重建性能模型，以保持準確性。&lt;/p&gt; 
&lt;h3&gt;3.3 Hybrid Auto-scaling 方案設計&lt;/h3&gt; 
&lt;p&gt;除了預測伸縮，我們還設計了基於排隊論的響應式兜底策略，用於處理由於預測不準或熱點事件負載突增導致的 QoS 違例。PASS 實時監控應用程序的性能指標。如果檢測到 QoS 違例，PASS 將根據 M/M/s 排隊論模型修改預測的 QPS，並重新查詢性能模型以快速擴容適當數量的實例。 具體而言，排隊論模型如下公式所示：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//27e47098c8346bda5703849d788e8ca1.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;𝑠表示當前實例的數量，𝑢代表每個實例的瓶頸 QPS，𝑝表示延遲的百分比，以及𝑡指的𝑝百分位尾延遲。使用排隊理論來估計 QPS 而不是延遲的原因是，從排隊論推導出的延遲往往低於實際值（2.2 節中排隊論的 QoS 保障率偏低證明瞭這一點），因此從實際延遲推導出的 QPS 將高於實際值。我們基於更高的 QPS 進行擴容，以快速響應 QoS 違例並將損失降至最低。並且我們通過實驗表明，這部分資源冗餘並不會導致大量浪費。&lt;/p&gt; 
&lt;h2&gt;4 測試結果&lt;/h2&gt; 
&lt;h3&gt;4.1 實驗環境&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;應用選擇&lt;/strong&gt;：我們從美團的各個業務線中隨機抽取了 225 個應用，以驗證預測模型的準確性。這些應用的歷史 QPS 數據是從美團內部的統一在線監控平台獲得的。我們根據數據的預測難度將其分為三個不同的級別。其中，164 個服務被定義為簡單（具有單一波形模式，並表現出&quot;單峯和多峯&quot;特徵），48 個服務被定義為中等難度（具有單一波形模式，並表現出&quot;尖峯&quot;或&quot;方形&quot;特徵），13 個服務被定義為難（混合波形模式，例如&quot;尖峯+方形&quot;）。我們還選擇了幾個具有代表性的應用進行端到端評估。這些應用是後端服務，提供 C 端用户基本屬性、用户行為查詢、搜索和聊天功能。我們記錄了在線請求流量，並在離線環境中回放，以恢復真實的在線環境。為了減少時間和資源成本，我們對記錄的流量進行了切片，忽略了無法觸發彈性伸縮的長期穩定的低峯值負載。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;對比 Baseline&lt;/strong&gt;：我們比較了各種類型的 SOTA 預測算法和彈性伸縮方法。預測算法包括：離線算法（週期因子、Prophet）和在線算法（LSTNet、PatchTST 和 TIDE）。我們評估的在線算法旨在預測從當前時刻開始的第三個時間點的值（時間粒度為 5min，也就是預測未來 15min），即 horizon 等於 3。這足以滿足預測伸縮的要求，因為絕大多數的應用實例能夠在 15 分鐘以內完成啓動。彈性伸縮方法包括：目標跟蹤和 AHPA。由於目標跟蹤通常可以比具有相同參數的閾值法更快地擴展到目標範圍（第 2.2 節也表明目標跟蹤的性能更好），我們沒有比較閾值法。其中 AHPA 是阿里巴巴基於排隊論的 SOTA 預測彈性伸縮方法。&lt;/p&gt; 
&lt;h3&gt;4.2 預測算法評估&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;結論 1&lt;/strong&gt;：ELPA 預測框架結合了在線和離線模型的優勢，在絕大多數場景中都取得最好的預測準確度。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//248d26761d7dd534067d4a7eb42f8ba5.png&quot; alt=&quot;表 2 各種預測算法在各種數據集上的準確度總結，分為三個預測難度級別&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;（每一行和每一列分別比較所有方法在特定級別的數據集和特定指標上的結果。粗體突出顯示每個指標的最佳結果；𝑀𝑒𝑡𝑟𝑖𝑐𝑠_𝑎𝑣𝑔表示當前難度級別數據集的平均預測結果，而𝑊𝑖𝑛𝑛𝑒𝑟顯示了按數據集類型評估的方法在準確性方面優於其他方法的比例）&lt;/p&gt; 
&lt;p&gt;我們對 ELPA 以及其他五種廣泛使用的預測算法進行了評估。表 2 顯示了各種預測算法的準確度比較。ELPA 明顯優於單個預測算法，由於其實現了一組優化規則，用於選擇最合適的在線/離線組合，並使用振幅校準來預測特定時間段的數據。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;結論 2&lt;/strong&gt;：離線模型雖然擅長捕捉數據的&quot;突變特徵&quot;，但其預測結果和真實值存在顯著差異。而在線模型在有效預測這些&quot;突變特徵&quot;方面面臨挑戰。通過兩個模型的集成和振幅校準的應用，ELPA 框架表現出顯著的魯棒性。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//f185367c2eb6c4fafef2983a5f92b637.png&quot; alt=&quot;圖 4 在線模型、離線模型（包括振幅調整）和 ELPA 的預測實例化展示&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;圖 4 提供了 ELPA 的示例，其中結合了在離線預測模型，並對離線模型進行振幅調整。需要注意的是，&quot;突變特徵&quot;可以在各種類型的業務流量中表現出來。由於篇幅限制，我們選擇了一個具有代表性的業務負載數據集來突出 ELPA 的穩健預測能力。圖 4(a) 比較了在線模型（PatchTST）的預測與真實數據。雖然在線模型在大多數情況下顯示出很高的預測精度，但在預測&quot;突變特徵&quot;時存在顯著的滯後問題（黑色圓圈突出顯示）。在圖 4(b) 中，藍色虛線代表離線模型（週期因子）的預測結果，儘管它在大多數情況下和真實值有一定的差距，但及時地預測出了&quot;突變特徵&quot;。圖 4(b) 中的紅色虛線表示 ELPA 在離線模型的&quot;突變特徵&quot;處的振幅校準結果。最後，圖 4(c) 展示了 ELPA 的結果，它集成了在線模型和校準的離線模型，表現出準確的預測效果和預測&quot;突變特徵&quot;的能力。&lt;/p&gt; 
&lt;h3&gt;4.3 端到端效果評估&lt;/h3&gt; 
&lt;p&gt;結論 3：PASS 的性能模型準確有效，在所有測試場景中都實現了最高的 QoS 保障率（表 3）。 與目標跟蹤和 AHPA 相比，平均 QoS 保障率分別提高了 5.54% 和 7.71%。在多個 QoS 指標的場景 6 中表現更為明顯，PASS 的 QoS 保障率分別提高了 19.21% 和 22.64%。PASS 在所有場景中的平均資源成本也是最低的。 平均資源成本較 AHPA 降低 8.91%，較目標跟蹤降低 17.02%。在場景 4 中降低了高達 40% 和 52.76%。只有兩個場景中 PASS 的資源成本略高於 AHPA，而在所有其他測試中，PASS 的資源成本都是最低的。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//f8042bb94099321b5b73f1379f6fae05.png&quot; alt=&quot;表 3 三種 auto-scaling 方法在不同 QoS 和測試時長（小時）場景下的端到端性能&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;（每個場景中，第一列是 QoS 保障率，第二列是資源成本（實例數*小時）； 粗體突出顯示每個指標的最佳結果）&lt;/p&gt; 
&lt;p&gt;我們從兩個方面來評估彈性伸縮方法的效果。QoS 保障率衡量了應用違反 QoS 的時間長度，其計算方法為 QoS 得到保證的持續時間與總時間長度的比值。資源成本通過計算實例數量和時間（以小時為單位）的積分得到。 端到端實驗結果如表 3 所示（測試過程詳細監控數據見圖 5 和圖 6，包括 QPS 系列、實例數以及 TP99、TP999 時延等 QoS 指標）。每個測試場景提供了 QoS 指標、測試時長以及三種方法的 QoS 保障率和資源成本。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//874fb9b6215cff21f99fb53ff95aa05c.png&quot; alt=&quot;圖 5 Scenario1 的綜合監控數據：QPS 系列、實例數和 QoS 指標&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//da94df33b5e1290de293477cd6e9d37c.png&quot; alt=&quot;圖 6 Scenario6 的綜合監控數據：QPS 系列、實例數和 QoS 指標&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;需要注意的是，應用實例啓動時並沒有進行預熱（例如數據庫連接初始化、緩存預測等），因此即使實例提前進行了擴容，初始大量的冷查詢仍然會導致尾延遲突然增加。如果業務方在其實例啓動邏輯中加入預熱，我們的 QoS 保障率將會進一步提高。&lt;/p&gt; 
&lt;h2&gt;5 經驗總結&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;不同企業的應用場景可能各不相同，場景複雜程度也不一，在實際落地過程中，一些頂會算法不一定適用我們場景，這個時候就需要我們仔細甄別，取長補短，圍繞自身場景特點進行算法的創新簡化。&lt;/li&gt; 
 &lt;li&gt;模型並不是越複雜性能越好，要結合預測的特徵與場景來選擇預測算法。&lt;/li&gt; 
 &lt;li&gt;除了模型性能外，模型的可落地性也是非常重要的，比如 LSTNet 支持同時預測多項時間序列，面對大規模服務時能夠極大的減少模型落地的開銷。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;6 合作方簡介&lt;/h2&gt; 
&lt;p&gt;中國人民大學柴雲鵬教授團隊致力於雲計算、數據庫等領域的系統研究和研發，近年團隊在系統和數據庫等領域的重要會議 ASPLOS、SOSP、HPCA、SIGMOD、VLDB、ICDE、WWW 等發表多篇高水平論文。在雲計算領域，該團隊針對資源隔離、資源分配、資源調度等核心問題，提出了一系列方法，可以提升各種複雜場景下的資源利用率，同時保障應用的服務質量，推動雲計算技術的進步。團隊高度重視科研工作的實用價值，積極推進與科技領域企業合作，針對企業面臨的核心挑戰，將創新性方法在實際系統中實現，推動研究成果的實際落地與應用，同時助力合作伙伴技術能力提升和商業價值實現。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;|&lt;/strong&gt; 關注「美團技術團隊」微信公眾號，在公眾號菜單欄對話框回覆【2024 年貨】、【2023 年貨】、【2023 年貨】、【2022 年貨】、【2021 年貨】、【2020 年貨】、【2019 年貨】、【2018 年貨】、【2017 年貨】等關鍵詞，可查看美團技術團隊歷年技術文章合集。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//020f49d3b7136e1cb000bb228bdbe36b.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;| 本文系美團技術團隊出品，著作權歸屬美團。歡迎出於分享和交流等非商業目的轉載或使用本文內容，敬請註明&quot;內容轉載自美團技術團隊&quot;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=mailto%3A%E3%80%82%E6%9C%AC%E6%96%87%E6%9C%AA%E7%BB%8F%E8%AE%B8%E5%8F%AF%EF%BC%8C%E4%B8%8D%E5%BE%97%E8%BF%9B%E8%A1%8C%E5%95%86%E4%B8%9A%E6%80%A7%E8%BD%AC%E8%BD%BD%E6%88%96%E8%80%85%E4%BD%BF%E7%94%A8%E3%80%82%E4%BB%BB%E4%BD%95%E5%95%86%E7%94%A8%E8%A1%8C%E4%B8%BA%EF%BC%8C%E8%AF%B7%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6%E8%87%B3tech%40meituan.com%E7%94%B3%E8%AF%B7%E6%8E%88%E6%9D%83%E3%80%82&quot; target=&quot;_blank&quot;&gt;。本文未經許可，不得進行商業性轉載或者使用。任何商用行為，請發送郵件至 tech@meituan.com 申請授權。&lt;/a&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/meituantech/blog/17576489</link>
            <guid isPermaLink="false">https://my.oschina.net/meituantech/blog/17576489</guid>
            <pubDate>Sun, 23 Mar 2025 02:49:00 GMT</pubDate>
            <author>原創</author>
        </item>
        <item>
            <title>Meta AI 研究主管 Joelle Pineau 計劃辭職</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Meta 人工智能研究副總裁 Joelle Pineau 宣佈，她將於 5 月離職，結束她在 Meta 的職務。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Pineau 自兩年前起擔任 Meta 人工智能研究實驗室 (FAIR) 主管，領導該實驗室在人工智能領域的前沿研究。FAIR 是由著名科學家 Yann LeCun 領導的 Meta 內部核心研究團隊。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;268&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-6ac61b95fe3a1bae2f82e2aa6f31f103781.png&quot; width=&quot;300&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Pineau 的離職正值 Meta 加大其人工智能投入的關鍵時刻。公司在 2025 年計劃投入高達 650 億美元用於人工智能基礎設施的建設，顯示出 Meta 對 AI 技術的重視和未來發展的決心。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;在接受彭博新聞的採訪時，Meta 的一位發言人表示，目前公司尚未找到 Pineau 的接替者，但正在積極尋找合適的候選人。值得注意的是，去年 Meta 曾對公司結構進行調整，使得其人工智能研究部門直接向首席產品官 Chris Cox 彙報，這一變化也反映了公司在 AI 領域發展的戰略佈局。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;對於即將到來的離職，Pineau 表示，離職後她計劃先休息一段時間，之後將開啓一場新的「冒險」，但具體計劃尚未透露。她在離職聲明中表示，自己對未來充滿期待，並期待迎接新的挑戰。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/342390</link>
            <guid isPermaLink="false">https://www.oschina.net/news/342390</guid>
            <pubDate>Sun, 23 Mar 2025 02:21:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>阿里將在 4 月第二週發佈新模型 Qwen3</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fn5SajAM_ZDqoMTvWK8jZuQ&quot; target=&quot;_blank&quot;&gt;根據虎嗅獨家報道&lt;/a&gt;&lt;/u&gt;，阿里即將在 2025 年 4 月第二週（&lt;span style=&quot;background-color:#ffffff; color:#3a3a3a&quot;&gt;即下週&lt;/span&gt;）發佈新模型 Qwen3，這將是阿里在 2025 年上半年最重要的模型產品。&lt;/p&gt; 
&lt;p&gt;報道稱，在 2024 年發佈 Qwen2.5 後，阿里雲內部的基礎模型團隊已經開始推動 Qwen3 相關項目。&lt;/p&gt; 
&lt;p&gt;但 2025 年初 DeepSeek 的火爆，改變了團隊的部分思路與重心。在 2024 年下半年，阿里雲基礎模型團隊對標的競品模型主要是 OpenAI 的 o1，而在 DeepSeek-R1 發佈後，DeepSeek-R1 已經成為了另一個主要對標模型。&lt;/p&gt; 
&lt;p&gt;知情人士表示，在阿里內部，基礎模型團隊最重要的考核維度是「模型影響力」。高層希望團隊可以在業內成功塑造「最強模型」的心智。由於阿里採取模型開源策略，基於 Qwen 開源模型的衍生模型總量，被視為一個關鍵指標。截至目前，這一數據已經超過 10 萬。&lt;/p&gt; 
&lt;p&gt;而在開發者社區的歡迎度，阿里會考慮多個具體指標，比如開源模型下載量等。據釋，2024 年 Qwen 系列模型在開發者社區的下載量超過了 2 億。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/342389</link>
            <guid isPermaLink="false">https://www.oschina.net/news/342389</guid>
            <pubDate>Sun, 23 Mar 2025 02:21:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>AI 搜索創企 Perplexity：公司資金充裕、2028 年前無 IPO 規劃</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;AI 搜索創企 Perplexity 聯合創始人兼首席執行官 Aravind Srinivas 在 Reddit &lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.reddit.com%2Fr%2Fperplexity_ai%2Fcomments%2F1jm2ekd%2Fmessage_from_aravind_cofounder_and_ceo_of%2F&quot; target=&quot;_blank&quot;&gt;發帖回應&lt;/a&gt;&lt;/u&gt;了網友近期對該企業狀況和產品的關切。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;5518&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-3ec5a737796ef123a3daf35370e97fd9c3c.png&quot; width=&quot;1494&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;在財務方面，他表示 Perplexity 目前資金充裕，收入正處於上升軌道，沒有在 2028 年前進行 IPO 計劃；此前推出的 Auto 自動選擇模型搜索模式不是為了節約成本，而是為了讓產品更好&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;對於 Auto 模式，Aravind Srinivas 表示目前包括 Perplexity 自身在內的 AI 模型開發企業正在快速推出新的模型產品，一個一個地添加搜索模型選項是不可持續的，Auto 模式簡化了用户的學習使用流程。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-07e1f07e2d722d7b2a07a6979e9844b1637.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Perplexity 還正在開發一種更為強大的深度研究智能體，可進行 30 分鐘乃至更長時間的思考。該企業正為此大規模重寫基礎架構，以便大規模實現這一智能體所需的人機交互、工具使用、代碼執行能力。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;閲讀更多&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/news/340590/perplexity-rebuilding-tiktok-in-america&quot; target=&quot;news&quot;&gt;Perplexity 欲收購 TikTok 並開源其算法&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/news/335658/perplexity-teases-a-web-browser-called-comet&quot; target=&quot;news&quot;&gt;AI 搜索引擎 Perplexity 將開發「代理搜索」 Web 瀏覽器 Comet&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/342330/perplexity-no-ipo-before-2028</link>
            <guid isPermaLink="false">https://www.oschina.net/news/342330/perplexity-no-ipo-before-2028</guid>
            <pubDate>Sat, 22 Mar 2025 11:44:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>MeiliSearch AI 發佈：集語義、混合搜索為一體，提供多模態能力</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;MeiliSearch 原本是一個 Rust 開發的 ElasticSearch 開源替代品，在&lt;strong&gt;開發體驗、部署難度、性能可用性&lt;/strong&gt;上，都是相當有競爭力的輕量級選擇。&lt;/p&gt; 
&lt;p&gt;MeiliSearch 近日發佈了&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.meilisearch.com%2Fblog%2Fintroducing-meilisearch-ai&quot; target=&quot;_blank&quot;&gt; MeiliSearch AI&lt;/a&gt;&lt;/u&gt;，這是 Meilisearch 的新版本，具有以下特點：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;語義搜索&lt;/strong&gt;&lt;/strong&gt;：不僅匹配關鍵詞，還能理解搜索意圖。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;混合搜索&lt;/strong&gt;&lt;/strong&gt;：結合全文搜索與 AI 向量搜索，提高搜索精準度。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;多模態能力&lt;/strong&gt;&lt;/strong&gt;：支持圖片搜索等多種搜索模式。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;內置向量數據庫&lt;/strong&gt;&lt;/strong&gt;：無需額外搭建向量數據庫，簡化基礎設施。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;優化性能&lt;/strong&gt;&lt;/strong&gt;：搜索結果極速返回，延遲低於 50 毫秒。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0401/182254_M83f_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;在 AI 時代，搜索早已不是簡單的「關鍵詞匹配」，而是成為了用户獲取知識、信息、甚至創意的&lt;strong&gt;重要入口，特別是向量數據庫為如今熱火朝天的 RAG 應用提供了更多的選擇&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;Meilisearch AI 這些新特性，在當前和未來的應用中具有非常大的價值。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;全文搜索：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;優點：&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;糾正拼寫錯誤&lt;/strong&gt;：例如，將「reutrn of the jedi」糾正為「return of the jedi」。&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;處理精確查詢&lt;/strong&gt;：適用於輸入確切產品名稱的情況。&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;處理不完整查詢&lt;/strong&gt;：如輸入「return of the j」也能找到相關結果。&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;缺點：&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;無法處理模糊查詢&lt;/strong&gt;：例如，輸入「拿着光劍戰鬥的人」時，可能無法找到相關結果。&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;缺乏上下文理解&lt;/strong&gt;：如搜索「冬季服裝」時，可能無法全面理解用户需求。&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;向量搜索：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;優點：&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;處理模糊查詢&lt;/strong&gt;：例如，輸入「第一部上映的星球大戰電影」時，能夠找到相關結果。&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;理解上下文&lt;/strong&gt;：如搜索「冬季服裝」時，能理解並提供相關的衣物推薦。&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;推薦相似文檔&lt;/strong&gt;：能夠根據語義相似性推薦相關內容。&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;缺點：&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;無法處理精確查詢&lt;/strong&gt;：對於需要精確匹配的查詢，向量搜索可能不夠準確。&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;資源需求較高&lt;/strong&gt;：計算和存儲向量表示需要更多的計算資源和存儲空間。&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;鑑於兩者各有優缺點，&lt;strong&gt;混合搜索&lt;/strong&gt;應運而生。混合搜索結合了全文搜索的詞彙匹配能力和向量搜索的語義理解能力，提供更全面和精確的搜索體驗。&lt;/p&gt; 
&lt;p&gt;例如，Meilisearch 的混合搜索功能允許開發者通過統一的 API 實現這一結合。在 Meilisearch 中，您可以通過設置&amp;nbsp;&lt;code&gt;hybrid&lt;/code&gt;&amp;nbsp;參數來配置混合搜索，調整&amp;nbsp;&lt;code&gt;semanticRatio&lt;/code&gt;&amp;nbsp;的值，以平衡全文搜索和向量搜索的比重，從而優化搜索結果的相關性和精確度。&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;// Hybrid search with Meilisearch

const results = await client.multiSearch({
queries: [{ 
indexUid: &#39;movies&#39;, 
q: &#39;batman&#39;,
hybrid: { embedder: &#39;default&#39;, semanticRatio: 0.5 }
}]
})&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;通過結合兩種搜索方法的優勢，混合搜索為用户提供了更智能、更高效的搜索體驗。&lt;/p&gt; 
&lt;p&gt;關於向量數據庫，MeiliSearch 專門寫了一篇博客進行介紹：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.meilisearch.com%2Fblog%2Fwhat-are-vector-embeddings&quot; target=&quot;_blank&quot;&gt;https://www.meilisearch.com/blog/what-are-vector-embeddings&lt;/a&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;相關文檔：&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.meilisearch.com%2Fdocs%2Flearn%2Fai_powered_search%2Fgetting_started_with_ai_search%23getting-started-with-ai-powered-search&quot; target=&quot;_blank&quot;&gt;https://www.meilisearch.com/docs/learn/ai_powered_search/getting_started_with_ai_search#getting-started-with-ai-powered-search&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.meilisearch.com%2Fblog%2Ffull-text-search-vs-vector-search&quot; target=&quot;_blank&quot;&gt;https://www.meilisearch.com/blog/full-text-search-vs-vector-search&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/342317/meilisearch-ai</link>
            <guid isPermaLink="false">https://www.oschina.net/news/342317/meilisearch-ai</guid>
            <pubDate>Sat, 22 Mar 2025 10:27:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>黑莓手機有望迴歸：搭載 Android 15、支持 AI</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;Reddit 上的一篇帖子&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.reddit.com%2Fr%2Fblackberry%2Fcomments%2F1jmalqp%2Fcomment%2Fmkgfbpi%2F&quot; target=&quot;_blank&quot;&gt;透露&lt;/a&gt;&lt;/u&gt;，一家英國的初創公司正悄悄努力復活 Blackberry Classic 及 Onward Mobility 未完成的產品。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-df07f63db4adb240ebafad1ec113d909b4f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;Reddit 用户 u/coldheartedsigma 在黑莓 subreddit 分享了這一消息，但由於簽署了保密協議，未能透露具體的品牌名稱或展示完整設計。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;帖子提到，新設備將具備 5G、AMOLED 顯示屏、12GB RAM 和 256GB 或 512GB 存儲空間等高端配置。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;據稱，新版黑莓將運行 Android 15 系統，並支持一定程度的生成式 AI 技術，同時配備電容式鍵盤。&lt;/p&gt; 
&lt;p&gt;此外，還有一款基於 QWERTY 鍵盤的新設備正在規劃中，該初創公司正與黑莓洽談專利獨家許可事宜。&lt;/p&gt; 
&lt;p&gt;雖然 u/coldheartedsigma 分享了一張圖片，但幾乎無法辨認任何有用信息。&lt;strong&gt;從模糊的圖像中只能勉強看出「Blackberry Patents」、「QWERTY」和「The world&#39;s first」這幾個詞。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-e7b1756db87a99da9db420f8d3d4fa31820.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;對於那些期待黑莓迴歸的人來説，這家初創公司即使成功開發出類似黑莓的設備，也可能不會使用黑莓的名字。&lt;/p&gt; 
&lt;p&gt;因為黑莓已退出智能手機市場，所以這家公司如果真的推出產品，那也只是一款外觀相似的手機，而非真正意義上的「黑莓」手機。不過，這也給懷念經典 QWERTY 鍵盤手機的用户帶來了一絲希望。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/342306/reddit-says-blackberry-is-back</link>
            <guid isPermaLink="false">https://www.oschina.net/news/342306/reddit-says-blackberry-is-back</guid>
            <pubDate>Sat, 22 Mar 2025 09:55:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>AI 時代軟件供應鏈面臨重大安全危機：機密泄露激增 64%</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;JFrog 公司近日發佈了《2025 年軟件供應鏈現狀報告》，揭示了在人工智能（AI）迅速發展的背景下，軟件供應鏈所面臨的嚴峻安全挑戰。根據該報告，研究團隊通過對 1400 多名專業人士的調研，以及來自 7000 多家客户的數據分析，勾勒出了一幅令人為之擔憂的安全圖景。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;報告指出，過去一年中，軟件供應鏈的安全漏洞急劇增加，其中 「秘密」 或機密信息的曝光案例同比增長了 64%，總計達到了驚人的 25229 例。這一數據表明，隨着企業對機器學習（ML）模型的依賴加深，安全風險也在不斷上升。儘管 94% 的公司表示使用認證清單來管理 ML 模型，但其中 37% 的公司仍依賴手動方式進行驗證，顯然這加大了安全隱患。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;329&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-099c553624560d1a8503e298c286617a7f9.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;與此同時，2024 年新增的安全漏洞（CVE）數量也高達 33000 個，相比 2023 年增加了 27%。令人擔憂的是，只有 12% 的 CVE 被證實真的具有 「嚴重」 級別，這或許反映出評分系統存在 「膨脹」 現象，可能導致開發者面臨不必要的修復壓力和工作疲憊。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;JFrog 的首席技術官 Yoav Landman 指出，儘管許多組織在積極採用公共 ML 模型推動創新，但缺乏自動化的工具鏈和治理流程使得安全管理愈加複雜。他呼籲，企業在快速發展的 AI 環境中，應加速自動化轉型，以確保在提升創新潛力的同時，也能保障軟件的安全性。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;整體來看，當前的軟件供應鏈安全問題不僅是技術上的挑戰，更是企業管理與運營方式的考驗。在 AI 時代，建立更為嚴密的安全防護措施，已經成為各大企業亟需面對的任務。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/342305</link>
            <guid isPermaLink="false">https://www.oschina.net/news/342305</guid>
            <pubDate>Sat, 22 Mar 2025 09:54:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
        <item>
            <title>亞馬遜發佈可控制 Web 瀏覽器的 AI 智能體 Nova Act</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;亞馬遜發佈了 Nova Act，這是一款通用 AI 代理，可以控制網絡瀏覽器並獨立執行一些簡單的操作。除了新的代理 AI 模型外，亞馬遜還發布了 Nova Act SDK，這是一個工具包，允許開發人員使用 Nova Act 構建代理原型。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-e2681b71429ea88e9c90005a9abadc4007f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Nova Act 由亞馬遜新近在舊金山開設的 AGI 實驗室開發，還將為該公司即將推出的 Alexa+ 升級版提供關鍵功能，Alexa+ 是亞馬遜廣受歡迎的語音助手的生成式 AI 增強版。不過，從今天開始提供的 Nova Act 版本略顯遜色。亞馬遜稱其為研究預覽版。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-d1adc5835a26d72e30965641b814c1462f0.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;開發人員可以通過新網站 nova.amazon.com 訪問 Nova Act 工具包，該網站也是亞馬遜各種 Nova 基礎模型的展示。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-2f6e472c5dea52938cb18c4e2cde8f294d4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Nova Act 是亞馬遜試圖利用自己的通用人工智能代理技術與 OpenAI 的 Operator 和 Anthropic 的 Computer Use 競爭的嘗試。幾家領先的科技公司認為，能夠為用户導航網絡的人工智能代理將使當今的人工智能聊天機器人更加有用。&lt;/p&gt; 
&lt;p&gt;亞馬遜可能不是第一個開發這種代理技術的公司，但通過 Alexa+，它的覆蓋範圍可能是最廣泛的。&lt;/p&gt; 
&lt;p&gt;亞馬遜表示，使用 Nova Act SDK 進行開發的開發人員應該能夠代表用户自動執行基本操作，例如從 Sweetgreen 訂購沙拉或預訂晚餐。藉助 Nova Act 工具包，開發人員可以整合工具，讓 AI 代理瀏覽網頁、填寫表格或在日曆上選擇日期。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-e7474d0e9288f2459b2a8abe46b37bc9287.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;亞馬遜聲稱，Nova Act 在公司內部的幾項測試中表現優於 OpenAI 和 Anthropic 的代理。例如，在衡量 AI 代理如何與屏幕上的文本交互的 ScreenSpot Web Text 中，Nova Act 得分為 94%，優於 OpenAI 的 CUA（得分為 88%）和 Anthropic 的 Claude 3.7 Sonnet（90%）。&lt;/p&gt; 
&lt;p&gt;不過，亞馬遜並沒有使用更常見的代理評估（例如 WebVoyager）來對 Nova Act 進行基準測試。&lt;/p&gt; 
&lt;p&gt;Nova Act 是亞馬遜上述 AGI 實驗室推出的首款公開產品，該項目由前 OpenAI 研究員 David Luan 和 Pieter Abbeel 共同領導。兩人之前都創立過自己的初創公司——Luan 創辦了 Adept，而 Abbeel 共同創辦了 Covariant——去年亞馬遜聘請他們來領導其 AI 代理工作。&lt;/p&gt; 
&lt;p&gt;雖然 AGI 實驗室開發能夠訂購 SweetGreen 的 AI 代理似乎有些奇怪，但 Luan 認為代理是創建超級智能 AI 系統的關鍵一步。Luan 將 AGI 定義為「一種能夠幫助您完成人類在計算機上所做的一切的 AI 系統」。&lt;/p&gt; 
&lt;p&gt;Luan 表示，他的團隊設計了 Nova Act SDK，以可靠地自動執行簡短的任務，併為開發人員提供工具，讓他們能夠精確定義何時需要人工幹預代理工作流程。他希望，這將使開發人員能夠創建更可靠的代理應用程序，儘管不一定是完全自主的應用程序。&lt;/p&gt; 
&lt;p&gt;亞馬遜在競爭激烈的市場中推出了首款通用人工智能代理，但這是該公司寄予厚望的一項關鍵技術。Nova Act 的早期測試可以讓人們一窺拖延已久的 Alexa+ 的一些功能，這對亞馬遜的人工智能努力來説是一個成敗攸關的時刻。&lt;/p&gt; 
&lt;p&gt;OpenAI、Google 和 Anthropic 的早期人工智能代理的主要問題是它們在不同領域的可靠性。在 TechCrunch 的測試中，這些系統速度很慢，難以長時間獨立運行，而且容易犯人類不會犯的錯誤。我們很快就會看到亞馬遜是否破解了密碼——或者它的代理是否也存在困擾競爭對手的同樣缺陷。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;相關鏈接&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.aboutamazon.com%2Fnews%2Finnovation-at-amazon%2Famazon-nova-website-sdk%E3%80%81https%3A%2F%2Fgithub.com%2Faws%2Fnova-act&quot; target=&quot;_blank&quot;&gt;https://www.aboutamazon.com/news/innovation-at-amazon/amazon-nova-website-sdk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.aboutamazon.com%2Fnews%2Finnovation-at-amazon%2Famazon-nova-website-sdk%E3%80%81https%3A%2F%2Fgithub.com%2Faws%2Fnova-act&quot; target=&quot;_blank&quot;&gt;https://github.com/aws/nova-act&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/342304/amazon-nova-website-sdk</link>
            <guid isPermaLink="false">https://www.oschina.net/news/342304/amazon-nova-website-sdk</guid>
            <pubDate>Sat, 22 Mar 2025 09:39:00 GMT</pubDate>
            <author>來源: OSCHINA</author>
        </item>
    </channel>
</rss>