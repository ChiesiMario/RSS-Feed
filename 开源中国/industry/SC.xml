<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - industry - 简体中文</title>
    <link>https://www.oschina.net/news/industry</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news/industry" rel="self" type="application/rss+xml"/>
    <description>已对该 RSS 进行格式化操作：中英字符之间插入空格、使用直角引号、标点符号修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 16 Sep 2025 21:40:21 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>北京拥有全球最密集重大科技基础设施、最大科研人才队伍</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbaijiahao.baidu.com%2Fs%3Fid%3D1843300877733716689%26wfr%3Dspider%26for%3Dpc" target="_blank"&gt;根据《北京日报》的报道&lt;/a&gt;，北京市发改委主任杨秀玲介绍，五年来，北京率先整合设立教育科技人才工作领导小组，着力提升创新体系整体效能，努力打造新质生产力的重要发动机。&lt;/p&gt; 
&lt;p&gt;现在的北京，已经拥有全球最密集的重大科技基础设施、最多人次的高被引科学家数量、最大规模的科研人才队伍、最具竞争力的开放创新生态。&lt;/p&gt; 
&lt;p&gt;这五年，中关村世界领先科技园区加快建设，「三城一区」主平台作用进一步凸显，国家实验室高质量在轨运行，全国重点实验室 145 家、占总量的近三成，全社会研发投入强度保持在 6% 左右、位居全球创新城市前列，入选中国科学十大进展的成果数量占全国半数以上。&lt;/p&gt; 
&lt;p&gt;这五年，北京抢抓产业变革机遇，打造「人工智能第一城」，累计备案上线大模型 158 款、全国占比约三成，豆包、智谱、kimi 等标杆模型性能稳居全球第一梯队；&lt;/p&gt; 
&lt;p&gt;商业航天产业培育壮大，拥有全国一半的核心研发单位、上市企业和独角兽企业，「朱雀三号」可重复使用运载火箭顺利完成一级动力系统试车；&lt;/p&gt; 
&lt;p&gt;近 200 种机器创新产品在 130 多种场景实现应用落地，机器人技术逐渐走出实验室、走进百姓生活；&lt;/p&gt; 
&lt;p&gt;系统布局未来产业，实现 6G 超宽带光电融合集成系统、「夸父」量子计算云平台等技术突破。&lt;/p&gt; 
&lt;p&gt;「五年来，我们坚持以经济体制改革为牵引，强化重点领域和关键环节改革攻坚，市场环境更加公平、企业经营更有活力。」杨秀玲说，要素配置更加高效，北京证券交易所开市；民营经济高质量发展，世界 500 强榜单北京上榜民营企业 6 家、居全国城市首位；「北京服务」更加贴心，迭代出台改革措施 1700 余项，非现场监管覆盖率近 70%，经营主体 2021-2024 年年均增长 6.2%、总量达到 268.6 万户。&lt;/p&gt; 
&lt;p&gt;五年来，本市大力推进高水平对外开放，主动为国家开放发展试制度，累计实施突破性政策 140 余项，全市货物贸易进出口规模连续三年超过 3.6 万亿，服务贸易 2021-2024 年年均增速达 9.4%。新落户国际组织 37 家，新设外资企业超 8000 家，综保区由 1 个扩大到 4 个。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/372601</link>
      <guid isPermaLink="false">https://www.oschina.net/news/372601</guid>
      <pubDate>Sat, 13 Sep 2025 11:38:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>TinyLisp — 99 行 C 代码实现的完整 Lisp 解释器</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                        
                                                                                    &lt;p&gt;TinyLisp 是用 99 行 C 代码实现的完整 Lisp 解释器，包含了 21 个内置函数、垃圾回收机制和 REPL 交互环境，甚至还能在 1980 年代的掌上电脑上运行，只需一行命令即可编译运行。&lt;/p&gt;

&lt;p&gt;主要特性&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;支持函数式编程、闭包、宏等高级特性&lt;/li&gt;
&lt;li&gt;内置简单垃圾回收机制和 REPL 环境&lt;/li&gt;
&lt;li&gt;配有详细技术文章解释实现原理&lt;/li&gt;
&lt;li&gt;多个优化版本适应不同性能需求&lt;/li&gt;
&lt;li&gt;能在 Sharp PC-G850 等古董设备上运行&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-5b51d9dbe46e01a62723005a08a00a27c4f.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-f8e89c8e5f5f4ec1b4d38c18fdc1a45b06b.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

                                                                    &lt;/div&gt;
                                                                </description>
      <link>https://www.oschina.net/p/tinylisp</link>
      <guid isPermaLink="false">https://www.oschina.net/p/tinylisp</guid>
      <pubDate>Sat, 13 Sep 2025 10:57:00 GMT</pubDate>
    </item>
    <item>
      <title>清华联手上海 AI Lab 发布开源 SimpleVLA-RL 框架</title>
      <description/>
      <link>https://www.oschina.net/news/372593</link>
      <guid isPermaLink="false">https://www.oschina.net/news/372593</guid>
      <pubDate>Sat, 13 Sep 2025 10:45:00 GMT</pubDate>
    </item>
    <item>
      <title>阿里上榜全球创新人才最佳雇主：AI 原生应用 Accio 备受关注</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;全球知名商业媒体《快公司》发布 2025 年度「创新人才最佳雇主」榜单，阿里巴巴成排名最高的中国科技公司。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="283" src="https://oscimg.oschina.net/oscnet/up-816fc66a4d8be8e5abdbe06b3898f9b8316.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;《快公司》「创新人才最佳雇主」榜旨在表彰重视员工创新并为团队创造前瞻性工作环境的企业。《快公司》称，阿里此次上榜缘于对 AI 原生应用 Accio 的创新探索。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;据悉，Accio 是全球首个贸易领域的 AI 原生应用，由出海平台阿里国际站推出，能帮中小企业自动化地完成全球采购流程，被称为「第一个会做生意的 AI Agent」。上线 9 个月来，Accio 的海外企业用户数快速突破 200 万，创新性的体验深受中小企业欢迎。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/372592</link>
      <guid isPermaLink="false">https://www.oschina.net/news/372592</guid>
      <pubDate>Sat, 13 Sep 2025 10:29:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>昆仑万维旗下 AI 音乐创作平台 Mureka 「Agent Studio」</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span&gt;昆仑万维旗下 AI 音乐创作平台 Mureka&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FLNjbrW6yNoKFmnPnywxiVg" target="_blank"&gt;上线&lt;/a&gt;了全新功能——&lt;strong&gt;Agent Studio。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="544" src="https://static.oschina.net/uploads/space/2025/0916/182234_7QJo_2720166.jpg" width="1280" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;该功能通过直观的方式让音乐创作变得触手可及，用户只需简单描述想法，AI 就能自动生成歌词和音乐。&lt;/p&gt; 
&lt;p&gt;Agent Studio 包含多个创作场景，如专辑制作、热点写歌等，为用户提供多样化的音乐体验。&lt;/p&gt; 
&lt;p&gt;-&amp;nbsp;Make Album（创作专辑）：不仅是一首歌，还能基于任意概念生成一整张专辑&lt;br&gt; -&amp;nbsp;Tarot Tunes（塔罗音愈）：问一个问题，AI 用歌来回答，像抽音乐塔罗&lt;br&gt; -&amp;nbsp;Buzz Tracks（热点写歌）：把热点新闻、流行梗做成歌，抓住当下&lt;br&gt; -&amp;nbsp;Diss Tracks（Diss 制造机）：写一首犀利的 Diss 歌，battle 随时开局&lt;br&gt; -&amp;nbsp;Gift a Song（以歌致礼）：专门为朋友、恋人、家人写的礼物歌&lt;br&gt; -&amp;nbsp;Spicy Song（撩人情歌）：大胆、暧昧，适合恋人之间的「私密歌」&lt;/p&gt; 
&lt;p&gt;&lt;img height="602" src="https://static.oschina.net/uploads/space/2025/0916/182334_t0iW_2720166.png" width="1116" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;体验 Mureka Agent Studio：&lt;em&gt;www.mureka.ai&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/372591</link>
      <guid isPermaLink="false">https://www.oschina.net/news/372591</guid>
      <pubDate>Sat, 13 Sep 2025 10:24:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>腾讯：AI 能力全面开放，全面适配主流国产芯片</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;9 月 16 日，在 2025 腾讯全球数字生态大会主峰会上，腾讯公布多项 AI 技术和产品最新进展，&lt;strong&gt;并宣布通过腾讯云全面开放腾讯 AI 落地能力及优势场景&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0916/181313_pacO_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;面对各界关注的算力问题，腾讯集团副总裁、腾讯云总裁邱跃鹏宣布，目前腾讯云已经全面适配主流的国产芯片，并积极参与和回馈开源社区。&lt;/p&gt; 
&lt;p&gt;与此同时，软硬件协同全栈优化是腾讯云的长期战略投入，通过异构计算平台的软件能力，整合不同类型的芯片对外提供高性价比的 Al 算力。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/372588</link>
      <guid isPermaLink="false">https://www.oschina.net/news/372588</guid>
      <pubDate>Sat, 13 Sep 2025 10:15:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>腾讯混元 3D 3.0 模型发布</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;腾讯&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FXzJIt8glOd82pVs_YXjf6w" target="_blank"&gt;宣布&lt;/a&gt;推出混元 3D 3.0 模型，其建模精度较前代提升 3 倍，可实现超高清 3D 内容创作，人物面部轮廓、纹理细节等呈现真人手办级效果。该模型采用首创的 3D-DiT 分级雕刻技术，通过「先搭结构后雕细节」的逻辑，优化复杂几何结构处理与纹理贴合度。&lt;/p&gt; 
&lt;p&gt;&lt;img height="607" src="https://static.oschina.net/uploads/space/2025/0916/180622_SG6I_2720166.jpg" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;混元 3D 3.0 已集成至混元 3D AI 创作引擎供用户免费使用，同步上线腾讯云 API，为游戏、影视、电商等行业提供专业 3D 内容生产能力。同期，支持七大核心制作环节的混元 3D Studio 专业工作台开启内测，多条件控制的混元 3D Omni 模型亦将于近期开源。&lt;/p&gt; 
&lt;p&gt;数据显示，腾讯混元 3D 系列模型在社区下载量已超 260 万，拓竹科技、创想三维等 3D 打印厂商已应用该技术实现效率提升。&lt;/p&gt; 
&lt;p&gt;体验：&lt;em&gt;https://3d.hunyuan.tencent.com/&lt;/em&gt;（混元 3D 创作引擎提供每日 20 次免费生成额度）&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/372585</link>
      <guid isPermaLink="false">https://www.oschina.net/news/372585</guid>
      <pubDate>Sat, 13 Sep 2025 10:06:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>MoonBit 正式加入 WebAssembly Component Model 官方文档</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;p&gt;&lt;img src="https://picx.zhimg.com/80/v2-07327fbdaa6f7471b79521f6d668e7fd_720w.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;我们非常高兴地宣布， &lt;strong&gt;MoonBit 已正式收录在 WebAssembly Component Model 的官方文档中&lt;/strong&gt; 。这不仅是对 MoonBit 技术路线的一次肯定，也让我们有机会和 Rust、Go、C# 等语言一起，出现在开发者查阅组件模型的入口页面中。&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;一、&lt;/em&gt; &lt;strong&gt;关于 WebAssembly Component Model&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;WebAssembly Component Model 是 Wasm 生态正在推进的一个核心标准，它的目标是让不同语言编写的组件能够无缝组合、运行和分发。这个模型提供了一套跨语言通用的接口规则，让开发者 &lt;strong&gt;不需要关心组件背后的语言实现&lt;/strong&gt; ，就能把它们拼装在一起，从而逐步成为 Wasm 世界的「通用接口层」。&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;二、&lt;/em&gt; &lt;strong&gt;关于 MoonBit&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;MoonBit 作为新一代 AI 原生编程语言，已经在 Wasm 后端上做了大量探索，包括 &lt;strong&gt;更小的代码体积&lt;/strong&gt; 、 &lt;strong&gt;更高的运行效率&lt;/strong&gt; ，以及对 &lt;strong&gt;现代编译器架构的支持&lt;/strong&gt; 。被收录到 WebAssembly Component Model 文档中，意味着语言以及工具链已经具备了进入跨语言互操作生态的能力，也得到了 Wasm 社区的认可。&lt;/p&gt; 
&lt;p&gt;接下来，我们会继续投入在：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;多后端支持&lt;/strong&gt; ：Wasm、JavaScript、Native、RISC-V 等；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;组件模型集成&lt;/strong&gt; ：提供更完善的接口定义与跨语言调用能力；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;AI 驱动工具链&lt;/strong&gt; ：让组件的构建、运行、调试过程更高效。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;MoonBit 相信：AI 时代需要新的编程语言和开发者平台，而我们正与全球开发者一起，共同建设这样的未来。&lt;/p&gt; 
&lt;p&gt;感谢大家一路以来的陪伴与信任。让我们继续努力，把 MoonBit 打造成 AI 时代最具活力与创造力的编程平台！&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://picx.zhimg.com/80/v2-31065b175c3b993e841a5738ec875a7f_720w.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;👉 如果你也在关注 Wasm Component Model，欢迎尝试用 MoonBit 来构建你的第一个组件，并和我们分享体验！&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcomponent-model.bytecodealliance.org%2F" target="_blank"&gt;https://component-model.bytecodealliance.org/&lt;/a&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/7007853/blog/18692079</link>
      <guid isPermaLink="false">https://my.oschina.net/u/7007853/blog/18692079</guid>
      <pubDate>Sat, 13 Sep 2025 10:02:00 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>命令行 AI 编程工具 Codex CLI 已集成全新 GPT-5-Codex 模型</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;OpenAI 为旗下命令行工具 Codex CLI 发布了&amp;nbsp;0.36.0，新版本集成了强大的&lt;a href="https://www.oschina.net/news/372453" target="_blank"&gt; GPT-5-Codex 模型&lt;/a&gt;，显著提升了代码生成速度、推理深度与质量。此次更新还引入了通过 AGENTS.md 文件精细控制其行为的能力。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0916/103155_65FN_2720166.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Codex CLI 是一个轻量级的 AI 编程助手，采用 TypeScript 和 Node.js 编写，可以直接在用户的终端命令行运行，旨在充分发挥 AI 模型强大的推理能力，连接本地代码环境，甚至支持处理截图或草图进行多模态编程。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Codex CLI 0.36.0 版本覆盖了会话恢复、统一执行、安全与测试加固、简化登录流程、JSON-RPC 与 MCP 接口扩展，以及终端交互优化等多个方面。&lt;/p&gt; 
&lt;p&gt;此外，实验性地引入了自动上下文压缩、沙盒范围扩展和 Azure Responses API 临时解决方案。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/372581</link>
      <guid isPermaLink="false">https://www.oschina.net/news/372581</guid>
      <pubDate>Sat, 13 Sep 2025 09:52:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>2025 大模型服务性能排行榜：PPIO 吞吐测试排名第一</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#0e0e0e; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;清华大学携手中国软件评测中心联合发布了《2025 大模型服务性能排行榜》，&lt;strong&gt;PPIO 在 DeepSeek-R1-0528 的吞吐测试中排名第一&lt;/strong&gt;。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#0e0e0e; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;该榜单从延迟、吞吐、可靠性等关键指标切入，由专业团队通过长周期、高频率、多时段的数据评测，直观呈现不同 MaaS 供应商的服务表现。而且，平台以匿名用户身份对 MaaS（Model as a Service）平台开展产品端到端的性能测评，从评测主体与流程上双重保障了客观公正性。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#0e0e0e; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;本次评测覆盖多个代表性模型，包括 DeepSeek-R1-0528、DeepSeek-V3.1、Kimi-K2-Instruct 等。PPIO 在 20 余家 MaaS 供应商中表现突出。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#0e0e0e; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;其中，在 DeepSeek-R1-0528 的吞吐测试中，PPIO 以&amp;nbsp;&lt;strong&gt;45.17 tokens/s&lt;/strong&gt;&amp;nbsp;的成绩&lt;strong&gt;位列第一&lt;/strong&gt;。在 DeepSeek V3.1、Kimi-K2-Instruct 等模型测试中，PPIO 在吞吐与延迟性能上也取得了前五名的成绩。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#0e0e0e; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;img height="422" src="https://oscimg.oschina.net/oscnet/up-c2218b9a340f30f9637b049f56c84eaa7a9.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#0e0e0e; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;strong&gt;吞吐 (Throughput)&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;与&lt;strong&gt;延迟 (Latency)&lt;/strong&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;与，是评测模型表现的两个重要维度。吞吐（Throughput）衡量在单位时间内可处理的 Token 数量，高吞吐代表平台能支撑更多用户同时使用，尤其适合大规模应用场景；延迟（Latency）衡量用户从输入到获得首个回复所需的时间，低延迟意味着更快的响应速度，直接影响用户体验。&lt;/p&gt; 
&lt;p style="color:#0e0e0e; margin-left:0; margin-right:0; text-align:justify"&gt;PPIO 在这两个维度都表现优秀，不仅能提供流畅的实时交互，也能在高并发和大规模任务处理中保持稳定。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/372579</link>
      <guid isPermaLink="false">https://www.oschina.net/news/372579</guid>
      <pubDate>Sat, 13 Sep 2025 09:40:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>上海 AI Lab 推出 Lumina-DiMOO</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;上海人工智能实验室与多所知名高校合作近日推出了新一代多模态生成与理解模型 ——Lumina-DiMOO。该模型以 「全方位扩散大语言模型」 命名，旨在推动多模态 AI 技术的发展。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;Lumina-DiMOO 采用了创新的 「全离散扩散架构」，突破了传统模型在文本与图像处理上的局限，提供了更为高效的解决方案。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="296" src="https://oscimg.oschina.net/oscnet/up-8e5b57a18c865ea2e56d87cfee874f973bb.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;多模态 AI 的核心在于如何将不同类型的数据有效整合。Lumina-DiMOO 通过将文本、图像和音频等数据映射到一个共享的高维 「语义空间」，使不同模态的数据能够实现更好的理解和生成。这种方法的成功依赖于强大的对比学习技术，让模型可以识别和对齐各类数据之间的关系。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在模型设计上，Lumina-DiMOO 的 「全离散扩散建模」 将所有数据视作可被逐步 「去噪」 和 「生成」 的对象。这种处理方式不仅简化了模型结构，还显著提升了生成质量和效率。与以往的多模态模型不同，Lumina-DiMOO 兼顾了速度与准确性，在图像生成任务中只需少量步骤即可获得高质量结果。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;此外，Lumina-DiMOO 在应用场景上具有广泛的适用性。无论是文本到图像生成、图像理解，还是主题驱动生成，模型都能表现出色。同时，它还具备较强的图像分析能力，能够识别图片中的细节和氛围，为用户提供深入的理解。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/372572</link>
      <guid isPermaLink="false">https://www.oschina.net/news/372572</guid>
      <pubDate>Sat, 13 Sep 2025 09:11:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>DeepMind 发布 VaultGemma 模型，具备差分隐私能力</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;谷歌 DeepMind 最近推出了一款名为 VaultGemma 的新型语言模型，这一创新的技术专注于用户隐私的保护。VaultGemma 不仅是开源的，而且是目前规模&lt;span&gt;最大&lt;/span&gt;的具备差分隐私能力的语言模型，参数数量达到了惊人的 10 亿。这项技术的发布，标志着人工智能领域在保护用户数据隐私方面的重大进步。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;传统的大语言模型在训练过程中可能会不小心记住一些敏感信息，比如姓名、地址和机密文档等。为了应对这一挑战，VaultGemma 引入了差分隐私技术，通过在训练过程中添加可控的随机噪声，确保模型的输出无法与特定的训练样本关联。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;这意味着，即使 VaultGemma 曾接触过机密文件，从统计学上也无法还原其内容。谷歌的初步测试结果显示，VaultGemma 确实没有泄露或复现任何训练数据，进一步提升了用户的信任感。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="257" src="https://oscimg.oschina.net/oscnet/up-279006f903387dbb56f6de856478ba712a4.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在技术架构方面，VaultGemma 是基于 Google 的 Gemma2 架构，采用了仅解码器的 Transformer 设计，包含 26 层，并使用了多查询注意力机制。一个关键的设计选择是将序列长度限制为 1024 个 Token，这样有助于管理私有训练所需的高密集计算。开发团队还借助一种新颖的 「差分隐私缩放定律」，为计算能力、隐私预算和模型效用之间的平衡提供了框架。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;尽管 VaultGemma 的性能与五年前的普通语言模型相当，在生成能力上略显保守，但它在保护隐私方面提供了更强的保障。谷歌的研究人员表示，他们将在 Hugging Face 和 Kaggle 上以开源许可证公开 VaultGemma 及其相关代码库，让更多人能够轻松访问这一私有 AI 技术。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/372565</link>
      <guid isPermaLink="false">https://www.oschina.net/news/372565</guid>
      <pubDate>Sat, 13 Sep 2025 08:43:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>阿里通义实验室发布端到端语音识别大模型 FunAudio-ASR</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;阿里巴巴通义实验室发布了其端到端语音识别大模型 FunAudio-ASR。该模型通过创新的 Context 模块，成功将高噪声场景下的幻觉率从 78.5% 大幅降低至 10.7%，降幅接近 70%。&lt;/p&gt; 
&lt;p&gt;模型使用了数千万小时的音频数据进行训练，并融合了大语言模型的语义能力，在远场、嘈杂、多说话人等复杂条件下，其性能已超越 Seed-ASR、KimiAudio-8B 等主流系统。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0916/160315_13vU_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;FunAudio-ASR 提供了轻量化版本 FunAudio-ASR-nano，在保持较高识别准确率的同时，具备更低的推理成本，适合对资源敏感的部署环境。&lt;/p&gt; 
&lt;p&gt;两个版本均支持低延迟流式识别、中英无缝切换以及用户自定义热词功能。目前，该技术已在钉钉的 「AI 听记」、视频会议以及 DingTalk A1 硬件中落地应用。其 API 也已在阿里云百炼平台上线。&lt;/p&gt; 
&lt;p&gt;体验地址：&lt;em&gt;https://modelscope.cn/studios/iic/FunAudio-ASR&lt;/em&gt;&lt;br&gt; 技术报告：&lt;em&gt;https://github.com/FunAudioLLM/FunAudioLLM.github.io/blob/master/pdf/FunAudio-ASR.pdf&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/372559</link>
      <guid isPermaLink="false">https://www.oschina.net/news/372559</guid>
      <pubDate>Sat, 13 Sep 2025 08:05:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>喜报 | GPTBots 与极光推送双双荣获「金灵光杯」重磅奖项！</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;近日，第二届「金灵光杯」中国互联网创新大赛颁奖仪式在雄安新区盛大举行。&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;「&lt;strong&gt;金灵光杯」大赛由中国互联网协会（&lt;/strong&gt;&lt;strong&gt;Internet Society of China&lt;/strong&gt;&lt;strong&gt;）主办。作为由国内从事互联网行业的网络运营商、服务提供商、设备制造商、系统集成商以及科研、教育机构等&lt;/strong&gt;&lt;strong&gt;700&lt;/strong&gt;&lt;strong&gt;多家会员单位组成的全国性、行业性、非营利性社会组织，中国互联网协会旨在搭建行业交流平台、推动产业健康发展，其主办的评选活动在业内享有极高的权威性和公信力。&lt;/strong&gt;本届大赛旨在发掘和表彰年度最具技术突破、商业价值和社会影响力的创新产品与企业，其评选结果被视为行业发展的风向标。&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;本届大赛竞争尤为激烈，吸引了超过 1000 家单位申报，提交项目多达 1500 余个。经过由多名院士及行业顶尖专家组成的评审委员会全方位、深层次的剖析与评估。&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;中国领先的客户互动和营销科技服务商极光（Aurora Mobile，纳斯达克股票代码：JG），凭借在人工智能和客户互动领域的持续创新与深厚积累，旗下两大核心产品——企业级 AI 智能体平台 GPTBots 与市场领先的极光推送（JPush）在 2025 年（第二届）「金灵光杯」中国互联网创新大赛中表现突出。GPTBots 凭借「AI 赋能宠物陪伴，打造宠物情感互动新模式」荣获&lt;strong&gt;「&lt;/strong&gt;&lt;strong&gt;AI+&lt;/strong&gt;&lt;strong&gt;创新应用」专题赛二等奖&lt;/strong&gt;，极光推送（JPush）则在&lt;strong&gt;「信息通信安全专题赛—网络和数据安全方向」中获得优秀奖。&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:center"&gt;&lt;img src="https://oscimg.oschina.net/oscnet//266f8e7be3b4b936d588ab74642e04fb.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:center"&gt;&lt;img src="https://oscimg.oschina.net/oscnet//aaa91f493ef707c7bbd39b2b84bed578.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:center"&gt;&lt;img src="https://oscimg.oschina.net/oscnet//34933ad54e536970960f1848fa6cd935.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;此次获奖，不仅彰显了行业对极光技术实力与创新能力的高度认可，也再次印证了其在 AI 时代「成熟业务+创新业务」双轮驱动战略的巨大成功。&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;&lt;strong&gt;GPTBots&lt;/strong&gt;&lt;strong&gt;：&lt;/strong&gt;&lt;strong&gt;AI&lt;/strong&gt;&lt;strong&gt;落地应用的破局者，荣获&lt;/strong&gt;&lt;strong&gt;「AI+&lt;/strong&gt;&lt;strong&gt;创新应用&lt;/strong&gt;&lt;strong&gt;」&lt;/strong&gt;&lt;strong&gt;专题赛二等奖&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;作为极光在 AIGC 时代的战略级产品，GPTBots 自推出以来，便致力于解决企业「AI 落地难」的核心痛点。它是一个强大的企业级 AI 智能体平台，通过连接大型语言模型（LLM）与企业数据和服务，帮助企业无需复杂的代码开发，即可快速构建专属的 AI 专家团队（Multi-Agent）。&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:center"&gt;&lt;img src="https://oscimg.oschina.net/oscnet//d25f1c3493cb27897cd9f196eebdacc4.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;&lt;strong&gt;在本次「金灵光杯」的评选中，评委会对&lt;/strong&gt;&lt;strong&gt;GPTBots&lt;/strong&gt;&lt;strong&gt;的以下几点给予了高度评价：&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;&lt;strong&gt;前瞻的&lt;/strong&gt;&lt;strong&gt;Multi-Agent&lt;/strong&gt;&lt;strong&gt;架构：&lt;/strong&gt;GPTBots 允许企业根据业务需求，灵活构建由多个 AI 智能体组成的「虚拟团队」，协同完成如市场分析、销售线索挖掘、客户服务等复杂任务，极大提升了业务自动化水平和决策效率。&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;&lt;strong&gt;强大的企业级能力：&lt;/strong&gt;平台提供丰富的官方工具、插件和知识库集成能力，支持私有化部署，确保了企业数据安全与业务流程的深度融合。&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;&lt;strong&gt;显著的商业价值：&lt;/strong&gt;从提升 SEO 效率、降低客服成本，到优化金融风控策略，GPTBots 已在电商、金融、教育、医疗等多个行业展现出「降本-增效」的巨大潜力，为企业数字化转型提供了强大的新引擎。&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:center"&gt;&lt;img src="https://oscimg.oschina.net/oscnet//a183b084c55a0c021632296267dbc386.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;&lt;strong&gt;极光推送（&lt;/strong&gt;&lt;strong&gt;JPush&lt;/strong&gt;&lt;strong&gt;）：十年行业基石，荣获&lt;/strong&gt;&lt;strong&gt;「&lt;/strong&gt;&lt;strong&gt;信息通信安全专题赛&lt;/strong&gt;&lt;strong&gt;—&lt;/strong&gt;&lt;strong&gt;网络和数据安全方向优秀奖&lt;/strong&gt;&lt;strong&gt;」&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;作为国内移动消息推送服务的开创者与市场份额遥遥领先的领导者，极光推送（JPush）此次再获殊荣，充分证明了其在行业中不可动摇的标杆地位。成立十余年来，极光推送（JPush）凭借其稳定、高效、安全、全球化的服务，为数以百万计的 App 提供了可靠的客户触达能力。&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:center"&gt;&lt;img src="https://oscimg.oschina.net/oscnet//7f5bf5e9707fa367270f6df73a079092.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;在技术日新月异的今天，极光推送（JPush）始终保持创新，不仅在推送速度、到达率和稳定性上持续优化，更前瞻性地与 AI 技术结合，为开发者和运营者提供更智能、更精准的用户互动解决方案。&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;&lt;strong&gt;双轮驱动：&lt;/strong&gt;&lt;strong&gt;AI&lt;/strong&gt;&lt;strong&gt;赋能，共筑企业增长新未来&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;此次 GPTBots 与极光推送（JPush）的双双获奖，是极光综合实力的最好体现。这不仅是两项独立技术的胜利，更是两大业务战略协同的成果。&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;&lt;strong&gt;数据与场景的滋养：&lt;/strong&gt;极光推送（JPush）海量的客户互动数据和丰富的应用场景，为 GPTBots 的 AI 模型训练和应用落地提供了最宝贵的「养料」。&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;&lt;strong&gt;智能与效率的赋能：&lt;/strong&gt;GPTBots 强大的 AI 能力，正反向赋能推送业务。例如，企业可通过构建「智能推送策略 Agent」，实现千人千面的精准营销和自动化运营，极大提升用户参与度与商业转化效果。&lt;/p&gt; 
&lt;p style="color:#444444; margin-left:0; margin-right:0; text-align:left"&gt;再次感谢「金灵光杯」大赛评委会及广大客户与合作伙伴的认可。这份荣誉是里程碑，更是新起点。未来，极光将继续深耕客户互动与人工智能领域，坚持以技术创新驱动产品进化，将 GPTBots 和极光推送打造为企业在数字化与智能化浪潮中最值得信赖的伙伴，助力千行百业实现可持续增长。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/372558</link>
      <guid isPermaLink="false">https://www.oschina.net/news/372558</guid>
      <pubDate>Sat, 13 Sep 2025 08:03:00 GMT</pubDate>
      <author>作者: 开源科技</author>
    </item>
    <item>
      <title>Bing 在搜索结果页面添加 Edge 与 Chrome 的「记分牌」对比表</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;本周，当你在 Bing 上搜索「Chrome」时，会亲眼见证微软新版 Edge 广告的有趣一幕——微软专门制作了一张详细的对比表，每当用户尝试下载安装 Chrome 时，该表会在 Bing 的显眼处展示，直接将 Edge 和 Chrome 进行对比。&lt;/p&gt; 
&lt;p&gt;当用户在 Bing 上搜索「Chrome」，Bing 会提示「一切上网所需就在这里」，显然指的就是 Microsoft Edge。广告下方的简短描述写道：「Microsoft Edge 基于与 Chrome 相同的技术，但拥有微软的信任加持。」&lt;/p&gt; 
&lt;p&gt;事实上 Bing 过去就常常以横幅推广 Edge，但此次最吸引人注意的，是新增了「记分牌」对比表，明确将 Edge 列为「赢家」。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-fac1f1040d745a70ff32e06328e5979a8f8.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;广告截图显示，该对比表卡片悬浮在搜索结果前方，左侧蓝色高亮的是 Edge 标志，而 Chrome 则被「灰掉」。微软用色彩和布局突出强调：Edge 可获得奖励积分、内置 VPN、AI 个性化，且为「微软推荐」，更安全，专为 Windows 11 设计。&lt;/p&gt; 
&lt;p&gt;表格中，每一项 Edge 都打了勾号，而 Chrome 全是叉号。&lt;/p&gt; 
&lt;p&gt;这一卡片出现在 Google Chrome 官方下载链接上方，用户首先看到的就是微软的「推荐理由」，下方还有「了解更多 Edge 功能」按钮，引导用户进一步了解 Edge。&lt;/p&gt; 
&lt;p&gt;如果用户继续下滑并访问 Google 官网，会遇到微软 Edge 的弹窗广告，提醒继续使用 Edge。如果仍选择无视微软推荐、坚持下载安装 Chrome，Google 网站页面顶部还会出现 Edge 横幅广告，将 Chrome 下载选项下移。这是微软最后一搏，试图说服用户转向 Edge。&lt;/p&gt; 
&lt;p&gt;值得注意的是，这并不是微软首次用激进手段推荐自家产品，但此次方式堪称更进一步。&lt;/p&gt; 
&lt;p&gt;不仅如此，早前在 Bing 上搜索 ChatGPT 或 Gemini 时也会弹出 Copilot 相关广告。如今，微软更是直接设计出「记分牌」，在用户搜索 Chrome 时宣称 Edge 为「胜者」。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/372549</link>
      <guid isPermaLink="false">https://www.oschina.net/news/372549</guid>
      <pubDate>Sat, 13 Sep 2025 07:47:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>​Meta AI 发布 MobileLLM-R1：轻量级边缘推理模型</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;Meta AI 近日推出了 MobileLLM-R1，这是一系列轻量级边缘推理模型，目前已在 Hugging Face 上发布。该系列模型参数范围从 140M 到 950M，专注于高效的数学、编码和科学推理，且在不足 10 亿的参数规模下实现了优秀的性能表现。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="365" src="https://oscimg.oschina.net/oscnet/up-30b7f801e9b13bb0046a53750505a0aa2ec.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;MobileLLM-R1 的&lt;span&gt;最大&lt;/span&gt;模型为 MobileLLM-R1-950M，采用了一系列架构优化设计:包括 22 层 Transformer 结构、24 个注意力头和 6 个分组 KV 头。模型的嵌入维度为 1536，隐藏层维度为 6144。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;此外，模型还采用了分组查询注意力（GQA）来减少计算和内存需求，块级权重共享技术降低了参数数量而不显著增加延迟，SwiGLU 激活函数提升了小模型的表示能力。模型支持 4K 的上下文长度和 32K 的后训练模型。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在训练效率方面，MobileLLM-R1 的表现同样引人注目。该模型总共在约 4.2 万亿个 token 上进行训练，相较于 Qwen3 的 0.6B 模型训练的 36 万亿 token，MobileLLM-R1 仅使用了约 11.7% 的数据便达到了或超越了 Qwen3 的准确率。同时，模型在数学、编码和推理数据集上进行了监督微调，从而降低了训练成本和资源需求。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在各项基准测试中，MobileLLM-R1-950M 的表现优异:在 MATH500 数据集上，其准确率比 OLMo-1.24B 高出约 5 倍，且比 SmolLM2-1.7B 高出约 2 倍。在 GSM8K、AIME 和 LiveCodeBench 等推理和编码任务上，MobileLLM-R1 甚至与 Qwen3-0.6B 相匹配或超越，尽管所使用的 token 数量远少于后者。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;不过，MobileLLM-R1 的聚焦也带来了局限性。虽然在数学、编码和结构化推理方面表现强劲，但在一般对话、常识推理和创造性任务上，MobileLLM-R1 的表现较大型模型有所不足。此外，模型在生产环境中的使用受到 FAIR NC（非商业）许可证的限制，较长的上下文 (32K) 也提高了推理时的 KV 缓存和内存需求。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/372545</link>
      <guid isPermaLink="false">https://www.oschina.net/news/372545</guid>
      <pubDate>Sat, 13 Sep 2025 07:29:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>开发者必看：隐语框架的分层拆解和使用</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;blockquote&gt; 
 &lt;p&gt;打开链接点亮社区 Star，照亮技术的前进之路。 &lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-edf8393baa8c76890c2e321dca39a57a078.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;每一个点赞，都是社区技术大佬前进的动力&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Github 地址： &lt;em&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fsecretflow%2Fsecretflow" target="_blank"&gt;https://github.com/secretflow/secretflow&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-f1f5ac60c4e44a644832b0e9d03aac9f579.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h2_1"&gt;&lt;/span&gt; 
&lt;h2&gt;一、"隐语"架构设计全貌&lt;/h2&gt; 
&lt;span id="OSC_h3_2"&gt;&lt;/span&gt; 
&lt;h3&gt;1.隐语框架设计思想&lt;/h3&gt; 
&lt;p&gt;隐私计算是一个新兴的跨学科领域，涉及&lt;strong&gt;密码学、机器学习、数据库、硬件&lt;/strong&gt;等多个领域。根据过去几年的实践经验，我们发现&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;隐私计算技术方向多样，&lt;/strong&gt;不同场景下有其各自更为合适的技术解决方案&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;隐私计算学习曲线很高&lt;/strong&gt;，非隐私计算背景的用户使用困难&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;隐私计算涉及领域众多，&lt;/strong&gt;需要领域专家共同协作&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-66d2fb38d008b759553f3f5a5253b809349.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;隐语的设计目标&lt;/strong&gt;是使得数据科学家和机器学习开发者可以非常容易地使用隐私计算技术进行数据分析和机器学习建模，而无需了解底层技术细节。&lt;/p&gt; 
&lt;p&gt;为达到这个目标，**隐语提供了一层设备抽象，**将多方安全计算 (MPC)、同态加密 (HE) 和可信执行环境 (TEE) 等隐私计算技术抽象为密文设备， 将单方计算抽象为明文设备。&lt;/p&gt; 
&lt;p&gt;基于这层抽象，数据分析和机器学习工作流可以表示为一张计算图，&lt;strong&gt;其中节点表示某个设备上的计算，边表示设备之间的数据流动，不同类型设备之间的数据流动会自动进行协议转换&lt;/strong&gt;。在这一点上，隐语借鉴了主流的深度学习框架，后者将神经网络表示为一张由设备上的算子和设备间的张量流动构成的计算图。&lt;/p&gt; 
&lt;p&gt;隐语框架围绕开放这一核心思想，提供了不同层次的设计抽象，希望为不同类型的开发者都提供良好的开发体验。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;在设备层，隐语提供了良好的设备接口和协议接口，支持更多的设备和协议插拔式的接入&lt;/strong&gt;，我们希望与密码学、可信硬件、硬件加速等领域专家通力合作，不断扩展密态计算的类型和功能，不断提升协议的安全性和计算性能。&lt;/p&gt; 
&lt;p&gt;同时，隐语提供了良好的设备接口，第三方隐私计算协议可作为设备插拔式接入。&lt;strong&gt;在算法层，为机器学习提供了灵活的编程接口，算法开发者可以很容易定义自己的算法。&lt;/strong&gt;&lt;/p&gt; 
&lt;span id="OSC_h3_3"&gt;&lt;/span&gt; 
&lt;h3&gt;2.架构分层总览&lt;/h3&gt; 
&lt;p&gt;隐语总体架构自底向上一共分为五层：&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-41a68e7f0ca01ba4db1273f9f2d109f05cd.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;资源管理层：&lt;/strong&gt; 主要承担了两方面的职责。第一是面向业务交付团队，可以屏蔽不同机构底层基础设施的差异，降低业务交付团队的部署运维成本。另一方面，通过对不同机构的资源进行集中式管理，构建出一个高效协作的数据协同网络。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;明密文计算设备与原语层：&lt;/strong&gt; 提供了统一的可编程设备抽象，将多方安全计算 (MPC)、同态加密 (HE)、可信硬件 (TEE) 等隐私计算技术抽象为密态设备，将单方本地计算抽象为明文设备。同时，提供了一些不适合作为设备抽象的基础算法，如差分隐私 (DP)、安全聚合 (Secure Aggregation) 等。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;明密文混合调度层：&lt;/strong&gt; 提供了统一的设备调度抽象，将上层算法描述为一张有向无环图，其中节点表示某个设备上的计算，边表示设备之间的数据流动，即逻辑计算图。逻辑计算图由分布式框架进一步拆分并调度至物理节点。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;AI &amp;amp; BI 隐私算法层：&lt;/strong&gt; 这一层的目的是屏蔽掉隐私计算技术细节，但保留隐私计算的概念，其目的是降低隐私计算算法的开发门槛，提升开发效率。&lt;/p&gt; 
&lt;p&gt;有隐私计算算法开发诉求的同学，可以根据自身场景和业务的特点，设计出一些特化的隐私计算算法，来满足自身业务和场景对安全性、计算性能和计算精度的平衡。&lt;/p&gt; 
&lt;p&gt;在这一层上，隐语本身也会提供一些通用的算法能力，比如 MPC 的 LR/XGB/NN，联邦学习算法，SQL 能力等。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;用户界面层：&lt;/strong&gt; 隐语的目标并不是做一个端到端的产品，而是为了让不同的业务都能够通过快速集成隐语而具备全面的隐私计算能力。因此我们会在最上层去提供一层比较薄的产品 API，以及一些 SDK，去降低业务方集成隐语的成本。&lt;/p&gt; 
&lt;span id="OSC_h3_4"&gt;&lt;/span&gt; 
&lt;h3&gt;&lt;strong&gt;3.架构细节拆解&lt;/strong&gt;&lt;/h3&gt; 
&lt;span id="OSC_h4_5"&gt;&lt;/span&gt; 
&lt;h4&gt;&lt;strong&gt;设备与原语层&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;隐语的设备分为物理设备和逻辑设备，其中，物理设备是隐私计算各个参与方的物理机器，逻辑设备则由一个或多个物理设备构成。&lt;/p&gt; 
&lt;p&gt;逻辑设备支持一组，特定的计算算子 (Device Ops)，有自己特定的数据表示 (Device Object)。&lt;strong&gt;逻辑设备分为明文和密文两种类型，前者执行单方本地计算，后者执行多方参与的隐私计算。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;逻辑设备的运行时负责内存管理、数据传输、算子调度等职责，运行在一个或多个物理设备上。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;逻辑设备和物理设备不是一对一的关系，一个物理设备可能同时属于多个逻辑设备&lt;/strong&gt;。在同一组物理设备上，可以根据不同的隐私协议和参与组合虚拟出不同的逻辑设备。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-814224da9f6ff77d2740d3b16167d0fbe4c.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;下表是隐语目前暂定支持的设备列表：&lt;/p&gt; 
&lt;table border="1" cellpadding="1" cellspacing="1" style="width:500px"&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;strong&gt;设备&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;strong&gt;类型&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;strong&gt;运行时&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;strong&gt;算子&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;strong&gt;协议&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;strong&gt;前端&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;strong&gt;状态&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;strong&gt;PYU&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;明文&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;Python Interpreter&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;—&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;—&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;Python&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;Release&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;strong&gt;SPU&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;密文&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;SPU VM&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;PSI, XLA HLO&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;SPDZ-2k, ABY3&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;JAX, TensorFlow, PyTorch&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;Alpha&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;strong&gt;HEU&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;密文&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;HEU Runtime&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;Add, XLA HLO&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;Paillier, OU, TFHE&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;Numpy, JAX&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#333333"&gt;Alpha&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;strong&gt;TEE&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;密文&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;TEE Runtime&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;XLA HLO&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;Intel SGX&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;JAX, TensorFlow, PyTorch&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;WIP&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;span id="OSC_h4_6"&gt;&lt;/span&gt; 
&lt;h4&gt;&lt;strong&gt;可编程性&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;逻辑设备具备可编程性，即用户可以在设备上自定义计算逻辑，每个设备对用户提供了协议无关的编程接口。**在一个设备上，用户可以定义从简单的矩阵运算， 到完整的深度模型训练。**当然，这一切取决于设备提供的计算能力。&lt;/p&gt; 
&lt;p&gt;对于明文设备 PYU，它的前端为 python，用户可以通过&lt;code&gt;@device&lt;/code&gt;将一段预定义 python 函数调度至其上执行。&lt;/p&gt; 
&lt;p&gt;对于密文设备 SPU、HEU、TEE，它们的前端可以是任何支持 XLA 的框架， 如 JAX, TensorFlow,PyTorch 等。同样的，用户也可以通过&lt;code&gt;@device&lt;/code&gt;将基于这些前端自定义的函数调度至指定的设备执行。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;import jax.numpy as jnp

dev = Device()  # maybe PYU, SPU, HEU, TEE


@device(dev)
def selu(x, alpha=1.67, lmbda=1.05):
    return lmbda * jnp.where(x &amp;gt; 0, x, alpha * jnp.exp(x) - alpha)


res = selu(x)  # res is a DeviceObject
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;用户自定义函数首先转换成&lt;strong&gt;XLA HLO Computation&lt;/strong&gt;，由 XLA 进行设备无关的代码优化和分析，并发往后端设备。后端设备进一步执行代码优化和分析，并生成最终，的可执行代码。&lt;/p&gt; 
&lt;p&gt;可执行代码或由设备的虚拟机解释执行 (SPU, HEU)，或由硬件直接执行 (TEE)。使用 XLA HLO 作为 IR，使得我们可以复用 XLA 前端和设备无关，代码优化，同时使得后端实现更加简洁干净。&lt;/p&gt; 
&lt;p&gt;对于密文设备（半同态）HEU，它仅支持一组有限的计算，因此提供了一组预定义算子如&lt;code&gt;__add__&lt;/code&gt;,&lt;code&gt;__mul__&lt;/code&gt;等，用户不能通过&lt;code&gt;@device&lt;/code&gt;进行自定义编程。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;x, y = HEUObject(), PYUObject()
z = x + y  # add
z = x * y  # mul
z = x @ y  # matmul
&lt;/code&gt;&lt;/pre&gt; 
&lt;span id="OSC_h4_7"&gt;&lt;/span&gt; 
&lt;h4&gt;&lt;strong&gt;协议转换&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;用户在逻辑设备上进行编程，构建逻辑计算图，**其节点表示设备上的一段函数或算子，边表示设备对象的流动。**逻辑计算图被设备进一步分割为子图，两个子图间的，边表示跨设备的对象流动，此时需要进行协议转换。设备对象的&lt;code&gt;DeviceObject.to&lt;/code&gt;接口用于转换至目标设备对象，任何新增的设备都应该提供相应的转换函数并，插入对象转换表中。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;下表是各个逻辑设备对象的转换表：&lt;/strong&gt;&lt;/p&gt; 
&lt;table border="1" cellpadding="1" cellspacing="1" style="width:500px"&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&amp;nbsp;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;PYU&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;SPU&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;HEU&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;TEE&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;PYU&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&amp;nbsp;&lt;/td&gt; 
   &lt;td&gt;share&lt;/td&gt; 
   &lt;td&gt;encrypt&lt;/td&gt; 
   &lt;td&gt;encrypt&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;SPU&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;reconstruct&lt;/td&gt; 
   &lt;td&gt;&amp;nbsp;&lt;/td&gt; 
   &lt;td&gt;encrypt+add&lt;/td&gt; 
   &lt;td&gt;reconstruct+encrypt&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;HEU&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;decrypt&lt;/td&gt; 
   &lt;td&gt;minus+decrypt&lt;/td&gt; 
   &lt;td&gt;&amp;nbsp;&lt;/td&gt; 
   &lt;td&gt;decrypt+encrypt&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;TEE&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;decrypt&lt;/td&gt; 
   &lt;td&gt;decrypt+share&lt;/td&gt; 
   &lt;td&gt;decrypt+encrypt&lt;/td&gt; 
   &lt;td&gt;&amp;nbsp;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;span id="OSC_h4_8"&gt;&lt;/span&gt; 
&lt;h4&gt;&lt;strong&gt;分布式引擎&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;用户基于设备构建了一张逻辑计算图，那么我们如何执行这张计算图？由于逻辑设备映射到一个或多个物理设备，&lt;strong&gt;因此我们需要将逻辑设备上的算子正确调度到其对应的物理设备，同时处理好这些物理设备间的数据传输关系&lt;/strong&gt;。毫无疑问，我们需要一个分布式图执行引擎来解决这些问题。&lt;/p&gt; 
&lt;p&gt;那么我们需要一个怎样的分布式图执行引擎？以下是隐语对它的要求&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;细粒度的异构计算：在一张逻辑计算图中，具有不同粒度的计算任务，既有简单的数据处理（秒级），也有复杂的多方训练（几个小时至几十小时）。同时，物理节点具有不同的硬件环境，CPU, GPU, TEE, FPGA 等。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;灵活的计算模型：在水平、垂直场景下，针对数据处理和模型训练等不同工作流，支持多种并行模型，如数据并行、模型并行、混合并行。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;动态执行：在联邦学习场景下，不同机构的数据规模、带宽延迟、机器性能可能有较大差异，这导致同步模式的效率受限于最慢的工作节点。因此，我们希望支持，异步训练模式，这要求图执行引擎具有动态执行能力。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-2c0fb0a2006a01dc6e88397c7510e42537d.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;隐语针对隐私计算场景，已经对框架进行了一些安全加固工作：&lt;strong&gt;通过身份认证、代码预装、代码存证等手段对框架做了整体加固&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;未来，还将探索沙箱隔离、访问控制、静态图等机制以进一步提升安全水位。在环境适配方面，为了适配跨机构网络通信的特点，推进了 GCS gRPC 通信、域名支持、弱网断线处理等相关功能的开发。&lt;/p&gt; 
&lt;span id="OSC_h4_9"&gt;&lt;/span&gt; 
&lt;h4&gt;&lt;strong&gt;AI &amp;amp; BI 隐私算法&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;这一层的目的是其目的是降低隐私计算算法的开发门槛，提升开发效率。有隐私计算算法开发诉求的同学，可以根据自身场景和业务的特点，设计出一些特化的隐私计算算法，来&lt;strong&gt;满足自身业务和场景对安全性、计算性能和计算精度的平衡&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;在这一层上，隐语本身也会提供一些通用的算法能力，比如 MPC 的 LR/XGB/NN，联邦学习算法，SQL 能力等。&lt;/p&gt; 
&lt;span id="OSC_h2_10"&gt;&lt;/span&gt; 
&lt;h2&gt;二、"隐语"框架的使用&lt;/h2&gt; 
&lt;p&gt;使用隐语构建隐私计算算法 &lt;strong&gt;逻辑设备抽象为算法开发者提供了极大的灵活性，&lt;/strong&gt; 他们可以像积木一样自由组合这些设备，在设备上自定义计算，从而构建自己的隐私计算算法。&lt;/p&gt; 
&lt;p&gt;接下来，我们通过一个具体的算法来展示隐语框架的通用编程能力。联邦学习算法联邦机器学习又名联邦学习、联合学习、联盟学习，是一种机器学习框架，能有效帮助多个机构在满足用户隐私保护、数据安全和政府法规的要求下，进行数据使用和机器学习建模。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-ac805372dc1d72a4712d5e495a399b0eba4.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;联邦学习的算法流程如上图所示，大致分为以下四个步骤：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;机构节点在本地进行多轮训练，得到模型参数&lt;/li&gt; 
 &lt;li&gt;机构节点使用加密协议，将模型参数上传至聚合节点&lt;/li&gt; 
 &lt;li&gt;聚合节点使用加密协议，对模型参数进行聚合，得到全局模型&lt;/li&gt; 
 &lt;li&gt;机构节点从聚合节点获取最新的全局模型，进入下一轮训练&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;节点本地训练&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;机构节点运行在机构本地，隐语提供了一个逻辑设备&lt;code&gt;PYU&lt;/code&gt;，执行本地的明文计算。&lt;/p&gt; 
&lt;p&gt;下面的&lt;code&gt;BaseTFModel&lt;/code&gt;定义了本地模型训练逻辑，用户可以选择自己喜好的机器学习框架，如&lt;code&gt;TensorFlow, PyTorch&lt;/code&gt;等。&lt;/p&gt; 
&lt;p&gt;隐语提供了&lt;code&gt;@proxy&lt;/code&gt;装饰器，对一个普通的类进行了初始设置，以便后续在逻辑设备上对其实例化。&lt;code&gt;@proxy(PYUObject)&lt;/code&gt;表明该类需要在 PYU 设备上实例化。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@proxy(PYUObject)
class BaseTFModel:
    def train_step(self, weights, cur_steps, train_steps) -&amp;gt; Tuple[np.ndarray, int]:
    self.model.set_weights(weights)
        num_sample = 0
        for _ in range(train_steps):
            x, y = next(self.train_set)
            num_sample += x.shape[0]
            self.model.fit(x, y)

        return self.model.get_weights(), num_sample
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;模型安全聚合&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;模型聚合对各个机构节点的模型参数进行加权平均，如下面&lt;code&gt;_average&lt;/code&gt;所示。隐语逻辑设备的最大特点在于可编程性，用户可以将一段函数调度到多种设备执行，以达到使用不同隐私计算技术的目的。&lt;/p&gt; 
&lt;p&gt;目前，&lt;code&gt;DeviceAggregator&lt;/code&gt;可以支持 PYU 明文聚合，也可以支持 SPU MPC 协议聚合，后续我们还将支持 TEE, HEU 等多种密文设备。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@dataclass
class DeviceAggregator(Aggregator):
    device: Union[PYU, SPU]

    def average(self, data: List[DeviceObject], axis=0, weights=None):
        # 2. 机构节点使用加密协议，将模型参数上传至聚合节点
        data = [d.to(self.device) for d in data]
        if isinstance(weights, (list, tuple)):
            weights = [w.to(self.device) if isinstance(w, DeviceObject) else w for w in weights]

        def _average(data, axis, weights):
            return [jnp.average(element, axis=axis, weights=weights) for element in zip(*data)]

        # 3. 聚合节点使用加密协议，对模型参数进行聚合，得到全局模型
        return self.device(_average, static_argnames='axis')(data, axis=axis, weights=weights)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;训练流程整合&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;有了节点本地训练、模型安全聚合，我们就可以将其整合起来形成完整的训练流程。首先，我们在每个 PYU 设备（代表机构节点）创建 BaseTFModel 实例。&lt;/p&gt; 
&lt;p&gt;同时，初始化聚合器，可以是 PYU, SPU, TEE, Secure Aggregation。然后，按照上述描述的联邦学习算法流程进行迭代训练。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;class FedTFModel:
    def __init__(self, device_list: List[PYU] = [], model: Callable[[], tf.keras.Model] = None, aggregator=None):
        # 在每个机构节点 (PYU) 创建一个 BaseTFModel 实例
        self._workers = {device: BaseTFModel(
            model, device=device) for device in device_list}
        # 聚合器，可以是 PYU, SPUPPU, TEE, Secure Aggregation
    self._aggregator = aggregator

    def fit(self, x: Union[HDataFrame, FedNdarray], y: Union[HDataFrame, FedNdarray], batch_size=32, epochs=1, verbose='auto',
            callbacks=None, validation_data=None, shuffle=True,
            class_weight=None, sample_weight=None, validation_freq=1, aggregate_freq=1):
    self.handle_data(train_x, train_y, batch_size=batch_size,
                     shuffle=shuffle, epochs=epochs)

    # 初始化模型参数
    current_weights = {
        device: worker.get_weights() for device, worker in self._workers.items()}

    for epoch in range(epochs):
        for step in range(0, self.steps_per_epoch, aggregate_freq):
            weights, sample_nums = [], []
            for device, worker in self._workers.items():
                # 1. 机构节点在本地进行多轮训练，得到模型参数
                weight, sample_num = worker.train_step(current_weights[device], epoch*self.steps_per_epoch+step, aggregate_freq)
                weights.append(weight)
                sample_nums.append(sample_num)
            # 模型参数聚合，可以是：PYU, SPU, TEE, Secure Aggregation
            current_weight = self._aggregator.average(
                weights, weights=sample_nums)
            # 4. 机构节点从聚合节点获取最新的全局模型，进入下一轮训练
            current_weights = {device: current_weight.to(device) for device, worker in self._workers.items()}
&lt;/code&gt;&lt;/pre&gt; 
&lt;span id="OSC_h4_11"&gt;&lt;/span&gt; 
&lt;h4&gt;&lt;strong&gt;更多算法：&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;通过以上联邦学习算法的例子，我们展示了隐语作为隐私计算框架的可编程性、可扩展性。&lt;/p&gt; 
&lt;p&gt;期待您基于隐语探索更多&lt;strong&gt;有趣的用法&lt;/strong&gt;！更多详情请参考我们的教程和实现。&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fsecretflow.readthedocs.io%2Fzh%2Flatest%2Ftutorial%2Findex.html%23" target="_blank"&gt;https://secretflow.readthedocs.io/zh/latest/tutorial/index.html#&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;在 SPU 进行 PSI 对齐，逻辑回归、神经网络训练&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;使用 SPU HEU 的组合构建 HESS-LR, HESS-XGB 算法&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;横向联邦学习，在 PYU 进行本地训练，使用 SPU、TEE、Secure Aggregation 进行梯度、权重聚合&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;纵向拆分学习，将一个模型拆分至多个 PYU，使用 PYU 聚合隐层，使用差分隐私保护前向隐层和反向梯度&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-edf8393baa8c76890c2e321dca39a57a078.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;打开链接即可点亮社区 Star，照亮技术的前进之路。&lt;/p&gt; 
&lt;p&gt;Github 地址：&lt;em&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fsecretflow%2Fsecretflow" target="_blank"&gt;https://github.com/secretflow/secretflow&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;本文由隐语社区统一发布，欢迎大家点 &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fsecretflow%2Fsecretflow" target="_blank"&gt;Star&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/5915128/blog/18691893</link>
      <guid isPermaLink="false">https://my.oschina.net/u/5915128/blog/18691893</guid>
      <pubDate>Sat, 13 Sep 2025 07:10:00 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>OpenAI 发布最大规模 ChatGPT 普通用户使用报告</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;OpenAI 经济研究团队与哈佛大学经济学家 David Deming 合作完成了名为&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopenai.com%2Findex%2Fhow-people-are-using-chatgpt%2F" target="_blank"&gt;&lt;em&gt;《How people are using ChatGPT》&lt;/em&gt;&lt;/a&gt;的研究报告，据称这是有史以来最大规模的一次关于普通用户（consumer users）如何使用 ChatGPT 的调查。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0916/151027_d0un_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;样本包括 150 万条对话，结合了 ChatGPT 在 2025 年中期的用户活动数据（当时每周有约 7 亿活跃用户）。&lt;/p&gt; 
&lt;p&gt;下面是该研究的一些发现：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;谁在使用 ChatGPT？&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;性别差距正在缩小。早期用户中，名字可分辨为「女性倾向」的占比曾比现在少；到了 2025 年 7 月，这一比例上升到超过一半。&lt;/li&gt; 
 &lt;li&gt;在低收入和中等收入国家的采用率增长尤其快。到 2025 年 5 月，最低收入国家的增长率是最高收入国家的 4 倍以上。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;人们用 ChatGPT 做什么？&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;大部分对话是关于处理日常任务（practical guidance）、获取信息（seeking information）和写作（writing）。写作是工作中最常见的任务类型。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;互动类型可以分为三类：「Asking」（提问）、「Doing」（执行任务／创造输出）、「Expressing」（表达）。在这些中：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;「Asking」（约 49%）— 用户寻求建议、信息等；&lt;/li&gt; 
   &lt;li&gt;「Doing」（约 40%）— 包括起草文本、策划、编程等实作型任务，其中约三分之一是用于工作用途。&lt;/li&gt; 
   &lt;li&gt;「Expressing」（约 11%）— 包括情感表达、反思、娱乐或个人探索等。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;使用形式与演变？&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;大约 30% 的使用与工作有关，约 70% 与非工作（个人生活）有关。两者都在增长，说明 ChatGPT 不仅是生产力工具，也在个人日常生活中创造价值。&lt;/li&gt; 
 &lt;li&gt;一个关键的价值是决策支持（decision support）：ChatGPT 在知识密集型职业中能够帮助改善判断和效率。&lt;/li&gt; 
 &lt;li&gt;使用随着时间加深：随着模型能力的提升和用户探索出新的用途，人们使用频次与深入程度都在上升。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;研究发现，ChatGPT 的用户基础越来越广泛，使用者的社会／经济背景差距在缩小。 它不仅在工作场景中提高效率，也在日常生活中提供帮助；因此其经济价值不仅体现在对 GDP 的直接贡献，也体现在人们日常生活中未被传统经济统计完全捕捉的价值。 OpenAI 从中得出的理念是，人工智能的访问应当被视为基本权利，让更多人能利用它释放潜力，塑造自己的未来。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/372540/how-people-are-using-chatgpt</link>
      <guid isPermaLink="false">https://www.oschina.net/news/372540/how-people-are-using-chatgpt</guid>
      <pubDate>Sat, 13 Sep 2025 07:08:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>腾讯跨端开源框架 Kuikly 适配「液态玻璃」</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Kuikly 是基于 Kotlin Multiplatform 的 UI 与逻辑全面跨端综合解决方案，由腾讯大前端领域 Oteam（公司级）推出，旨在提供一套一码多端、极致易用、动态灵活的全平台高性能开发框架。&lt;/p&gt; 
&lt;p&gt;目前支持如下平台：Android、iOS、鸿蒙、Web（beta）和小程序（beta）。&lt;/p&gt; 
&lt;p&gt;Kuikly 团队介绍称，项目已完成对「液态玻璃」的首阶段适配，并对外开源发布。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0916/143023_KFsf_2720166.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="640" src="https://static.oschina.net/uploads/space/2025/0916/143104_iaPI_2720166.jpg" width="295" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;为了适配「液态玻璃」，Kuikly 没有引入新的独立组件，而是为现有组件提供了简洁的视图属性扩展。例如，开发者只需通过一行&amp;nbsp;&lt;code&gt;glassEffectIOS()&lt;/code&gt;代码，即可为任意容器视图启用液态玻璃效果。&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-javascript"&gt;View {
    attr {
        glassEffectIOS() // iOS 平台将自动添加液态玻璃效果
    }
    // ... 其他子视图
}&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;团队表示，Kuikly 的适配工作并非简单的 UI 改造，而是充分利用原生提供的基础能力，在框架渲染层和 DSL 驱动层两方面进行扩展，旨在为开发者提供一套便捷、低成本的适配方案。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/372524</link>
      <guid isPermaLink="false">https://www.oschina.net/news/372524</guid>
      <pubDate>Sat, 13 Sep 2025 06:32:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>逻辑智能开源语音大模型框架 LLaSO</title>
      <description/>
      <link>https://www.oschina.net/news/372521</link>
      <guid isPermaLink="false">https://www.oschina.net/news/372521</guid>
      <pubDate>Sat, 13 Sep 2025 06:26:00 GMT</pubDate>
    </item>
  </channel>
</rss>
