<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - industry - 简体中文</title>
    <link>https://www.oschina.net/news/industry</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news/industry" rel="self" type="application/rss+xml"/>
    <description>已对该 RSS 进行格式化操作：中英字符之间插入空格、使用直角引号、标点符号修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-cn</language>
    <lastBuildDate>Wed, 30 Jul 2025 07:46:09 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>商汤发布「日日新 SenseNova V6.5」大模型体系</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;商汤科技在 WAIC 2025 上发布了「日日新 SenseNova V6.5」大模型体系，其推理和多模态能力超越多个主流模型，且性价比提升 3 倍。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0730/153957_Ek9w_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0730/154010_eiFA_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0730/154040_VjKR_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;日日新 V6.5 重点升级了强推理、高效率和智能体三大能力。该模型在国内率先突破图文交错思维链技术，引入形象思维，并改进了多模态模型的融合架构，使得文本和多模态推理能力超越 Gemini 2.5 Pro 和 Claude-4 Sonnet，多模态交互能力超越 Gemini 2.5 Flash 和 GPT-4o，同时性价比相较 V6.0 提升了 3 倍。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/363167</link>
      <guid isPermaLink="false">https://www.oschina.net/news/363167</guid>
      <pubDate>Wed, 30 Jul 2025 07:41:07 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>AI 编程软件 Cline 回应 Anthropic 限制 Max 用量的新政策</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;日前，Anthropic 开始针对 Claude Code 订阅用户&lt;u&gt;&lt;a href="https://www.oschina.net/news/362871"&gt;加入新的每周用量限制&lt;/a&gt;&lt;/u&gt;，并且根据目前使用情况来计算，这一调整将影响不到 5% 的用户。&lt;/p&gt; 
&lt;p&gt;具体来看，从 8 月 28 日起，Anthropic 将在现有的每 5 小时重置的用量限制基础上，增加每周用量限制：每 7 天重置的总体每周用量上限；针对 Claude Opus 4 的每周用量上限，每 7 天重置。&lt;/p&gt; 
&lt;p&gt;Anthropic 表示，Claude Code 作为其订阅服务的一部分，该产品用户增长速度前所未有。但同时 Claude Code 存在一些违反政策的行为，以及一些超常规的使用模式。而这些行为影响了所有用户的系统容量。对此，Anthropic 推出的新用量限制旨在解决这些问题，为所有用户提供一个更公平的使用体验。&lt;/p&gt; 
&lt;p&gt;而 AI 编程软件 Cline &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Fcline%2Fstatus%2F1949943033891307589" target="_blank"&gt;也回应了 Anthropic 的新政策。&lt;/a&gt;&lt;strong&gt;Cline 将 AI 订阅比作加油站，而车（AI 工具）和油（AI 推理服务）都由同一家 AI 公司控制，用户买了油却不知道实际加了多少，甚至还没法了解实际消耗和服务内容&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-b714d24bfd8ec4d5f5174c7a6b4d395f74e.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Cline 认为，任何宣称自己是无限服务都是不可持续的，AI 推理本质上是像汽油、电力一样的商品。Cline 还指出，重度用户的使用成本远超订阅费用，从而导致服务商亏损，不得不设置限额或限制使用，最终用户体验也变得受损，得不偿失。&lt;/p&gt; 
&lt;p&gt;Cline 还表示，&lt;strong&gt;将来订阅模式终将被市场淘汰，未来应选择利益与用户一致、架构透明的工具和平台&lt;/strong&gt;。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/363163</link>
      <guid isPermaLink="false">https://www.oschina.net/news/363163</guid>
      <pubDate>Wed, 30 Jul 2025 07:24:07 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>TPU Deep Dive：Google TPU 架构深度分析</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;编者按：&lt;/strong&gt; 在人工智能算力军备竞赛愈演愈烈的今天，为什么 Google 会选择与主流 GPU 截然不同的技术路线，开发出架构独特的 TPU？这种专用芯片究竟凭借什么优势，能够支撑起 Gemini、Veo&amp;nbsp;等&amp;nbsp;AI 模型的训练与推理？&lt;/p&gt; 
 &lt;p&gt;文章从单芯片架构出发，深入剖析了 TPU 的核心设计理念：首先解释了 TPU 如何通过脉动阵列和流水线技术优化矩阵运算，然后阐述了 XLA 编译器如何通过预先编译减少缓存依赖，大幅降低能耗。在多芯片层面，作者详细介绍了 TPU 从托盘、机架、Pod 到 Multi-Pod 的层级扩展架构，特别是 OCS 光交换技术如何实现灵活的拓扑重构和故障容错。文章还通过具体案例展示了不同拓扑结构对并行训练策略的影响，以及 Multi-Pod 架构如何支撑超大规模模型训练。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;作者 | Henry Ko&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;编译 | 岳扬&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;最近我大量使用 TPU，发现它们与 GPU 的设计理念非常不同，感觉很有趣。&lt;/p&gt; 
&lt;p&gt;TPU 的主要优势在于其可扩展性。这是通过硬件层面（例如能效方面和模块化）与软件层面（例如 XLA compiler）的协同设计实现的。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;01 背景信息&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;简单介绍一下 TPU，它是谷歌的专用集成电路（ASIC），其设计聚焦于两大要素：极高的矩阵运算（matmul）吞吐量和能源效率。&lt;/p&gt; 
&lt;p&gt;它们的起源可追溯到 2006 年的谷歌。当时，他们正在评估是采用 GPU、FPGA 还是定制的 ASIC。当时，只有少数应用需要使用专用硬件，他们判断通过从大型数据中心调配多余的 CPU 算力即可满足这些需求。但这一情况在 2013 年发生了变化，当时谷歌的语音搜索功能运行在神经网络上，而内部预测认为，如果该功能发展起来，将需要远超以往的算力。&lt;/p&gt; 
&lt;p&gt;时至今日，TPU 已为谷歌的大多数人工智能服务提供算力支撑。当然，也包括 Gemini 或 Veo 的训练和推理，也包括他们的推荐模型。&lt;/p&gt; 
&lt;p&gt;让我们从底层开始，深入了解一下 TPU 的内部构造。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;02 单个 TPU 芯片内部的架构层级&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;下文图示均以 TPUv4 为例，但其整体布局基本也适用于最新一代 TPU（如 TPUv6p 「Trillium」。TPUv7 「Ironwood」 的细节截至 2025 年 6 月尚未公布）。&lt;/p&gt; 
&lt;p&gt;单颗 TPUv4 芯片的结构如下：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-b7a986ab2586ba485d59aac151bc5dbf109.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;TPU Single Chip + TensorCore&lt;/p&gt; 
&lt;p&gt;每颗芯片内含两个 TPU TensorCore，负责所有计算。（注：面向推理的专用 TPU 仅有一个 TensorCore）。两个 TensorCore 共享同一份内存：CMEM（128 MiB）和 HBM（32 GiB）。&lt;/p&gt; 
&lt;p&gt;而在每个 TensorCore 内部，都有计算单元和较小的内存缓冲区：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;1）矩阵乘法单元 (MXU)&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;这是 TensorCore 的核心部件，是一个 128x128 的脉动阵列（systolic array）。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;脉动阵列的原理稍后说明。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2）向量单元（VPU）&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;负责执行通用的逐元素操作（例如 ReLU、点加/点乘、归约操作）&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;3）向量内存（VMEM；32 MiB）&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;内存缓冲区。HBM 中的数据需先复制到 VMEM，TensorCore 才能开始计算。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;4）标量单元 + 标量内存（SMEM；10 MiB）&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;用于调度 VPU 和 MXU 的执行指令。&lt;/li&gt; 
 &lt;li&gt;负责管理控制流、标量运算和内存地址生成。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;如果你使用的是英伟达（NVIDIA）GPU，那么一些初步观察结果可能会让你大吃一惊：&lt;/p&gt; 
&lt;p&gt;1）TPU 的片上内存单元（CMEM、VMEM、SMEM）远大于 GPU 的 L1/L2 缓存。&lt;/p&gt; 
&lt;p&gt;2）TPU 的 HBM 容量却远小于 GPU 的 HBM。&lt;/p&gt; 
&lt;p&gt;3）负责计算的"核心"（cores）数量明显更少。&lt;/p&gt; 
&lt;p&gt;这与 GPU 架构完全相反 —— GPU 拥有较小的 L1/L2 缓存（以 H100 为例，分别为 256KB 和 50MB）、更大的 HBM（H100 为 80GB）以及数以万计的计算核心（cores）。&lt;/p&gt; 
&lt;p&gt;在我们进一步讨论之前，需明确的是，TPU 与 GPU 同样具备极高的吞吐量。单颗 TPU v5p 芯片可达 500 TFLOPs/sec，由 8960 颗芯片组成的完整 pod 集群可实现约 4.45 ExaFLOPs/sec。而最新的 "Ironwood" TPUv7 每个 pod（9216 颗芯片）据称可达 42.5 ExaFLOPS/sec。&lt;/p&gt; 
&lt;p&gt;要理解 TPU 如何实现这种性能，我们需要深入探究其设计理念。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;03 TPU 的设计理念&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;TPU 通过两大技术支柱和一个核心前提实现了惊人的吞吐量与能源效率：systolic array（脉动阵列） + pipelining（流水线）、Ahead-of-Time (AoT) compilation（预先编译），以及假设绝大多数运算都可通过适配 systolic array（脉动阵列）的方式表达。幸运的是，在现代深度学习（DL）领域，计算的大部分都是矩阵运算，而这些运算都适合使用 systolic array（脉动阵列）。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;3.1 TPU 设计选择之一：Systolic Array + Pipelining&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;问：什么是 Systolic Array？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;答：Systolic Array 是一种硬件设计架构，由相互连接的处理单元（PE）网格组成。每个 PE 执行少量运算（例如乘法和累加运算），并将结果传递给相邻 PE。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-35d3a118a74284c503027134a4294314974.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;这种设计的好处是，数据一旦输入 systolic array（脉动阵列），便无需额外的控制逻辑来处理数据。此外，当脉动阵列的规模足够大时，除输入输出外再无内存读写操作。&lt;/p&gt; 
&lt;p&gt;由于脉动阵列的刚性结构设计（rigid organization），其仅能处理具有固定数据流模式的操作，但幸运的是，矩阵乘法和卷积运算（convolutions）恰好完美适配这种架构范式。&lt;/p&gt; 
&lt;p&gt;不仅如此，pipelining（流水线技术）显然有机会将计算与数据移动重叠执行。下图展示了 TPU 架构上 pipelined pointwise operation （通过流水线技术，加速 pointwise operation（逐点操作） 的执行过程。）的示意图。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-038f3eb06befc2339ee00a80b38e0b91727.gif" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Pipelined Pointwise Operation (from "How to Scale Your Model"&amp;nbsp;[4])&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;旁注：Systolic Arrays（脉动阵列）的局限性 —— 稀疏性&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;我们可以看到，脉动阵列（systolic arrays）非常喜欢稠密矩阵（dense matrices）（即每个 PE 几乎每个时钟周期都处于活跃状态）。然而，其劣势是，相同规模的稀疏矩阵（sparse matrices）无法获得性能提升 —— 即使对于零值元素（zero-valued elements），PE 仍需执行相同数量的计算周期（cycles），导致资源浪费。&lt;/p&gt; 
&lt;p&gt;如若深度学习（DL）领域更倾向于采用更不规则的稀疏性（例如 MoE 架构），应对脉动阵列的这一系统性局限将变得愈发重要。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;3.2 TPU 设计选择之二：预先（AoT）编译 + 减少对缓存的依赖&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;本节将回答 TPU 如何通过软硬件协同设计（TPU + XLA 编译器）来避免使用缓存，从而实现高能效。&lt;/p&gt; 
&lt;p&gt;首先，请记住传统缓存是为了处理不可预测的内存访问模式而设计的。一个应用程序的内存访问模式（memory access patterns），可能与另一个应用程序大相径庭。从本质上讲，缓存允许硬件灵活地适应各种应用场景。这也是 GPU（相较于 TPU）灵活性极高的一个重要原因。&lt;/p&gt; 
&lt;p&gt;然而，缓存访问（以及一般意义上的内存访问）会消耗大量能源。下面是对芯片（45 纳米，0.9V；[18]）上各类操作的能耗粗略估计。这里的主要启示是，&lt;strong&gt;内存的访问和控制占用了大部分的能耗，而算术操作本身的能耗占比则小得多。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-828c725d5f974a2973ca0fdeaa15296501f.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;但是，如果你的应用非常特殊，而且其计算和内存访问模式具有很高的可预测性呢？&lt;/p&gt; 
&lt;p&gt;举个极端的例子，如果我们的编译器能提前确定所有需要的内存访问，那么硬件仅需一个暂存器作为缓冲区就足以满足需求，根本不需要缓存。&lt;/p&gt; 
&lt;p&gt;这正是 TPU 的设计理念所追求的，也是 TPU 使用 XLA 编译器设计以实现这一目标的根本原因。XLA 编译器通过提前分析计算图来生成优化过的程序。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;问：但 JAX 在 TPU 上也运行良好，它们使用 &lt;a href="https://my.oschina.net/u/3233893"&gt;@jit&lt;/a&gt; 吗？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;TPU 上的 JAX+XLA 实际处于 JIT 与 AOT 的混合模式，因此容易产生混淆。当首次调用 JAX 中被 &lt;a href="https://my.oschina.net/u/3233893"&gt;@jit&lt;/a&gt; 修饰的函数时，JAX 会进行代码追踪并生成静态计算图。然后将其传递给 XLA 编译器，在那里被转化为适用于 TPU 的完全静态二进制文件。在最后的转化阶段，编译器会实施针对 TPU 的优化（例如，最大限度地减少内存访问），使整个过程适合 TPU。&lt;/p&gt; 
&lt;p&gt;但有一点需要注意：当输入张量的形状（shape）发生变化时，已编译的 JIT 函数需重新编译并缓存。这就是为什么 JAX 在处理动态填充（dynamic padding）或长度随输入变化的 for 循环层时表现不佳。&lt;/p&gt; 
&lt;p&gt;当然，这种方案虽有优势，却也存在明显的局限。它缺乏灵活性，而对编译器的重度依赖犹如一把双刃剑。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;那么，Google 为何仍要坚持这种设计理念？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;TPU 及其能源效率（TPUv4）&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;前文的能耗示意图并不能精确反映 TPU 的实际情况，此处是 TPUv4 的能耗细目。注意，TPUv4 采用 7nm 工艺，表中 45nm 的数据仅用于对比（[3], [16]）。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-929fea9deb2dd94de8a7dbbbf5af8b32f14.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-bae4859f266ed820ecd3b512e9ceb4f62b0.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;单次操作能耗对比（TPUv4, 7 nm）&lt;/p&gt; 
&lt;p&gt;上方的柱状图展示了具体数值，但需注意，现代芯片采用的是 HBM3 内存，其能耗远低于本图表中显示的 DDR3/4 DRAM。尽管如此，该图仍表明内存操作的能耗仍高出计算操作数个数量级。&lt;/p&gt; 
&lt;p&gt;这恰与 scaling laws 形成呼应：我们非常乐意通过增加浮点运算量（FLOPS）来换取更少的内存操作。因此减少内存操作能带来双重优化收益——不仅提升程序运行速度，还可显著降低能耗。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;04 TPU 的多芯片互联层级结构&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;现在升级到更高层级，观察 TPU 在多芯片环境中的运作方式。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;4.1 托盘层级（即"板卡"；含 4 个芯片）&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-8e3fe72f71ddb1cea8fad910065ba1c07b1.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;单块 TPU 托盘包含 4 个 TPU 芯片或 8 个 TensorCore（简称"核心"）。每块托盘配备独立 CPU 主机（注：推理型 TPU 的每个主机可访问 2 块托盘，因其每芯片仅含 1 个核心）。&lt;/p&gt; 
&lt;p&gt;主机（Host） ⇔ 芯片（Chip）的连接采用 PCIe 接口，但芯片（Chip）⇔芯片（Chip）之间通过 Inter-Core Interconnect（ICI）连接，该接口具备更高带宽。&lt;/p&gt; 
&lt;p&gt;不过 ICI 连接还可进一步扩展至多块托盘。为此，我们需要继续提升到机架层级（Rack level）。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;4.2 机架层级（4x4x4 芯片）&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;TPU 最令人兴奋的特性在于其可扩展性，这一点从机架层级开始显现。&lt;/p&gt; 
&lt;p&gt;一个 TPU 机架包含 64 个 TPU 芯片，通过 4x4x4 三维环面网络互联。如果您看过谷歌的 TPU 宣传资料（如下图），这张图展示的是 8 个 TPU 机架的集群。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-4bd138cd1af2330bfc7d037d4a01afa568f.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;8 个 TPU 机架（TPUv4）&lt;/p&gt; 
&lt;p&gt;但在深入讨论机架之前，我们需要澄清几个容易混淆的术语：机架（Rack）、Pod 和切片（Slice）的区别。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;问：TPU 机架、TPU Pod 和 TPU 切片有何不同？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;不同谷歌资料对这些术语的使用存在差异，有时甚至混用"TPU Pod"和"TPU Slice"。本文采用谷歌 TPU 论文和 GCP 官方文档的定义（[3][7][9]）：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;1）TPU 机架（Rack）&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;包含 64 块芯片的物理单元，也称为「立方体（cube）」。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2）TPU Pod&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;通过 ICI 和光纤连接的 TPU 最大单元。&lt;/li&gt; 
 &lt;li&gt;又称"Superpod"或"Full Pod"。例如 TPUv4 的 TPU Pod 包含 4096 块芯片（或 64 个机架）。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;3）TPU 切片（Slice）&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;介于 4 块芯片到 Superpod 规模之间的任何 TPU 配置组合。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;主要区别在于，TPU 机架和 TPU Pod 是物理计量单位，而 TPU 切片是抽象计量单位。当然，TPU 切片的设置涉及重要的物理拓扑约束，但现阶段我们暂不展开讨论。&lt;/p&gt; 
&lt;p&gt;现在，我们将聚焦物理计量单位：TPU 机架和 TPU Pod。这是因为，理解 TPU 系统的物理连接方式，能更深入地掌握其设计哲学。&lt;/p&gt; 
&lt;p&gt;现在回到 TPUv4 机架的具体结构：&lt;/p&gt; 
&lt;p&gt;单个 TPU 机架通过 ICI 和 OCS（Optical Circuit Switching）技术连接 64 个芯片。实质上，我们通过组合多个托盘（trays）来构建一个 64 芯片的完整系统。这种"将小型单元组装成超级计算机"的设计理念将持续贯穿后续层级。&lt;/p&gt; 
&lt;p&gt;下图展示了 TPUv4 单个机架的拓扑结构。它采用 4x4x4 三维环面网络，其中每个节点都代表一块芯片，蓝色箭头表示 ICI 链路，而各个面上的连接线则代表 OCS（根据文献 [7] 重绘）。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-d720ac051e5ed63612d648da6093008d2c0.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;使用 OCS 的 TPU 单机架架构&lt;/p&gt; 
&lt;p&gt;然而，这张图表引出了两个关键问题：为何 OCS 仅应用于环面结构的表面？换句话说 —— 使用 OCS 的核心优势是什么？共有三大核心优势，我们将在后文再详述另外两点。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;OCS 的优势 #1：环绕连接 (Wraparound)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;通过环形拓扑优化节点间的通信效率。&lt;/p&gt; 
&lt;p&gt;OCS 还承担特定 TPU 配置的环绕连接功能。该设计将两节点间的跳数从最坏情况下 N-1 跳降至每轴 (N-1)/2 跳，因为每条轴均形成一个环形（一维环面拓扑）。&lt;/p&gt; 
&lt;p&gt;随着规模的进一步扩大，这种影响变得更加重要，因为降低芯片间的通信延迟对于高度并行化的实现至关重要。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;附注：并非所有 TPU 都采用 3D 环面拓扑&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;注意，早期 TPU（如 TPUv2/v3）及推理专用 TPU（如 TPUv5e/v6e）使用 2D 环面拓扑而非下文所述的 3D 环面。不过 TPUv7"Ironwood" 虽定位为推理芯片，但其拓扑疑似 3D 环面（注：仅根据官方宣传材料推测）。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-73f622dc9ad94d85b7665298423d5669470.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;2D 环面拓扑示意图&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;4.3 Full Pod 层级（又称 "Superpod"；TPUv4 为 4096 块芯片）&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;正如我们通过互联多个芯片构建 TPU 机架，我们也可连接多个机架组成大型 Superpod。&lt;/p&gt; 
&lt;p&gt;Superpod 特指仅通过 ICI 和 OCS 互联的最大 TPU 集群规模。虽然存在 multi-pod 层级，但这种层级需依赖更慢速的连接方式，后续将展开说明。&lt;/p&gt; 
&lt;p&gt;芯片数量会因版本不同而变化，但 TPUv4 的芯片数量为 4096（即 64 个 4x4x4 芯片的机架）。最新的 TPUv7 "Ironwood" 则高达 9216 块芯片。&lt;/p&gt; 
&lt;p&gt;下图展示了 TPUv4 的一个 Superpod：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-1f3fbc86962bf3edf862e7aa966347174fb.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;TPUv4 Superpod 架构（64 个机架）&lt;/p&gt; 
&lt;p&gt;请注意，每个立方体（即 TPU 机架）是如何通过 OCS 相互连接的，这种设计也支持在 Pod 内灵活划分 TPU 切片。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;采用 OCS 的 TPU 切片&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;我们可在 Pod 内申请 TPU 子集，即 TPU 切片。但即使所需芯片数 (N) 相同，也存在多种拓扑结构可供选择。&lt;/p&gt; 
&lt;p&gt;例如，若总共需要 512 块芯片，可选择立方体 (8x8x8)、条状拓扑 (4x4x32) 或矩形拓扑 (4x8x16)。选择切片的拓扑结构本身就是一个超参数。&lt;/p&gt; 
&lt;p&gt;所选拓扑结构直接影响节点间通信带宽，进而影响各类并行策略的性能表现。&lt;/p&gt; 
&lt;p&gt;以立方体结构（如 8x8x8）为例，它特别适合需要全连接通信的并行计算模式，比如数据并行或张量并行，因为这种拓扑结构能提供最高的二分带宽（bisection bandwidth）。而条状结构（如 4x4x32）则更适用于流水线计算，这种布局可以让顺序排列的计算层之间实现更快速的数据传输（前提是单个计算层能够适配 4x4 芯片的子切片配置）。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-143cbdf8803aec31540f9640581582d1b4f.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;典型 TPU 拓扑示例&lt;/p&gt; 
&lt;p&gt;当然，最优拓扑取决于具体模型结构，其寻优过程本身即是一门学问。TPUv4 论文[9]实测表明，拓扑优化可大大提升吞吐量（注：我不确定第一行指的是哪种 LLM 架构，因为没有具体说明）。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-3f8cb4956e8dbbdc78ec97d12a6ad2c70f0.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;不同拓扑结构的吞吐量优化对比&lt;/p&gt; 
&lt;p&gt;前文阐述了 TPU 切片，但另有一项重要的特性有助于提高 TPU 的运行稳定性。&lt;/p&gt; 
&lt;p&gt;借助 OCS 技术，这些切片无需占据物理连续的机架空间。这正是 OCS 的第二大优势 —— 可能也是其最大优势，但我们此前尚未展开讨论。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;OCS 的优势 #2：可重新配置的非连续多节点切片&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;需注意，这不同于将多个节点硬连在一起来模拟非连续切片。由于 OCS 采用光交换技术而非硬连线架构，跨节点间的物理线缆数量大幅减少，从而支持更大规模的集群扩展（即可构建超大规模 TPU Pod）。&lt;/p&gt; 
&lt;p&gt;这样就可以进行灵活的节点规模配置。例如，假设我们想在单个 Pod 上运行三个任务。虽然传统的调度方式不允许这样做，但 OCS 连接允许我们抽象出节点的物理位置，使整个 Pod 可视为一个"节点资源池"（根据参考文献[6]重绘）。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-4ea87894ad75b8b37c6e3913584dffafa2c.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;单任务可将 Pod 内机架视为"节点资源池"&lt;/p&gt; 
&lt;p&gt;此举不仅提高了 Pod 的利用率，而且能在节点出现故障的情况下简化维护流程。谷歌将其描述为"故障节点的影响范围很小"。但尚不确定其液冷系统在部分节点停机时如何运作。&lt;/p&gt; 
&lt;p&gt;最后，这种灵活的 OCS 还有项延伸应用：我们还可以改变 TPU 切片的拓扑结构（例如将规则环面调整为扭曲环面）。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;OCS 的优势 #3：扭曲环面拓扑&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;此前我们通过改变固定芯片数量下的 (x,y,z) 维度来实现不同的 TPU 切片拓扑结构。本节则聚焦固定维度配置，通过改变布线方式构造新型拓扑。&lt;/p&gt; 
&lt;p&gt;典型案例如下：将常规条状环面改造为扭曲条状环面。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-85b47be563d32b35b9ca154cf668cbd2637.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;常规环面 vs 扭曲环面（来源：TPUv4 论文[9]）&lt;/p&gt; 
&lt;p&gt;扭曲环面拓扑结构能加速扭曲二维平面上的芯片之间的通信，该特性对提升全局通信效率尤其有用。&lt;/p&gt; 
&lt;p&gt;下文将深入分析其具体应用场景。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;使用扭曲环面加速训练&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;理论上，扭曲环面对张量并行（TP）的加速效益最大，因为每层涉及多次 all-gather 和 reduce-scatter 操作。对数据并行（DP）也有适度提升，因为每个训练步需执行 all-reduce 操作，但发生频率较低。&lt;/p&gt; 
&lt;p&gt;想象一下，假设我们训练一个标准的仅解码器架构的 Transformer 模型，并采用多种并行策略来加速训练。下面我们将看到两种场景：&lt;/p&gt; 
&lt;p&gt;场景 #1：4x4x16 拓扑结构（TP+PP；共 256 块芯片）&lt;/p&gt; 
&lt;p&gt;设定 z 轴为流水线 (PP) 维度，二维 TP 维度为 4x4。本质上，假设第 k 层位于 z=k 平面，且每层分片至 16 块芯片。若未明确绘制，默认采用 OCS 最近邻连接。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-c158c58b081b3127519935fe864668599b0.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;TP+PP 的 4x4x16 拓扑架构&lt;/p&gt; 
&lt;p&gt;通过在每个 z=k 平面实施 2D 环面扭曲，可加速 TP 层内芯片通信。由于 PP 层主要依靠点对点通信，因此没有必要沿 PP 层扭曲。&lt;/p&gt; 
&lt;p&gt;注：实际应用中，扭曲环面在芯片数＞4x4 时效益显著。本示例使用 4x4 仅出于可视化的目的。&lt;/p&gt; 
&lt;p&gt;场景 #2：16x4x16 拓扑（DP+TP+PP；共 1024 块芯片）&lt;/p&gt; 
&lt;p&gt;作为延伸方案，我们在前一场景基础上增加 DP 维度（x 轴 4 个实例），即沿 x 轴部署 4 组场景 #1 的模型。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-4e493d83184c5815acc5549e6e4ca1cac25.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;DP+TP+PP 的 16x4x16 拓扑架构&lt;/p&gt; 
&lt;p&gt;请注意，扭曲环面仅应用于每个 DP 模型内的每个 TP 维度（即对每个 z=k 平面实施 4x4 二维扭曲，k 取值 1…16）。DP 维度仅维持基础的环绕连接，使每行构成长度为 16 的水平环。&lt;/p&gt; 
&lt;p&gt;你可能已经发现还有一种拓扑结构方案（如 8x8x16，即 2x2 DP 维度），但这会混合 DP 与 TP 维度 —— 这就变得更加复杂了。具体来说，我们还不清楚如何在 y 轴构建 OCS 环绕连接的同时兼容各 TP 维度的扭曲环面？&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;4.4 Multi-Pod 层级（即"Multislice"；TPUv4 支持 4096+ 块芯片）&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-e021ce9f1d8e82723b22c8fbdf6be01f8e7.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;TPU 层次结构的最终层级是 Multi-pod 架构。此时可将多个 Pod 视为一台大型机器，但 Pod 之间的通信需通过数据中心网络（DCN） 进行 —— 其带宽低于 ICI。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-c5bb1c222fa2d34a24b8b089ee833b622c1.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;通过 DCN 互联的双 Pod 架构&amp;nbsp;[1]&lt;/p&gt; 
&lt;p&gt;PaLM 模型即采用此方案进行训练。在 6144 个 TPUv4 芯片（2 个 Pod）上耗时 56 天完成。下图是 6 个 Pod 中的 TPU 任务分配情况：绿色为 PaLM 任务，红色为空闲状态，其余为其他任务。注意每个方格代表一个 4x4x4 的 TPU 芯片立方体。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-791b4b38218cbfe875837f430b10f698ff7.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;PaLM 训练过程中的 TPU Pod 利用率&amp;nbsp;[6]&lt;/p&gt; 
&lt;p&gt;实现这一架构已属不易，但更关键的是开发者体验设计，具体来说，就是要关注：&lt;strong&gt;如何实现模型扩展过程中系统/硬件层面的最大程度抽象化？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;谷歌的解决方案是：由 XLA 编译器在大规模计算场景下协调芯片间的通信。研究人员只需配置相关参数（如 DP、FSDP、TP 等并行维度及切片数量），XLA 编译器即会根据当前 TPU 拓扑结构自动插入分层集合通信操作（Xu et al, 2021: GSPMD&amp;nbsp;[2]）。我们的目标是在尽可能少修改代码的情况下实现大规模训练。&lt;/p&gt; 
&lt;p&gt;例如，谷歌博客[1]展示了跨多 TPU 切片的 all-reduce 操作分解流程：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-864563517512833776c5a4b7dd55fc20f47.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;XLA 实现的跨 Pod All-Reduce 规约操作&lt;/p&gt; 
&lt;p&gt;这表明 XLA 编译器可以同时处理切片内与切片间的集合通信操作。&lt;/p&gt; 
&lt;p&gt;举个具体例子，在训练模型时，TPU 的拓扑结构可能如下所示。激活值的通信在切片内通过 ICI 进行，而梯度的通信则需跨切片通过 DCN 完成（即在 DCN 的 DP 维度上）[1]。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-f87c248ffd203825db16d7c22f26d12bcb7.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;05 实物图示对照解析&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;结合硬件实拍图理解架构图会更直观，以下为综合解析。&lt;/p&gt; 
&lt;p&gt;若看过谷歌 TPU 宣传资料，可能见过下图：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-ae14450a22f237528185172376402b3a6d3.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;8 个 TPU 机架（TPUv4）&lt;/p&gt; 
&lt;p&gt;此图为 8 个 TPU Pods 的集群，每个单元即前述的 4x4x4 三维环面架构。一个 Pod 中的每一行有 2 个托盘，这意味着每一行有 8 个 TPU 芯片。&lt;/p&gt; 
&lt;p&gt;单块 TPUv4 托盘实拍图：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-b5b9d5a5afd11a5c6d6ace8039c121749ab.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;请注意，图中简化为只有一个 PCIe 端口，但实际托盘上有 4 个 PCIe 端口（在左侧） —— 每个 TPU 一个。&lt;/p&gt; 
&lt;p&gt;单芯片结构图：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-5d7b5b890cac752887d5cf0368066797a60.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;TPUv4 芯片：中央是 ASIC + 4 组 HBM 内存堆栈&lt;/p&gt; 
&lt;p&gt;中央区域为 ASIC 芯片，周围 4 个区块为 HBM 内存堆栈。因 TPUv4 内含 2 个 TensorCore，故配置 4 组 HBM 内存堆栈。&lt;/p&gt; 
&lt;p&gt;未找到 TPUv4 芯片平面图，此处展示结构近似的 TPUv4i（推理芯片），其仅含 1 个 TensorCore[3]：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-77ee6c51de3ea94847bc8083d166140baba.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;可见 CMEM（芯片内存）在 TPUv4i 的布局中占据了相当大的空间。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;06 致谢&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;感谢 Google TPU Research Cloud（TRC）提供的 TPU 资源支持！&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;[1] Google Blog: TPU Multi-Slice Training（&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcloud.google.com%2Fblog%2Fproducts%2Fcompute%2Fusing-cloud-tpu-multislice-to-scale-ai-workloads%EF%BC%89" target="_blank"&gt;https://cloud.google.com/blog/products/compute/using-cloud-tpu-multislice-to-scale-ai-workloads）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[2] Xu, et al. "GSPMD: General and Scalable Parallelizaton for ML Computation Graphs"（&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2105.04663%EF%BC%89" target="_blank"&gt;https://arxiv.org/pdf/2105.04663）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[3] Jouppi et al. "Ten Lessons From Three Generations Shaped Google's TPUv4i"（&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgwern.net%2Fdoc%2Fai%2Fscaling%2Fhardware%2F2021-jouppi.pdf%EF%BC%89" target="_blank"&gt;https://gwern.net/doc/ai/scaling/hardware/2021-jouppi.pdf）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[4] How to Scale Your Model - TPUs（&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fjax-ml.github.io%2Fscaling-book%2Ftpus%2F%EF%BC%89" target="_blank"&gt;https://jax-ml.github.io/scaling-book/tpus/）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[5] Domain Specific Architectures for AI Inference - TPUs（&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ffleetwood.dev%2Fposts%2Fdomain-specific-architectures%23google-tpu%EF%BC%89" target="_blank"&gt;https://fleetwood.dev/posts/domain-specific-architectures#google-tpu）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[6] HotChips 2023: TPUv4（&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhc2023.hotchips.org%2Fassets%2Fprogram%2Fconference%2Fday2%2FML%2Btraining%2FHC2023.Session5.ML_Training.Google.Norm_Jouppi.Andy_Swing.Final_2023-08-25.pdf%EF%BC%89" target="_blank"&gt;https://hc2023.hotchips.org/assets/program/conference/day2/ML+training/HC2023.Session5.ML_Training.Google.Norm_Jouppi.Andy_Swing.Final_2023-08-25.pdf）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[7] Google Cloud Docs: TPUv4（&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcloud.google.com%2Ftpu%2Fdocs%2Fv4%EF%BC%89" target="_blank"&gt;https://cloud.google.com/tpu/docs/v4）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[8] Jouppi et al. "In-Datacenter Performance Analysis of a Tensor Processing Unit" -- TPU origins paper（&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F1704.04760%EF%BC%89" target="_blank"&gt;https://arxiv.org/abs/1704.04760）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[9] Jouppi et al. "TPU v4"-- TPUv4 paper（&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2304.01433%EF%BC%89" target="_blank"&gt;https://arxiv.org/abs/2304.01433）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[10] PaLM training video（&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D0yPFBxkOKRY%EF%BC%89" target="_blank"&gt;https://www.youtube.com/watch?v=0yPFBxkOKRY）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[11] HotChips 2021: "Challenges in large scale training of Giant Transformers on Google TPU machines"（&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhc33.hotchips.org%2Fassets%2Fprogram%2Ftutorials%2FHC2021.Google.Sameer%2BKumar.pdf%EF%BC%89" target="_blank"&gt;https://hc33.hotchips.org/assets/program/tutorials/HC2021.Google.Sameer+Kumar.pdf）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[12] HotChips 2020: "Exploring Limits of ML Training on Google TPUs"（&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhc32.hotchips.org%2Fassets%2Fprogram%2Ftutorials%2FHC2020.Google.SameerKumarDehaoChen.v02.pdf%EF%BC%89" target="_blank"&gt;https://hc32.hotchips.org/assets/program/tutorials/HC2020.Google.SameerKumarDehaoChen.v02.pdf）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[13] Google Blog: Ironwood（&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.google%2Fproducts%2Fgoogle-cloud%2Fironwood-tpu-age-of-inference%2F%EF%BC%89" target="_blank"&gt;https://blog.google/products/google-cloud/ironwood-tpu-age-of-inference/）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[14] HotChips 2019: "Cloud TPU: Codesigning Architecture and Infrastructure"（&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fold.hotchips.org%2Fhc31%2FHC31_T3_Cloud_TPU_Codesign.pdf%EF%BC%89" target="_blank"&gt;https://old.hotchips.org/hc31/HC31_T3_Cloud_TPU_Codesign.pdf）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[15] ETH Zurich's Comp Arch Lecture 28: Systolic Array Architectures（&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DXkgtANeDrm8%EF%BC%89" target="_blank"&gt;https://www.youtube.com/watch?v=XkgtANeDrm8）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[16] Patterson presentation: "A Decade of Machine Learning Accelerators: Lessons Learned and Carbon Footprint"（&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.cs.ucla.edu%2Fwp-content%2Fuploads%2Fcs%2FPATTERSON-10-Lessons-4-TPU-gens-CO2e-45-minutes.pdf%EF%BC%89" target="_blank"&gt;https://www.cs.ucla.edu/wp-content/uploads/cs/PATTERSON-10-Lessons-4-TPU-gens-CO2e-45-minutes.pdf）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[17] Camara et al. "Twisted Torus Topologies for Enhanced Interconnection Networks."（&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fpersonales.unican.es%2Fvallejoe%2FPublications%2FC%25C3%25A1mara%2B-%2BTPDS%2710%2B-%2BTwisted%2BTorus%2BTopologies%2Bfor%2BEnhanced%2BInterconnection%2BNetworks.pdf%EF%BC%89" target="_blank"&gt;https://personales.unican.es/vallejoe/Publications/C%C3%A1mara+-+TPDS'10+-+Twisted+Torus+Topologies+for+Enhanced+Interconnection+Networks.pdf）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[18] Horowitz article: "Computing's Energy Problem(and what we can do about it)"（&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgwern.net%2Fdoc%2Fcs%2Fhardware%2F2014-horowitz-2.pdf%EF%BC%89" target="_blank"&gt;https://gwern.net/doc/cs/hardware/2014-horowitz-2.pdf）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;END&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;本期互动内容 🍻&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;❓&lt;strong&gt;您更倾向 TPU 的专用化路线（牺牲灵活性换取能效），还是 GPU 的通用化路线（保留灵活性但能耗较高）？请结合您的应用场景说明理由。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;本文经原作者授权，由 Baihai IDP 编译。如需转载译文，请联系获取授权。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;原文链接：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhenryhmko.github.io%2Fposts%2Ftpu%2Ftpu.html" target="_blank"&gt;https://henryhmko.github.io/posts/tpu/tpu.html&lt;/a&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/IDP/blog/18686348</link>
      <guid isPermaLink="false">https://my.oschina.net/IDP/blog/18686348</guid>
      <pubDate>Wed, 30 Jul 2025 07:14:07 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>Claude Code 支持在单会话中添加多个工作目录</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Anthropic 旗下 AI 编程工具 Claude Code 现已支持在单个会话中&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2F_catwu%2Fstatus%2F1950288312033562751" target="_blank"&gt;添加来自不同位置的多个工作目录&lt;/a&gt;，方便用户进行跨项目操作。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1728" src="https://static.oschina.net/uploads/space/2025/0730/145517_KA6C_2720166.png" width="1280" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;据介绍，用户输入命令/add-dir 即可添加新的工作目录，让模型能够跨多个项目或文件夹进行操作和分析。&lt;/p&gt; 
&lt;p&gt;添加多个目录有助于：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;单一代码库：无需切换会话即可跨目录工作&lt;/li&gt; 
 &lt;li&gt;共享配置：从任何地方访问记忆、待办事项或其他文件&lt;/li&gt; 
 &lt;li&gt;跨项目工作：在仓库间迁移代码&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.anthropic.com%2Fen%2Fdocs%2Fclaude-code%2Foverview" target="_blank"&gt;详情查看文档&lt;/a&gt;。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/363159</link>
      <guid isPermaLink="false">https://www.oschina.net/news/363159</guid>
      <pubDate>Wed, 30 Jul 2025 07:01:07 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>2025 年用户增速最快亿级 APP 榜单发布，DeepSeek 位居第一</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;根据 Quest Mobile 的最新数据，2025 年 6 月活跃用户规模超过 1 亿且同比增长率最高的前 20 款应用程序新鲜出炉。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;在这份榜单中，新兴的 AIGC 应用 DeepSeek 以 1.63 亿的月活跃用户数量拔得头筹，成为 AIGC 行业的领军者。尽管 DeepSeek 是新上线的应用，导致其同比增长率无法显示，但其快速崛起已经引起了市场的广泛关注。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;img height="407" src="https://oscimg.oschina.net/oscnet/up-523191ebc93cc7e261abb37db7e5fee206f.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;与此同时，豆包在 AIGC 领域中表现同样亮眼，月活跃用户达到了 1.41 亿，同比增幅高达 410.69%。这意味着豆包在短短一年内增加了 1.13 亿的月活跃用户，显示出其强大的市场吸引力。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;在视频娱乐行业，红果免费短剧同样表现不俗，月活跃用户数达到 2.12 亿，同比增长率为 178.99%。在支付领域，云闪付的月活跃用户也达到了 1.88 亿，同比增长率为 78.50%。此外，这份榜单还涵盖了多个行业的知名应用，如中国移动、大众点评和米家等。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;总的来看，2025 年的 APP 用户增长趋势显示出 AIGC 和在线视频等领域的强劲潜力，随着用户需求的不断演变，这些应用有望在未来继续引领市场潮流。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/363155</link>
      <guid isPermaLink="false">https://www.oschina.net/news/363155</guid>
      <pubDate>Thu, 17 Jul 2025 06:45:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>腾讯开源「短视频理解模型」 ARC-Hunyuan-Video-7B</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;腾讯发布了一款名为&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farc.tencent.com%2Fzh%2Fdocument%2FARC-Hunyuan-Video-7B" target="_blank"&gt;ARC-Hunyuan-Video-7B&lt;/a&gt;的开源多模态模型，专为真实世界短视频进行「结构化理解 (Structured Video Comprehension)」而设计，具备强大的跨模态推理和时间感知能力。&lt;/p&gt; 
&lt;p&gt;该模型旨在解决用户生成内容（如 TikTok、微信视频号视频）中常见的复杂视觉元素、高信息密度和快节奏等挑战。模型通过端到端处理视觉、音频和文本信号，实现对视频的深度结构化理解。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0730/144325_Cgc2_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0730/144337_XSCf_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;ARC-Hunyuan-Video-7B 引入了结构化视频理解的新范式，具备多项关键能力：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;通过同步处理原始视听信号进行复杂的跨模态推理；&lt;/li&gt; 
 &lt;li&gt;精确的时间感知能力；&lt;/li&gt; 
 &lt;li&gt;通过包含强化学习（RL）的多阶段训练实现的强大推理能力。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;ARC-Hunyuan-Video-7B 基于 Hunyuan-7B 视觉语言模型构建，并增加了额外的音频编码器以实现视听同步，同时采用时间戳叠加机制来增强时间感知。&lt;/p&gt; 
&lt;p&gt;该模型已在 Hugging Face 上开源，并提供了推理代码（包括 vLLM 版本）和 API 服务。官方表示，在 H20 GPU 上，处理一分钟视频的推理时间仅为 10 秒。同时，团队也发布了指令调优的训练代码。&lt;/p&gt; 
&lt;p&gt;https://huggingface.co/TencentARC/ARC-Hunyuan-Video-7B&lt;br&gt; https://github.com/TencentARC/ARC-Hunyuan-Video-7B&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/363154</link>
      <guid isPermaLink="false">https://www.oschina.net/news/363154</guid>
      <pubDate>Thu, 17 Jul 2025 06:43:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>京东开源 Genie 智能体 8 大亮点</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;p style="color:#222222; margin-left:0px; margin-right:0px; text-align:justify"&gt;&lt;span style="color:#4d4d4d"&gt;京东开源的 Genie 智能体的 8 大亮点：&lt;br&gt; 可插拔多 Agent 和多种工具&lt;br&gt; 迭代式规划&lt;br&gt; 跨任务上下文和文件共享&lt;br&gt; 数字员工提升用户体验&lt;br&gt; 大模型+搜索构建深度搜素&lt;br&gt; CodeTool 构建智能代码生命周期管理&lt;br&gt; 精心打磨的 System Prompt&lt;br&gt; 可配置的 MCP Server&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//2343b439a2243a95f8d2c5f8f2116b33.jpeg" referrerpolicy="no-referrer"&gt; 
 &lt;p style="margin-left:0px; margin-right:0px"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//0ddf82ca93c3e10913e6fa3a23bdd2bc.jpeg" referrerpolicy="no-referrer"&gt; 
 &lt;p style="margin-left:0px; margin-right:0px"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//7d3b0ac3731f217801ab1194f0b9c2e4.jpeg" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/4090830/blog/18686023</link>
      <guid isPermaLink="false">https://my.oschina.net/u/4090830/blog/18686023</guid>
      <pubDate>Thu, 17 Jul 2025 06:36:00 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>Meta 允许求职者在编程面试中使用 AI</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;据报道，Meta 正在开发一种新型编程面试，候选人可使用 AI 助手，以更贴近未来工作环境。该公司还招募现有员工参与模拟 AI 辅助面试，以优化面试流程。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0730/142510_ttrT_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;404 Media 获得的内部通讯&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.404media.co%2Fmeta-is-going-to-let-job-candidates-use-ai-during-coding-tests%2F" target="_blank"&gt;显示&lt;/a&gt;，Meta 在本月早些时候发布了一则内部公告：「AI 编程面试 —— 招募模拟候选人」。公告写道：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Meta 正在开发一种新型编程面试，让候选人可以使用 AI。这更能代表我们未来员工将要工作的开发环境，同时也让基于 LLM 的作弊变得不那么有效。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Meta CEO 马克・扎克伯格多次公开表示，预计到 2025 年，Meta 将拥有能担任中级工程师的 AI，未来大部分代码将由 AI 编写。&lt;/p&gt; 
&lt;p&gt;尽管许多科技公司鼓励工程师使用 AI，但允许面试中使用 AI 尚属罕见，引发业界争议，有观点认为这可能导致新一代程序员过度依赖 AI 而缺乏问题解决能力。Meta 发言人表示，此举旨在测试如何为申请人提供与日常工作中相同的 AI 工具。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/363146</link>
      <guid isPermaLink="false">https://www.oschina.net/news/363146</guid>
      <pubDate>Thu, 17 Jul 2025 06:23:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>HarmonyOS 5 终端数量破千万</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;华为常务董事、终端 BG 董事长余承东发文称，截至 7 月 30 日，鸿蒙 5 终端数量已经突破了 1000 万。「非常感谢每一位伙伴、开发者和用户的支持和反馈！」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img alt="" height="533" src="https://oscimg.oschina.net/oscnet/up-28b80ec8e1707997154312822ba6469bbd9.webp" width="300" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;余承东称，发布不到一年，便以突破性速度完成生态蜕变，微信、支付宝、抖音、淘宝等国民级 APP 共同创造了「鸿蒙速度」。这一里程碑是标志鸿蒙生态开启正循环的重要起点！&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在今年 6 月的开发者大会上，余承东宣布华为鸿蒙注册开发者数量已突破 800 万，目前已有超 3 万个鸿蒙应用和元服务全速开发 / 更新中。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/363145</link>
      <guid isPermaLink="false">https://www.oschina.net/news/363145</guid>
      <pubDate>Thu, 17 Jul 2025 06:21:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>豆包·图像编辑模型 3.0 发布</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;火山引擎&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FSumKouPO-7zllWnbnZPPGQ" target="_blank"&gt;宣布&lt;/a&gt;正式发布豆包·图像编辑模型 3.0、豆包·同声传译模型 2.0，豆包大模型 1.6 系列全新升级。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;「面向 Agent 开发和落地，火山引擎持续优化 AI 云原生全栈服务，开源扣子核心能力，并发布企业自有模型托管方案、Responses API 等多个模型服务和工具产品，为企业和开发者构建 Agent、落地 AI 夯实基础设施。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;根据介绍，豆包·图像编辑模型 SeedEdit 3.0，具备更强的指令遵循能力、图像保持能力和更强的图像生成质量。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="263" src="https://oscimg.oschina.net/oscnet/up-b7778cf08ae149be0084fa9d16f514815a1.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;用户只需通过自然语言指令，即可完成消除多余内容、改变光影效果、替换文字等操作。同时，豆包·图像编辑模型 3.0 具备对风格、结构与语义的精准控制力，能够像人类大脑一样理解指令、深度思考，解锁更多创新的修图场景，例如图像风格转换、变换材质、变化人物姿势、根据提示词进行推理等 P 图玩法。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;豆包·图像编辑模型 3.0 可广泛应用于影像创作、广告营销、游戏宣传等领域，企业用户可在火山方舟平台调用该模型 API，个人用户可使用即梦或豆包 App 的「豆包 P 图」功能体验。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;此次全新发布的豆包·同声传译模型 Seed-LiveInterpret 2.0，支持全双工语音理解和生成框架，将传统机器同传的语音延迟从 8-10 秒降低到 2-3 秒，实现文本与语音的同步生成；无需提前录制，一边说话一边采样，实现 0 样本声音复刻，让同一个人同音色开口说外语，带来更沉浸的体验。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;今年 6 月，豆包大模型 1.6 系列多个模型正式发布。此次，极速版 Doubao-Seed-1.6-flash 模型在保持出色的视觉理解能力的同时，升级了代码、推理、数学等大语言模型能力。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;Doubao-Seed-1.6-flash 模型，非常适合智能巡检、手机助手、智能硬件等对模型效果、速度和成本都有要求的大规模商业化场景。该模型具有业界领先的极低延迟，TPOT 仅 10ms，并依然具备强大的视觉理解能力。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;价格上，Doubao-Seed-1.6-flash 在输入文本长度 0-32k 的区间中（企业使用量最大），每百万 tokens 输入仅需 0.15 元，输出仅 1.5 元。在真实的客户案例中，该模型帮助客户延迟下降 60%，成本降低 70%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;此外，火山引擎发布全模态向量化模型 Seed1.6-Embedding，首次实现了「文本+图像+视频」混合模态的融合检索，帮助企业构建更强大的多模态知识库。在权威测评榜单中，该模型包揽了多模态全面任务、中文文本的 SOTA 成绩。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/363144</link>
      <guid isPermaLink="false">https://www.oschina.net/news/363144</guid>
      <pubDate>Thu, 17 Jul 2025 06:12:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>开源基础设施项目 AGNTCY 加入 Linux 基金会</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;Linux 基金会发文&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FUhkUiPjmmkPr07DfWeb5ew" target="_blank"&gt;宣布&lt;/a&gt;了 AGNTCY 的加入。AGNTCY 是一个开源基础设施，支持不同厂商和框架的 AI 代理之间的发现、身份认证、消息传递和可观测性。Cisco、Dell Technologies、Google Cloud、Oracle 和 Red Hat 作为创始成员加入了 Linux 基金会旗下的 AGNTCY 项目。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;根据介绍，AGNTCY 最初由 Cisco 于 2025 年 3 月开源，合作伙伴包括 LangChain 和 Galileo。项目已扩大到超过 65 家支持公司，提供「代理互联网」的基础设施，这是一层新的协作层，使多智能体系统无论由谁开发、运行在哪里都能协同工作。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="262" src="https://oscimg.oschina.net/oscnet/up-ef4177066b1da696075c9462c652e3063f1.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;随着 AI 代理的普及，碎片化和厂商孤岛问题加剧，阻碍代理间的安全通信、上下文共享和跨平台协作。AGNTCY 项目通过开放、通用的基础设施，为开发者和组织提供安全的代理身份、可靠的消息传递和端到端的可观测性，提升透明度、性能、效率和信任度。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;此外，AGNTCY 项目兼容主流 AI 代理技术，包括最近贡献给 Linux 基金会的 Agent2Agent（A2A）项目和 Anthropic 的 Model Context Protocol（MCP）。AGNTCY 通过目录使 A2A 代理和 MCP 服务器可发现，利用可观测 SDK 提升透明度，并支持基于 Secure Low Latency Interactive Messaging（SLIM）协议的消息传输，从而实现动态多代理环境。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="234" src="https://oscimg.oschina.net/oscnet/up-7f3a1ce042aed946646fda984b12bb530d0.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;Linux 基金会执行董事 Jim Zemlin 表示：「AGNTCY 项目为自主代理间的安全、互操作协作奠定基础。我们很高兴欢迎 AGNTCY 项目加入 Linux 基金会，确保其基础设施保持开放、中立和社区驱动。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;作为代理协作的综合基础设施层，AGNTCY 项目的核心功能包括：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;代理发现：利用 Open Agent Schema Framework（OASF），使任何代理都能发现并理解其他代理的能力。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;代理身份：提供密码学验证的身份和访问控制，确保代理能跨组织安全行动。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;代理消息：通过 SLIM 支持多模式、人工参与和量子安全通信。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;代理可观测性：提供端到端可观测工具，帮助评估和调试跨厂商和框架的复杂多代理工作流。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/363143</link>
      <guid isPermaLink="false">https://www.oschina.net/news/363143</guid>
      <pubDate>Thu, 17 Jul 2025 05:58:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>记 Codes 研发项目管理平台 —— 拖拽式无代码 CICD 创新实现</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;h1&gt;&lt;strong&gt;背景&lt;/strong&gt;&amp;nbsp;&lt;/h1&gt; 
&lt;p style="color:#000000; margin-left:auto; margin-right:auto; text-align:left"&gt;&lt;span&gt;DevOps 中，少不了要做流水线编排，不管用 jenkins 实现还是其他工具，或是对这些工具的包装实现，都是换一个地方写脚本。流水线&lt;span&gt;编排&lt;span&gt;算技术不？，无他唯手熟尔，所以&lt;span&gt;流水线&lt;span&gt;编排更偏向于技巧而不是技术。有没有好的办法，让团队中任何成员，哪怕实习生都可以在相关环境中自己编排流水线并执行相关测试，通过 0 脚本来提高工作效率，把精力专注在更高优先级的事项上&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;；对于编码能力弱的测试同学来说，能帮助他们轻松实现测试左右移。&lt;/p&gt; 
&lt;h1&gt;有没有办法呢 ？&lt;/h1&gt; 
&lt;p style="color:#000000; margin-left:auto; margin-right:auto; text-align:left"&gt;&lt;span&gt;Codes 产品团队始终以用户为中心，从用户的使用场景来思考问题。解决用户痛点，如何让用户爽，就如何实现，这也是我们创新的源动力，换句话说就是，不固守陈规，拥抱&lt;strong&gt;零基思维；&lt;/strong&gt;于是又一个大胆的想法在酝酿了。让 ci cd 也无代码，无脚本化。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:auto; margin-right:auto; text-align:left"&gt;&lt;span&gt;我们&lt;span&gt;做了调研及技术验证，&lt;span&gt;技术上完全可行，以可视化方式&lt;span&gt;拖拽&lt;span&gt;一系列相关组件，然后把他们编排到一个流水线中，只要设置好相关属性&lt;span&gt;，最后解析这些组件的属性，生成调用相关编排的 Job 即可,，最后调 jenkins api ，把这些流水线及相关 job 发布到 jenkins 中 。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;span&gt;&lt;span&gt;技术选型&lt;/span&gt;&lt;/span&gt;&lt;/h1&gt; 
&lt;p style="color:#000000; margin-left:auto; margin-right:auto; text-align:left"&gt;&lt;span&gt;jenkins 虽然老旧且比较重，但是生态完善，有一系列插件。其他的比如：&lt;strong&gt;&lt;span&gt;GoCD、&lt;strong&gt;&lt;span&gt;Spinnaker、&lt;strong&gt;&lt;span&gt;Buildkite&lt;span&gt;等开源的，虽然轻量，但生态没 jenkins 强，所以我们选择于基 jenkins 来做改造 。&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;span&gt;&lt;span&gt;上功能截图&lt;/span&gt;&lt;/span&gt;&lt;/h1&gt; 
&lt;h2&gt;&lt;span&gt;&lt;span&gt;线水线编排：&lt;/span&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p style="color:#000000; margin-left:auto; margin-right:auto; text-align:left"&gt;&lt;span&gt;1 拖拽代码仓库图标过来，双击选一个仓库。2 再拖一个构建图标过来，如 maven 或 NPM &amp;nbsp;&lt;span&gt;。3 再拖通知组件过来，当构建完后可以发钉钉消息。4 &amp;nbsp;拖&lt;span&gt;拽&lt;span&gt;要部署主机图标过来，选一个部署的主机。5 如需要代码扫描，还可拖 sorna qube 过来 &amp;nbsp;。其他组件一样的实现思路。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="487" src="https://oscimg.oschina.net/oscnet/up-9dc334861abd29848e631cd438fb8bf73bc.png" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:auto; margin-right:auto; text-align:left"&gt;&lt;span&gt;同类工具都是以 web 形式写脚本，然后把 Job 以可视化形式显示出来，不是&lt;span&gt;拖拽式做流水线编排。Codes 的实现方式要麻烦得多，但是 Codes 的实现方式，让用户爽很多！&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;&lt;span&gt;设置相关组件属性&lt;/span&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;img height="471" src="https://oscimg.oschina.net/oscnet/up-f2c850b1fe9e01213884398357045e4c45d.png" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;&lt;span&gt;执行线水线：&lt;/span&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;img height="417" src="https://oscimg.oschina.net/oscnet/up-017ec21e42e4a9bf25874b747c75ab921d5.png" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;查看构建日志&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;img height="346" src="https://oscimg.oschina.net/oscnet/up-2b32c59582a0edc81db8a745df3f1338706.png" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="453" src="https://oscimg.oschina.net/oscnet/up-dc26bf54269fe9325afd63b5b92fdeeb5a7.png" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;编排前的准备：&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/h2&gt; 
&lt;p style="color:#000000; margin-left:auto; margin-right:auto; text-align:left"&gt;&lt;strong&gt;只需要维护好代码仓库信息，要部署的主机信息，构建完成后钉钉通知等&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="318" src="https://oscimg.oschina.net/oscnet/up-0b6de4b756874dee5aab1d7c573706819af.png" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h2&gt;从安装到编排省时省力&lt;/h2&gt; 
&lt;p style="color:#000000; margin-left:auto; margin-right:auto; text-align:left"&gt;&lt;span&gt;工具有了，安装复杂也不行，一般来讲，手动搭建 CI CD 环境很费时，为了解决安装部署我们全做成镜像了，且也做好了配置，执行一个 Curl 便可&lt;span&gt;&amp;nbsp;0 配置&lt;span&gt;安装。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="205" src="https://oscimg.oschina.net/oscnet/up-18c1063f630699bc6269753069cbad076bf.png" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:auto; margin-right:auto; text-align:left"&gt;&lt;span&gt;来看看之前我们写的安装文档 107 页，要是不容器化，安装配置太麻烦了，安装&lt;span&gt;配置&lt;span&gt;时间相当可怕&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="791" src="https://oscimg.oschina.net/oscnet/up-e66a8a0d9e6632ea46032f4f10cc8201d35.png" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;span&gt;&lt;span&gt;总结：&lt;/span&gt;&lt;/span&gt;&lt;/h1&gt; 
&lt;p style="color:#000000; margin-left:auto; margin-right:auto; text-align:left"&gt;&lt;span&gt;通过 Codes 创新的无代码实现后，ci cd 可以忘记 jenkins 的存在，安装整套环境也是只是分分钟的事，&lt;span&gt;拖拽式流水线编排&lt;span&gt;确实省时省事。虽然在实现这一目标的过程中我们费了很多心思也倍受一些技术问题的折磨，但是为了让用户爽，我们受点折磨算不了什么，Codes 团队从来不为炫技，只为真心解决用户的痛点而打磨产品。&lt;span&gt;&lt;strong&gt;匠心打磨，持续创新是 Codes 的产品基因&lt;/strong&gt;。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:auto; margin-right:auto; text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="color:#000000; margin-left:auto; margin-right:auto; text-align:left"&gt;Codes 简介 Codes 是国内首款重新定义 SaaS 模式的开源项目管理平台，支持云端认证、本地部署、全部功能开放（不限制功能，商业版用免费版只有用户数的差别），并且对 15 人（最多 20 人）以下团队免费。它通过创新的方式简化研发协同工作，使敏捷开发更易于实施。并提，供低成本的敏捷开发解决方案，如事件驱动实现的 「事找人」、自动生成工作周报，多事项闭环迭代，日报与工时填报融合、同步在线离线测试用例、流程化管理缺陷、低代码接口自动化测试和 CI/CD，以及基于迭代的研发管理和测试管理等，践行敏捷开发。全面的功能覆盖，有需求池、原型管理、工单管理、工作汇报、需求管理、任务管理、测试管理、缺陷管理、自动化测试、项目文档、工时进度管理、风险管理、项目管理（支持多种模式），统计分析等功能。适用场景：无论是需要需要全面的项目管理和协作功能的中大型企业，还是追求轻量化和易用性的中小型团队都适用。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1179" src="https://oscimg.oschina.net/oscnet/up-f433956cae0591b9cda9b2c51c8070a38bf.png" width="1936" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/363120</link>
      <guid isPermaLink="false">https://www.oschina.net/news/363120</guid>
      <pubDate>Thu, 17 Jul 2025 03:42:00 GMT</pubDate>
      <author>来源: 资讯</author>
    </item>
    <item>
      <title>开源中国亮相 WAIC 2025，联合发布《国际人工智能开源合作倡议》</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;7 月 26-29 日，2025 世界人工智能大会暨人工智能全球治理高级别会议（简称「WAIC 2025」）在上海成功举办。会上，开源中国等多家单位共同发布《国际人工智能开源合作倡议》，共同探讨开源与人工智能融合发展的未来方向。&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;《国际人工智能开源合作倡议》正式发布&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-bd50bdf6400c4f4f94d6bcec1db2dcbddfb.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;开幕式上，工业和信息化部推动&lt;strong&gt;中国 —&lt;/strong&gt; &lt;strong&gt;金砖国家人工智能发展与合作中心联合开放原子开源基金会、开源中国、中国开发者网络&lt;/strong&gt;等机构，共同发布了《国际人工智能开源合作倡议》。鼓励各方以开源为纽带，深化人工智能领域开源合作，携手打造人工智能开源开放生态体系。&lt;/p&gt; 
&lt;p&gt;当前，由智能化驱动的全球新一轮科技革命和产业变革加速演进，人工智能新技术不断突破、新业态持续涌现、新应用加快拓展，开源已经成为人工智能创新发展的重要引擎。倡议呼吁全球开发者、企业、学术机构与公共部门携手，以开源为纽带，支持产学研各界，共商技术创新路线、共促技术成果赋能、共建开放包容社区、共享时代发展红利。&lt;/p&gt; 
&lt;p&gt;倡议提出五大方向：一是引领创新，驱动技术新突破。鼓励开源合作创新，分享人工智能领域的研究成果、技术经验，缩短创新链路。二是培育生态，探索共治新路径。支持开源基金会等开源组织加强沟通交流，打造开放包容的国际开源社区，提供良好的基础设施，促进算力开放、数据共享、算法开源、应用探索，引导人工智能标准共建，促进开源生态与标准化进程协同发展。三是赋能转化，加速应用新进程。加速人工智能创新前沿技术成果转化，促进智能体协议等基础规则共识互认，探索从研发端到产业应用的高效落地路径，以开源协作机制弥合技术研发与市场应用间的鸿沟。四是守护权益，尊重创新新价值。知识产权保护是确保开源项目可持续发展的关键，尊重各方的知识产权，鼓励行业自律，支持开源许可协议跨境互认和合规使用，促进开源标准制定，保护开发者和社区的合法权益。五是开放共享，普惠全球新未来。秉持「智能向善」的理念，鼓励不同国家、不同领域的开发者参与跨界合作，促进多元文化的交流与融合。&lt;/p&gt; 
&lt;p&gt;开源中国致力于为开发者搭建高效的开源协作平台，汇聚全球优质开源资源，助力国内企业在开源技术创新上不断突破。在数字技术高速演进、人工智能重塑产业格局的时代背景下，构建开放、包容、可持续的全球协作机制显得尤为重要。开源中国将继续秉持开放精神，深度参与社区建设，携手全球伙伴共建共享，为推动构建人类数字命运共同体贡献更多开源力量。&lt;/p&gt; 
&lt;h3&gt;关于开源中国&lt;/h3&gt; 
&lt;p&gt;开源中国（成立于 2008 年）致力于为开发者搭建高效的开源协作平台与 AI 创新支持，助力国内企业在开源技术创新上不断突破，通过覆盖开源内容与活动、软件工程研发工具、人才培训和 AI 开发平台等多元产品体系，搭建起从社区学习、协作、研发全流程到创新高质量落地的完整链条，为中国开源生态与 AI 创新提供坚实技术基础和活力生态。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-38740ec5ecaf47b6d70b3fc235c78d84cda.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;2013 年，开源中国发布代码托管平台 Gitee ，并于 2020 年开始牵头建设工信部国家开源托管平台项目。Gitee 于 2017 年上线发布针对企业级的研发效能平台 Gitee 企业版。截至目前，Gitee 已经服务 1350 万开发者用户、36 万家企业以及 2000 多家高等院校。&lt;/p&gt; 
&lt;p&gt;2020 年以来，开源中国深耕 DevOps 全生命周期国产替代方案，在满足开发者需求的同时，打造出一个自主创新、安全可信的本土开源软件工具与生态，减少开发者对海外开源软件的过度依赖，构建安全可控的中国信息化体系。当前 Gitee DevOps 工具链已在金融、军工等关键领域实现 80% 市场渗透率，成为信创替代工程的标杆案例，验证了开源商业化的中国路径。&lt;/p&gt; 
&lt;p&gt;在人工智能时代，开源中国于 2024 年推出 AI 大模型平台模力方舟，首创「模型数据 - 算力调度 - 应用开发」全栈服务体系。&lt;/p&gt; 
&lt;p&gt;截至目前，模力方舟已汇聚 10000+ 模型，覆盖文本生成、图像生成、语义理解、多模态等任务形态，可广泛应用于政务、金融、运营商、制造、零售等多个行业场景。而在算力支持方面，模力方舟目前已支持包括天数智芯、沐曦、升腾、燧原、壁仞等国产算力，加速推进国产 AI 应用的百花齐放。&lt;/p&gt; 
&lt;p&gt;开源中国下一步将以模力方舟为核心，打造全方位的 AI 业务布局，助力 AI 应用创新、科技人才培养和新质生产力提升。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/363117</link>
      <guid isPermaLink="false">https://www.oschina.net/news/363117</guid>
      <pubDate>Thu, 17 Jul 2025 03:35:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>前谷歌 CEO：开源已成为 AI 发展中的重要特点</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;7 月 26 日，在世界人工智能大会（WAIC）上，前 Google CEO 埃里克·施密特（Eric Schmidt）围绕「人工智能全球合作展望」的主题，与港科技大学校董会主席沈向洋展开了一场「炉边对话」。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-8c2365f2f0b1260c4f8f9b0577b7c3ed1e1.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;施密特强调，过去两年，中国的 AI 技术，特别是 DeepSeek 和 Mini Max、Kimi 等大模型，已经取得了举世瞩目的成就。施密特指出，「在中国，这些领先的 AI 模型并非所有主要模型都像美国那样采取封闭策略。」而这也成为了当前 AI 发展中的一个重要特点。&lt;/p&gt; 
&lt;p&gt;对话中，施密特坦言自己更倾向于支持开源。其表示，开源技术虽然有一些潜在风险，但人类可以通过设定一些限制措施来管理这些风险，并根据需要对其进行调整。而对于这种情况，施密特则认为：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;问题的关键在于技术的「编辑者」是谁，以及这些技术在哪些地方得以应用，在哪些地方去设置「防护栏」。理想的场景是，我们能根据人类的价值观来训练和对齐这些模型。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;另外，施密特认为，未来超级智能之间的协作最终是不可避免的。因为随着技术的发展，人类将会拥有一个超级智能系统，未来这些系统会有能力去相互协作和协调。而对于这种情况，施密特则认为「需要让中国和西方的研究人员能够互相交流，合作探讨，达成在价值观方面一致性」。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/363114</link>
      <guid isPermaLink="false">https://www.oschina.net/news/363114</guid>
      <pubDate>Thu, 17 Jul 2025 03:28:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Java volatile 关键字到底是什么</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;span id="OSC_h2_1"&gt;&lt;/span&gt; 
&lt;h2&gt;一,前言&lt;/h2&gt; 
&lt;p&gt;volatile 作为 Java 的基础关键字，一直是个熟悉又神秘的存在。我们在日常做并发编程的过程中经常用到，我们知道在什么场景下需要用到，但却始终不清楚底层究竟做了什么。互联网上搜出来的大多数博客都在解释 volatile 关键字是为了解决指令重排序、内存可见性问题，或是什么内存屏障、缓存一致性协议一类「形而上的词汇」。而究竟什么是指令重排序，为什么要重新排序，什么是可见性问题，底层原理是什么，volatile 又是如何解决的却鲜有提及。引得 Java 开发者们如雾里看花，线上线下充满了疑惑的空气。&lt;/p&gt; 
&lt;p&gt;本文将浅浅探究一下这一切的底层原理，一起来学习「没有用」的知识，各位看官看懂了可以出去和面试官对线。&lt;/p&gt; 
&lt;span id="OSC_h2_2"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;二,指令重排序&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;在了解指令重排序问题之前，我们先来看一个由指令重排序造成并发问题的例子：&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;static int x = 0, y = 0;


&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;happens-before 八条原则&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;程序次序规则：在一个线程内，按照控制流顺序，书写在前面的操作先行发生于书写在后面的操作。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;管程锁定规则：一个 unlock 操作先行发生于后面对同一个锁的 lock 操作。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;volatile 变量规则：对一个 volatile 变量的写操作先行发生于后面对这个变量的读操作。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;线程启动规则：Thread 对象 start() 方法先行发生于此线程的每一个动作。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;线程终止规则：线程 A 等待线程 B 完成，在线程 A 中调用线程 B 的 join() 方法实现），当线程 B 完成后（线程 A 调用线程 B 的 join() 方法返回），则线程 A 能够访问到线程 B 对共享变量的操作。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;线程中断规则：对线程 interrupt() 方法的调用先行，发生于被中断线程的代码，检测到中断事件的发生，可以通过 Thread.interrupted() 方法检测到是否有中断发生。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;对象终结规则：一个对象的初始化完成（构造函数结束）先行发生于它的 finalize() 方法的开始。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;传递性：如果操作 A 先行发生于操作 B，操作 B 先行发生于操作 C，那就可以得出操作 A 先行发生于操作 C 的结论。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;hanpens-beofre 是 JVM 对开发者的保证，即不管 JVM 如何优化（JIT 编译），都会保证上述原则一定成立。而对于开发者来说，只要了解上述原则，无需硬件交互的复杂性，也能够写出可预测的代码，从而保证线程安全。&lt;/p&gt; 
&lt;p&gt;从 hanpens-beofre 中&amp;nbsp;程序次序规则&amp;nbsp;和&amp;nbsp;线程终止规则&amp;nbsp;可得，上述代码最终运行结果的可能性会有以下几种：&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//3cdd1d958189182a00e4240e8478e7d2.webp" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;可以明显看出，理论上不会存在 x =0 &amp;amp;&amp;amp; y = 0 的运行结果，然而实际上程序在执行了一段时间后，最终的确产生了 x = 0 &amp;amp;&amp;amp; y = 0 的结果！&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//d299df0dffd0aed1af23392270f21099.webp" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;这就引出了 volatile 解决的第一个问题：避免指令重排序。指令重排序在&lt;strong&gt;编译器&lt;/strong&gt;和&amp;nbsp;&lt;strong&gt;CPU 层面&lt;/strong&gt;（乱序执行）都会发生。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;CPU 的乱序执行&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;我们知道，CPU 运算的本质就是不断获取下一条指令然后执行，编译器给它什么指令它就执行什么，何来的乱序执行呢？&lt;/p&gt; 
&lt;p&gt;这还要从计算机的诞生之初讲起。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;内存拖后腿&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;计算机诞生之初，CPU 和内存之间的速度差异并不明显，一切相安无事。随着科学的进步，CPU 的运算速度越来快，根据摩尔定律计算，相当于 CPU 的性能每年增长 60%，相比之下，内存性能的增长却相对缓慢，每年约为 7%。到今天，CPU 运算和内存访问的速度产生了巨大鸿沟，已经达到了 120 倍之多。这时如果 CPU 还以传统计算机架构，数据从内存中读取的话，将会&lt;strong&gt;严重拖慢 CPU 的运行速度&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//74f2bda3199bee06b728910d5812a79f.webp" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;※ 局部性原理&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在程序运行过程中，芯片工程师总结了两条存在局部性原理：&lt;strong&gt;时间局部性&lt;/strong&gt;、&lt;strong&gt;空间局部性&lt;/strong&gt;。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;时间局部性：&lt;/strong&gt; 由于在代码中循环操作的普遍存在，因此当某部分数据被访问时，不久后该数据很可能会再次被访问，基于此原理诞生了&amp;nbsp;CPU 的高速缓存。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;空间局部性：&lt;/strong&gt; 由于代码是顺序执行的，因此当某一份数据被访问时，后续的数据也将很快被访问，基于此原理诞生了缓存行。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;※ CPU 内的高速缓存&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;为了弥补 CPU 运行速度与内存访问速度之间的巨大差异，提升 CPU 执行效率，CPU 在内部封装了高速缓存。&lt;/p&gt; 
&lt;p&gt;高速缓存是一种静态随机访问存储器（SRAM），相对于使用电容存储的内存（DRAM）来说，速度快得多，访问速度在纳秒级别，终于能勉强不再拖 CPU 后腿了。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//df49b1c83f72ca41af1507ee63c095c0.webp" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;CPU 缓存共分为三级：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;按访问速度从大到小排序为：L1 &amp;gt; L2 &amp;gt; L3&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;按容量从大到小排序为：L3 &amp;gt; L2 &amp;gt; L1&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;其中 L3 缓存 CPU 共享，L1、L2 缓存为各 CPU 独占。CPU 在访问内存数据时，会优先从高速缓存中访问，访问顺序依次为 L1、L2、L3，若高速缓存中都不存在，则再访问内存。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;缓存的引入，降低了 CPU 直接访问内存的频率，大大提升了 CPU 的执行效率。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;※ 缓存行&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;根据空间局部性原理，当 CPU 访问了一块数据时，相邻的数据很可能也即将被访问。那么是否可以通过预加载相邻的数据到高速缓存中，提升高速缓存的命中率呢？&lt;/p&gt; 
&lt;p&gt;当然可以，我们把预加载的这部分内存数据叫做缓存行。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//95b79b1e2f5130409268c63e893e219d.webp" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;由图所示，内存被划分为若干缓存行的块，缓存行的大小是固定的，通常为 64 字节，高速缓存数据块最小粒度就是缓存行（换句话说，高速缓存内的数据就是由一个个缓存行构成的）。当 CPU 需要访问位于内存的数据 X 时，会将整个缓存行同时加载到高速缓存中，以提升程序后续执行时的缓存命中率。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;CPU 内的「分布式」问题&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;高速缓存是把双刃剑，在大幅提升 CPU 运行效率的同时，也引来了一个 「分布式」 问题。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//c15707c1d91622f6ffd07aa5081c4768.webp" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;记得我们前面说过，CPU 的 L1、L2 缓存是各核心独占的，在两个 CPU 的 L2 缓存同时加载了同一个缓存行的情况下，当 CPU 0 数据 X 做了写操作（X = 1），其他 CPU 对这一修改是不可见的，这时 CPU 1 如果依然访问自己高速缓存中的数据，势必会产生数据不一致。&lt;/p&gt; 
&lt;p&gt;为了解决这个问题，缓存一致性协议便诞生了。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;※ &amp;nbsp;MESI 协议&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;缓存一致性协议有多种，最出名的就是 MESI 协议。&lt;/p&gt; 
&lt;p&gt;MESI 是&amp;nbsp;Modified&amp;nbsp;&amp;nbsp;&amp;nbsp;Exclusive Shared&amp;nbsp;&amp;nbsp;&amp;nbsp;Invalid&amp;nbsp;四个单词的缩写，分别表示缓存行的四种状态：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Modified：表示缓存行中数据已经被 CPU 修改了。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Exclusive：缓存行处于独占但尚未修改的状态，该状态表示其他 CPU 不可以预读取这个缓存行到自己的高速缓存中。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Shared：表示缓存行数据已经被多个 CPU 预加载到缓存中，且各 CPU 均未对该缓存行做修改。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Invalid：表示有其他 CPU 修改了该缓存行，缓存行数据已经失效。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//50733d9a050bd27bace430e3aea6fb73.webp" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;CPU 0 需要修改缓存行中 X 的数据时，将当前缓存行标记为&amp;nbsp;Modified&amp;nbsp;，并向总线发送一条消息，表明缓存行 CPU 0 已经修改。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;CPU 1 接收到该缓存行已失效的消息后，会将本地缓存行标记为&amp;nbsp;Invalid&amp;nbsp;，并 ACK 给 CPU 0。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;CPU 0 收到其他 CPU 已经将本地缓存行标记失效消息后，修改 X 的值。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;此时 CPU 0 高速缓存中缓存了 X 的最新值，其他 CPU 如果需要访问 X ，将会通过总线从 CPU 0 中获取。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;MESI 协议非常复杂，比如各 CPU 之间是如何通信的、多个 CPU 同时发送失效事件怎么办等等。&lt;/p&gt; 
&lt;p&gt;篇幅所限仅做本文用的着的部分介绍。有兴趣了解具体实现可以点击&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fxie.infoq.cn%2Flink%3Ftarget%3Dhttps%253A%252F%252Fwww.scss.tcd.ie%252FJeremy.Jones%252FVivioJS%252Fcaches%252FMESI.htm" rel="nofollow" target="_blank"&gt;https://www.scss.tcd.ie/Jeremy.Jones/VivioJS/caches/MESI.htm&lt;/a&gt;&amp;nbsp;查看动画演示。&lt;/p&gt; 
&lt;p&gt;缓存一致性协议有效解决了各 CPU 间数据一致性问题。那么，代价是什么呢？&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;禁止 CPU 摸鱼&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;上图可以看出 CPU 0 在执行修改 X 的值之前，需要与其他 CPU 进行通讯，收到其他 CPU 将本地消息修改完成后，才可修改本地缓存行的数据。在这期间 CPU 0 一直无事可做。而不管是前面提到过的编译器指令重排序还是超线程、指令流水线等技术，目的都是在提升 CPU 的运行效率，减少 CPU 空跑时间。如果由于缓存一致性协议造成 CPU 空闲的话，这对于我们来说显然是不可接受的！&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//5f847754f3567b6847c05e20931cc4fc.webp" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;为了让 CPU 满负荷运转，芯片工程师在 CPU 与 L1 缓存之间又加了一层——store buffer。&lt;/p&gt; 
&lt;p&gt;引入了 store buffer 后，CPU 写缓存行不再需要等待其他 CPU 回复消息，而是直接读写 store buffer，等到特定时刻，再将 store buffer 中的数据 flush 到高速缓存中&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//0bfd788352134ddf050c04c07fd3ba19.webp" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;CPU 0 需要修改缓存行中 X 的数据时，将当前缓存行标记为&amp;nbsp;Modified&amp;nbsp;，并向总线发送一条消息。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;CPU 0 不再等待 CPU 1 回复，而是直接将修改后的数据写入 store buffer 中。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;CPU 0 收到 CPU 1 标记缓存已经失效的回复消息后，将 store buffer 中的值 flush 到高速缓存中。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;问题会这么完美的解决吗？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;1,乱序执行&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;现在我们将 store buffer 纳入考量，再来回头看本节开始的这段代码：&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//005e5ed86a1fa2d021244995173dd8e2.webp" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;最终由于 store buffer 中数据的 flush 时间晚于 CPU 直接写高速缓存中数据的时间，客观上产生了 CPU 执行的指令顺序与实际代码中不一致的现象（X = B 早于 A = 1 执行，Y = A 早于 B = 1 执行），即乱序执行。最终得到了（A = 1，B = 1，X = 0，Y = 0）的结果。&lt;/p&gt; 
&lt;p&gt;既然发现了问题，那么要如何解决呢？这就要提到另一项技术：内存屏障。&lt;/p&gt; 
&lt;p&gt;随着 store buffer 技术的引入引起的问题还有很多，于是牵扯出一系列其他技术，如 Store Forwarding、Invalidate Queues 等技术。由于与本文涉及到的内容无关，这里就不做赘述了，各位有兴趣可以自行了解相关内容。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2,内存屏障&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;内存屏障听起来比较高大上，实际总结起来非常简单，就一句话：&lt;/p&gt; 
&lt;p&gt;去告诉 CPU，我在此处定义了一个内存屏障，自这里开始，后续所有针对高速缓存的写入，都必须先把 store buffer 中的数据全部 flush 回高速缓存中！&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//e356e96b22c7fc2a5b93771d0f05f855.webp" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//645eb0c5306cea04465dd863a54df479.webp" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在 Java 中 volatile 关键字避免 CPU 乱序执行的原理其实就是在访问 volatile 变量时添加了内存屏障。限制后续数据的写入操作一定把当前 store buffer 中的数据 flush 到高速缓存中，再通过缓存一致性协议保证数据一致性。&lt;/p&gt; 
&lt;p&gt;回到本节一开始的代码，你一定想到了要如何让这段程序永远执行下去的办法了？&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;static int x = 0, y = 0;


&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;没错，我们只需要限制针对数据 X、Y 的写操作之前，位于 store buffer 中的数据 A、B 全部 flush 到高速缓存即可。所以最终的解决方案就是给变量 A、B 添加 volatile 关键字即可！&lt;/p&gt; 
&lt;p&gt;想想为什么在变量 X、Y 上加 volatile 不可以？说加 4 个 volatile 的那位同学，课后把内存屏障这一章节抄写 3 遍！&lt;/p&gt; 
&lt;p&gt;前面说过，指令重排序问题在 CPU 和编译器层面都存在，CPU 层面说完了，那编译器层面呢？&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;3,编译器重排序&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;由于 JIT 编译后的指令不好扒，我们以 C 语言为例，先来看看下面的 C 语言例子：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;int func(int a, int b, int c, int d) {


&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;如果编译器不做优化，如果完全顺从我们代码语义，以上函数生成的汇编伪代码如下：&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//bce796732b45c8f9715c0e642c9c0d85.webp" referrerpolicy="no-referrer"&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//d067e3dadbb45a3a86d13e436f95b568.webp" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;而现代 CPU 会有多个执行单元，例如读写单元、运算单元，这些执行单元之间可以独立工作。在执行上面的指令时，只能顺序执行，不能并行执行。要想发挥两个执行单元的效率，只需调整一下顺序即可：&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//1e11767b2686f416c6fc88589e7aa957.webp" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//95fc29473b1f7b9a5427d6c06d51045e.webp" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;可以看出，虽然指令的数量一样，但在对指令做简单的重新排序后，优化一下指令的提交顺序，就可以更快的完成任务。&lt;/p&gt; 
&lt;p&gt;在 JVM 中，JIT 编译同样也会遵循这一原则，在不改变源码语义的情况下，改变 CPU 指令的执行顺序，就可以更快的完成运算任务，提升执行效率。这在单线程情况下运行良好，但多线程运行时，就可能会存在一些意料之外的问题。&lt;/p&gt; 
&lt;span id="OSC_h2_3"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;三,可见性问题&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;再来看另一个示例：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;private static boolean running = true;


&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;上面的程序，并不会按照我们预期的那样正常输出程序结束后退出，而是会永远的执行下去（不要尝试用前文中的 store buffer 来强行解释这个问题，store buffer 本质上也是个 buffer，在某一时刻数据依然会 flush 到高速缓存中，从而让其他线程感知到最新的值）。&lt;/p&gt; 
&lt;p&gt;这就引出 volatile 关键字解决的另一个问题：内存可见性问题。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;分层编译&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;我们知道，Java 是跨平台的。一个 Java 源码文件的执行需要两个过程：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;AOT 编译：源代码文件编译为 class 文件。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;JIT 编译：JVM 加载 class 文件，将 class 文件中字节码转换为计算器可执行的机器指令。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;字节码的执行也有两种方式：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;解释执行：优点是启动速度快，缺点是需要逐条将字节码解释为机器指令，开销大，性能低。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;编译执行（JIT 编译）：优点是执行效率高，与本地编译性能基本没差别，缺点是编译本身需要消耗 CPU 资源，以及编译后的指令数据需要存储，需要消耗内存空间。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;而 JIT 编译器又分为两种，Client Compiler、Server Compiler。之所以叫 client，server 是因为在一开始设计这俩编译器的时候，前者是设计给客户端程序用的，就比如像 idea 这种运行在个人电脑上的 Java 程序，不会长时间使用，反而更注重应用的启动速度以及快速达到相对较优性能。而后者则是设计给服务端程序用的。&lt;/p&gt; 
&lt;p&gt;HotSpot 虚拟机带有一个 Client Compiler —— C1 编译器。这种编译器启动速度快，但是性能相对 Server Compiler 来说会差一些。Server Compiler 则更为激进，优化后的性能要比 Client Compiler 高 30% 以上。HotSpot 虚拟机则带有两个 Server Compiler，默认的 C2 以及 Graal。&lt;/p&gt; 
&lt;p&gt;在 Java 7 之前，需要开发者根据服务的性质手动选择编译器。自 Java 7 开始，则引入了分层编译（Tiered Compilation）。&lt;/p&gt; 
&lt;p&gt;0：由解释器解释执行&lt;/p&gt; 
&lt;p&gt;1：C1 NO profiling：执行不带 profiling 的 C1 代码。&lt;/p&gt; 
&lt;p&gt;2：C1 LIMITED profiling：执行仅带方法调用次数以及循环回边执行次数 profiling 的 C1 代码。&lt;/p&gt; 
&lt;p&gt;3：C1 FULL profiling：执行带所有 profiling 的 C1 代码。&lt;/p&gt; 
&lt;p&gt;4：C2：执行 C2 代码。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;谁动了我的代码&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;运行时编译有那么多层级，到底是哪层影响了代码的？&lt;/p&gt; 
&lt;p&gt;想要探究个问题很简单，只需要在 JVM 启动参数里增加 -XX:TieredStopAtLevel=XX 参数即可。TieredStopAtLevel 是控制 JVM 最大分层编译级别的参数，当我们配置的值 &amp;lt; 4 时，前文的代码均可以正常终止，那么结论很明显了：C2 编译器全责！&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;C2 你在干什么？！&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;为了探究 C2 编译对我们代码做了什么，我们决定使用一款工具：JITWatch。JITWatch 专门用来探究 JIT 编译后的代码对应汇编指令。&lt;/p&gt; 
&lt;p&gt;关于 JITWatch 使用的流程这里就不做赘述了，网上有大量说明。本节的案例也很好复现，大家可以动手试一试。&lt;/p&gt; 
&lt;p&gt;我们将前文的源码文件编译后，提交以下命令执行：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# Ubuntu 22.04 下


&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;接着启动 JITWatch，加载 jit.log 文件：&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//1acdea4138e33b21ea4057fa78e698c9.webp" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;双击右侧 main 方法，查看 C2 编译后的结果：&lt;/p&gt; 
&lt;p&gt;记得选 OSR（栈上替换） 那个 C2，因为代码属于死循环，代码块不会退出，无法完整替换 C2 编译后的机器指令，只能通过栈上替换技术来进行。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//d8140526e1396da45c08bbda37be496c.webp" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;绿色框中为我们的核心代码，我在此处将它放大并增加了注释，如下：&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//4a58338befde76e1885fc0305a185d48.webp" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;什么？无条件跳转？！&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;可以看出，由于 C2 编译器的激进优化，编译后的机器码不再判断 running 变量，从而产生了内存可见性问题。而即使 C2 编译后的机器指令依然会执行安全点检查。想想是不是可以利用安全点检查机制，用一些操作来让进程停止？比如提交执行一次 GC、打个断点之类的。&lt;/p&gt; 
&lt;span id="OSC_h2_4"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;四,总结&lt;/strong&gt;&lt;/h2&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;说了那么多，能不能说点有用的？&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;有的有的，我们在多线程开发中，只要变量被多个线程共享，且是可变的，加上 volatile 准没错：）&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:center"&gt;&lt;strong&gt;&lt;strong&gt;往期回顾&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;1.&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fxie.infoq.cn%2Flink%3Ftarget%3Dhttps%253A%252F%252Fmp.weixin.qq.com%252Fs%253F__biz%253DMzkxNTE3ODU0NA%253D%253D%2526mid%253D2247540596%2526idx%253D1%2526sn%253D25f44a0aba699c43c90c55065a6d5ec4%2526scene%253D21%2523wechat_redirect" rel="nofollow" target="_blank"&gt;社区搜索离线回溯系统设计：架构、挑战与性能优化｜得物技术&lt;/a&gt;&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;2.&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fxie.infoq.cn%2Flink%3Ftarget%3Dhttps%253A%252F%252Fmp.weixin.qq.com%252Fs%253F__biz%253DMzkxNTE3ODU0NA%253D%253D%2526mid%253D2247540484%2526idx%253D1%2526sn%253D6b9e3947c74051d6778e9bf5436e9d87%2526scene%253D21%2523wechat_redirect" rel="nofollow" target="_blank"&gt;从 Rust 模块化探索到 DLB 2.0 实践｜得物技术&lt;/a&gt;&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;3.&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fxie.infoq.cn%2Flink%3Ftarget%3Dhttps%253A%252F%252Fmp.weixin.qq.com%252Fs%253F__biz%253DMzkxNTE3ODU0NA%253D%253D%2526mid%253D2247540454%2526idx%253D1%2526sn%253Df279b38d1e8d5e0b77dc96719066ddea%2526scene%253D21%2523wechat_redirect" rel="nofollow" target="_blank"&gt;eBPF 助力 NAS 分钟级别 Pod 实例溯源｜得物技术&lt;/a&gt;&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;4.&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fxie.infoq.cn%2Flink%3Ftarget%3Dhttps%253A%252F%252Fmp.weixin.qq.com%252Fs%253F__biz%253DMzkxNTE3ODU0NA%253D%253D%2526mid%253D2247540409%2526idx%253D1%2526sn%253Df3ae16d2ea439828c4452d92a5e46d53%2526scene%253D21%2523wechat_redirect" rel="nofollow" target="_blank"&gt;正品库拍照 PWA 应用的实现与性能优化｜得物技术&lt;/a&gt;&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;5.&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fxie.infoq.cn%2Flink%3Ftarget%3Dhttps%253A%252F%252Fmp.weixin.qq.com%252Fs%253F__biz%253DMzkxNTE3ODU0NA%253D%253D%2526mid%253D2247540215%2526idx%253D1%2526sn%253D72a0573520a8032d33b622f25bdd0671%2526scene%253D21%2523wechat_redirect" rel="nofollow" target="_blank"&gt;得物社区活动：组件化的演进与实践&lt;/a&gt;&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="text-align:center"&gt;文 / 空载&lt;/p&gt; 
&lt;p style="text-align:center"&gt;关注得物技术，每周二、四更新技术干货&lt;/p&gt; 
&lt;p style="text-align:center"&gt;要是觉得文章对你有帮助的话，欢迎评论转发点赞～&lt;/p&gt; 
&lt;p style="text-align:center"&gt;未经得物技术许可严禁转载，否则依法追究法律责任。&lt;/p&gt; 
&lt;p style="text-align:center"&gt;如有任何疑问，或想要了解更多技术资讯，请添加小助手微信&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/5783135/blog/18686270</link>
      <guid isPermaLink="false">https://my.oschina.net/u/5783135/blog/18686270</guid>
      <pubDate>Thu, 17 Jul 2025 03:22:00 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>苹果又一位 AI 研究员将跳槽 Meta，核心模型团队动荡加剧</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;苹果公司一个月内失去第四位人工智能（AI）研究员，转投竞争对手 Meta。知情人士称，苹果公司在多模态 AI 领域的关键研究员 Bowen Zhang 已于周五离职，将加入 Meta 新近成立的超级智能团队。Zhang 曾为苹果基础模型团队（AFM）成员，该团队构建苹果 AI 平台背后的核心技术。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;此前报道，Meta 此前以超过 2 亿美元的一揽子薪酬挖走该团队的负责人 Ruoming Pang。该团队另外两位研究员 Tom Gunter 和 Mark Lee 近期也加入了 Meta。AFM 团队由位于加州库比蒂诺与纽约的数十名工程师和研究人员组成。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;因这些变动为非公开信息而不愿具名的知情人士表示，为应对来自 Meta 等公司的挖角，苹果一直在对 AFM 团队成员小幅加薪，无论其是否表达过离职意向。尽管如此，相比竞争对手，苹果的薪资水平仍然相形见绌。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;苹果和 Meta 的发言人均拒绝发表评论。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在纽约，苹果股价一度下跌 1.5%，至 210.82 美元，创下盘中新低。截至周一收盘，该股今年以来已累计下跌 15%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;这些离职让苹果的模型团队陷入动荡。Pang 在制定该部门的发展路线图和研究方向方面发挥了核心作用，AFM 内部多位人士目前表示，该部门的未来并不明朗。知情人士称，其他工程师正在积极面试其他职位。另一位团队成员 Floris Weers 最近几周离职，加入了一家初创公司。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="338" src="https://oscimg.oschina.net/oscnet/up-b10a450d3c7e2ff7f765952beb662f23c6c.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;em&gt;苹果高管 John Giannandrea（左）和 Craig Federighi&lt;/em&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;AFM 团队对苹果整体 AI 战略至关重要。该团队的工作为去年推出的 Apple Intelligence 平台奠定了基础。但现在，该公司正在考虑转向使用更多第三方模型。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;知情人士称，一些苹果高管把其自主研发的模型视为追赶 AI 对手的绊脚石。而且围绕是否外包该技术的不确定性打击了公司士气，加剧了员工流失。AFM 团队目前由 Zhifeng Chen 领导，向苹果 AI 研究主管 Daphne Luong 汇报。她向 AI 高级副总 John Giannandrea 汇报工作。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;相关阅读：&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.oschina.net/news/360982" target="news"&gt;&lt;span style="color:#2980b9"&gt;Meta 在挖走苹果 AI 部门主管后，再次挖走两名核心专家&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/363101</link>
      <guid isPermaLink="false">https://www.oschina.net/news/363101</guid>
      <pubDate>Thu, 17 Jul 2025 02:52:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>ChatGPT 上线全新「学习模式」</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;ChatGPT 正式上线了&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopenai.com%2Findex%2Fchatgpt-study-mode%2F"&gt;「学习模式」&lt;/a&gt;，通过交互式提问而非直接给答案的方式，引导用户深入学习和解决问题。&lt;/p&gt; 
&lt;p&gt;据介绍，学习模式（study mode）是一种新的学习体验，旨在通过分步指导帮助用户解决问题，而不是直接提供答案。它最大的亮点在于引入了苏格拉底式提问，通过一连串循序渐进的问题，引导你沿着逻辑脉络一点点构建知识体系。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-0734bb018ad7b0af7ef233b004905294ef5.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;OpenAI 表示这项功能主要面向大学生群体，尤其适用于作业辅导、考试准备和学习新知识。使用方式非常简单，在 ChatGPT 的工具中选择「研究与学习」，然后直接输入想问的问题即可。&lt;/p&gt; 
&lt;p&gt;目前，学习模式由系统提示词驱动，并未使用专门训练的 AI 模型，这种机制的优势在于迭代快、调整灵活。OpenAI 表示，未来将逐步把这一交互模式融合进核心模型中，让教学逻辑成为底层能力的一部分。&lt;/p&gt; 
&lt;p&gt;体验方面，免费版、Plus、Pro、Team 版用户均可使用这项功能，Edu 用户将在接下来的几周内上线。&lt;/p&gt; 
&lt;p&gt;OpenAI 表示，这是改善 ChatGPT 学习体验的第一步，未来计划将这种行为直接训练到核心模型中，并探索更清晰的可视化、目标设定和更深度的个性化等功能。同时，OpenAI 正通过 NextGenAI 计划及与斯坦福大学 SCALE Initiative 的合作，进一步研究 AI 在教育中的应用。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/363093/chatgpt-study-mode</link>
      <guid isPermaLink="false">https://www.oschina.net/news/363093/chatgpt-study-mode</guid>
      <pubDate>Thu, 17 Jul 2025 02:30:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>深圳先进院提出新型图像复原大模型 HYPIR</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;中国科学院深圳先进技术研究院数字所董超研究员团队&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F1cqDxurR1QxB4HXNoKmLkg" target="_blank"&gt;发布&lt;/a&gt;了一项名为 HYPIR 的图像复原大模型，不仅比现有的图像复原技术快数十倍，更在高清分辨率、文字保真、理解能力、用户控制灵活性等方面展现出了优异性能。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;董超团队曾于去年提出了智能画质增强大模型 SUPIR，将低质量的图像恢复到接近原始状态的高清图像，有效修复多种退化类型的图像。而此次图像大模型 HYPIR 作为升级版，舍弃了迭代式的扩散模型训练，改用单步的对抗生成模型训练方式，将原有的算法速度提升了数倍，同时采用更新的文生图基模型进一步提升算法效果，实现了 8K 级别的细节生成，在生成图像的稳定性和可控性方面远超 SUPIR 大模型。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="353" src="https://oscimg.oschina.net/oscnet/up-f2529fde69524742505eeda933f93526e22.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;「以往图像复原方法中往往包括扩散模型蒸馏、ControlNet 适配器或者多步推理过程。而 HYPIR 则不需要依赖这些步骤，使用方法更加简单。在训练和推理速度上较传统方法提升了一个数量级以上，且性能更优。」董超介绍，HYPIR 主要有两个创新点，一是使用预训练扩散模型初始化复原网络；二是从理论角度出发解释这一简单方法背后蕴含的深刻原理。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;实验数据显示，在单张显卡（图像处理器）上，HYPIR 仅需 1.7 秒即可完成一张 1024x1024 分辨率图像的复原。相比现有的图像复原方法，研究人员提出的 HYPIR 在复原图像的质量上性能更优，且能够适用于各种尺寸的预训练扩散模型，为不同应用场景提供了灵活性。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="254" src="https://oscimg.oschina.net/oscnet/up-5ce496faabfb63ab3ab3085ef4ffef77130.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在应用层面，研究人员介绍，HYPIR 在图像高清分辨率、文字保真、理解能力、用户控制灵活性等方面均展现出了优异的性能。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;例如，在老照片修复方面，研究人员运用 HYPIR 修复了国内外经典电影、电视剧老照片，让模糊的影像重现清晰的细节，为文化记忆传承提供了技术支持。在高分辨率图像修复领域，HYPIR 同样表现出色，因其兼具速度与效果，HYPIR 成功攻克了传统方法在生成 8k 分辨率图像时往往面临速度慢或效果不佳的难题。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="379" src="https://oscimg.oschina.net/oscnet/up-67b91f31bbedb73b1544419c5b2214c6180.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在文字保真方面，传统基于扩散模型的方法常导致复原出的文字模糊或扭曲，缺乏精确性，而 HYPIR 则能够使复原出的文字保持高保真度和清晰度，无论是简单的标识还是复杂的文档，HYPIR 都能精准地还原其原始形态，使图像中的文字清晰可读。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;值得一提的是，HYPIR 还具备了突出的自然语言理解能力，能够精准捕捉和理解用户的输入指令，在图像复原过程中准确地反映用户的意图。此外，用户可以根据需求灵活调节生成与复原的平衡，或精细控制图像细节程度，从而获得符合自身偏好的结果。这种用户友好的设计使得 HYPIR 不仅适用于专业领域，也能满足普通用户的需求。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="315" src="https://oscimg.oschina.net/oscnet/up-8a5dec52e332077222c82eb21fe0166e4c8.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/363092</link>
      <guid isPermaLink="false">https://www.oschina.net/news/363092</guid>
      <pubDate>Thu, 17 Jul 2025 02:25:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>昆仑万维开源多模态统一预训练模型 Skywork UniPic</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;昆仑万维宣布正式推出并开源采用自回归路线的「多模态统一预训练模型&amp;nbsp;&lt;strong&gt;Skywork UniPic&lt;/strong&gt;」，在单一模型中深度融合图像理解、文本到图像生成、图像编辑三大核心能力。该模型基于大规模高质量数据进行端到端预训练，具备良好的通用性与可迁移性。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="239" src="https://oscimg.oschina.net/oscnet/up-9bb339f656b62f41563018515770ad49264.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;Skywork-UniPic 模型核心能力包含：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;图文理解：&lt;/strong&gt;基于 token 预测完成文本的自回归建模&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;图像生成&lt;/strong&gt;：采用掩码自回归方式，逐步生成图像 patch&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;图像编辑：&lt;/strong&gt;引入参考图与编辑指令作为条件，生成编辑后的图像&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;此外，Skywork-UniPic 完成端到端优化流程，能够实现生成、理解、编辑三大能力的协同训练和相互促进，突破传统方法中能力权衡的技术瓶颈。这一架构设计不仅保持了自回归模型的简洁高效，更通过共享编码器实现了跨任务的深度协同，为多模态统一模型的实用化部署奠定了坚实基础。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;用户只需要输入提示词，Skywork-UniPic 既可以像 VLM 一样理解图像、像 T2I 模型一样生成图片，还可以像美图工具一样，一键实现风格转绘/吉卜力化的编辑功能。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;&lt;img height="299" src="https://oscimg.oschina.net/oscnet/up-12b968fc4978131cb565b8847b729558f47.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;Skywork UniPic 技术亮点：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;&lt;strong&gt;指令遵循能力媲美大型模型：&lt;/strong&gt;&lt;/strong&gt;在 GenEval 指令遵循评估中取得 0.86 的优异成绩，超越了绝大多数同类统一模型，在无 CoT 的情况下取得了 SOTA 分数，逼近较大模型 BAGEL（7B+7B*）带 CoT 的 0.88 分；&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;复杂指令生图能力领先：&lt;/strong&gt;在 DPG-Bench 复杂指令生图基准上达到 85.5 分的行业 SOTA 水平；&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;图像编辑能力统一模型第一梯队：&lt;/strong&gt;GEditBench-EN 获得 5.83 分，ImgEdit-Bench 达到 3.49 分，展现出精准的编辑执行能力；&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;参数效率优势显著：&lt;/strong&gt;相比同类大参数统一模型（如 BAGEL 的 14B 总参数、UniWorld-V1 的 19B 总参数），Skywork UniPic 以 1.5B 的轻量级规模实现了接近甚至超越大参数模型的性能表现；&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;部署友好，真正可落地：&lt;/strong&gt;模型在 RTX 4090 消费级显卡上均可流畅运行，为广大开发者和研究者提供了真正可落地的统一模型解决方案，大幅降低了技术应用门槛。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="220" src="https://oscimg.oschina.net/oscnet/up-200975b864b698a0f7821912288be54d815.png" width="500" referrerpolicy="no-referrer"&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;更多详情可&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FPX-pKw0N341590wm7GpdYw" target="_blank"&gt;查看官方公告&lt;/a&gt;。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/363085</link>
      <guid isPermaLink="false">https://www.oschina.net/news/363085</guid>
      <pubDate>Thu, 17 Jul 2025 01:58:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>🔥造物分享：像 VisionPro 那样酷，全球首个远距离动态手势交互技术！</title>
      <description/>
      <link>https://www.oschina.net/ai-creation/details/2117</link>
      <guid isPermaLink="false">https://www.oschina.net/ai-creation/details/2117</guid>
      <pubDate>Thu, 17 Jul 2025 01:55:00 GMT</pubDate>
    </item>
  </channel>
</rss>
