<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - industry - 简体中文</title>
    <link>https://www.oschina.net/news/industry</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news/industry" rel="self" type="application/rss+xml"/>
    <description>已对该 RSS 进行格式化操作：中英字符之间插入空格、使用直角引号、标点符号修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-cn</language>
    <lastBuildDate>Fri, 27 Jun 2025 07:44:29 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>开源 | MeiGen-MultiTalk：基於单张照片实现多人互动演绎</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;blockquote&gt; 
 &lt;p&gt;近日，美团推出了音频驱动的多人对话视频生成框架 MultiTalk，并在 GitHub 上开源，首创 L-RoPE 绑定技术，通过标签旋转位置编码精准解决多音频流与人物错位难题。该框架创新性地采用局部参数训练+多任务学习策略，在保留复杂动作指令跟随能力的同时，实现自适应动态人物定位。只需输入多人音频流、参考图像和文本提示，即可生成口型精准同步、肢体自然的交互视频，可支持影视制作、直播电商等场景的工具升级。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-1f49ae4f4fb275deb2b5f21c7d98b147cc3.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;如果给你一张图片，再给你一段语音，怎么能让它们完美融合在一起，让图片中人物自然说话和做动作，甚至多人之间还能互动起来呢？近日，美团视觉智能团队在 GitHub 上开源了一款产品&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmeigen-ai.github.io%2Fmulti-talk%2F" target="_blank"&gt;MeiGen-MultiTalk&lt;/a&gt;，它就非常巧妙地解决了这个问题。先上视频，看一下它实力如何：&lt;/p&gt; 
&lt;p&gt;1.输入图像+对话语音&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//7ad2d3b67c25b4c879c9baeab11830b8.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;2.使用 MultiTalk 生成视频&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fuser-attachments%2Fassets%2F94ce6060-4591-4179-833b-b0e7da3bd31c" target="_blank"&gt;点击查看视频&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;注：图像和音频均由 AI 生成。&lt;/p&gt; 
&lt;p&gt;还有下面这部《Smile》短片中所有镜头，也都是由 MeiGen-MultiTalk 合成的，是不是很惊艳？&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fuser-attachments%2Fassets%2Fb9a7ef25-6068-4296-ab41-1aa94e9b9545" target="_blank"&gt;点击查看视频&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;注：每个镜头首帧图像和音频来源《Smile》- Morgan Wallen&lt;/p&gt; 
&lt;p&gt;不仅仅是这种风格，还有很多其他很多类型的融合，让小猫说话，给动画片配音，甚至还让双人对唱飚高音，它也表现的相当不错。感兴趣的同学，可移步到&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmeigen-ai.github.io%2Fmulti-talk%2F" target="_blank"&gt;项目主页&lt;/a&gt;进行查看。或者查看美团技术团队微信公众号的推文：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FEJHbR0ShT53BhMTUceY9pg" target="_blank"&gt;开源 | MeiGen-MultiTalk：基於单张照片实现多人互动演绎&lt;/a&gt;。展示完毕，接下来就是最重要的部分，上链接！&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;项目主页&lt;/strong&gt; ：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmeigen-ai.github.io%2Fmulti-talk%2F" target="_blank"&gt;https://meigen-ai.github.io/multi-talk/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;开源代码&lt;/strong&gt; ：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FMeiGen-AI%2FMultiTalk" target="_blank"&gt;https://github.com/MeiGen-AI/MultiTalk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;技术报告&lt;/strong&gt; ：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2505.22647" target="_blank"&gt;https://arxiv.org/abs/2505.22647&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;一、引言：超越"会说话的头"------AI 人像视频的下一个前沿&lt;/h2&gt; 
&lt;p&gt;当前，人工智能在视觉内容生成领域取得了令人瞩目的进展，尤其是在音频驱动的人像视频方面。无论是"会说话的头"还是"会说话的身体"技术，都已能够从音频信号生成与面部动作高度同步、视觉质量令人满意的视频。这些技术在模拟单人讲话方面表现出色，例如在虚拟主播或数字替身等应用中展现出逼真的效果。&lt;/p&gt; 
&lt;p&gt;然而，现有方法在处理更复杂的场景时，其局限性也日益凸显，面对多人对话视频生成时面临三大挑战：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;多音频流输入适配&lt;/strong&gt;：如何区分并绑定不同人物的音频信号？&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;动态人物定位&lt;/strong&gt;：当人物在画面中移动时，如何精准定位其运动区域？&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;指令遵循能力&lt;/strong&gt;：如何让生成的视频严格遵循文本描述的复杂动作（如大幅肢体动作）？&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;这些挑战促使研究人员思考，AI 人像视频的下一个前沿究竟在哪里。从最初仅关注面部表情的"会说话的头"，到能够模拟全身动作的"会说话的身体"，再到如今 MultiTalk 所提出的"多人物对话视频生成"，这清晰地揭示了 AI 人像视频领域从关注局部细节到全身动作，再到模拟复杂社会互动的演进趋势。这种演进不仅仅是技术能力的简单提升，更体现了对真实世界复杂性模拟需求的增长，以及 AI 在内容创作中扮演更高级角色的潜力。用户对 AI 生成内容的"真实感"和"复杂性"要求越来越高，简单的"动起来"已不足够，现在需要 AI 能够"自然地互动"并"理解和执行复杂指令"。&lt;/p&gt; 
&lt;h2&gt;二、MultiTalk 的框架图：如何实现 AI 对话视频生成&lt;/h2&gt; 
&lt;p&gt;MultiTalk 实现音频驱动的多人物对话视频生成的技术框架，如下图 2 所示：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//1692b1d06eb9109b7420450994f1fcf1.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;2.1 基础模型结构：DiT 与 3D VAE&lt;/h3&gt; 
&lt;p&gt;MultiTalk 以 DiT（Diffusion-in-Transformer）为基础的视频扩散模型作为其核心骨架。DiT 模型因其在图像和视频生成方面的卓越性能而备受关注，它用 Transformer 结构替代了传统的 U-Net，能够更好地捕捉长距离依赖关系。&lt;/p&gt; 
&lt;p&gt;为了高效处理视频数据，MultiTalk 集成了 3D 变分自编码器（VAE）。3D VAE 能够对视频数据在空间和时间维度上进行压缩，将高维原始视频数据编码成更紧凑的潜在表示。这种压缩大大降低了后续扩散模型的计算负担，同时保留了关键的视觉信息。&lt;/p&gt; 
&lt;p&gt;首先，使用文本编码器，将用户输入的文本提示（例如"一个男人和女人正在舞台上唱歌"）转化为文本条件嵌入，指导视频内容的生成。其次，通过 CLIP 图像编码器提取的全局上下文信息也被注入到 DiT 模型中。这些图像上下文与文本条件通过解耦的交叉注意力机制协同作用，为生成视频提供视觉和语义指导，确保生成内容与参考图像和文本提示保持一致。&lt;/p&gt; 
&lt;h3&gt;2.2 让 AI"说话"：单人音频集成&lt;/h3&gt; 
&lt;p&gt;基础的图像到视频（I2V）扩散模型通常不原生支持音频输入。为了让模型能够"说话"，MultiTalk 在每个 DiT 块的文本交叉注意力层之后，添加了新的层，这些层包含层归一化和音频交叉注意力机制，专门用于处理和整合音频条件。&lt;/p&gt; 
&lt;p&gt;在音频嵌入的提取与上下文整合方面，MultiTalk 采用了 Wav2Vec，这是一种广泛使用的音频特征提取器，能够将音频波形转换为高维的音频嵌入。在音频驱动的人体视频中，当前时刻的动作不仅受当前音频帧影响，也受前后音频帧的影响。因此，MultiTalk 遵循现有方法，将与当前帧相邻的音频嵌入进行拼接（通过上下文长度 k 参数控制），形成更具时间上下文信息的音频嵌入，以更好地捕捉语音的动态变化。&lt;/p&gt; 
&lt;p&gt;一个重要的挑战是，由于 3D VAE 对视频数据进行了时间压缩，视频潜在空间的帧长度通常比原始音频嵌入的帧长度短，这使得两者之间无法直接进行帧对帧的交叉注意力计算。为了解决这种时序长度不匹配的问题，MultiTalk 使用了一个音频适配器。该适配器通过一系列操作对音频嵌入进行压缩和对齐：首先将输入音频嵌入分割为初始帧和后续帧；然后对后续帧进行下采样；接着分别通过多个 MLP 层编码初始帧和下采样后的后续帧；将编码后的特征拼接起来；最后，再次通过 MLP 层对拼接后的特征进行编码，从而获得与视频潜在空间帧长度匹配的压缩音频条件。音频适配器解决了视频和音频数据固有的时间粒度不匹配问题，确保了信息流的顺畅，使得不同模态的数据能够高效地在同一框架内进行交互。&lt;/p&gt; 
&lt;h3&gt;2.3 核心挑战：当多重声音让 AI"困惑"&lt;/h3&gt; 
&lt;p&gt;与单人视频相比，多人物对话视频生成带来了多重复杂性，这些是现有方法无法解决的。首先，对话场景中，音频信号来自多个人物，模型需要能够同时、独立地处理这些不同的音频流，这是"多流音频输入处理"的挑战。其次，也是最核心的挑战之一，是"音频与人物的精确绑定"。必须确保视频中的每个人物只由其对应的音频流驱动，以防止唇形同步错误地出现在所有人物身上，导致不自然的"齐声说话"现象，这在真实对话中是极不自然的。最后，生成视频中的人物是动态的，他们的位置和姿态会随着对话和动作而变化。这要求模型具备一种"自适应方法"，能够精确追踪每个人物在视频帧中的运动区域，以便将音频准确地映射到正确的视觉区域。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//8563dcaa19a0ce3dd0f9b554af583220.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在探索多流音频注入方案时，MultiTalk 尝试了多种直觉性的方法，如上图 3 所示。但多数都未能有效解决音频与人物的绑定问题，这凸显了问题本身的复杂性，并非简单的拼接或分割就能解决。最初的尝试包括：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;直接拼接多流音频嵌入&lt;/strong&gt;： 将多流音频的嵌入直接拼接起来，然后与视频潜在空间进行交叉注意力计算。然而，这种方法未能将拼接后的多流音频与视频中对应的特定人物区域绑定，导致混乱的同步。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;分别计算后相加&lt;/strong&gt;： 分别计算每个音频流与视频潜在空间的交叉注意力结果，然后将这些结果相加。然而，这种方法同样未能解决绑定问题，模型无法区分哪个音频应该驱动哪个人物。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;分割视频潜在空间（左右区域）&lt;/strong&gt;： 考虑到视频中人物通常位于左右两侧，MultiTalk 尝试将视频潜在空间简单地分割成左右两部分，并让每个部分与对应的音频流计算注意力。虽然这种方法在一定程度上成功绑定了多流音频到不同人物，但其泛化能力极其有限。它仅适用于人物动作范围很小的视频；一旦人物出现大范围移动或交叉，这种简单的空间分割就会导致音频绑定失败。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;这些传统方法失败的根本原因在于它们缺乏自适应的对动态主体进行定位的能力。直接拼接、简单相加或基于固定空间位置的分割，无法让模型理解哪个音频流应该对应视频中哪个动态变化的人物。缺乏这种深层的"人物感知"和"语义绑定"机制，导致了"错误绑定"------所有人都同步说话，这在对话场景中是极不自然的，严重影响了生成视频的真实感和可用性。&lt;/p&gt; 
&lt;h3&gt;2.4 让 AI"交谈"：L-ROPE 实现无缝多人物绑定&lt;/h3&gt; 
&lt;p&gt;为了解决这个问题，MultiTalk 提出了 L-ROPE。在应用 L-ROPE 进行音频绑定之前，MultiTalk 首先需要解决一个基础问题：如何在视频中动态地识别并追踪每个人物的位置。给定包含多个人物的参考图像，模型首先识别出每个人物的掩码区域以及背景掩码。在 DiT 模型中，视频的第一帧通常作为参考图像。MultiTalk 利用"参考图像到视频的自注意力图"。如图 4a），通过计算视频潜在空间中每个 Token 与参考图像中每个人物掩码的平均相似度，模型能够得到一个相似度矩阵。利用这个相似度矩阵，模型可以自适应地确定视频中每个潜在 Token 属于哪个人物或背景，从而实现了对每个人物的动态定位和追踪。&lt;/p&gt; 
&lt;p&gt;Label Rotary Position Embedding （L-ROPE）是 MultiTalk 的核心创新，它基于 ROPE（Rotary Position Embedding）的思想。ROPE 是一种在大型语言模型（LLMs）和视频扩散模型中广泛使用的相对位置编码技术，以其在捕捉 Token 间关系和处理时空信息方面的卓越能力而闻名。L-ROPE 的创新之处在于，它将"类别标签"融入到位置编码中，从而在 DiT 块的音频交叉注意力层中，实现了多流音频与多个人物的精准绑定。&lt;/p&gt; 
&lt;p&gt;在标签分配策略上，视频潜在空间包多个类别，比如多个人物和背景的区域。MultiTalk 为每个人物分配了一个特定的数值范围作为标签（例如，第一个人物的视觉标签范围是{0-4}，第二个人物是{20-24}）。视频潜在空间中每个 Token 的最终标签，是根据其与对应人物掩码的相似度，通过归一化函数在这个范围内计算得出的。背景区域则被赋予一个静态标签，以确保它不与任何音频流关联，避免背景元素被音频驱动。对于多流音频嵌入，MultiTalk 首先将它们拼接起来，然后为每个音频流分配一个静态的、唯一的标签。为了与视频中的人物绑定，这些音频标签被精心选择，与对应人物的视觉标签范围"接近"或"匹配"（例如，第一个音频流标签为 2，第二个音频流标签为 22）。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//368e33e0bdb9af15aa2b5d480b2b1de5.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;L-ROPE 的特点在于它将"类别信息"（哪个像素属于哪个人物类或背景类）巧妙地融入了"位置编码"中。传统的 ROPE 处理的是纯粹的时空位置信息，而 L-ROPE 则更进一步，将"类别"信息编码进去。它使得模型能够区分场景中的不同个体。在音频交叉注意力机制中，Q（来自视频潜在空间）和 K（来自多流音频嵌入）都经过 L-ROPE 处理。通过这种带有语义标签的旋转，当视频潜在空间中某个区域（例如，对应人物 1 的区域）的标签与音频 1 的标签"匹配"时，它们之间的注意力权重就会被有效激活，从而强制模型将音频 1 的驱动作用集中到人物 1 身上，解决了不正确的绑定问题，如图 4c)。这种策略能够有效激活音频交叉注意力图中的特定区域，从而确保音频与对应人物的唇形和动作精确同步。&lt;/p&gt; 
&lt;p&gt;为了验证 L-ROPE 的有效性，论文进行了一项消融研究，重点关注标签范围的选择。实验结果（如下表 3 所示）表明，即使为不同人物选择不同的标签范围，所产生的性能指标接近。这说明 L-ROPE 对具体的标签范围变化不敏感。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//3180443fcdf3b17a4c8b2edea7406776.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;2.5 训练策略&lt;/h3&gt; 
&lt;p&gt;MultiTalk 框架采用了多项训练策略，这些策略共同确保了模型在多人物场景下的高性能、精确的音频同步以及指令遵循能力。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;1. 两阶段训练：循序渐进的技能提升&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;MultiTalk 的训练过程被划分为两个阶段，旨在逐步增强模型的音频处理和唇形同步能力。第一阶段的主要目标是开发模型对单人视频的强大能力，此阶段模型使用单人说话视频数据集进行训练。在模型掌握了单人视频能力之后，进入第二阶段。第二阶段使用专门收集的包含双流音频的训练数据，以促进模型学习多人物视频和交互。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2. 部分参数训练：精准调优，避免退化&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;这是 MultiTalk 训练中的一个关键策略。在整个训练过程中，研究者仅更新音频交叉注意力层和音频适配器中的网络参数，而冻结了所有其他基础模型的网络参数。论文发现表明，在计算资源和数据量有限的情况下，如果进行全参数训练，会导致模型指令遵循能力的显著下降（特别是对于复杂的动作和人物交互），甚至可能引起生成视频中手部和物体变形等视觉伪影。相反，通过仅训练与音频输入直接相关的特定层，MultiTalk 能够很好地保留基础模型原有的强大指令遵循能力，并避免了上述视觉退化问题。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;3. 多任务训练：丰富场景理解，强化指令遵循&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;MultiTalk 采用了多任务混合训练范式，将模型训练任务分为音频+图像到视频（AI2V）训练和图像到视频（I2V）训练。尽管任务不同，但它们共享相同的网络参数。在 AI2V 任务中，模型同时使用参考图像和音频作为条件输入，专注于学习音频驱动的唇形同步和动作生成。在 I2V 任务中，音频条件被移除（通过将音频嵌入置零）。I2V 任务使用的训练数据是独特的，主要包含大量多事件视频。这些视频涵盖了人物、物体和场景之间复杂的交互，例如人物拿起杯子、与环境互动等。这种多事件数据集对于确保模型能够准确理解和执行文本提示中描述的复杂动作和交互至关重要。论文指出，如果仅使用说话的头和身体数据进行 AI2V 训练，网络的指令遵循能力会显著削弱。然而，通过将 I2V 训练纳入多任务范式，模型能够有效地保留其强大的指令遵循能力，从而生成更符合用户意图的视频，如下图 5 所示。这种策略体现了泛化与鲁棒性，即通过多任务训练，在保持特定任务能力的同时，增强模型的通用理解和指令遵循能力。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//77370c360f865826c0a5aeb1f35060db.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;2.6 长视频生成&lt;/h3&gt; 
&lt;p&gt;尽管 MultiTalk 模型能够生成高质量的短视频（例如 3-5 秒），但这对于实际应用场景（如制作电影片段、直播内容）来说远远不够，因为这些场景通常需要持续更长的视频。为了突破单次生成长度的限制，MultiTalk 引入了一种基于自回归（Autoregressive）的方法来生成长视频。将之前生成视频的末尾部分作为条件，来生成新的视频片段，从而实现时间上的连续性和扩展。&lt;/p&gt; 
&lt;p&gt;在具体的实现机制上，传统的图像到视频（I2V）模型通常只使用视频的第一帧作为生成后续帧的条件。MultiTalk 在此基础上进行了关键改进。在生成新的视频片段时，它不再仅仅依赖第一帧，而是将先前已生成视频的最后 5 帧作为额外的条件输入到当前的推理步骤中。这使得模型能够"记住"并延续之前的动作和场景状态。这些作为条件的 5 帧视频，首先会通过 3D VAE 进行压缩，将其转化为更紧凑的 2 帧潜在噪声表示。随后，为了匹配 DiT 模型的输入格式，新的视频帧（除了从历史信息得来的 2 帧潜在噪声）会用零填充。这些填充的帧、来自历史信息的潜在噪声以及一个视频掩码被拼接在一起，形成完整的输入。最终，这个包含历史上下文信息的输入被送入 DiT 模型进行推理，生成新的视频片段。下面视频展示了生成结果的流畅性。&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;输入图像+对话语音&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//d68a049c134922b777999b4d82e04f34.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;使用 MultiTalk 生成视频&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fuser-attachments%2Fassets%2Fec77e7e3-f03b-41ac-810b-49fd8d70968a" target="_blank"&gt;点击查看视频&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;注：图像和音频源于《破产姐妹》。&lt;/p&gt; 
&lt;h2&gt;三、MultiTalk 实战：性能表现&lt;/h2&gt; 
&lt;p&gt;MultiTalk 的性能通过广泛的实验进行了验证，包括与现有最先进方法的定量和定性比较，充分展示了其在多人物对话视频生成方面的能力。&lt;/p&gt; 
&lt;p&gt;在数据集与评估指标方面，MultiTalk 的训练数据集在第一阶段使用了约 2K 小时的单人说话视频，用于学习基础的音频驱动视频能力；第二阶段则使用了 100 小时的双人对话视频，用于专门训练多人物交互和绑定。MultiTalk 在三类不同的测试数据集上进行了评估：说话的头数据集（HDTF 和 CelebV-HQ ）、说话的身体数据集（EMTDT ）以及双人说话身体数据集（MTHM）。评估采用了行业内通用的多维度指标：FID (Frechet Inception Distance) 和 FVD (Fréchet Video Distance) 用于评估生成数据质量；E-FID (Expression-FID) 用于评估生成视频中面部表情的表现力；Sync-C 和 Sync-D 用于精确测量生成视频中唇部动作与音频的同步程度。&lt;/p&gt; 
&lt;p&gt;在定量评估中，MultiTalk 在说话的头和说话的身体生成任务上，与 AniPortrait、VExpress、EchoMimic、Hallo3、Sonic、Fantasy Talking 等多个最先进的方法进行了对比。结果显示，MultiTalk 在大多数指标上超越了这些方法，尤其在唇形同步（Sync-C, Sync-D）和视频质量（FID, FVD）方面表现出卓越性能。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//3e0ee05f747d8947c63d6637507dc3ee.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;此外，我们还专门探讨了多流音频训练是否会导致单人视频性能下降的问题（具体可以参考&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2505.22647" target="_blank"&gt;论文&lt;/a&gt;）。实验结果（表 1 和表 2 中"MultiTalk-single"与"MultiTalk-multiple"的对比）显示，MultiTalk 的多人视频模型在单人数据集上表现与单人视频模型相当。这表明，MultiTalk 在引入多人物处理能力时，并未牺牲原有的单人视频性能，实现了能力的无损叠加。&lt;/p&gt; 
&lt;p&gt;在定性评估中，MultiTalk 取得了不错的效果，如下图 6 所示。其显著优势之一是强大的指令遵循能力。当提供复杂的文本提示（例如"一个男人合上笔记本电脑并放在桌上"、"一个女人戴着耳机坐在桌旁，然后她拿起耳机"）时，MultiTalk 能够成功生成精确响应这些指令的视频，而其他同类方法则难以做到，往往出现动作不符或物体变形。MultiTalk 生成的视频中，视觉伪影（如手部或物体扭曲）显著减少，整体视觉质量更高，画面更自然真实。作为首个专门针对多人物生成任务设计的方法，MultiTalk 在处理复杂的交互场景时表现出色。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//f3b1a346abe9f61b3fb9bcce7710c261.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;与简单的"视频拼接"方法（即将左右人物视频分别生成再拼接）相比（如下图 7 所示），MultiTalk 能够有效处理人物间的互动，避免了拼接方法中常见的左右片段不一致性问题，使得多人物对话和互动更加流畅自然。论文还通过可视化自注意力图，直观地展示了 MultiTalk 能够自适应地识别视频中特定人物的定位，这进一步证明了 L-ROPE 方法在实现精确音频绑定方面的有效性。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//e2edf1964be1420e0dcd5b658a2c58cb.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h2&gt;四、总结与展望&lt;/h2&gt; 
&lt;p&gt;MultiTalk 提出一种音频驱动多人物对话视频生成方案，其核心突破在于其创新的 L-ROPE 方法，它通过结合自适应人物定位和带有类别信息的标签编码，有效解决了多流音频的注入和人物绑定这一难题。此外，其精心设计的部分参数训练和多任务训练策略，确保了模型在有限资源下依然能够保持强大的指令遵循能力和高质量的视觉输出。&lt;/p&gt; 
&lt;p&gt;MultiTalk 的诞生，预示着其在多角色电影制作、虚拟直播、游戏开发、教育内容创作等领域具有广阔的应用前景。我们深信，未来它将极大地降低多角色视频的制作门槛，使个性化、交互式内容创作变得更加高效和便捷。尽管仍存在真实音频与合成音频的性能差距等局限，但 MultiTalk 为未来的研究指明了方向。我们期待 MultiTalk 及其后续研究能够进一步推动 AI 在模拟和创造复杂人机交互方面的能力，使数字世界中的人物更加栩栩如生。&lt;/p&gt; 
&lt;p&gt;现在，MultiTalk 已经在 GitHub 上&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FMeiGen-AI%2FMultiTalk" target="_blank"&gt;开源&lt;/a&gt;，欢迎更多的同学加入我们，一起共建。&lt;/p&gt; 
&lt;h2&gt;五、关于美团视觉智能部&lt;/h2&gt; 
&lt;p&gt;美团视觉智能部围绕丰富的本地生活电商场景，建设从基础通用到细分领域的视觉技术能力，包括：视觉生成大模型、多模交互虚拟人，助力营销创意生产和商家低成本直播；文档、商品、安全多模态大模型，助力商家开店经营、平台商品治理和违规账号治理；人脸识别、文字识别、细粒度图像分析、高性能检测分割、街景理解，成为公司基础设施能力。曾开源行业最大规模&lt;a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2F123.57.42.89%2FFoodProject.html" target="_blank"&gt;食品图像数据集 Food2K&lt;/a&gt;被全球各地区上百家机构使用，&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fmeituan%2FYOLOv6" target="_blank"&gt;目标检测框架 YOLOV6&lt;/a&gt;荣登 2023 年度世界开源贡献榜，获得 10+项国际竞赛冠军，上百项发明专利，60+篇顶会顶刊论文。曾与国内多家知名科研机构合作，多次获得省部级科技进步奖项。&lt;/p&gt; 
&lt;p&gt;| 关注「美团技术团队」微信公众号，在公众号菜单栏对话框回复【2024 年货】、【2023 年货】、【2022 年货】、【2021 年货】、【2020 年货】、【2019 年货】、【2018 年货】、【2017 年货】等关键词，可查看美团技术团队历年技术文章合集。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//2b7e833c2041e319a96878b7c8c571f0.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;| 本文系美团技术团队出品，著作权归属美团。欢迎出于分享和交流等非商业目的转载或使用本文内容，敬请注明 "内容转载自美团技术团队"。本文未经许可，不得进行商业性转载或者使用。任何商用行为，请发送邮件至 &lt;a href="https://www.oschina.net/action/GoToLink?url=mailto%3Atech%40meituan.com" target="_blank"&gt;tech@meituan.com&lt;/a&gt; 申请授权。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/meituantech/blog/18638557</link>
      <guid isPermaLink="false">https://my.oschina.net/meituantech/blog/18638557</guid>
      <pubDate>Fri, 27 Jun 2025 07:31:26 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>没人喜欢写 README？Gitee：现在你不用写了</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;README 是开源项目的第一张脸，它决定着项目的专业度，也直接影响用户能否快速上手。但你我都知道，写代码有意思，写文档太难了。尤其在项目初期，README 不是空白，就是随便应付两句草草了事。&lt;/p&gt; 
&lt;p&gt;现在，Gitee 帮你解决这个老大难问题——&lt;strong&gt;全新上线的「AI README 生成与优化」，能一键生成结构清晰、内容专业的项目文档&lt;/strong&gt;，还能自动优化现有 README，让你的项目门面焕然一新！&lt;/p&gt; 
&lt;h2&gt;核心亮点&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;自动识别项目类型&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;深度分析依赖、配置文件与代码结构，精准提取关键信息。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;社区最佳实践模板&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;内置项目简介、快速开始、使用指南、贡献规范、许可证等完整模块。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;一键生成 &amp;amp; 智能优化&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;适配新旧仓库：没有 README 时快速生成，有 README 时自动补全、重排结构、提升可读性。&lt;/p&gt; 
&lt;h2&gt;上手指南&lt;/h2&gt; 
&lt;p&gt;该功能已向所有 Gitee 社区版&amp;amp;企业版开源仓库开放，新老项目均可直接体验。&lt;/p&gt; 
&lt;h3&gt;进入仓库主页&lt;/h3&gt; 
&lt;p&gt;Gitee 会自动检测是否存在 README。若缺失，将显示「AI 生成 README」按钮。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0627/150448_MUT2_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;启动 AI 生成&lt;/h3&gt; 
&lt;p&gt;点击「AI 生成 README」按钮后，马建仓 AI 助手将：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;🔍 分析仓库内容&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;📊 摘取关键信息&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;📝 生成专业 README&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;📤 将变更以轻量级 PR 提交&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0627/150509_02hr_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;审核并合并 PR&lt;/h3&gt; 
&lt;p&gt;检查自动生成的内容，确认无误后合并即可。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0627/150522_Aq2Q_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0627/150531_yTAj_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;完成展示&lt;/h3&gt; 
&lt;p&gt;合并后，新 README 会立即呈现在仓库首页。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0627/150541_gohB_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;如何优化现有 README？&lt;/h3&gt; 
&lt;p&gt;对于已有 README 文件的项目，也可使用 AI 优化功能：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;补全缺失板块（贡献指南、许可证等）&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;重排段落结构、统一格式&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;强化示例代码与项目特色描述&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0627/150552_701l_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h2&gt;智能开发，就在 Gitee&lt;/h2&gt; 
&lt;p&gt;AI README 生成与优化功能，是 Gitee 在智能开发领域迈出的又一步。它回应了开发者在文档撰写上的真实痛点，&lt;strong&gt;将重复性工作交由 AI 自动完成&lt;/strong&gt;，让开发者能够专注于更有价值的创新；同时通过标准化模板与智能润色机制，&lt;strong&gt;帮助项目迅速建立专业形象，提升传播效果&lt;/strong&gt;；清晰、完善的文档结构，也&lt;strong&gt;为协作与社区建设提供了坚实基础&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;未来，Gitee 将持续拓展智能工具链能力，打造更开放、更高效的开发环境，让 AI 真正融入开发全流程，服务每一位开发者与每一个项目。&lt;/p&gt; 
&lt;p&gt;欢迎体验：&lt;a href="https://gitee.com/"&gt;https://gitee.com/&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357583</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357583</guid>
      <pubDate>Fri, 27 Jun 2025 07:07:26 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>谷歌发布 Gemma 3n，专为移动设备打造的全新 AI 模型</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Google&lt;u&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdevelopers.googleblog.com%2Fen%2Fintroducing-gemma-3n-developer-guide%2F" target="_blank"&gt;宣布&lt;/a&gt;&lt;/u&gt;推出 Gemma 3n，这是其下一代的开放 AI 模型，与我们之前看到的相比有了显著的提升。继上个月在 Google I/O 大会上进行预览后，完整版现已发布，可直接在移动硬件上运行。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-3a9a3ee99131ac4d39d56d44480a911816f.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;与 Gemini 的不同之处在于，Gemma 是为开发者下载和修改而设计的，而 Gemini 是 Google 的封闭式专有模型。&lt;/p&gt; 
&lt;p&gt;该模型现在可以原生处理图像、音频和视频等输入并生成文本，这比仅仅基于文本的模型有了很大的飞跃。它甚至可以在内存仅为 2GB 的硬件上运行，并且据称在编码和推理等任务上表现更佳。以下是 Google 列出的所有改进：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;多模式设计： Gemma 3n 原生支持图像、音频、视频和文本输入和文本输出。&lt;/li&gt; 
 &lt;li&gt;专为设备端优化： Gemma 3n 型号以效率为设计重点，提供两种基于有效参数的尺寸：E2B 和 E4B。虽然它们的原始参数数量分别为 5B 和 8B，但架构创新使其运行内存占用与传统的 2B 和 4B 型号相当，仅需 2GB (E2B) 和 3GB (E4B) 内存即可运行。&lt;/li&gt; 
 &lt;li&gt;突破性的架构： Gemma 3n 的核心是新颖的组件，例如用于计算灵活性的 MatFormer 架构、用于提高内存效率的每层嵌入 (PLE) 以及针对设备用例优化的新型音频和基于 MobileNet-v5 的视觉编码器。&lt;/li&gt; 
 &lt;li&gt;增强质量： Gemma 3n 在多语言（支持 140 种文本语言和 35 种语言的多模式理解）、数学、编码和推理方面实现了质量改进。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Gemma 3n 高效的核心是 Google 称之为 MatFormer 的新架构。Google 用俄罗斯套娃的比喻来描述它&lt;strong&gt;：一个较大的模型里面包含一个较小的、功能齐全的版本&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;这使得单个模型能够以不同的规模运行不同的任务。至于基准测试，更大的 E4B 模型是第一个在 10B 参数下突破 LMArena 1300 分的模型。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-429797bc92f4d938d5e5f8d3ab6c9e9af2b.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;该模型的音频功能现在支持设备上的语音转文本和翻译，并使用能够精细处理语音的编码器。视觉方面则由名为 MobileNet-V5 的全新编码器提供支持，该编码器比其前代产品速度更快、效率更高。它能够在 Google Pixel 设备上以高达 60FPS 的速度处理视频。&lt;/p&gt; 
&lt;p&gt;如果您有兴趣，可以立即开始使用，因为这些模型可以通过 Hugging Face 和 Kaggle 等熟悉的平台获得，您甚至可以直接在 Google AI Studio 中对它们进行试验：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Faistudio.google.com%2Fprompts%2Fnew_chat%3Fmodel%3Dgemma-3n-e4b-it" target="_blank"&gt;https://aistudio.google.com/prompts/new_chat?model=gemma-3n-e4b-it&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;u&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdevelopers.googleblog.com%2Fen%2Fintroducing-gemma-3n-developer-guide%2F" target="_blank"&gt;更多详情请参阅官方公告帖&lt;/a&gt;&lt;/u&gt;。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357582/google-gemma-3n</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357582/google-gemma-3n</guid>
      <pubDate>Fri, 27 Jun 2025 06:55:26 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>我国自主研发首套航空运输大模型发布</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;我国自主研发的首套航空运输大模型天牧」低空大模型日前在南京发布，该大模型可以作为空中交通指挥专家，同时具备智能问答、辅助决策等核心能力，其研发在低空智能管理领域创下多项技术首发成果，实现了多项关键技术的突破。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;「天牧」低空大模型属于低空飞行智慧大脑「天行」中枢的系列产品，该系列产品可以作为空中交通指挥专家，解决低空飞行中的空情监控、资源调度等问题。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;中国电科首席专家丁一波介绍称，「我们最主要的功能可以归纳为管、协、服三大能力。管理就是对航空器的登记注册和飞行管理工作。我们的协作主要是在相应的飞行活动过程中着力解决有效协同问题。我们的服务主要是提供情报的服务、气象的服务和我们各种飞行中的数据服务。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="367" src="https://oscimg.oschina.net/oscnet/up-be2d6109d171ae5f30cc313a4110597193e.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在「天牧」低空大模型的加持下，「天行」系统通过对超千万条低空运行规则的学习训练，在高算力集群支撑下实现了对复杂场景自主查询效率提升 50% 的技术跨越，首次实现了「AI 驱动」的低空管服模式。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;除了「天行」及「天牧」低空大模型，此次大会上还发布了聚焦低空安全的「天衞」系统以及聚焦飞行器产品研发的「天工」等系列产品。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357561</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357561</guid>
      <pubDate>Sun, 11 May 2025 05:42:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>美团在 AI 投入超百亿，由于 GPU 价格昂贵</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F-Cj0BqaRie7Jk8WNA7QRjA" target="_blank"&gt;根据《科创板日报》的报道&lt;/a&gt;，美团核心本地商业 CEO 王莆中 6 月 26 日透露，美团在 AI 上的投入非常大，每年的投入超过百亿元。&lt;/p&gt; 
&lt;p&gt;王莆中特别指出，由于图形处理器（GPU）等硬件设施价格昂贵，AI 技术的研发成本居高不下。然而，他认为这种投入是必要的，因为只有建立起坚实的 AI 基础设施，并不断推进大模型的研究开发，才能让过去十几年间积累的宝贵数据资源焕发出新的活力，从而更好地服务于用户。&lt;/p&gt; 
&lt;p&gt;根据最新的财务报告，美团在 2024 年度的研发总支出达到了 211 亿元。王莆中预测，到 2030 年，中国服务零售行业的线上化率将从当前的 9% 提升至 25%，市场规模将达到 7 万亿元人民币。届时，预计将有 300 个品牌能够开设至少一千家店铺，成为行业的领军者。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;相关阅读&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/news/354675/meituan-nocode" target="news"&gt;美团发布 AI&amp;nbsp;Coding Agent 工具「NoCode」&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/news/354604" target="news"&gt;美团王兴详解 AI 布局：No Code 平台免费开放，1680 个应用已上线&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/news/352148" target="news"&gt;美团：过去一季度内 52% 代码由 AI 生成&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357551</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357551</guid>
      <pubDate>Sun, 11 May 2025 04:03:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Ubuntu 开发商 Canonical 去年营收近 3 亿美元</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;十年前， Ubuntu Linux 背后的公司 Canonical 的营收约为 8100 万美元（2014 年），员工人数约为 337 人。当时，他们的 Linux 桌面业务仍在 OEM/ODM 预装系统、企业桌面环境以及利润丰厚的服务器/云领域站稳脚跟。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Canonical 最近提交了 2024 年年度报告，目前其营收已接近 3 亿美元，员工人数超过 1100 人&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;几天前，Canonical 向其总部所在地英国公司注册局提交了截至 2024 年 12 月 31 日的年度报告。这份报告为公众提供了一些关于这家 Ubuntu Linux 背后公司的健康状况和整体增长情况的有趣见解。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="image.webp" src="https://static.oschina.net/uploads/img/202506/27112815_bbMf.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Canonical 2024 年年终财报显示，公司营收为 2.92 亿美元，较 2023 年的 2.51 亿美元大幅增长。而前一年，也就是 2022 年，他们的营收为 2.05 亿美元。他们的毛利率也从前一年的 80% 提升至 83%。2024 年，&lt;/p&gt; 
&lt;p&gt;Canonical 招聘了 100 多名新员工，公司平均员工人数从 1034 人增长至 1175 人。2022 年，他们的平均员工人数为 858 人。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="image.webp" src="https://static.oschina.net/uploads/img/202506/27112816_FIDc.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在其 2.91 亿美元的营收中，他们报告的毛利润为 2.58 亿美元，营业利润为 1550 万美元。1550 万美元的营业利润高于 2023 年的 1120 万美元，比 Canonical 早期的情况要好得多，那时他们通常每年都处于亏损状态，并且依靠 Ubuntu 创始人 Mark Shuttleworth 的资金维持运营。随着 Ubuntu Linux 的发展，Canonical 已经稳固地站稳了脚跟，到目前为止，这种情况已经持续了很多年。&lt;/p&gt; 
&lt;p&gt;已经有一段时间没有听到任何关于 Canonical 可能进行 IPO 的传闻/谈论了……上一次是在 2022 年，据此前报道他们计划在 2023 年进行 IPO，但后来就没了下文。&lt;/p&gt; 
&lt;p&gt;无论如何，至少他们的财务状况依然强劲，并且继续朝着正确的方向前进，为明年 Ubuntu 26.04 LTS 的重大发布大步迈进。 感兴趣的朋友可以通过&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ffind-and-update.company-information.service.gov.uk%2Fcompany%2F06870835%2Ffiling-history%2FMzQ2OTM2OTYwNWFkaXF6a2N4%2Fdocument%3Fformat%3Dpdf%26download%3D0" target="_blank"&gt;英国公司注册处 (UK Companies House)&lt;/a&gt;获取完整的 Canonical 2024 年年度报告。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357540/canonical-2024-annual-report</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357540/canonical-2024-annual-report</guid>
      <pubDate>Sun, 11 May 2025 03:28:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>DeepSeek R2 推迟发布：因 H20 算力短缺、以及梁文锋对其性能尚不满意</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.theinformation.com%2Farticles%2Fdeepseeks-progress-stalled-u-s-export-controls" target="_blank"&gt;根据《The Information》的报道&lt;/a&gt;，DeepSeek 工程师在过去几个月一直致力于完善 R2 模型，但梁文锋对 R2 现在的性能还不满意，工程师团队仍在全力优化和打磨，发布时间待定。梁文峰要求模型达到更出色的结果才批准发布。&lt;/p&gt; 
&lt;p&gt;此外，由于美国出口管制导致中国市场英伟达服务器芯片（H20）短缺，R2 的大规模普及可能面临困难。&lt;/p&gt; 
&lt;p&gt;目前，大多数使用 DeepSeek R1 模型的中国云客户仍依赖 H20 芯片。报道指出，如果 DeepSeek 即将推出的 R2 模型其性能超过目前市面上的开放替代模型，预计使用量将激增，&lt;strong&gt;超出中国云平台的处理能力&lt;/strong&gt;。因为他们需要先进的英伟达芯片来运行 AI 模型。&lt;/p&gt; 
&lt;p&gt;DeepSeek 已向部分中国云公司提供了 R2 的技术规范，以指导其托管和分发模型的计划，但尚未公布具体的发布日期。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;相关阅读&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/news/352701/deepseek-r1-0528-release-notes" target="news"&gt;DeepSeek-R1-0528 更新：思考更深，推理更强&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/news/352460/deepseek-r1-0528" target="news"&gt;DeepSeek R1 模型完成小版本试升级，逻辑理解能力提升&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357535/deepseeks-progress-stalled-u-s-export-controls</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357535/deepseeks-progress-stalled-u-s-export-controls</guid>
      <pubDate>Sun, 11 May 2025 03:16:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Nacos 3.0 架构全景解读，AI 时代服务注册中心的演进</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;p&gt;作者：杨翊（席翁），柳遵飞（翼严），罗鑫（子葵）&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Nacos&lt;/strong&gt; &lt;code&gt;/nɑ:kəʊs/&lt;/code&gt;是 Dynamic &lt;strong&gt;Na&lt;/strong&gt; ming and &lt;strong&gt;Co&lt;/strong&gt; nfiguration &lt;strong&gt;S&lt;/strong&gt; ervice 的首字母简称，随着 Nacos 3.0 的发布，定位由&lt;code&gt;"更易于构建云原生应用的动态服务发现、配置管理和服务管理平台"&lt;/code&gt;升级至&lt;code&gt;" 一个易于构建 AI Agent 应用的动态服务发现、配置管理和 AI 智能体管理平台"&lt;/code&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-334a4cbf68666f5998460020dbedad24d97.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Nacos 从 2018 年 7 月宣布开源以来，已经走过了第六个年头，在这六年里，备受广大开源用户欢迎，收获许多社区大奖。Nacos 在社区共同的建设下不断成长，逐步开始帮助用户解决实际问题，助力企业数字化转型，目前已经广泛使用在国内的公司中，根据微服务领域调查问卷，Nacos 在注册配置中心领域已经成为&lt;strong&gt;国内首选&lt;/strong&gt; ，占有 &lt;strong&gt;50%+国内市场&lt;/strong&gt; 份额，被&lt;strong&gt;各行各业的头部企业&lt;/strong&gt;广泛使用。在此期间，Nacos 的部署包下载量突破 300w 次，官网每年访问用户数超过 90w 人，被国内各主流云厂商托管服务。&lt;/p&gt; 
&lt;p&gt;随着 AI 时代到来以及 Nacos 3.0 版本的正式发布，Nacos 未来的演进目标以及架构也会随之升级。本文会对比 Nacos 3.0 与 Nacos 2.0 的架构异同，对 Nacos 3.0 的主要功能原理进行介绍。&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Nacos 2.0 架构回顾&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr&gt; 
&lt;p&gt;Nacos 2.0 的架构主要聚焦对&lt;code&gt;性能&lt;/code&gt;和&lt;code&gt;可扩展性&lt;/code&gt;进行优化和提升。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-92aef5263493adf4644595a964f278a1c2f.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;对于&lt;code&gt;性能&lt;/code&gt;升级，Nacos 2.0 通过将通信模型从 HTTP 升级至 gRPC，从短连接模型升级到长连接模型，使得 Nacos 的通信吞吐量中极大提升；同时配合数据存储和数据结构模型的升级，进一步减少核心操作所涉及的步骤和链路，最终实现性能的 &lt;strong&gt;10 倍提升&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;关于&lt;code&gt;可扩展性&lt;/code&gt;升级，Nacos 2.0 通过将一些具有个性化需求的通用能力进行抽象，进行插件化改造的方式，允许 Nacos 用户和运维人员能够开发自定义插件，适配个人或企业的个性化需求。&lt;/p&gt; 
&lt;p&gt;虽然 Nacos 2.0 在&lt;code&gt;性能&lt;/code&gt;和&lt;code&gt;可扩展性&lt;/code&gt;实现了一些突破，但仍然还存在一些挑战。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-37c34e83fbdfb2c9872dfe9451498e525a9.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;其中一个主要的挑战就是 Nacos 的安全风险。比如：Nacos 2.0 中所有的 HTTP API 均使用 8848 端口， 这其中及包含了 1.X 客户端使用的 API，也包含了运维人员以及控制枱的 API。不同类型的 API， 对于权限的需求其实是不同的，对于网络访问的连通性要求也是不同的。使用单端口并且使用唯一的鉴权开关，导致了网络的访问控制，以及鉴权控制都不是很灵活。许多用户为了方便使用，将此端口暴露在办公网甚至公网环境，同时未开启鉴权，这就造成了安全风险。&lt;/p&gt; 
&lt;p&gt;另一个问题就是默认命名空间的使用，Nacos 最初的版本中定义了命名空间作为数据资源的强隔离属性，不同命名空间之间的服务和配置不能互相发现和获取；但在最初版本中因为历史原因，注册中心和配置中心对于默认命名空间的处理方式有一定的不统一，这导致了许多用户在使用默认命名空间时经常配置错误或者出现疑惑；并且在 Nacos 2.0 提供各种插件能力之后，许多插件实现时需要额外工作进行适配，严重阻碍了插件的开发以及插件的稳定性。&lt;/p&gt; 
&lt;p&gt;随着 AI 时代来临，AI Agent 应用的部署形态在之前云原生可弹性可伸缩的基础上，要求更加轻量，更加弹性，例如 FC 场景；在这种要求下，我们需要考虑 Nacos 之前的服务发现和配置管理的能力是否还能承载 AI Agent 的应用的部署。同时，随着越来越多的 AI Agent 的应用贯穿业务全线，Nacos 能否帮助更好地管理 AI Agent 的应用，是 Nacos 在当前的挑战，同时也是新的机遇。&lt;/p&gt; 
&lt;p&gt;为了应对这些挑战以及机遇，Nacos 3.0 架构也做了对应的升级。目标是在 AI 时代成为更安全的 Registry。设计理念也由之前的&lt;code&gt;一个更易于构建云原生应用的动态服务发现、配置管理和服务管理平台&lt;/code&gt;升级为&lt;code&gt;一个易于构建 AI Agent 应用的动态服务发现、配置管理和 AI 智能体管理平台&lt;/code&gt;。&lt;/p&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Nacos 3.0 架构&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr&gt; 
&lt;h3&gt;2.1 Nacos 3.0 整体架构解析&lt;/h3&gt; 
&lt;p&gt;Nacos 3.0 升级后的整体架构仍然以一致性协议、通信模块、其他模块等通用功能模块为基座，承载出注册中心、配置中心、AI Registry、协议增强等功能；同时通过各类多语言 SDK，桥接各个生态组件。架构的左右两侧，分别是 Nacos 的插件以及 Nacos 的一些拓展组件，它们一起构成了 Nacos 3.0 的整体架构。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-36bacdaa811ec267912daebb99f3bff0018.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;我们来重点关注 Nacos 3.0 的新增能力，即图中绿色和棕色的部分。&lt;/p&gt; 
&lt;p&gt;这其中既包括对原本注册配置中心的增强功能，即模糊订阅，也包括了对 AI 相关能力的实现和规划，如 MCP 和管理，MCP Router，动态 Prompt 及 A2A 协议支持；同时也通过支持 xDS 协议及 Nacos Controller 继续加强和探索 Mesh 生态。&lt;/p&gt; 
&lt;h3&gt;2.2 Nacos 3.0 AI Registry 架构&lt;/h3&gt; 
&lt;p&gt;了解完 Nacos 3.0 的整体架构，接下来我们来看 Nacos AI 中心（AI Registry）的架构设计。作为 Nacos 3.0 规划中最重要的能力，Nacos AI Registry 的架构被分为 3 个层次，分别是&lt;code&gt;模型层、``工具层&lt;/code&gt;和&lt;code&gt;应用层&lt;/code&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-84c758dce0cbc64de279f716cb03fd3cd8d.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在&lt;code&gt;模型层&lt;/code&gt;中，主要通过对 AI 模型中一些常用的动态参数，比如 Prompt、学习率、联网参数等进行管理，采取复用在云原生应用中配置动态管理和分发能力的方式，帮助 AI 智能体在模型层进行快速调整及试错。&lt;/p&gt; 
&lt;p&gt;模型层之上是&lt;code&gt;工具层&lt;/code&gt;，工具层主要帮助 LLM 模型和提供数据的 MCP 工具之间进行自动的发现、注册以及检索等能力，复用在云原生应用中服务的动态注册、管理、发现的能力，帮助 AI 智能体应用快速及便捷地发现 MCP 工具，同时快速过滤无关工具，减少 Token 损耗。&lt;/p&gt; 
&lt;p&gt;最顶层是 Agent 的&lt;code&gt;应用层&lt;/code&gt;，实现 AI 应用与 AI 应用之间的发现与协作。目前规划是通过支持 A2A 等社区标准协议，同时配合 Spring AI Alibaba 等 AIAgent 应用框架，帮助 AIAgent 应用便捷的自动注册自身 AI 应用，同时发现其他 AI 应用，并能够像云原生应用一样，进行任务的分发以及结论的构成。&lt;/p&gt; 
&lt;p&gt;如果从功能视角出发，Nacos AI Registry 又可被分为针对大模型 LLM 的&lt;code&gt;模型动态配置调优，&lt;/code&gt;针对 AI 应用平台的&lt;code&gt;应用开发管理&lt;/code&gt;以及针对 AI Agent 应用的&lt;code&gt;运行时能力增强&lt;/code&gt;。Nacos 希望通过不同的功能点，帮助 AI 应用像微服务云原生应用一样，能动态的调整 Prompt，学习率等参数，无需重新发布，从而帮助 AI 应用简化开发，调试过程中的繁琐操作，提高 AI 应用的开发和运行效率。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-dd07e5e90a95088b6e84dc992e28fb225fe.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-d4273e4083d0c6f838fde1393f16872b30f.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;2.3 Nacos 3.0 安全架构&lt;/h3&gt; 
&lt;p&gt;Nacos 2.0 中面临的一个主要的风险就是 Nacos 所有的 HTTP OpenAPI 均通过统一的端口进行暴露，同时使用了统一的鉴权开关，这使得使用者必须在便捷性和安全性中作出取舍，导致在许多部署的环境中可能存在安全风险。&lt;/p&gt; 
&lt;p&gt;Nacos 3.0 为了解决这个问题，从 Nacos 的部署架构上作出演进，&lt;code&gt;独立控制枱部署&lt;/code&gt;，&lt;code&gt;拆分鉴权开关&lt;/code&gt;，&lt;code&gt;分类 API &lt;/code&gt;并&lt;code&gt;默认开启控制枱及管控类 API 的鉴权。&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-3fc2b32b2e404eafbf929142ad4585d3c3b.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;同时配合&lt;code&gt;配置加密插件，``TLS 传输&lt;/code&gt;，来实现 Nacos 3.0 的零信任安全架构。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-f835e58de73cbab938bc6e5d6ed9849472a.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;除了针对 Nacos 自身的安全零信任架构外，Nacos 3.0 还将与 Druid，Spring AI Alibaba/Spring Cloud Alibaba 等开源社区，及 KMS 等安全云产品合作，提供面向应用侧的数据源运行期动态轮转方案。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-f6a73d965abc4e2b27e2a584508f18ea32d.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在这套解决方案中，数据源的凭据始终由 KMS 等凭据托管平台和系统保存，全程无人工传递和配置的过程。用户可以设置定期进行凭据的自动轮转，或在怀疑密钥泄漏时手动触发凭据轮转；触发后会通过 Nacos 动态无损的将新的加密凭据通知到 Druid 或 Spring AI Alibaba/Spring Cloud Alibaba，进行凭据的动态刷新和无损替换。这种方式极大降低了凭据泄漏的可能性，同时显著提高了安全性及出现安全风险时的收敛恢复速度。&lt;/p&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Nacos MCP Registry&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr&gt; 
&lt;h3&gt;3.1 Nacos MCP Registry 架构&lt;/h3&gt; 
&lt;p&gt;Nacos 3.0 最主要的能力升级就是作为 MCP Registry，支持了 MCP 服务的管理能力。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-88c91519a2722fe1b60a505a553c7428a19.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Nacos MCP Registry 支持三类 MCP 服务的注册方式：&lt;/p&gt; 
&lt;p&gt;第一类是将存量 HTTP 或 RPC 的服务，通过声明自动转化为 MCP 服务，配合 Higress 的协议转换能力，实现 0 代码改造成 MCP 服务协议，如何将存量 API 转化为 MCP 服务，详情可参见文档【1】。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-3f1b620f54932854273dacb77d9d5d1d201.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;第二类就是新构建的 MCP 服务注册， 配合 Spring AI 等 AI Agent 应用框架和 Nacos-MCP 的 sdk，能够做到像微服务一样自动注册到 Nacos 中进行统一的管理和维护，如何通过 Spring AI 或 Nacos-MCP 的 sdk 进行 MCP 服务的自动注册与发现，请参见文档【2】。&lt;/p&gt; 
&lt;p&gt;第三类就是已经构建好的或其他供应商提供的 MCP 服务，可以导入到 Nacos 中，进行其描述、工具列表、工具 Schema 等内容的动态修改和维护，让调试 MCP 服务变得更加简单。&lt;/p&gt; 
&lt;h3&gt;3.2 Nacos MCP Router&lt;/h3&gt; 
&lt;p&gt;Nacos 3.0 支持用户通过 3 种方式发布 MCP 服务，并对 MCP 服务的元数据和版本进行管理，但如果最终不能将这些元数据和版本信息进行实际的使用，这些信息就没有意义。&lt;/p&gt; 
&lt;p&gt;因此 Nacos 3.0 提供 Nacos MCP Router 帮助终端使用者无需实际感知 MCP 服务列表，即可自动发现和使用需要的 MCP 服务。&lt;/p&gt; 
&lt;p&gt;Nacos MCP Router 提供两种工作模式，&lt;code&gt;动态路由&lt;/code&gt;和&lt;code&gt;动态代理&lt;/code&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-054053f13b5e4c1b1712330bffd28c95d5d.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;动态路由模式将会根据 LLM 所提供的关键字信息，对注册在 Nacos 中的 MCP 服务进行相关性过滤和筛选，选择出与关键字相关的 MCP 服务进行实际的使用，从而减少对 LLM 上下文的消耗，实现&lt;code&gt;路由 &lt;/code&gt;MCP 服务的能力。&lt;/p&gt; 
&lt;p&gt;而代理模式能够进行 MCP 协议的转换，将 &lt;code&gt;stdio &lt;/code&gt;和 &lt;code&gt;sse &lt;/code&gt;类型的 MCP 服务，代理成 &lt;code&gt;streamable &lt;/code&gt;类型的 MCP 服务。代理模式下的 Nacos MCP Router 不根据关键字进行筛选，仅是将注册在 Nacos 中的 &lt;code&gt;stdio &lt;/code&gt;和 &lt;code&gt;sse &lt;/code&gt;类型的 MCP 服务，转化成 &lt;code&gt;streamable &lt;/code&gt;类型，同时应用用户在 Nacos 上修改和编辑的 Tool 描述信息，将转化后的 MCP 服务列表，返回给 LLM 供其使用。&lt;/p&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;Nacos 3.0 RoadMap&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr&gt; 
&lt;p&gt;Nacos 3.0 的目标是成为&lt;code&gt;全面拥抱 AI 时代的服务、配置、AI Registry 平台&lt;/code&gt;，因此 Nacos3.0 的 RoadMap 将会逐步实现 AI Registry 的能力，从当前的 MCP 管理，拓展到 Prompt 管理，Agent 的自动注册发现，再到 LLM 模型的参数管理和托管；同时进一步加强注册配置中心的能力和更多相关领域协议的支持（如 DNS，Mesh）。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-445ba3a6100745072fa880c31873dc11d3d.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Nacos 也希望有更多的社区贡献者加入进 Nacos 社区，帮助 Nacos 更快更好的完善和实现 Nacos3.0。&lt;/p&gt; 
&lt;ol start="5"&gt; 
 &lt;li&gt;欢迎加入 Nacos 社区&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr&gt; 
&lt;p&gt;Nacos 致力于帮助您发现、配置和管理微服务。Nacos 提供了一组简单易用的特性集，帮助您快速实现动态服务发现、服务配置、服务元数据及 AI 管理。&lt;/p&gt; 
&lt;p&gt;Nacos 帮助用户更敏捷和容易地构建、交付和管理云原生 AI 应用的平台。 Nacos 是构建以"服务"为中心的现代应用架构 (例如微服务范式、云原生范式、AI 原生范式) 的服务基础设施。&lt;/p&gt; 
&lt;p&gt;Nacos 3.0 还有很多待完成的功能及大量待探索和开发的领域，欢迎大家扫码加入 Nacos 社区群及 Nacos MCP 社区讨论群，参与 Nacos 社区的贡献和讨论，在 Nacos 社区一起搭把手，让你的代码和能力有机会能在各行各业领域内进行释放能量，期待认识你和你一起共建 Nacos 社区；&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;"Nacos 相信一切都是服务，每个服务节点被构想为一个星球，每个服务都是一个星系；Nacos 致力于帮助这些服务建立连接赋予智能，助力每个有面向星辰的梦想能够透过云层，飞在云上，更好的链接整片星空。"&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Nacos 官网：&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnacos.io%2F" target="_blank"&gt;https://nacos.io/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Nacos 仓库地址：&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Falibaba%2Fnacos" target="_blank"&gt;https://github.com/alibaba/nacos&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;"Nacos 社区群 5"群的钉钉群号：&lt;/p&gt; 
&lt;p&gt;120960003144&lt;/p&gt; 
&lt;p&gt;"Nacos MCP 社区讨论群"群的钉钉群号：&lt;/p&gt; 
&lt;p&gt;97760026913&lt;/p&gt; 
&lt;ol start="6"&gt; 
 &lt;li&gt;更多了解 Nacos 3.0&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr&gt; 
&lt;p&gt;6 月 6 日，Nacos 在上海举办了开源开发者沙龙 MeetUp 活动，此次是 Nacos 社区成员今年首次线下分享最新的能力和实践，并邀请了 Spring AI Alibaba 和 Higress 一起分享一站式的开源解决方案。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-fecce2cf0706af578321cb7f3935150e704.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;有需要 MeetUp 的 PPT 或希望回看 MeetUp 活动视频的同学，欢迎加入本文末尾的群中获取。&lt;/p&gt; 
&lt;p&gt;同时如果对 Nacos 3.0 的架构，运行原理，最佳实践等内容感兴趣的同学，欢迎阅读 Nacos 3.0 更多相关文章：&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzUzNzYxNjAzMg%3D%3D%26mid%3D2247574941%26idx%3D1%26sn%3D18ea079839f6c0e5563f0e0b3bd2e80d%26scene%3D21%23wechat_redirect" target="_blank"&gt;《0 代码改造实现应用运行时数据库密码无损轮转》&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzUzNzYxNjAzMg%3D%3D%26mid%3D2247574770%26idx%3D1%26sn%3Df975121e36eb751a7659a66f42cbac04%26scene%3D21%23wechat_redirect" target="_blank"&gt;《Nacos MCP Router 新版发布：支持 Docker 远程部署，MCP 的多协议 stido、SSE、Streamable 互相转换》&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzUzNzYxNjAzMg%3D%3D%26mid%3D2247574635%26idx%3D1%26sn%3D383a5c81ec298585f660b687a5dd4b12%26scene%3D21%23wechat_redirect" target="_blank"&gt;《企业生产环境中，实现 MCP 服务的统一管理和智能路由的实践》&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU4NzU0MDIzOQ%3D%3D%26mid%3D2247519792%26idx%3D1%26sn%3D4c1c6491fb2d1f32f8370057be15e6ba%26scene%3D21%23wechat_redirect" target="_blank"&gt;《Nacos 3.0 正式发布：MCP Registry、安全零信任、链接更多生态》&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;【1】存量 API 转换 MCP 手册&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnacos.io%2Fdocs%2Flatest%2Fmanual%2Fuser%2Fai%2Fapi-to-mcp%2F%3Fspm%3D5238cd80.2ef5001f.0.0.3f613b7ciRMNL5" target="_blank"&gt;https://nacos.io/docs/latest/manual/user/ai/api-to-mcp/?spm=5238cd80.2ef5001f.0.0.3f613b7ciRMNL5&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;【2】MCP Server 自动注册手册&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnacos.io%2Fdocs%2Flatest%2Fmanual%2Fuser%2Fai%2Fmcp-auto-register%2F%3Fspm%3D5238cd80.2ef5001f.0.0.3f613b7ciRMNL5" target="_blank"&gt;https://nacos.io/docs/latest/manual/user/ai/mcp-auto-register/?spm=5238cd80.2ef5001f.0.0.3f613b7ciRMNL5&lt;/a&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/3874284/blog/18638349</link>
      <guid isPermaLink="false">https://my.oschina.net/u/3874284/blog/18638349</guid>
      <pubDate>Sun, 11 May 2025 03:14:00 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>雷军：一些人说小米自研芯片产品卖不动，瞎扯</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;在小米人车家全生态发布会后，雷军进行了分享表示，做玄戒 O1 的时候，小米完全没有想到 O1 做的这么好。「我真的没想到，所以整个 O1 的芯片总量定的就不够。规划是做了 4 款产品」。&lt;/p&gt; 
&lt;p&gt;&lt;img height="305" src="https://oscimg.oschina.net/oscnet/up-b6b4bb5a4c2a474139aff036b5980fbc097.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;雷军还特别强调，「特别感谢朱丹领军的整个芯片团队为小米做出的巨大贡献，我自己用的也是玄戒的手机，现在体验特别好」。&lt;/p&gt; 
&lt;p&gt;雷军透露，第二代玄戒芯片会考虑在车上应用。「第一代主要是验证技术，技术好到我无法相信」。&lt;/p&gt; 
&lt;p&gt;「我们这几款手机和平板备货都很少，我也看到一些说我们卖不动，瞎扯」，他说，「主要是三四年前，你能知道芯片做的有这么好？肯定不知道。我们下一步肯定会芯片上车，我们全部自研了四合一的域控制器，我们就是为了掌握这个技术，为将来小米芯片上车做好准备」。（新浪科技）&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357527</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357527</guid>
      <pubDate>Sun, 11 May 2025 02:52:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>阿里巴巴 2025 财年收入 9963 亿元</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;6 月 26 日晚，阿里巴巴集团发布 2025 财年年报显示，2025 财年阿里巴巴集团收入达 9963.47 亿元，净利润同比增长 77% 至 1259.76 亿元，展现出强劲的盈利能力。在 AI 需求的推动下，阿里云财年收入突破双位数增长，AI 相关产品收入连续七个季度实现三位数同比增长。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在 AI 领域，过去一年阿里发布并开源多款模型，覆盖全尺寸、全模态、多场景。4 月最新发布的阿里通义 Qwen3（简称「千问 3」）大模型，开源仅一个月全球累计下载量突破 1250 万。截至 4 月底，阿里通义已开源 200 余款模型，全球下载量超过 3 亿次，千问系列衍生模型数量超 10 万个，成为全球最大的开源模型家族。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;阿里云加速 AI 产品国际化，截至 2025 年 3 月 31 日，为全球 34 个地区提供云计算服务。以通义大模型为底座，淘宝天猫、1688、阿里国际站、夸克、钉钉、高德、飞猪、闲鱼等阿里多业务 AI 升级加速。其中，阿里 AI 旗舰应用夸克用户规模同比迅速增长，截至 2025 财年末，月活跃用户数已突破 2 亿；2025 年 3 月，钉钉的平均付费周活跃用户数达 4200 万，目前钉钉是国内最大的效率办公类 App。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在致股东信中，阿里巴巴表示，「阿里的基因里没有守成，只有创造。今天的阿里巴巴，正在以创业者的姿态，开启面向 AI 时代的全新征程。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="373" src="https://oscimg.oschina.net/oscnet/up-074b55c1ba3ac1d7195b638a6c1bada394d.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;此外，阿里巴巴合伙人名单相比 2024 财年年报披露时发生变化，总数从 26 人减少至 17 人，戴珊、方永新、彭蕾、宋洁、孙利军、武衞、俞永福、张勇、朱顺炎等 9 人退出合伙人之列。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357517</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357517</guid>
      <pubDate>Sun, 11 May 2025 02:19:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>快手开源多模态大模型 Kwai Keye-VL</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;快手宣布并开源其最新自研的多模态大语言模型 Kwai Keye-VL。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;根据介绍，Kwai Keye-VL 以 Qwen3-8B 语言模型为基础，引入了基于开源 SigLIP 初始化的 VisionEncoder，能够深度融合并处理文本、图像、视频等多模态信息，凭借其创新的自适应交互机制与动态推理能力，旨在为用户提供更智能、全面的多模态交互体验。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;Kwai Keye-VL 支持动态分辨率输入，按原始比例将图像切分为 14x14 &amp;nbsp;patch 序列，由一个 MLP 层将视觉 Token 进行映射与合并。模型采用 3D RoPE （旋转位置编码）统一处理文本、图像和视频，并通过位置编码与时间戳对齐，精准捕捉视频时序变化。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="366" src="https://oscimg.oschina.net/oscnet/up-f9bb9b208e03575669510048f8ff6cabc1e.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="138" src="https://oscimg.oschina.net/oscnet/up-a5530c104b198c517ed650e3e67740584c5.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在视觉理解与逻辑推理能力方面，Kwai Keye-VL 的综合感知能力媲美同规模顶尖模型，并在复杂推理任务中展现出显著优势。尤其是逻辑推理方面，Kwai Keye-VL 在最新的 2025 年高考全国数学卷中取得了 140 分的成绩。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="314" src="https://oscimg.oschina.net/oscnet/up-d0c30a1de0375792399c2797d5e075fce36.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;为突破公开数据集的数据污染、语言覆盖局限及任务单一性等问题，快手构建了内部评测集 KC-MMBench。结果显示：该模型在 VideoMME 等权威公开 Benchmark 中以 67.4 分超越 Qwen2.5-VL-7B（62.7）与 InternVL-3-8B（65.5）；在内部短视频场景评测中优势进一步扩大，综合得分领先 SOTA 模型超 10%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="296" src="https://oscimg.oschina.net/oscnet/up-d5f95b60cd23280d98fa13ffda02bd537e7.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;更多详情可&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F2JRGYhB_VDPecXMjp3gZsQ" target="_blank"&gt;查看官方公告&lt;/a&gt;。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357515</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357515</guid>
      <pubDate>Sun, 11 May 2025 02:12:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Gartner：超 40% 的 AI 智能体项目活不过两年</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;研究咨询公司 Gartner 最新发布的一份报告&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.gartner.com%2Fen%2Fnewsroom%2Fpress-releases%2F2025-06-25-gartner-predicts-over-40-percent-of-agentic-ai-projects-will-be-canceled-by-end-of-2027" target="_blank"&gt;指出&lt;/a&gt;，预计到 2027 年底，超过 40% 的 AI 智能体项目将被取消，原因是成本不断上升和商业价值不明确。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;Gartner 高级总监分析师 Anushree Verma 表示：「目前大多数 AI 智能体项目都处于早期实验或概念验证阶段，这些项目大多受到炒作的驱动，并且经常被误用。这可能会让企业忽视大规模部署 AI 智能体的实际成本和复杂性，从而阻碍项目投入生产。他们需要拨开炒作的迷雾，谨慎地制定战略决策，确定在何处以及如何应用这项新兴技术。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="281" src="https://oscimg.oschina.net/oscnet/up-4f7c22926062ebd6122d8165e030afd0db5.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;Gartner 基于 3412 名受访者的调查结果显示，19% 的人表示其组织已对 AI 智能体项进行了大量投资，42% 的人进行了保守投资，8% 的人没有投资，其余 31% 的人采取观望态度或不确定。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;许多供应商通过「洗牌」来炒作，即对现有产品（例如 AI 助手、机器人流程自动化 (RPA) 和聊天机器人）进行品牌重塑，而这些产品本身并不具备实质性的智能体功能。Gartner 估计，在数千家 AI 智能体供应商中，只有大约 130 家是有真材实料的。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;「大多数 AI 智能体方案缺乏显著的价值或投资回报率 (ROI)，因为目前的模型还不够成熟，无法自主实现复杂的业务目标或持续遵循细微的指令。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;Gartner 预测，到 2028 年，至少 15% 的日常工作决策将通过 AI 智能体自主做出，而 2024 年这一比例为 0%。此外，到 2028 年，33% 的企业软件应用程序将包含 AI 智能体，而 2024 年这一比例还不到 1%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在早期阶段，Gartner 建议仅在能够带来明确价值或投资回报率 (ROI) 的情况下才应采用 AI 智能体。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;「为了从 AI 智能体中获得真正的价值，组织必须专注于企业生产力，而不仅仅是增强单个任务。他们可以先在需要决策时使用 AI 智能体，在日常工作流程中实现自动化，并在简单检索时使用助手。这关乎通过成本、质量、速度和规模来推动业务价值。」&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357455/gartner-over-40-percent-agentic-ai-projects-cancel-2027</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357455/gartner-over-40-percent-agentic-ai-projects-cancel-2027</guid>
      <pubDate>Sat, 10 May 2025 10:38:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>LowCodeEngine —— 企业级低代码技术体系</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                        
                                                                                    &lt;p&gt;&lt;span style="background-color:#ffffff; color:#1f2328"&gt;一套面向扩展设计的企业级低代码技术体系。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style="background-color:#ffffff; color:#1f2328"&gt;&lt;img alt="" height="276" src="https://static.oschina.net/uploads/space/2025/0619/160054_sOPF_4252687.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt;

&lt;div style="text-align:start"&gt;
&lt;h2&gt;特性&lt;/h2&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;提炼自企业级低代码平台的面向扩展设计的内核引擎，奉行最小内核，最强生态的设计理念&lt;/li&gt;
&lt;li&gt;开箱即用的高质量生态元素，包括，物料体系、设置器、插件，等&lt;/li&gt;
&lt;li&gt;完善的工具链，支持，物料体系、设置器、插件，等生态元素的全链路研发周期&lt;/li&gt;
&lt;li&gt;强大的扩展能力，已支撑 100+ 个各种类型低代码平台&lt;/li&gt;
&lt;li&gt;使用 TypeScript 开发，提供完整的类型定义文件&lt;/li&gt;
&lt;/ul&gt;

&lt;div style="text-align:start"&gt;
&lt;h2&gt;兼容环境&lt;/h2&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;现代浏览器（Chrome &amp;gt;= 80, Edge &amp;gt;= 80, last 2 safari versions, last 2 firefox versions）&lt;/li&gt;
&lt;/ul&gt;

&lt;div style="text-align:start"&gt;
&lt;h2&gt;引擎协议&lt;/h2&gt;
&lt;/div&gt;

&lt;p style="color:#1f2328; text-align:start"&gt;引擎完整实现了《低代码引擎搭建协议规范》和《低代码引擎物料协议规范》，协议栈是低代码领域的物料能否流通的关键部分。&lt;/p&gt;

&lt;p&gt;&lt;img height="277" src="https://static.oschina.net/uploads/space/2025/0619/160028_gMUx_4252687.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

                                                                    &lt;/div&gt;
                                                                </description>
      <link>https://www.oschina.net/p/lowcode-engine</link>
      <guid isPermaLink="false">https://www.oschina.net/p/lowcode-engine</guid>
      <pubDate>Sat, 10 May 2025 09:49:00 GMT</pubDate>
    </item>
    <item>
      <title>Mozilla 终止维护开源语音转文本引擎项目「DeepSpeech」</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;DeepSpeech 是 Mozilla 开发的一款开源语音转文本引擎，基于百度 2014 年发表的研究论文《Deep Speech: Scaling up end-to-end speech recognition》所提出的端到端语音识别方法开发。&lt;/p&gt; 
&lt;p&gt;从&amp;nbsp;DeepSpeech 的仓库动态来看，Mozilla 已于上周将项目仓库归档，并表示停止维护。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0626/155240_73ji_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;作为一款端到端自动语音识别（ASR）引擎，DeepSpeech 即使在 Raspberry Pi SBC 和其他低功耗系统上运行时，也能提供出色的实时通信性能。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-762647632f4a326522c5f510328561b4af1.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;遗憾的是，&lt;span&gt;近年来 DeepSpeech 项目的活跃度持续降低，其&lt;/span&gt;最后一个标记版本是 2020 年 12 月发布的 0.9.3。&lt;/p&gt; 
&lt;p&gt;DeepSpeech&lt;span&gt;&amp;nbsp;GitHub 仓库已经有近 4 年没有任何 commit，社区贡献和更新频率都不尽如人意，这使得项目的进一步发展受到限制，因此 Mozilla 选择终止该项目。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357400/mozilla-deepspeech-discontinued</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357400/mozilla-deepspeech-discontinued</guid>
      <pubDate>Sat, 10 May 2025 08:04:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Cursor 如何保障「代码索引」的安全、高效</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;编者按：&lt;/strong&gt; AI 编程工具如何迅速检索海量代码库，并精准定位到最相关的代码片段？这个看似不可能完成的任务，却是决定现代 AI 编程工具用户体验的关键技术挑战。&lt;/p&gt; 
 &lt;p&gt;我们今天为大家带来的这篇文章，作者的观点是：Cursor 通过巧妙运用默克尔树数据结构，实现了对大型代码库的快速索引和高效增量更新，这正是其能够提供精准 AI 辅助编程服务的技术基础。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;作者 | Engineer's Codex&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;编译 | 岳扬&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Cursor ------ 这家最近宣布斩获 3 亿美元年营收的热门 AI 开发工具 ------ 正是利用默克尔树（Merkle trees）实现对代码的快速索引。本篇文章将为你详细介绍其运作原理。&lt;/p&gt; 
&lt;p&gt;在深入了解 Cursor 的具体实现方法之前，我们先来了解一下默克尔树的基本概念。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;01 默克尔树的简单解释&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;默克尔树（Merkle tree）是一种树状数据结构，其每个"叶"节点都标注了对应数据块的加密哈希值，而每个非叶节点则存储其子节点哈希值组合后的新哈希值。这种层级结构通过比较哈希值，能有效地侦测任何层级的数据变动。&lt;/p&gt; 
&lt;p&gt;通俗理解，它就像是数据的指纹系统：&lt;/p&gt; 
&lt;p&gt;1）每份数据（例如文件）都拥有自己独一无二的指纹（哈希值）&lt;/p&gt; 
&lt;p&gt;2）成对的指纹被组合在一起，生成一个新的指纹&lt;/p&gt; 
&lt;p&gt;3）此过程层层递进，直至形成唯一的主指纹（根哈希）&lt;/p&gt; 
&lt;p&gt;根哈希（root hash）概括了所有底层数据块的指纹信息，相当于对整个数据集做了一次加密公证。只要根哈希不变，就能证明原始数据分毫未改。此机制的精妙之处在于：&lt;strong&gt;任何一个数据块发生变化，都将牵一发而动全身 ------ 改变其上所有层级的指纹，最终彻底改变根哈希值。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-54b3916d0a8c97b04ff5e18eb75930572e1.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;02 Cursor 如何利用默克尔树实现代码库索引功能&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;默克尔树 (Merkle trees) 是 Cursor 代码库索引功能的核心组件。根据 Cursor 创始人发布的帖子[1]和 Cursor 的安全文档[2]，其工作流程如下：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-434fca0d5d62079cc39b5f46a8de5f6cdc9.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;2.1 步骤 1：代码分块与预处理&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;Cursor 首先在本地对代码库文件进行分块处理，将代码分割成具有语义含义的片段。此步骤是后续操作的必要前提。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;2.2 步骤 2：默克尔树的构建与同步&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;启用代码库索引功能后，Cursor 会扫描编辑器当前打开的文件夹，并为所有有效文件计算哈希值组成的默克尔树。随后，该默克尔树会与 Cursor 的服务器同步，其安全文档[2]详细描述了此过程。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;2.3 步骤 3：生成嵌入向量&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;将代码分块并发送至 Cursor 服务器后，将使用 OpenAI 的嵌入 API 或自研的嵌入模型（我未能验证 Cursor 具体采用的是哪种方法）生成嵌入向量 (embeddings)。这些向量表征能够捕捉代码片段的语义信息。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;2.4 步骤 4：存储与索引&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;生成的嵌入向量，连同起始/结束行号及文件路径等元数据，会被存储在一个远程向量数据库（Turbopuffer）中。为兼顾路径筛选功能与隐私保护，Cursor 会为每个向量附加经过混淆处理的相对文件路径。Cursor 创始人曾明确表示[1]："我们的数据库中不会存储任何代码。请求处理完毕立即销毁存储的代码数据。"&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;2.5 步骤 5：基于默克尔树的定期更新&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;每隔 10 分钟，Cursor 就会检测哈希值的变化情况，利用默克尔树精准定位哪些文件发生了变动。如 Cursor 的安全文档[2]所述，只需上传所定位到的发生变动的文件，从而大幅降低带宽消耗。&lt;strong&gt;默克尔树结构的最大价值正体现于此 ------ 它能实现高效的增量更新。&lt;/strong&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;03 代码分块策略&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;代码库索引的有效性很大程度上取决于代码的分块方式。尽管我先前的说明未深入探讨代码分块方法，但这篇关于构建类 Cursor 代码库功能的博客[3]揭示了一些技术细节：&lt;/p&gt; 
&lt;p&gt;简单的分块方式（按字符/按单词/按行）往往会遗漏语义边界 ------ 导致嵌入向量质量下降。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;尽管可根据固定的 token 数分割代码，但这种方式可能导致函数或类等代码块被强制截断。&lt;/li&gt; 
 &lt;li&gt;更有效的方案是使用能够理解代码结构的智能分割器，例如递归文本分割器（recursive text splitters），它使用高级分隔符（如类定义和函数声明）在恰当的语义边界处进行精准切分。&lt;/li&gt; 
 &lt;li&gt;一个更优雅的解决方案是根据代码的抽象语法树（AST）结构来分割代码。通过深度优先遍历 AST，将代码分割成符合 token 数量限制的子树结构。为避免产生过多的碎片化分块，系统会在满足 token 限制的前提下，将同级语法节点合并为更大的代码块。此类 AST 解析工作可借助 tree-sitter[4] 等工具实现，其支持绝大多数主流编程语言。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;&lt;strong&gt;04 嵌入向量在推理阶段的应用&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;在了解 Cursor 如何创建和存储代码嵌入向量后，一个自然而然的问题就出现了：这些嵌入向量在生成之后究竟是如何使用的？&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;4.1 语义搜索（Semantic Search）与上下文检索（Context Retrieval）&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;当你使用 Cursor 的 AI 功能（例如通过 &lt;a href="https://my.oschina.net/u/135318"&gt;@Codebase&lt;/a&gt; 或 ⌘ Enter 询问代码库相关问题时），将触发以下流程：&lt;/p&gt; 
&lt;p&gt;1）将查询转换为向量：Cursor 会为您的提问或当前代码上下文生成对应的嵌入向量。&lt;/p&gt; 
&lt;p&gt;2）向量相似性搜索：该查询向量被发送至 Turbopuffer（Cursor 的向量数据库），通过最近邻搜索找出与查询语义相似的代码块。&lt;/p&gt; 
&lt;p&gt;3）访问本地文件：Cursor 客户端接收到的检索结果包含经过混淆处理的文件路径和最相关代码块的行号范围。实际代码内容始终保留在用户本地设备，仅在需要时从本地读取。&lt;/p&gt; 
&lt;p&gt;4）上下文整合：客户端从用户本地文件读取这些相关代码块，并将其作为上下文与您的问题一并发送至服务器供大语言模型处理。&lt;/p&gt; 
&lt;p&gt;5）生成响应：此时大语言模型已获取代码库中的相关上下文，可据此提供精准回答或生成符合场景的代码补全。&lt;/p&gt; 
&lt;p&gt;这种由嵌入向量驱动的检索机制支持以下功能：&lt;/p&gt; 
&lt;p&gt;1）根据上下文生成代码：在编写新代码时，Cursor 可参考现有代码库中的相似实现，保持代码模式与代码风格的一致性。&lt;/p&gt; 
&lt;p&gt;2）代码库智能问答：可以获取基于代码库中实际代码的精准解答，而非通用回复。&lt;/p&gt; 
&lt;p&gt;3）智能代码补全：代码补全建议会融合项目的特定约定与特定模式。&lt;/p&gt; 
&lt;p&gt;4）智能重构辅助：重构代码时，系统可自动识别代码库中所有需要同步修改的关联代码段。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;05 Cursor 为何选择默克尔树&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;这些设计细节多与安全有关，具体可参阅 Cursor 的安全文档[2]。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;5.1 高效的增量更新&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;默克尔树使 Cursor 能精准定位自上次同步后变更的文件。因此，无需重新上传整个代码库，仅需上传修改过的特定文件。对于大型代码库来说这一点非常重要 ------ 重新索引所有文件会消耗过多的带宽和处理时间。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;5.2 数据完整性验证&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;默克尔树结构让 Cursor 能高效验证所索引的文件与服务器上存储的文件是否一致。分层的哈希结构可轻松检测传输过程中的数据异常或数据损坏。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;5.3 缓存优化&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;Cursor 将嵌入向量（embeddings）存储在以代码块（chunk）哈希值为索引的缓存中，使得重复索引相同代码库时速度大幅提升。这对多人协作开发同一代码库的团队尤为有利。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;5.4 隐私保护型索引&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;为保护文件路径中的敏感信息，Cursor 采用路径混淆技术：通过用 "/" 和 "." 为分隔符切割路径，并用存储在客户端的密钥加密每一段。虽然这样做会暴露部分目录结构，但能隐藏绝大多数敏感细节。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;5.5 集成 Git 版本历史&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;在 Git 仓库中启用代码库索引时，Cursor 还会索引 Git 的版本历史记录。它会存储 commit 的 SHA 值、父提交信息（parent information）及混淆后的文件名。为实现同 Git 仓库且同团队用户间的数据结构共享，用于混淆文件名的秘钥来自最近 commit 内容的哈希值。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;06 嵌入模型的选择与技术考量&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;嵌入模型的选择直接影响代码搜索与理解的质量。&lt;/strong&gt; 部分系统采用开源模型（如 all-MiniLM-L6-v2[5]），而 Cursor 可能使用 OpenAI 的嵌入模型或针对代码场景进行优化的定制模型。对于专用的代码嵌入模型，微软的 unixcoder-base[6] 或 Voyage AI 的 voyage-code-2[7] 等模型对代码的语义理解效果显著。&lt;/p&gt; 
&lt;p&gt;由于嵌入模型存在 token 容量限制，使得该技术的实现难度大幅增加。以 OpenAI 的 text-embedding-3-small[8] 为例，其 token 上限为 8192。有效的分块策略能在保留语义的前提下不超出该限制。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;07 握手同步流程&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;Cursor 默克尔树实现的核心在于同步时的握手机制。根据应用日志显示：在初始化代码库索引时，Cursor 会创建一个"merkle client"并与服务器进行"初始化握手流程"（详见 GitHub Issue #2209[9] 与 Issue #981[10]），该握手流程涉及向服务器发送本地计算的默克尔树的根哈希值。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;握手流程使服务器能精准判定需同步的代码范围。&lt;/strong&gt; 如握手日志所示（参照 GitHub Issue #2209[11]），Cursor 会计算代码库的初始哈希值，并将其发送至服务器进行验证。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;08 技术实现挑战&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;虽然默克尔树方案有许多优势，但其实现过程仍存在一些技术难点。&lt;/strong&gt; Cursor 的索引服务常因瞬时流量过载，导致大量请求失败。如安全文档所述[2]，用户可能观察到向 repo42.cursor.sh 发送的网络流量比预期要高 ------ 这正是由于文件需多次重传才能被完全索引。&lt;/p&gt; 
&lt;p&gt;另一项挑战与嵌入向量的安全性有关。学术研究表明，特定条件下存在逆向解析嵌入向量的可能性。虽然当前的攻击手段通常需同时满足：1) 拥有嵌入模型的访问权限 2) 仅对短文本有效。但若攻击者获取 Cursor 向量数据库的访问权限，仍存在从存储的嵌入向量中提取代码库信息的风险。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;END&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;本期互动内容 🍻&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;❓Cursor 通过路径混淆和本地哈希计算保护隐私，但同步时仍需上传部分数据。在团队协作场景下，你更倾向于完全本地化的方案，还是接受有限数据上传以换取更强的 AI 辅助？为什么？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;文中链接&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;[1]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fforum.cursor.com%2Ft%2Fcodebase-indexing%2F36" target="_blank"&gt;https://forum.cursor.com/t/codebase-indexing/36&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[2]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.cursor.com%2Fen%2Fsecurity" target="_blank"&gt;https://www.cursor.com/en/security&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[3]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.lancedb.com%2Frag-codebase-1%2F" target="_blank"&gt;https://blog.lancedb.com/rag-codebase-1/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[4]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftree-sitter.github.io%2Ftree-sitter%2F" target="_blank"&gt;https://tree-sitter.github.io/tree-sitter/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[5]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fsentence-transformers%2Fall-MiniLM-L6-v2" target="_blank"&gt;https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[6]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fmicrosoft%2Funixcoder-base" target="_blank"&gt;https://huggingface.co/microsoft/unixcoder-base&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[7]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.voyageai.com%2Fembeddings%2Fmodels%2F" target="_blank"&gt;https://docs.voyageai.com/embeddings/models/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[8]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fplatform.openai.com%2Fdocs%2Fguides%2Fembeddings" target="_blank"&gt;https://platform.openai.com/docs/guides/embeddings&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[9]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fgetcursor%2Fcursor%2Fissues%2F2209" target="_blank"&gt;https://github.com/getcursor/cursor/issues/2209&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[10]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fgetcursor%2Fcursor%2Fissues%2F981" target="_blank"&gt;https://github.com/getcursor/cursor/issues/981&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[11]&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fgetcursor%2Fcursor%2Fissues%2F2209" target="_blank"&gt;https://github.com/getcursor/cursor/issues/2209&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;本文经原作者授权，由 Baihai IDP 编译。如需转载译文，请联系获取授权。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;原文链接：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fread.engineerscodex.com%2Fp%2Fhow-cursor-indexes-codebases-fast" target="_blank"&gt;https://read.engineerscodex.com/p/how-cursor-indexes-codebases-fast&lt;/a&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/IDP/blog/18638294</link>
      <guid isPermaLink="false">https://my.oschina.net/IDP/blog/18638294</guid>
      <pubDate>Sat, 10 May 2025 08:01:00 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>龙芯发布新一代处理器，进军服务器和 AI 处理器市场​​​​​​​</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;「龙芯中科」在今天举办的 2025 龙芯产品发布暨用户大会上发布了基于国产自主指令集龙架构（LoongArchTM）研发的服务器处理器龙芯 3C6000 系列芯片、工控领域及移动终端处理器龙芯 2K3000/3B6000M 芯片，以及相关整机和解决方案。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0626/153553_ZF0H_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;据介绍，3C6000 系列服务器 CPU 采用自主指令系统龙架构，于 2024 年上半年流片成功。3C6000 单硅片 16 核 32 线程，可通过自研的龙链接口通过多硅片封装形成 32 核 64 线程的 3C6000/D（又称 3D6000）及 60/64 核 120/128 线程的 3C6000/Q（又称 3E6000）。&lt;/p&gt; 
&lt;p&gt;根据中国电子技术标准化研究院赛西实验室测试报告，单路 3C6000/S 服务器在 2.2GHz 运行 SPEC CPU 2017 单核单线程定/浮点分值为 5.56/6.93 分，多核定/浮点分值为 73.2/58.5 分；双路 3C6000/D 服务器在 2.1GHz 运行 SPEC CPU 2017 多核定/浮点分值为 284/261 分；双路 3C6000/Q 服务器在 2.1GHz 运行 SPEC CPU 2017 多核定/浮点分值为 450/283 分；四路 3C6000/D 服务器在 2.1GHz 运行 SPEC CPU 2017 多核定/浮点分值为 547/412 分。上述 3C6000/S、3C6000/D 实测单核/多核性能分别达到 Intel 公司 2021 年上市的 16 核至强 Silver 4314、32 核至强 Gold 6338 的水平，64 核 3C6000/Q 性能超过 40 核至强 Platinum 8380 的水平。&lt;/p&gt; 
&lt;p&gt;结合 Intel 公司第三代至强可扩展架构服务器芯片出货情况，3C6000 系列服务器 CPU 综合性能达到 2023 年市场主流产品水平。&lt;/p&gt; 
&lt;p&gt;2K3000/3B6000M 工控/终端 CPU 采用自主指令系统龙架构，面向工控和终端（笔记本、云终端等）应用，于 2024 年底流片成功。3B6000M 集成 8 个 LA364E 处理器核，主频 2.5GHz 时实测 SPEC CPU 2006 Base 单核定点分值达到 30 分；集成第二代自研 GPGPU 核心 LG200 和独立硬件编解码模块，4K 高清视频处理性能达到每秒 60 帧；集成安全处理器提供可信支持和密码服务，包括 SM2/3/4 硬件算法模块以及可供软件编程使用的可重构密码模块。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1800" src="https://static.oschina.net/uploads/space/2025/0626/153709_EMEW_2720166.png" width="2486" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;来源：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FU1_yQYdi-47nhn5ojBvSEA" target="_blank"&gt;https://mp.weixin.qq.com/s/U1_yQYdi-47nhn5ojBvSEA&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357386</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357386</guid>
      <pubDate>Sat, 10 May 2025 07:37:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>OceanBase 正式启用中文名：海扬数据库</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;国产数据库 OceanBase 正式启用中文品牌名「海扬数据库」，品牌战略全面升级。&lt;/p&gt; 
&lt;p&gt;官方解释称：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;「海」，既象征 OceanBase 对海量数据的承载能力，能够应对像支付宝每秒 42 万笔交易这样的高并发处理需求，体现其分布式架构在数据存储与处理上的强大优势，也象征着如海一样开源开放，以兼容幷蓄的姿态携手开发者、合作伙伴推动行业创新。&lt;/p&gt; 
 &lt;p&gt;「扬」，既寓意昂扬向上，象征 OceanBase 在技术海洋中不断突破边界，以根自研深耕行业，也寓意扬帆出海，不断走向全球化。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;OceanBase CEO 杨冰表示，中文名的推出，一方面代表着 OceanBase 深耕本土市场的决心，也是 OceanBase 继续引领世界舞台上分布式数据库技术创新和应用的宣言。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-8c286d418602fc6ed5232b2c2249cf0d73c.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;OceanBase 始创于 2010 年，是蚂蚁集团完全自主研发的国产数据库。2020 年 OceanBase 成立北京奥星贝斯科技有限公司并开始独立商业化运作。2021 年，OceanBase&amp;nbsp;&lt;a href="https://www.oschina.net/news/144034"&gt;正式开源&lt;/a&gt;(&lt;a href="https://gitee.com/oceanbase"&gt;https://gitee.com/oceanbase&lt;/a&gt;)，300 万行核心代码向社区开放。2024 年 3 月 19 日，蚂蚁集团宣布，旗下的蚂蚁国际、OceanBase 和蚂蚁数科已成立董事会，独立面向市场。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357376</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357376</guid>
      <pubDate>Sat, 10 May 2025 06:59:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Meta 成功挖角三名 OpenAI 研究人员</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;《华尔街日报》报道称，在争夺顶尖人工智能人才的斗争中，Meta 刚刚取得了胜利，尽管竞争对手 Sam Altman 公开嘲笑马克·扎克伯格的奢侈招聘策略，但 Meta 仍然挖走了三名 OpenAI 研究人员。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;Lucas Beyer、Alexander Kolesnikov 和 Xiaohua Zhai （OpenAI 苏黎世办事处的创始人）现已加入 Meta 的超级智能团队。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="356" src="https://oscimg.oschina.net/oscnet/up-0044b1bf5ef2f1f7a26cba89cd241222c75.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;此前，OpenAI 首席执行官 Altman 在与其兄弟 Jack 一起的播客透露，扎克伯格一直在提供超过 1 亿美元的薪酬方案，以吸引 OpenAI 的顶尖人才。并表示，「我很高兴，至少到目前为止，我们最好的员工中还没有人决定接受他的（那些提议）。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;《华尔街日报》随后报道称，扎克伯格一直在通过 WhatsApp 与数百名顶尖 AI 研究人员进行私人交流，通过他的「Recruiting Party」聊天室协调目标人才，然后在 Palo Alto 和 Lake Tahoe 的家中举办晚宴。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;这一策略的成果好坏参半。扎克伯格最近斥资 140 亿美元，签下了 Scale AI 的首席执行官 Alexandr Wang，这位 28 岁的年轻人也因此成为科技界有史以来身价最高的人才之一。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;相关阅读：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li style="text-align:start"&gt;&lt;a href="https://www.oschina.net/news/355944/sam-altman-meta-tried-100m-offers" target="_blank"&gt;Sam Altman：Meta 曾试图以 1 亿美元挖走 OpenAI 人才，但未能成功&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357367/metas-recruiting-three-openai-researchers</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357367/metas-recruiting-three-openai-researchers</guid>
      <pubDate>Sat, 10 May 2025 06:28:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Scale AI 被曝使用谷歌文档泄露客户机密信息</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;人工智能初创公司 Scale AI 陷入了一场严重的数据安全风波。这家估值不菲、并被 Meta 以 148 亿美元收购 49% 股份的公司，被曝出竟然使用公共的谷歌文档来存储包括 Meta、谷歌和 xAI 在内的众多客户的绝密信息。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;报道称，任何知道 Scale AI 文档链接的人，都可以轻易访问这些包含绝密项目、电子邮件地址和付款信息等敏感内容的谷歌文档。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;Scale AI 的一位发言人对此回应表示，公司正在进行彻底的调查，并且已经禁止任何用户公开分享 Scale AI 管理系统中的文档。然而，谷歌和 xAI 尚未对此事发表评论，而 Meta 则选择拒绝置评。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="250" src="https://oscimg.oschina.net/oscnet/up-a25cf5b5fc9703e4e067c5e37806e5bd265.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;据五名 Scale AI 的承包商透露，谷歌文档在 Scale AI 内部被广泛使用。网络安全专家指出，尽管目前尚无迹象表明这些公开文件已导致实际的数据泄露，但这种存储方式无疑让 Scale AI 极易受到黑客攻击。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;调查显示，在 Scale AI 采取措施之前，人们可以查看多达 85 份、数千页的项目信息，其中详细记录了 Scale AI 与大型科技客户之间的敏感合作。例如，这些文档揭示了谷歌如何利用 OpenAI 的 ChatGPT 来微调自己的聊天机器人。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;更令人震惊的是，至少有七份被标记为机密信息的谷歌项目手册向公众开放，其中包括如何改进其聊天机器人 Bard 的具体建议。此外，公开的谷歌文档中甚至包含了埃隆·马斯克 「木琴计划」（Project Xylophone）的详细内容，比如用于训练 xAI 人工智能模型的 700 个对话提示。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;承包商们还透露，尽管这些项目通常有秘密代号，但他们仍然能清晰辨别自己为哪个客户工作。在使用人工智能产品时，聊天机器人有时在被询问时甚至会直接透露客户信息。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;除了客户的机密项目信息，Scale AI 的谷歌文档中还赫然披露了该公司数千名员工的姓名和私人联络方式。更甚者，有些文件甚至详细列出了个体承包商的工资数额，包括有关工资纠纷和差异的详细说明。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;这些信息无疑充分暴露了 Scale AI 在数据安全性上的严重纰漏，并可能引发法律纠纷。值得注意的是，就在 Meta 入股 Scale AI 后不久，业内便有传言称包括谷歌在内的大客户已经与 Scale AI 进行了业务上的切割，以防止 Scale AI 向 Meta 透露敏感内容。此次谷歌文档事件，无疑将进一步加剧客户对 Scale AI 数据安全能力的担忧。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;span style="color:#000000"&gt;相关阅读：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.oschina.net/news/356119/openai-drops-scale-ai-meta" target="news"&gt;OpenAI 终止与 Scale AI 合作&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357357</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357357</guid>
      <pubDate>Sat, 10 May 2025 05:49:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>ResNet 主要发明人何恺明加入谷歌 DeepMind，担任「杰出科学家」</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;计算机视觉领域代表人物何恺明官宣加入谷歌 DeepMind，担任杰出科学家（Distinguished Scientist）。 他在个人主页上表示，自己在 DeepMind 的工作是兼职，还将继续保留 MIT 终身副教授的身份。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0626/110724_YwBV_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;何恺明是残差网络（ResNet）的主要发明人，而这项技术成为了深度学习及后续人工智能进步的基础。我们今天看到的 ChatGPT、AlphaGo、AlphaFold 都离不开它的影响。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0626/111204_Czoq_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;u&gt;&lt;em&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_cvpr_2016%2Fpapers%2FHe_Deep_Residual_Learning_CVPR_2016_paper.pdf" target="_blank"&gt;https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf&lt;/a&gt;&lt;/em&gt;&lt;/u&gt;&lt;/p&gt; 
&lt;p&gt;2003 年，何恺明以标准分 900 分获得广东省高考总分第一，被清华大学物理系基础科学班录取。在清华物理系基础科学班毕业后，他进入香港中文大学多媒体实验室攻读博士学位，师从汤晓鸥。何恺明曾于 2007 年进入微软亚洲研究院视觉计算组实习，实习导师为孙剑博士。&lt;/p&gt; 
&lt;p&gt;2011 年博士毕业后，何恺明加入微软亚洲研究院工作任研究员。2016 年，何恺明加入 Facebook 人工智能实验室，任研究科学家。2024 年，何恺明加入 MIT，成为该校一名副教授。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0626/111129_NW0H_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357334</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357334</guid>
      <pubDate>Sat, 10 May 2025 03:12:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
  </channel>
</rss>
