<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>开源中国-综合资讯</title>
        <link>https://www.oschina.net/news/industry</link>
        <atom:link href="http://8.134.148.166:30044/oschina/news/industry" rel="self" type="application/rss+xml"></atom:link>
        <description>开源中国-综合资讯 - Powered by RSSHub</description>
        <generator>RSSHub</generator>
        <webMaster>contact@rsshub.app (RSSHub)</webMaster>
        <language>en</language>
        <lastBuildDate>Wed, 19 Feb 2025 07:36:12 GMT</lastBuildDate>
        <ttl>5</ttl>
        <item>
            <title>百度 Q4 业绩会实录：DeepSeek 让我们明白要将最优秀的模型开源</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;百度昨天&lt;a href=&quot;https://www.oschina.net/news/334611&quot;&gt;发布&lt;/a&gt;了截至 12 月 31 日的 2024 年第四季度及全年财报。第四季度，营收为 341 亿元，同比下滑 2%。属于百度的净利润为 52 亿元。不按美国通用会计准则，归属于百度的净利润为 67 亿元。整个 2024 年，总营收为 1331 亿元，同比下滑 1%。归属于百度的净利润为 238 亿元。不按美国通用会计准则，归属于百度的净利润为 270 亿元。&lt;/p&gt; 
&lt;p&gt;财报发布后，百度董事长兼 CEO 李彦宏，移动生态事业群总裁罗戎，智能云事业群总裁沈抖，代理 CFO 何俊杰等高管出席随后召开的财报电话会议，解读财报要点并回答分析师提问。&lt;/p&gt; 
&lt;p&gt;以下是分析是问答环节主要内容：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;花旗银行分析师 Alicia Yap&lt;/strong&gt;：DeepSeek 最近备受关注，百度也宣布即将开源文心大模型 4.5 系列，并且将免费提供使用。请问管理层，此举背后的战略考量是什么？另外，我们如何看待当前基础模型领域的竞争格局？百度计划如何在这个不断演变的市场中保持领先地位？&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;李彦宏&lt;/strong&gt;：生成式人工智能和基础模型市场仍处于初期阶段，但发展速度极快，DeepSeek 的成功案例肯定会加快基础模型的应用速度。随着基础模型变得更容易获取且成本降低，我们正进入一个真正的变革阶段，我们会看到新的人工智能应用和使用案例在数量上呈爆发式增长，这将为所有人带来巨大的机遇，并拓展人工智能的边界，增加更多可能性。&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;从 DeepSeek 我们学到一点，那就是将最为优秀的模型开源供所有人使用，将可以极大地推动其应用，因为大家出于好奇自然会想去尝试开源模型，进而推动其更广泛的应用。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;文心大模型 4.5 早期版本将是我们有史以来最出色的模型，我们希望用户和客户试用起来能比以往更容易，更轻松。我们决定将 4.5 早期系列进行开源，也源于对自身技术领先地位的坚定信心，这种信心源自我们数十年在研发方面的持续投入、不断的技术创新，以及我们作为全球为数不多具备全栈人工智能能力公司之一的独特地位。&lt;/p&gt; 
&lt;p&gt;文心一言已经展现出强大的市场吸引力，日 API 调用量在短短一年内从 5000 万激增至 16.5 亿。通过开源，我们相信更多开发者和用户将认识到文心一言的真正价值，推动其更广泛应用，并扩大其在更多场景中的影响力。&lt;/p&gt; 
&lt;p&gt;同样，将文心一言机器人免费提供使用，能让更多用户在同等条件下将我们的基础模型与其他模型进行比较，尤其是其他模型收费而我们不收费的情况。我们上次对文心大模型进行重大升级是在 2023 年 10 月，距今已有很长时间，所以，未来几个月大家敬请期待文心大模型的 4.5 版本。&lt;/p&gt; 
&lt;p&gt;话说回来，我也想强调一个关键的重点。无论开源还是闭源，基础模型只有在能够大规模有效解决现实世界问题时才真正有价值，我们致力于以应用为导向，持续迭代文心大模型。秉持这种理念，自推出以来，我们一边利用文心大模型升级内部产品，一边服务企业客户。&lt;/p&gt; 
&lt;p&gt;借助文心大模型成功改造了百度面向消费者的产品，如百度搜索和百度文库；此外，通过千帆平台，我们正在提升企业客户的模型和应用开发体验。文心一言在指令遵循、先进的检索增强生成技术（该技术将幻觉问题降至最低）等方面的行业领先能力，使其在各种场景中得到广泛应用。千帆平台的综合工具链让我们的客户能够针对自身应用场景定制任何模型。展望未来，我们将瞄准性能提升和成本削减的潜力，加快文心大模型的迭代，继续在对现实世界影响潜力最大的领域对其进行开发。我们对人工智能发展的新篇章感到兴奋，期待看到人工智能技术带来更多具有开创性且对社会有持久价值的应用。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;高盛分析师 Lincoln Kong&lt;/strong&gt;：我有一个关于公司搜索业务的问题。在经历了最近几个季度的搜索改版后，我们应该如何看待未来的调整或变化？目前生成式人工智能结果在搜索结果中的占比是多少，我们的目标占比又是多少？另外，随着人工智能整合的不断深入，鉴于像 Deepseek 和豆包这样的人工智能聊天机器人也具备搜索功能，能否分享更多关于用户指标方面的信息？您如何看待百度搜索与这些人工智能聊天机器人之间的竞争态势？&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;罗戎&lt;/strong&gt;：目前，约 22% 的搜索结果页面包含人工智能生成的内容，但幕后还有更多的工作在进行，我们正在从根本上对搜索服务进行了变革，使其比以往任何时候都更强大、更高效。就像李彦宏在事先准备好的发言中提到的，借助文心一言，我们将搜索范围从文本和链接扩展到提供多样化的内容形式，包括短视频、直播、智能助理笔记和商品展示。这些不同的形式可以动态组合，创造出个性化的搜索体验。在持续改革搜索产品的过程中，我们也在开发能实现更深度个性化的功能，以适应每位用户的习惯和偏好。&lt;/p&gt; 
&lt;p&gt;我们的努力带来了更好的用户使用效果，包括在每月使用百度进行搜索的活跃用户中，83% 的用户会与生成式人工智能进行内容互动。更令我们倍受鼓舞的是，12 月百度应用上每位用户的搜索查询量同比增长了 2%。我们专注于不断提升用户体验，考虑到用户参与度日益积极，我们也会扩大人工智能的作用。&lt;/p&gt; 
&lt;p&gt;在此基础上，我可以同大家分享一个文心一言智能助理如何在刚刚过去的春节假期为我们的广告客户创造价值的案例。春节期间，许多中小企业放假一周，但某些行业的客户需求却达到高峰。我们的文心一言智能助理有效地弥补了这一缺口。例如，一家助听器公司因为家庭团聚，子女关心年迈父母的健康需求，咨询量有所增加，但该公司的客服团队因放假不在岗。通过文心一言智能助理，该公司有效地识别并筛选出高度相关的销售线索，使其即便在假期人员减少的情况下，也能高效跟进并无缝服务客户。&lt;/p&gt; 
&lt;p&gt;关于你提到的人工智能聊天机器人与搜索的问题，我们认为，由基础模型驱动的人工智能革命仍处于非常早期的阶段。无论是像 Deepseek 这样原生的人工智能工具，还是我们基于人工智能的产品，这些努力都代表了探索人工智能潜力的不同方式。作为拥有数亿用户的中国搜索市场领导者，我们通过采用最优秀的创新成果并融入真正创新的人工智能功能，保持灵活性和全面的市场视野。百度将继续引领人工智能变革，为我们庞大的用户群体提升搜索体验。&lt;/p&gt; 
&lt;p&gt;此外，搜索本质上深深扎根于语言和文本理解，这与大语言模型的能力完美契合，使我们能够在人工智能赋能的搜索变革中占据领先地位。我们相信，搜索正在演变成一个集成平台，它超越了人工智能驱动的探索阶段，不仅能提供智能答案，还能引导用户完成整个流程，从寻找答案、进行深入分析、完成任务，最终提供全面的服务和解决方案。&lt;/p&gt; 
&lt;p&gt;虽然人工智能驱动的探索是人工智能应用开发的一个重要但仍处于早期发展的阶段，目前尚未出现具有决定性影响力的应用程序，而对我们来说，关键是要保持这种快速且坚定的发展态势。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;摩根大通分析师 Alex Yao&lt;/strong&gt;：能否请管理层谈谈 2025 年第一季度和 2025 年全年核心广告业务的增长前景？支撑这些预测的宏观假设是什么？我们看到业务在 2024 年第四季度触底并开始复苏。最后一个问题是，生成式人工智能搜索的潜在盈利机会有哪些，预计何时能实现盈利？&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;罗戎&lt;/strong&gt;：过去几个月，我们看到各类支持政策相继出台，比如货币宽松政策、财政政策以及贸易政策。我们相信这些举措最终会推动经济增长，但它们需要时间才能产生效果。鉴于百度的广告业务与中小企业高度相关，而中小企业对宏观经济状况尤为敏感，再加上竞争环境持续严峻，尽管短期内可能面临压力，我们还会继续利用基础模型对搜索进行变革，这些工作正在稳步推进。正如我们在事先准备的发言稿中提到的，相信这种人工智能变革将持续提升用户体验，并创造新的可能性。&lt;/p&gt; 
&lt;p&gt;本季度，我们进一步深化了搜索在人工智能方向的变革，用户指标也出现了令人鼓舞的改善。我们认为这将推动收入增长，并在长期内开启新的盈利机会。此外，我们尚未大规模实现人工智能生成搜索结果的盈利，目前这类结果约占总查询量的 22%，而一旦我们的人工智能驱动搜索功能得到充分优化，我们将凭借更高质量的用户产品推进盈利进程。基于这些因素，我们看到了未来增长的机会，判断我们的广告业务正在触底。我们预计业务未来将逐步改善，今年上半年的表现会好于第四季度，下半年相比上半年会有进一步提升。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;摩根士丹利分析师 Gary Yu&lt;/strong&gt;：我的问题是关于人工智能云服务的。鉴于公司人工智能云业务增长强劲，我们能否期待该业务能够在 2025 年继续保持良好的发展态势？在收入和盈利能力方面，其主要驱动因素是什么？另外，管理层能否分享一下对 2025 年人工智能云市场的展望？我们应如何看待企业云服务需求？人工智能又给这一领域带来了哪些新增机会？&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;沈抖&lt;/strong&gt;：正如李彦宏前面提到的，我们人工智能云业务的收入在 2024 年第四季度同比增长加速至 26% ，推动 2024 年全年收入增长 17%。值得注意的是，2024 年与生成式人工智能相关的收入同比增长近两倍，这一增长得益于对文心一言和人工智能基础设施的需求不断上升，以及市场对百度在技术方面的领导力高度认可。因此，我们成功吸引了多样化的客户群体，并建立了强大的潜在业务渠道。&lt;/p&gt; 
&lt;p&gt;在人工智能基础设施方面，我们已与多个行业建立合作关系，涵盖互联网、汽车、智能设备、制造业、能源、金融、公用事业以及众多人工智能生成内容初创企业。我们的客户群体持续健康增长，在大中型客户中均取得显著进展，这表明我们在中国云市场的份额正在不断扩大。&lt;/p&gt; 
&lt;p&gt;通过千帆大模型平台，我们为市场提供具有行业领先性价比的全面基础模型。我们推出了从旗舰版到轻量版的文心大模型系列，旨在满足各种不同的需求。除了文心大模型，我们还提供广泛的优质第三方基础模型，包括 DeepSeek V3 和 R1 模型。得益于百度的全栈人工智能能力和端到端优化，我们能够确保平台上托管的任何模型都具备最佳性能和稳定性，同时保持极具竞争力的价格。此外，我们还提供一整套用于微调模型和构建原生人工智能应用程序的工具，客户能够轻松制定满足自身特定需求的解决方案。&lt;/p&gt; 
&lt;p&gt;关于人工智能云市场展望的问题，我们认为未来将迅速增长。一方面，在最近的春节假期期间，大语言模型成为了广泛讨论的话题，这不仅进一步提高了公众对基础模型的认知度，还促使更多人深入思考如何利用这些模型来提升自身业务。另一方面，我们相信基础模型的性能将不断提升，而成本会稳步下降，这无疑将进一步降低使用这些模型的门槛。因此，我们认为更多企业会将基础模型整合到从研发到生产的业务运营各个环节，从而推动 API 调用量的显著增长。&lt;/p&gt; 
&lt;p&gt;我们也预计百度人工智能云服务的支出将会增加，因为过去我们已经观察到一个明显的趋势：通过千帆大模型平台的 API 调用而使用基础模型的客户，也倾向于增加在我们人工智能云服务上的支出。在利润方面，由于我们专注于符合战略重点的高价值机会，推动了稳健的可持续增长，我们 2024 年第四季度非通用会计准则下的运营利润率持续同比扩大。展望 2025 年，我们有信心人工智能云业务收入增长将保持强劲势头，同时继续产生正的运营利润。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;瑞银证券分析师 Wei Xiong&lt;/strong&gt;：我想问一下利润率趋势的问题。鉴于核心广告业务近期面临的压力以及云业务收入占比的不断增加，我们应该如何看待 2025 年第一季度和全年的核心利润率水平？在运营效率方面还有进一步优化的空间吗？另外，公司全年有哪些投资计划？&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;何俊杰&lt;/strong&gt;：尽管面临短期压力，但对于人工智能领域的战略投资将产生更为可持续的影响方面，我们仍然非常乐观。我们始终专注于提升业务运营各方面的韧性，并且在各项业务中都看到了令人鼓舞的进展。&lt;/p&gt; 
&lt;p&gt;首先，对于我们的在线营销业务，预计受到人工智能生成搜索结果和技术变现计划的推动，以及百度在捕捉增长机遇方面的准备和市场宏观环境的改善，我们的广告收入将逐步提升。其次，我们的人工智能云业务已经展现出强劲的增长，随着市场份额的不断扩大，以及我们在自主研发独特全层级人工智能架构和全栈人工智能能力方面的竞争优势，利润率得到持续改善，因此，我们有信心在未来保持这一强劲势头。第三，对于我们的智能驾驶业务，尤其是萝卜快跑，我们将继续致力于通过提高运营效率和改善单位经济效益来缩小亏损，我们也在探索创新的运营模式，包括轻资产模式。&lt;/p&gt; 
&lt;p&gt;关于你问到 2025 年的投资与优化情况，我们将保持对高增长机遇的最优资源配置，同时与我们的长期战略保持一致。我们的投资将继续聚焦于那些既拥有巨大未来机遇，又具备强大投资回报率潜力的项目，包括进一步提升盈利能⼒、深化搜索业务的人工智能转型、增强我们的人工智能云产品，以及拓展我们的自动驾驶项目。在推进这些项目的过程中，我们将继续致力于提高运营效率，促进各业务集团之间的协同效应，以最大限度地发挥这些战略投资的影响力。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;美银美林分析师 Miranda Zhuang&lt;/strong&gt;：我有一个关于萝卜快跑业务的问题。管理层能否介绍一下 2025 年该业务的进展情况，包括车队规模目标、能够贡献哪些独特的经济效益，以及业务的价值主张是什么？管理层认为行业目前处于什么阶段？我们是否正在接近一个转折点？鉴于国内自动驾驶出租车市场上的竞争对手正在与汽车制造商和打车平台合作，采用生态系统战略，管理层对于接下来该行业的竞争态势有何看法？请问百度的竞争和规模化策略是什么？&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;李彦宏&lt;/strong&gt;：我们在自动驾驶技术领域已经投入了十多年，通过萝卜快跑，我们将宏伟愿景变为了现实，确立了公司在自动驾驶技术领域的全球领先地位。中国的自动驾驶市场环境是非常具有挑战性的，由于中国人口众多、路况多样、交通场景动态变化以及城市布局繁复，其交通状况是非常复杂的，而在中国乘坐自动驾驶出租车的价格约为美国的七分之一。因此，我们在中国的成功运营展示了百度卓越的技术和运营能力，包括我们所设计的全新第六代量产无人车 RT6，是世界上有史以来最具成本效益的自动驾驶出租车。凭借我们的这些优势，这一商业模式得到成功验证，并为进一步规模化发展和全球扩张奠定了坚实基础。&lt;/p&gt; 
&lt;p&gt;在第四季度，萝卜快跑在全国范围内提供了约 110 万次出行服务，同比增长 36%。到 1 月份，向公众提供的累计出行服务次数已超过 900 万次。此外，正如我之前提到的，我们在中国实现了 100% 完全无人驾驶运营，这意味着车辆上不再配备安全员，这是一个新的行业标杆，巩固了我们在该领域的领先地位。正如我之前提到的，去年 11 月，自动驾驶业务获得批准在香港开始开放道路测试，这是非常重要的一步，因为香港是我们首个右舵驾驶和靠左行驶的交通市场。这表明我们有能力使自动驾驶技术适应不同的交通系统，为拓展到其他具有类似驾驶设置的市场打开了大门。&lt;/p&gt; 
&lt;p&gt;今年对我们的业务拓展至关重要，随着业务的推进，预计车队规模和出行服务量的增长速度将超过以往任何时候。同时，我们也在积极寻找合作机会，我们已经确定了各种潜在合作伙伴，包括出行服务提供商、当地出租车公司、第三方车队运营商以及其他潜在合作伙伴，这种轻资产模式将使我们能够高效扩大规模，同时保持灵活性。通过与不同类型的合作伙伴合作，我们旨在加强市场地位，让更多人体验到自动驾驶服务。从更广阔的市场来看，正如我在上次财报电话会议中提到的，由于市场仍处于起步阶段，竞争实际上有助于加速市场发展，并营造更有利于创新的监管环境。在这个不断增长的市场中，我们的使命始终明确，那就是为更多用户提供更安全、便捷和舒适的出行体验。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;杰富瑞分析师 Thomas Chong&lt;/strong&gt;：展望 2025 年，百度业务投资的战略重点会是什么？具体而言，资源将如何在搜索、自动驾驶、云服务和基础模型之间分配？此外，管理层对 2025 年全年的资本支出以及股东回报有何展望？&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;何俊杰&lt;/strong&gt;：在评估 2025 年的资本配置选项时，我们的方法基于以下战略考量。我们将继续把提升人工智能能力作为长期战略重点进行投资。在此基础上，我们将进一步深化全产品线的人工智能转型，尤其是搜索业务。在人工智能云业务方面，我们旨在推动企业客户采用我们的人工智能基础设施、飞桨（PaddlePaddle）深度学习平台以及文心一言，满足他们对人工智能产品和解决方案以及人工智能云服务日益增长的需求。在智能驾驶方面，我们专注于扩大国内业务规模，探索创新运营模式，并拓展国际业务。&lt;/p&gt; 
&lt;p&gt;在确保严格的投资回报率管理和有效控制资本支出损失的同时，有两个关键优先事项驱动我们的决策，那就是强化我们的技术领先地位，以及加速人工智能产品和人工智能云服务在不同行业的应用。至于你问到的股东回报，自 2024 年初以来，我们的股票回购金额已超过 10 亿美元，这显著高于 2023 年全年的回购总额。对于截至 2025 年 12 月，规模为 50 亿美元的股票回购计划，我们总计已回购 17 亿美元。展望未来，作为持续回报股东长期信任的举措之一，我们计划加快股票回购计划的推进。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334727</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334727</guid>
            <pubDate>Wed, 19 Feb 2025 07:17:09 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>宇树科技申请春晚机器人图形商标</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;天眼查资料显示，2 月 5 日，杭州宇树科技有限公司申请注册一枚「春晚机器人」样式图形商标，国际分类为厨房洁具，当前商标状态为等待实质审查。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;img height=&quot;269&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-bc4fda9d3052fe48a476a7f163206880819.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;杭州宇树科技有限公司成立于 2016 年 8 月，法定代表人为王兴兴，注册资本约 259 万人民币，并已于 2024 年完成了 C 轮，交易金额数亿人民币。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;经营范围包括智能机器人的研发、智能机器人销售、工业机器人制造、工业机器人销售等，由王兴兴、汉海信息技术（上海）有限公司、宁波红杉科盛股权投资合伙企业（有限合伙）等共同持股。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;目前，宇树科技有四足机器狗和通用人形机器人两大系列产品。在创业早期，宇树科技以四足机器狗起家，第一款产品为 XDog。随后，Laikago、AlienGo、A1、Go1、B1 等一系列机器狗产品相继研发问世，推出市场。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334724</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334724</guid>
            <pubDate>Wed, 19 Feb 2025 07:15:09 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>Omdia：2029 年电信 IT 人工智能软件市场达 50 亿美元</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;研究机构 Omdia 最新发布了一份《电信 IT 人工智能市场预测》报告，展示了 Omdia 对于在跟踪的电信 IT 软件的五个主要产品类别中 AI 所占价值的估算。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Omdia 观点：&lt;/span&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;我们估计，2023 年，电信 IT 软件中 AI 的价值为 18 亿美元，而且我们预测，到 2024 年底，其价值会达到 23 亿美元。从 2024 年至 2029 年，整体价值会以 17% 的复合年增长率（CAGR）增长，达到 50 亿美元。&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img height=&quot;233&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-175f0c5940f60ad73ea3821a680982bf8f1.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;预测性 AI 将占整个预测期中大部分价值，于 2029 年达到总价值的 76%。在预测期期间，预测性 AI 将以 13% 的 CAGR 增长。然而，生成式 AI 会增长得更快，CAGR 为 37%，到 2029 年占电信 IT 软件中 AI 价值的 24%。&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;本预测的前几年将由自动化和相关用例驱动，相比于预计到预测期末会更普遍的推荐和预测用例，这些用例相对价值更低。消费者互动和网络管理将占大部分的增长，因为 AI 已经在这些细分市场获得关注，也正在推动运营效率提升，CSP 可以在此基础上再接再厉。在没有 AI 益处的情况下，分析已经能够完成很多任务，所以其增长会稍慢一点。目前 AI 对于创收和服务管理的作用更加受限，并且我们预测，在预测期期间这些细分市场的增长最低。&lt;/span&gt;&lt;/p&gt; 
&lt;/blockquote&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334718</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334718</guid>
            <pubDate>Wed, 19 Feb 2025 06:54:09 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>2024 年中国在开源人工智能模型领域的崛起和变革</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;最近，开源中国 OSCHINA、Gitee 与 Gitee AI&lt;a href=&quot;https://www.oschina.net/news/330623/china-open-source-2024-annual-report&quot;&gt;联合发布了《2024 中国开源开发者报告》&lt;/a&gt;。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;报告地址：&lt;a href=&quot;https://talk.gitee.com/report/china-open-source-2024-annual-report.pdf?fr=news0120&quot;&gt;2024 中国开源开发者报告.pdf&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;报告聚焦 AI 大模型领域，对过去一年的技术演进动态、技术趋势、以及开源开发者生态数据进行多方位的总结和梳理。&lt;/p&gt; 
&lt;p&gt;在第二章《TOP 101-2024 大模型观点》中，Hugging Face 工程师 &lt;strong&gt;Tiezhen&lt;/strong&gt;、Hugging Face 中文社区项目经理 &lt;strong&gt;Adina &lt;/strong&gt;以及 Hugging Face Fellow &lt;strong&gt;Lu Cheng&lt;/strong&gt;，从崛起与变革两个维度，探讨中国开源模型在 2024 年取得的重大成就和未来展望：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;2024 年中国在开源人工智能模型领域从 「追随者」 到 「引领者」 转变，体现技术实力且反映人工智能生态系统快速完善。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2024 年中国学术界和产业界推进自主研发，在技术创新和模型能力上飞跃，多款自主研发模型在国内外评测表现卓越。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Qwen 系列因多尺寸选项、多语言支持及友好授权功能获高度评价；DeepSeek 引入 MLA 技术实现性能成本突破；智谱 CogVideoX 系列成全球首批开源文生视频模型之一。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;中国开源模型从质疑中崛起获广泛认可，其成功得益于政府支持与行业巨额投入，中国人工智能生态体系迅速完善，未来可能在全球占更核心地位。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;随着开源模型影响力提高，中国开源社区活跃度提升，企业、研究机构、个体开发者积极参与，如 Qwen 系列被广泛集成促进交流协作，智源研究院等机构建立协作机制贡献基础工作和资源。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2024 年中国开源社区涌现高质量自发研究成果，如 MAP 团队的 Map Neo、InstantX 团队的 InstantID，为中国模型赢得国际认可。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;中国在推动人工智能技术发展同时建立完善透明治理机制，如《人工智能示范法 2.0（专家建议稿）》《生成式人工智能服务管理暂行办法》，为开源模型发展提供稳定政策环境并确保技术应用符合社会价值导向。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;端上模型兴起，中国 AI 社区推出多款移动友好型模型，如 Qwen2 - 1.5B 等，虽有挑战但代表 AI 技术隐私保护和成本优化未来方向。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;中国开源社区在逻辑推理领域推出创新项目，如 Macro - o1、QwQ 等，通过开源策略分享研究细节，推动小模型推理能力提升与行业技术进步。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;中国开源模型发展从 「百模大战」 迈向多元化和深度细分，发布大量高质量开源模型，涵盖多模态理解与生成等多个领域，模型竞争转向应用场景细化。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0219/135245_kWl7_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;完整全文：&lt;a href=&quot;https://my.oschina.net/u/3859945/blog/17503717&quot; target=&quot;_blank&quot;&gt;https://my.oschina.net/u/3859945/blog/17503717&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334705</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334705</guid>
            <pubDate>Sat, 08 Feb 2025 05:54:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>Meta 将在 4 月底举办首届 AI 开发者大会 LlamaCon</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;Meta&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.llama.com%2Fevents%2Fllamacon%2Fsignup%2F&quot; target=&quot;_blank&quot;&gt;宣布&lt;/a&gt;将在今年 4 月 29 日举行首届 LlamaCon——专门面向生成式人工智能的开发者大会。大会的名字源于 Meta 的开源 AI 模型 Llama 系列。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-26ace593ef0174eac7b8b3e6bcbe581ade6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Meta&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fai.meta.com%2Fblog%2Ffuture-of-ai-built-with-llama%2F&quot; target=&quot;_blank&quot;&gt;表示&lt;/a&gt;，随着开源 Llama 模型和工具集合的空前增长和发展势头强劲，公司决定于 4 月 29 日举行首届专门面向 AI 领域的开发者大会 LlamaCon。他们将&lt;span style=&quot;background-color:rgba(255, 255, 255, 0.65); color:#151631&quot;&gt;在大会上分享开源 AI 发展的最新动态，来帮助开发者构建「令人惊叹的应用和产品」。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;background-color:rgba(255, 255, 255, 0.65); color:#151631&quot;&gt;在接下来的几周里，Meta 也将公布更多与 LlamaCon 有关的信息。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;在 1 月底的财报说明会上，扎克伯格曾表示，Llama 4 的目标是引领（市场），具备原生的多模态能力。他当时也暗示 Llama 4 最早也要到今年二季度才会发布，所以 4 月 29 日会是一个较为合适的日期。&lt;/p&gt; 
&lt;p&gt;留出两个月的时间，也给 Meta 公司带来了不确定性。目前 OpenAI 已经确认会在近期发布 GPT-4.5、Anthropic 也被爆料将在「数周内」发布混合 AI 模型 Claude 4。&lt;/p&gt; 
&lt;p&gt;最令 Meta 忌惮的是，DeepSeek 是否会在未来两个月里搞出更多的「大新闻」。&lt;/p&gt; 
&lt;p&gt;据报道，在 DeepSeek 发布 R1 模型之后，Meta 迅速组建了四个「战情室」，核心忧虑是 DeepSeek 的最新大模型可能会比 Llama AI 的下一代版本更强。&lt;/p&gt; 
&lt;p&gt;知情员工透露，四个「战情室」中有两个负责研究 DeepSeek 如何降低训练和运行 AI 模型的成本，并将心得用于训练 Llama。另外两个团队，负责尝试找出 DeepSeek 用于训练的数据，以及如何将中国 AI 的先进训练方法用于重构 Meta 自己的产品。&lt;/p&gt; 
&lt;p&gt;在分析师电话会议上，扎克伯格曾表示：「他们（DeepSeek）做了一些新颖的事情，我认为我们仍在消化中。他们取得的一些进展，我们希望在我们的系统中应用，这就是开源世界运作的本质。」&lt;/p&gt; 
&lt;p&gt;与此同时，扎克伯格也已经宣布，今年将在人工智能相关的项目上投资 600-650 亿美元，包括建设一个超大型数据中心和更多的人才招聘。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334689/meta-llamacon-2025</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334689/meta-llamacon-2025</guid>
            <pubDate>Sat, 08 Feb 2025 04:06:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>OpenAI 前 CTO 官宣新创业 AI 公司，团队成员多来自 OpenAI</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;OpenAI 前 CTO Mira Murati 今天凌晨&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Fmiramurati%2Fstatus%2F1891918876029616494&quot; target=&quot;_blank&quot;&gt;宣布&lt;/a&gt;创立了新的 AI 公司 Thinking Machines Lab（思维机器实验室）。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-d07fc6d37ca4e1534888a3ca6098802c5d2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fthinkingmachines.ai%2F&quot; target=&quot;_blank&quot;&gt;官网写道&lt;/a&gt;，公司将专注于构建人工智能（AI）模型和产品，以支持更多、跨工作领域的「人类-AI 协作」，「虽然当前的系统擅长编程和数学，但我们正在构建能够适应人类所有专业知识并实现更广泛应用的人工智能。」 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;他们还强调这会是一家重视研究开放的公司，&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Fthinkymachines%2Fstatus%2F1891919141151572094&quot; target=&quot;_blank&quot;&gt;其推文中承诺&lt;/a&gt;&lt;/u&gt;：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;我们致力于通过论文发表和代码发布来开放科学，同时会重点关注应用于不同领域的人机协作。我们的方法包括共同设计研究和产品，以便从实际部署和快速迭代中学习。这项工作需要三个核心基础：&lt;strong&gt;SOTA 的模型智能、高质量的基础设施和先进的多模态能力&lt;/strong&gt;。我们致力于构建处于能力领先的模型来兑现这一承诺。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;该公司官方网站对这三核心基础进行了展开说明：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;模型智能是基石。除了强调人机协作和定制之外，模型智能也至关重要，我们正为科学和编程等领域构建前沿能力模型。最终，最先进的模型将解锁最具变革性的应用和优势，例如实现新颖的科学发现和工程突破。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;基础设施质量是重中之重。研究生产力至关重要，在很大程度上取决于基础设施的可靠性、效率和易用性。我们的目标是长期正确地构建事物，以最大限度地提高生产力和安全性，而不是走捷径。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;先进的多模态能力。我们认为多模态对于实现更自然、更高效的通信、保存更多信息、更好地捕捉意图以及支持与现实环境的更深入集成至关重要。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;官网贴出了公司 29 人团队名单，其中超过 20 人有在 OpenAI 供职的经验。其中较为知名的有 OpenAI 联合创始人约翰·舒尔曼（John Schulman），他正担任新公司的首席科学家。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334684</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334684</guid>
            <pubDate>Sat, 08 Feb 2025 03:35:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>没有所谓的 1875 纪元，美国 150 多岁老人领社保福利不是 COBOL 语言的锅</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;近期，一位美国政府官员曾宣称：「我们这里有些人看起来都已经 150 岁了」，并指出这些人正在领取社会保障福利。由此，有人开始流传这样一种说法：社会保障局（SSA）在存储日期时使用了一个 1875 年的纪元，把那些未知出生年份的记录存为 0，从而默认显示为 1875 年。&lt;/p&gt; 
&lt;p&gt;这种观点的起源可以追溯到某个帖子，帖子中有人调侃道：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;「看起来埃隆那群天才程序员根本就不懂 COBOL 的工作原理。社会保障系统正是运行在 COBOL 上，而 COBOL 并没有专门的日期或时间类型。于是日期就以数字形式存储，按照 ISO 8601 标准计算，纪元定在了 150 年前（1875 年）——也就是米制标准的开始。结果如果不知道某个日期，就会存储成 0，而在 COBOL 中这就会默认解析为 1875 年，也就是 150 年前。」&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img height=&quot;1668&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0219/112214_BPqK_3820517.png&quot; width=&quot;1198&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;然而，笔者对此并不认同，主要基于以下几点理由：&lt;/p&gt; 
&lt;h2&gt;数据库中存在 1875 年前的出生年份&lt;/h2&gt; 
&lt;p&gt;2007 年，社会保障局曾发布过一份数据集，该数据集包含了在 2007 年 1 月之前发放的社会保障号码持有者的收入记录（约占全部数据的 1%）。在这份数据集中，他们明确说明：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;移除了出生年份早于 1870 年的 5,935 条记录&lt;/li&gt; 
 &lt;li&gt;移除了出生年份等于 2007 的 1,096 条记录&lt;/li&gt; 
 &lt;li&gt;以及少数缺失出生年份的记录&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;这表明，SSA 的数据库中确实保存了 1875 年前（甚至 1869 年及更早）的出生年份数据，并非将未知年份一律默认为 1875。&lt;/p&gt; 
&lt;h2&gt;数据中没有 1875 年出生人数激增的异常&lt;/h2&gt; 
&lt;p&gt;如果系统将所有未知出生年份的记录默认转换为 1875 年，那么在统计数据中，1875 年的出生人数应该会异常增多。但实际上，从公开数据来看，并不存在这样一个「高峰」。（注：该数据集只是 1% 的样本，若存在默认值问题，趋势应当更加明显。）&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;698&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0219/112303_XwHt_3820517.png&quot; width=&quot;1174&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;社会保障局并未使用 ISO 8601 标准存储日期&lt;/h2&gt; 
&lt;p&gt;负责跟踪社会保障福利支付的主记录（Master Beneficiary Record, MBR）建立于 1962 年，这远早于 ISO 8601 标准于 1988 年的发布。即便是其前身 ISO 2016 标准也在 1976 年发布，并且并没有任何依据指向 1875 年。实际上，有研究论文基于 SSA 数据指出，SSA 对生日等信息的存储采用的是固定宽度格式，而不是 ISO 8601 标准的日期字符串格式。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;The data abstracted from the MBR consisted of a 26-character record for each deceased individual. The four data items on each record were… the month and year of death&lt;/span&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ISO 8601 标准本身并不涉及纪元概念&lt;/h2&gt; 
&lt;p&gt;ISO 8601 仅仅是一种用于表示日期和时间的字符串格式，其本质并不是基于数字计算时间流逝，因此根本无需设定一个「纪元」。虽然 ISO 8601:2004 版曾固定引用 1875 年 5 月 20 日——即《米制公约》在巴黎签署的那一天——作为参考日期，但这一引用在 ISO 8601-1:2019 版中已被移除。换句话说，这个日期仅用于定义格里高利历，并非作为一个时间计数的起点。&lt;/p&gt; 
&lt;h2&gt;没有任何证据显示 1875 年被用作时间计算的起点&lt;/h2&gt; 
&lt;p&gt;经过查找，笔者没有发现任何系统或标准会将 1875 年作为时间纪元。尤其在 COBOL 语言中，也没有这样的约定或实践。所有迹象都表明，所谓的「1875 纪元」只是个误解。&lt;/p&gt; 
&lt;p&gt;总的来说，从 SSA 的数据实践、存储方式以及国际标准的角度来看，都没有任何证据支持「1875 纪元」这一说法。该观点看似有趣，但实际上缺乏坚实的依据。&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;原文：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fiter.ca%2Fpost%2F1875-epoch%2F&quot; target=&quot;_blank&quot;&gt;https://iter.ca/post/1875-epoch&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334682/1875-epoch-cobol-150-american</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334682/1875-epoch-cobol-150-american</guid>
            <pubDate>Sat, 08 Feb 2025 03:26:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>没有处理过遗留项目，别自称资深工程师</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;大家都不喜欢维护遗留项目，我也不例外。命运总爱跟人开玩笑，最近一个遗留项目正好落到了我手上。虽然在这个项目上工作的经历并没有减少我对遗留系统的厌恶，反而让我对当下所采用的流程与实践有了更深刻的认识。&lt;/p&gt; 
&lt;p&gt;我为自己所在的团队感到自豪，因为我们遵循了许多业界最佳实践：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;编写简洁且易维护的代码，并配以自动化测试&lt;/li&gt; 
 &lt;li&gt;积极参与代码合并请求和任务评审&lt;/li&gt; 
 &lt;li&gt;合并到主分支后，当天就能将应用推向生产环境&lt;/li&gt; 
 &lt;li&gt;高度采用敏捷开发模式&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;当然，一切并非尽善尽美。合并请求中偶尔会出现一些无关痛痒的建议和讨论；运维团队有时也会搞砸一些事情（至少在我们开发人员看来是这样）；而产品负责人也时不时催促我们加快推出某些「简单」的新功能……总的来说，情况还算不错。&lt;/p&gt; 
&lt;h2&gt;穿越回 Ant 时代&lt;/h2&gt; 
&lt;p&gt;由于团队表现出色，公司决定将我们的开发效率借调给另一个由其他部门负责的产品。令我们略感失望的是，这个项目不仅使用的是较老版本的 Java，其代码风格也与我们的习惯大相径庭。&lt;/p&gt; 
&lt;p&gt;任务要求我们添加几个简单的监控指标，比如应用是否正常运行、运行时长、数据处理是否足够迅速等。由于项目正处于维护模式，已经有段时间没有添加新功能了。按理说，添加这些指标对我们来说应该是小菜一碟。&lt;/p&gt; 
&lt;p&gt;然而，当我们卷起袖子开始工作时，首先发现这个项目竟然使用了一种非常古老的构建方式——Ant 构建文件。那是一种庞大的 XML 文件，详细描述了如何构建整个项目：从编译、测试到打包，每个环节都必须显式配置，包括源码路径、目标路径以及资源位置。过去，这种做法在许多编程语言中都很常见：写好一个构建文件，复制到每个新项目中，再不断调整直至适应新项目需求。&lt;/p&gt; 
&lt;p&gt;例如，一个简单的「Hello World」项目的 Ant 构建文件可能如下所示：&lt;/p&gt; 
&lt;pre&gt;&amp;lt;project&amp;gt;

&amp;nbsp; &amp;nbsp; &amp;lt;target name=&quot;clean&quot;&amp;gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;lt;delete dir=&quot;build&quot;/&amp;gt;
&amp;nbsp; &amp;nbsp; &amp;lt;/target&amp;gt;

&amp;nbsp; &amp;nbsp; &amp;lt;target name=&quot;compile&quot;&amp;gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;lt;mkdir dir=&quot;build/classes&quot;/&amp;gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;lt;javac srcdir=&quot;src&quot; destdir=&quot;build/classes&quot;/&amp;gt;
&amp;nbsp; &amp;nbsp; &amp;lt;/target&amp;gt;

&amp;nbsp; &amp;nbsp; &amp;lt;target name=&quot;jar&quot;&amp;gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;lt;mkdir dir=&quot;build/jar&quot;/&amp;gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;lt;jar destfile=&quot;build/jar/HelloWorld.jar&quot; basedir=&quot;build/classes&quot;&amp;gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;lt;manifest&amp;gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;lt;attribute name=&quot;Main-Class&quot; value=&quot;oata.HelloWorld&quot;/&amp;gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;lt;/manifest&amp;gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;lt;/jar&amp;gt;
&amp;nbsp; &amp;nbsp; &amp;lt;/target&amp;gt;

&amp;nbsp; &amp;nbsp; &amp;lt;target name=&quot;run&quot;&amp;gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;lt;java jar=&quot;build/jar/HelloWorld.jar&quot; fork=&quot;true&quot;/&amp;gt;
&amp;nbsp; &amp;nbsp; &amp;lt;/target&amp;gt;

&amp;lt;/project&amp;gt;
&lt;/pre&gt; 
&lt;p&gt;难道就没有更便捷的方式吗？正因如此，「约定优于配置」的理念应运而生。这一理念主张：开发者只需关心那些偏离约定的特殊情况，而现代构建工具正是基于这一思想，通过提供可覆盖的默认配置，免去了重复配置的麻烦。正因为如此，大多数 Java 源码统一存放在 &lt;code&gt;src/main/java&lt;/code&gt; 目录下，而编译后的文件则放在 &lt;code&gt;target&lt;/code&gt; 目录中，避免了繁琐的重复设置。&lt;/p&gt; 
&lt;p&gt;这一发现启发了我们：或许同样的原则也能应用到我们当前项目中的应用配置上。面对一个庞大且大部分数值雷同（如应用端口）的配置文件，采用默认值机制无疑能让配置文件变得更精简。&lt;/p&gt; 
&lt;h2&gt;被我们视为理所当然的事情&lt;/h2&gt; 
&lt;p&gt;回到遗留项目，我们顺利构建并打包了应用！那枯燥的部分终于过去，可以安心开始编码了。但问题随之而来：如何将我们负责监控的指标组件嵌入到这套老旧的代码库中？在我们的常规开发框架中，这一切通常是自动处理的，因此我们一度认为这毫不费事。&lt;/p&gt; 
&lt;p&gt;但实际上，将指标组件「注入」到遗留代码的各个角落，最佳方案是什么呢？初看起来，单例模式似乎是最简单的选择；不过，开发社区普遍认为单例是一种反模式。为什么呢？毕竟，我们钟爱的某某框架不也依赖单例吗？如果不是，那它到底采用了什么机制？依赖注入究竟是什么？其底层又是如何运作的？&lt;/p&gt; 
&lt;p&gt;这一连串问题促使我们重新审视那些一直视为理所当然的基本概念。虽然在这种情况下使用单例并非最糟，因为代码大部分缺乏单元测试，但要让我们心安理得，代码必须经得起推敲。经过尝试，我们最终采用了一种不同的方案，写出了既简洁又清晰的代码——既没有依赖单例，也未引入多余的抽象层。&lt;/p&gt; 
&lt;h2&gt;开发者角色的局限性&lt;/h2&gt; 
&lt;p&gt;项目的最后一步是部署，只有部署成功后才能进行测试。但问题来了——这一次，我们既不负责部署，也不负责测试。部署工作由运维团队完成，而测试则交由专门的测试团队。为什么开发者就不能全程掌控，从开发到上线，而要先提交工单，再等待其他团队的配合呢？&lt;/p&gt; 
&lt;p&gt;首先，由于代码测试覆盖率不足，手动测试不可避免；其次，公司的基础设施也不允许我们自行部署应用。&lt;/p&gt; 
&lt;p&gt;这一系列经历使我们开始思考职责分离的原因，以及现行模式为何更为合理。事实证明，这个项目的任务交付时间和迭代周期远远超过平时（通常几天就能交付的工作，此次竟拖延了数周），这无疑验证了分工合作的必要性。&lt;/p&gt; 
&lt;h2&gt;通过旧实践理解现代方法&lt;/h2&gt; 
&lt;p&gt;到了月底，我们的监控指标终于在生产环境中顺利运行。虽然我对遗留项目的看法依旧——我仍然讨厌它们，也不奢望你会因此改变看法——但这段经历给了我们宝贵的启示。&lt;/p&gt; 
&lt;p&gt;我们无法选择所分配到的项目，但我们可以调整对待遗留系统的态度。与其心存无奈，不如把它当作一个提问、学习与成长的机会。通过了解过去的做法及其局限，我们不仅掌握了当下的最佳实践，更获得了背后历史的宝贵经验。&lt;/p&gt; 
&lt;p&gt;一旦你积累了这种深厚的知识，其他开发者自然会认可并信赖你的专业能力。如果你希望成为这样的人，就得勇于钻研那些看似繁琐的遗留项目。&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;原文：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.infobip.com%2Fdevelopers%2Fblog%2Fseniors-working-on-a-legacy-project&quot; target=&quot;_blank&quot;&gt;https://www.infobip.com/developers/blog/seniors-working-on-a-legacy-project&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;作者：Alen Kosanovic&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334678/seniors-working-on-a-legacy-project</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334678/seniors-working-on-a-legacy-project</guid>
            <pubDate>Sat, 08 Feb 2025 03:09:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>Grok 3 是否意味着大力出奇迹的大模型法则仍然成立？</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;blockquote&gt; 
 &lt;p&gt;本文转载自：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F24609799526&quot; target=&quot;_blank&quot;&gt;https://zhuanlan.zhihu.com/p/24609799526&lt;/a&gt;&lt;br&gt; 作者：张俊林（中科院软件所，博士）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;媒体风向变化太快，让人目不暇接。早上还在夸 Deepseek 成本低，性价比高，预训练 Scaling Law 死了，不需要太多机器和 GPU 卡，性价比优先，英伟达休矣；中午 Grok 3 一出来，说是用了 10 万张英伟达 H100 卡，效果力压 OpenAIo3 mini 和 Deepseek R1，就转向说 Scaling law 还成立，还需要大量的卡，英伟达股价有救了，还是要大力出奇迹……&lt;/p&gt; 
&lt;p&gt;这两个观点明显对立，有一真必有一假，那事实的真相到底是啥呢？我们来推一推。&lt;/p&gt; 
&lt;h2&gt;一、预训练阶段的 Scaling Law 是否仍然成立&lt;/h2&gt; 
&lt;p&gt;-预训练阶段的 Scaling Law 成立吗？当然是成立的，所谓「Scaling Law 撞墙」，大家普遍遇到的问题是数据不够了，没有大量新数据，导致预训练阶段的 Scaling Law 走势趋缓，注意是趋缓但不是停顿，预训练阶段的 Scaling Law 并没到天花板。按照 Chinchilla Scaling Law 推断，即使没有新数据，也并不意味着模型效果提不上去了，很简单，只要增加基座模型尺寸，效果仍然会提高，只是从付出的算力和获得的效果提升来说很不合算，性价比过低，这是为何大家转到 RL Scaling Law 和 Test Time Scaling Law 的原因，是因为付出同样的算力，在后面两个阶段大模型智商提升更明显，就是性价比高。&lt;/p&gt; 
&lt;p&gt;-&lt;strong&gt;目前可以提高模型效果的 Scaling 方法，按照性价比由高到低排序的话: Test time Scaling Law&amp;gt; RL Scaling Law&amp;gt;预训练阶段 Scaling Law(数据不够了，只能推大模型尺寸)&lt;/strong&gt;，有性价比高的 Scaling，当然优先做这种，性价比低的 Scaling，只有在没有性价比更高的情况下才会采用。这跟购物一个道理，有性价比高的当然不会去买性价比低的商品。&lt;/p&gt; 
&lt;p&gt;-&lt;strong&gt;如果哪天 RL Scaling Law 和 Test Time Scaling Law 到了天花板，又没有找到新的性价比更合算的 Scaling law，也不是说模型效果就提不上去了，大家仍然可以回归预训练阶段的 Scaling Law，没有新数据也没关系，推大模型尺寸规模就可以，效果仍然会上升&lt;/strong&gt;。但这基本是最后的选择，没办法的办法，只要有性价比高的方法就不会走这条路。&lt;/p&gt; 
&lt;p&gt;-有人问了：那按照你的意思，囤那么多 GPU 算力，其实对训最好的模型也没啥用？要是按照上面的理论，那确实是没有太大必要，比如 Deepseek 2000 卡也可以作出最好的模型不是。但是卡多有个好处，就是能压缩实验新想法和训练大模型基座的时间周期。比如你总得探索一些不同的算法、参数或数据配比的模型进行各种实验，你有 10 个新想法，如果只有 2000 张卡，可能得跑 5 天才能得出结论，要是有几万张卡，可能 1 天就能得出结论，所以卡多对于探索效率是有极大帮助的。卡多创新多，这点肯定成立。&lt;/p&gt; 
&lt;h2&gt;二、Grok 3 基座模型（对标 Deepseek V3，非 R1 这种逻辑推理模型）&lt;/h2&gt; 
&lt;p&gt;-为何 Grok 3 作为通用基座模型，它的评测指标只有数学、科学和代码数据集？没有通用能力比如最常用的 MMLU 指标的对比，这是不太规范的对比模式。推断可能 Grok 3 的通用能力相对 OpenAI 和 Deepseek 的模型没有大幅提升，所以不拿出来比？&lt;/p&gt; 
&lt;p&gt;-&lt;strong&gt;如果想要提升基座模型的数学、科学和代码能力，无论从方法还是从成本角度来讲，难度并不大，目前比较标准的做法是类似 Deepseek V3 从 Deepseek R1 蒸馏数学、代码等逻辑题的长 COT 数据，即深度思考过程数据，就是说把深度思考长 COT 数据引入基座的 Post-Training 阶段、甚至前置到预训练阶段（所谓大模型「左脚（Deepseek 基座）踩右脚（Deepseek R1）自我飞升」的模式），这样就能大幅提升基座模型在数学和代码方面相关的能力，也就是 Grok3 宣传具备的「有思维链推理和自我纠错机制」，评测指标看着会比较好看，而且蒸馏的数据总量也不会太大（几百 B 级别应该够了），成本很低，对算力要求不高。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;-OpenAI 很快会发布的非逻辑推理模型 GPT 4.5，大概也应是类似的思路，从 o3 模型蒸馏 COT 数据，用深度思考数据来提升 GPT 4.5 基座模型的智商，大模型「左脚踩右脚自我飞升」大法，这会是之后基座模型提升能力的主要手段。&lt;/p&gt; 
&lt;p&gt;-Grok 3 的算力消耗是 Grok 2 的 10 倍，如果遵照 Chinchilla Scaling Law，最佳做法是 Grok 3 的训练数据量比 Grok 2 增加 3 倍，模型大小同时比 Grok 2 增加 3 倍（但是目前的趋势是减小模型大小，增大数据量[就是说「小模型大数据」的模式]，尽管这样不满足训练最优原则，但因为模型尺寸小了，所以这种模型更适合在线推理服务，降低服务成本）。&lt;/p&gt; 
&lt;p&gt;-如果像发布会宣称的，Grok 3 耗费算力是 Grok 2 的 10 倍消息为真的话，那有两种可能。一种是数据量增长极大，这样只能是增加了大量多模态数据，比如数据量从 10T 增长到 30T（目前文本模型使用的数据量，最多到 18T 到 20T 之间，基本到顶，再多没有了，要大幅增加只能加多模态数据，但是增加多模态数据对提升大模型智商帮助不大，所以这个增量按理说不应该太大），如果这样推算，Grok3 的模型规模增长 3 倍左右；第二种可能是训练数据量比 20T 增加的不多，如果这样可以推出 Grok3 模型尺寸比 Grok 2 要大很多，至少 4 到 5 倍起步（若新增数据不多，那只能靠增加模型尺寸来消耗新增算力）。不论是哪种可能，Grok 3 的模型大小肯定比 Grok 2 大了很多，而 Grok 2 模型本身可能就不小（Grok 2 发布网页评测效果超过 Llama 3.1405B，所以无论数据还是模型大小，都不会太小，要是 Dense 模型， 70B 是最小的估计了），&lt;strong&gt;所以 Grok 3 的尺寸规模很可能不是一般的大&lt;/strong&gt;（感觉在 200B 到 500B 之间）。&lt;/p&gt; 
&lt;p&gt;-很明显，Grok 3 仍然在采取推大基座模型尺寸的「传统」做法，也就是上面「Scaling Law」部分分析的预训练阶段增大模型尺寸的方法来提升基座模型能力，上面分析过，这种做法是性价比很低的。比较时髦的做法是把训练重心放在 RL Scaling 方面，性价比会高太多。但是为啥他要做这种赔本买卖呢？在后面会给出一个可能的解释。&lt;/p&gt; 
&lt;h2&gt;三、Grok 3 逻辑推理版本 (深度思考版本，对标 Deepseek R1)&lt;/h2&gt; 
&lt;p&gt;-Grok 3 的深度思考版本，不说体验，单从评测指标看，达到或者超过了 o3 mini，确实是目前效果最好的，或者说最好的之一没有什么问题。&lt;/p&gt; 
&lt;p&gt;-说回上面提到的问题，为啥明知靠推大预训练阶段模型尺寸规模性价比低，Grok 3 还要用这种模式呢？很可能内在的原因在于（推断无证据）：&lt;strong&gt;Post-Training 阶段采取 RL Scaling，其效果可能跟基座模型的大小是有正相关关系的，就是说，同样的 RL 阶段的算力消耗，如果基座模型尺寸更大，则 RL 阶段的 Scaling 效果越好。&lt;/strong&gt;只有这样，才有在预训练阶段尽量把模型规模推大的必要性。而我们可以假设，Grok 3 之所以采取这种过于耗费算力，看着性价比不高的方式，是希望通过加大基座，把深度思考版本的能力明显提起来。&lt;/p&gt; 
&lt;p&gt;-貌似 Deepseek R1 效果很好又开源，获得一片好评，但大家想要实际用起来，会发现基座太大，部署难度和消耗资源太高，对下游应用不太友好。那为啥 Deepseek 非得推这种对下游应用来说明显过大的模型呢？（小点的蒸馏模型看着指标很好，但是实际应用效果貌似差不少），是否也是因为基座模型如果不够大，深度思考模型效果就没那么好的原因？&lt;/p&gt; 
&lt;p&gt;-如果上述假设成立，那意味着：&lt;strong&gt;三个 Scaling Law(Pre-train、RL 、Test Time)，从提高大模型智商的性价比来说，由高到低是：Test Time &amp;gt; RL &amp;gt; Pre-Train，这个是之前的结论。但如果上述假设成立，说明 Test Time Scaling 的天花板最低，它的天花板依赖于 RL 阶段的 Scaling 能力，而 RL 阶段 Scaling 天花板次低，它的天花板依赖于预训练阶段 Pre-Train 的 Scaling？&lt;/strong&gt;如果这样，如果有一天当 RL 和 Test Time 天花板到顶，意味着我们可以再启动一轮，去推大基座模型的模型尺寸，RL 阶段 Scaling 的天花板随之升高，然后可以再去 Scale RL 和 Test Time，就进一步得到智商更高的大模型。如果这成立，那意味着 AGI 的解决方案已经完整了？其实不需要新的 Scaling Law 存在就够？&lt;/p&gt; 
&lt;p&gt;-上述推论，是在一个前提成立的条件下的推出来的，这个前提是：Grok 3 耗费这么大算力推大模型规模，这是个深思熟虑或小规模实验的结果，而不是仅仅受到之前老观念（预训练阶段算力越高效果越好）影响下的决策。&lt;/p&gt; 
&lt;p&gt;如果这个前提不成立，则上述推论不成立。总之，一切责任在马斯克，Over。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334674</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334674</guid>
            <pubDate>Sat, 08 Feb 2025 03:02:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>DeepSeek 提出新的注意力机制：原生稀疏注意力 (NSA)，创始人亲自提交论文</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;2 月 18 日，DeepSeek 官方发文公布了一篇新的论文，&lt;strong&gt;论文提出了一种新的注意力机制「NSA」&lt;/strong&gt;。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-adf24ce9b3e5ac8760a1ec13688871f61ab.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
 &lt;p&gt;论文地址：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2502.11089v1&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/pdf/2502.11089v1&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;据 DeepSeek 介绍，&lt;strong&gt;&lt;span&gt;「原生稀疏注意力 (Native Sparse Attention, NSA) 」&lt;/span&gt;&lt;/strong&gt;是一个用于超快长上下文训练和推断的本地可训练的稀疏注意力机制，并且还具有与硬件对齐的特点。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;论文摘要：&lt;/p&gt; 
 &lt;p&gt;长文本建模对下一代语言模型来说至关重要，但标准注意力机制的高计算成本带来了显著的计算挑战。稀疏注意力为提高效率同时保持模型能力提供了一个有前景的方向。我们提出了 NSA（原生稀疏注意力），这是一个将算法创新与硬件对齐优化相结合的、原生可训练的稀疏注意力机制，用于实现高效的长文本建模。&lt;/p&gt; 
 &lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-d43b8ad2a0cb2e2f280f11b7d2d96a63fba.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;NSA 核心组件包括：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;动态分层稀疏策略&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;粗粒度 token 压缩&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;细粒度 token 选择&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;研究通过对现实世界语言语料库的综合实验来评估 NSA。其中作者评估了 NSA 在通用语言评估、长上下文评估和链式推理评估中的表现。实验结果表明，NSA 实现了与 Full Attention 基线相当或更优的性能，同时优于现有的稀疏注意力方法。&lt;/p&gt; 
&lt;p&gt;此外，与 Full Attention 相比，NSA 在解码、前向和后向阶段提供了明显的加速，且加速比随着序列长度的增加而增加。这些结果验证了分层稀疏注意力设计有效地平衡了模型能力和计算效率。&lt;/p&gt; 
&lt;p&gt;另外，有网友发现，arXiv 上 NSA 这篇论文的提交记录显示，它于 2 月 16 日提交，提交者正是梁文锋本人，他也是这篇论文的合著者。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-521d0ee94e504b1caa033cbf984b6fde938.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334671/deepseek-nsa</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334671/deepseek-nsa</guid>
            <pubDate>Sat, 08 Feb 2025 02:46:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>Linus Torvalds 将不顾维护者反对合并 Rust 内核代码</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;在 &lt;a href=&quot;https://www.oschina.net/news/334317/marcan-resigning-as-asahi-linux-project-lead&quot;&gt;Asahi Linux 创始人宣布辞去项目负责人职务&lt;/a&gt;之后，围绕 Linux 内核中 Rust 代码的争议还在继续。 &lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;DMA 映射助手及内核其他多个领域的维护者 Christoph Hellwig 一直对 Linux 内核中的 Rust 代码及其长期可维护性持批评态度，他在最新发布的一封邮件列表帖子中指出， Linus Torvalds 私下提到将推翻维护者对内核中 Rust 代码的否决权。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;409&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-84071f688bda1ac854a0ee922963f204016.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;考虑到最近几天的讨论，我决定发布此页面，其中包含我们的理解：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Frust-for-linux.com%2Frust-kernel-policy&quot; target=&quot;_blank&quot;&gt;https://rust-for-linux.com/rust-kernel-policy&lt;/a&gt;……&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Linus 私下表示，他绝对会不顾维护者的反对合并 Rust 代码。因此，从现在开始，作为 Linux 开发者或维护者，无论你是否愿意，都必须处理 Rust。&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;这里的 Rust 代码不仅仅是指 Rust 代码——这些绑定看起来一点也不像地道的 Rust 代码，它们是一种完全不同的存在，试图弥合巨大的语义鸿沟。而且它们在某些地方并没有做到这一点，因为它们现在被塞进了每个小子系统和库中。&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;因此，这些绑定会像癌症一样蔓延到各处，并迅速从一个允许并追求全局改进的软件项目，转向日益增加的隔离化。这将使 Linux 变成一个用多种语言编写的项目，而没有明确的指南说明在何处使用何种语言。即使在绑定之外，由于内核数据结构（如无处不在的链表）的侵入性和自引用特性，许多代码也不会是非常地道的 Rust。我们是否既对不起那些试图将现有代码库带入更安全空间的人，也对不起那些用 Rust 进行系统编程的人？&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;我曾经在类似的代码库上工作过，它们是我最糟糕的噩梦，因为由于原因 X，不断有部分代码从语言 A 重写为语言 B，然后又由于原因 Z 重写回去。而这还没有算上 Linux 维护者之间常见的‘创造性’内斗。&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;我想了解这个 Rust ‘实验’的目标是什么：如果我们想解决现有的内存安全问题，我们需要针对现有代码进行修复，并找到改进的方法。最近在这方面做了很多工作，我们还需要更多。但这也表明，核心维护者对诸如检查整数溢出或编译器强制同步（如 clang hread sanitizer)）等琐碎事情感到厌烦。我们如何弥合内核中一部分甚至不接受相对简单的安全改进规则，而另一部分却强制执行更严格规则之间的差距？&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;如果我们只是想使编写驱动程序更容易，那么引入一种新语言只会增加更多工作，并加重已经超负荷工作的核心基础设施维护者的负担。&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;因此，我认为这份政策文件没有太大用处。目前的规则是，Linus 可以强迫你做任何他想要的事情，我认为他需要非常清楚地阐明这一点，包括对贡献者的期望。&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;就我个人而言，我可以很好地处理 Rust 本身，我很乐意将内核带入一个更安全的内存世界，但处理一个不受控制的多语言代码库肯定会让我把业余时间花在其他事情上。我听到其他一些人嘀咕类似的话，但并不是每个人都像我这样直言不讳。&lt;/span&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;详情可&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flore.kernel.org%2Frust-for-linux%2FZ7SwcnUzjZYfuJ4-%40infradead.org%2F&quot; target=&quot;_blank&quot;&gt;查看邮件列表&lt;/a&gt;。&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334666/linus-torvalds-rust-code</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334666/linus-torvalds-rust-code</guid>
            <pubDate>Sat, 08 Feb 2025 02:34:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>中软国际被曝不协商直接降薪，有人直降 35%，引发员工抗议</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fweibo.com%2F7884923627%2FPeQTeBEot&quot; target=&quot;_blank&quot;&gt;有网友爆料&lt;/a&gt;，华为外包大厂中软国际员工在公司楼下聚集维权，抗议公司未提前协商突然降薪操作，疑似降薪幅度 10%-35%，甚至还有传出 0 元工资的极端情况，引发员工强烈不满。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0219/102639_R1Xq_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0219/102923_eWur_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;据悉，目前 IT 中心产品部确认降薪，其他受影响部门暂不清楚。有员工反映，公司在降薪的同时还在大力拓展新业务，高层薪资福利却丝毫未减，这让员工心里很不平衡。&lt;/p&gt; 
&lt;p&gt;据透露，中软国际提供了三种选择：一是按照薪资比例折合 13 个月+的薪资，多出来的月份按照绩效年终发放；第二种方案为直接降薪 18%；或者选择在 3 月底主动离职。三种方案任选其一，且没有书面形式，只是口头通知。更让员工难以接受的是，此次降薪并未明确说明原因，且强制执行。&lt;/p&gt; 
&lt;p&gt;面对员工的质疑，截至目前，该外包大厂派了一个所谓的办事员前来收集员工的诉求，并承诺将与高层协商解决。「他们只是来听我们说话，却没有给出任何实质性的答复。」一位参与沟通的员工表示，「感觉就像是走过场，根本没有解决问题的诚意。」&lt;/p&gt; 
&lt;p&gt;值得注意的是，据员工爆料，中软国际的邮箱让员工可以收到邮件，但是员工发送邮件出去，发件箱和发件记录是空的，「这是怕员工留痕特意设置的。」该员工表示。更有网友爆料，甲方开价不低，到手的钱却层层缩水，之前就有裁员 2.2 万人的先例，现在又不协商直接降薪。&lt;/p&gt; 
&lt;p&gt;资料显示，中国软件外包市场规模已突破 4454 亿元，年增速超 10%，而该公司作为在岸外包龙头，客户包括了如华为等互联网大厂。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334665</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334665</guid>
            <pubDate>Sat, 08 Feb 2025 02:29:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>李彦宏：文心大模型 4.5 系列将开源，是最强大的文心大模</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;在百度 2024 年 Q4 及全年财报电话会上，百度创始人、董事长兼首席执行官李彦宏透露，文心大模型 4.5 将开源，4.5 将是百度有史以来最强大的大模型，「希望客户和用户能比之前更方便地体验这款模型」。&lt;/p&gt; 
&lt;p&gt;他表示，开源 4.5 系列的决策源自于对技术领先地位的坚定信心，开源将进一步促进文心大模型的广泛应用，并在更多场景中扩大其影响力，「但我想强调的是，无论开源闭源，基础模型只有在大规模解决现实问题时，才具备真实价值」。未来，百度将加速推动文心大模型的性能升级与成本降低。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;400&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-3f76b9accbca6162153bf21f08d99a59ecc.jpg&quot; width=&quot;300&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;此外，李彦宏还在业绩会上表示，2025 年是萝卜快跑重要的扩张之年。他透露，百度将寻求与移动服务运营商、出租车公司及第三方车队运营方等合作，以加速业务扩展。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334659</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334659</guid>
            <pubDate>Sat, 08 Feb 2025 02:05:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>天天 AI-20250218</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;h3&gt;DeepSeek 加持，北大通院几何模型达 IMO 金牌水平！32 个 CPU 核心和 1 块 4090 就能实现满血解题&lt;/h3&gt; 
&lt;p&gt;北大通院的 TongGeometry 模型在 DeepSeek 的加持下，达到了国际数学奥林匹克竞赛（IMO）金牌水平。该模型不仅能够解决 IMO-AG-30 数据集中的所有 30 题，还在 IMO-AG-50 数据集上解决了 42 题，超越了人类金牌选手的平均水平。TongGeometry 通过使用归纳数据库方法（DD）、构造对称图形、利用策略网络和价值网络联合搜索等技术，实现了高效解题。此外，该模型还具备出题能力，其生成的题目已被权威数学竞赛收录。值得注意的是，TongGeometry 仅需 32 个 CPU 核心和 1 块 4090 显卡即可实现满血解题，展现了极高的效率。&lt;br&gt; &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Feswmr-dcYx_oeYrKJwX0qQ&quot; target=&quot;_blank&quot;&gt;来源&lt;/a&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Fdaily-ai-20250218%2F&quot; target=&quot;_blank&quot;&gt;&amp;nbsp;原文&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;6 个 DeepSeek 指令，让 AI 成为你的超级助手！掌握这些，效率翻倍！&lt;/h3&gt; 
&lt;p&gt;在信息爆炸的时代，高效处理复杂信息、全面分析问题、精准预测趋势已成为现代人必备的技能。而 DeepSeek，作为一款强大的 AI 工具，凭借其独特的指令功能，能够帮助你轻松应对这些挑战。今天，我们就来揭秘 DeepSeek 的 6 个神奇指令，让你在内容传播和推广中如虎添翼！&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FIBow-N4IFbwNKEQGmuWdNQ&quot; target=&quot;_blank&quot;&gt;来源&lt;/a&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Fdaily-ai-20250218%2F&quot; target=&quot;_blank&quot;&gt;&amp;nbsp;原文&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;大决战！OpenAI 可能发布 GPT-4.5，狙击马斯克 Gork3&lt;/h3&gt; 
&lt;p&gt;OpenAI 首席执行官 Sam Altman 透露，GPT-4.5 已进入测试阶段，可能在近期发布。这一消息正值马斯克宣布发布「地球最聪明的 AI」——Gork3 之际，引发了行业的广泛关注。GPT-4.5 被认为是对抗 Gork3 的有力武器，OpenAI 团队甚至计划在 Gork3 发布后决定是否推出 GPT-4.5。这一竞争不仅展示了 AI 领域的激烈竞争，也预示着未来 AI 技术的快速发展。&lt;br&gt; &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FX33kBcSq3ieMSfPx-gnW_Q&quot; target=&quot;_blank&quot;&gt;来源&lt;/a&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Fdaily-ai-20250218%2F&quot; target=&quot;_blank&quot;&gt;&amp;nbsp;原文&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;10W+爆款 AI 流水线：Coze 深度写作×DeepSeek 算法洞察×HTML 极速排版&lt;/h3&gt; 
&lt;p&gt;熬夜赶稿、追热点到头秃、爆文全靠运气……这些是不是你创作路上的「噩梦」？别急，这里有个秘密武器，能让你轻松告别这些烦恼，甚至让爆款内容像印钞机一样源源不断！想知道是什么「黑科技」吗？往下看，解锁 15 分钟生成爆款的神奇公式！&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FSA9KUAscPemgXMnwDssHyw&quot; target=&quot;_blank&quot;&gt;来源&lt;/a&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Fdaily-ai-20250218%2F&quot; target=&quot;_blank&quot;&gt;&amp;nbsp;原文&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;深圳上线 70 名「AI 员工」，满足 240 个政务场景&lt;/h3&gt; 
&lt;p&gt;深圳福田区上线了基于 DeepSeek 开发的「AI 数智员工」，覆盖政务服务全链条 240 个场景。这些「AI 员工」能够处理公文、提供民生服务、支持应急管理等任务。福田区政务大模型 2.0 版以 DeepSeek R1 为核心，通过混合专家架构（MoE）和强化学习技术，实现了高效、精准的政务服务。该模型不仅提升了公文处理和审核效率，还为招商引资、执法文书生成等场景提供了强大支持。&lt;br&gt; &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FcRalKSPJxLYgzsApONMTUw&quot; target=&quot;_blank&quot;&gt;来源&lt;/a&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Fdaily-ai-20250218%2F&quot; target=&quot;_blank&quot;&gt;&amp;nbsp;原文&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Adobe 发布全新生成式 AI 应用 Firefly，进军商业化&lt;/h3&gt; 
&lt;p&gt;Adobe 推出了生成式 AI 应用 Firefly，集成了图像、矢量图形和视频生成功能。Firefly 支持从文本提示生成高质量视频，并提供多种语言翻译功能，能够将音频翻译成 20 多种语言。该应用还与 Adobe 全家桶深度集成，支持无缝切换和创作流程优化。Firefly 的推出标志着 Adobe 在生成式 AI 领域的商业化布局，为创意产业带来了革命性体验。&lt;br&gt; &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F-AiIZ08-vk1-xWOPt7a97A&quot; target=&quot;_blank&quot;&gt;来源&lt;/a&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Fdaily-ai-20250218%2F&quot; target=&quot;_blank&quot;&gt;&amp;nbsp;原文&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;搞定 DeepSeek R1 部署，这个开源项目有点东西！&lt;/h3&gt; 
&lt;p&gt;开源项目 GPUStack 为 DeepSeek R1 的部署提供了高效解决方案。该项目支持 Windows、Linux 和 macOS，能够自动处理资源分配，支持多机协同计算和异构硬件适配。GPUStack 通过分布式推理技术，解决了单机资源不足的问题，使 DeepSeek R1 能够在各种硬件环境下稳定运行。此外，GPUStack 还支持模型管理、高可用性、监控与可视化等功能，为 AI 模型的部署和管理提供了全面支持。&lt;br&gt; &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FQmbVjWXO2tGNPFHIjwfa3A&quot; target=&quot;_blank&quot;&gt;来源&lt;/a&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Fdaily-ai-20250218%2F&quot; target=&quot;_blank&quot;&gt;&amp;nbsp;原文&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;清华最新综述解读，大模型推理能力的进化之路！&lt;/h3&gt; 
&lt;p&gt;清华大学发布了一篇关于大模型推理能力发展的综述论文，全面梳理了 LLM 推理能力的进化路径。论文指出，大模型的推理能力建立在预训练、微调和对齐三个关键阶段之上。高质量的推理数据和先进的训练方法（如强化学习）是提升模型推理能力的关键。此外，论文还探讨了测试时优化技术（如树搜索和集束搜索）对提升推理能力的作用。该综述为理解大模型推理能力的发展提供了重要参考。&lt;br&gt; &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FY_bmJUObwyoD0AI6yCqYIQ&quot; target=&quot;_blank&quot;&gt;来源&lt;/a&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Fdaily-ai-20250218%2F&quot; target=&quot;_blank&quot;&gt;&amp;nbsp;原文&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;微信也接不住 DeepSeek 的流量？&lt;/h3&gt; 
&lt;p&gt;微信开始灰度测试接入 DeepSeek R1，用户可以在微信搜索中体验 AI 搜索功能。这一举措不仅是微信对 AI 技术的积极探索，也反映了腾讯对流量变现的布局。尽管 DeepSeek 的接入带来了高昂的算力成本，但微信希望通过 AI 技术提升用户体验，吸引更多用户。此外，腾讯旗下的多个产品也在持续接入 DeepSeek，显示出腾讯在 AI 领域的全面布局。&lt;br&gt; &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FJ9XMqSdz00x_TFsMXRZbfg&quot; target=&quot;_blank&quot;&gt;来源&lt;/a&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Fdaily-ai-20250218%2F&quot; target=&quot;_blank&quot;&gt;&amp;nbsp;原文&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;DeepSeek 颠覆了什么？学习不靠「人盯」，AI 自己「卷」自己&lt;/h3&gt; 
&lt;p&gt;DeepSeek 通过纯强化学习路线，颠覆了传统 AI 训练模式。其 R1 模型证明了无需过程监督，仅通过结果控制即可训练出优秀的推理模型。DeepSeek 的 Zero 研究展示了模型自主生成思维链的能力，为 AI 的平民化铺平了道路。此外，DeepSeek 在语言文字创作和风格模仿方面也取得了显著突破，进一步拓展了 AI 的应用范围。&lt;br&gt; &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FsYRgyuGy7rE5sfMjzHsWlA&quot; target=&quot;_blank&quot;&gt;来源&lt;/a&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Fdaily-ai-20250218%2F&quot; target=&quot;_blank&quot;&gt;&amp;nbsp;原文&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;比亚迪掀起「全民智驾」风暴：接入 DeepSeek，7 万级车型标配高阶智驾&lt;/h3&gt; 
&lt;p&gt;比亚迪发布了「天神之眼」高阶智能驾驶系统，将高阶智驾功能推广至 7 万级车型。该系统由比亚迪全栈自研，能够实现全程高速自动驾驶和城区稳定驾驶。比亚迪还宣布将接入 DeepSeek R1 大模型，进一步提升车辆的智能化水平。这一举措标志着智能驾驶技术的普及化，为未来交通带来了新的可能性。&lt;br&gt; &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F6hNhK_NmeKCsAcE0QnS80A&quot; target=&quot;_blank&quot;&gt;来源&lt;/a&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Fdaily-ai-20250218%2F&quot; target=&quot;_blank&quot;&gt;&amp;nbsp;原文&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;马斯克用程序员和 AI 算法「整顿」华盛顿，此刻已乱成一锅粥&lt;/h3&gt; 
&lt;p&gt;马斯克领导的 DOGE（政府效率部）正在通过 AI 技术和高强度工作模式对美国联邦政府机构进行「现代化改造」。DOGE 团队由一群年轻工程师组成，他们被赋予极高权限，直接接入财政部、教育部等核心部门的数据库，利用 AI 算法审查开支、识别欺诈行为，并推动自动化流程。马斯克甚至声称，AI 已发现财政部每年有高达 500 亿美元的可疑支出。此外，DOGE 还计划开发名为「GSAi」的生成式 AI 聊天机器人，用于梳理政府合同和采购数据。然而，这一系列激进措施引发了争议，许多联邦机构员工面临裁员，部分项目被暂停，甚至引发了多起法律诉讼。批评者认为，马斯克的团队缺乏透明度和问责机制，可能对公共服务造成负面影响。&lt;br&gt; &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F5Co4_4bNFiZCc9x7opxuvA&quot; target=&quot;_blank&quot;&gt;来源&lt;/a&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Fdaily-ai-20250218%2F&quot; target=&quot;_blank&quot;&gt;&amp;nbsp;原文&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;DeepSeek 冲击之下，大模型六小强如何「回应」？&lt;/h3&gt; 
&lt;p&gt;DeepSeek 的出现对全球大模型市场产生了巨大冲击。国内六家独角兽大模型公司（零一万物、百川智能、阶跃星辰、智谱华章、月之暗面、MiniMax）纷纷采取行动应对。零一万物与苏州高新区合作，成立产业大模型基地，聚焦垂直产业解决方案；百川智能发布全场景推理大模型 Baichuan-M1-preview，并推出「AI 儿科医生」；阶跃星辰发布多款新模型，并在应用中接入 DeepSeek；智谱华章继续与三星合作，推动大模型在手机端的应用；月之暗面发布 Kimi k1.5 多模态思考模型；MiniMax 开源 MiniMax-01 系列模型，推动技术进化。这些动作表明，大模型公司正在通过技术创新和产业合作来应对 DeepSeek 带来的挑战。&lt;br&gt; &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FfdbEWhQekN1w3WvI_vCg7A&quot; target=&quot;_blank&quot;&gt;来源&lt;/a&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Fdaily-ai-20250218%2F&quot; target=&quot;_blank&quot;&gt;&amp;nbsp;原文&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;1100 万人都在玩的 AI 视频神器，三步把你 P 进任何名场面&lt;/h3&gt; 
&lt;p&gt;Pika 是一款 AI 视频工具，最近推出了 Pikaddition 功能，允许用户通过简单的三步操作将任何图片融入视频中。用户只需上传视频和图片，并输入提示词，Pika 就能生成融合效果。该功能支持多种创意玩法，例如将人物融入影视名场面、制作表情包等。尽管 Pika 的特效可能不是专业级，但其低门槛和高趣味性吸引了大量用户，注册用户已突破 1100 万。此外，Pika 还推出了 Pikamemes 功能，一键生成表情包，进一步降低了 AI 视频创作的门槛。&lt;br&gt; &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FjcUdGPLBYHV6TT5zuOhK7Q&quot; target=&quot;_blank&quot;&gt;来源&lt;/a&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Fdaily-ai-20250218%2F&quot; target=&quot;_blank&quot;&gt;&amp;nbsp;原文&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;政务云，DeepSeek 下一个风口？&lt;/h3&gt; 
&lt;p&gt;多地政务系统开始接入 DeepSeek 大模型，推动政务数字化转型。山东烟台、苏州、广州、深圳、无锡等地已部署 DeepSeek 模型，用于提升政务服务效率、优化数据管理和降低错误率。例如，深圳市的「AI 公务员」错误率控制在 5% 以内，显著提升了政务处理效率。相关研报指出，DeepSeek 的开源特性为云服务厂商提供了低门槛部署世界级 AI 应用的机会，政务云作为重要细分领域有望加速发展。&lt;br&gt; &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FeAlm4IVXiqpJXRizqMCywQ&quot; target=&quot;_blank&quot;&gt;来源&lt;/a&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Fdaily-ai-20250218%2F&quot; target=&quot;_blank&quot;&gt;&amp;nbsp;原文&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;🔥 热门文章推荐（2AGI.NET）&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Fdaily-ai-20250218%2F&quot; target=&quot;_blank&quot;&gt;天天 AI-20250218&lt;/a&gt; &lt;p&gt;作者：2AGI&lt;/p&gt; 2025 年 2 月 18 日&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Fdaily-ai-20250217%2F&quot; target=&quot;_blank&quot;&gt;天天 AI-20250217&lt;/a&gt; &lt;p&gt;作者：2AGI&lt;/p&gt; 2025 年 2 月 17 日&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2F10w-plus-hit-ai-pipeline-coze-depth-writing-deepseek-algorithm-insight-html-fast-typesetting%2F&quot; target=&quot;_blank&quot;&gt;10W+爆款 AI 流水线：Coze 深度写作×DeepSeek 算法洞察×HTML 极速排版&lt;/a&gt; &lt;p&gt;作者：2AGI&lt;/p&gt; 2025 年 2 月 16 日&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Fexploring-the-secrets-of-world-models%2F&quot; target=&quot;_blank&quot;&gt;探索世界模型奥秘&lt;/a&gt; &lt;p&gt;作者：2AGI&lt;/p&gt; 2025 年 2 月 16 日&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Funderstanding-intelligent-emergence%2F&quot; target=&quot;_blank&quot;&gt;如何理解智能涌现（emergence）&lt;/a&gt; &lt;p&gt;作者：2AGI&lt;/p&gt; 2025 年 2 月 15 日&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Fdaily-ai-20250214%2F&quot; target=&quot;_blank&quot;&gt;天天 AI-20250214&lt;/a&gt; &lt;p&gt;作者：2AGI&lt;/p&gt; 2025 年 2 月 14 日&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Fdaily-ai-20250213%2F&quot; target=&quot;_blank&quot;&gt;天天 AI-20250213&lt;/a&gt; &lt;p&gt;作者：2AGI&lt;/p&gt; 2025 年 2 月 13 日&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Fdaily-ai-20250212%2F&quot; target=&quot;_blank&quot;&gt;天天 AI-20250212&lt;/a&gt; &lt;p&gt;作者：2AGI&lt;/p&gt; 2025 年 2 月 12 日&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Fdaily-ai-20250211%2F&quot; target=&quot;_blank&quot;&gt;天天 AI-20250211&lt;/a&gt; &lt;p&gt;作者：2AGI&lt;/p&gt; 2025 年 2 月 11 日&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;h4&gt;&lt;strong&gt;扫码加入社群，参与讨论&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;&lt;img alt=&quot;2AGI 技术社区，欢迎扫码加入&quot; height=&quot;558&quot; src=&quot;https://oscimg.oschina.net/oscnet//88cbcd3a747ff3c416c80d739b5a3a82.png&quot; width=&quot;1180&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#212121; margin-left:0; margin-right:0; text-align:start&quot;&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Ftag%2Fagi%2F&quot; target=&quot;_blank&quot;&gt;AGI&lt;span&gt;(102)&lt;/span&gt;&lt;/a&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Ftag%2Fai-agent%2F&quot; target=&quot;_blank&quot;&gt;AI Agent&lt;span&gt;(3)&lt;/span&gt;&lt;/a&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Ftag%2Fai-app%2F&quot; target=&quot;_blank&quot;&gt;AI App&lt;span&gt;(1)&lt;/span&gt;&lt;/a&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Ftag%2Fai-celebrity%2F&quot; target=&quot;_blank&quot;&gt;AI Celebrity&lt;span&gt;(9)&lt;/span&gt;&lt;/a&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Ftag%2Faigc%2F&quot; target=&quot;_blank&quot;&gt;AIGC&lt;span&gt;(126)&lt;/span&gt;&lt;/a&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Ftag%2Fai-%25e5%2590%258d%25e4%25ba%25ba%25e5%25a0%2582%2F&quot; target=&quot;_blank&quot;&gt;AI 名人堂&lt;span&gt;(9)&lt;/span&gt;&lt;/a&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Ftag%2Fai-%25e6%2590%259c%25e7%25b4%25a2%2F&quot; target=&quot;_blank&quot;&gt;AI 搜索&lt;span&gt;(1)&lt;/span&gt;&lt;/a&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Ftag%2Fai%25e6%2595%2599%25e7%25a8%258b%2F&quot; target=&quot;_blank&quot;&gt;AI 教程&lt;span&gt;(7)&lt;/span&gt;&lt;/a&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Ftag%2Fai-%25e6%2595%2599%25e7%25a8%258b%2F&quot; target=&quot;_blank&quot;&gt;AI 教程&lt;span&gt;(2)&lt;/span&gt;&lt;/a&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Ftag%2Fai%25e7%2594%259f%25e4%25ba%25a7%25e5%258a%259b%25e5%25b9%25b3%25e5%258f%25b0%2F&quot; target=&quot;_blank&quot;&gt;AI 生产力平台&lt;span&gt;(1)&lt;/span&gt;&lt;/a&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Ftag%2Fai%25e7%2594%25b5%25e5%25bd%25b1%25e5%2588%25b6%25e4%25bd%259c%2F&quot; target=&quot;_blank&quot;&gt;AI 电影制作&lt;span&gt;(2)&lt;/span&gt;&lt;/a&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Ftag%2Fclaude%2F&quot; target=&quot;_blank&quot;&gt;Claude&lt;span&gt;(1)&lt;/span&gt;&lt;/a&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Ftag%2Fclaude-3-5-sonnet%2F&quot; target=&quot;_blank&quot;&gt;claude 3.5 sonnet&lt;span&gt;(1)&lt;/span&gt;&lt;/a&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.2agi.net%2Fblog%2Ftag%2Fcoze%2F&quot; target=&quot;_blank&quot;&gt;Coze&lt;span&gt;(2)&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334648</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334648</guid>
            <pubDate>Sat, 08 Feb 2025 01:11:00 GMT</pubDate>
            <author>来源: 投稿</author>
        </item>
        <item>
            <title>百度 2024 年总营收 1331 亿元，对 AI 投资充满信心</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;百度刚刚发布了 2024 年 Q4 及全年财报：全年总营收 1331 亿元，归属百度核心净利润达 234 亿元，同比增长 21%。&lt;/p&gt; 
&lt;p&gt;百度联合创始人兼 CEO 李彦宏表示：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;2024 年是百度从以互联网为中心向以 AI 为先转型的关键一年。我们的全栈 AI 能力得到了市场的广泛认可，从而推动了人工智能云的发展势头。在移动生态系统方面，我们坚定不移地推进 AI 转型，使搜索更接近原生 AI 能力，从而提供更好的用户体验。&lt;/p&gt; 
 &lt;p&gt;Apollo Go 经过多年的投入也充分验证了其商业模式，为全球扩张和可扩展的轻资产战略铺平了道路。随着我们的战略远见逐渐得到验证，我们预计我们的 AI 投资将在 2025 年取得更显著的成果。&lt;/p&gt; 
 &lt;p&gt;&lt;img height=&quot;1440&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0218/184432_hRLQ_2720166.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;受 AI 驱动，百度智能云呈高速增长，四季度收入同比增长达 26%。近期，百度智能云成功点亮昆仑芯三代万卡集群，未来还将进一步点亮三万卡集群。&lt;/p&gt; 
&lt;p&gt;12 月，文心大模型日均调用量达 16.5 亿次，一年增长 33 倍。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;1440&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0218/184634_17th_2720166.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334611</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334611</guid>
            <pubDate>Fri, 07 Feb 2025 10:47:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>开源策略是大模型最好的竞争策略</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;最近，开源中国 OSCHINA、Gitee 与 Gitee AI&lt;a href=&quot;https://www.oschina.net/news/330623/china-open-source-2024-annual-report&quot;&gt;联合发布了《2024 中国开源开发者报告》&lt;/a&gt;。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;报告地址：&lt;a href=&quot;https://talk.gitee.com/report/china-open-source-2024-annual-report.pdf?fr=news0120&quot; target=&quot;_blank&quot;&gt;2024 中国开源开发者报告.pdf&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;报告聚焦 AI 大模型领域，对过去一年的技术演进动态、技术趋势、以及开源开发者生态数据进行多方位的总结和梳理。&lt;/p&gt; 
&lt;p&gt;在第二章《TOP 101-2024 大模型观点》中，资深开发者社区运营专家&lt;strong&gt;顾钧&lt;/strong&gt;直言，开源策略是大模型最好的竞争策略，并分享了诸多思考：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;大模型是一项较新技术，赛道的主要玩家在技术和商业化上有差距但未到无法翻盘程度，该赛道涵盖模型训练与服务，市场化程度高，商业化场景覆盖 2B 与 2C。&lt;/li&gt; 
 &lt;li&gt;大模型赛道在海外是 「一超多强」，国内是 「多头并举」，元素丰富，为分析开源策略重要性提供素材。&lt;/li&gt; 
 &lt;li&gt;大模型竞争赛点包括技术先进性、C 端用户基数、依赖软件的生态系统大小等，但技术先进性目前更多用于公关宣传，大模型商业化还处于摸索阶段。&lt;/li&gt; 
 &lt;li&gt;C 端用户没有忠诚度且难以产生独特粘性，大模型厂商维系 C 端流量成本可能很高，且大模型赛道内卷，普通用户使用随意性强、准确性要求不高。&lt;/li&gt; 
 &lt;li&gt;大模型生态系统大小是评价其能否胜出的关键指标，构建开发者生态有提供 API 云服务和 「开源」 两种方法，闭源大模型多采用前者，开源模型采用后者。&lt;/li&gt; 
 &lt;li&gt;抛开成本和易用性空谈技术先进性是常见错误，大模型领域开源更有优势，因为大模型赛道核心制约条件是成本太高，开源可省去拓展开发者生态的大模型运行成本。&lt;/li&gt; 
 &lt;li&gt;闭源大模型厂商维持云资源、工程师资源支撑开发者调试需求，投入产出可能算不过来，且大模型对开发者粘性有限。&lt;/li&gt; 
 &lt;li&gt;大模型 API 云服务接口简单且高度一致，开发者构建应用时与具体大模型难形成强绑定，主动权在开发者。&lt;/li&gt; 
 &lt;li&gt;开源策略目标是以最小代价消耗闭源对手资源与心气，是 &lt;strong&gt;「先为不可胜，以待敌之可胜」&lt;/strong&gt; 的策略。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img height=&quot;1920&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0218/175851_PK73_2720166.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;完整全文：&lt;a href=&quot;https://my.oschina.net/u/3859945/blog/17503844&quot; target=&quot;_blank&quot;&gt;https://my.oschina.net/u/3859945/blog/17503844&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334605</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334605</guid>
            <pubDate>Fri, 07 Feb 2025 10:04:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>KubeSphere 产品生命周期管理政策公告正式发布</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                    
                                                                                                                                                    &lt;p&gt;亲爱的 KubeSphere 用户：&lt;/p&gt; 
&lt;p&gt;在云原生技术飞速发展的今天，KubeSphere 始终以技术创新和用户价值为核心，持续优化产品与服务。为更好地服务全球用户、保障业务连续性，基于多年的技术积累与用户反馈，我们正式对外公开发布 &lt;strong&gt;《KubeSphere 产品生命周期管理政策》&lt;/strong&gt;。通过清晰的支持策略与版本管理，助力用户高效规划升级，规避潜在风险，实现业务可持续发展。&lt;/p&gt; 
&lt;h2&gt;KubeSphere 产品生命周期管理政策公告&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;文件版本&lt;/strong&gt;: v1.0 &lt;strong&gt;更新时间&lt;/strong&gt;: 2025.02.14&lt;/p&gt; 
&lt;h2&gt;概述&lt;/h2&gt; 
&lt;p&gt;在快速变化的市场环境中，青云科技针对 KubeSphere 云原生产品与服务推出了产品生命周期管理政策。该政策旨在及时调整和终止不再符合市场需求的产品，以确保我们的产品始终满足客户期望。通过清晰的产品终止方案，我们将为客户提供必要的支持与指导，降低业务风险，提升客户信任与满意度。我们致力于推动技术创新，实现持续的业务发展，为客户创造更大的价值。&lt;/p&gt; 
&lt;h2&gt;适用范围&lt;/h2&gt; 
&lt;p&gt;本文中描述的 KubeSphere 产品生命周期终止政策适用于以下 KubeSphere 云原生产品与服务：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;KubeSphere 开源版。涉及 v1、v2、v3 及 v4 版本。&lt;/li&gt; 
 &lt;li&gt;KubeSphere 企业版（包含更名前的青云 QKCP）。涉及 v1、v2、v3 及 v4 版本。&lt;/li&gt; 
 &lt;li&gt;青云容器平台（可信版）。涉及所有产品版本。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;KubeSphere 产品版本定义&lt;/h2&gt; 
&lt;p&gt;在 KubeSphere 产品版本控制中，软件版本号通常用来表示其代表的发布阶段与更新内容。其中，版本号格式包括主版本号 (Major Version)、次版本号 (Minor Version)、补丁版本号 (Patch Version) 和热修复版本号 (HotFix)。以下是这些术语的详细说明：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;1. 主版本 (Major Version)&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;主版本号的设置通常表示软件有重大更新或变化，这可能包括全新的产品架构、功能模块、操作体验或与旧版本不再兼容的变更。&lt;/li&gt; 
 &lt;li&gt;主版本号的设置通常意味着版本号的其他部分（如：次版本号、补丁版本号）重置为零。例如，产品版本从 v3.4.1 升级到 v4.0.0。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2. 次版本 (Minor Version)&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;次版本号的设置表示软件有一般的更新或改进，这可能包括新增功能模块、性能改善、安全性强化或兼容性增强等。&lt;/li&gt; 
 &lt;li&gt;次版本号的设置通常意味着补丁版本号重置为零，但主版本号保持不变。例如，产品版本从 v4.1.2 升级到 v4.2.0。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;3. 补丁版本 (Patch Version)&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;补丁版本号的设置表示软件有较小的修复或优化，这通常用于修复错误、改进稳定性、或优化产品体验等。&lt;/li&gt; 
 &lt;li&gt;补丁版本不会开启新的产品生命周期。例如，KSE v4.1.3 将共享 KSE v4.1 的生命周期。&lt;/li&gt; 
 &lt;li&gt;补丁版本号的设置不会影响主版本号和次版本号。例如，产品版本从 v4.1.2 升级到 v4.1.3。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;4. 热修复 (HotFix)&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;热修复是指对软件进行的紧急修复，其通常用于解决如无法继续的阻塞或严重的安全隐患导致客户业务停滞或没有临时方案的问题。&lt;/li&gt; 
 &lt;li&gt;热修复仅就对应发现的严重问题进行修复和针对性测试，不会像上述三类正式产品版本一样进行全量的测试验证，是解决紧急问题的临时性处置措施。最佳方案依然是升级至后续正式产品版本。&lt;/li&gt; 
 &lt;li&gt;热修复通常不改变产品版本号的主要部分，仅在现有版本上立即应用。例如，如果产品版本 v3.5.0 存在严重错误，可能会发布一个热修复版本 3.5.0-hotfix-version-number。&lt;/li&gt; 
 &lt;li&gt;热修复可能会在下一个补丁版本或次版本中被正式包含。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;KubeSphere 服务与支持等级定义&lt;/h2&gt; 
&lt;p&gt;在 KubeSphere 云原生产品与服务中，通常会涉及到以下多种类型的服务与支持。根据等级的划定，可分为：FS（Full Support，全面服务与支持）与 ES（Extended Support，延长服务与支持）。下方表格中」Y」表示」YES」，即支持；」N」表示」NO」，即不支持。表格中的数字标识表示有进一步的文字说明，详见下方对应标号的内容。&lt;/p&gt; 
&lt;p&gt;&amp;lt;center&amp;gt;&amp;lt;span style=&quot;color: #00A971;&quot;&amp;gt;KubeSphere 服务与支持等级表&amp;lt;/span&amp;gt;&amp;lt;/center&amp;gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;服务与支持类型&lt;/th&gt; 
   &lt;th&gt;FS&lt;/th&gt; 
   &lt;th&gt;ES&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;服务与支持时长 (Duration: Months)&lt;/td&gt; 
   &lt;td&gt;12-36&lt;/td&gt; 
   &lt;td&gt;06-24&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;问题分析 (Root-Cause Analysis)&amp;lt;sup&amp;gt;(1)&amp;lt;/sup&amp;gt;&lt;/td&gt; 
   &lt;td&gt;Y&lt;/td&gt; 
   &lt;td&gt;Y&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;产品新特性 (Features)&lt;/td&gt; 
   &lt;td&gt;Y&lt;/td&gt; 
   &lt;td&gt;N&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;产品版本升级 (Upgrade)&amp;lt;sup&amp;gt;(2), (3)&amp;lt;/sup&amp;gt;&lt;/td&gt; 
   &lt;td&gt;Y&lt;/td&gt; 
   &lt;td&gt;Y&amp;lt;sup&amp;gt;(2)&amp;lt;/sup&amp;gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;功能与体验优化 (Enhancements)&lt;/td&gt; 
   &lt;td&gt;Y&lt;/td&gt; 
   &lt;td&gt;N&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;普通缺陷修复 (General BugFix)&lt;/td&gt; 
   &lt;td&gt;Y&lt;/td&gt; 
   &lt;td&gt;N&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;严重缺陷修复 (Critical BugFix)&amp;lt;sup&amp;gt;(4)&amp;lt;/sup&amp;gt;&lt;/td&gt; 
   &lt;td&gt;Y&lt;/td&gt; 
   &lt;td&gt;Y&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;严重安全漏洞修复 (Critical Security Fix)&amp;lt;sup&amp;gt;(5)&amp;lt;/sup&amp;gt;&lt;/td&gt; 
   &lt;td&gt;Y&lt;/td&gt; 
   &lt;td&gt;Y&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;兼容性支持 (Compatibility Support)&amp;lt;sup&amp;gt;(6)&amp;lt;/sup&amp;gt;&lt;/td&gt; 
   &lt;td&gt;N&amp;lt;sup&amp;gt;(7)&amp;lt;/sup&amp;gt;&lt;/td&gt; 
   &lt;td&gt;N&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;迁移协助 (Migration Support)&lt;/td&gt; 
   &lt;td&gt;N&lt;/td&gt; 
   &lt;td&gt;N&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;(1) 「问题分析」是指对非产品自身、从外部引入的第三方软硬件带来的问题进行分析与支持。如客户自己开发的业务系统、自采购硬件设备、自行部署的第三方软件等，青云科技仅承诺协助性分析与支持，不承诺能定位或解决此类非青云科技提供的产品的问题。&lt;/li&gt; 
 &lt;li&gt;(2) 产品版本升级主要包括提供主版本、次版本、补丁版本或热修复的升级。ES 等级下，仅针对「严重缺陷修复」与「严重安全漏洞修复」提供」热修复「升级。&lt;/li&gt; 
 &lt;li&gt;(3) 一般情况下，产品不支持跳过任意主和次版本进行升级，需按版本号顺序依次递进升级。若遇不确定情况，请联系青云科技客服人员或售后服务团队，咨询并评估专属的产品升级方案。&lt;/li&gt; 
 &lt;li&gt;(4) 「严重缺陷」是指客户、产品经理、测试经理、产品技术负责人、技术支持工程师及项目经理等多方共同认定下，明确影响到客户业务连续性、稳定性、可靠性等的问题。&lt;/li&gt; 
 &lt;li&gt;(5) 「严重安全漏洞」是指 CVSS 大于等于 7 的安全漏洞问题。&lt;/li&gt; 
 &lt;li&gt;(6) 兼容性支持主要包括兼容 Kubernetes 新版本、新的 CPU 架构、新的 OS 类型或架构、OS 的新版本等。&lt;/li&gt; 
 &lt;li&gt;(7) 兼容性支持取决于软件主版本、次版本发布时的适配兼容情况，在后续的产品生命周期中不再改变。FS 不再新增兼容适配。&lt;/li&gt; 
 &lt;li&gt;(8) FS、ES 仅针对青云科技的商业产品。ES 可视为 FS 的延续，不一定每个版本都可以额外订阅 ES。ES 在经过青云科技确认并同意的前提下最多允许订阅两次即两年，之后不再提供续订。若有需要，请联系青云科技客服人员或售后服务团队咨询详情。&lt;/li&gt; 
 &lt;li&gt;(9) FS、ES 默认通过如维保工单支持平台、远程桌面、电话、微信、企业微信等远程方式提供服务与支持。现场或驻场类型的服务与支持不在 FS、ES 范围内，若有需要，请联系青云科技客服人员或售后服务团队咨询详情。&lt;/li&gt; 
 &lt;li&gt;(10) 青云科技 KubeSphere 云原生产品与服务的具体服务与支持范围，可参考青云科技售后服务团队出具的 SLA 声明。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;KubeSphere 产品生命周期说明&lt;/h2&gt; 
&lt;h3&gt;1. Standard 类型产品版本说明&lt;/h3&gt; 
&lt;p&gt;&amp;lt;center&amp;gt;&amp;lt;span style=&quot;color: #00A971;&quot;&amp;gt;KubeSphere Standard 类型产品版本说明表&amp;lt;/span&amp;gt;&amp;lt;/center&amp;gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;里程碑阶段&lt;/th&gt; 
   &lt;th&gt;GA&lt;/th&gt; 
   &lt;th&gt;EoFS&lt;/th&gt; 
   &lt;th&gt;EoES&lt;/th&gt; 
   &lt;th&gt;EoL&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;中英文名称&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;General Availability, 一般可获得性，即批量销售日。&lt;/td&gt; 
   &lt;td&gt;End of Full Support，停止全面服务与支持。&lt;/td&gt; 
   &lt;td&gt;End of Extended Support，停止延长服务与支持。&lt;/td&gt; 
   &lt;td&gt;End of Life，生命周期完全终止。&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;定义&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;GA 是产品生命周期开启的日期。该日期表明该产品版本正式开始售卖，并可按需、批量交付到客户生产环境使用。&lt;/td&gt; 
   &lt;td&gt;EoFS 是产品停止全面服务与支持的日期。产品停止提供补丁版本，仅针对「严重缺陷」与「严重安全漏洞」提供「热修复」。同时，该日期表明产品（主版本、次版本）已不再销售。&lt;/td&gt; 
   &lt;td&gt;EoES 是产品停止延长服务与支持的日期。产品不再修复严重缺陷与严重安全漏洞。&lt;/td&gt; 
   &lt;td&gt;EoL 是产品生命周期完全终止的日期。完全停止此产品（版本）的一切活动，包括商业售卖、版本升级（含热修复）、各种等级的服务与支持等。青云科技将不再为此产品（版本）上产生的任何问题负责。&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;服务与支持等级&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;FS（产品 GA 之后，根据客户的产品授权与维保订阅情况提供）&lt;/td&gt; 
   &lt;td&gt;ES（产品 EoFS 之后，产品不再提供标准的 FS。客户额外订阅 ES 的情况后提供延长服务与支持）&lt;/td&gt; 
   &lt;td&gt;——&lt;/td&gt; 
   &lt;td&gt;——&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;客户影响&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;——&lt;/td&gt; 
   &lt;td&gt;(1) 不建议客户使用已经 EoFS 的版本进行新建或扩容；&amp;lt;br&amp;gt;(2) 客户无法得到新的产品补丁版本；&amp;lt;br&amp;gt;(3) 推荐客户尽快升级使用全新 GA 的版本；&amp;lt;br&amp;gt;(4) 客户可在订阅 ES 后享受对应等级服务与支持。&lt;/td&gt; 
   &lt;td&gt;(1) 客户无法得到新的产品热修复；&amp;lt;br&amp;gt;(2) 若继续使用，客户必须进行版本升级；&amp;lt;br&amp;gt;(3) 对应产品（版本）官方文档很快将无法查看；&amp;lt;br&amp;gt;(4) 对应产品（版本）安装、升级等程序（镜像 Chart 等）很快将无法使用。&lt;/td&gt; 
   &lt;td&gt;(1) 客户无法查看对应产品（版本）官方文档；&amp;lt;br&amp;gt;(2) 客户无法使用安装、升级等程序（镜像 Chart 等）；&amp;lt;br&amp;gt;(3) 客户无法获得针对该产品（版本）的任何服务；&amp;lt;br&amp;gt;(4) 客户需自行维护或迁移业务环境，客户需考虑购买新的产品（版本）。&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;2. Preview 类型产品及扩展组件版本说明&lt;/h3&gt; 
&lt;p&gt;KubeSphere 云原生产品与服务中，Preview 类型产品版本及扩展组件版本非售卖，不支持商用，仅供实验室、开发测试等非正式生产环境体验使用，不提供 SLA 与 EOS 商业保障。通常情况下，Preview 类型产品版本及扩展组件版本生命周期为 6 个月。&lt;/p&gt; 
&lt;h2&gt;KubeSphere 产品生命周期时间表&lt;/h2&gt; 
&lt;h3&gt;1. KubeSphere 企业版（包含更名前的青云 QKCP、青云容器平台（可信版））&lt;/h3&gt; 
&lt;p&gt;&amp;lt;center&amp;gt;&amp;lt;span style=&quot;color: #00A971;&quot;&amp;gt;KubeSphere 企业版产品生命周期时间表&amp;lt;/span&amp;gt;&amp;lt;/center&amp;gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;版本号&lt;/th&gt; 
   &lt;th&gt;版本类型&lt;/th&gt; 
   &lt;th&gt;GA&lt;/th&gt; 
   &lt;th&gt;EoFS&lt;/th&gt; 
   &lt;th&gt;EoES&lt;/th&gt; 
   &lt;th&gt;EoL&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;KSE v4.2&lt;/td&gt; 
   &lt;td&gt;Standard&lt;/td&gt; 
   &lt;td&gt;——&lt;/td&gt; 
   &lt;td&gt;——&lt;/td&gt; 
   &lt;td&gt;——&lt;/td&gt; 
   &lt;td&gt;——&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;KSE v4.1&lt;/td&gt; 
   &lt;td&gt;Standard&lt;/td&gt; 
   &lt;td&gt;2024 年 04 月 16 日&lt;/td&gt; 
   &lt;td&gt;2027 年 04 月 16 日&lt;/td&gt; 
   &lt;td&gt;2028 年 10 月 16 日&lt;/td&gt; 
   &lt;td&gt;2028 年 12 月 16 日&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;KSE v4.0&lt;/td&gt; 
   &lt;td&gt;Preview&lt;/td&gt; 
   &lt;td&gt;2023 年 08 月 16 日&lt;/td&gt; 
   &lt;td&gt;——&lt;/td&gt; 
   &lt;td&gt;——&lt;/td&gt; 
   &lt;td&gt;2024 年 06 月 28 日&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;KSE v3.5&lt;/td&gt; 
   &lt;td&gt;Standard&lt;/td&gt; 
   &lt;td&gt;2023 年 10 月 13 日&lt;/td&gt; 
   &lt;td&gt;2025 年 07 月 13 日&lt;/td&gt; 
   &lt;td&gt;2026 年 01 月 13 日&lt;/td&gt; 
   &lt;td&gt;2026 年 03 月 13 日&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;KSE v3.4&lt;/td&gt; 
   &lt;td&gt;Standard&lt;/td&gt; 
   &lt;td&gt;2023 年 04 月 25 日&lt;/td&gt; 
   &lt;td&gt;2025 年 05 月 25 日&lt;/td&gt; 
   &lt;td&gt;2025 年 11 月 25 日&lt;/td&gt; 
   &lt;td&gt;2025 年 12 月 25 日&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;KSE v3.3&lt;/td&gt; 
   &lt;td&gt;Standard&lt;/td&gt; 
   &lt;td&gt;——&lt;/td&gt; 
   &lt;td&gt;2025 年 03 月 31 日&lt;/td&gt; 
   &lt;td&gt;2025 年 09 月 30 日&lt;/td&gt; 
   &lt;td&gt;2025 年 10 月 31 日&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;KSE v3.2 及之前版本&lt;/td&gt; 
   &lt;td&gt;Standard&lt;/td&gt; 
   &lt;td&gt;——&lt;/td&gt; 
   &lt;td&gt;——&lt;/td&gt; 
   &lt;td&gt;——&lt;/td&gt; 
   &lt;td&gt;2025 年 03 月 31 日&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;青云容器平台（可信版）请参考 KubeSphere 企业版与青云 QKCP 对应版本号的产品生命周期情况。&lt;/li&gt; 
 &lt;li&gt;KubeSphere 企业版 v4.2 之后，各扩展组件可能出现独立发版、迭代的情况，但其产品生命周期不单独设立，仍与 KubeSphere 企业版产品生命周期一致。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;2. KubeSphere 开源版&lt;/h3&gt; 
&lt;p&gt;&amp;lt;center&amp;gt;&amp;lt;span style=&quot;color: #00A971;&quot;&amp;gt;KubeSphere 开源版产品生命周期时间表&amp;lt;/span&amp;gt;&amp;lt;/center&amp;gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;版本号&lt;/th&gt; 
   &lt;th&gt;版本类型&lt;/th&gt; 
   &lt;th&gt;GA&lt;/th&gt; 
   &lt;th&gt;EoL&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;KubeSphere v4.2&lt;/td&gt; 
   &lt;td&gt;Standard&lt;/td&gt; 
   &lt;td&gt;——&lt;/td&gt; 
   &lt;td&gt;——&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;KubeSphere v4.1&lt;/td&gt; 
   &lt;td&gt;Standard&lt;/td&gt; 
   &lt;td&gt;2024 年 09 月 12 日&lt;/td&gt; 
   &lt;td&gt;2027 年 09 月 12 日&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;KubeSphere v3.4&lt;/td&gt; 
   &lt;td&gt;Standard&lt;/td&gt; 
   &lt;td&gt;——&lt;/td&gt; 
   &lt;td&gt;2025 年 12 月 25 日&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;KubeSphere v3.3 及之前版本&lt;/td&gt; 
   &lt;td&gt;Standard&lt;/td&gt; 
   &lt;td&gt;——&lt;/td&gt; 
   &lt;td&gt;2025 年 10 月 31 日&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;补充说明&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;在 KubeSphere 企业版/开源版 v4 版本中，可能存在一些来自第三方合作伙伴的生态扩展组件。针对这些扩展组件，其产品生命周期管理策略请查看产品的相关说明文档或联系对应产品提供商。&lt;/li&gt; 
 &lt;li&gt;因产品软著与软件采购退税等政策性原因，青云科技销售目录与合同中所列举的产品版本可能与实际交付部署的产品版本不一致。若出现该情况，一切都以客户首次采购、实际交付部署并颁发许可证授权的产品版本为准。&lt;/li&gt; 
 &lt;li&gt;在 KubeSphere 云原生产品与服务的销售中，当出现采购产品永久授权与多年 FS/ES 的场景时，其适用的版本可能不是采购当时的产品版本。此时以当前实际交付部署并颁发许可证授权的产品版本为准。&lt;/li&gt; 
 &lt;li&gt;上方「KubeSphere 产品生命周期时间表」中展示的时间，为各里程碑阶段承诺服务与支持的最短时间。我们可能会基于产品版本质量、安装部署人数、新产品（版本）研发情况等进行动态调整 EoFS/EoES/EoL 的时间。若有调整，我们将另行公告，欢迎关注。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;注意事项&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;第三方扩展组件：生命周期策略请以提供商文档为准。&lt;/li&gt; 
 &lt;li&gt;版本交付一致性：实际授权版本以首次交付并颁发许可证的版本为准。&lt;/li&gt; 
 &lt;li&gt;动态调整说明：如遇产品迭代或用户需求变化，生命周期时间可能调整，请持续关注官方公告。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;完整声明全文&lt;/h3&gt; 
&lt;p&gt;中英双语全文请参见 KubeSphere 官网：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;全球站：&lt;/strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fkubesphere.io%2Fnews%2Fkubesphere-product-lifecycle-policy%2F&quot; target=&quot;_blank&quot;&gt;KubeSphere Product Lifecycle Policy&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;中文站：&lt;/strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fkubesphere.io%2Fzh%2Fnews%2Fkubesphere-product-lifecycle-policy%2F&quot; target=&quot;_blank&quot;&gt;KubeSphere 产品生命周期管理政策公告&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;关于 KubeSphere&lt;/h3&gt; 
&lt;p&gt;KubeSphere （&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fkubesphere.io%EF%BC%89%E6%98%AF%E5%9C%A8&quot; target=&quot;_blank&quot;&gt;https://kubesphere.io）是在&lt;/a&gt; Kubernetes 之上构建的开源容器平台，提供全栈的 IT 自动化运维的能力，简化企业的 DevOps 工作流。&lt;/p&gt; 
&lt;p&gt;KubeSphere 已被 Aqara 智能家居、本来生活、东方通信、微宏科技、东软、华云、新浪、三一重工、华夏银行、四川航空、国药集团、微众银行、紫金保险、去哪儿网、中通、中国人民银行、中国银行、中国人保寿险、中国太平保险、中国移动、中国联通、中国电信、天翼云、中移金科、Radore、ZaloPay 等海内外数万家企业采用。KubeSphere 提供了开发者友好的向导式操作界面和丰富的企业级功能，包括 Kubernetes 多云与多集群管理、DevOps (CI/CD)、应用生命周期管理、边缘计算、微服务治理 (Service Mesh)、多租户管理、可观测性、存储与网络管理、GPU support 等功能，帮助企业快速构建一个强大和功能丰富的容器云平台。&lt;/p&gt; 
&lt;p&gt;&amp;gt; 本文由博客一文多发平台 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopenwrite.cn%3Ffrom%3Darticle_bottom&quot; target=&quot;_blank&quot;&gt;OpenWrite&lt;/a&gt; 发布！&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/u/4197945/blog/17646180</link>
            <guid isPermaLink="false">https://my.oschina.net/u/4197945/blog/17646180</guid>
            <pubDate>Fri, 07 Feb 2025 09:37:00 GMT</pubDate>
            <author>原创</author>
        </item>
        <item>
            <title>阿里巴巴在上海成立智信普惠科技公司</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;天眼查 App 显示，上海智信普惠科技有限公司于 2025 年 2 月 17 日成立，位于上海市，是一家以从事软件和信息技术服务业为主的企业。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;法定代表人为余志干，企业注册资本 1000 万人民币。经营范围含计算机软硬件及辅助设备零售、网络技术服务、信息技术咨询服务、数字内容制作服务、数据处理服务、人工智能通用应用系统、人工智能应用软件开发、互联网信息服务等。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;股东信息显示，该公司由阿里巴巴旗下广州大鱼快乐信息技术有限公司全资持股。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;189&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-50d477f860fc194e7c26bfc25727db08669.png&quot; width=&quot;700&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334598</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334598</guid>
            <pubDate>Fri, 07 Feb 2025 09:33:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>Stanford 团队展现 RWKV 多智能体优势，UVa 团队突破 RWKV 端侧性能</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;近日 RWKV 生态新增来自海外名校的两项工作：Stanford（斯坦福大学）团队的 RWKV 多智能体研究，和 UVa（弗吉尼亚大学） 团队的 RWKV 端侧优化研究。&lt;/p&gt; 
&lt;h2&gt;RWKV 多智能体强化学习&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;开源项目地址：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fsocialdeductionllm.github.io%2F&quot; target=&quot;_blank&quot;&gt;https://socialdeductionllm.github.io/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;论文：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2502.06060&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2502.06060&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;四名斯坦福大学研究人员共同发布了《Training Language Models for Social Deduction with Multi-Agent Reinforcement Learning》论文，研究&lt;strong&gt;使用多智能体强化学习&lt;/strong&gt;（multi-agent reinforcement learning）训练 RWKV 模型，使其能&lt;strong&gt;通过自然语言交流&lt;/strong&gt;完成**《Among Us》游戏**的推理过程并赢下游戏。&lt;/p&gt; 
&lt;p&gt;论文已被 AAMAS 2025 主会（口头报告）接收，论文作者在 RWKV Discord 频道分享了这一消息，并分享了「&lt;strong&gt;为什么使用 RWKV-4-World 模型&lt;/strong&gt;」。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;为何选择 RWKV 而非 Transformer？&lt;/strong&gt; 因为 RWKV 的显存占用恒定、理论上支持无限上下文长度。Among Us 游戏单局轨迹可达数万 token，Transformer 模型显存占用过高，而 RWKV 的循环结构通过 T-BPTT 实现无限上下文训练，单 GPU 即可完成训练（论文的实验基于一张 48G 显存的 A40 显卡）&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;为何使用 RWKV-4，而非性能更好的 RWKV-5/6/7 模型？&lt;/strong&gt; 研究在 2023 年夏季启动，当时 RWKV-4 是唯一可用版本。团队通过修改 RWKV-4 的 CUDA 内核优化计算效率，没有时间适配 RWKV 新架构。未来计划适配 RWKV-7，进一步提升模型性能。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-02983c1404f33542f90159bef9c5fdf2346.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;接下来我们一起看看该工作的创新点，以及 RWKV 模型在论文中表现出来的强大性能：&lt;/p&gt; 
&lt;h3&gt;游戏规则&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;《Among Us》是类似于狼人杀/鸭鹅杀的社交推理游戏。&lt;/p&gt; 
 &lt;p&gt;游戏规则：在一辆宇宙飞船上有&lt;strong&gt;船员&lt;/strong&gt;（Crewmates）和&lt;strong&gt;内鬼&lt;/strong&gt;（Impostors）两种角色。内鬼的目标是暗中破坏飞船设施、杀死船员，并&lt;strong&gt;在讨论时伪装成普通船员&lt;/strong&gt;以避免被发现，船员的目标则是&lt;strong&gt;通过讨论进行逻辑推理，然后投票淘汰内鬼&lt;/strong&gt;。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-0b88f3704e3b60ed6f4133f07c8ad866e18.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;下图是论文中智能体在《Among Us》游戏的循环示意图，游戏开始时同时向所有智能体发送观察结果，然后在每个时间步从一组有效的行动中收集标记化的行动历史。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-c38fda38f6c099df315a41dec73b26848b5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;无需人工数据，纯 Self-Play&lt;/h3&gt; 
&lt;p&gt;这项工作创新的地方在于&lt;strong&gt;完全不依赖人工标注数据&lt;/strong&gt;，而是通过&lt;strong&gt;纯自我对抗学习（Self-Play）&lt;/strong&gt; 如环境反馈（投票结果、任务进度）和智能体（Agent）间交互来训练 AI 的语言交流能力。AI 智能体通过多轮博弈，逐步学习如何在讨论中提取关键信息，并形成自己的投票策略。&lt;/p&gt; 
&lt;p&gt;完整的训练框架引入了 &lt;strong&gt;RL + 听说双重训练机制&lt;/strong&gt;。先通过强化学习（RL），使得 AI 在没有人工数据示例的情况下学会如何行动。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;以下是用于优化 AI 长期游戏胜率的&lt;strong&gt;强化学习损失函数&lt;/strong&gt;，同时使用 &lt;strong&gt;KL 约束&lt;/strong&gt; 限制 AI 不能偏离自然语言分布。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-1bb307c78419f8a009bcef196d0887c397e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;此外，研究团队引入了一种新的&lt;strong&gt;听/说双重奖励机制&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;听力奖励（Listening Reward）&lt;/strong&gt;：听力的损失函数：&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-7fc4544ddeccf7c24fb8efec492c74509f0.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt; ，用于训练 AI 通过讨论预测环境信息，从而预测谁是内鬼。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;融合听力奖励后，强化学习的损失函数如下：&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-576c520ddcf32c888d2379a877c54178f71.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;表达奖励（Speaking Reward）&lt;/strong&gt;：奖励 AI 生成&lt;strong&gt;能影响队友决策&lt;/strong&gt;的消息，好的发言会获得更高的奖励&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-c2a008d036b28c097870a2f5bda300801f6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;融合了 RL + 听说双重奖励后，用于训练智能体的强化学习损失函数如下：&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-5d895b7a72f4c08f0ea1364b96b6d292b4a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;但&lt;strong&gt;RL 算法非常善于 Hack 规则的漏洞&lt;/strong&gt;。如果不加干预，智能体可能会&lt;strong&gt;抓住 Among Us 游戏规则的漏洞来「作弊」并进入失效模式（Failure Modes）&lt;/strong&gt;，比如船员们使用非自然语言来「对暗号」（非自然语言交流），或者在讨论阶段集体沉默等内鬼说话（作弊合作）等。&lt;/p&gt; 
&lt;h3&gt;失效模式与解决方案&lt;/h3&gt; 
&lt;p&gt;为了避免模型偏离自然语言的轨道或偏离任务目标，作者团队采取了一些巧妙的解决方案：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;KL 约束&lt;/strong&gt;：为了避免模型在讨论过程中「跑偏」，团队在训练中加入了 KL 约束，强制模型始终保持使用自然语言进行交流&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;冻结部分智能体&lt;/strong&gt;：为了防止模型在训练过程中学会不自然的策略（比如大家都不发言，只等内鬼发言），研究团队选择冻结部分智能体，让它不参与策略调整，从而避免了「集体摆烂」的现象&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;世界建模损失（World Modeling Loss）&lt;/strong&gt;：为了确保模型在每次讨论时都能记住重要的上下文信息，论文引入了世界建模损失：&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-6ab0ff0c1f9563ddec1fc4028d4f477870f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;世界建模损失用于帮助智能体&lt;strong&gt;学习更长期、更合理的策略&lt;/strong&gt;，避免出现像&lt;strong&gt;等待策略&lt;/strong&gt;（Waiting Strategy，智能体一直待在起始房间不动，然后投票淘汰移动过的玩家）等退化现象。这些策略虽然在短期内有效，但严重破坏了游戏的真实性和挑战性。&lt;/p&gt; 
&lt;p&gt;最终，&lt;strong&gt;完整的损失函数&lt;/strong&gt;结合了&lt;strong&gt;强化学习（RL）、听力（Listening）、表达（Speaking）、世界建模（WM）&lt;/strong&gt;：&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-a294cb582f4032308a32934f5867d528edc.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;通过新颖的「听说奖励」 结合 KL 约束和世界建模损失等优化，斯坦福团队的研究突破了 RL 传统上的局限，训练出来的 $\text {RWKV}_{RL + L + S}$ 模型在社交推理任务中展现出了&lt;strong&gt;更接近人类的行为模式&lt;/strong&gt;，为多智能体协作和复杂场景下的语言模型训练提供了新范式。&lt;/p&gt; 
&lt;h3&gt;RWKV 模型：胜率碾压 + 类人行为涌现&lt;/h3&gt; 
&lt;p&gt;论文选择 RWKV-4-World 模型作为语言模型基座，实验结果验证了其强大性能：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;1. 胜率碾压&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;完整训练框架（RL + 听说）的 &lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-b4c7cc6029197805d2a1990e222c0876327.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 模型（RWKV-4-World-1.5B）， &lt;strong&gt;Among US 游戏胜率是传统强化学习模型的 2 倍&lt;/strong&gt;，&lt;strong&gt;且优于 4 倍参数量的 RWKV 基底模型（RWKV-4-World-7B）&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-65d89373722bae0d1baa0845b81a2c8516e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;上图：不同算法训练的模型在基础环境（2 × 2 网格，每名队员 4 项任务，共 5 名玩家）中的胜率，经过完整框架训练的 RWKV 模型（橙色）胜率大幅领先传统 RL 模型（浅灰色）。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;在未训练过的环境配置中（如不同地图布局、任务数量），RWKV 模型仍能保持高胜率，展现了强大的泛化能力。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-37b7ac0067fd82298bcbd9f8e9653fcfd9d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;上图：使用不同算法训练的 AI 船员，在不同环境配置下的获胜率，环境修改包括更改游戏地图形状、任务数量和玩家数量。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;2. 类人行为涌现&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;AI 学会指控嫌疑人（如「Player Green 在尸体房间离开」），会提供证据支持自己的观点。甚至会编造谎言，试图误导队友（类似人类玩家策略）。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;3. 强适应能力&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;值得一提的是，游戏里的内鬼也是特别强化（反指控、转移焦点等）过的，其损失函数：&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-53e0a4041e0bdd1e07d38ec3634d7acb668.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;然而，面对自适应训练的内鬼，RWKV 船员仍能保持 50% 以上胜率，展现了强大的抗干扰能力。&lt;/p&gt; 
&lt;p&gt;实验数据验证了 RWKV 在&lt;strong&gt;多智能体社交推理&lt;/strong&gt;中的卓越性能，更揭示了 RWKV 在&lt;strong&gt;轻量化部署与长序列决策场景&lt;/strong&gt;的独特优势。&lt;/p&gt; 
&lt;h3&gt;未来工作&lt;/h3&gt; 
&lt;p&gt;论文作者表示后续将开展更多 RWKV 相关研究，包括：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;训练 RWKV 模型去&lt;strong&gt;塑造其他 LLM 智能体的行为和逻辑&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;训练 RWKV &lt;strong&gt;向人类解释多智能体的团队决策逻辑&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;高效的世界建模&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;利用 RWKV 长序列处理能力&lt;strong&gt;分析市场数据&lt;/strong&gt;，实现金融时序预测&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;RWKV 4/5/6/7 的纯 Jax 实现&lt;/strong&gt;，实现更高效训练和推理&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;此外，作者认为 &lt;strong&gt;RWKV 的 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Frwkv.cn%2FRWKV-Fine-Tuning%2FState-Tuning&quot; target=&quot;_blank&quot;&gt;state tuning&lt;/a&gt; 在多智能体的研究上拥有极大的优势。&lt;/strong&gt; 通过切换 state 来改变智能体的「基因」，远比切换模型、切换 LoRA 等方式更方便、更无缝。&lt;/p&gt; 
&lt;h2&gt;RWKV 端侧部署优化&lt;/h2&gt; 
&lt;p&gt;UVa（弗吉尼亚大学） 团队提出了 &lt;strong&gt;RWKV-Lite&lt;/strong&gt;，一套从模型架构优化到后训练压缩的&lt;strong&gt;高效 RWKV 模型压缩技术&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;在保持&lt;strong&gt;模型准确率&lt;/strong&gt;基本不变的情况下，RWKV-Lite 将内存占用降低了 &lt;strong&gt;3.4 – 5 倍&lt;/strong&gt;；若结合量化，整体内存需求甚至可降低 &lt;strong&gt;10&lt;/strong&gt; 倍。与此同时，该方法带来的计算开销微乎其微，非常适合边缘部署。&lt;/p&gt; 
&lt;p&gt;该论文已被机器学习顶会 ICML 2024 收录。论文地址：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fhtml%2F2412.10856v3&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/html/2412.10856v3&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-f08360bb6b5f456bbe5d984a3838d621393.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;RWKV-Lite 的压缩方向大致有以下三点：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;低秩近似（Low-Rank Approximation）&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;针对 RWKV 块中的投影权重矩阵（如 &lt;code&gt;channel-mix&lt;/code&gt; 和 &lt;code&gt;time-mix&lt;/code&gt; 层），通过 &lt;strong&gt;奇异值分解（SVD）&lt;/strong&gt; 将大型矩阵拆分为两个低秩矩阵，减少参数量的同时保留关键信息。&lt;/p&gt; 
&lt;p&gt;实验显示，低秩压缩可实现 4 倍参数压缩，且可以通过持续训练（Continual Training）恢复精度损失。&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;稀疏性利用（Sparsity Exploitation）&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;发现 RWKV 的 FFN 层存在显著稀疏性（67%-83% 的神经元激活率为零），提出混合预测器（MLP + 1-bit 量化）动态加载关键神经元权重，减少推理时内存占用。&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;嵌入缓存与分层分类头&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt;嵌入缓存：通过缓存高频词嵌入，减少对大型嵌入层的依赖&lt;/li&gt; 
 &lt;li&gt;分层分类头：将词汇表聚类，仅加载与当前预测相关的词权重&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;下图是论文演示在一个可穿戴设备上运行压缩后的 RWKV 模型（带可视化屏幕），开发板型号为 Orange Pi Zero 2W，板载 CPU 1.5GHz 4x Cortex-A53，内存 4GB 。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-2fa17ed229ff21b66bd05d7600daba391fc.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;欢迎开展 RWKV 学术研究&lt;/h2&gt; 
&lt;p&gt;我们欢迎大家基于最新最强的 RWKV-7 架构开展学术研究！&lt;/p&gt; 
&lt;p&gt;最新发布的 RWKV-7 2.9B 模型在各类评测中表现出色，其英文和多语言能力显著超越所有同尺寸模型（英文评测 71.1%，多语言评测 62.3%），超越了包括 Llama 3.2 3B（英文评测 68.7%，多语言评测 57.3%）、Qwen2.5 3B（英文评测 68.6%，多语言评测 57.0%）等知名优秀开源模型。&lt;/p&gt; 
&lt;p&gt;此外，我们为 RWKV 学术研究&lt;strong&gt;提供全面的支持和激励&lt;/strong&gt;，包括：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;力所能及的&lt;strong&gt;技术支持&lt;/strong&gt;和&lt;strong&gt;算力支持&lt;/strong&gt;，具体支持请在公众号内发消息联系我们沟通&lt;/li&gt; 
 &lt;li&gt;对 RWKV 学术研究提供&lt;strong&gt;生态奖金&lt;/strong&gt;，详情查看：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FUeKemVw9HnTU6FBL1iM0bg&quot; target=&quot;_blank&quot;&gt;RWKV 2025 生态内容征集大赛 &lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;微软已经将 RWKV 全面引入 Windows 10/11 系统&lt;/strong&gt;，足以证明 RWKV 的端侧优势。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-a4517caf796e82678e9381efeae1b07dd15.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334597</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334597</guid>
            <pubDate>Fri, 07 Feb 2025 09:31:00 GMT</pubDate>
            <author>来源: 投稿</author>
        </item>
        <item>
            <title>2025 年 xbatis 诞生了，它是一款超级好用的基于 mybatis 的 ORM 框架，没有之一！！！</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;div&gt; 
 &lt;h1&gt;官网：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fxbatis.cn&quot; target=&quot;_blank&quot;&gt;https://xbatis.cn&lt;/a&gt;&lt;/h1&gt; 
 &lt;h1&gt;xbatis 是什么&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fxbatis.cn%2Fzh-CN%2Fintro%2FwhatIs.html%23xbatis-%25E6%2598%25AF%25E4%25BB%2580%25E4%25B9%2588&quot; target=&quot;_blank&quot;&gt;&lt;/a&gt;&lt;/h1&gt; 
 &lt;p style=&quot;margin-left:0; margin-right:0&quot;&gt;xbatis 是一款基于 mybatis 的 ORM 框架，ORM 程度非常高，几乎不需要再写 SQL;&lt;br&gt; &lt;br&gt; 同时内置多种&lt;strong&gt;数据库函数&lt;/strong&gt;,具有良好的不同数据库迁移能力,注意它可以&lt;strong&gt;同时支持多种数据库！！！&lt;/strong&gt;，一款真正意义上的 ORM 框架&lt;br&gt; &lt;br&gt; xbatis 具有良好程序设计，非常稳定（经过 testcase 验证）；优雅的 API、简而易懂的方法操作，让你写代码和写 SQL 几乎一样，学习成本几乎为零。&lt;br&gt; &lt;br&gt; 功能强大，支持&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;strong&gt;多表/子查询,自动分页，优雅的 XML 自动分页&lt;/strong&gt;等众多功能！！&lt;/p&gt; 
 &lt;h1&gt;特征&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fxbatis.cn%2Fzh-CN%2Fintro%2FwhatIs.html%23%25E7%2589%25B9%25E5%25BE%2581&quot; target=&quot;_blank&quot;&gt;&lt;/a&gt;&lt;/h1&gt; 
 &lt;h2&gt;1、很轻量,非常轻量&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fxbatis.cn%2Fzh-CN%2Fintro%2FwhatIs.html%23_1%25E3%2580%2581%25E5%25BE%2588%25E8%25BD%25BB%25E9%2587%258F-%25E9%259D%259E%25E5%25B8%25B8%25E8%25BD%25BB%25E9%2587%258F&quot; target=&quot;_blank&quot;&gt;&lt;/a&gt;&lt;/h2&gt; 
 &lt;p style=&quot;margin-left:0; margin-right:0&quot;&gt;轻量级封装 mybatis。 其他框架都比较深度修改了 mybatis 源码。&lt;/p&gt; 
 &lt;h2&gt;2、高性能&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fxbatis.cn%2Fzh-CN%2Fintro%2FwhatIs.html%23_2%25E3%2580%2581%25E9%25AB%2598%25E6%2580%25A7%25E8%2583%25BD&quot; target=&quot;_blank&quot;&gt;&lt;/a&gt;&lt;/h2&gt; 
 &lt;p style=&quot;margin-left:0; margin-right:0&quot;&gt;对比其他 mybatis 框架，性能不差，接近最优。&lt;/p&gt; 
 &lt;h2&gt;3、灵活方便&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fxbatis.cn%2Fzh-CN%2Fintro%2FwhatIs.html%23_3%25E3%2580%2581%25E7%2581%25B5%25E6%25B4%25BB%25E6%2596%25B9%25E4%25BE%25BF&quot; target=&quot;_blank&quot;&gt;&lt;/a&gt;&lt;/h2&gt; 
 &lt;p style=&quot;margin-left:0; margin-right:0&quot;&gt;高度实现 ORM，查询 API 零学习成本。&lt;/p&gt; 
 &lt;h2&gt;4、高可用&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fxbatis.cn%2Fzh-CN%2Fintro%2FwhatIs.html%23_4%25E3%2580%2581%25E9%25AB%2598%25E5%258F%25AF%25E7%2594%25A8&quot; target=&quot;_blank&quot;&gt;&lt;/a&gt;&lt;/h2&gt; 
 &lt;p style=&quot;margin-left:0; margin-right:0&quot;&gt;可应付 90% 的 SQL 需求。&lt;/p&gt; 
 &lt;h2&gt;5、可靠，安全&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fxbatis.cn%2Fzh-CN%2Fintro%2FwhatIs.html%23_5%25E3%2580%2581%25E5%258F%25AF%25E9%259D%25A0-%25E5%25AE%2589%25E5%2585%25A8&quot; target=&quot;_blank&quot;&gt;&lt;/a&gt;&lt;/h2&gt; 
 &lt;p style=&quot;margin-left:0; margin-right:0&quot;&gt;没有过于复杂的设计，但是 API 却很丰富，足够使用！&lt;br&gt; 其他框架或多或少设计的过于复杂，反而容易出现各种问题。&lt;/p&gt; 
 &lt;h2&gt;6、优秀的分页和 SQL 优化能力&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fxbatis.cn%2Fzh-CN%2Fintro%2FwhatIs.html%23_6%25E3%2580%2581%25E4%25BC%2598%25E7%25A7%2580%25E7%259A%2584%25E5%2588%2586%25E9%25A1%25B5%25E5%2592%258Csql%25E4%25BC%2598%25E5%258C%2596%25E8%2583%25BD%25E5%258A%259B&quot; target=&quot;_blank&quot;&gt;&lt;/a&gt;&lt;/h2&gt; 
 &lt;p style=&quot;margin-left:0; margin-right:0&quot;&gt;自动过滤多余的 left join 、count 查询，自动去除 order by ，无效的 left join&lt;br&gt; 以及 select 部分替换成 select count(&lt;em&gt;) 或 select 1 后，在 select count(&lt;/em&gt;)&lt;br&gt; &lt;br&gt; 内置分页功能，超级牛逼！&lt;/p&gt; 
&lt;/div&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/334593</link>
            <guid isPermaLink="false">https://www.oschina.net/news/334593</guid>
            <pubDate>Fri, 07 Feb 2025 09:14:00 GMT</pubDate>
            <author>来源: 投稿</author>
        </item>
    </channel>
</rss>