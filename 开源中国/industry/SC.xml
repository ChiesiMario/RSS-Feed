<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>开源中国-综合资讯</title>
        <link>https://www.oschina.net/news/industry</link>
        <atom:link href="http://8.134.148.166:30044/oschina/news/industry" rel="self" type="application/rss+xml"></atom:link>
        <description>开源中国-综合资讯 - Powered by RSSHub</description>
        <generator>RSSHub</generator>
        <webMaster>contact@rsshub.app (RSSHub)</webMaster>
        <language>en</language>
        <lastBuildDate>Mon, 21 Apr 2025 21:36:17 GMT</lastBuildDate>
        <ttl>5</ttl>
        <item>
            <title>预计中国市场 2025 年人形机器人本体产值将超 45 亿元</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;根据 TrendForce 集邦咨询最新数据，中国市场已有 11 家主流人形机器人本体厂商启动 2024 年量产计划。其中，宇树科技、优必选、智元机器人、银河通用、众擎机器人、乐聚机器人等 6 家领先企业更是将 2025 年的量产规划设定在千台以上。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style=&quot;color:#242424; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;TrendForce 集邦咨询预计，2025 年中国市场人形机器人本体产值有望突破 45 亿元人民币。加上马斯克关于 Tesla Optimus 2025 年数千台量产目标，预计头部本体厂商的量产计划将拉动中国市场人形机器人零部件供应链生态布局与完整性。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#242424; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;img height=&quot;336&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-545b82ebd4c04ad7f2c9a719209e9659c66.png&quot; width=&quot;700&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#242424; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span style=&quot;background-color:#ffffff; color:rgba(0, 0, 0, 0.9)&quot;&gt;当下人形机器人产品主要应用在 B 端工业场景、高校科研以及少部分 B 端商用场景，而 C 端家用场景要求人形机器人功能多元，对机器人数据处理和自主交互能力要求较高。人形机器人从 B 端跨越到 C 端应用场景需要政策、法规、技术等行业多方面的共同努力，C 端应用场景的商业化落地仍任重道远。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/345784</link>
            <guid isPermaLink="false">https://www.oschina.net/news/345784</guid>
            <pubDate>Sun, 13 Apr 2025 09:43:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>得物自研 DGraph4.0 推荐核心引擎升级之路</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                    
                                                                                                                                                    &lt;span id=&quot;OSC_h1_1&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;一、前言&lt;/h1&gt; 
&lt;p&gt;DGraph 是得物自主研发的新一代推荐系统核心引擎，基于 C++语言构建，自 2021 年启动以来，经过持续迭代已全面支撑得物社区内容分发、电商交易等核心业务的推荐场景。DGraph 在推荐链路中主要承担数据海选和粗排序功能，为上层精排提供高质量候选集。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;核心技术特性：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;索引层 - 支持 KV（键值）、KVV（键-多值）、INVERT（倒排）、DENSE-KV（稠密键值）等。&lt;strong&gt;索引存储&lt;/strong&gt;支持磁盘 &amp;amp; 内存两种模式，在预发等延迟压力低场景，通过磁盘索引使用低规格服务器提供基本服务。线上场景使用内存索引保证服务稳定性，提供毫秒级延迟响应。&lt;strong&gt;索引更新&lt;/strong&gt;支持双 buff 热更新【内存足够】、服务下线滚动更新【内存受限】、Kafka 流式数据实时更新等三种模式。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;查询层 - 支持向量检索 IVF &amp;amp; HNSW、键值 (KV) 查询、倒排检索、X2I 关联查询、图查询。对外提供 JavaSDK &amp;amp; C++ SDK。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;系统依赖架构：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;索引全生命周期管理由得物索引平台 DIP 统一管控。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;服务发现基于 ZooKeeper(zk)。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;集群资源调度基于得物容器平台，目前已经支持 HPA。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;服务规模：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;目前在线 100+集群，2024 年双 11 在线突破了 100W qps。&lt;/p&gt; 
&lt;p&gt;本文主要介绍 DGraph 系统在 2024 年的一些重要改进点。主要包括两次架构调整 + 性能优化 + 用户体验提升方面的一些工作。&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_2&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;二、架构升级&lt;/h1&gt; 
&lt;span id=&quot;OSC_h2_3&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;2.1 垂直拆分业务集群支持&lt;/h2&gt; 
&lt;p&gt;在 2023 年前，DGraph 系统始终采用单一集群架构提供服务。该架构模式在平台发展初期展现出良好的经济性和运维便利性，但随着业务规模扩张，单集群架构在系统层面逐渐显露出三重刚性约束：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;存储容量瓶颈 - 单节点内存上限导致数据规模受限；&lt;/li&gt; 
 &lt;li&gt;网络带宽瓶颈 - 单物理机 Pod 共享 10Gbps 带宽，实际可用带宽持续承压，推荐引擎业务中部分核心集群 200 余张数据表（单表需 20 分钟级更新）的实时处理需求已遭遇传输瓶颈；&lt;/li&gt; 
 &lt;li&gt;计算能力瓶颈 - 单实例最大 64 核的算力天花板，难以支撑复杂策略的快速迭代，核心场景响应时效与算法复杂度形成显著冲突；&lt;/li&gt; 
 &lt;li&gt;稳定性 - 大规格集群对于容器调度平台不友好，在扩容、集群故障、集群发布时耗时较久；基于得物平台推荐数据量增长和算法迭代需求，我们实施业务垂直拆分的多集群架构升级，通过资源解耦与负载分离，有效突破了单节点资源约束，为复杂算法策略的部署预留出充足的技术演进空间。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;系统改进点是在 DGraph 中增加了访问了其他 DGraph 集群 &amp;amp; FeatureStore 特征集群的能力 (图 1)。为了成本考虑，我们复用了之前系统的传输协议 flatbuffers，服务发现仍基于 ZooKeeper。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-c817ffca6dd3d7ecea36fdd95d20183d04a.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;图 1 DGraph 访问架构改进&lt;/p&gt; 
&lt;p&gt;改造的难点在图化集群！&lt;/p&gt; 
&lt;p&gt;目前推荐业务的核心场景都进行了图化改造，图化查询是把多路召回、打散、融合、粗排等策略打包到一个 DAG 图中一次发送到 DGraph，DGraph 的算子调度模块根据 DAG 的描述查询索引数据 &amp;amp; 执行算子最终把结果返回给业务系统，但这些 DAG 图规模都很大，部分业务 DAG 图涉及 300+算子，因此如何在垂直拆分业务中把这些 DAG 图拆分到不同的 DGraph 集群中是一个非常复杂的问题，我们主要做了三方面改进：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;DAG 管理 - 集群分主集群和从集群【多个】，DAG 图部署在存在主集群中，DIP 平台会分析 DAG 的拓步结构并把属于从集群的部分复制出来分发给从集群，为了保证 DAG 的一致性，只允许从主集群修改 DAG 图；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;集群划分 - 通常按召回划分，比如 Embedding 召回、X2I 召回、实验召回可以分别部署在不同的集群，另外也可以把粗排等算力需求大的部分单独放在一个集群，具体根据业务场景调整；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;性能优化 - 核心表多个集群存放，减少主集群和从集群间数据交换量。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-ed9ac500b1e0e379e07f0f870f4f6ae62bb.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;图 2 DGraph 业务垂直拆分集群&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_4&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;2.2 分布式能力支持&lt;/h2&gt; 
&lt;p&gt;垂直拆分集群，虽然把推荐 N 路召回分散到了 M 个集群，但是每个集群中每个表依然是全量。随着得物业务的发展，扩类目、扩商品，部分业务单表的数据量级已经接近单集群的存储瓶颈。因此需要 DGraph 中引入数据水平拆分的能力。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-3da38581237c44ef46782fe58c5c0566ed6.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;图 3 DGraph 分布式集群架构图&lt;/p&gt; 
&lt;p&gt;在 DGraph 分布式架构设计中，重点考虑了部署成本优化与业务迁移工作量：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;分布式集群采用【分片数 2】×【双活节点 2】×【数据副本数 2】的最小拓扑结构，理论上需要 8 台物理节点保障滚动更新与异常容灾时的稳定性。针对 CPU 负载较轻的场景，为避免独立 Proxy 集群带来的额外资源开销，DGraph 将 Proxy 模块和 DGraph 引擎以对称架构部署到所有节点，通过本地优先的智能路由策略（本地节点轮询优先于跨节点访问）实现资源利用率与访问效率的平衡；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;在业务兼容性方面，基础查询接口（KV 检索、倒排索引、X2I 关联查询）保持完全兼容以降低迁移成本，而 DAG 图查询需业务侧在查询链路中明确指定 Proxy 聚合算子的位置以发挥分布式性能优势。数据链路层面，通过 DIP 平台实现索引无缝适配，支持 DataWorks 原有任务无需改造即可对接分布式集群，同时增量处理模块内置分片过滤机制，可直接复用现有 Flink 实时计算集群进行数据同步。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;span id=&quot;OSC_h1_5&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;三、性能优化&lt;/h1&gt; 
&lt;span id=&quot;OSC_h2_6&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;3.1 算子执行框架优化&lt;/h2&gt; 
&lt;p&gt;在 DGraph 中，基于 DGraph DAG 图 (参考图 9) 的一次查询就是图查询，内部简称 graphSearch。在一个 DAG 图中，每个节点都是一个算子 (简称 Op)，算子通过有向边连接其他算子，构成一个有向无环图，算子执行引擎按 DAG 描述的关系选择串行或者并发执行所有算子，通过组合不同算子 DAG 图能在推荐场景中灵活高效的完成各种复杂任务。&lt;/p&gt; 
&lt;p&gt;在实际应用场景中受 DAG 图规模 &amp;amp; 超时时间 (需要控制在 100ms 内) 限制，算子执行框架的效率非常重要。在最开始的版本中我们使用过 Omp &amp;amp; 单队列线程池，集群在 CPU 负载低于 30% 时表现尚可，但在集群 CPU 负载超过 30% 后，rt99 表现糟糕。在降本增效的背景下，我们重点对算子执行框架进行了优化，引入了更高效的线程池 &amp;amp; 减少了调度过程中锁的使用。优化后目前 DGraph 在 CPU 压力超过 60% 依然可以提供稳定服务。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-314750530e377031be7838774eae9b7fecd.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;图 4 DGraph 算子执行框架优化&lt;/p&gt; 
&lt;p&gt;线程池优化：将原 1:N 的线程池-队列架构调整为 M:N 分组模式。具体实现为将 N 个工作线程划分为 M 个执行组（每组 N/M 线程），各组配备独立任务队列。任务提交采用轮询分发机制至对应组队列，通过资源分区有效降低线程调度时的锁竞争强度。&lt;/p&gt; 
&lt;p&gt;调度器优化：在 DAG 调度过程中存在两个典型多写场景&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;前驱算子节点完成时需并行更新后继节点标记；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;DAG 全局任务计数器归零判断。原方案通过全局锁（Graph 锁+Node 锁）保障原子性，但在高负载场景引发显著锁竞争开销，影响线程执行效率。经分析发现这两个状态变更操作符合特定并发模式：所有写操作均为单调增减操作，因此可将锁机制替换为原子变量操作。针对状态标记和任务计数场景，分别采用原子变量的 FetchAdd 和 FetchSub 指令即可实现无锁化同步，无需引入 CAS 机制即满足线程安全要求。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;span id=&quot;OSC_h2_7&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;3.2 传输协议编码解码优化&lt;/h2&gt; 
&lt;p&gt;优化 JavaSDK - DGraph 数据传输过程：在 DGraph 部分场景，由于请求引擎返回的数据量很大，解码编码耗时占整个请求 20% 以上。分析已有的解码编码模块，引擎在编码阶段会把待传输数据编码到一个 FlatBuffer 中，然后通过 rpc 协议发送到业务侧的 JavaSDK，sdk 解码 FlatBuffer 封装成 List&amp;lt;map&amp;gt; 返回给业务代码，业务代码再把 List&amp;lt;map&amp;gt; 转化成 List&amp;lt;业务 Object&amp;gt;。过程中没有并发 &amp;amp; sdk 侧多了一层冗余转换。&lt;/p&gt; 
&lt;p&gt;优化方案如下：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;串行编码调整为根据文档数量动态调整编码块数量。各子编码块可以并发编码解码，加快编码&amp;amp;解码速度，提升整体传输性能；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;sdk 侧由 Doc -&amp;gt; Map -&amp;gt; JavaObject 的转化方式调整为 Doc -&amp;gt; JavaObject，减少解码端算力开销。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-781cc5f8b62293731a12f4578bddbafd4b1.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;图 5 DGraph 传输编码解码过程优化&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_8&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;四、用户体验优化&lt;/h1&gt; 
&lt;span id=&quot;OSC_h2_9&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;4.1 DAG 图调试功能优化&lt;/h2&gt; 
&lt;p&gt;目前我们已经把 DGraph DAG 图查询的调试能力集成到 DIP 平台。其原理是：DGraph 的算子基类实现了执行结果输出，由于算子的中间结果数据量极大，当调试模块发现调试标志后会先把当前算子的中间结果写入日志中，数据按 TraceID + DAGID+ NodeID 组织，最终这些数据被采集到 SLS 日志平台。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-2d918720f61f5f0f7d019c68f05998aecca.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;图 6 DGraph DAG 图查询调试&lt;/p&gt; 
&lt;p&gt;从 DIP 平台调试 DAG 图请求，首先通过 DGraph JavaSDK 的调试入口拿到 DAG 图请求 json，填入 DIP 平台图请求调试入口，发起请求。索引平台会根据请求体自动关联 DAG 图并结合最终执行结果通过页面的方式展示。DIP 平台拿到结果后，在 DAG 图中成功的算子节点标记为绿色，失败的节点标记为红色 (图 6)。点击任意节点可以跳转到日志平台查看该节点的中间结果输出。可用于分析 DAG 图执行过程中的各种细节，提升业务排查业务问题效率。&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_10&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;4.2 DAG 图支持 TimeLine 分析&lt;/h2&gt; 
&lt;p&gt;基于 Chrome 浏览器中的 TimeLine 构建，用于 DGraph DAG 图查询时算子性能分析优化工作。TimeLine 功能集成在算子基类中，启动时会记录每个算子的启动时间、等待时间、完成时间、执行线程 pid 等信息，这些信息首先输出到日志，然后被 SLS 日志平台采集。用户可以使用查询时的 TraceID 在日志平台搜索相关的 TimeLine 信息。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-8775652c29620ccf9e0595b35dd3c9cebd3.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;图 7 DGraph DAG 图例子&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-6a92e653aa398223b8be061bf6a425c268d.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;图 8 使用浏览器查看 DGraph DAG 图 TimeLine&lt;/p&gt; 
&lt;p&gt;当我们拿到请求的 TimeLine 信息后，通过浏览器加载可以通过图形化的方式分析 DAG 执行过程中耗时分布。图 7 是一个 DAG 请求，它有 9 个算子节点，图 8 是它的一次请求的 TimeLine。通过分析这些算子的耗时，可以帮助我们定位当前 DAG 图查询的瓶颈点在哪里，从而精准去解决性能方面的问题。&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_11&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;4.3 DAG 图支持动态子图&lt;/h2&gt; 
&lt;p&gt;在 DAG 图召回中，业务的召回通常都带有一些固定模式，比如一个业务在一个 DAG 图召回中有 N 路召回，每一路召回都是：① 查找数据；② 关联可推池；③ 打散； 它们之间的区别可能仅仅是召回数据表名不同或者传递的参数不同。通常我们业务调整或者算法实验调整只需要增加或者减少部分召回，原有模式下这些操作需要去新增或者修改 DAG 图，加上算法实验很多，业务维护 DAG 图的成本会非常高。&lt;/p&gt; 
&lt;p&gt;DAG 动态子图的引入就是为了解决这类问题，首先我们在 DAG 图中配置一个模板子图，它仅仅描述一个行为模式，代表会涉及几个算子，算子之间的关系如何，实际的参数以及召回路的数量则由业务方在发起请求时动态决定。子图的执行和主图的执行共用同一套调度框架，共享运行时资源以降低运行开销。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-08756615857b6bf3b35868c2f1b8d772291.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;图 9 DGraph 子图&lt;/p&gt; 
&lt;p&gt;图 9 是一个 DAG 召回使用 DAG 子图后的变化，它有 8 路召回，一个 Merge 节点，这些召回分为两类，一类是基于 KV 表 (ForwardSearch) 触发的向量召回，另外一类是基于 KVV 表 (IvtSearch) 触发的向量召回。引入 DAG 子图后，在主图中节点数量由 17 个降为 3 个。&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_12&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;五、展望未来&lt;/h1&gt; 
&lt;p&gt;过去四年，DGraph 聚焦于实现得物推荐引擎体系从 0 到 1 的突破，重点完成了核心系统架构搭建、算法策略支持及业务迭代空间拓展，取得多项基础性成果。基于 2024 年底的用户调研反馈结合 DGraph 当前的发展，后续将重点提升产品易用性、开发与运维效能及用户体验，同时在系统稳定性、可扩展架构和平台化建设方面持续深化。&lt;/p&gt; 
&lt;p&gt;算法团队大量 HC，欢迎加入我们：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzkxNTE3ODU0NA%3D%3D%26mid%3D2247537831%26idx%3D1%26sn%3Ddb1464cd87a75dd8f7bcf512fb50bf70%26scene%3D21%23wechat_redirect&quot; target=&quot;_blank&quot;&gt;得物技术大量算法岗位多地上线，「职」等你来！&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;文 / 寻风&lt;/p&gt; 
&lt;p&gt;关注得物技术，每周一、三更新技术干货&lt;/p&gt; 
&lt;p&gt;要是觉得文章对你有帮助的话，欢迎评论转发点赞～&lt;/p&gt; 
&lt;p&gt;未经得物技术许可严禁转载，否则依法追究法律责任。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/u/5783135/blog/18181570</link>
            <guid isPermaLink="false">https://my.oschina.net/u/5783135/blog/18181570</guid>
            <pubDate>Sun, 13 Apr 2025 09:35:00 GMT</pubDate>
            <author>原创</author>
        </item>
        <item>
            <title>字节系 Agent 产品 —— 扣子空间 (Coze Space) 开启内测</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;4 月 18 日晚间，字节系 Agent 产品 —— 扣子空间 (Coze Space) &lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FJlWyPmOwIYTXUD7tCvS1lg&quot; target=&quot;_blank&quot;&gt;宣布&lt;/a&gt;&lt;/u&gt;开启内测，定位通用 Agent。与其他类似产品如 manus 一样，扣子空间采用了邀请码制。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0421/173325_Er29_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0421/173418_u2N4_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;扣子空间有什么特点？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;🌟从回答问题，到解决问题，让 Agent 帮你完成更多的工作&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;自动分析需求，拆解为多个子任务&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;自主调用工具（浏览器、代码编辑器等），执行任务&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;输出完整的结果报告，例如网页、PPT 、飞书文档等&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;🌟&lt;strong&gt;专家 Agent 生态，让更专业 Agent 来为你提供服务&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;「华泰 A 股观察助手」可以为你进行每日早报生成；针对股票分析问题，助手也能可以为你答疑解惑&lt;br&gt; &lt;img height=&quot;567&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0421/173807_CWme_2720166.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;「用户研究专家 」可以协助你进行用研资料深度分析，省时省力地助你获取更多用户洞察&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;🌟&lt;strong&gt;探索/规划双模式，更好地和 Agent 一起协作完成高难度任务&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;探索模式：让 AI 自主动态探索，完成速度更快&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;规划模式：让 AI 深度思考，适合高复杂性任务&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;🌟&lt;strong&gt;MCP 扩展集成，无限拓展 Agent 能力边界&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;首批官方支持飞书多维表格、高德地图、图像工具、语音合成等 MCP&lt;br&gt; &lt;img height=&quot;566&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0421/173727_B7jG_2720166.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;即将支持「扣子开发平台」发布 MCP 至「扣子空间」&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;扣子空间官网：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fspace.coze.cn%2F&quot; target=&quot;_blank&quot;&gt;https://space.coze.cn/&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/345779/coze-space-preview</link>
            <guid isPermaLink="false">https://www.oschina.net/news/345779/coze-space-preview</guid>
            <pubDate>Sun, 13 Apr 2025 09:34:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>Meta 旗下 APP 禁用苹果 Apple Intelligence</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;外媒报道称，苹果最新推出的 Apple Intelligence 功能在 Meta 旗下应用（包括 Facebook、Instagram、WhatsApp 和 Threads）中遭到禁用，用户无法使用其核心功能，如写作工具 (Writing Tools) 和自定义表情符号生成器 (Genmoji)。此举被认为与 Meta 推动自家 Meta AI 工具的战略有关。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;223&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-2e17e1e4324f772634e33ac142954924659.png&quot; width=&quot;300&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Apple Intelligence 是苹果于 2024 年随 iOS18 推出的 AI 功能套件，旨在通过智能写作、图像生成和个性化体验提升用户生产力。其中，写作工具可实现文本校对、改写和总结，Genmoji 则允许用户生成定制化表情符号。这些功能通常通过长按 iOS 文本输入框激活，理论上适用于大多数应用。然而，Meta 旗下应用已明确禁用这些功能。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;自 2024 年 12 月起，Meta 开始逐步移除对 Apple Intelligence 的支持。用户在 Facebook、Instagram、WhatsApp 和 Threads 中无法调用写作工具或 Genmoji，甚至此前可在 Instagram Stories 中使用的 Memoji 和键盘贴纸功能也被移除。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;相比之下，X（原 Twitter）、Bluesky 和 Signal 等第三方应用仍支持 Apple Intelligence 的写作工具。值得注意的是，Apple Intelligence 在浏览器版本的 Meta 服务中仍可正常使用，因为浏览器环境不受 Meta 应用的限制。苹果开发者文档显示，iOS 和 iPadOS 应用需主动选择启用 Apple Intelligence 功能，而 Meta 显然选择了禁用。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Meta 并未公开解释禁用 Apple Intelligence 的原因，但业内普遍认为，此举旨在推广其自研的 Meta AI 工具。Meta AI 基于 Llama 模型，已深度整合至 Facebook、Instagram、WhatsApp 和 Threads，提供文本生成、图像创作和搜索增强等功能。例如，在 Instagram 中，用户尝试编辑文本时，会看到「Write with AI」选项，引导至 Meta AI 界面，而非 Apple Intelligence。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/345768</link>
            <guid isPermaLink="false">https://www.oschina.net/news/345768</guid>
            <pubDate>Sun, 13 Apr 2025 08:47:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>OpenAI CEO：对 GPT 说谢谢会带来千万开销</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;OpenAI CEO Sam Altman 近日在社交媒体回复网友称，OpenAI 仅仅为了处理用户日常的寒暄和礼貌性交流，就需要花费「数千万美元」。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-fd16677fc16d984d5d09457cb0d9c467367.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;据悉，「谢谢」「请」，这些看似微不足道的礼貌用语，虽然在情感上让用户与 AI 的互动显得更有「人味」，但其背后却带来高昂的能源消耗。&lt;/p&gt; 
&lt;p&gt;最新报告指出，即使是像「不客气」这样的简短回复，大型语言模型（LLM）也需要消耗大约 40-50 毫升的水。&lt;/p&gt; 
&lt;p&gt;虽然礼貌的用语会增加 OpenAI 每月的资源支出，但该公司似乎并不介意。目前，许多用户已经不再将 AI 看作一个冷冰冰的工具，而是能够带情感化交流的「虚拟用户」。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-c2a9253b61a8988e0b8eb110384ab80d1b2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;而据 OpenAI 和麻省理工学院的研究人员指出，随着 AI 对话越来越难以与人类对话区分开来，用户可能会对 AI 聊天机器人产生情感依赖，甚至出现成瘾的情况。而这种成瘾可能会导致用户在离开 AI 时出现类似戒断反应的症状。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/345767</link>
            <guid isPermaLink="false">https://www.oschina.net/news/345767</guid>
            <pubDate>Sun, 13 Apr 2025 08:36:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>Intel 开源专为本地生成式 AI 设计的 AI Playground</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Intel 近日宣布，其专为本地生成式 AI 设计的 AI Playground 软件正式开源，为 Intel Arc GPU 用户提供了一个强大的 AI 模型运行平台。AI Playground 支持多种图像、视频生成模型以及大型语言模型（LLMs），通过优化本地计算资源，显著降低了 AI 应用的硬件门槛。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;401&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-0f8a10ae7e6da81cf50fdc5a2b76750462c.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color:#000000&quot;&gt;核心功能：多模态 AI 模型一站式支持&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;AI Playground 作为一款用户友好的「AI 中心」，集成了丰富的生成式 AI 功能，涵盖图像生成、图像风格化、文本生成与聊天机器人等场景。AIbase 梳理了其支持的模型与功能:&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;图像与视频生成：支持 Stable Diffusion1.5、SDXL、Flux.1-Schnell 和 LTX-Video 模型，可实现文本到图像、图像风格化以及文本到视频生成，生成结果在分辨率与细节上表现出色。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;大型语言模型：兼容 Safetensor PyTorch 格式的 DeepSeek R1、Phi3、Qwen2、Mistral，以及 GGUF 格式的 Llama3.1、Llama3.2，结合 OpenVINO 优化的 TinyLlama、Mistral7B、Phi3mini 和 Phi3.5mini，提供高效的本地聊天与推理能力。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;ComfyUI 工作流：通过集成 ComfyUI，AI Playground 支持高级图像生成工作流，如 Line to Photo HD 与 Face Swap，提升创作灵活性。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;AI Playground 不直接附带模型，用户需从 Hugging Face 或 CivitAI 下载模型并放置于指定文件夹，平台提供直观的模型加载界面，确保操作简便。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color:#000000&quot;&gt;技术架构：OpenVINO 优化本地性能&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;AI Playground 基于 Intel 的 OpenVINO 框架，针对 Arc GPU 与 Core Ultra 处理器进行了深度优化。AIbase 分析，其关键技术包括:&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;OpenVINO 加速：为聊天与图像生成提供高效推理支持，显著提升低 vRAM 设备（如 8GB Arc GPU）的性能。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;Llama.cpp 与 GGUF 支持：实验性后端扩展了 GGUF 模型的兼容性，预填充模型列表简化用户配置。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;模块化设计：通过 Add Model 功能，用户可直接输入 Hugging Face 模型 ID 或本地路径，灵活加载自定义模型。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;硬件要求方面，AI Playground 支持 Intel Core Ultra-H/V 处理器或 Arc A/B 系列 GPU（最低 8GB vRAM）。尽管为开源 Beta 版，Intel 提供了详细的故障排查指南，确保用户快速上手。低 vRAM 设备在运行 SDXL 等高分辨率模型时可能速度较慢，建议优先使用 Flux.1-Schnell 等轻量化模型。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/345762</link>
            <guid isPermaLink="false">https://www.oschina.net/news/345762</guid>
            <pubDate>Sun, 13 Apr 2025 08:25:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>【直播】如何让 AI 「跑得快」 又 「用得好」？</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                    
                                                                                                                                                    &lt;div&gt; 
 &lt;p style=&quot;text-align:left&quot;&gt;当前，人工智能技术正加速向大模型时代迈进，在政务、金融、医疗、工业等领域展现出颠覆性潜力。然而，大模型的训练与部署面临算力成本高、技术生态依赖性强、行业落地门槛高三大挑战。在此背景下，升腾与国产大模型的深度结合，为破解上述瓶颈提供了新路径。&lt;/p&gt; 
 &lt;p style=&quot;text-align:left&quot;&gt;为加速技术普惠，&lt;span style=&quot;color:#2980b9&quot;&gt;4 月 23 日晚&lt;/span&gt;，开源中国直播栏目《数智漫谈》邀请&lt;span style=&quot;color:#2980b9&quot;&gt;升腾生态技术专家与行业先行者&lt;/span&gt;，分享一线开发经验，聊一聊升腾结合大模型，如何促进创新，助力开发者与企业用户抓住国产 AI 新红利。&lt;/p&gt; 
 &lt;p style=&quot;text-align:left&quot;&gt;🌟&lt;strong&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;演讲议题 1：升腾插件化接入 vLLM 加速大模型推理创新最佳实践&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p style=&quot;text-align:left&quot;&gt;&lt;strong&gt;演讲专家：&lt;/strong&gt;姚圣伟，华为云 HCDE、微软 Insider Dev Tour China&lt;/p&gt; 
 &lt;p style=&quot;text-align:left&quot;&gt;&lt;strong&gt;议题简介：&lt;/strong&gt;随着大模型技术的快速发展，如何高效部署与加速推理成为业界核心挑战。基于自主研发的升腾处理器及 CANN 异构计算架构，升腾推出插件化接入方案，与开源推理框架 vLLM 深度适配，为大模型推理提供高性能、低时延的创新实践。用户可以实现自己的 Woker、ModelRunner、Attention、Communicator 以及自定义算子。在进一步促进 vLLM 多样性发展的同时，尽可能的解决了兼容性、可维护性的问题。实践案例覆盖自然语言处理、多模态交互等场景，验证了升腾生态的开放性与技术普适性，为行业提供可复用的国产化大模型部署范式，推动 AI 基础设施高效进化。&lt;/p&gt; 
 &lt;hr&gt; 
 &lt;p style=&quot;text-align:left&quot;&gt;🌟&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;演讲议题 2：基于升腾+大模型的国内智慧园区项目实践&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
 &lt;p style=&quot;text-align:left&quot;&gt;&lt;strong&gt;演讲专家：&lt;/strong&gt;李小雨，唐山爱尚产品总监，AI 应用探索者与出海实践者&lt;/p&gt; 
 &lt;p style=&quot;text-align:left&quot;&gt;&lt;strong&gt;议题介绍：&lt;/strong&gt;年初 Deepseek 引发了国产大模型浪潮，国内涌现大量需要通过 AI 提效、优化体验的需求。但是目前大部分传统行业对于 AI 提效的具体实践还在探索中，没有明确的 AI 落地场景。智慧园区为我们为某国企开发的系统，包括车辆道闸、人脸终端消费、考勤机、监控筒机等多种类业务、多种类设备，是集成一脸通、一平台、数据共享、数据可视的完整解决方案。本次分享，我们站在企业的角度分析 AI，能给实际业务带来哪些方式的效率提升，并结合实际的某智慧园区项目，分享如何结合升腾与大模型，在产品体验和功能形态上做出创新和提效。&lt;/p&gt; 
 &lt;hr&gt; 
 &lt;p style=&quot;text-align:left&quot;&gt;🌟&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;演讲议题 3：基于香橙派 AI Studio 实现本地大模型部署和应用最佳实践&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
 &lt;p style=&quot;text-align:left&quot;&gt;&lt;strong&gt;演讲专家：&lt;/strong&gt;徐洋帆，香橙派系统工程师，升腾社区核心开发者&lt;/p&gt; 
 &lt;p style=&quot;text-align:left&quot;&gt;&lt;strong&gt;议题简介：&lt;/strong&gt;随着大模型技术的快速发展，个人和企业对大模型的需求呈现爆发式增长。在云端大模型层出不穷的同时，隐私安全问题也日渐严峻。因此实现低成本的本地化 AI 大模型部署和应用势在必行。香橙派携手华为升腾，推出了 orangePi AI studio 和 orangePi AI studio Pro 产品，旨在为用户提供低成本的本地化 AI 大模型部署能力。在本次议题中，香橙派将展示算力高达 352Tops，超大的 192G 显存的 orangePi AI studio Pro 产品上极简部署 AI 大模型的步骤，半小时实现从 0 到体验本地 AI 聊天机器人。&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;微信扫码，预约直播：&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img height=&quot;930&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-7d0ffe73416168db3a2cde17f3629df8acb.jpg&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
 &lt;p&gt;另外，我们还建了一个交流群，一起聊聊自己 AI 技术～～当然啦，如果你有什么特别棒的开源项目，可以推荐过来呀～&lt;/p&gt; 
 &lt;div&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;img height=&quot;559&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-579ffec27c910f889cf6d9bbcfc234e8144.jpg&quot; width=&quot;400&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div&gt; 
  &lt;hr&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span&gt;&lt;span style=&quot;background-color:#ffffff&quot;&gt;&lt;span&gt;&lt;strong&gt;&lt;strong&gt;&lt;span&gt;&lt;span style=&quot;background-color:#ffffff&quot;&gt;&lt;span&gt;&lt;span style=&quot;color:#27ae60&quot;&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;【数智漫谈】&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span&gt;&lt;span style=&quot;background-color:#ffffff&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;OSCHINA 视频号直播畅聊栏目【数智漫谈】，每期一个技术话题，三五位专家围坐，各抒己见，畅聊开源。给大家带来最新的行业前沿、最热门的技术话题、最有趣的开源项目、最犀利的思想交锋。如果你手上也有新点子、好项目，想要跟同行交流分享，欢迎联系我们，讲坛随时开放～&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
  &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:center&quot;&gt;&lt;img height=&quot;537&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-4dd54c1b0b817689ceefa15aa66d79cfae8.png&quot; width=&quot;400&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/div&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/u/3859945/blog/18211605</link>
            <guid isPermaLink="false">https://my.oschina.net/u/3859945/blog/18211605</guid>
            <pubDate>Sun, 13 Apr 2025 08:02:00 GMT</pubDate>
            <author>原创</author>
        </item>
        <item>
            <title>GoFr —— 微服务开发框架</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                                                                        
                                                                                    &lt;p&gt;GoFr 旨在&lt;strong style=&quot;color:#1f2328&quot;&gt;简化微服务开发&lt;/strong&gt;，重点关注&lt;strong style=&quot;color:#1f2328&quot;&gt;Kubernetes 部署&lt;/strong&gt;和&lt;strong style=&quot;color:#1f2328&quot;&gt;开箱即用的可观察性&lt;/strong&gt;。虽然它能够构建通用应用程序，但&lt;strong style=&quot;color:#1f2328&quot;&gt;微服务&lt;/strong&gt;仍然是其核心。&lt;/p&gt;

&lt;div style=&quot;text-align:start&quot;&gt;
&lt;h4&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;color:#1f2328&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;background-color:#ffffff&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;主要特点&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;/div&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;简单的 API 语法&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;默认的 REST 标准&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;配置管理&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://gofr.dev/docs/quick-start/observability&quot;&gt;可观察性&lt;/a&gt;&lt;/strong&gt;（日志、跟踪、指标）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;内置&lt;a href=&quot;https://gofr.dev/docs/advanced-guide/http-authentication&quot;&gt;身份验证中间件&lt;/a&gt;&lt;/strong&gt;和自定义中间件支持&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://gofr.dev/docs/advanced-guide/grpc&quot;&gt;gRPC 支持&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;支持断路器的&lt;strong&gt;&lt;a href=&quot;https://gofr.dev/docs/advanced-guide/http-communication&quot;&gt;HTTP 服务&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://gofr.dev/docs/advanced-guide/using-publisher-subscriber&quot;&gt;发布/订阅&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;所有数据源的&lt;strong&gt;&lt;a href=&quot;https://gofr.dev/docs/advanced-guide/monitoring-service-health&quot;&gt;健康检查&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://gofr.dev/docs/advanced-guide/handling-data-migrations&quot;&gt;数据库迁移&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://gofr.dev/docs/advanced-guide/using-cron&quot;&gt;计划任务&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;支持无需重启即可&lt;a href=&quot;https://gofr.dev/docs/advanced-guide/remote-log-level-change&quot;&gt;更改日志级别&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://gofr.dev/docs/advanced-guide/swagger-documentation&quot;&gt;Swagger 渲染&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://gofr.dev/docs/advanced-guide/handling-file&quot;&gt;Abstracted File Systems&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://gofr.dev/docs/advanced-guide/handling-file&quot;&gt;WebSockets&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

                                                                    &lt;/div&gt;
                                                                </description>
            <link>https://www.oschina.net/p/gofr</link>
            <guid isPermaLink="false">https://www.oschina.net/p/gofr</guid>
            <pubDate>Sun, 13 Apr 2025 07:59:00 GMT</pubDate>
        </item>
        <item>
            <title>微软近 5 万 star 的开源项目 —— MarkItDown 已支持 MCP</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;MarkItDown 是微软开源的 Python 实用工具库，支持将各种文件转换为 Markdown 格式，适用于索引、文本分析等用途。&lt;/p&gt; 
&lt;p&gt;MarkItDown 目前支持以下文件：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PDF&lt;/li&gt; 
 &lt;li&gt;PowerPoint&lt;/li&gt; 
 &lt;li&gt;Word&lt;/li&gt; 
 &lt;li&gt;Excel&lt;/li&gt; 
 &lt;li&gt;图片（EXIF 元数据和 OCR）&lt;/li&gt; 
 &lt;li&gt;音频（EXIF 元数据和语音转录）&lt;/li&gt; 
 &lt;li&gt;HTML&lt;/li&gt; 
 &lt;li&gt;基于文本的格式（CSV、JSON、XML）&lt;/li&gt; 
 &lt;li&gt;ZIP 文件（遍历内容）&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;该项目最近发布了一项「史诗级」更新 —— 支持 MCP。MarkItDown 现已提供 MCP（模型上下文协议）服务器 (MarkItDown-MCP)，以便与 LLM 应用程序如 Claude Desktop 集成。&lt;/p&gt; 
&lt;p&gt;MarkItDown-MCP 提供两种主要的服务器模式：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;STDIO 模式&lt;/strong&gt;（默认）：通过标准输入/输出进行通信，非常适合与命令行工具和脚本集成。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;SSE 模式&lt;/strong&gt;&lt;/strong&gt;：作为服务器发送事件 (Server-Sent Events) 服务器在指定主机和端口上运行，支持基于 Web 和网络的集成。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Docker 支持&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;为了增强可移植性和隔离性，MarkItDown-MCP 提供了 Docker 支持。这在以下情况特别有用：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;确保在不同系统上的环境一致性&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;将转换过程与主机系统隔离&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;与 Claude Desktop 等远程服务协作&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Docker 集成包括挂载本地目录的功能，允许容器访问和转换本地文件，同时维持安全边界。&lt;/p&gt; 
&lt;p&gt;更多信息查看&amp;nbsp;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fmarkitdown%2Ftree%2Fmain%2Fpackages%2Fmarkitdown-mcp&quot; target=&quot;_blank&quot;&gt;markitdown-mcp&lt;/a&gt;。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/345742/markitdown-mcp-server</link>
            <guid isPermaLink="false">https://www.oschina.net/news/345742/markitdown-mcp-server</guid>
            <pubDate>Sun, 13 Apr 2025 07:37:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>宇树科技将举办全球首场「人形机器人格斗大赛」</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;2025 年 5 月至 6 月，杭州的宇树科技将举办全球首场「人形机器人格斗大赛」。据悉，宇树科技的技术团队在过去数周内进行高强度的算法训练与硬件调试，为这场比赛打造了最强的参赛机器人。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;此次比赛将通过中央广播电视总枱的相关平台全网直播。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;360&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-b3acd1231b7817a91dee16dee6276eecb65.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;为了预热赛事，宇树科技发布了视频《Unitree 铁甲拳王：觉醒！》。视频中，参赛的 G1 人形机器人不仅展现了卓越的灵活性与迅猛的出拳能力，还能够完成左右勾拳、侧踢等高难度格斗动作。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;特别引人注目的是，在被击倒后，G1 能够迅速自我恢复并重新投入战斗。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/345738</link>
            <guid isPermaLink="false">https://www.oschina.net/news/345738</guid>
            <pubDate>Sun, 13 Apr 2025 07:29:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>H.266 席卷头部平台成主流：渗透率超 70%、比 H.265 视频减小一半</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;在近期由阿里巴巴达摩院举办的视频技术前沿研究与应用研讨会上，达摩院视频技术实验室负责人叶琰介绍，新一代视频编解码标准 H.266 正从成熟走向主流，在头部视频平台的渗透率已超 70%。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-3d7871b9c0453ba3ab7b384ad1bb0145b8f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;叶琰还表示，达摩院正在积极推进自研视频编解码方案 DAMO266 的应用推广与生态共建。&lt;/p&gt; 
&lt;p&gt;当前视频应用消耗全网超过 80% 的流量，且 4K 等超高清内容占比持续上升，这就需要算法更先进、压缩性能更强的编解码技术，&lt;strong&gt;相较于 H.265 标准，H.266 在保证相同视频质量下，可减少约 50% 的数据大小与带宽成本&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0421/151436_BZat_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;叶琰指出，H.266 标准正在成为越来越多企业的选择，在国内头部视频平台中，top 5 短视频平台均已上线 H.266 服务，top 5 长视频平台中 2 家已上线、2 家正调研或计划上线。&lt;/p&gt; 
&lt;p&gt;对于企业而言，采用 H.266 标准可获得单流 50% 的带宽节省收益，约等同于 16% 的综合成本下降，并提升用户体验，如流量消耗减半、卡顿率减半等。&lt;/p&gt; 
&lt;p&gt;而阿里巴巴自研 DAMO266 是业内少数较为成熟的 H.266 标准解决方案，已在多个国民级应用落地，处理的日均视频播放量（VV）已破亿，超过 99% 的移动设备支持 DMAO266 软解。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0421/151459_Y29u_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;达摩院还积极与行业伙伴共建 H.266 生态，联合优酷、vivo 推出了业内首个 H.266 手机软解异构优化方案，在 1080P 60fps 的优酷帧享视频播放场景下实现 17% 的解码提速和 13% 的功耗下降；与高通合作，在搭载骁龙 X Elite 的 Windows 11 AI PC 上首次实现 4K 120fps 视频的流畅播放。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;参考：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FTCb8FhZdf-ctegim-4N1wA&quot; target=&quot;_blank&quot;&gt;https://mp.weixin.qq.com/s/TCb8FhZdf-ctegim-4N1wA&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/345734</link>
            <guid isPermaLink="false">https://www.oschina.net/news/345734</guid>
            <pubDate>Sun, 13 Apr 2025 07:15:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>OpenAI 详解 o3、o4-mini 和 o3-mini 使用限制</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;OpenAI 在最近更新的一份文档中详细&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhelp.openai.com%2Fen%2Farticles%2F9824962-openai-o3-o4-mini-and-o3-mini-usage-limits-on-chatgpt-and-the-api%23h_0151d07654&quot; target=&quot;_blank&quot;&gt;阐述&lt;/a&gt;了 o3、o4-mini 和 o3-mini 三种新推理在 ChatGPT 和 API 上的使用限制。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;具体来说，ChatGPT Plus、Team 或 Enterprise 帐户，每周可以使用 o3 访问 50 条消息，每天可以使用 o4-mini 访问 150 条消息，每天可以使用 o4-mini-high 访问 50 条消息。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;对于 ChatGPT Pro 用户，OpenAI 称其提供「接近无限制」的 o3、o4-mini 和 4o 访问权限。前提是必须遵守一些&lt;/span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopenai.com%2Fterms%2F&quot; target=&quot;_blank&quot;&gt;使用条款&lt;/a&gt;，&lt;span style=&quot;color:#1a1a1a&quot;&gt;禁止以下行为：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style=&quot;color:#1a1a1a&quot;&gt;滥用，例如自动或以编程方式提取数据。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style=&quot;color:#1a1a1a&quot;&gt;共享帐户凭据或向任何其他人提供帐户。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;转售访问权限或使用 ChatGPT 为第三方服务提供支持。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:start&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;OpenAI 在文档中指出：「我们已设置安全防护措施以防止滥用，并始终致力于改进我们的系统。这可能偶尔会涉及暂时限制您的使用。发生这种情况时，我们会通知您。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;该公司预计将在几周内发布 OpenAI o3‑pro，并提供全面的工具支持。目前，Pro 用户仍然可以使用 o1‑pro。&amp;nbsp;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;所有付费使用套餐的 API 用户均可使用 o1、o3 和 o4-mini 模型。可参阅&lt;/span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fplatform.openai.com%2Fdocs%2Fguides%2Frate-limits%2Fusage-tiers&quot; target=&quot;_blank&quot;&gt;平台文档&lt;span style=&quot;color:#000000&quot;&gt;，&lt;/span&gt;&lt;/a&gt;&lt;span style=&quot;color:#000000&quot;&gt;查看各套餐的速率限制。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/345733/openai-o3-o4-mini-and-o3-mini-usage-limits</link>
            <guid isPermaLink="false">https://www.oschina.net/news/345733/openai-o3-o4-mini-and-o3-mini-usage-limits</guid>
            <pubDate>Sun, 13 Apr 2025 07:12:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>深圳大学人工智能学院正式揭牌成立</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;今天（4 月 21 日）深圳大学人工智能学院正式揭牌成立。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;853&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0421/150304_YzCt_2720166.png&quot; width=&quot;1280&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;据悉，深圳大学人工智能学院，是响应国家人工智能发展战略，契合大湾区产业蓬勃发展需求，在国家战略引领下积极布局的前沿学院，致力打造人工智能领域的教育与科研高地。&lt;/p&gt; 
&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FyLhYKWCdvOSrwJE94SzOjQ&quot; target=&quot;_blank&quot;&gt;据报道&lt;/a&gt;&lt;/u&gt;，深圳大学人工智能学院首批汇聚了一批国内外顶尖人才，构建包含 2 位中国科学院院士、1 位日本工程院院士、5 位国家级人才、2 位国家青年人才的约 80 人教研团队。&lt;/p&gt; 
&lt;p&gt;学院构建「需求牵引、突破关键、百花齐放」的科研体系，依托全国重点实验室、国家工程实验室等强大平台，建设基础学科研究中心和算力平台，与腾讯云共建产业学院，为科研创新、技术转化和人才培养提供坚实保障。&lt;/p&gt; 
&lt;p&gt;学院以创新的学科布局，构建起全面的本硕博一体化专业体系，学科方向覆盖人工智能基础理论、具身智能等前沿。&lt;/p&gt; 
&lt;p&gt;今年 2 月 13 日，香港中文大学（深圳）正式成立人工智能学院，计划于 2025 年 9 月招收首批学生，拟开设人工智能本科专业及人工智能哲学硕士-博士项目。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/345730</link>
            <guid isPermaLink="false">https://www.oschina.net/news/345730</guid>
            <pubDate>Sun, 13 Apr 2025 07:05:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>罗永浩创业公司细红线招聘多名算法工程师</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;罗永浩今天在微博发布了一则招聘公告，为创业公司细红线招聘多名算法、研发工程师。工作地点均为上海：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;资深互联网产品经理（5 名）&lt;/li&gt; 
 &lt;li&gt;多模态大语言模型微调算法工程师（6 名）&lt;/li&gt; 
 &lt;li&gt;多模态信息检索算法工程师（4 名）&lt;/li&gt; 
 &lt;li&gt;大语言模型微调算法工程师（5 名）&lt;/li&gt; 
 &lt;li&gt;桌面端研发工程师（5 名）&lt;/li&gt; 
 &lt;li&gt;资深后端研发工程师（搜索方向）（4 名）&lt;/li&gt; 
 &lt;li&gt;AI 后端开发工程师（5 名）&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img height=&quot;453&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-f5ab9b4bd073e70cc29c7664c17f163eaab.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;据 XR Vision 昨日报道，罗永浩旗下细红线科技早在 2024 年已放弃 AR 智能眼镜类产品研发，继而转向为 AI 智能硬件和 AI 大模型的研发。但 2025 年年初在 AI 智能硬件完成之后，整个硬件团队已被全部裁撤，只留下 20 多个软件工程师负责 AI 软件相关产品的研发和打磨，继续完成软硬件一体的产品在海外上市和销售。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/345697</link>
            <guid isPermaLink="false">https://www.oschina.net/news/345697</guid>
            <pubDate>Sun, 13 Apr 2025 05:40:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>讯飞星火 X1 全新升级，基于全国产算力训练的深度推理大模型</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;科大讯飞今日官宣，讯飞星火 X1 全新升级，号称是「&lt;strong&gt;当前&lt;strong&gt;&lt;strong&gt;业界&lt;/strong&gt;&lt;/strong&gt;唯一&lt;/strong&gt;的基于全国产算力训练的深度推理大模型」，&lt;/p&gt; 
&lt;p&gt;本次升级有这些关键信息⬇️&lt;/p&gt; 
&lt;p&gt;✨实现了数学、代码、逻辑推理、文本生成、语言理解、知识问答等通用任务效果显著提升，&lt;strong&gt;在模型参数&lt;/strong&gt;&lt;strong&gt;&lt;strong&gt;比业界同类模型小一个数量级&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;的情况下，&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;整体效果&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;对&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;标&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;OpenAI o1 和 DeepSeek R1&lt;/strong&gt;，再次证明了&lt;/strong&gt;基于国产算力训练的全栈自主可控大模型具备登顶业界最高水平的实力和持续创新的潜力。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;✨融入了更多场景复杂类型数据，模型的泛化性也取得了进步，多个行业任务上展现出了业界领先的能力，&lt;strong&gt;在重点行业如教育、医疗、司法等进一步扩大了领先优势&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;✨&lt;strong&gt;首发快思考、慢思考统一模型&lt;/strong&gt;，由一个模型同时支持两种思考模式，私有化部署简便；&lt;strong&gt;全新升级模型定制优化工具链&lt;/strong&gt;，支持 SFT、强化学习两种模型定制优化方案，&lt;strong&gt;定制门槛低。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;✨&lt;strong&gt;三大核心技术创新&lt;/strong&gt;——大规模多阶段强化学习训练方法、基于快慢思考的统一训练方法、工程技术系统创新保障基于国产算力的高效长稳训练，助力星火 X1 全面升级。&lt;/p&gt; 
&lt;p&gt;✨&lt;strong&gt;星火 X1 API 已同步上线讯飞开放平台&lt;/strong&gt;，面向广大开发者和企业开放服务。&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;据介绍，此次星火 X1 升级，在多个任务上效果继续突破，展现出优异的性能。根据最新测试集评测结果，&lt;strong&gt;星火 X1 在通用任务效果评测中全面对标 OpenAI o1 和 DeepSeek R1&lt;/strong&gt;，在数学、知识问答等方面表现突出。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;857&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0421/115133_nUoJ_2720166.png&quot; width=&quot;1280&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;星火 X1 此次全新升级，背后有三大技术创新：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;1、大规模多阶段强化学习训练方法&lt;/strong&gt;：提出基于问题难度的大规模多阶段强化学习方法，在复杂推理、数学、代码、语言理解等场景全面提升模型效果及泛化性；同时提出强化学习动态更新算法，基于样本采样长度动态调整强化学习更新速度，进一步提升深度思考强化学习效率及效果。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2、基于快慢思考的统一训练方法&lt;/strong&gt;：提出统一模型下快慢思考混合训练方法，充分发挥快慢思考数据相互促进作用，实现基于系统指令控制模型是否深度思考，支撑下游更高效便捷地部署使用。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;3、工程技术系统创新保障基于国产算力的高效长稳训练&lt;/strong&gt;：实现多项工程技术创新，显存动态卸载技术大幅提升长文本推理并发、训推共卡协同实现高效训推资源转换、推理引擎冬眠机制实现快速拉起和恢复，实现国产算力平台上高效和稳定的强化学习训练全流程。&lt;/p&gt; 
&lt;p&gt;访问&amp;nbsp;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fxinghuo.xfyun.cn%2Fsparkapi&quot; target=&quot;_blank&quot;&gt;https://xinghuo.xfyun.cn/sparkapi&lt;/a&gt;&amp;nbsp;体验星火 X1 API&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/345688</link>
            <guid isPermaLink="false">https://www.oschina.net/news/345688</guid>
            <pubDate>Sun, 13 Apr 2025 03:53:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>CISA 扩大资金投入，确保「关键 CVE 服务不出现中断」</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;CISA 表示，美国政府已扩大资金投入，以确保关键的通用漏洞和暴露 (CVE) 计划不会出现连续性问题。这家美国网络安全机构表示：「CVE 项目对网络社区至关重要，也是 CISA 的首要任务。昨晚，CISA 执行了合同中的选择期，以确保关键的 CVE 服务不会出现中断。我们感谢合作伙伴和利益相关者的耐心。」&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-1f100a8af5b380712b4c8e12772bf8e3305.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;此前，MITRE 副总裁 Yosry Barsoum 曾警告称，政府对 CVE 和 CWE 项目的资助将于今天（4 月 16 日）到期，这可能会导致整个网络安全行业出现大范围混乱。&lt;/p&gt; 
&lt;p&gt;Barsoum 表示：「如果服务中断，我们预计 CVE 将受到多重影响，包括国家漏洞数据库和公告、工具供应商、事件响应操作以及各种关键基础设施的恶化。」&lt;/p&gt; 
&lt;p&gt;MITRE 维护着&amp;nbsp;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcve.mitre.org%2F&quot; target=&quot;_blank&quot;&gt;CVE&lt;/a&gt;，这是一个被广泛采用的计划，它在讨论安全漏洞时提供准确性、清晰度和共享标准，资金来自美国国土安全部 (DHS) 的国家网络安全部门。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;新成立的 CVE 基金会&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在 CISA 宣布这一消息之前，一组 CVE 董事会成员宣布成立 CVE 基金会，这是一个非营利组织，旨在确保 CVE 计划的独立性，因为 MITRE 警告称美国政府可能不会续签管理该计划的合同。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0417/165002_yPMw_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.thecvefoundation.org%2Fhome&quot; target=&quot;_blank&quot;&gt;他们在上周三的新闻稿中表示&lt;/a&gt;： 「自成立以来，CVE 项目一直由美国政府资助，并通过合同进行监督和管理。虽然这种结构支持了项目的发展，但也引发了 CVE 董事会成员长期以来的担忧，他们担心一个全球依赖的资源与单一政府资助机构捆绑在一起，其可持续性和中立性会受到影响。」&lt;/p&gt; 
&lt;p&gt;在过去的一年里，参与启动的人员一直在制定一项战略，将该计划过渡到这个专门的基金会，消除「漏洞管理生态系统中的单点故障」，并确保「CVE 计划仍然是一个全球信赖的、社区驱动的计划」。&lt;/p&gt; 
&lt;p&gt;虽然 CVE 基金会计划在未来几天发布有关其过渡计划的更多信息，但下一步行动仍不明确，特别是考虑到 CISA 已确认 MITRE 合同的资金已延长。&lt;/p&gt; 
&lt;p&gt;欧盟网络安全局 (ENISA) 还推出了欧洲漏洞数据库 (&amp;nbsp;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Feuvd.enisa.europa.eu%2F&quot; target=&quot;_blank&quot;&gt;EUVD&lt;/a&gt;&amp;nbsp;)，该数据库「通过从多个来源收集公开的漏洞信息，采取多利益相关方方式」。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;相关阅读&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/news/345117/the-cve-foundation&quot; target=&quot;news&quot;&gt;CVE 基金会成立&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/news/345038&quot; target=&quot;news&quot;&gt;美国政府不再为 CVE/CWE 项目提供资金支持&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/345685</link>
            <guid isPermaLink="false">https://www.oschina.net/news/345685</guid>
            <pubDate>Sun, 13 Apr 2025 03:46:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>「DeepSeek-V3 技术解析」：DeepSeek-V3-Base 预训练阶段解析</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                    
                                                                                                                                                    &lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;编者按：&lt;/strong&gt; 这篇技术解析详细阐述了 DeepSeek-V3-Base 的预训练阶段所采用的关键技术。&lt;/p&gt; 
 &lt;p&gt;文章重点介绍了三项核心技术：Document Packing 技术有效解决了输入序列长度差异导致的资源浪费问题；Fill-in-the-Middle（FIM）采用 PSM 框架和特殊 tokens，使模型具备上下文感知的中间内容生成能力；基于 YaRN 的长上下文窗口扩展技术则通过频率插值策略解决了位置编码的扩展挑战。&lt;/p&gt; 
 &lt;p&gt;随后，文章详细描述了 DeepSeek-V3-Base 的预训练过程，包括数据构建、训练策略和评估结果。&lt;/p&gt; 
 &lt;p&gt;评估显示，这些技术组合使 DeepSeek-V3 每训练 1T token 仅需 180K NVIDIA H800 GPU 小时数，并在&quot;大海捞针&quot;测试中展现卓越的长文本理解能力，为后续 RL 阶段奠定了优质基座。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;作者 | Shirley Li&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;编译 | 岳扬&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;这是 DeepSeek 系列文章的第五篇，也是首篇聚焦 DeepSeek-V3 [1, 2] 训练流程的文章。&lt;/p&gt; 
&lt;p&gt;如下图所示，DeepSeek-V3 的训练分为多个阶段：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;产出 DeepSeek-V3-Base 基础模型的预训练阶段&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;基于 DeepSeek-V3-Base，通过大规模强化学习（RL）分别训练出 DeepSeek-R1-Zero（无需监督式微调冷启动）和 DeepSeek-R1（含有监督式微调）&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;利用 DeepSeek-R1 生成推理数据，用于 DeepSeek-V3 的监督式微调（SFT），接着是未在图中展示的 RL 阶段。&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-71379f71e34c5adf46d001de06b46394737.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;图 1. DeepSeek-V3 训练流程示意图（由原文作者绘制）&lt;/p&gt; 
&lt;p&gt;本文将重点关注产出 DeepSeek-V3-Base 的预训练阶段，阐述该阶段实现高效预训练的关键技术。后续文章将涵盖：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;群组相对策略优化（GRPO）[7]&lt;/li&gt; 
 &lt;li&gt;DeepSeek-R1-Zero 和 DeepSeek-R1 的训练细节&lt;/li&gt; 
 &lt;li&gt;DeepSeek-V3 的后训练阶段（监督式微调与 RL 阶段）&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;目录&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;技术背景&lt;/strong&gt;：解析 DeepSeek-V3 预训练阶段的相关技术，包括 Document Packing，Fill-in-Middle 和 long context extension。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;预训练阶段&lt;/strong&gt;：详解如何构建预训练数据、强调一些关键的训练策略，并回顾评估结果。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;总结&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;参考文献&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;&lt;strong&gt;01 技术背景&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;本节将介绍预训练 DeepSeek-V3 过程中使用的几种技术，包括 document packing、Fill-in-the-Middle（FIM）和基于 YaRN 的长上下文窗口扩展技术。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;1.1 Document Packing&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;要理解为什么需要 document packing，我们首先需要回顾一下 Transformer 模型是如何构建输入序列 tokens 的。&lt;/p&gt; 
&lt;p&gt;Transformer 模型默认情况下需要固定长度的 token 序列作为输入，然而同一 batch 的文本输入往往长度不同。为了适应这种情况，文本输入通常需要经过以下预处理步骤：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;将所有原始文本输入分词为 token 序列&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;将 token 序列截断或填充到预定义的固定长度（max_seq_len）：若原始序列过长则截断，否则用特殊 [PAD] token 进行填充&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;生成掩码 IDs 使模型在训练时能忽略填充的 token&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;为了更清晰地展示这个过程，以下这个示例我们将使用 GPT-2 [10]的分词器处理两个句子：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-1b9f3887a8de379742b0f676da612804903.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;运行上述脚本后，会得到如下输出，其中：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;第一句话被填充了 4 个额外的 padding token，体现在 input_ids 和 mask_ids 中；&lt;/li&gt; 
 &lt;li&gt;第二句被截断，因此无需添加 padding token。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-4e4ef991506b81b0784aa5c3673b1c09afb.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;图 2. 填充操作示例（此图由作者绘制）&lt;/p&gt; 
&lt;p&gt;上述截断和填充方法虽然能让模型处理不同长度的输入，但当输入序列长度差异过大时（这在 LLM 训练中非常常见）会引发一系列问题：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;对超长序列，截断可能导致有用信息丢失&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;对较短的序列，填充过多 token 会造成计算资源浪费&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;因此，LLM 训练通常采用 document packing 技术来处理输入序列。&lt;/p&gt; 
&lt;p&gt;更具体地说，如果给定若干长度不同的文档，我们首先将其分割为较小的块（chunk），如下图所示（用不同颜色代表不同文档）：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-f01ac2b3145cefa739b7a42248a7574c35c.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;图 3. 文档分割（图片改编自文献[3]）&lt;/p&gt; 
&lt;p&gt;随后，我们将不同文档的块（chunk）进行拼接，以避免对长文档进行截断和对短文档进行填充：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-ac5be1d745432dc595122a730f8cb882143.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;图 4. 传统拼接方式（图片改编自文献[3]）&lt;/p&gt; 
&lt;p&gt;在上例中：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;第一个输入（译者注：图 4 第一行）仅包含文档 1 的 tokens&lt;/li&gt; 
 &lt;li&gt;第二个输入（译者注：图 4 第二行）拼接自文档 1 和文档 2 的 tokens&lt;/li&gt; 
 &lt;li&gt;第三个输入（译者注：图 4 第三行）拼接自文档 2 和文档 3 的 tokens&lt;/li&gt; 
 &lt;li&gt;第四个输入（译者注：图 4 第四行）拼接自文档 3、4、5 的 tokens&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;这种方法虽能在一定程度上避免进行填充和截断，但由于仅按数据中的相对顺序拼接来自不同文档的块（chunks），无法控制最终输入序列的构建方式。&lt;/strong&gt; 例如：文档 3（紫色）被不必要地分割为两部分，尽管其实际长度小于 max_seq_len，可以完整放入。&lt;/p&gt; 
&lt;p&gt;为了解决这个问题，文献 [3] 提出了 Best-fit Packing 技术，通过两个步骤完全消除不必要的分割：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Step 1：将每个文档分割为更小的块。&lt;/li&gt; 
 &lt;li&gt;Step 2：以一种智能的方式将这些块（chunks）分组为训练序列，确保在不进一步分割任何块（chunks）的前提下生成最少量的序列。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-409730df94907d865b72f59522cd798a544.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;图 5. Best-fit packing 技术（此图改编自文献[3]）&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;1.2 Fill-in-the-Middle（FIM）&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;在传统的自回归生成中，只能以从左到右的方式训练模型，即模型只能根据前面的 tokens 预测下一个 token。&lt;strong&gt;然而在实际应用中，模型常需根据上下文生成中间缺失的内容。&lt;/strong&gt; 尤其在代码生成场景中 ------ 我们常会给定输入/输出和部分代码片段，要求模型填充中间逻辑，如下例所示：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-e731fcaf7f46097251e8e7202a61173df28.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;为了适配此类需求，文献 [4] 提出了一种简单有效的方法，称为 &quot;fill-in-the-middle&quot;：即将文档随机切分为 prefix、middle 和 suffix 三部分，然后将 middle 部分移至末尾：&lt;/p&gt; 
&lt;p&gt;由于数据组织形式为 &quot;Prefix-Suffix-Middle&quot;，该方法常被称为 PSM 框架。实际实现时通过添加特殊 token 来标记各部分的边界：&lt;/p&gt; 
&lt;p&gt;其中：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&amp;lt;|fim_begin|&amp;gt;和&amp;lt;|fim_hole|&amp;gt;标记 prefix 部分&lt;/li&gt; 
 &lt;li&gt;&amp;lt;|fim_hole|&amp;gt;和&amp;lt;|fim_end|&amp;gt;标记 suffix 部分&lt;/li&gt; 
 &lt;li&gt;&amp;lt;|fim_end|&amp;gt;和&amp;lt;|eos_token|&amp;gt;标记 middle 部分&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;以如下输入为例：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-f337b48ebcf7340d63b8119d0d1842db85d.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;若需模型预测第二行代码，可将该行作为 middle 部分，并构造 FIM 输入如下：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-826b6da51a2ee6ac5b1847427cb9b4ebece.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;图 6. PSM 框架示意图（此图由作者绘制）&lt;/p&gt; 
&lt;p&gt;此时模型的预期输出应为：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-85f95b26c05ae4243e57b8fbca16df200af.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;1.3 基于 YaRN 的长上下文窗口扩展技术&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;现代 LLM 常需处理极长的提示词（如整个代码仓库），但直接使用 128K 等长上下文窗口进行预训练并不现实。多数 LLM 采用分阶段渐进式扩展策略：先在较小的上下文窗口进行预训练，再分多个阶段逐步扩展到更长的上下文窗口，从而大大降低训练成本。&lt;/p&gt; 
&lt;p&gt;例如，在 DeepSeek-V3 中，模型首先使用 4K 的上下文窗口完成预训练，然后再分两阶段扩展到 128K：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;第一阶段：从 4K 到 32K（1000 steps）&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;第二阶段：从 32K 到 128K（再 1000 steps）&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;需特别指出的是，这种扩展不能通过简单调大上下文窗口实现，而需借助基于旋转位置编码（RoPE）改进的 YaRN（Yet another RoPE extensioN）技术对位置编码进行修改。&lt;/p&gt; 
&lt;p&gt;关于 RoPE 的详细介绍，请参阅我们之前的文章《「DeepSeek-V3 技术解析」：多头潜在注意力机制（MLA）》。&lt;/p&gt; 
&lt;p&gt;RoPE 是一种相对位置编码方法，其核心思想是通过使用复杂的旋转嵌入修改 Query 和 Key，使得二者的内积依赖于它们的相对位置：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-b844eb693468aac0d86585c525f0128cec8.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;然而，由于余弦函数和正弦函数是周期性的，(pos_i, pos_j) 之间的内积可能看起来与 (pos_i, pos_k) 之间的内积相似，因此在固定 θ 的情况下，仅使用 1K tokens（即位置索引 1~1000） 进行预训练的模型在测试时可能会混淆，因为测试时遇到的位置索引（如 5K 或 10K）可能远远超出了预训练时的上下文窗口。&lt;/p&gt; 
&lt;p&gt;下图展示了这种现象：&lt;strong&gt;当 32K 上下文窗口的预训练模型在超出该窗口的位置测试时，困惑度（Perplexity）急剧上升&lt;/strong&gt;：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-b4570f8269f6199cd4f7f261e30d709ef95.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;图 7. 困惑度与上下文窗口的关系（此图由作者绘制）&lt;/p&gt; 
&lt;p&gt;那么，YaRN 是如何应对这一挑战的呢？&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;既然外推法（extrapolate）效果欠佳，YaRN 转而采用插值频率（interpolate the frequency）的策略。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;假设我们有一个在 4 个 token 长度的输入上训练的模型，希望将其扩展到 8 个 token，且基础频率 θ=0.5。&lt;/p&gt; 
&lt;p&gt;对于原始 RoPE，直接使用 cos(θ×pos) 和 sin(θ×pos) 对 Query 和 Key 进行旋转即可。&lt;/p&gt; 
&lt;p&gt;而对于 YaRN：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;首先，计算扩展后的上下文长度与原始长度的比值作为缩放因子，本例中为 2。&lt;/li&gt; 
 &lt;li&gt;然后，生成新频率 θ&#39; = θ / 2 = 0.25。&lt;/li&gt; 
 &lt;li&gt;再使用新频率对 Query 和 Key 进行旋转，即 cos(θ&#39;×pos) 和 sin(θ&#39;×pos)。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;下图对比了 RoPE 与 YaRN 的 cos 和 sin 值：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-250d721643a9dbe2ab68b4192345a39774f.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;图 8. YaRN 工作原理示意图（此图由作者绘制）&lt;/p&gt; 
&lt;p&gt;通过该图可观察到：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;在 RoPE 中，cos 和 sin 值会随位置索引的增加而快速振荡，导致扩展到更长的上下文时出现问题。&lt;/li&gt; 
 &lt;li&gt;而在 YaRN 中，原始的余弦和正弦函数通过频率缩放被插值到扩展后的上下文长度（如蓝色高亮区域所示），实现了更平滑的过渡，使得模型能够更有效地处理长序列。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;下图展示了 DeepSeek-V3 在&quot;大海捞针&quot;（Needle In A Haystack，NIAH）测试中的表现，表明其在 128K 以下的上下文窗口长度中均表现出色：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-db027db8caa140504a6c552ecc8436d2bae.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;图 9. DeepSeek-V3 的&quot;大海捞针&quot;测试结果（引自文献[2]）&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;02 预训练阶段&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;本节将介绍 DeepSeek-V3-Base 的训练方法，重点解析数据构建流程，并强调预训练阶段中的一些关键策略。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;2.1 数据构建&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;数据规模与质量对 LLM 训练至关重要。DeepSeek-V3 的预训练语料库通过持续优化策略构建，具体优化路径如下：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;在 DeepSeek 67B [8] 中，&lt;strong&gt;训练语料采用去重-过滤-再混合策略构建。&lt;/strong&gt; 首先对 Common Crawl 语料进行去重，随后通过严格的文档质量评估标准进行过滤，最后通过数据再混合阶段解决数据不平衡问题。&lt;/li&gt; 
 &lt;li&gt;在 DeepSeek-V2 [9] 中，通过以下方式扩展训练语料：1) &lt;strong&gt;增加更多中文数据及来自不同来源的高质量数据&lt;/strong&gt; ；2) &lt;strong&gt;通过优化数据清洗流程，恢复大量此前在文献 [8] 的策略中被删除的数据。同时，通过改进基于质量的过滤算法提升数据质量。&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;在 DeepSeek-V3 [2] 中，&lt;strong&gt;预训练语料进一步扩充，加入更多数学与编程样本，以及除中英文之外的多语言样本。&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;收集的预训练语料会通过前文提出的 Prefix-Suffix-Middle（PSM）框架结合 FIM（Fill-in-Middle）策略进行预处理，并应用 document-packing 技术。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;2.2 训练策略&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;原论文[2]对预训练参数进行了详细描述，此处我们仅强调几个关键点：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;长上下文窗口扩展&lt;/strong&gt;：首先在 14.8T token 上以 4K 上下文窗口进行预训练，随后通过 1000 steps 扩展到 32K 上下文，最终再通过 1000 steps 扩展到 128K 上下文。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;多词元预测&lt;/strong&gt;：如我们本系列前一篇文章《「DeepSeek-V3 技术解析」：多词元预测技术（Multi-Token Prediction, MTP）》所述，DeepSeek-V3 采用了优化版的多词元预测机制，允许模型同时解码多个词元（tokens），以加速训练中的解码过程。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;以 FP8 精度进行训练&lt;/strong&gt;：DeepSeek-V3 采用混合精度计算提升效率，对部分计算使用低精度格式（如 8-bit 浮点数），在不过度影响精度的前提下减少内存占用并加速计算。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;学习率的调度&lt;/strong&gt;：在前 2K steps 中，学习率（learning rate）从 0 线性增长至 2.2e--4，并在 10T token 的训练过程中保持恒定；随后在 4.3T token 的训练过程中按照余弦曲线下降至 2.2e-5；在最后 500B token 的训练过程中，前 333B token 保持恒定的学习率，剩余 167B token 进一步降至 7.3e-6。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Batch size 的调度&lt;/strong&gt;：在前 469B token 的训练过程中，Batch size 从 3072 逐步提升至 15360，后续训练中保持恒定。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;strong&gt;2.3 评估结果&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;下表对比了 DeepSeek-V3 与其他开源基座模型在不同任务上的表现。&lt;strong&gt;其中 DeepSeek-V3 在多数数据集上都取得了最佳性能，尤其是在数学与代码相关的任务中表现突出。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;需特别说明，得益于本系列文章中介绍的各项创新技术，DeepSeek-V3 的优异性能是在极高的训练效率下实现的。具体而言，&lt;strong&gt;DeepSeek-V3 每训练 1T token 仅需 180K H800 GPU hours，远低于训练 72B 或 405B 稠密模型的成本。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-29ea2b2418d59f8a4f9333eed10ff77c37d.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;文献[2]中的表 3&lt;/p&gt; 
&lt;p&gt;文献 [2] 还通过全面的消融实验验证了无辅助损失函数的负载均衡、多词元预测等关键技术。由于我们已在前文中讨论过相关内容，此处不再赘述。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;03 总结&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;本文探讨了 DeepSeek-V3 预训练策略中的关键创新，旨在提升效率、可扩展性与性能。由此产生的 DeepSeek-V3-Base 模型成为更高级推理模型（如 DeepSeek-R1-Zero 和 DeepSeek-R1）的基础，而这些模型又通过知识蒸馏反哺优化 DeepSeek-V3。&lt;/p&gt; 
&lt;p&gt;除此前讨论的架构创新 ------ 多头潜在注意力（Multi-head Latent Attention）、DeepSeekMoE、无辅助损失函数的负载均衡及多词元预测（Multi-token Prediction）外，本文还引入了包括 document packing、Fill-in-the-Middle（FIM）和基于 YaRN 的长上下文窗口扩展在内的多项技术。&lt;/p&gt; 
&lt;p&gt;这些技术共同推动了大语言模型效率与可扩展性边界的突破，为高性能 AI 模型设立了新标杆。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;参考文献&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;[1] DeepSeek（&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.deepseek.com%2F%EF%BC%89&quot; target=&quot;_blank&quot;&gt;https://www.deepseek.com/）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[2] DeepSeek-V3 Technical Report（&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fdeepseek-ai%2FDeepSeek-V3%2Fblob%2Fmain%2FDeepSeek_V3.pdf%EF%BC%89&quot; target=&quot;_blank&quot;&gt;https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[3] Fewer Truncations Improve Language Modeling（&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2404.10830%EF%BC%89&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2404.10830）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[4] Efficient Training of Language Models to Fill in the Middle（&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2207.14255%EF%BC%89&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2207.14255）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[4] DeepSeek-Coder: When the Large Language Model Meets Programming --- The Rise of Code Intelligence（&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2401.14196%EF%BC%89&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2401.14196）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[5] DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence（&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2406.11931%EF%BC%89&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2406.11931）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[6] YaRN: Efficient Context Window Extension of Large Language Models（&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2309.00071%EF%BC%89&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2309.00071）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[7] DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models（&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2402.03300%EF%BC%89&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2402.03300）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[8] DeepSeek LLM: Scaling Open-Source Language Models with Longtermism（&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2401.02954%EF%BC%89&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/pdf/2401.02954）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[9] DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model（&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2405.04434%EF%BC%89&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2405.04434）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[10] Language Models are Unsupervised Multitask Learners（&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcdn.openai.com%2Fbetter-language-models%2Flanguage_models_are_unsupervised_multitask_learners.pdf%EF%BC%89&quot; target=&quot;_blank&quot;&gt;https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf）&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Thanks for reading!&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Hope you have enjoyed and learned new things from this blog!&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;strong&gt;About the author&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Shirley Li&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;I am a Machine Learning Engineer working on building multi-modality models to solve real-world problems.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;END&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;本期互动内容 🍻&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;❓&lt;strong&gt;当前位置编码方案（RoPE/YaRN）已支持 128K 上下文，但人类书籍平均长度约 200K tokens。要实现真正无损的长文档理解，您认为下一代位置编码需要突破哪些理论瓶颈？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;原文链接：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmedium.com%2Fdata-science-collective%2Fdeepseek-explained-5-deepseek-v3-base-86c078ed5504&quot; target=&quot;_blank&quot;&gt;https://medium.com/data-science-collective/deepseek-explained-5-deepseek-v3-base-86c078ed5504&lt;/a&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/IDP/blog/18210553</link>
            <guid isPermaLink="false">https://my.oschina.net/IDP/blog/18210553</guid>
            <pubDate>Sun, 13 Apr 2025 03:31:00 GMT</pubDate>
            <author>原创</author>
        </item>
        <item>
            <title>3D 空间视频生成技术探索与应用</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                    
                                                                                                                                                    &lt;span id=&quot;OSC_h1_1&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;1. 背景&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h1&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;近年来，随着社交媒体、流媒体平台以及 XR 设备的快速发展，沉浸式 3D 空间视频的需求迅猛增长，尤其是在短视频、直播和电影领域，正在重新定义观众的观看体验。2023 年，苹果公司发布的空间视频技术为这一趋势注入了新的活力，2025 年以来，轻量化 AI/AR 眼镜迎来爆发，持续推动对 3D 空间视频内容的需求。然而，尽管消费端对 3D 内容的需求不断上升，供给端仍面临创作瓶颈，主要体现在可用于拍摄 3D 视频内容的专业相机设备稀缺、制作专业度要求高以及成本高昂等问题。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;我们创新性地提出了一种基于 3D 视觉和 AIGC 生成技术的方法，将存量的 2D 视频资源不断转化为 3D 空间视频资源，极大降低了 3D 内容的供给成本，提升了覆盖量。最新的研究成果已被多媒体领域的旗舰会议 ICME 2025 接受，并在京东.Vision 视频频道等业务场景落地。ICME（International Conference on Multimedia and Expo）是由 IEEE 主办的国际多媒体与博览会，2025 年会议将在法国举行，主题涵盖 3D 多媒体、增强现实（AR）、虚拟现实（VR）、沉浸式多媒体和计算机视觉等领域。本次会议共计收到来自全球 3700 多篇投稿，录用率为 27%。我们提出的基于人工智能的 2D 视频转换为 3D 空间视频的方法，涉及深度估计、图像生成等算法，并构建了一个 3D 视频数据集，作为后续行业发展的评测基准。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//2440c913a035dcf280447f858d1fda4e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;图 1 研究成果被 ICME 2025 接收&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_2&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;2. 技术方案&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h1&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;3D 空间视频生成属于新视角合成任务（Novel View Synthesis），指的是在给定源图像和目标姿态的情况下，通过算法渲染生成与目标姿态对应的图像。最新的通用新视角合成方案包括基于 NeRF 神经辐射场、Gaussian Splatting 高斯喷射以及 Diffusion Model 扩散模型等。与通用的任意视角合成不同，3D 空间视频需为双眼分别提供具有视角差的画面，算法需根据输入的一帧左视角图像，生成对应的固定姿态右眼视角图像。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;为了实现端到端的 3D 空间视频生成，我们的算法技术方案主要包含三个部分，分别是单目深度估计、新视角合成（包括视差图计算、Warp 和空洞区域填充）以及 MV-HEVC 编码，整体方案如下图 2 所示。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//5a917dda91662d6cf46790c1c9b32119.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;图 2 3D 空间视频生成架构&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;我们的最新研究成果基于上述架构，针对单目深度估计、新视角合成和 MV-HEVC 编码等三个核心模块进行了创新和优化。此外，考虑到该领域内用于训练与评测的 Benchmark 数据集在质量和规模上普遍较差的现状，我们创建了一个高质量、大规模的立体视频数据集 StereoV1K。该数据集包含在各种真实场景中捕获的 1000 个视频，分辨率为 1180×1180，总帧数超过 50 万帧。StereoV1K 将作为该领域的重要基准数据集。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_3&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;2.1 单目深度估计&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;深度估计是计算机视觉领域的一个基础性问题，旨在从图像或视频中推断出场景中物体的距离或深度信息。这项技术对增强现实、虚拟现实、机器人导航以及自动驾驶汽车等应用至关重要。深度估计的目标是根据给定的输入图像，预测每个像素点或图像中物体的相对距离或真实深度值。常见的深度估计方法包括基于深度相机等 TOF（Time of Flight）和激光雷达（LiDAR）硬件设备的方案、基于双目图像的立体匹配算法方案，以及基於单目深度估计（Monocular Depth Estimation, MDE）算法模型的方案。其中，单目深度估计由于成本较低、适用场景广泛，更容易普及，但算法的难度也相对较大。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//c0a8b225eef3135a88ff5f660e9c98db.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;图 3 基于双目图像立体匹配以及硬件的深度估计方案&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;伴随着 AI 大模型算法的快速发展，单目深度估计在技术方案上经历了从传统方法到基于深度学习的方法，再到最新的基于大模型或生成式方法的演变。根据处理对象的不同，单目深度估计可以进一步细分为图像深度估计和视频深度估计。通常情况下，图像深度估计在细节表现上更为出色，而视频深度估计则在时序一致性方面表现更佳。此外，从估计结果的角度来看，单目深度估计还可以分为绝对深度估计和相对深度估计。绝对深度估计指的是从图像中估计出每个像素到摄像机的真实物理距离，而相对深度估计则关注图像中物体之间的深度关系，而非绝对距离。基于我们的应用场景，我们采用了一种结合图像和视频深度估计优点的单目相对深度估计算法。该算法架构如下：我们使用 DINO v2 作为 Backbone，并结合 DPT Head，同时尝试引入多帧序列的 memory bank 和注意力机制，以提升深度估计结果在时序上的准确性与稳定性。算法架构如图 4 所示。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//35458a1826f0810bd2491c396110af32.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;图 4 视频单目深度估计算法架构&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;通过在短视频等数据上构建百万级的伪标签训练数据集，并采用 SFT（Supervised Fine-Tuning）和蒸馏等技术手段，我们对开源模型进行了优化。效果如图 5 所示，可以明显看到，我们不仅提升了深度估计的细节表现，还确保了估计结果的时序稳定性。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//99d3c7e01c2af117db99dca6172b7e33.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt; &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;span&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//f8858f3e1c4cb53c4264c3a5ed60e5b3.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;图 5 视频单目深度估计算法优化效果对比（原始输入-开源模型-优化后模型）&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_4&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;2.2 新视角合成&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;新视角合成是视觉领域中的一项关键任务，其目标是在有限的视图基础上生成场景或物体的其他视角。这项技术在虚拟现实、增强现实、电影特效和游戏开发等领域具有广泛应用。尽管基于 NeRF、3DGS 和 Diffusion 的方法近年来取得了显著进展，但仍面临诸多挑战。例如，NeRF 和 3DGS 方法通常只能针对单一场景进行建模，而扩散模型在生成视频时难以保证稳定性和一致性。在分析任务的特殊需求时，我们发现只需生成固定姿态的右眼视角图像，且场景具有位移小、丰富多样以及视频稳定性和一致性要求高等特点。基于这些考虑，我们最终选择采用深度 Warp 和空洞区域填充 InPaint 的方法来完成新视角合成任务。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;在 2.1 部分获取到视频对应的深度信息后，我们首先计算视差图，并引导输入的单目视频进行 Warp 操作，从而生成对应的待填充右视角视频和掩码视频。接下来，我们将这些数据输入到我们设计的 InPaint 填充框架中，以完成空洞区域的补全，最终得到完整的新视角结果。整体框架图如图 6 所示：&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//dc6228cb2a058ec55408c71fab0b45d3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;图 6 新视角合成端到端算法架构&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h3_5&quot;&gt;&lt;/span&gt; 
&lt;h3&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;2.2.1 多分支 InPaint 模块&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;为了获得高质量和高一致性的 InPaint 填充效果，我们采用了多分支填充策略。该模块集成了三种 InPaint 分支：传统多边形插值修复（Poly-base）、深度学习神经网络修复（DL-base）和视差扩展策略修复（DE-base）。每个分支各有优缺点：(i) Poly-base 能够保证视频的稳定性并减少字幕抖动，但在边缘填充时容易出现像素拉伸和毛刺；(ii) DL-base 在前景和背景边缘的填充效果良好，但视频稳定性较差，可能导致字幕抖动和前景渗透；(iii) DE-base 优化了像素拉伸和前景渗透问题，但在复杂背景和几何结构处可能提供错误的参考像素。我们结合这些分支的互补优势，以实现更好的填充效果。下图展示了我们视差扩展策略的有效性。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//342752027cd84ce3c173756a91127463.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;图 7 视差扩展策略与优化效果&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;为了更好地融合上述三个分支的结果&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;, &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;并进行进一步优化，我们提出了一种新颖的基于层级化特征更新的掩码融合器。该融合器的输入为多分支填充模块三个分支的输出结果，输出则为三张单通道的掩码图 M1，M2，M3 和一张三通道的内容图 C，通过融合这些信息，我们能够获得最终的生成结果。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;我们的结果在与当前先进模型的定量和定性比较中均表现出色，达到了 SOTA（State-of-the-Art）水平。特别是在 LPIPS 指标上，我们的方法相比其他方法提升了超过 28%，充分体现了结果的真实性和优越性。同时，在可视化效果方面，我们的方法显著减少了生成区域中的模糊伪影和前背景的错误拉伸，呈现出更加清晰自然的边缘和内容，如图 8 所示。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//95b10ed1bf652a34643ecb41a3df42c1.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;图 8 多分支 InPaint 方法与其他方法结果对比&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h3_6&quot;&gt;&lt;/span&gt; 
&lt;h3&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;2.2.2 &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;StereoV1K 立体视频数据集&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;在 3D 空间视频生成领域，现有的公开数据集存在量级小、分辨率低、场景单一和真实性差等问题，限制了行业算法的发展与提升。为了解决这些问题，我们创建了 StereoV1K，这是第一个高质量的真实世界立体视频数据集。我们使用最新的佳能 RF-S7.8mm F4 STM DUAL 镜头和 EOS R7 专业相机，在室内和室外场景中拍摄了 1000 个空间视频。每个视频裁剪后的分辨率为 1180×1180，时长约 20 秒，录制速度为 50 fps，最终整个数据集的总帧数超过了 500,000 帧。图 9 展示了与其他数据集的对比以及我们数据集的示例。该数据集将作为该领域的基准数据集，推动行业的发展。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//139902fb31116a46abe4f65d987b27cd.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;图 9 StereoV1K 数据集与现有数据集对比&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_7&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;2.3 MV-HEVC 编码&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;通过上述算法框架，我们能够生成高质量的双目 3D 视频，包括左眼视频和右眼视频，其数据量是传统 2D 视频的两倍。因此，高效压缩和编码 3D 视频在实际应用中显得尤为重要，这直接关系到在线播放视频的清晰度和带宽。目前，3D 视频编码主要分为两类方法：传统的 SBS（Side-by-Side）HEVC 编码方式以及 MV-HEVC（Multi-View HEVC）编码。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;span&gt;•&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;SBS-HEVC：该方法将 3D 视频在相同时间点的左右眼画面拼接为一个普通的 2D 画面，并采用传统的 HEVC 编码技术进行压缩。该方案实现简单，可以使用如 ffmpeg 等开源软件进行处理。然而，SBS-HEVC 的编码压缩率较低，因此需要更大的传输带宽。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;•&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;MV-HEVC：该方法将 3D 视频的不同视角编码到同一码流中，允许用户在不同视角之间自由切换。编码器可以利用左右眼画面之间的相似性来进一步减少冗余，从而显著提升压缩编码效率。MV-HEVC 编码是对标准 HEVC 的扩展，目前除了苹果 AVFoundation 框架中提供的闭源工具外，尚无自主可控的编码软件可供部署，用户需要自定义编码器来实现这一功能。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//ff52e63f038ab9e962bc58dad16c896e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;图 10 SBS-HEVC 和 MV-HEVC 编码方式对比&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;以 1920x1080 的视频为例，在 SBS-HEVC 编码流程中，画面以「左+右」的形式合并为 3840x1080 的新视频帧，然后作为普通视频进行 HEVC 编码，此时只能使用帧间预测（Inter frame prediction）。而在 MV-HEVC 编码流程中，左眼和右眼分别被称为基本层（Layer 0）和增强层（Layer 1）。除了帧间预测外，MV-HEVC 还可以利用「视间预测」（Inter view prediction），因为同一时间点的左眼和右眼画面之间具有较高的相似性和冗余性，因此视间预测能够进一步提升压缩效率。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;我们在标准 HEVC 编码器的基础上添加了对 MV-HEVC 扩展的支持，从而在编码性能和编码速度上都取得了显著提升。在典型测试场景中，MV-HEVC 相比 SBS-HEVC 的 BD-Rate 降低了 33.28%，这意味着在相同画质下，视频带宽可以减少 33%；同时，编码速度平均提升了 31.62%，具体数据如图 11 所示。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//14e0056f1c083fe7ef0e093d97f36b44.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;span&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//ea3722a6f0223d2402f05f622da27a8e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;图 11 SBS-HEVC 和 MV-HEVC 的编码 RD 性能对比&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;在使用 MV-HEVC 解决双目 3D 视频的压缩编码问题后，还需要将视频和音频数据打包存储，以实现在线流媒体播放。苹果为 MV-HEVC 定制了封装格式，但通过 ffmpeg、mp4box 等开源媒体工具封装的文件在 Vision Pro、iPhone 等苹果设备上无法正常显示立体视频。为此，我们对由 AVFoundation 封装出的正常码流进行了逆向分析，从中提取出与苹果设备兼容的码流格式，并在自研编码器中实现了这一格式。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;color:#333333&quot;&gt;与苹果设备兼容的码流格式为：&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;span&gt;•&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;使用&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;color:#333333&quot;&gt;mov 格式，&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;整体符合 QuickTime File Format Specification；同时符合 mp4 格式标准&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;color:#333333&quot;&gt;ISO/IEC 14496-15 的标准定义。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;•&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;在视频轨道的描述信息中，在 stsd 中重新定义&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;hvc1&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;、&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;hvcC&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;、&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;lhvC&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;，增加对于不同视角视频的描述，其中&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;hvcC&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;描述基本层码流信息，包括&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;VPS, SPS, PPS, SEI&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;这 4 个 HEVC &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;NAL&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;头信息；&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;lhvC&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;描述增强层码流信息，包括&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt; SPS, PPS&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;信息。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;•&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;增加苹果自定义信息&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;vexu、hfov&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;，用于描述自定义信息，其数据结构如图 12 所示，其中关键字段有：&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;◦&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;blin&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;：定义&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;baseline&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;，表示相机基线，以实际数值的 1000 倍来记录，例如 Vision Pro 的 63.54mm 记录为 63540。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;◦&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;dadj&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;：定义&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;disparity&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;，表示水平视差调整，以实际数值的 10000 倍来记录，例如 Vision Pro 的 2.93% 记录为 293。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;◦&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;hfov&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;：表示水平视场角，以实际数值的 1000 倍来记录，例如 Vision Pro 的 71.59 度记录为 71590。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//3d0c538634b101790f04b888c79426cc.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;图 12 苹果自定义信息 vexu、hfov 示意图&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_8&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;2.4 应用与落地&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;为了在实际业务中落地，我们首先简化了单目深度估计模型的尺寸，采用 ViT-S 作为特征编码器，并对模型进行了 SFT 微调。随后，我们在自建的 StereoV1K 数据集上训练了多分支 InPaint 模型，并将论文中的 InPaint 基础模型更换为更轻量的 Transformer 网络。通过这些手段，我们实现了速度与质量的平衡。在对实际业务中的大量视频进行测试后，我们发现我们的算法生成的 3D 空间视频很好地满足了业务需求，但仍有少数生成结果存在一些不理想的情况。未来，我们将持续迭代优化相关模型。此外，当前的生成速度也是一个重点优化方向。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;当前，3D 空间视频可以在多种 XR 设备上观看与体验，包括 Vision Pro、Pico、Quest 以及 AI 眼镜等双目设备。例如，我们为京东.Vision 视频频道提供了空间视频内容的算法服务。Vision Pro 视频频道的效果如图 13 左图所示：通过将 2D 商品短视频、宣传片和发布会等资源转换为 3D 立体空间视频，极大提升了用户的沉浸式和立体观看体验。此外，在更轻量的 AI/AR 眼镜中，用户也可以方便地体验到 3D 视频内容带来的震撼与沉浸感，如图 13 右图所示。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//6f20321a8f00def4d8dd5343264adf72.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt; &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;span&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//c829dd078afa100e0d732b7ef12b83da.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;图 13 京东.Vision 视频频道以及 XREAL AR 眼镜观看效果（图中无法显示 3D 效果，实际体验为 3D 效果）&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_9&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;3. 未来展望&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h1&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;上述介绍的 3D 空间视频为用户带来了全新的沉浸式体验，并为 3D 视频域提供了批量内容供给。然而，3D 领域的内容表现形式还有很多种，例如 3D 模型、3D/4D 空间和完整世界等。随着大模型的快速发展，算法对人类世界的建模正经历以下几个阶段：大语言生成模型 → 图像生成模型 → 视频、3D/4D 生成模型 → 世界模型。可以预见，未来将有更多的工作集中在 AIGC 3D/4D 和世界模型生成等方向。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_10&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;3.1 AIGC 3D/4D&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;2024 年，3D/4D 领域的 AIGC 发展迅速，尤其是从下半年开始，呈现出加速趋势。同时，新的发展方向也开始显现。从技术路线来看，有 Google 的 CAT3D，通过单图到多图再到 3D 表示的方式；还有使用 LRM 的单图到 3D 表示的方案，如 InstantMesh，以及近期基于结构化 3D 表征的 Trellis。此外，一些学者正在基于 4D Gaussian Splatting 实现空间序列的建模。值得一提的是，3D/4D 模型的可编辑性是一个重要的关注点，因为即使是专业建模师也要在生产过程中需要不断编辑和修改。最新的研究方向也开始关注生成过程的可控性与可编辑性等属性。图 14 所示为 AIGC 3D 模型与 4D 视频生成示例。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//842829b6a978dceae11af20e0594ff40.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt; &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;span&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//d0acb4fba1f4557975f9affa379de194.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;图 14 AIGC 3D 模型与 4D 视频生成示例&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;在当前的 AIGC 3D 模型生成技术中，像 Trellis 这样采用 3D 表征的端到端训练方案展现出显著优势。通过对 3D 表征进行直接的结构化编码，该模型在几何形状和纹理贴图的生成上实现了更高的准确性和鲁棒性，能够生成高质量且多样化的 3D 资产，具备复杂的形状和纹理细节。此外，由于模型处理的是结构化信息，它支持灵活的 3D 编辑，例如根据文本或图像提示进行局部区域的删除、添加和替换，如图 15 左图所示。在 4D 视频生成技术中，当前主流的方案是采用带有时序的 Gaussian Splatting 表征进行建模，如图 15 右图所示。由于高斯表征本身的大小以及存在维度提升，4D 视频面临着数据体量大、模型复杂度高、渲染性能压力大等挑战。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//7cf9795f5440bc61bb3bae767681c1d3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt; &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;span&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//d6979590f8cd845640d885df37f9ad1d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;图 15 Trellis 根据文本提示词进行纹理材质以及几何结构的局部编辑以及典型 4D Gaussian Splatting 架构&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h2_11&quot;&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;3.2 世界模型&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;目前，世界模型在学术界和工业界尚未形成明确的概念，关于其是模拟世界还是感知世界也没有统一的范式。然而，从近期的进展来看，世界模型需要具备时序和立体空间的结构化建模能力。建模后的数据应具有稠密的语义表征和局部可编辑性，同时整个时序与空间域需具备可交互性。最终目标是实现对现实空间的复刻，甚至对现实空间进行创作与未来预测。图 16 所示为 World Labs 世界模型以及 Meta orion AI 眼镜空间万物感知。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//5800f8336c22fb37004ee21f37bcd088.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt; &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div&gt; 
 &lt;span&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet//831ec7557a877625cde34a1729efe8cb.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span&gt;&lt;span style=&quot;color:transparent&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;图 16 World Labs 世界模型以及 Meta orion AI 眼镜空间万物感知&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:justify&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;我们将持续关注并深入跟进 3D 领域的最新进展，特别是在技术创新和应用实践方面的动态。结合京东广泛的业务场景，我们致力于将这些前沿技术落地并转化为实际应用，以满足用户日益增长的需求。通过不断探索 3D 技术在电商、广告、内容等多个领域的潜力，我们希望为用户带来全新的体验，提升他们的购物乐趣和互动感。我们的目标是通过创新的解决方案，推动行业的发展，为用户创造更高的价值和更丰富的体验。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_12&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;4.参考文献&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h1&gt; 
&lt;div&gt; 
 &lt;span&gt;1.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Zhang J, Jia Q, Liu Y, et al. SpatialMe: Stereo Video Conversion Using Depth-Warping and Blend-Inpainting[J]. arXiv preprint arXiv:2412.11512, 2024.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;2.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Yang, Sung-Pyo, et al. &quot;Optical MEMS devices for compact 3D surface imaging cameras.&quot;Micro and Nano Systems Letters7 (2019): 1-9.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;3.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Bhat S F, Birkl R, Wofk D, et al. Zoedepth: Zero-shot transfer by combining relative and metric depth[J]. arXiv preprint arXiv:2302.12288, 2023.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;4.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;LiheYang, BingyiKang, ZilongHuang, ZhenZhao, XiaogangXu, Jiashi Feng, and Hengshuang Zhao, 「Depth anything v2,」 arXiv preprint arXiv:2406.09414, 2024.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;5.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Teed Z, Deng J. Raft: Recurrent all-pairs field transforms for optical flow[C]//Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part II 16. Springer International Publishing, 2020: 402-419.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;6.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Zhang K, Fu J, Liu D. Flow-guided transformer for video inpainting[C]//European conference on computer vision. Cham: Springer Nature Switzerland, 2022: 74-90.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;7.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Shangchen Zhou, Chongyi Li, Kelvin CK Chan, and Chen Change Loy, 「Propainter: Improving propagation and transformer for video inpainting,」 in ICCV, 2023, pp. 10477–10486.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;8.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Han Y, Wang R, Yang J. Single-view view synthesis in the wild with learned adaptive multiplane images[C]//ACM SIGGRAPH 2022 Conference Proceedings. 2022: 1-8.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;9.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Wang L, Frisvad J R, Jensen M B, et al. Stereodiffusion: Training-free stereo image generation using latent diffusion models[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 7416-7425.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;10.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Zhen Lv, Yangqi Long, Congzhentao Huang, Cao Li, Chengfei Lv, and Dian Zheng, 「Spatialdreamer: Self-supervised stereo video synthesis from monocular input,」 arXiv preprint arXiv:2411.11934, 2024.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;11.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Mildenhall B, Srinivasan P P, Tancik M, et al. Nerf: Representing scenes as neural radiance fields for view synthesis[J]. Communications of the ACM, 2021, 65(1): 99-106.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;12.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Kerbl B, Kopanas G, Leimkühler T, et al. 3d gaussian splatting for real-time radiance field rendering[J]. ACM Trans. Graph., 2023, 42(4): 139:1-139:14.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;13.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Gao R, Holynski A, Henzler P, et al. Cat3d: Create anything in 3d with multi-view diffusion models[J]. arXiv preprint arXiv:2405.10314, 2024.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;14.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Xu J, Cheng W, Gao Y, et al. Instantmesh: Efficient 3d mesh generation from a single image with sparse-view large reconstruction models[J]. arXiv preprint arXiv:2404.07191, 2024.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;15.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Xu Z, Xu Y, Yu Z, et al. Representing long volumetric video with temporal gaussian hierarchy[J]. ACM Transactions on Graphics (TOG), 2024, 43(6): 1-18.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;16.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Dosovitskiy A, Beyer L, Kolesnikov A, et al. An image is worth 16x16 words: Transformers for image recognition at scale[J]. arXiv preprint arXiv:2010.11929, 2020.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;17.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Xiang, Jianfeng, et al. &quot;Structured 3d latents for scalable and versatile 3d generation.&quot; arXiv preprint arXiv:2412.01506 (2024).&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;18.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Wu G, Yi T, Fang J, et al. 4d gaussian splatting for real-time dynamic scene rendering[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2024: 20310-20320.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;19.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F17003931453&quot; target=&quot;_blank&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;https://zhuanlan.zhihu.com/p/17003931453&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;20.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F15449644319&quot; target=&quot;_blank&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;https://zhuanlan.zhihu.com/p/15449644319&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;21.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Gerhard T, Ying Chen, Karsten Müller, et al. Overview of the Multiview and 3D Extensions of High Efficiency Video Coding. IEEE TRANS. ON CSVT, VOL. 26, NO. 1, JANUARY 2016&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;22.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.apple.com%2Fav-foundation%2FHEVC-Stereo-Video-Profile.pdf&quot; target=&quot;_blank&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;https://developer.apple.com/av-foundation/HEVC-Stereo-Video-Profile.pdf&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;23.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;H.265 : High efficiency video coding Spec, &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.itu.int%2Frec%2FT-REC-H.265-202407-I%2Fen&quot; target=&quot;_blank&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;https://www.itu.int/rec/T-REC-H.265-202407-I/en&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;24.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;ISO/IEC 14496-15:2022(en), &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.iso.org%2Fobp%2Fui%2Fen%2F%23iso%3Astd%3Aiso-iec%3A14496%3A-15%3Aed-6%3Av1%3Aen&quot; target=&quot;_blank&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;https://www.iso.org/obp/ui/en/#iso:std:iso-iec:14496:-15:ed-6:v1:en&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;25.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;QuickTime File Format, &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.apple.com%2Fdocumentation%2Fquicktime-file-format&quot; target=&quot;_blank&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;https://developer.apple.com/documentation/quicktime-file-format&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;26.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbbs.nreal.cn%2Finfo%2F613d924045b547599e2495a9509f6bc0%3Fcsr%3D1&quot; target=&quot;_blank&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;https://bbs.nreal.cn/info/613d924045b547599e2495a9509f6bc0?csr=1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;27.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.worldlabs.ai%2F&quot; target=&quot;_blank&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;https://www.worldlabs.ai/&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;span&gt;28.&lt;/span&gt; 
 &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fabout.fb.com%2Fnews%2F2024%2F09%2Fintroducing-orion-our-first-true-augmented-reality-glasses%2F&quot; target=&quot;_blank&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;https://about.fb.com/news/2024/09/introducing-orion-our-first-true-augmented-reality-glasses/&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;/span&gt; 
&lt;/div&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/u/4090830/blog/18190045</link>
            <guid isPermaLink="false">https://my.oschina.net/u/4090830/blog/18190045</guid>
            <pubDate>Sun, 13 Apr 2025 03:07:00 GMT</pubDate>
            <author>原创</author>
        </item>
        <item>
            <title>OpenAI 新的推理模型幻觉更严重</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;OpenAI 近日推出的 &lt;u&gt;&lt;a href=&quot;https://www.oschina.net/news/345032/openai-gpt-o3-and-o4-mini&quot;&gt;o3/o4-mini&lt;/a&gt;&lt;/u&gt; 虽然在多方面有了不小的进步，然而新模型在「幻觉」内容（虚构的内容）方面，相较于旧模型会产生更多。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;552&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0421/110513_bW9f_2720166.png&quot; width=&quot;1378&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;在近期发布的 o3 和 o4-mini 模型的系统卡（技术报告）中，OpenAI 承认这些模型出现了比其前代模型更频繁的「幻觉」（编造不实信息）现象。&lt;/p&gt; 
&lt;p&gt;在一项衡量模型对人物知识准确性的基准测试中，o3 的幻觉率达到 33%，接近前代模型 o1 和 o3-mini 的两倍；而 o4-mini 的表现更差，幻觉率达 48%。&lt;/p&gt; 
&lt;p&gt;OpenAI 承认目前尚不完全清楚为何，并指出这可能与模型做出更多宏观陈述有关。第三方研究也印证了这一问题，包括发现 o3 有时会编造实际上无法执行的中间步骤，或者生成无效链接。&lt;/p&gt; 
&lt;p&gt;据 OpenAI 的内部测试，o3 和 o4-mini 两款新模型比 OpenAI 此前的推理模型（o1、o1-mini、o3-mini）以及传统的「非推理」模型，都更容易产生幻觉。报道还表示，更令人担忧的是连 ChatGPT 的开发人员都不知道为何会这样：&lt;/p&gt; 
&lt;p&gt;OpenAI 在 o3/o4-mini 的技术报告中表示，需要更多的研究内容来了解「为什么随着推理模型的发展，反而幻觉情况反而更糟糕」这一问题。报道指出，尽管 o3/o4-mini 在编程和数学等方面优于以往的模型，但由于模型输出的答案总量增加，导致其会给出更多准确的判断，同时也不可避免地出现更多错误的内容甚至是「幻觉」。&lt;/p&gt; 
&lt;p&gt;在 OpenAI 设计的内部基准测试 PersonQA（用于衡量模型对知识准确性的基准测试）中，o3 出现幻觉的比例达到 33%，约是前代推理模型 o1（16%）和 o3-mini（14.8%）的两倍。在同一基准测试中，o4-mini 的表现更差，幻觉率高达 48%。&lt;/p&gt; 
&lt;p&gt;另据第三方机构 Transluce 的测试，o3 在回答问题时经常会编造出某些「过程操作」。据一次测试显示，o3 声称自己在一台 2021 款 MacBook Pro 上通过「ChatGPT 之外」的方式运行了生成的代码，并将结果复制到答案中。但事实上，o3 只是拥有一部分工具的访问权，但并不具备执行操作的能力。&lt;/p&gt; 
&lt;p&gt;另据斯坦福大学兼职教授 Kian Katanforoosh 告诉 TechCrunch，其团队测试 o3 的编程能力时发现，o3 经常会援引错误的网站链接，提供的网站实际上是不存在的。&lt;/p&gt; 
&lt;p&gt;报道指出，提高模型准确性的一种办法即是「联网搜索」，OpenAI 的 GPT-4o 便是依靠联网搜索能力在 SimpleQA 中获得 90% 的准确率。&lt;/p&gt; 
&lt;p&gt;目前，OpenAI 发言人 Niko Felix 回应表示：「解决幻觉问题是我们一直在推进的重点研究方向，OpenAI 也在不断努力提升模型的准确性与可靠性。」&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/345677/openais-new-reasoning-ai-models-hallucinate-more</link>
            <guid isPermaLink="false">https://www.oschina.net/news/345677/openais-new-reasoning-ai-models-hallucinate-more</guid>
            <pubDate>Sun, 13 Apr 2025 03:07:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>微软开源 MineWorld：基于 Minecraft 的实时交互式世界模型</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;微软发布了名为「MineWorld」的开源项目，这是一个基于 Minecraft（&lt;em&gt;《我的世界》&lt;/em&gt;）的实时交互式世界模型。&lt;/p&gt; 
&lt;p&gt;MineWorld 以 Transformer&amp;nbsp;为核心，并结合大热门沙盒游戏《我的世界》开发而成。这是因为游戏是评估、训练 Agent 在感知、决策、预测，以及在动态复杂环境的综合处理能力的最佳场景之一。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-7cfc4ac028c3a02164b8a1dddab6f340d5c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;作为世界模型，MineWorld 可依据过去观察和当前动作预测未来游戏状态，智能体借此评估不同行动后果，选择最优策略，例如，在游戏中根据预测状态决定前进、后退等动作以达成目标。&lt;/p&gt; 
&lt;p&gt;MineWorld 还在训练过程中学习到的状态与动作关系，帮助智能体更好理解动作效果，精准执行决策，提高行动成功率。在与环境交互时，实时性十分关键。MineWorld 通过创新的并行解码算法，实现每秒生成&amp;nbsp; 4-7 帧的速度，快速响应玩家动作输入。这使得智能体在与玩家或其他智能体交互时，能及时获取最新环境信息并做出相应反应。&lt;/p&gt; 
&lt;p&gt;根据测试数据显示，MineWorld 在多方面远超知名世界模型 Oasis。视频质量上，3 亿参数的 MineWorld 的 FVD 值 246 低于 Oasis 的 377，SSIM 值 0.38 高于 Oasis 的 0.36。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0421/105514_it9H_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;可控性方面，MineWorld 的 3 亿和 7 亿参数模型 F1 分数达 0.70，12 亿参数模型为 0.73，远高于 Oasis 的 0.41；相机控制 L1 损失也更低。推理速度上，MineWorld 每秒生成 5.91 帧，远超 Oasis 的 2.58 帧。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;开源地址：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2FMineWorld&quot; target=&quot;_blank&quot;&gt;https://github.com/microsoft/MineWorld&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/345675/microsoft-opensource-mineworld</link>
            <guid isPermaLink="false">https://www.oschina.net/news/345675/microsoft-opensource-mineworld</guid>
            <pubDate>Sun, 13 Apr 2025 02:56:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
    </channel>
</rss>