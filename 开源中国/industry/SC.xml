<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - industry - 简体中文</title>
    <link>https://www.oschina.net/news/industry</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news/industry" rel="self" type="application/rss+xml"/>
    <description>已对该 RSS 进行格式化操作：中英字符之间插入空格、使用直角引号、标点符号修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 26 Aug 2025 07:41:10 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>鸿蒙新手福音！每日 300 分钟免费时长+海量鸿蒙真机，轻松上手云测云调​</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="margin-left:0; margin-right:0"&gt;开发者们是否常因真机设备不足、测试流程繁琐及硬件成本高昂而受阻？HUAWEI AppGallery Connect 云测试、云调试能力，通过免设备投入、低操作门槛及海量鸿蒙真机资源，让鸿蒙应用测试变得简单又高效。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;核心能力亮点：&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;海量鸿蒙真机在线选：平台配备了多种型号的鸿蒙真机，覆盖主流/热门机型，满足多样化测试场景需求，满足开发者在各种场景下的测试需求，无需自己购买设备。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;每天 300 分钟免费使用时长：每天提供 300 分钟的免费使用时间，足够支撑新手尝鲜、轻量级项目测试或多次验证，0 成本起步测试，立省真机购买投入！&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;上手快且操作简单：平台界面简洁，操作流程直观，新手无需复杂学习，按照操作指引很快就能上手使用，专注于应用测试本身。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;新手常见问题解答：&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Q1：应用马上要上线了，自己的手机不是鸿蒙系统，有什么测试渠道吗？&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;A1：通过云测试+云调试申请很便捷。登录 AppGallery Connect 平台后，在设备列表中选择你需要的鸿蒙真机型号，点击申请即可，无需繁琐的审批流程，还能享受每日 300 分钟免费时长。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&lt;img alt="鸿蒙新手福音！每日 300 分钟免费时长+海量鸿蒙真机，轻松上手云测云调" src="https://oscimg.oschina.net/oscnet//f7983403a41ded04543d099c78760d95.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Q2：每日免费的 300 分钟时长，是只能用一台测试机吗？&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;A2：不是的。每日都会发放 300 分钟使用时长，可以在平台上切换不同的鸿蒙真机进行测试，只要每日累计使用时间不超过 300 分钟，都可以免费使用。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Q3：测试过程中，能像操作自己的手机一样操控测试机吗？&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;A3：可以。远程操控体验和操作自己的手机类似，可以在测试机上安装应用、点击操作、输入内容等，真实还原应用的使用场景。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Q4：除了基础的功能测试，能测试应用的性能吗？&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;A4：可以。云测试可全面检测应用兼容性、性能、稳定性、功耗及 UX 等关键指标，帮助你了解应用在真机上的性能表现，便于进行优化。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Q5：在云调试时，能实时查看代码运行情况并修改吗？&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;A5：可以。云调试支持实时查看代码运行状态，真实运行环境精准复现用户场景，断点、日志即时获取，可对代码进行修改并重新调试，快速定位并解决问题。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Q6：测试完成后，能保存测试过程中的数据或截图吗？&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;A6：可以。平台支持保存测试过程中的截图、日志等数据，方便你后续查看和分析，更好地排查应用存在的问题。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Q7：如果每日 300 分钟免费时长用完了，还想继续使用怎么办？&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;A7：每日的免费时长用完后，可以等待次日免费时长刷新或在平台上选择付费套餐继续使用，套餐价格灵活，能满足不同开发者的需求，成本远低于购置真机，按需付费毫无压力！。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&lt;img alt="鸿蒙新手福音！每日 300 分钟免费时长+海量鸿蒙真机，轻松上手云测云调" height="260" src="https://oscimg.oschina.net/oscnet//8afb5d635df5346fdba209f87279676e.png" width="800" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;如果你是鸿蒙应用开发新手，想要轻松解决真机测试难题，不妨试试云测试+云调试能力。每日赠 300 分钟免费时长！轻量测试 0 成本起步，极简操作，高效输出报告。成本低、易上手，点此立即试用 &amp;gt;&amp;gt;&lt;/p&gt; 
&lt;p style="color:#2b2b2b; margin-left:0; margin-right:0; text-align:start"&gt;AppGallery Connect 致力于为应用的创意、开发、分发、运营、经营各环节提供一站式服务，构建全场景智慧化的应用生态体验。为给你带来更好服务，请扫描下方二维码或者点击此处免费咨询。&lt;/p&gt; 
&lt;p style="color:#2b2b2b; margin-left:0; margin-right:0; text-align:start"&gt;&lt;img alt="鸿蒙新手福音！每日 300 分钟免费时长+海量鸿蒙真机，轻松上手云测云调" height="120" src="https://oscimg.oschina.net/oscnet//1602863077126f569fd497524b470ed6.png" width="120" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2b2b2b; margin-left:0; margin-right:0; text-align:start"&gt;如有任何疑问，请发送邮件至 agconnect@huawei.com 咨询，感谢你对 HUAWEI AppGallery Connect 的支持！&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368551</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368551</guid>
      <pubDate>Tue, 26 Aug 2025 07:21:07 GMT</pubDate>
      <author>作者: 开源科技</author>
    </item>
    <item>
      <title>得物新商品审核链路建设分享</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;span id="OSC_h1_1"&gt;&lt;/span&gt; 
&lt;h1&gt;一、 前言&lt;/h1&gt; 
&lt;p&gt;得物近年来发展迅猛，平台商品类目覆盖越来越广，商品量级越来越大。而以往得物的上新动作更多依赖于传统方式，效率较低，无法满足现有的上新诉求。那么如何能实现更加快速的上新、更加高效的上新，就成为了一个至关重要的命题。&lt;/p&gt; 
&lt;p&gt;近两年 AI 大模型技术的发展，使得发布和审核逐渐向 AI 驱动的方式转变成为可能。因此，我们可以探索利用算法能力和大模型能力，结合业务自身规则，构建更加全面和精准的规则审核点，以实现更高效的工作流程，最终达到我们的目标。&lt;/p&gt; 
&lt;p&gt;本文围绕 AI 审核，介绍机审链路建设思想、规则审核点实现快速接入等核心逻辑。&lt;/p&gt; 
&lt;span id="OSC_h1_2"&gt;&lt;/span&gt; 
&lt;h1&gt;二、如何实现高效审核&lt;/h1&gt; 
&lt;p&gt;对于高效审核的理解，主要可以拆解成「高质量」、「高效率」。目前对于「高质量」的动作包括，基于不同的类目建设对应的机审规则、机审能力，再通过人工抽查、问题 Case 分析的方式，优化算法能力，逐步推进「高质量」的效果。&lt;/p&gt; 
&lt;p&gt;而「高效率」，核心又可以分成业务高效与技术高效。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;业务高效&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;逐步通过机器审核能力优化审核流程，以解决资源不足导致上新审核时出现进展阻碍的问题。&lt;/li&gt; 
 &lt;li&gt;通过建设机审配置业务，产品、业务可以直观的维护类目-机审规则-白名单配置，从而高效的调整机审策略。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;技术高效&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;通过建设动态配置能力，实现快速接入新的机审规则、调整机审规则等，无需代码发布，即配即生效。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Q2 在搭建了动态配置能力之后，算法相关的机审规则接入效率提升了 70% 左右。&lt;/p&gt; 
&lt;span id="OSC_h1_3"&gt;&lt;/span&gt; 
&lt;h1&gt;三、动态配置实现思路&lt;/h1&gt; 
&lt;p&gt;建设新版机审链路前的调研中，我们对于老机审链路的规则以及新机审规则进行了分析，发现算法类机审规则占比超过 70% 以上，而算法类的机审规则接入的流程比较固化，核心分成三步：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;与算法同学沟通定义好接口协议&lt;/li&gt; 
 &lt;li&gt;基于商品信息构建请求参数，通过 HTTP 请求算法提供的 URL，从而获取到算法结果。&lt;/li&gt; 
 &lt;li&gt;解析算法返回的结果，与自身商品信息结合，输出最终的机审结果。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;而算法协议所需要的信息通常都可以从商品中获取到，因此通过引入「反射机制」、「HTTP 泛化调用」、「规则引擎」等能力，实现算法规则通过 JSON 配置即可实现算法接入。&lt;/p&gt; 
&lt;span id="OSC_h1_4"&gt;&lt;/span&gt; 
&lt;h1&gt;四、商品审核方式演进介绍&lt;/h1&gt; 
&lt;p&gt;&lt;img alt="" src="https://mp.toutiao.com/mp/agw/article_material/open_image/get?code=YmRjMjQ4ZmI2YTM2MmZkMWE4ZDI0NzI4MzhmMDA0ZGUsMTc1NjE4OTk2Nzc1Ng==" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;商品审核方式的演进&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;人审&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;依赖商管、运营，对商品上架各字段是否符合得物上新标准进行人工核查。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;机审&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;对于部分明确的业务规则，比如白底图、图片清晰度、是否重复品、是否同质品等，机审做前置校验并输出机审结果，辅助人工审核，降低审核成本，提升审核效率。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;AI 审核&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;通过丰富算法能力、强化 AI 大模型能力、雷达技术等，建设越来越多的商品审核点，并推动召回率、准确率的提升，达标的审核点可通过自动驳回、自动修改等 action 接管商品审核，降低人工审核的占比，降低人工成本。&lt;/p&gt; 
&lt;span id="OSC_h1_5"&gt;&lt;/span&gt; 
&lt;h1&gt;五、现状问题分析&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;产品层面&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;机审能力不足，部分字段没覆盖，部分规则不合理：&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;机审字段覆盖度待提升&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;机审规则采纳率不足&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;部分机审规则不合理&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;缺少产品配置化能力，配置黑盒化，需求迭代费力度较高：&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;规则配置黑盒&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;规则执行结果缺乏 trace 和透传&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;调整规则依赖开发和发布&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;缺少规则执行数据埋点&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;技术层面&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;系统可扩展性不足，研发效率低：&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;业务链路（AI 发品、审核、预检等）不支持配置化和复用&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;规则节点不支持配置化和复用&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="OSC_h1_6"&gt;&lt;/span&gt; 
&lt;h1&gt;六、流程介绍&lt;/h1&gt; 
&lt;p&gt;搭建机审配置后台，可以通过配置应用场景+业务身份+商品维度配置来确定所需执行的全量规则，规则可复用。&lt;/p&gt; 
&lt;p&gt;其中应用场景代表业务场景，如商品上新审核、商家发品预检、AI 发品预检等；业务身份则表示不同业务场景下不同方式，如常规渠道商品上新的业务场景下，AI 发布、常规商品上新（商家后台、交易后台等）、FSPU 同款发布品等。&lt;/p&gt; 
&lt;p&gt;当商品变更，通过 Binlog 日志触发机审，根据当前的应用场景+业务身份+商品信息，构建对应的机审执行链（ProcessChain）完成机审执行，不同的机审规则不通过支持不同的 action，如自动修正、自动驳回、自动通过等。&lt;/p&gt; 
&lt;p&gt;链路执行流程图如下：&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://mp.toutiao.com/mp/agw/article_material/open_image/get?code=NjYyNDM4M2JmNzY2Njg5ZDFjYmI4MDQwZTkzZTkwODMsMTc1NjE4OTk2Nzc1Ng==" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h1_7"&gt;&lt;/span&gt; 
&lt;h1&gt;七、详细设计&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;整体架构图&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://mp.toutiao.com/mp/agw/article_material/open_image/get?code=MjZiYmQyYWJhMWFjYTJjNzU3NzM3Zjg1NGQ4OTMyNWMsMTc1NjE4OTk2Nzc1Ng==" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;业务实体&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ER 图&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://mp.toutiao.com/mp/agw/article_material/open_image/get?code=ODQyN2IzNjk1ZjU3NDJiYmY4MGYxNzQwNTNiODBlMmEsMTc1NjE4OTk2Nzc1Ng==" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;含义解释&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;※ 业务场景&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;触发机审的应用场景，如新品发布、商家新品预检等。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;※ 业务身份&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;对于某个应用场景，进一步区分业务场景，如新品发布的场景下，又有 AI 发品、常规发品、FSPU 同款发品等。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;※ 业务规则&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;各行业线对于商品的审核规则，如校验图片是否是白底图、结构化标题中的类目需与商品类目一致、发售日期不能超过 60 天等。同一个业务规则可以因为业务线不同，配置不同的机审规则。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;※ 规则组&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;对规则的分类，通常是商品字段模块的名称，&lt;strong&gt;一个规则组下可以有多个业务规则，如商品轮播图作为规则组，可以有校验图片是否白底图、校验图片是否清晰、校验模特姿势是否合规&lt;/strong&gt;等。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;※ 机审规则&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;对商品某个商品字段模块的识别并给出审核结果，&lt;strong&gt;数据依赖机审能力以及 spu 本身&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;※ 机审能力&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;商品信息（一个或多个商品字段模块）的审核数据获取，&lt;strong&gt;通常需要调用外部接口&lt;/strong&gt;，用于机审规则审核识别。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;※ 业务&amp;amp;机审规则关联关系&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;描述业务规则和机审规则的关联关系，同一个业务规则可以根据不同业务线，给予不同的机审规则，如轮播图校验正背面，部分业务线要求校验全量轮播图，部分业务线只需要校验轮播图首图/规格首图。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;机审执行流程框架&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;流程框架&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;通过责任链、策略模式等设计模式实现流程框架。&lt;/p&gt; 
&lt;p&gt;触发机审后会根据当前的业务场景、业务身份、商品信息等，获取到对应的业务身份执行链（不同业务身份绑定不同的执行节点，最终构建出来一个执行链）并启动机审流程执行。&lt;/p&gt; 
&lt;p&gt;由于机审规则中存在数据获取 rt 较长的情况，如部分依赖大模型的算法能力、雷达获取三方数据等，我们通过异步回调的方式解决这种场景，也因此衍生出了「异步结果更新机审触发」。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;※ 完整机审触发&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;完整机审触发是指商品变更后，通过 Binlog 日志校验当前商品是否满足触发机审，命中的机审规则中如果依赖异步回调的能力，则会生成 pendingId，并记录对应的机审结果为「pending」（其他规则不受该 pending 结果的影响），并监听对应的 topic。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://mp.toutiao.com/mp/agw/article_material/open_image/get?code=ZGM5YTNjMzUyMzA5YjQzMDczZDgzNzc0ODQyOWNiMzIsMTc1NjE4OTk2Nzc1Ng==" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;※ 异步结果更新机审触发&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;部分 pending 规则产出结果后发送消息到机审场景，通过 pendingId 以及对应的商品信息确认业务身份，获取异步结果更新责任链（&lt;strong&gt;与完整机审的责任链不同&lt;/strong&gt;）再次执行机审执行责任链。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://mp.toutiao.com/mp/agw/article_material/open_image/get?code=ODdlOWEyYTc1MTA2MGU2YzM4YTJmNTRlOWFmOGFlNzIsMTc1NjE4OTk2Nzc1Ng==" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;动态配置能力建设&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;调研&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;新机审链路建设不仅要支持机审规则复用，支持不同业务身份配置接入，还要&lt;strong&gt;支持新机审规则快速接入&lt;/strong&gt;，降低开发投入的同时，还能快速响应业务的诉求。&lt;/p&gt; 
&lt;p&gt;经过分析，机审规则绝大部分下游为算法链路，并且算法的接入方式较为固化，即「构建请求参数」 -&amp;gt; 「发起请求」 -&amp;gt; 「结果解析」，并且数据模型通常较为简单。因此技术调研之后，通过&lt;strong&gt;HTTP 泛化调用&lt;/strong&gt;实现&lt;strong&gt;构建请求参数&lt;/strong&gt;、&lt;strong&gt;发起请求&lt;/strong&gt;，利用**规则引擎（规则表达式）**实现结果解析。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;规则引擎技术选型&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;调研市面上的几种常用规则引擎，基于历史使用经验、上手难度、文档阅读难度、性能等方面综合考虑，最终决定选用&lt;strong&gt;QLExpress&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://mp.toutiao.com/mp/agw/article_material/open_image/get?code=N2Y4ZDNlOWE1ZWNkODE4OGExZmRlZmU2ODIxNDVmODUsMTc1NjE4OTk2Nzc1Ng==" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;**&amp;nbsp;HTTP 泛化调用能力建设**&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;※ 实现逻辑&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;定义 MachineAuditAbilityEnum 统一的动态配置枚举，并基于 MachineAuditAbilityProcess 实现其实现类。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;统一入参为 Map 结构，通过反射机制、动态 Function 等方式，实现商品信息映射成算法请求参数；&lt;strong&gt;另外为了提升反射的效率，利用预编译缓存的方式，将字段转成 MethodHandle&lt;/strong&gt;，后续对同一个字段做反射时，可直接获取对应的 MethodHandle，提升效率。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;/**
&amp;nbsp;* 缓存类字段的 MethodHandle（Key: Class+FieldName, Value: MethodHandle）
&amp;nbsp; */
private&amp;nbsp;static&amp;nbsp;final&amp;nbsp;Map&amp;lt;String,&amp;nbsp;MethodHandle&amp;gt;&amp;nbsp;FIELD_HANDLE_CACHE&amp;nbsp;=&amp;nbsp;new&amp;nbsp;ConcurrentHashMap&amp;lt;&amp;gt;();


/**
&amp;nbsp;* 根据配置从对象中提取字段值到 Map
&amp;nbsp;*&amp;nbsp;@return&amp;nbsp;提取后的 Map
&amp;nbsp;*/
public&amp;nbsp;Map&amp;lt;String,&amp;nbsp;Object&amp;gt;&amp;nbsp;fieldValueMapping(AutoMachineAlgoRequestConfig requestConfig,&amp;nbsp;Object&amp;nbsp;spuResDTO) {
&amp;nbsp; &amp;nbsp;&amp;nbsp;AutoMachineAlgoRequestConfig.RequestMappingConfig&amp;nbsp;requestMappingConfig = requestConfig.getRequestMappingConfig();
&amp;nbsp; &amp;nbsp;&amp;nbsp;Map&amp;lt;String,&amp;nbsp;Object&amp;gt; targetMap =&amp;nbsp;Maps.newHashMap();
&amp;nbsp; &amp;nbsp;&amp;nbsp;//1.简单映射关系，直接将 obj 里的信息映射到 resultMap 当中


&amp;nbsp; &amp;nbsp;&amp;nbsp;//2.遍历复杂映射关系，value 是基础类型
&amp;nbsp; &amp;nbsp;&amp;nbsp;//3.遍历复杂映射关系，value 是对象


&amp;nbsp;&amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;return&amp;nbsp;targetMap;
}


/**
&amp;nbsp;* &amp;nbsp;预编译 FieldMapping
&amp;nbsp; */
private&amp;nbsp;List&amp;lt;AutoMachineAlgoRequestConfig.FieldMapping&amp;gt;&amp;nbsp;compileConfig(List&amp;lt;AutoMachineAlgoRequestConfig.FieldMapping&amp;gt; fieldMappingList,&amp;nbsp;Object&amp;nbsp;obj) {
&amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;List&amp;lt;AutoMachineAlgoRequestConfig.FieldMapping&amp;gt; mappings =&amp;nbsp;new&amp;nbsp;ArrayList&amp;lt;&amp;gt;(fieldMappingList.size());
&amp;nbsp; &amp;nbsp;&amp;nbsp;//缓存反射 mapping
&amp;nbsp; &amp;nbsp;&amp;nbsp;return&amp;nbsp;mappings;
}


private&amp;nbsp;Object&amp;nbsp;getFieldValue(Object&amp;nbsp;request,&amp;nbsp;String&amp;nbsp;fieldName) throws&amp;nbsp;Throwable&amp;nbsp;{
&amp;nbsp; &amp;nbsp;&amp;nbsp;String&amp;nbsp;cacheKey = request.getClass().getName() +&amp;nbsp;"#"&amp;nbsp;+ fieldName;
&amp;nbsp; &amp;nbsp;&amp;nbsp;MethodHandle&amp;nbsp;handle =&amp;nbsp;FIELD_HANDLE_CACHE.get(cacheKey);
&amp;nbsp; &amp;nbsp;&amp;nbsp;return&amp;nbsp;handle !=&amp;nbsp;null&amp;nbsp;? handle.invoke(request) :&amp;nbsp;null;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;基于实现@FeignClient 注解，实现 HTTP 调用的执行器，其中@FeignClient 中的 URL 表示域名，autoMachineAuditAlgo 方法中的 path 表示具体的 URL，requestBody 是请求体，另外还包含 headers，不同算法需要不同 headers 也可动态配置。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;返回结果均为 String，而后解析成 Map&amp;lt;String,Object&amp;gt;用于规则解析。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;@FeignClient(
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; name =&amp;nbsp;"xxx",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; url =&amp;nbsp;"${}"
)
public&amp;nbsp;interface&amp;nbsp;GenericAlgoFeignClient&amp;nbsp;{


&amp;nbsp; &amp;nbsp;&amp;nbsp;@PostMapping(value =&amp;nbsp;"/{path}")
&amp;nbsp; &amp;nbsp;&amp;nbsp;String&amp;nbsp;autoMachineAuditAlgo(
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;@PathVariable("path")&amp;nbsp;String&amp;nbsp;path,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;@RequestBody&amp;nbsp;Object&amp;nbsp;requestBody,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;@RequestHeader&amp;nbsp;Map&amp;lt;String,&amp;nbsp;String&amp;gt; headers
&amp;nbsp; &amp;nbsp; );
&amp;nbsp; &amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;@GetMapping("/{path}")
&amp;nbsp; &amp;nbsp;&amp;nbsp;String&amp;nbsp;autoMachineAuditAlgoGet(
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;@PathVariable("path")&amp;nbsp;String&amp;nbsp;path,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;@RequestParam&amp;nbsp;Map&amp;lt;String,&amp;nbsp;Object&amp;gt; queryParams,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;@RequestHeader&amp;nbsp;Map&amp;lt;String,&amp;nbsp;String&amp;gt; headers
&amp;nbsp; &amp;nbsp; );


}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;动态配置 JSON。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;{
&amp;nbsp; &amp;nbsp;&amp;nbsp;"url":&amp;nbsp;"/ai-check/demo1",
&amp;nbsp; &amp;nbsp;&amp;nbsp;"requestMappingConfig":&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"fieldMappingList":&amp;nbsp;[
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"sourceFieldName":&amp;nbsp;"categoryId",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"targetKey":&amp;nbsp;"categoryId"
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;},
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"sourceFieldName":&amp;nbsp;"brandId",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"targetKey":&amp;nbsp;"brandId"
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;}
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;],
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"perItemMapping":&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"mappingFunctionCode":&amp;nbsp;"firstAndFirstGroundPic",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"fieldMappingList":&amp;nbsp;[
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"sourceFieldName":&amp;nbsp;"imgId",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"targetKey":&amp;nbsp;"imgId"
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;},
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"sourceFieldName":&amp;nbsp;"imgUrl",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"targetKey":&amp;nbsp;"imgUrl"
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;}
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;]
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;}
&amp;nbsp; &amp;nbsp;&amp;nbsp;}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;机审规则动态解析建设&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;※ 实现逻辑&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;定义 MachineAuditRuleEnum 统一的动态配置枚举，并基于 MachineAuditRuleProcess 实现其统一实现类。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;搭建 QLExpress 规则引擎，&lt;strong&gt;为了提升 QLExpress 规则引擎的效率，同样引入了缓存机制&lt;/strong&gt;，在机审规则配置表达式时，则触发 loadRuleFromJson，将表达式转换成规则引擎并注入到缓存当中，真正机审流程执行时会直接从缓存里获取规则引擎并执行，效率上有很大提升。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;// 规则引擎实例缓存
private&amp;nbsp;static&amp;nbsp;final&amp;nbsp;Map&amp;lt;String,&amp;nbsp;ExpressRunner&amp;gt; runnerCache =&amp;nbsp;new&amp;nbsp;ConcurrentHashMap&amp;lt;&amp;gt;();


// 规则配置缓存
private&amp;nbsp;static&amp;nbsp;final&amp;nbsp;Map&amp;lt;String,&amp;nbsp;GenericEngineRule&amp;gt; ruleConfigCache =&amp;nbsp;new&amp;nbsp;ConcurrentHashMap&amp;lt;&amp;gt;();


// 规则版本信息
private&amp;nbsp;static&amp;nbsp;final&amp;nbsp;Map&amp;lt;String,&amp;nbsp;Integer&amp;gt; ruleVersionCache =&amp;nbsp;new&amp;nbsp;ConcurrentHashMap&amp;lt;&amp;gt;();


/**
&amp;nbsp;* 加载 JSON 规则配置
&amp;nbsp;*&amp;nbsp;@param&amp;nbsp;jsonConfig 规则 JSON 配置
&amp;nbsp;*/
public&amp;nbsp;GenericEngineRule&amp;nbsp;loadRuleFromJson(String&amp;nbsp;ruleCode,&amp;nbsp;String&amp;nbsp;jsonConfig) {


&amp;nbsp; &amp;nbsp;&amp;nbsp;//如果缓存里已经有并且是最新版本，则直接返回
&amp;nbsp; &amp;nbsp;&amp;nbsp;if(machineAuditCache.isSameRuleConfigVersion(ruleCode) &amp;amp;&amp;amp; machineAuditCache.getRuleConfigCache(ruleCode) !=&amp;nbsp;null) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;return&amp;nbsp;machineAuditCache.getRuleConfigCache(ruleCode);
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp;&amp;nbsp;// 如果是可缓存的规则，预加载


&amp;nbsp;&amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;return&amp;nbsp;rule;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;机审规则执行时，通过配置中的规则名称，获取对应的规则引擎进行执行。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;/**
&amp;nbsp;* 根据规则名称执行规则
&amp;nbsp;*&amp;nbsp;@param&amp;nbsp;ruleCode 规则名称
&amp;nbsp;*&amp;nbsp;@param&amp;nbsp;context 上下文数据
&amp;nbsp;*&amp;nbsp;@return&amp;nbsp;规则执行结果
&amp;nbsp;*/
public&amp;nbsp;MachineAuditRuleResult&amp;nbsp;executeRuleByCode(String&amp;nbsp;ruleCode,&amp;nbsp;Map&amp;lt;String,&amp;nbsp;Object&amp;gt; context, MachineAuditRuleProcessData ruleProcessData) {
&amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(StringUtils.isBlank(ruleCode)) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;throw&amp;nbsp;new&amp;nbsp;IllegalArgumentException("机审-通用协议-规则-规则名称不能为空");
&amp;nbsp; &amp;nbsp; }


&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;//从缓存中获取规则引擎


&amp;nbsp; &amp;nbsp;&amp;nbsp;//基于规则引擎执行 condition


&amp;nbsp; &amp;nbsp;&amp;nbsp;//统一日志
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;※ 配置 demo&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;动态配置 JSON。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;{
&amp;nbsp; &amp;nbsp;&amp;nbsp;"ruleCode":&amp;nbsp;"demo1",
&amp;nbsp; &amp;nbsp;&amp;nbsp;"name":&amp;nbsp;"规则 demo1",
&amp;nbsp; &amp;nbsp;&amp;nbsp;"ruleType":&amp;nbsp;1,
&amp;nbsp; &amp;nbsp;&amp;nbsp;"priority":&amp;nbsp;100,
&amp;nbsp; &amp;nbsp;&amp;nbsp;"functions":&amp;nbsp;[
&amp;nbsp; &amp;nbsp;&amp;nbsp;],
&amp;nbsp; &amp;nbsp;&amp;nbsp;"conditions":&amp;nbsp;[
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"expression":&amp;nbsp;"result.code == null || result.code != 0",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"action":&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"type":&amp;nbsp;"NO_RESULT",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"messageExpression":&amp;nbsp;"'无结果'"
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;}
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;},
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"expression":&amp;nbsp;"result.data == 0",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"action":&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"type":&amp;nbsp;"PASS",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"messageExpression":&amp;nbsp;"'机审通过"
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;}
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;},
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"expression":&amp;nbsp;"result.data == 1",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"action":&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"type":&amp;nbsp;"REJECT",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"messageExpression":&amp;nbsp;"'异常结果 1'",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"suggestType":&amp;nbsp;2,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"suggestKey":&amp;nbsp;"imgId",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"preAuditSuggestKey":&amp;nbsp;"imgUrl"
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;}
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;},
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"expression":&amp;nbsp;"result.data == 2",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"action":&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"type":&amp;nbsp;"REJECT",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"messageExpression":&amp;nbsp;"'异常结果 2'",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"suggestType":&amp;nbsp;2,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"suggestKey":&amp;nbsp;"imgId",
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"preAuditSuggestKey":&amp;nbsp;"imgUrl"
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;}
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;}
&amp;nbsp; &amp;nbsp;&amp;nbsp;],
&amp;nbsp; &amp;nbsp;&amp;nbsp;"defaultAction":&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;"type":&amp;nbsp;"PASS"
&amp;nbsp; &amp;nbsp;&amp;nbsp;}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;span id="OSC_h1_8"&gt;&lt;/span&gt; 
&lt;h1&gt;八、关于数据分析&amp;amp;指标提升&lt;/h1&gt; 
&lt;p&gt;在经历了 2-3 个版本搭建完新机审链路 + 数据埋点之后，指标一直没有得到很好的提升，曾经一度只是维持在 20% 以内，甚至有部分时间降低到了 10% 以下；经过大量的数据分析之后，识别出了部分规则产品逻辑存在漏洞、算法存在误识别等情况，并较为有效的通过数据推动了产品优化逻辑、部分类目规则调整、算法迭代优化等，在一系列的动作做完之后，指标提升了 50%+。&lt;/p&gt; 
&lt;p&gt;在持续了比较长的一段时间的 50%+覆盖率之后，对数据进行了进一步的剖析，发现这 50%+在那个时间点应该是到了瓶颈，原因是像「标题描述包含颜色相关字样」、「标题存在重复文案」以及部分轮播图规则，实际就是会存在不符合预期的情况，因此紧急与产品沟通，后续的非紧急需求停止，先考虑将这部分天然不符合预期的情况进行处理。&lt;/p&gt; 
&lt;p&gt;之后指标提升的动作主要围绕：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;算法侧产出各算法能力的召回率、准确率，达标的算法由产品与业务拉齐，是否配置自动驳回的能力。&lt;/li&gt; 
 &lt;li&gt;部分缺乏自动修改能力的机审规则，补充临时需求建设对应的能力。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;经过产研业务各方的配合，以最快速度将这些动作进行落地，指标也得到了较大的提升。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;往期回顾&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;1.营销会场预览直通车实践｜得物技术&lt;/p&gt; 
&lt;p&gt;2.基于 TinyMce 富文本编辑器的客服自研知识库的技术探索和实践｜得物技术&lt;/p&gt; 
&lt;p&gt;3.AI 质量专项报告自动分析生成｜得物技术&lt;/p&gt; 
&lt;p&gt;4.社区搜索离线回溯系统设计：架构、挑战与性能优化｜得物技术&lt;/p&gt; 
&lt;p&gt;5.eBPF 助力 NAS 分钟级别 Pod 实例溯源｜得物技术&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;文 / 沃克&lt;/p&gt; 
&lt;p&gt;关注得物技术，每周更新技术干货&lt;/p&gt; 
&lt;p&gt;要是觉得文章对你有帮助的话，欢迎评论转发点赞～&lt;/p&gt; 
&lt;p&gt;未经得物技术许可严禁转载，否则依法追究法律责任。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/5783135/blog/18689561</link>
      <guid isPermaLink="false">https://my.oschina.net/u/5783135/blog/18689561</guid>
      <pubDate>Tue, 26 Aug 2025 07:04:07 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>阿里国际发布并开源新一代多模态大模型 Ovis2</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;阿里国际&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FHTfKZDEzf197FzXEMn8htw" target="_blank"&gt;正式发布&lt;/a&gt;新一代多模态大模型 Ovis2.5。Ovis2.5 是一款面向原生分辨率视觉感知、深度推理与高性价比场景设计的多模态大模型。据称在主流多模态评测套件 OpenCompass 上的综合得分相较 Ovis2 进一步提升，并在同类开源模型中继续保持&amp;nbsp;SOTA 水平。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0826/150018_hr7W_2720166.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;本次开源包含两个版本：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Ovis2.5-9B：OpenCompass 综合得分&amp;nbsp;78.3，超越众多更大参数量的模型，在 40B 以下参数规模的开源模型中排名第一。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Ovis2.5-2B：OpenCompass 综合得分 73.9，延续了 Ovis 系列「小身板，大能量」的理念，在同尺寸模型中性能显著领先，是端侧和资源受限场景的理想选择。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Ovis2 整体框架：&lt;/p&gt; 
&lt;p&gt;&lt;img height="1131" src="https://static.oschina.net/uploads/space/2025/0826/145646_bXvR_2720166.png" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;官方介绍称，Ovis2.5 在架构、训练与数据三方面进行了系统性创新。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;模型架构：延续 Ovis 系列创新的结构化嵌入对齐设计。Ovis2.5 由三大组件构成：动态分辨率 ViT 高效提取视觉特征，Ovis 视觉词表模块实现视觉与文本嵌入的结构对齐，最后由强大的 Qwen3 作为语言基座，处理多模态嵌入并生成文本输出。&lt;/li&gt; 
 &lt;li&gt;训练策略：采用更精细的五阶段训练范式，从基础的视觉预训练、多模态预训练、大规模指令微调，到利用 DPO 和 GRPO 等算法进行偏好对齐和推理能力强化，循序渐进构建模型能力。同时，通过多模态数据打包和混合并行等优化，实现了 3-4 倍的端到端训练加速。&lt;/li&gt; 
 &lt;li&gt;数据工程：Ovis2.5 的数据规模相比 Ovis2 增加了 50%，重点聚焦视觉推理、图表、OCR、Grounding 等关键方向。尤其是合成了大量与 Qwen3 深度适配的「思考（thinking）」数据，有效激发了模型的反思与推理潜能。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;详情查看&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;代码：https://github.com/AIDC-AI/Ovis&lt;br&gt; 模型： https://huggingface.co/AIDC-AI/&lt;br&gt; Ovis2.5-2B https://huggingface.co/AIDC-AI/&lt;br&gt; Ovis2.5-9B Demo： https://huggingface.co/spaces/AIDC-AI/&lt;br&gt; Ovis2.5-2B https://huggingface.co/spaces/AIDC-AI/&lt;br&gt; Ovis2.5-9B 技术报告: https://arxiv.org/abs/2508.11737&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368548</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368548</guid>
      <pubDate>Tue, 26 Aug 2025 07:03:07 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>AI 搜索创企 Perplexity AI 推出版权收入分成模式</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;AI 搜索创企 Perplexity AI 宣布了一&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.bloomberg.com%2Fnews%2Farticles%2F2025-08-25%2Fperplexity-to-let-publishers-share-in-revenue-from-ai-searches" target="_blank"&gt;项重大举措&lt;/a&gt;，将设立数百万美元资金用于支付给媒体出版商和记者，旨在构建一种全新的搜索收入共享模式。&lt;/p&gt; 
&lt;p&gt;据这家总部位于旧金山的初创公司透露，其媒体合作伙伴很快将在作品被 Perplexity 的浏览器或 AI 助手用于解答用户疑问与请求时获得报酬。Perplexity 团队在一篇博客文章中表示：「&lt;strong&gt;我们正以适合人工智能时代的模式对出版商进行补偿。&lt;/strong&gt;」&lt;/p&gt; 
&lt;p&gt;此次资金发放将通过一项名为 Comet Plus 的订阅服务实施，该服务预计在未来几个月推出。据悉，Perplexity 已预留 4250 万美元资金用于与出版商分享，且该资金池有望随时间增长。Comet Plus 每月订阅费用为 5 美元，这对于已付费使用 Perplexity 高级版本的用户来说是一项额外福利。&lt;/p&gt; 
&lt;p&gt;此前，Perplexity AI 面临诸多媒体机构的诉讼，如《华尔街日报》《纽约时报》以及日本《读卖新闻》等，这些机构指控该公司不正当地从其作品中获利。而此次收入共享模式的推出，或可视为 Perplexity AI 向出版商抛出的 「橄榄枝」，以改善关系并应对相关指控。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368546</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368546</guid>
      <pubDate>Tue, 26 Aug 2025 06:53:07 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>宇树科技人形机器人足专利获授权</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;天眼查 App 显示，杭州宇树科技股份有限公司 8 月 26 日申请的「一种人形机器人足和一种人形机器人」专利获授权。&lt;/p&gt; 
&lt;p&gt;&lt;img height="287" src="https://oscimg.oschina.net/oscnet/up-85540c9e38b8e5950068b200c4d25b74447.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;摘要显示，本实用新型涉及人形机器人技术领域，提供的一种人形机器人足，包括足部外壳和弹性足底件，足部外壳和弹性足底件固定连接，弹性足底件内设有若干可形变的气腔，气腔连接有穿过足部外壳并与外部连通的气管件。&lt;/p&gt; 
&lt;p&gt;本实用新型提供的一种人形机器人足，将传感器等脆弱部件设置在远离频繁与地面撞击的位置，仅在弹性足底件内设置形变气腔，不易受到碰撞而损坏；通过气管件与气腔的空间变化传递挤压信息，响应快，不受电机本身特性的影响；弹性足底件与气腔一体化设计，更换维修方便快捷。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368544</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368544</guid>
      <pubDate>Tue, 26 Aug 2025 06:42:07 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>开发者反馈 DeepSeek-V3.1 出现严重 bug：返回内容随机插入「极/极/extreme」等字符</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;近日有开发者&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flinux.do%2Ft%2Ftopic%2F898502" target="_blank"&gt;反馈&lt;/a&gt;DeepSeek V3.1 在生成文本时会在完全不可预期的位置插入「极」「极」「extreme」三个 token。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0826/142557_OpI9_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;开源社区用户给出多组复现场景：在 Go 等语言生成里，模型会把词元「粘」到标识符中，`Second` 前随机插入「极/极/extreme」，即便是 `top_k=1， temperature=1` 的保守解码也躲不过。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-5528ee293cd32634855ecedec24f895f010.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;问题最早在火山、chutes 等第三方 API 被发现，最初怀疑与 IQ1_S 高压缩量化、imatrix 校准数据异常或部署配置错误有关，但随后测试证实官方网页端在 FP8 全精度下亦出现同样现象，&lt;strong&gt;且官方端出现概率最低，第三方显著升高&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0826/142634_Kbh4_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;中文简体「极」对应 ID 2577，繁体「极」对应 ID 16411，英文「extreme」对应 ID 15075。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;社区推测可能是训练数据清洗残留，或与模型「偷懒」机制相关，但部分案例仍无法解释。一旦触发，后续生成会愈发频繁，已严重影响编程及任何对结构敏感的任务可用性。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368538</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368538</guid>
      <pubDate>Tue, 19 Aug 2025 06:27:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>新一代中国操作系统银河麒麟 V11 正式发布</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;全新一代中国操作系统——银河麒麟操作系统 V11 在 2025 中国操作系统产业大会正式发布。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="332" src="https://oscimg.oschina.net/oscnet/up-e78b1fe4f30a2acbeef45b2c7d54dfd9611.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;这款系统使用了全新磐石架构，操作体验、安全性和生态丰富度大幅提升。作为首个突破百万生态的国产操作系统，银河麒麟的生态成熟度国内领先，与国产主流 CPU、GPU 及板卡实现全面兼容，构建起完整的国产化生态体系。已在嫦娥探月、天问探火等国家重大工程中发挥支撑作用，在政务领域保持较高覆盖率，同时在金融、能源、教育、医疗等行业实现长期规模化应用。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;目前，银河麒麟操作系统已部署超 1600 万套。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368537</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368537</guid>
      <pubDate>Tue, 19 Aug 2025 06:23:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>68% 的科技专业人士对 AI 招聘工具不信任</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span&gt;最新&lt;/span&gt;发布的 Dice 报告指出，68% 的科技专业人士对 AI 驱动的招聘系统表示不信任，同时 80% 的人更倾向于人类主导的招聘方法。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;报告显示，近 30% 的受访者考虑完全离开科技行业，因为他们对 AI 增强招聘过程的挫败感更为强烈，尤其是女性群体对此反应更为明显。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在接受 TechRepublic 的电子邮件采访时，Dice 首席执行官阿特・齐尔（Art Zeile）表示，尽管 AI 在提升招聘团队的工作效率方面发挥了积极作用，但 68% 的不信任比例 「并非小数目，这一信号表明，从候选人的角度来看，招聘系统的根本性问题依然存在。」 他补充说，30% 科技工作者考虑退出行业的调查结果尤其令人担忧。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;齐尔指出，这种不信任的根本原因在于 AI 在招聘中的应用方式，尤其是在缺乏透明度和人类监督的情况下。他强调：「候选人明确表示，当招聘过程像一个黑箱时，信任感就会消失。」 尽管雇主可能在提高运营效率，但如果这导致&lt;span&gt;顶尖&lt;/span&gt;人才的疏远，那对于整个行业来说就是一个双输的局面。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;根据 Dice 报告，92% 的科技专业人士认为，由于 AI 工具偏向于关键词优化，很多合格的候选人可能被忽视。齐尔表示：「这反映出一种认知，认为系统更看重一致性而非能力。」 此外，78% 的受访者感觉目前的招聘实践迫使他们夸大自己的资历，65% 的人已经调整过简历，以便更好地符合 AI 筛选的标准。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;齐尔强调，Dice 报告表明 AI 并不是敌人，而是需要更具思考性的整合。科技专业人士并非拒绝创新，而是希望招聘过程更加公平、有人性和透明。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;他表示：「解决方案不是放弃 AI，而是更负责任地应用它。采用 AI 来支持而不是取代人类决策的混合招聘模型，其信任度是完全自动化方法的三倍。」&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368529</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368529</guid>
      <pubDate>Tue, 19 Aug 2025 05:46:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>通义万相预告新模型 Wan2.2-S2V</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;通义万相团队深夜&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2FAlibaba_Wan%2Fstatus%2F1960012297059057935" target="_blank"&gt;发布预告推文&lt;/a&gt;，称即将推出新模型 Wan2.2-S2V，该模型将具备生成带音频视频的能力。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1774" src="https://static.oschina.net/uploads/space/2025/0826/115937_9VKc_2720166.png" width="2356" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;通义万相 Wan2.2 是一款开源视频生成模型，它率先在视频生成扩散模型中引入 MoE 架构，有效解决视频生成处理 Token 过长导致的计算资源消耗大问题。Wan2.2 还首创了「电影美学控制系统」，光影、色彩、构图、微表情等能力媲美专业电影水平。例如，用户输入「黄昏」、「柔光」、「边缘光」、「暖色调」「中心构图」等关键词，模型可自动生成金色的落日余晖的浪漫画面；使用「冷色调」、「硬光」、「平衡图」、「低角度」的组合，则可以生成接近科幻片的画面效果。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368515</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368515</guid>
      <pubDate>Tue, 19 Aug 2025 04:04:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>阿里开源 Vivid-VR：AI 视频修复神器</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;阿里云推出了一款名为 Vivid-VR 的开源生成式视频修复工具，基于先进的文本到视频（T2V）基础模型，结合 ControlNet 技术，确保视频生成过程中的内容一致性。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="211" src="https://static.oschina.net/uploads/space/2025/0826/112840_Ip58_4252687.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;该工具能够有效修复真实视频或 AIGC(AI 生成内容) 视频中的质量问题，消除闪烁、抖动等常见缺陷，为内容创作者提供了一个高效的素材补救方案。无论是对低质量视频的修复，还是对生成视频的优化，Vivid-VR 都展现出了卓越的性能。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;Vivid-VR 的核心技术在于其结合了 T2V 基础模型与 ControlNet 的创新架构。T2V 模型通过深度学习生成高质量视频内容，而 ControlNet 则通过精准的控制机制，确保修复后的视频在帧间保持高度的时间一致性，避免了常见的闪烁或抖动问题。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;据悉，该工具在生成过程中能够动态调整语义特征，显著提升视频的纹理真实感和视觉生动性。这种技术组合不仅提高了修复效率，还为视频内容保持了更高的视觉稳定性。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;Vivid-VR 的另一大亮点是其广泛的适用性。无论是传统拍摄的真实视频，还是基于 AI 生成的内容，Vivid-VR 都能提供高效的修复支持。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;对于内容创作者而言，低质量素材常常是创作过程中的痛点，而 Vivid-VR 能够通过智能分析和增强，快速修复模糊、噪点或不连贯的视频片段，为短视频、影视后期制作等领域提供了实用工具。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;此外，该工具支持多种输入格式，开发者可以根据需求灵活调整修复参数，进一步提升创作效率。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368510</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368510</guid>
      <pubDate>Tue, 19 Aug 2025 03:31:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>微软开源文本转语音模型 VibeVoice，支持最多 4 位说话人同时发声</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;微软正式开源了其最新的文本转语音（TTS）模型&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmicrosoft.github.io%2FVibeVoice%2F"&gt;VibeVoice-1.5B&lt;/a&gt;，该模型主打 「超长、多人、高压缩」，单次即可生成长达 90 分钟的连续语音，并支持最多 4 位说话人同时发声。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-b4cb0ed81f165defca082e67898ced868fb.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;VibeVoice-1.5B 的核心创新在于其双 Tokenizer 设计。模型分为两个独立但协同工作的模块。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-0a84db4a09d68c549f237fc0f439ca53cec.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;1. 声学 Tokenizer：负责保留声音特征并实现高压缩率&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;采用变分自编码器（VAE）的对称编码 - 解码结构，解决了传统 VAE 在长序列建模中容易出现的 「方差坍缩」 问题（即数据多样性丢失）。&lt;/p&gt; 
&lt;p&gt;通过 7 阶段的改进型 Transformer 模块和 1D 深度可分离因果卷积，将 24kHz 采样率的原始音频压缩为每秒仅 7.5 个潜在向量，累计压缩率达 3200 倍，压缩效率是主流 Encodec 模型的 80 倍。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2. 语义 Tokenizer：专注于提取与文本对齐的语义特征。&amp;nbsp;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;架构与声学 Tokenizer 的编码器部分一致，但移除了变分自编码器组件，以确保语义特征的确定性。&lt;/p&gt; 
&lt;p&gt;训练过程中，语义 Tokenizer 通过 「自动语音识别」 任务强制绑定语音与文本，最终舍弃解码器以提升推理速度 40%。&lt;/p&gt; 
&lt;p&gt;这种分工协作的模式，既保留了语音的细节（如音色、节奏），又确保了内容与文本的语义一致性，避免了传统模型中常见的 「音色与情绪不匹配」 问题。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;开源地址&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;https://github.com/microsoft/VibeVoice&lt;/em&gt;&lt;br&gt; https://huggingface.co/microsoft/VibeVoice-1.5B&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368509</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368509</guid>
      <pubDate>Tue, 19 Aug 2025 03:31:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>播放器视频后处理实践</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;span id="OSC_h1_1"&gt;&lt;/span&gt; 
&lt;h1&gt;&lt;strong&gt;&lt;strong&gt;1. 前言&lt;/strong&gt;&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;在播放器架构不断演进的今天，视频后处理技术正在成为提升用户体验的关键环节。相比传统的解码即播，现代播放器越来越多地引入后处理链路，通过增强画质、渲染氛围等手段，为用户提供更具沉浸感的视听体验。&lt;/p&gt; 
&lt;p&gt;本系列文章将系统介绍我们在播放器视频后处理模块中的技术方案与工程实现，涵盖从效果设计、算法选型，到性能优化和跨平台兼容的全链路细节。第一期内容聚焦在两类核心能力：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;视频增强：提升画面清晰度、对比度与色彩表现，尤其针对暗场、低码率等场景进行针对性优化；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;氛围模式：基于视频内容实时生成边缘延展光效，打造更强沉浸感，适配大屏与移动端场景。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;本文将着重介绍我们如何在性能受限的设备上实现视频增强效果，如何结合 GPU/OpenGL、Shader 编程以及平台图像处理 API 构建高效可控的处理链路。后续我们将陆续推出如氛围模式等视频后处理文章，敬请期待。&lt;/p&gt; 
&lt;span id="OSC_h1_2"&gt;&lt;/span&gt; 
&lt;h1&gt;&lt;strong&gt;&lt;strong&gt;2. 视频增强（亮度和色彩）&lt;/strong&gt;&lt;/strong&gt;&lt;/h1&gt; 
&lt;span id="OSC_h2_3"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;丨 2.1 什么是视频增强技术&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;视频增强技术是指一系列用于改善视频质量的技术手段，其目的是在不改变原始内容的情况下提升视频的视觉效果。技术的应用场景包括视频播放、编辑、传输、存储等领域，常用于提高图像清晰度、对比度、色彩饱和度等，使观看者获得更好的视觉体验。&lt;/p&gt; 
&lt;span id="OSC_h2_4"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;丨 2.2 常见视频增强技术&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-9e08c3b613e8e25991a9a403c48dbb6b777.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-dae5e8769705e37ee7fe3451db365b10a0c.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;移动端实践：亮度与色彩增强。针对 Android/iOS 平台的视频播放场景，我们重点实现了亮度增强与色彩增强两项关键技术。本文将分享技术落地中的核心方案与优化经验。&lt;/p&gt; 
&lt;span id="OSC_h2_5"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;丨 2.3 亮度增强&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-111882e6b535a736ac5c3ae7f921187c316.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;亮度增强效果示意图（左：原图，右：增强后）&lt;/p&gt; 
&lt;span id="OSC_h3_6"&gt;&lt;/span&gt; 
&lt;h3&gt;2.3.1 技术选型&lt;/h3&gt; 
&lt;p&gt;亮度增强是图像/视频处理中非常基础且常见的操作，常见的亮度增强原理可以分为以下几类，每种方式背后的核心思想略有不同。下面是详细的分类和解释：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;线性亮度增强（线性增益）&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;原理：RGB 整体直接乘以一个大于 1 的系数（或加一个偏移量）。&lt;/p&gt; 
&lt;p&gt;公式：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;color.rgb&amp;nbsp;= color.rgb * gain; &amp;nbsp; &amp;nbsp; &amp;nbsp; // 乘法增强 color.rgb = color.rgb + offset; &amp;nbsp; &amp;nbsp; // 加法增强

&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;简而言之，这种做法就是简单粗暴的在原本的 RGB 上进行提升，从这里，可以想到 RGB 颜色调整后容易出现色偏。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;那么我们可能会想到，如果先将 RGB 转换为 YUV，调节 Y 分量，再反变换为 RGB。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;公式：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Y&amp;nbsp;=&amp;nbsp;0.299*R +&amp;nbsp;0.587*G +&amp;nbsp;0.114*B;
Y_new&amp;nbsp;= Y * gain;

&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;这确实是视频增强中一种常用且理论上「更稳」的方式，因为它分离了亮度（Y）和色彩（UV / IQ / CbCr）信息。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;但这种处理方式有一个严重的问题，不处理图像的对比度或中间的关系，且不能保留高光细节（Clipping），也就是调整后，超过范围[0.0,1.0]的值会被截断（clamp），造成高光过曝。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;直方图均衡（Histogram Equalization）&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;原理：通过调整像素分布，让亮度值均匀分布在整个区间，从而整体提升视觉亮度。&lt;/p&gt; 
&lt;p&gt;特点：增强暗部和亮部的对比，对低对比度图像尤其有效。&lt;/p&gt; 
&lt;p&gt;实现相对复杂，不常用于实时 shader，考虑到其运算复杂性，我们也 pass 了这种方式。&lt;/p&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Gamma 变换（幂律调整）&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;原理：使用幂函数对像素进行非线性拉伸。&lt;/p&gt; 
&lt;p&gt;公式：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;color.rgb&amp;nbsp;= pow(color.rgb, vec3(gamma));

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;特点：γ &amp;lt; 1：图像变亮，主要拉升暗部；γ &amp;gt; 1：图像变暗，压缩亮部。&lt;/p&gt; 
&lt;p&gt;具有两个优点：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;调整方式具有非线性特点，能更细腻地控制中间调亮度，避免简单加法可能引起的局部过曝或暗部细节丢失。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;模拟现实中显示设备的响应曲线，效果较为自然。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;这也是我们最后选择的方式，他的运算量简单，适合端上视频播放的实时处理。&lt;/p&gt; 
&lt;span id="OSC_h3_7"&gt;&lt;/span&gt; 
&lt;h3&gt;2.3.2 背后的原理&lt;/h3&gt; 
&lt;p&gt;我们引申一下，这种方式的优点是怎么得出来的呢。&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;为何能避免简单加法可能引起的局部过曝或暗部细节丢失&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;从公式看，原本亮度较低的像素会被相对「提亮」更多，而原本亮度较高的像素提升幅度较小。暗部像素相对于原值会获得更大的「提拉」，而亮部像素则变化较小，从而既能提升整体曝光，又能保留高光细节。&lt;/p&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;为什么说模拟现实中显示设备的响应曲线，更为自然呢？&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;因为显示器、人眼视觉和视频编码，都是非线性系统，不是简单线性变化。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;真实世界的光亮度是线性的，比如两支灯加起来就是两倍亮。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;但人眼感知亮度是对数感知的（小亮度变化很敏感，大亮度变化不敏感）。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;视频和图像在存储时通常经过一个 Gamma 编码，原本线性光 → 压缩（比如取 1/2.2 次方） → 存成文件。这种光和电的转换过程，就是 OETF/EOTF 响应曲线。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;所以这种 pow(color, gamma) 的调整方式，实际就是在模拟显示端的响应曲线。&lt;/p&gt; 
&lt;p&gt;总结一句话：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;编码有 Gamma，所以显示端或后处理也必须按照 Gamma 空间规则来调节，才能保持自然感知。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;span id="OSC_h2_8"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;丨 2.4 色彩增强&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-329f6cfe91ee30986ddb96559406ffacab5.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;色彩增强效果示意图（左：原图，右：增强后）&lt;/p&gt; 
&lt;p&gt;从上图可以看到山体、草地上的花，饱和度增强。&lt;/p&gt; 
&lt;span id="OSC_h3_9"&gt;&lt;/span&gt; 
&lt;h3&gt;2.4.1 调节的目标&lt;/h3&gt; 
&lt;p&gt;1. 增强色彩感知&lt;/p&gt; 
&lt;p&gt;提高图像的「鲜艳度」或「视觉吸引力」，让图像更生动。&lt;/p&gt; 
&lt;p&gt;特别是在图像颜色偏灰、曝光不佳或图像压缩后颜色损失的情况下。&lt;/p&gt; 
&lt;p&gt;2. 突出主体&lt;/p&gt; 
&lt;p&gt;通过饱和度调节，增强主体与背景之间的色彩对比，提高视觉聚焦度。&lt;/p&gt; 
&lt;p&gt;3. 修复/还原真实色彩&lt;/p&gt; 
&lt;p&gt;对摄像头采集后色彩不足的图像进行还原，尤其是肤色、植物、天空等自然色彩。&lt;/p&gt; 
&lt;p&gt;针对上述目标，我们主要依赖主观评测感受，同时需要避免以下问题：&lt;/p&gt; 
&lt;p&gt;主观评估（人眼视觉）&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;色彩鲜明但不刺眼：增强后色彩更加明显但不过饱和。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;肤色自然：人脸或皮肤色调不过红或黄（肤色是视觉最敏感区域）。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;色彩分布均衡：图像中颜色种类丰富但不过分集中某一色调。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;无色彩断层：调节后颜色过渡应平滑，不能有色阶突变。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="OSC_h3_10"&gt;&lt;/span&gt; 
&lt;h3&gt;2.4.2 技术选型&lt;/h3&gt; 
&lt;p&gt;目前业界对色彩增强主要有以下 2 种方向的研究：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;传统 SDR 色彩增强。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;SDR2HDR，模拟 HDR 效果，达到增强目的。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;从实现方式上，主要也有 2 种主流方式：&lt;/p&gt; 
&lt;p&gt;1. 非神经网络（传统算法 or 结合 lut 查找表）&lt;/p&gt; 
&lt;p&gt;2. 基于神经网络（模型）&lt;/p&gt; 
&lt;p&gt;模型需要较高的技术储备，且在移动端运行耗时大，所以目前我们没有选择这种方式，而是寻找效果较好且可控的算法。&lt;/p&gt; 
&lt;span id="OSC_h4_11"&gt;&lt;/span&gt; 
&lt;h4&gt;2.4.2.1 色彩三要素&lt;/h4&gt; 
&lt;p&gt;我们先了解下「色彩三要素」。他们是色彩学中用于描述颜色感知的三个基本维度，分别是：色相、饱和度、明度。这三者共同定义了一个颜色的完整视觉特性。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-2b30a9ad27103861d53891b58e1a3e94cdb.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-732e1f0d9f8fb805edc7e32085ce32787f6.jpg" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;色相&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-84e06db649fa950d64da12884126416abef.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;饱和度&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-627291ade068565461ff9ac2bbf87a6ece7.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;明度&lt;/p&gt; 
&lt;p&gt;在色彩增强中，一般主要调节的是饱和度（Saturation），其次可能会适当调整明度（Brightness / Value） ，而色相（Hue）通常不会主动改变。原因如下：&lt;/p&gt; 
&lt;p&gt;常调节的要素及原因：&lt;/p&gt; 
&lt;p&gt;1. 饱和度（Saturation）&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;最常调节的要素，增强后画面显得更鲜艳、更有吸引力，尤其适用于风景、商品、动漫类画面。可提升视觉冲击力和色彩表现力。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;明度 / 亮度（Brightness / Value）&lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt;有时作为辅助增强项，提高整体图像的通透感。与 Gamma 调节、曝光补偿常一起使用，即配合使用上一章节的亮度调整即可。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;色相（Hue）&lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt;一般不调整，因为改变色相会改变物体本身颜色，可能导致不真实（如人脸偏色、草地变蓝等）。只在需要艺术化或特殊滤镜（如复古风格、红外效果）时才会使用。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="OSC_h4_12"&gt;&lt;/span&gt; 
&lt;h4&gt;2.4.2.2 颜色空间的选择&lt;/h4&gt; 
&lt;p&gt;选择好色彩增强的调节方向为『饱和度』后，第二步，我们需要选择好颜色空间。&lt;/p&gt; 
&lt;p&gt;当视频一帧画面作为 GL 纹理输入到后处理链路时，为 RGB 颜色模型，我们想要调节饱和度，则需要将其转换为其他颜色空间进行调节，那么面临的第一个问题是如何选择合适的颜色模型去进行算法设计？&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;RGB&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;HSV&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;LCH/LAB&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="OSC_h4_13"&gt;&lt;/span&gt; 
&lt;h4&gt;2.4.2.3 基于 RGB 空间&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;基于 RGB 颜色直接调节&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;们可以理解，饱和度是色彩的纯度，即色彩相对于灰度（无色）的程度。那么我们可以基于 RGB 颜色模型，并根据灰度进行差值混合即可。&lt;/p&gt; 
&lt;p&gt;如 GPUImage 的 GPUImageSaturationFilter 提供了类似例子，它对饱和度调节，是基于 RGB 颜色，然后取出灰度值通过在原始颜色和灰度之间插值，mix(vec3(luma), color.rgb, saturation) 实现了饱和度的变化：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;插值因子 saturation 越接近 0，图像越趋向于灰度；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;saturation 越高，图像越接近原始颜色或超出原始饱和度，色彩更鲜艳。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;这种简单的算法存在一个问题：原本局部饱和度已经比较高，如果依然提高饱和度，则局部细节消失。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-27d7b0994fc4fac57b89d002c584918656a.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;过饱和，细节丢失&lt;/p&gt; 
&lt;p&gt;2. 为了解决上述问题，我们基于自然饱和度的调整。&lt;/p&gt; 
&lt;p&gt;自然饱和度（Vibrance）的概念最先由 photoshop 提出，重点在于适应性，自然饱和度调整后一般比饱和度调整要自然。其核心特点：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-d0784de60346b9dcbeef45c7d5a1ca98fcc.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;进行自适应饱和度调节的流程：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;计算亮度（Luma）：使用加权平均公式从 RGB 获取亮度：luma = 0.2126 * r + 0.7152 * g + 0.0722 * b&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;计算饱和度（Saturation）：使用 RGB 最大值和最小值之差估算色彩纯度：saturation = max(r, g, b) - min(r, g, b)&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-1a7eddfe1e8eed43260770fef02643989f3.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;计算调节因子 k：根据当前饱和度和用户设置的 Vibrance 强度进行非线性调节：k = 1.0 + Vibrance * (1.0 - saturation / 255.0)（Vibrance 取值范围通常为 0.0 ~ 1.0）&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;应用颜色调整：将颜色向亮度方向插值，使低饱和度颜色更鲜艳，同时高饱和区域变化较小：color.rgb = mix(vec3(luma), color.rgb, k)&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;其调整倾向于将 RGB 值往同一个 luma 值进行靠近，也是无法保证颜色保持稳定，容易会发生偏色的情况。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-6e84cd82793fb316a2c29384bc13928c8cc.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;色彩增强效果示意图（左：原图，右：自然饱和度增强后）&lt;/p&gt; 
&lt;p&gt;于是，我们继续探索其他的颜色模型。&lt;/p&gt; 
&lt;span id="OSC_h4_14"&gt;&lt;/span&gt; 
&lt;h4&gt;2.4.2.4 基于 HSV 颜色模型的饱和度调整&lt;/h4&gt; 
&lt;p&gt;基于 HSV 饱和度的调整方法是将 RGB 颜色模型转换为 HSV 颜色模型，其中 HSV 分别表示色相（Hue）、饱和度（Saturation）、明度（Value）。只调整饱和度可以在不影响明暗和色相的情况下增强色彩的鲜艳程度。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-8f18a721fd76edb4555d4d7841ea77d99e9.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;将常见的调整方法有整体抬升，按比例增加，或者曲线调整，达到将整体饱和度提高的目的。但是饱和度调整同时提升所有颜色的强度，比较粗暴。&lt;/p&gt; 
&lt;p&gt;有可能导致：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;本来局部饱和度已经比较高，调节后过饱和，局部细节的消失。(和上一章节例子一样)。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;本来局部饱和度较低，接近白色，加大饱和度后，容易出现色块。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-57e3e6fbadb52f1a1f7d3ffa28d34c30508.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;普通调节&lt;/p&gt; 
&lt;p&gt;如何优化：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;对此引入对源的饱和度的检测，设定上下限制，平滑调节。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;在 HSV 颜色模型上，引入了类似自适应饱和度调整的方式。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;目的：在低饱和度区域，避免突然增加饱和度。低饱和度的颜色（例如接近灰色的颜色）通常对饱和度调整非常敏感，因此需要一种平滑的方式。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;目的：在高饱和度区域减少权重，避免过度增强饱和度。高饱和度的区域本身已经很饱和，进一步增加饱和度会导致过饱和，视觉上显得不自然。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-aba8ad7b559850b5ef9d9043a47954299ca.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;加入自适应后&lt;/p&gt; 
&lt;span id="OSC_h4_15"&gt;&lt;/span&gt; 
&lt;h4&gt;2.4.2.5 肤色保护&lt;/h4&gt; 
&lt;p&gt;采用 HSV 空间调整后，我们还需要考虑一个核心问题：&lt;/p&gt; 
&lt;p&gt;在图像色彩增强（如饱和度调整、色调映射）时，肤色区域容易因过度调整而失真（如过红、过黄或惨白）。需通过肤色识别技术，对检测到的肤色区域进行保护，限制增强幅度，保持自然观感。&lt;/p&gt; 
&lt;p&gt;在此引入了基于 HSV 色彩模型的肤色识别，HSV 色彩模型也同样将亮度与颜色进行了分离，因此对于光照变化也有很强的抗干扰能力，可以较好的识别出肤色。&lt;/p&gt; 
&lt;p&gt;结合 HSV 色彩模型和高斯概率模型实现肤色保护，具体步骤如下：参考 GPUImageSkinToneFilter 的肤色识别方法。&lt;/p&gt; 
&lt;p&gt;(1) RGB 转 HSV 空间&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;将图像从 RGB 转换到 HSV 空间，分离色调（H）、饱和度（S）、亮度（V）。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;优势：HSV 的色调通道（H）对光照变化鲁棒，更适合肤色识别。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;(2) 肤色概率计算&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;肤色色调模型：&lt;/p&gt; &lt;p&gt;统计肤色色调的均值 skinHue = 0.05（典型值，对应黄红色调）。&lt;/p&gt; &lt;p&gt;方差相关参数 skinHueThreshold = 40（控制肤色范围宽度）。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;距离计算：&lt;/p&gt; &lt;p&gt;计算当前像素色调 h 与 skinHue 的归一化距离。&lt;/p&gt; &lt;p&gt;dist = abs(h - skinHue) / 0.5 高斯权重（概率）。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;通过高斯函数计算肤色概率：&lt;/p&gt; &lt;p&gt;skinProb = exp(-dist * dist * skinHueThreshold) 结果范围 [0, 1]，越接近 1 表示越可能是肤色。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;(3) 肤色区域保护&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;阈值分割：&lt;/p&gt; &lt;p&gt;设定阈值（如 skinProb &amp;gt; 0.95），二值化得到肤色掩膜（Mask）。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;动态衰减增强强度：&lt;/p&gt; &lt;p&gt;对检测到的肤色区域，按 skinProb 权重衰减色彩增强效果。例如：enhanced_pixel = original_pixel * (1 - skinProb) + adjusted_pixel * skinProb * alphaalpha 为衰减系数（如 0.2），控制保护力度。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;增加肤色保护后，可以看到效果明显更好，人脸不会有过于突兀的颜色变化。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-6986499c034b5ea751eaffd6e0ff1778e9d.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;左：增强（无保护）中：原图 &amp;nbsp;右：增强（肤色保护）&lt;/p&gt; 
&lt;span id="OSC_h3_16"&gt;&lt;/span&gt; 
&lt;h3&gt;2.4.3 效果对比&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;HSV 空间的调节后色彩更加自然。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;RGB 空间调节则更加绚丽。但容易色偏。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;基于综合考虑，我们采用 HSV 空间调节，以适应更多的源，避免色偏。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="OSC_h1_17"&gt;&lt;/span&gt; 
&lt;h1&gt;&lt;strong&gt;&lt;strong&gt;三. 总结与展望&lt;/strong&gt;&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;本研究聚焦于移动端视频增强技术的工程化落地，重点验证了亮度增强与色彩增强两种核心算法的实际应用效果。从主观评测效果看，在部分视频上，两项技术均能显著提升视频观感质量，有效改善用户体验。&lt;/p&gt; 
&lt;p&gt;目前，亮度增强功能已在「好看 App」成功上线，且收获了良好的应用效果。现阶段，我们正着力研发亮度增强与色彩增强相叠加的综合优化方案，计划通过这一方案对更多视频内容进行品质升级，从而为用户带来更优质的观看体验。以下为您呈现亮度增强结合色彩增强的部分应用案例：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-7b37220cf68793bbd80ae5e7ad807cc0e2f.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;例子 1：后层次感更好（右）&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-e3ba30fd9431f8cbf07932f3a0bb380f123.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;例子 2：色彩更鲜明（右）&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-c4cc30ba8a7c9cbe11075a6e6bac0dea018.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;例子 3：画面更清晰明亮（右）&lt;/p&gt; 
&lt;p&gt;未来研究将围绕以下方向展开：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;场景化优化：建立典型场景特征库，针对性优化算法参数配置。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;实时性提升：通过模型轻量化与硬件加速技术，更加快速的视频实时处理。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/4939618/blog/18689451</link>
      <guid isPermaLink="false">https://my.oschina.net/u/4939618/blog/18689451</guid>
      <pubDate>Tue, 19 Aug 2025 03:20:00 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>英伟达发布 Jetson AGX Thor 开发者套件： 基于 Blackwell 架构、专为物理 AI 和机器人打造</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;英伟达宣布&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.nvidia.cn%2Fautonomous-machines%2Fembedded-systems%2Fjetson-thor%2F" target="_blank"&gt;NVIDIA Jetson AGX Thor™&lt;/a&gt;&amp;nbsp;开发者套件和量产级模组现已发售。这是一款功能强大的新一代机器人计算机，旨在为制造、物流、交通、医疗、农业和零售等行业的数百万台机器人提供算力支持。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-2947794e4a78f4a5e0dfa4396d7c54809a3.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Jetson Thor 基于 NVIDIA Jetson™ 软件平台，专为物理 AI 和人形机器人打造，支持所有主流 AI 框架与生成式 AI 模型。同时，它完全兼容 NVIDIA 从云到边缘的软件栈，包括用于机器人仿真与开发的 NVIDIA Isaac、人形机器人基础模型 Isaac GR00T、用于视觉 AI 的 NVIDIA Metropolis 以及用于实时传感器处理的 NVIDIA Holoscan。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-35c925129f4a1c612c53f4f41decaa3e78b.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Jetson AGX Thor 核心信息：&amp;nbsp;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;套件搭载 &lt;strong&gt;Blackwell 架构 GPU&lt;/strong&gt;，峰值算力可达 &lt;strong&gt;2070 TFLOPS（FP4 稀疏运算）&lt;/strong&gt;，相较前代 Orin 提升 &lt;strong&gt;7.5 倍算力&lt;/strong&gt;、&lt;strong&gt;3.5 倍能效&lt;/strong&gt;。内置 &lt;strong&gt;128GB LPDDR5X 内存&lt;/strong&gt;，带宽高达 &lt;strong&gt;273GB/s&lt;/strong&gt;。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;配备 14 核 Arm Neoverse-V3AE CPU、2560 CUDA 核心及 96 个 Tensor 核心，并支持 MIG 技术，可将 GPU 切分成多个隔离实例。功耗范围 &lt;strong&gt;40–130W&lt;/strong&gt;，兼顾性能与能效。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;包含 Jetson T5000 模组、参考载板、主动散热系统、电源适配器及丰富 I/O（如 QSFP28 100GbE、HDMI/DP、USB-C、1TB NVMe SSD、Wi-Fi 6E 等）。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;开发者套件售价 &lt;strong&gt;3499 美元&lt;/strong&gt;；量产模块 &lt;strong&gt;T5000&lt;/strong&gt; 在千片级订单中单价 &lt;strong&gt;2999 美元&lt;/strong&gt;。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;详情查看&amp;nbsp;&lt;em&gt;https://blogs.nvidia.cn/blog/nvidia-blackwell-powered-jetson-thor-now-available-accelerating-the-age-of-general-robotics/&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368504</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368504</guid>
      <pubDate>Tue, 19 Aug 2025 03:18:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>亚马逊 AGI 实验室掌门人首度回应</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;去年，当亚马逊以一种前所未有的方式招揽 AI 初创公司 Adept 的创始团队时，整个行业都为之震动。这种被称为"反向人才收购"的全新交易模式，让大型科技公司无需完全收购初创企业，而是通过挖走核心团队并获得技术授权来达到目的。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;这场交易的核心人物 David Luan，从 Adept 联合创始人兼 CEO 摇身一变，成为了亚马逊全新 AGI 实验室的掌舵人。如今，面对外界的质疑声浪，Luan 终于打破沉默，在接受 The Verge 采访时为自己的选择进行了辩护。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="272" src="https://oscimg.oschina.net/oscnet/up-16d507b464b128d9ba6a42ba65af0fa1d69.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;当被问及这种"反向人才收购"趋势时，Luan 的回应颇具深意。他坦言希望自己未来能够"作为 AI 研究创新者而非交易结构创新者被人们铭记"。但从他的角度来看，像亚马逊这样的科技巨头在当下"集中人才和计算资源的关键质量"是完全合理的战略选择。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;更加引人深思的是 Luan 对于离开自己创立公司的解释。他毫不讳言地表示，自己并不愿意将 Adept 打造成"一家只销售小型模型的企业级公司"。在他眼中，还有"通向 AGI 的四个关键研究难题"亟待解决，而这正是他真正的使命所在。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;最令人震撼的是 Luan 对于资源需求的坦率描述。他直言不讳地指出，要解决这些核心问题，"每一个都需要价值数百亿美元的计算集群来运行"。面对如此天文数字般的资源需求，他反问道："除此之外，我还有什么其他机会能够实现这个目标呢?"&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368502</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368502</guid>
      <pubDate>Tue, 19 Aug 2025 03:09:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>谷歌 NotebookLM 升级：支持 80 种语言的视频与音频概述</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;谷歌&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.google%2Ftechnology%2Fgoogle-labs%2Fnotebook-lm-audio-video-overviews-more-languages-longer-content%2F"&gt;宣布&lt;/a&gt;其 AI 研究助手 NotebookLM 迎来重大更新，其 「Video Overviews（视频概述）」 功能现已支持 80 种语言（包括简体中文），并同步升级了 Audio Overviews（音频概述）。Video Overviews 最初于 7 月&lt;a href="https://www.oschina.net/news/362937" target="_blank"&gt;推出&lt;/a&gt;，此次更新后，全球用户可用本地语言生成笔记本内容的视频摘要。 &lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-11ee666c4dac7986a9fc9a46eb5a50015b2.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-ea7002e11e165dbd0fe526919e864976102.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;此次升级的核心亮点如下：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;视频概述扩展至 80 种语言：NotebookLM 的 AI 讲解视频功能现已面向全球用户，支持多达 80 种语言。用户可将笔记内容自动生成带有画面和解说的视频，更适合需要视觉化学习或分享的场景。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;音频概述全面升级：原本仅提供简短亮点的 「音频概述」 如今扩展为完整版本，覆盖与英文版同等质量的连贯讲解，并同步支持多语言。这意味着无论用户选择何种语言，都能获得深度的音频总结体验。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;内容更长，生成更灵活：Google 同时强化了 NotebookLM 的 Studio 面板。用户现在可在一个笔记本中生成多个不同版本的音频或视频概述，以满足不同受众与学习目标的需求。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;从最初的 「播客式音频总结」 到如今支持图表、网页、幻灯片甚至视频讲解，NotebookLM 正逐步演变为一个集多模态知识提炼、个性化讲解于一体的 AI 学习与研究助手。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368497</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368497</guid>
      <pubDate>Tue, 19 Aug 2025 03:03:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>快手可灵&amp;港大提出 Context-as-Memory，上下文记忆力媲美 Genie3 且问世更早</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;香港大学和快手可灵团队近日联合&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FlsLYxySrtNN7QY3g7dVoZA" target="_blank"&gt;发表&lt;/a&gt;论文《Context as Memory: Scene-Consistent Interactive Long Video Generation with Memory Retrieval》，提出一种创新性方法：将历史生成的上下文作为「记忆」（即 Context-as-Memory），通过 context learning 技术学习上下文条件，从而实现对长视频前后场景一致性的有效控制。研究发现：视频生成模型能够隐式学习视频数据中的 3D 先验，无需显式 3D 建模辅助，这一理念与 Genie 3 不谋而合。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;为了高效利用理论上可无限延长的历史帧序列，论文还提出了基于相机轨迹视场（FOV）的记忆检索机制（Memory Retrieval），从全部历史帧中筛选出与当前生成视频高度相关的帧作为记忆条件，大幅提升视频生成的计算效率并降低训练成本。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在数据构建上，研究团队基于 Unreal Engine 5 收集了多样化场景、带有精确相机轨迹标注的长视频，用于充分训练和测试上述技术。用户只需提供一张初始图像，即可沿设定的相机轨迹自由探索生成的虚拟世界。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;根据介绍，Context as Memory 可以在几十秒的时间尺度下保持原视频中的静态场景记忆力，并在不同场景有较好的泛化性。Context as Memory 方法旨在实现无需显式三维建模的场景一致的长视频生成。该方法的核心创新包括：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;提出了 Context as Memory 方法，强调将历史生成的上下文作为记忆，无需显式 3D 建模即可实现场景一致的长视频生成。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;设计了 Memory Retrieval 方法，采用基于视场（FOV）重叠的相机轨迹规则进行动态检索，显著减少了需要学习的上下文数量，从而提高了模型训练与推理效率。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;实验结果表明，Context as Memory 在长视频生成中的场景记忆力表现优越，显著超越了现有的 SOTA 方法，并且能够在未见过的开放域场景中保持记忆。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img height="193" src="https://oscimg.oschina.net/oscnet/up-a93eaecc14503bf0ea35d8a0966f2d4dbc6.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#3e3e3e; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;如上图（a）所示，Context-as-Memory 的长视频生成是通过基于 Context learning 的视频自回归生成来实现的，其中，所有历史生成的视频帧作为 context，它们被视为记忆力的载体。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#3e3e3e; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;如上图（b）所示，为了避免将所有历史帧纳入计算所带来的过高计算开销，提出了 Memory Retrieval 模块。该模块通过根据相机轨迹的视场（FOV）来判断预测帧与历史帧之间的重叠关系，从而动态筛选出与预测视频最相关的历史帧作为记忆条件。此方法显著减少了需要学习的上下文数量，大幅提高了模型训练和推理的效率。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#3e3e3e; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;在实验中，研究者将 Context-as-Memory 与最先进的方法进行了比较，结果表明，Context-as-Memory 在长视频生成的场景记忆力方面，相较于这些方法，表现出了显著的性能提升。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#3e3e3e; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;img height="192" src="https://oscimg.oschina.net/oscnet/up-1dcc527f72fd57d136741cd74d4c8831830.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368496</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368496</guid>
      <pubDate>Tue, 19 Aug 2025 02:55:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>微软分享有关开源 Windows 11 UI 的新细节</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;本月初， 微软公布了&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.neowin.net%2Fnews%2Fmicrosoft-is-taking-steps-to-open-sourcing-windows-11-user-interface-framework%2F" target="_blank"&gt;有关开源&lt;/a&gt;&amp;nbsp;Windows 11 用户界面框架 WinUI 的细节。开源 WinUI 一直是开发者们的长期呼声，但实现起来并非轻而易举。由于 WinUI 在操作系统的专有层面「根深蒂固」，开源该框架需要谨慎且深思熟虑的方法。在首次发布几周后， 微软准备分享更多关于 WinUI OSS 项目的信息。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-a574d1f8d2589c1f12d0d92889eb094019d.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;微软希望分四个阶段完成这项任务。第一阶段主要是在 GitHub 上提供更多内部贡献。第二阶段将允许开发人员克隆存储库并在本地构建 WinUI。第三阶段将允许第三方开发人员为该项目做出贡献，最后阶段将 GitHub 设为「开发、问题跟踪和社区参与的主要场所」，并逐步淘汰内部存储库。&lt;/p&gt; 
&lt;p&gt;目前， &lt;strong&gt;微软开发人员正忙于将 WinUI 从 Windows 中无法公开共享的专有部分中「解开」&lt;/strong&gt;。一旦 Windows App SDK 1.8 于本月晚些时候发布（目前处于预览阶段，WinUI 与 WASDK 绑定），开发人员将开始在 GitHub 上实施拉取请求。 微软计划在 2025 年 10 月初完成第一阶段。&lt;/p&gt; 
&lt;p&gt;至于允许第三方开发者克隆代码库并在本地构建，&amp;nbsp;微软表示目前正在「积极探索」这个想法。这需要采取更加谨慎的态度，因此预计该公司还需要一些时间才能公布更多细节。以下是该公司的声明：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;由于 WinUI 的发布计划与 Windows App SDK 紧密相关，我们的分支策略现在允许我们在即将发布的 WASDK 1.8 版本的同时开始完成 PR。1.8 预览版已于 8 月 19 日发布，稳定版也即将发布，这为我们开始集成变更奠定了良好的基础。基于此，我们计划在 10 月初完成第一阶段的工作。&lt;/p&gt; 
 &lt;p&gt;第二阶段仍在积极探索中，虽然我们对此更加谨慎，但我们希望很快分享切实的进展。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fmicrosoft-ui-xaml%2Fdiscussions%2F10700" target="_blank"&gt;您可以在 GitHub 上&lt;/a&gt;关注有关 WinUI 开源的讨论。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368494</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368494</guid>
      <pubDate>Tue, 19 Aug 2025 02:53:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>华为鸿蒙 HarmonyOS 5 终端设备数突破 1200 万</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;鸿蒙生态发展再获新进展。8 月 25 日，在智界及问界秋季新品发布会上，华为技术有限公司（以下简称「华为」）常务董事、终端 BG 董事长余承东宣布搭载 HarmonyOS 5（以下简称「鸿蒙 5」）的终端设备数量突破 1200 万台。&lt;/p&gt; 
&lt;p&gt;而这距离今年 7 月 30 日，余承东透露鸿蒙 5 终端数量突破千万台，仅不足一个月，再次创造了「鸿蒙速度」。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0826/104411_Nc6e_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;自鸿蒙 5 系统发布以来，其终端设备数量呈现指数级增长。从 7 月 30 日突破 1000 万台到 8 月 25 日已达 1200 万台，鸿蒙 5 终端设备数量用了不到一个月时间新增 200 万台，增速达 20%。&lt;/p&gt; 
&lt;p&gt;余承东表示，鸿蒙生态的高速发展，是每一位开发者、伙伴和消费者的共同托举，让鸿蒙生态走向正循环。 &amp;nbsp;同时，鸿蒙生态向全场景渗透，覆盖手机、平板、电脑、智能穿戴、智能家居及汽车等设备，其中华为 Mate 70 系列、Mate X6 系列等 50 余款主力机型开启 HarmonyOS 5.1 升级，进一步扩大用户覆盖面。 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;余承东此前在接受采访时透露，鸿蒙 5.0 设备已经超过 1000 万，度过了鸿蒙生态的一个生死线。在生态这个领域，感谢中国整个科技产业的集体冲锋，把不可能变成了可能。华为存量几个亿用户的手机也会陆陆续续开放升级到鸿蒙 5.0，升级之后会更流畅，体验更丝滑。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368493</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368493</guid>
      <pubDate>Tue, 19 Aug 2025 02:44:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>字节跳动内测全新 3D 模型生成工具 「3D Model Generator」</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;《&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FU5zMvqko4Ckz_YKa4WwQig" target="_blank"&gt;读佳&lt;/a&gt;》消息称，&lt;span style="background-color:#ffffff; color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;字节&lt;/span&gt;&lt;/span&gt;&lt;span style="background-color:#ffffff; color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;的&lt;/span&gt;&lt;/span&gt;&lt;span style="background-color:#ffffff; color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;豆包内部正在研发测试名为「3D Model Generator」的 3D 模型生成工具。该工具致力于可控大规模生成模型，为创建高质量 3D 资产提供有力支持，&lt;strong&gt;尤其在游戏中的 3D 建模领域&lt;/strong&gt;。该工具或不久后对外开放使用。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="254" src="https://oscimg.oschina.net/oscnet/up-339b979e24b142fa3e8ef2b96f2731b7fef.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="background-color:#ffffff; color:rgba(0, 0, 0, 0.9)"&gt;从测试页面可以看出，「3D Model Generator」支持两种生成方式，一种是基于图像生成，选取本地图像文件，点击「生成」，即可快速生成 3D 模型，降低了 3D 建模的入门门槛。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="228" src="https://oscimg.oschina.net/oscnet/up-8483f178e9e5f6f5cff323eb4fd921324db.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="background-color:#ffffff; color:rgba(0, 0, 0, 0.9)"&gt;另外一种是基于图像+模型生成，通过图像文件与模型文件的结合，实现更复杂或更具针对性的 3D 资产创作，这对于游戏建模等需要大量 3D 素材的场景来说，具备实际应用价值。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="197" src="https://oscimg.oschina.net/oscnet/up-bc28c3c09e53aab094043b85aec04f160bd.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368492</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368492</guid>
      <pubDate>Tue, 19 Aug 2025 02:42:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>华为将发布自研 AI SSD</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;华为将于 8 月 27 日发布新品 AI SSD，目标直指 AI 存储器市场。传统 HBM 存在容量限制，而华为或将通过技术创新提供大容量 SSD。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="295" src="https://oscimg.oschina.net/oscnet/up-a05e595e422680f655edc3e44fc1a9c29bd.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;另据中国基金报记者报道，在当前的 AI 存储器领域，HBM（高带宽内存）占据重要地位。HBM 是一种通过 3D 堆叠和超宽接口，实现极高数据传输带宽的先进内存技术，通常直接封装在 GPU（图形处理器）卡中。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;问题在于，相比于之前的内存技术，HBM 牺牲容量换取极致带宽和能效的策略，导致现有算力卡上 HBM 的容量比较有限。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;华为将发布的全新 AI SSD 产品，可以有效满足 AI 训练推理过程中的超大容量和超强性能的需求，助力 AI 训练推理、大模型部署。此举将进一步强化华为存储器在 AI 时代的竞争力，推动国产存储生态发展。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;据悉，华为计划与一体机厂商合作，改变现有局面，为 AI 存储器市场注入新活力，带来更多的可能性。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/368488</link>
      <guid isPermaLink="false">https://www.oschina.net/news/368488</guid>
      <pubDate>Tue, 19 Aug 2025 02:15:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
  </channel>
</rss>
