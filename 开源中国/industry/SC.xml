<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - industry - 简体中文</title>
    <link>https://www.oschina.net/news/industry</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news/industry" rel="self" type="application/rss+xml"/>
    <description>已对该 RSS 进行格式化操作：中英字符之间插入空格、使用直角引号、标点符号修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-cn</language>
    <lastBuildDate>Thu, 12 Jun 2025 07:42:13 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>vivo Pulsar 万亿级消息处理实践（1）- 数据发送原理解析和性能调优</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                                        &lt;blockquote&gt; 
 &lt;p&gt;作者：vivo 互联网大数据团队- Quan Limin&lt;/p&gt; 
 &lt;p&gt;本文是 vivo 互联网大数据团队《vivo Pulsar 万亿级消息处理实践》系列文章第 1 篇。&lt;/p&gt; 
 &lt;p&gt;文章以 Pulsar client 模块中的 Producer 为解析对象，通过对 Producer 数据发送原理进行逐层分析，以及分享参数调优实战案例，帮助读者理解与使用好 Producer，并体会到 Producer 对消息中间件系统稳定性以及处理性能所起到的关键作用。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;一、Pulsar 简要介绍&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://static001.geekbang.org/infoq/fb/fbcab3bc275f1a6943f142e8a110d07c.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Pulsar 是新一代的云原生消息中间件，由 Apache 软件基金会孵化和开源。它的设计目的是为了满足现代数据处理和计算应用程序对可扩展性、可靠性和高性能的需求，具备存储与计算分离、节点对等、独立扩展、实时均衡、节点故障快速恢复等特性。&lt;/p&gt; 
&lt;p&gt;Pulsar 由四个核心模块组成：broker、bookKeeper 和 client（Producer 和 Consumer）、zk（元数据管理和节点协调）。broker 接受来自 Producer 的消息，将消息路由到对应的 topic；bookKeeper 用于数据持久化存储和数据复制；Consumer 消费 topic 上的数据。Pulsar 支持多种编程语言和协议（如 Java、C++、Go、Python 等），可以运行在云、本地和混合环境中，扩展性好，支持多租户和跨数据中心复制等特性。因此，Pulsar 被广泛应用于云计算、大数据、物联网等领域的实时消息传递和处理应用中。&lt;/p&gt; 
&lt;h1&gt;二、Pulsar Producer 解析&lt;/h1&gt; 
&lt;p&gt;首先需要了解 Producer 的数据发送流程，这里以「开启压缩、batch 发送消息给 partitioned topic「这样的一个线上常规场景为例，解析数据的发送的关键环节。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;tips：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在 Pulsar 中有无分区（Non-Partitioned）Topic 和有分区 (Partitioned) 的 Topic 之分，Partitioned topic 最小分区数为 1，为满足任务的拓展性，在线上一般使用 Partitioned topic。&lt;/p&gt; 
&lt;h2&gt;2.1 消息生产与发送的详细流程&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://static001.geekbang.org/infoq/66/6663c40abe8c2e1562b1ef8e8393a55d.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Producer 发送数据主要分为&lt;strong&gt;12 个步骤：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;① 创建 Producer：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;partitioned topic 创建的是一个 Partitioned-&lt;/p&gt; 
&lt;p&gt;ProducerImpl 对象，该对象包含了所有分区及其对应的 ProducerImpl 对象，ProducerImpl 对象负责所对应分区数据的维护和发送。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;② 构造消息：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;一条消息被发送前首先会被封装成为一个 Message 对象，对象中包含了所发送的 topic name、消息体、消息大小、schema 类型、metadata（是否指定 key 等）等信息。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;③ 确定目标分区：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在发送消息前需要通过路由策略决定发往哪一个分区，选择对应分区的 ProducerImpl 对象进行进一步处理。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;④ 拦截器：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Producer 可以设置自定义的拦截器，拦截器需要实现 producerInterceptor 接口，在消息发送前可对消息进行拦截修改。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;⑤ 消息堆积控制：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Producer 可以处理的消息是有限的，接收新的消息时会分别进行信号量和内存使用率校验，控制接收消息的速率，防止消息无限在本地堆积。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;⑥ batch 容器管理：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;默认情况下分好区的消息不是直接被发送，而是放入了生产者的一个 batch 缓存容器中里面。在这个缓存里面，多条消息会被封装成为一个批次（batch）。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;⑦ 消息序列化：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;Pulsar 的消息需要从客户端传到服务端，涉及到网络传输，因此 Producer 将 batch 缓冲区中的所有消息逐一进行序列化。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;⑧ 压缩：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Pulsar 内置了多种压缩算法，在发送前会根据所选择的压缩算法对 batch 整体进行压缩，这将优化网络传输以提高 Pulsar 消息传输的性能。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;⑨ 构建消息发送对象：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;无论是开启 batch 的批次消息，还是关闭 batch 的单条消息，都会被包装为一个 OpSendMsg 对象，OpSendMsg 也是 Producer 发送和 pulsar broker 接收处理的最小单位。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;⑩ pending 队列：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;所有构建好的 OpSendMsg 在发送前都会被放入 pendingMessages 队列中，消息处理完成后才会从队列中移除。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;⑪&amp;nbsp;消息传输：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Pulsar 使用 netty 将消息异步的从客户端发送到服务端，Broker 节点将在收到消息后对其进行确认，并将其存储在指定主题的持久存储中。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;⑫ 响应处理：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Pulsar Broker 在收到消息时会返回一个响应，如果写入成功，消息将会从 pendingMessages 队列中移除。如果写入失败，会返回一个错误，生产者在收到可重试错误之后会尝试重新发送消息，直到重试成功或超时。&lt;/p&gt; 
&lt;h2&gt;2.2 关键环节原理分析&lt;/h2&gt; 
&lt;p&gt;接下来会对上述流程中关键环节的设计和原理作进一步的剖析，帮助读者更好的理解 Producer。&lt;/p&gt; 
&lt;h3&gt;2.2.1 创建 Producer&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://static001.geekbang.org/infoq/e5/e52d9588e782f913a00d092f1cdd80f5.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在 Pulsar 中，PartitionedProducerImpl 用于将多个 ProducerImpl 对象包装成为一个逻辑生产者，以便向 Partitioned Topic 发送消息时能够批量操作。其中，PartitionedProducerImpl.producers 成员变量维护了每个分区及其对应的 ProducerImpl 对象，该设计主要有以下&lt;strong&gt;3 个好处：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;① 每个分区对应一个单独的生产者：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在 Pulsar 中，Partitioned Topic 按照分区（Partition）将多个 ProducerImpl 对象进行分配，以便能够同时发往多个 Broker 节点，因此对于每个分区，需要拥有一个单独的生产者以便进行发送操作。在 PartitionedProducerImpl 类中，需要为每个分区维护一个 ProducerImpl 对象，以便在消息被分配好「目标分区」后可以调用对应的 ProducerImpl 进行处理。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;②简化代码逻辑：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在 PartitionedProducerImpl 中，将每个分区及其对应的 ProducerImpl 对象维护在一个 HashMap 中，能够更加方便的维护并管理不同分区的生产者，使得代码逻辑更加清晰简明。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;③ 提高容错性：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;当某个分区的 ProducerImpl 对象无法工作时，可以选择其他可用的 ProducerImpl 对象，从而保证系统整体的可用性。由于将不同分区的 ProducerImpl 对象分别进行维护，因此具备更加灵活的容错处理策略。&lt;/p&gt; 
&lt;p&gt;在线上实践中我们也基于该设计，在 PartitionedProducerImpl 层做了进一步优化，通过感知下一层每个 ProducerImpl 的阻塞状态（信号量的使用情况）来决定新的消息发送，避免将消息持续发往阻塞较为严重的分区，规避了 topic 被某一个分区阻塞而影响到整体发送性能的情况，也提高了线上系统的稳定性，具体的实现可以详见这篇文章《&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzI4NjY4MTU5Nw%3D%3D%26mid%3D2247494958%26idx%3D3%26sn%3Db2f02d545627a1457958289d8f623af3%26scene%3D21%23wechat_redirect" target="_blank"&gt;构建下一代万亿级云原生消息架构：Apache Pulsar 在 vivo 的探索与实践&lt;/a&gt;》。&lt;/p&gt; 
&lt;p&gt;关键代码：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;//对每一个分区都创建一个 ProducerImpl 对象
  private void start(List&amp;lt;Integer&amp;gt; indexList) {
        AtomicReference&amp;lt;Throwable&amp;gt; createFail = new AtomicReference&amp;lt;Throwable&amp;gt;();
        AtomicInteger completed = new AtomicInteger();
 
        for (int partitionIndex : indexList) {
            createProducer(partitionIndex).producerCreatedFuture().handle((prod, createException) -&amp;gt; {
.......
            });
        }
    }
 
    private ProducerImpl&amp;lt;T&amp;gt; createProducer(final int partitionIndex) {
        return producers.computeIfAbsent(partitionIndex, (idx) -&amp;gt; {
            String partitionName = TopicName.get(topic).getPartition(idx).toString();
            return client.newProducerImpl(partitionName, idx,
                    conf, schema, interceptors, new CompletableFuture&amp;lt;&amp;gt;());
        });
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2.2.2 确定目标分区&lt;/h3&gt; 
&lt;p&gt;在发送消息前需要决定发往哪一个分区，确定好分区后便调用对应分区的 ProducerImpl 对象进一步处理，目标分区的确定主要跟「路由策略」和「是否指定 key」有关：&lt;/p&gt; 
&lt;p&gt;**（1）如果消息没有指定 key：**则按照三种路由策略的效果选择分区进行发送，三种路由策略如下：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;SinglePartition：&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;如果消息没有指定 Key，Producer 会随机选择一个 Partition，然后把所有的消息都发送到这个 Partition 上。&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;RoundRobinPartition：&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;生产者将以轮询方式在所有 Partition 之间发布消息，以实现最大吞吐量。需要注意的是如果开启了 batch 发送，则轮询将会以批为单位进行消息发送，批次发送时每隔 partitionSwitchMs 会轮询一个 Partition。如果关闭了批量发送，那么每条消息发送都会轮询一个 Partition。（partitionSwitchMs 至少为一个 batchingMaxPublishDelay 时间）。&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;CustomPartition：&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;使用用户自定义的消息路由实现，根据自定义的 Router 实现决定消息要发往哪个分区。用户自定义的 Router 可以通过 messageRoute 参数设置。自定义的 Router 需要实现 MessageRouter 接口的 choosePartition 方法。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;**（2）如果消息指定 key：**则会对 Key 做哈希处理,然后找到对应的 Partition，把 key 所对应的消息都发送到同一个分区：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;对消息的 Key 进行哈希处理后如何找到对应的 Partition 的？即使用 Key 的哈希值对总的 Partition 数取模：N=(Key 的哈希值% 总的 Partition 数)，得到的 N 就是第 N 个 Partition，Producer 可以通过设置 hashingscheme 来使用不同的哈希算法 ，现在已经支持 JavastringHash 和 Murmur3_32Hash 两种哈希算法，前者直接调用 String.hash.Code()，后者使用 Murmur3。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;路由策略的关键代码如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;//SinglePartition 路由策略：
public int choosePartition(Message&amp;lt;?&amp;gt; msg, TopicMetadata metadata) {
    // If the message has a key, it supersedes the single partition routing policy
    if (msg.hasKey()) {
        return signSafeMod(hash.makeHash(msg.getKey()), metadata.numPartitions());
    }
 
    return partitionIndex;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;//RoundRobin 路由策略：
public int choosePartition(Message&amp;lt;?&amp;gt; msg, TopicMetadata topicMetadata) {
    // If the message has a key, it supersedes the round robin routing policy
    if (msg.hasKey()) {
        return signSafeMod(hash.makeHash(msg.getKey()), topicMetadata.numPartitions());
    }
 
    if (isBatchingEnabled) { // if batching is enabled, choose partition on `partitionSwitchMs` boundary.
        long currentMs = clock.millis();
        return signSafeMod(currentMs / partitionSwitchMs + startPtnIdx, topicMetadata.numPartitions());
    } else {
        return signSafeMod(PARTITION_INDEX_UPDATER.getAndIncrement(this), topicMetadata.numPartitions());
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2.2.3 消息堆积控制&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://static001.geekbang.org/infoq/2f/2fe4f4750d448407920510d97421b11b.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Producer 不可能无限接收新的消息，如果某些分区数据发送较慢，消息就会堆积在 Prouducer 缓存中，导致已经阻塞的分区堆积大量的消息，又无法重新发往其他分区，同时也可能因为无限堆积的消息占用了大量的内存，使得任务频繁 GC 甚至 OOM。&lt;/p&gt; 
&lt;p&gt;在 Pulsar 提供了两个核心的速率限制策略和一个阻塞时的消息处理策略：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;消息数量限制：&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;maxPendingMessages 控制每个分区某一时刻最大可处理消息数量，通过信号量的方式控制「新进入的消息」的信号量分配和「处理完成消息「的信号量释放，防止某个分区的消息严重堆积。&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;消息占用内存大小限制：&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;memoryLimit 控制整个 Pulsar client 的消息最大占用内存大小，通过计数器方式控制「新进入的消息」有效载荷的内存分配和「处理完成消息「有效载荷的内存释放，这里需要特殊说明的是 memoryLimit 是 client 的参数，针对的是该 client 对象下的所有 topic，因此并不建议一个 Pulsar client 对象 new 多个 Producer topic ，因为很容易出现某一个 topic 占用内存过多，导致另一个 topic 无空间可分配的情况。&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;阻塞处理策略：&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;由 blockIfQueueFull 进行控制，当 blockIfQueueFull 为 true 时，代表阻塞等待，Producer 会等待获取信号量；当 blockIfQueueFull 为 false 时，一旦获取不到信号量，就会立刻失败，需要注意的是如果 blockIfQueueFull 为 false，业务需要处理好消息失败后的回调策略，否则会导致数据在 Producer 上「丢失」。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;关键代码如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;public void sendAsync(Message&amp;lt;?&amp;gt; message, SendCallback callback) {
......
        MessageImpl&amp;lt;?&amp;gt; msg = (MessageImpl&amp;lt;?&amp;gt;) message;
        MessageMetadata msgMetadata = msg.getMessageBuilder();
        ByteBuf payload = msg.getDataBuffer();
        int uncompressedSize = payload.readableBytes();
        //对发送队列大小以及 client memory 进行判断是否有空间放入新的消息
        if (!canEnqueueRequest(callback, message.getSequenceId(), uncompressedSize)) {
            return;
        }
......
    }
 
    private boolean canEnqueueRequest(SendCallback callback, long sequenceId, int payloadSize) {
        try {
            if (conf.isBlockIfQueueFull()) {
                //当 blockIfQueueFull 为 true 时，等待获取信号量
                if (semaphore.isPresent()) {
                    semaphore.get().acquire();
                }
                //分配消息有效载荷所需要的内存空间
                client.getMemoryLimitController().reserveMemory(payloadSize);
            } else {
                //当 blockIfQueueFull 为 false 时，如果无法获取到信号量，则快速失败
                if (!semaphore.map(Semaphore::tryAcquire).orElse(true)) {
                    callback.sendComplete(new PulsarClientException.ProducerQueueIsFullError("Producer send queue is full", sequenceId));
                    return false;
                }
                //如果没有如何的内存空间用于消息分配，则报错
                if (!client.getMemoryLimitController().tryReserveMemory(payloadSize)) {
                    semaphore.ifPresent(Semaphore::release);
                    callback.sendComplete(new PulsarClientException.MemoryBufferIsFullError("Client memory buffer is full", sequenceId));
                    return false;
                }
            }
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            callback.sendComplete(new PulsarClientException(e, sequenceId));
            return false;
        }
 
        return true;
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2.2.4 消息 batch 容器打包&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://static001.geekbang.org/infoq/57/57e8428ca0b0093978799d8411231e60.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;（1）batch 关键组成信息&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Messages:&lt;/strong&gt; 保存消息的 list，保存跟这个 batch 相关所有的 MessageImpl 对象。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;**Metadata：**保存 batch 相关的元数据，如批量消息的序列号、消息发送的时间戳等信息。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;**Callback：**保存消息回调逻辑的集合，记录了每一条消息对应的 callback 策略，在 batch 消息发送并等到服务端响应后，依次对消息的回调进行处理。&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;（2）batch 打包条件&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;batch 打包条件的三个关键参数：满足其一数据就会被打包发送出去。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;**批次大小：**batchingMaxBytes&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;**批次条数：**batchingMaxMessages&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;批次延迟发送时间：&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;batchingMaxPublishDelay&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Pulsar 使用两个模块设计来实现上面的参数控制：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;**accumulator：**在 BatchMessage-&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;ContainerImpl 中通过计数器的方式去控制 batch 的大小和条数，numMessages-&lt;/p&gt; 
 &lt;p&gt;InBatch 记录已经缓存的消息数量，currentBatchSizeBytes 用于记录已缓存的消息的大小。当这些变量达到阈值时，BatchMessageContainerImpl 将会触发批量消息的发送。&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;**batchTimerTask：**当生产者使用批量消息发送模式时，Producer 将会创建一个定时器任务（batchTimerTask），并通过计时器的方式定时将 BatchMessageContainer 容器中的消息进行批量发送。&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;2.2.5 消息压缩&lt;/h3&gt; 
&lt;p&gt;如果开启了消息压缩，在发送前都需要进行压缩处理。对於单条消息发送的场景，是对每一条消息进行单独压缩后进行发送；而如果开启了 batch 则是对整个 batch 进行压缩后再整个进行发送。&lt;/p&gt; 
&lt;p&gt;在线上实践中，推荐在不影响业务延迟的情况下 batch 越大越好，主要有&lt;strong&gt;两个理由&lt;/strong&gt;：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;可以优化网络 IO 降低 CPU 负载：&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;不论 Producer 发送的是一条消息还是一批消息，在 pulsar 客户端都会被构建为一个 OpSendMsg 对象，同时 pulsar broker 接收到消息进行写入处理时，也是按照 OpSendMsg 为一个处理单位将消息写入磁盘，因此当消息数量一定时，batch 越大，则代表需要处理的 OpSendMsg 越少。&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;batch 越大「压缩效果则越好」：&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;压缩算法对应的压缩率并不固定，它通常取决于所要压缩的数据对象的内容和压缩算法本身，压缩的本质在于通过消除或利用数据中存在的冗余来实现数据的压缩和重构。而 Pulsar 是以 batch 来进行打包的，batch 越大，压缩的目标包体越大压缩效果则可能越好，同时也能够尽可能避免单条消息因为包体较小导致越压缩后包体越大的情况出现。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;以下是开启了 batch 情况下，构建发送消息和压缩的关键代码：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    public OpSendMsg createOpSendMsg() throws IOException {
        //对数据进行压缩、加密等操作
        ByteBuf encryptedPayload = producer.encryptMessage(messageMetadata, getCompressedBatchMetadataAndPayload());
......
 
        ByteBufPair cmd = producer.sendMessage(producer.producerId, messageMetadata.getSequenceId(),
                messageMetadata.getHighestSequenceId(), numMessagesInBatch, messageMetadata, encryptedPayload);
        //对整个 batch 构建一个 OpSendMsg
        OpSendMsg op = OpSendMsg.create(messages, cmd, messageMetadata.getSequenceId(),
                messageMetadata.getHighestSequenceId(), firstCallback);
......
        return op;
    }
 
    //对 batch 进行压缩，并将压缩后信息更新到 messageMetadata 中
    private ByteBuf getCompressedBatchMetadataAndPayload() {
......
        int uncompressedSize = batchedMessageMetadataAndPayload.readableBytes();
        ByteBuf compressedPayload = compressor.encode(batchedMessageMetadataAndPayload);
        batchedMessageMetadataAndPayload.release();
......
        return compressedPayload;
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2.2.6 pending 队列&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://static001.geekbang.org/infoq/64/6470193ab1e788eb19b49948423a9665.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Pulsar 中的 pendingMessages 队列是客户端用来暂存「未处理完成的消息」的一个缓存队列。用于存储当 Producer 连接到 Broker 服务器后，还未发送或尚未得到 Broker 系统的 ACK 确认的所有生产者（Producer）的消息。在发送消息之前，Producer 首先会将消息缓存到 pendingMessages 队列中，并记录保存缓存消息的 OpSendMsg 对象，直到它被成功发送到了 Broker 端并收到 Broker 发送的 ACK 确认之后，相关的元信息和消息信息才会从队列中移除。&lt;/p&gt; 
&lt;p&gt;需要注意的是：&lt;strong&gt;pending 队列的本质是一个回调处理队列，而不是发送队列&lt;/strong&gt;，消息在放入 pending 队列的同时就被异步发送到服务端了，所以这里需要重点理解什么是「未处理完成的消息」。&lt;/p&gt; 
&lt;p&gt;pendingMessages 队列的&lt;strong&gt;作用在于&lt;/strong&gt;：对于已经发送但尚未收到 ACK 确认的消息，防止在连接出现异常时丢失消息。当连接中断时，缓存在 pendingMessages 队列中的未确认消息将被认为是需要重发的，当连接恢复时，缓存的消息将重新发送到 Broker 端，以确保生产者生产的消息不会丢失。&lt;/p&gt; 
&lt;p&gt;**总的来说，**pendingMessages 队列是 Pulsar 客户端保证消息可靠性和一致性的关键功能组件，在 Pulsar 的生产者（Producer）和消息确认的机制中担任着非常重要的角色。&lt;/p&gt; 
&lt;p&gt;关键代码如下：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;add() 方法用于在追加消息时将指定元素插入队列中的队尾，remove() 用于消息在完成后移除队列头部的元素。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code&gt;    protected void processOpSendMsg(OpSendMsg op) {
        if (op == null) {
            return;
        }
        try {
            if (op.msg != null &amp;amp;&amp;amp; isBatchMessagingEnabled()) {
                batchMessageAndSend();
            }
            //将消息放入「待处理消息队列」
            pendingMessages.add(op);
......
                // If we do have a connection, the message is sent immediately, otherwise we'll try again once a new
                // connection is established
                op.cmd.retain();
                cnx.ctx().channel().eventLoop().execute(WriteInEventLoopCallback.create(this, cnx, op));
                stats.updateNumMsgsSent(op.numMessagesInBatch, op.batchSizeByte);
...... 
    }
 
       //添加消息到 pendingMessages 队列
       public boolean add(OpSendMsg o) {
            // postpone adding to the queue while forEach iteration is in progress
            //batch 的计数是按照 batch 中消息的总量进行计数
            messagesCount.addAndGet(o.numMessagesInBatch);
            if (forEachDepth &amp;gt; 0) {
                if (postponedOpSendMgs == null) {
                    postponedOpSendMgs = new ArrayList&amp;lt;&amp;gt;();
                }
                return postponedOpSendMgs.add(o);
            } else {
                return delegate.add(o);
            }
        }
        //将消息从 pendingMessages 队列移除
        public void remove() {
            OpSendMsg op = delegate.remove();
            if (op != null) {
                messagesCount.addAndGet(-op.numMessagesInBatch);
            }
        }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2.2.7 消息传输&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://static001.geekbang.org/infoq/2d/2d4bd56bfcbf7bbe62118828ee858a69.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Producer 和 broker 都维护了分区维度的 pending 队列来保证消息处理的顺序性，以及实现消息重新发送、重新写入持久化存储的能力。在 Producer 端，消息被顺序追加到 pending 队列并异步发送到服务端，服务端的 pending 队列在接收到消息后，按照顺序追加到队列中，并按照顺序将数据写入 bookie 进行持久化处理，处理完成后按照顺序返回响应 Producer，并将消息从 broker pending 和 producer pending 队列中移除。&lt;/p&gt; 
&lt;p&gt;另外在数据传输过程中，无论是使用 Pulsar Producer 的同步发送还是异步发送，在消息传输环节本质上都是使用 netty 将消息异步的从客户端发送到服务端，区别在于 send() 方法封装了 sendAsync() 方法，使其可以在向服务器发送 Pulsar 消息时阻塞等待 Broker 的响应，直到确认消息已经被 Broker 成功处理后才会返回，常规情况下，建议使用异步的方式发送 Pulsar 消息，因为同步方式必须在 Broker 端成功接收到消息之后才会返回，因此会带来较大的性能损耗和延迟。但是在部分场景下，需要使用同步方式来保证可靠性，以防 Broker 端接收失败，可以考虑使用 send() 方法实现同步方式的方式发送 Pulsar 消息。&lt;/p&gt; 
&lt;p&gt;使用 netty 执行的代码：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    private static final class WriteInEventLoopCallback implements Runnable {  
......
        @Override
        public void run() {
            if (log.isDebugEnabled()) {
                log.debug("[{}] [{}] Sending message cnx {}, sequenceId {}", producer.topic, producer.producerName, cnx,
                        sequenceId);
            }
 
            try {
                cnx.ctx().writeAndFlush(cmd, cnx.ctx().voidPromise());
                op.updateSentTimestamp();
            } finally {
                recycle();
            }
        }
......
    
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2.2.8 处理响应&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://static001.geekbang.org/infoq/28/28dd6d8c28f93ecae7372ea40b5c372b.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Pulsar Producer 使用「ACK 跟踪机制」来实现对 Broker 返回的 ACK 确认消息的处理，用于检测和处理到达生产者的全部消息状态信息。&lt;/p&gt; 
&lt;p&gt;对于 Producer 发送的消息，Pulsar 会对每个消息分配一个唯一的 sequenceId 序号，并记录该消息的创建时间（createdAt）等元数据信息。当 Broker 确认收到某个消息时，Producer 会依据返回的 ACK 序号和 Broker 返回的确认时间来判断当前 ACK 是否有效，并从已缓存的 pendingMessages 队列中找到对应的消息元数据信息，以进行确认处理，在 Broker 确认消息接收成功时，Producer 将从等待确认的消息队列中删除对应的消息元数据信息，如果 Broker 返回的 ACK 消息不符合生产者预期的消息状态信息，Producer 将会重发消息，直到重试成功或多次重试失败后抛出异常后再从队列中移除对应消息元数据信息并释放对应内存、信号量等资源。&lt;/p&gt; 
&lt;p&gt;消息重发的关键代码如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    private void resendMessages(ClientCnx cnx, long expectedEpoch) {
        cnx.ctx().channel().eventLoop().execute(() -&amp;gt; {
            synchronized (this) {
                //判断连接状态：当连接正在关闭或者已经关闭则不进行重发
                if (getState() == State.Closing || getState() == State.Closed) {
                    // Producer was closed while reconnecting, close the connection to make sure the broker
                    // drops the producer on its side
                    cnx.channel().close();
                    return;
                }
......
                //调用重发消息方法
                recoverProcessOpSendMsgFrom(cnx, null, expectedEpoch);
            }
        });
    }
 
 
   // Must acquire a lock on ProducerImpl.this before calling method.
    private void recoverProcessOpSendMsgFrom(ClientCnx cnx, MessageImpl from, long expectedEpoch) {
......
        final boolean stripChecksum = cnx.getRemoteEndpointProtocolVersion() &amp;lt; brokerChecksumSupportedVersion();
        Iterator&amp;lt;OpSendMsg&amp;gt; msgIterator = pendingMessages.iterator();
        OpSendMsg pendingRegisteringOp = null;
        while (msgIterator.hasNext()) {
            OpSendMsg op = msgIterator.next();
......
            op.cmd.retain();
            if (log.isDebugEnabled()) {
                log.debug("[{}] [{}] Re-Sending message in cnx {}, sequenceId {}", topic, producerName,
                          cnx.channel(), op.sequenceId);
            }
            //发送消息
            cnx.ctx().write(op.cmd, cnx.ctx().voidPromise());
            op.updateSentTimestamp();
            stats.updateNumMsgsSent(op.numMessagesInBatch, op.batchSizeByte);
        }
        cnx.ctx().flush();
......
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;三、Pulsar 数据发送端参数调优实践&lt;/h1&gt; 
&lt;p&gt;根据以上对原理解析，我们对 Producer 已经有了一个大致理解，下面通过一个 Producer 参数调优实践案例来帮助读者基于原理进一步理解客户端参数之间的联系。&lt;/p&gt; 
&lt;h2&gt;3.1 调优目的&lt;/h2&gt; 
&lt;p&gt;首先要清楚为什么要进行参数调优，有以下两个目的：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;降低参数使用门槛：&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Pulsar client 和 Producer 的几十个配置参数，参数多且联系紧密，需要花费较多的时间成本去理解，同时参数之间存在协同生效互相影响的情况，对普通使用者而言场景复杂理解门槛高，我们希望能够有一套较为通用的参数配置，或有公式化的参数配置方法论。&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;提升单机处理性能：&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;站在客户端的角度，相同时间内处理的数据量越多，则认为单机处理性能更强。作为中间件系统的提供者，我们经常认为性能提升是服务端的事情，想尽办法在 pulsar 的 broker 和 bookie 上去提升单机处理性能，但 pulsar client 作为整个消息中间件系统的核心组件，它能否发送好一份数据，对整个消息中间件系统的性能和稳定性也发挥着至关重要的作用。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;3.2 调优实践&lt;/h2&gt; 
&lt;p&gt;下面就围绕「参数通用模版化」和「提升单机处理性能」两个目的出发并结合上述讲解的数据发送原理，来分享一些实践经验。&lt;/p&gt; 
&lt;h3&gt;3.2.1 关联与场景相关的重点参数&lt;/h3&gt; 
&lt;p&gt;Pulsar 客户端参数虽多但都提供了默认值，不需要一一调整。只需要对业务场景相关的针对性的去调整即可，如我们本次的参数调优目的是提升单机处理性能，则重点关注哪些场景哪些参数可以提升客户端的发送速率、降低服务端的压力，让服务端可以处理更多的数据，有以下四点最为关键：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;batch 打包发送：&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;消息多条批次发送，在降低客户端和服务端网络 IO 的同时也降低了两者的 cpu 的负载。这里需强调的是我们希望 batch 是一个均匀的、「完整」的包，如 pending 队列被打满，batch 只能空等到延迟发送时间过后被发送，没有构建出预期中的 batch，那么可以认为这个 batch 是一个不完整的包，这种 batch 包含的数据量少，对发送效率有着极大的影响。&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;数据压缩：&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Pulsar 是 IO 密集型系统，常规情况下磁盘是系统的主要瓶颈，开启压缩可以有效降低网络 I/O，提升处理相同数据量下的读写能力。由于压缩是针对 batch 的，在发送时间一定的情况下，batch 越大其压缩效果也越好，代表着处理的消息量也更多。&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;RoundRobin 发送：&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;将数据均匀地分配到多个分区中。它的基本思想是轮询将新的数据写入到不同的分区中，以均衡地分散负载。&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;消息堆积控制：&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;maxPendingMessages 信号量和 memoryLimit 限制不直接提升发送速率，但它能够有效保障我们客户端的稳定，也是控制或限制发送效率的重要参数之一。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;涉及的客户端关键参数以及默认值和我们线上调优后设置的数值如下表：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static001.geekbang.org/infoq/fa/fa59c582553e31ab2ecc8027cdce5f13.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;3.2.2 结合 Producer 发送原理分析参数的效果&lt;/h3&gt; 
&lt;p&gt;接下来我们以参数的效用角度来描述一条消息从构建到发送的过程，进一步解释参数如此设置的意义：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;（1）选择分区&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;构建消息后，通过 messageRoutingMode 参数所设置的路由策略来选择分区，这里以 RoundRobinPartition 为路由策略，开启 batch 时则每间隔 partitionSwitchMs 时间换一个分区进行数据发送，partitionSwitchMs 的值为「batchingPartitionSwitchFrequencyByPublish&lt;/p&gt; 
&lt;p&gt;-Delay、batchingMaxPublishDelayMicros」这两个 Producer 参数之积，也就是每 batchingPartition&lt;/p&gt; 
&lt;p&gt;-SwitchFrequencyByPublishDelay 个 batch 的最大打包时间，消息就会轮换一个分区发送。&lt;/p&gt; 
&lt;p&gt;为了能在 batchingMaxPublishDelayMicros 内得到一个较大的包，我们希望这个 batch 接收的消息是连续的，因此 batchingPartitionSwitchFrequency-&lt;/p&gt; 
&lt;p&gt;ByPublishDelay 不能小于 1，同时也希望一个分区之间数据是较为均匀的，所以 batchingPartition-&lt;/p&gt; 
&lt;p&gt;SwitchFrequencyByPublishDelay 也要尽量小，否则分区对应的信号量 maxPendingMessages 耗尽还没有切换分区，就会导致 batch 必须等待一个 batchingMaxPublishDelayMicros 时间。因此将 batchingPartitionSwitchFrequencyByPublishDelay 修改成了 1，保证打包了一个 batch 之后就切换分区，这也极大的避免了分区信号量耗尽，出现发送阻塞。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;（2）消息堆积控制&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;maxPendingMessages 作为分区的信号量，也是「pending 队列」的大小，代表着每个分区能够同时处理的最大消息上限，而 maxPendingMessages-&lt;/p&gt; 
&lt;p&gt;AcrossPartitions 则是针对整个 topic 生效的，maxPendingMessages=min( maxPending-&lt;/p&gt; 
&lt;p&gt;Messages,maxPendingMessagesAcrossPartitions/Partition），由于线上分区可能会变化，有不确定性，因此就使用上而言除非有特殊的使用场景，建议将 maxPendingMessagesAcrossPartitions 设置的比较大，让 maxPendingMessages 生效即可。&lt;/p&gt; 
&lt;p&gt;除了 maxPendingMessages 以外，消息能否接收被放入 pending 队列中，还要看当前正在处理的消息体大小总和是否超过了 memoryLimit 参数的限制，memoryLimit 控制了消息待处理队列中未压缩前的消息有效荷载总和，可以避免在消息有效荷载非常大时，还未触发 maxPendingMessages 限制，就导致内存占用过多出现频繁 GC 和 oom 的问题。由于 memoryLimit 是 client 级别的策略，因此也建议一个 client 对应一个 Poducer。&lt;/p&gt; 
&lt;p&gt;总而言之 maxPendingMessages 控制了每个分区可以处理消息数量的上限，memoryLimit 控制了所有分区可以消息占用内存的上限，两者相辅相成。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;（3）消息 batch 容器打包&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;决定一个 batch 是否打包完成有三个条件控制，batchingMaxBytes、batchingMaxMessages、batchingMaxPublishDelayMicros 满足其一即可，根据这三个参数的含义去设置值看似是容易的，但容易忽略的是 batch 中用来打包的消息也是受 memoryLimit 和 maxPendingMessages 制约的，应该避免出现 batch 中消息的数量超过 memoryLimit 和 maxPendingMessages 导致 batch 打包效率受影响。举个例子，当 maxPendingMessages 设置为 500，而 batchingMaxMessages 设置 1000 时，batch 就永远无法满足消息条数达到 1000 的条件，只能空等 batchingMaxPublishDelayMicros 或者 batchingMaxBytes 两者生效。&lt;/p&gt; 
&lt;h3&gt;3.2.3 公式化模版&lt;/h3&gt; 
&lt;p&gt;通过上述分析，大致了解了关键参数的生效效果，且彼此相互关联，根据这些关系就能够输出一个较为简单的参数调优模版。&lt;/p&gt; 
&lt;p&gt;假设我们发送的单条消息大小为：messageByte；分区数量为：partitionNum。&lt;/p&gt; 
&lt;p&gt;那么对应参数调整公式如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;//业务发送速率越大，这里设置的值越大
maxPendingMessages：一般 1000-2000 之间
 
//这里值也可以设置大一些，让 maxPendingMessages 生效即可
maxPendingMessagesAcrossPartitions = maxPendingMessages * partitionNum
 
//memoryLimit 的值就是打算阻塞总消息大小，这与消息体和 maxPendingMessages 有关
memoryLimit=(maxPendingMessages * partitionNum * messageByte)
 
//batch 的条数不超过「待处理消息队列」大小的一半
batchingMaxMessages=maxPendingMessages/2，这样可以保证在消息发送等待 ack 的时候，该分区剩下一半的空间还能用来构建一个 batch
 
//batch 大小同理，batch 大小不超过「待处理消息队列」消息大小的一半
batchingMaxBytes= Math.min(memoryLimit * 1024 * 1024 /partitionNum/2,1048576)
 
//业务能够接受的延迟大小，一般延迟时间越大，batch 越大
batchingMaxPublishDelayMicros=1ms-100m 皆可
 
//每构建一个 batch 就转换一个分区
batchingPartitionSwitchFrequencyByPublishDelay=1
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;可以看到根据上面的分析，参数之间是有一个模版化的公式，但这也不是唯一的，读者可以根据自己的业务场景进行调整。在真实使用过程中线上的消息大小以及分区数量实际上是会变化的，因此真正的参数设置还需要根据实际情况来确定，比如我们线上通常的做法是根据机器配置将 memoryLimit 直接设置为 64M-256M，分区数量我们线上不会超过 1000，那么这里就假设为 1000，确定了这两个参数，其他的参数的值也就确定了。&lt;/p&gt; 
&lt;h3&gt;3.2.4 效果对比&lt;/h3&gt; 
&lt;p&gt;以线上一个业务参数调优为例，前后都开启压缩的情况下调整上述参数后的一个效果。&lt;/p&gt; 
&lt;p&gt;服务端（Pulsar）：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static001.geekbang.org/infoq/c4/c45ed239c62c651c39708935958c9416.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static001.geekbang.org/infoq/3c/3c0534426c68d4af315d6d436ea2fcdc.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;优化前后对比数据：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static001.geekbang.org/infoq/3b/3bf425c9885d7cb1c6c6f50f6afde351.jpeg" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;相同的写入速率，Pulsar 服务端网卡流量缩减约 50%（batch 包体增加，压缩效果提升），cpu 负载降低约 90%，Pulsar 服务端总体成本相较优化前至少可降低 50% 以上，客户端也有一定程度的负载降低。&lt;/p&gt; 
&lt;p&gt;参数调整后，CPU 负载得到明显降低，一定程度上避免了 CPU 成为系统的瓶颈，同时由于压缩效果的提升，Pulsar 的磁盘 IO 负载得到显著降低，可以用更少的机器处理更多的数据。&lt;/p&gt; 
&lt;h1&gt;四、总结&lt;/h1&gt; 
&lt;p&gt;理解 Producer 发送原理以及核心参数是写好数据发送程序最为有效的手段，最简单的客户端参数优化反而隐藏了巨大的收益。本文通过对 Producer 原理进行剖析、对消息的流转过程中参数效用进行讲解，并配合参数调优实践案例，介绍了具体的分析思路和调优的方法，在实际使用过程中通过对核心的几个上游系统进行调优，服务端单机处理能力至少提升了一倍以上，成本得到了极大的降低。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;参考文章：&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fpulsar.apache.org%2Fdocs%2F4.0.x%2Fconcepts-overview%2F" target="_blank"&gt;https://pulsar.apache.org/docs/4.0.x/concepts-overview/&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/vivotech/blog/18619282</link>
      <guid isPermaLink="false">https://my.oschina.net/vivotech/blog/18619282</guid>
      <pubDate>Thu, 12 Jun 2025 07:18:11 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>《智能体网络协议技术报告》发布</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;W3C&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzA3OTY5NDI0OA%3D%3D%26mid%3D2247489930%26idx%3D1%26sn%3Dc3c4ed0e725c88f03e2e6cb6c82c13dc%26scene%3D21%23wechat_redirect" target="_blank"&gt;&lt;strong&gt;AI Agent Protocol 社区组&lt;/strong&gt;&lt;/a&gt;于今年 5 月成立，致力于孵化下一代智能体之间的交互协议，让智能体能够在互联网上使用协议进行高效的连接与协作，推动智能体在 Web 上的安全、高效、可信连接与协作。&lt;/p&gt; 
&lt;p&gt;小组现发布《智能体网络协议技术报告》：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fw3c-cg.github.io%2Fai-agent-protocol%2F" target="_blank"&gt;https://w3c-cg.github.io/ai-agent-protocol/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;另见该报告的中文翻译参考：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fw3c-cg.github.io%2Fai-agent-protocol%2Findex_cn.html" target="_blank"&gt;https://w3c-cg.github.io/ai-agent-protocol/index_cn.html&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0612/145659_Z93C_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0612/145939_LjvL_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;这份报告探讨了从语义网（Semantic Web）的未竟愿景到智能体网络（Agentic Web）的演进历程，并分析了构建标准化智能体网络协议的必要性。&lt;/p&gt; 
&lt;p&gt;尽管二十年前提出的语义网构想极具前瞻性，但受限于当时人工智能技术的能力不足，未能充分实现。随着大型语言模型（LLMs）等现代 AI 技术的飞速发展，智能体已具备自主执行任务、进行复杂推理和解决多步骤问题的能力，从而催生了 Agentic Web 的出现。&lt;/p&gt; 
&lt;p&gt;通过系统分析，该报告给出智能体网络的&lt;strong&gt;四大核心趋势&lt;/strong&gt;：&lt;strong&gt;智能体取代传统软件成为互联网基础设施&lt;/strong&gt;、&lt;strong&gt;智能体间实现普遍互联互通&lt;/strong&gt;、&lt;strong&gt;基于协议的原生连接模式&lt;/strong&gt;、以及&lt;strong&gt;智能体的自主组织与协作能力&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;同时，研究揭示了当前互联网架构对 Agentic Web 发展的&lt;strong&gt;三大挑战&lt;/strong&gt;：&lt;strong&gt;数据孤岛限制智能体决策质量、人机界面阻碍智能体交互效率、以及标准协议缺失阻碍智能体协作&lt;/strong&gt;。针对这些挑战，报告详细阐述了智能体网络协议的设计原则与核心需求，并对当前主要智能体网络协议倡议（MCP、A2A、ACP、ANP 等）进行了系统比较与分析。&lt;/p&gt; 
&lt;p&gt;报告强调，建立标准化智能体网络协议对于打破数据孤岛、实现异构智能体协作、构建 AI 原生数据网络，以及最终实现开放、高效的 Agentic Web 具有关键作用，并呼吁各利益相关方积极参与 W3C 的标准化进程。这是一个塑造未来网络的机会：一个更智能、更协作、更赋能的网络，建立在开放和可信的基础之上。一个精心设计的 Agentic Web 具有巨大的变革潜力，而现在正是为其奠定坚实基础的关键时刻。&lt;/p&gt; 
&lt;p&gt;欢迎参与 AI Agent Protocol 社区组，共同定义 AI Agent 的 Web 通信标准，携手构建可信、安全的智能体互联网生态！&lt;/p&gt; 
&lt;p&gt;参与方式详见：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.w3.org%2Fcommunity%2Fagentprotocol%2Fjoin" target="_blank"&gt;https://www.w3.org/community/agentprotocol/join&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;关于 W3C 社区组&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;W3C 通过社区组（Community Groups）为全球社区提供一个广泛交流 Web 技术进而探索孵化未来新标准的开放平台，从而满足日益增长的各方 Web 参与者的技术交流需求。W3C 社区组面向公众开放，任何感兴趣的单位及个人均可参与。&lt;/p&gt; 
&lt;p&gt;W3C 目前设有 144 个社区组：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.w3.org%2Fgroups%2Fcg%2F" target="_blank"&gt;https://www.w3.org/groups/cg/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;欢迎了解如何参与社区组讨论（中文指南）：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.chinaw3c.org%2Fhowtocg.html" target="_blank"&gt;https://www.chinaw3c.org/howtocg.html&lt;/a&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;来源：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FlSPl8HprLBPuGmkJYvEuBA" target="_blank"&gt;https://mp.weixin.qq.com/s/lSPl8HprLBPuGmkJYvEuBA&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/355009/w3-org-agent-network-protocol-whiter-paper</link>
      <guid isPermaLink="false">https://www.oschina.net/news/355009/w3-org-agent-network-protocol-whiter-paper</guid>
      <pubDate>Thu, 12 Jun 2025 07:00:11 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>黄仁勋论 AI 与量子技术驱动新浪潮，微美全息正加速量子计算产业化融合</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;北京时间 6 月 12 日，英伟达 CEO 黄仁勋在法国巴黎召开的 VivaTech2025 上表示，对量子计算越来越看好。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;「量子计算正迎来拐点，我们即将能够在一些有趣的领域应用量子计算。」黄仁勋在在本次演讲中表示，英伟达会以多种方式与世界各地的量子计算公司合作。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;div&gt; 
   &lt;div&gt; 
    &lt;div&gt; 
     &lt;div&gt; 
      &lt;div&gt;
       &lt;img src="https://oscimg.oschina.net/oscnet//95030e6360550155a4bc4588318030aa.png" width="719" referrerpolicy="no-referrer"&gt; 
       &lt;div&gt;
        &amp;nbsp;
       &lt;/div&gt; 
      &lt;/div&gt; 
     &lt;/div&gt; 
    &lt;/div&gt; 
   &lt;/div&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;此前，在 GTC 2025 大会上，英伟达举办了首个「量子日」活动，黄仁勋公开表示，「对时间表判断错误」，并宣布设立量子研究中心，旨在帮助量子计算公司利用英伟达硬件助力其工作。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;现如今，黄仁勋称，未来几年或至少下一代超级计算机中，它们中的每一个都将拥有连接到 GPU 的 QPU（量子处理器），QPU 将进行量子计算，而 GPU 将用于预处理、控制、纠错、后处理。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;div&gt; 
   &lt;div&gt; 
    &lt;div&gt; 
     &lt;div&gt; 
      &lt;div&gt;
       &lt;img src="https://oscimg.oschina.net/oscnet//2723d8c890a73ac2a0a84317244e87dc.png" width="719" referrerpolicy="no-referrer"&gt; 
       &lt;div&gt;
        &amp;nbsp;
       &lt;/div&gt; 
      &lt;/div&gt; 
     &lt;/div&gt; 
    &lt;/div&gt; 
   &lt;/div&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;更重要的是，黄仁勋在开场中还表示，GB200 NVL72 系统将加速量子计算产业发展。英伟达正借助 GB200 NVL72 平台与 CUDA-Q 软件栈，推动 AI 与量子计算的协同发展。比如，GB200 NVL72 输出量子训练数据的速度比基于 CPU 的技术快 4000 倍，有助于将最新的 AI 进展引入量子计算。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;事实上，量子计算机是利用量子力学定律，解决对经典计算机来说过于复杂的问题的机器，其目的是处理更多的数据量，以促进医学、科学和金融等领域的突破。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;div&gt; 
   &lt;div&gt; 
    &lt;div&gt; 
     &lt;div&gt; 
      &lt;div&gt;
       &lt;img src="https://oscimg.oschina.net/oscnet//e6c8dec8e124671c4d3a31f832222047.png" width="719" referrerpolicy="no-referrer"&gt; 
       &lt;div&gt;
        &amp;nbsp;
       &lt;/div&gt; 
       &lt;div&gt;
        &lt;span&gt;&lt;span&gt;业内人士分析，过去 5 年，人工智能技术特别是生成式 AI 的爆发，看到计算模式出现了很多颠覆性的发展。未来 5 年，量子计算很可能从实验室走向应用，所以，人工智能与量子计算的融合有望成为必然趋势。&lt;/span&gt;&lt;/span&gt;
       &lt;/div&gt; 
      &lt;/div&gt; 
     &lt;/div&gt; 
    &lt;/div&gt; 
   &lt;/div&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;显然，越来越多的企业在加入开拓「量子计算」未来产业新赛道的行列，资料显示，量子计算概念股微美全息（WIMI.US），积极推动量子计算融入 AI 生态，正加速「量子+AI」技术落地，通过构建 AI 平台基础设施、布局技术研究中心及推进量子技术融合，以全栈式布局加码量子计算+人工智能市场。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;当前，微美全息深刻认识到人工智能是底座，量子科技是跃迁力，而两者的融合正是抢占未来产业、未来话语权的关键路径。因此，微美全息以人工智能为基座，聚焦量子技术、人形机器人、人工智能三大风口，扩展布局量子产业前沿领域，未来，将重点关注量子算法加速 AI 训练、神经拟态计算等融合赛道，让更多成果涌现。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;整体而言，量子计算与经典计算的融合将成为未来技术发展的重要方向，许多企业在这一领域的布局显示了其对前沿技术的敏锐洞察。不过也要意识到一点，目前量子计算的产业格局仍处于早期阶段，格局尚未成型，全球量子计算机数量较少，量子计算芯片将是整个领域未来发展的重点和难点。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
&lt;/div&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/355008</link>
      <guid isPermaLink="false">https://www.oschina.net/news/355008</guid>
      <pubDate>Thu, 12 Jun 2025 06:56:11 GMT</pubDate>
      <author>来源: 投稿</author>
    </item>
    <item>
      <title>Meta 发布开源世界模型 V-JEPA 2</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;Meta 发布了最新的开源世界模型&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fai.meta.com%2Fblog%2Fv-jepa-2-world-model-benchmarks%2F" target="_blank"&gt;V-JEPA 2&lt;/a&gt;，称其在物理世界中实现了最先进的视觉理解和预测，从而提高了 AI agents 的物理推理能力。&lt;/p&gt; 
&lt;p&gt;&lt;img height="559" src="https://static.oschina.net/uploads/space/2025/0612/144127_0qiP_2720166.png" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;开源地址：https://github.com/facebookresearch/vjepa2&lt;/em&gt;&lt;br&gt; &lt;em&gt;官网地址&lt;/em&gt;&lt;em&gt;：https://ai.meta.com/vjepa/&lt;br&gt; 论文地址：https://ai.meta.com/research/publications/v-jepa-2-self-supervised-video-models-enable-understanding-prediction-and-planning/&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;V-JEPA 2 是一种&lt;strong&gt;联合嵌入预测架构&lt;/strong&gt;（&lt;strong&gt;Joint Embedding Predictive Architecture&lt;/strong&gt;）模型，这也是「JEPA」的名称由来。&lt;/p&gt; 
&lt;p&gt;模型包括两个主要组成部分：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;一个编码器，负责接收原始视频，并输出包含对于观察世界状态语义上有用的内容的嵌入（embeddings）。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img height="597" src="https://static.oschina.net/uploads/space/2025/0612/150421_f1zo_2720166.png" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;一个预测器，负责接收视频嵌入和关于要预测的额外内容，并输出预测的嵌入。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0612/150628_SYFp_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;V-JEPA 2 跟传统预测像素的生成式模型有很大性能差异，根据 Meta 测试数据，V-JEPA 2 执行任务时每个步骤的规划用时缩短至 Cosmos 模型的三十分之一，不仅用时短，V-JEPA 2 的成功率还更高。&lt;/p&gt; 
&lt;p&gt;V-JEPA 2 的能力对现实世界 agents 理解复杂运动和时间动态（temporal dynamics），以及根据上下文线索预测动作都非常关键。基于这种预测能力，世界模型对于规划给定目标的动作顺序非常有用，比如从一个杯子在桌子上的状态到杯子在桌子边上的状态，中间要经历怎样的动作。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0612/150646_SoAo_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;据介绍，V-JEPA 2 的核心架构是一个自监督学习框架，通过互联网规模的视频数据来训练模型，使其能够学习到视频中的动态和静态信息。预训练阶段使用了超过 100 万小时的视频和 100 万张图像，这些数据涵盖了各种动作和场景。预训练的目标是让模型能够通过观察学习到世界的背景知识，而无需依赖于大量的标注数据。&lt;/p&gt; 
&lt;p&gt;值得一提的是，图灵奖获得者、Meta 首席科学家杨立昆（Yann LeCun）参与了该模型的开发，这在 Meta 开源的众多大模型中很罕见。他在官方视频中提到，在世界模型的帮助下，AI 不再需要数百万次的训练才能掌握一项新的能力，世界模型直接告诉了 AI 世界是怎样运行的，这可以极大提升效率。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/355002/meta-vjepa-2-world-model</link>
      <guid isPermaLink="false">https://www.oschina.net/news/355002/meta-vjepa-2-world-model</guid>
      <pubDate>Thu, 12 Jun 2025 06:45:11 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Liquid Glass React —— 「液态玻璃」的 React 实现</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                            &lt;p&gt;Liquid Glass React 是苹果「液态玻璃（Liquid Glass）」设计语言的 React 实现。&lt;/p&gt;

&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-09f93126aa8a24944a023d1e74e2ab9d9a5.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-f90f27edf2a257be2d13ededab62bc276f0.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;特性&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;良好的边缘弯曲和折射&lt;/li&gt;
&lt;li&gt;多种折射模式&lt;/li&gt;
&lt;li&gt;可配置的冰霜级别&lt;/li&gt;
&lt;li&gt;支持任意子元素&lt;/li&gt;
&lt;li&gt;配置的填充&lt;/li&gt;
&lt;li&gt;修正悬停和点击效果&lt;/li&gt;
&lt;li&gt;边缘和高亮会像苹果一样呈现底层光线&lt;/li&gt;
&lt;li&gt;可配置的色差&lt;/li&gt;
&lt;li&gt;可配置的弹性参数，以模仿苹果的"液体"触感&lt;/li&gt;
&lt;/ul&gt;

                                                                    &lt;/div&gt;
                                                                </description>
      <link>https://www.oschina.net/p/liquid-glass-react</link>
      <guid isPermaLink="false">https://www.oschina.net/p/liquid-glass-react</guid>
      <pubDate>Sun, 11 May 2025 06:21:00 GMT</pubDate>
    </item>
    <item>
      <title>小红书、B 站等平台清理违规 AI 产品营销信息</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;&lt;span style="color:#000000"&gt;网信上海微信公号&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fqmfe8tiUlXJDnt525ucA-Q" target="_blank"&gt;发文称&lt;/a&gt;，为贯彻落实中央网信办「清朗·整治 AI 技术滥用」工作部署，4 月下旬以来，上海市委网信办聚焦 6 类突出问题深入开展第一阶段专项行动。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="329" src="https://oscimg.oschina.net/oscnet/up-11e6dea0c86ec77bcd50df467bb75257ff6.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;上海市委网信办指导小红书、哔哩哔哩、拼多多等 15 家重点网站平台，集中清理「一键脱衣」、未经授权的人脸或人声克隆编辑、未备案等违规 AI 产品、商品及相关营销、炒作、推广、教程信息。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;小红书、哔哩哔哩主动发布专项行动治理公告，开通了有害 AI 内容的举报受理处置渠道；星野开展智能体全面排查清理。各重点网站和 AI 平台共拦截清理相关违法违规信息 82 万余条，处置违规账号 1400 余个，下线违规智能体 2700 余个。经整治，网络违规 AI 信息显著减少。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;目前，上海市已完成 82 款大模型备案，87 款应用登记。对 3 款未履行备案或登记程序提供服务且存在风险的应用，上海市委网信办依法约谈并给予行政处罚。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354996</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354996</guid>
      <pubDate>Sun, 11 May 2025 06:19:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>10+ PG 国际大咖助阵！IvorySQL 生态大会 6.27 开启</title>
      <description>IvorySQL 2025 生态大会将于 6 月 27 日在济南开幕，汇集来自开源和 PostgreSQL 社区的采用者和国内外技术专家。本次大会将以 PostgreSQL 技术生态为核心，聚焦全球数据库技术发展趋势、开源创新与行...</description>
      <link>https://howconf.cn/</link>
      <guid isPermaLink="false">https://howconf.cn/</guid>
      <pubDate>Sun, 11 May 2025 05:52:00 GMT</pubDate>
    </item>
    <item>
      <title>苹果高管回应「个性化版 Siri」延期：技术架构限制导致未达到预期标准</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;在今年的 WWDC25 主题演讲中，苹果的 AI 智能助手 Siri 显然没有占据重要位置。苹果仅简要提到它，并重申了开发进展比预期更慢，集成苹果智能（Apple Intelligence）需要更长时间，预计将在 「明年」 推出。&lt;/p&gt; 
&lt;p&gt;演讲结束后，苹果软件工程高级副总裁克雷格・费德里吉（Craig Federighi）和全球营销副总裁格雷格・乔斯维亚克（Greg Joswiak）&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.tomsguide.com%2Fai%2Fapple-intelligence%2Fwwdc-interview-apples-craig-federighi-and-greg-joswiak-on-siri-delay-voice-ai-as-therapist-and-whats-next-for-apple-intelligence"&gt;进行了深度对话&lt;/a&gt;，解释了苹果在 Apple Intelligence、Siri 和 AI 领域的战略思考。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-fcd007add07610a84ea1a7e78288a2d830c.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;苹果曾承诺在 2024 年底发布集成 Apple Intelligence 的 Siri 更新，但最终未能如期交付，并在 2025 年春季承认该功能还需要更多时间。至于原因，外界一直未能搞清楚，而苹果一向不会展示那些无法按时交付的技术或产品。&lt;/p&gt; 
&lt;p&gt;访谈中，费德里吉详细解释了问题的所在，并说明了苹果如何继续推进这一计划。&lt;/p&gt; 
&lt;p&gt;费德里吉提到在开发过程中意识到可以&lt;strong&gt;基于设备端的大语言模型、私人云计算基础设施和设备端的语义索引技术提升 Siri 的智能化水平&lt;/strong&gt;，&lt;strong&gt;并设想通过 V1 架构协调应用意图来触发设备上的更多操作&lt;/strong&gt;，让 Siri 执行更多任务。例如，利用语义索引中的个人知识，当用户询问特定问题时，Siri 能从消息或邮件中找到相关内容，并通过应用意图执行相关操作。但这些功能在 V1 架构下尚未完全交付。&lt;/p&gt; 
&lt;p&gt;在苹果致力于开发 Siri 架构 V1 的同时，它也在打造 V2 架构 —— 费德里吉称其为 「&lt;strong&gt;更深层次的端到端架构&lt;/strong&gt;，我们知道这才是最终要实现的架构，是让 Siri 具备完整功能的架构。」&lt;/p&gt; 
&lt;p&gt;费德里吉说道：「我们花了几个月的时间，不断优化 V1 架构在更多应用意图和搜索功能方面的表现。但从根本上看，我们发现该架构的局限性没有办法达到客户所期望的质量水平。因此，我们决定转向 V2 架构。但当我们意识到这一点时，已经是春季了，于是我们向外界说明，无法按计划发布，并将继续转向新架构。」&lt;/p&gt; 
&lt;p&gt;费德里吉进一步表示，即便采用第二代架构，苹果仍在不断优化 Siri 功能，确保其达到最佳状态。&lt;/p&gt; 
&lt;p&gt;苹果市场负责人 Greg Joswiak 在访谈中确认，「未来一年」 指向 2026 年，科技媒体 MacRumors 推测个性化 Siri 功能很可能随 iOS 26.4 版本于 2026 年春季面世。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;相关阅读：&lt;a href="https://www.oschina.net/news/337918/apple-says-some-ai-improvements-siri-delayed" target="news"&gt;苹果推迟上线 Siri 中的 AI 相关功能&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354969/apples-on-siri-delay</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354969/apples-on-siri-delay</guid>
      <pubDate>Sun, 11 May 2025 03:31:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>阿里蔡崇信：被 DeepSeek 逼急，工程师春节彻夜留守搞研发</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;&lt;span style="color:#000000"&gt;彭博社报道称，DeepSeek 在，今年 1 月推出低成本、功能强大的人工智能模型震惊全球科技行业后，也给阿里巴巴带来了巨大的紧迫感。为迅速追赶这一技术突破，阿里巴巴的工程师们甚至取消了最重要的中国传统节日——春节的休假，彻夜留守公司，全力投入 AI 研发。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="325" src="https://oscimg.oschina.net/oscnet/up-a5dcc123152fe1c8b0935e049dda3c97f2b.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;阿里巴巴董事长蔡崇信在本周三举行的巴黎 VivaTech 科技大会上，生动地讲述了这段「争分夺秒」的经历。他表示，当 DeepSeek 发布 R1 模型时，阿里巴巴内部意识到自身在 AI 领域已然落后。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;面对这一挑战，公司的工程负责人当机立断，决定取消春节假期，要求所有工程师留在公司，甚至睡在办公室里，以最快的速度进行开发。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;「几周之内，我们推出了自己的版本——通义（Qwen）系列模型，表现相当不错，竞争力很强。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;此后，阿里巴巴推出了一系列新的 AI 模型，并将公司的业务重心进一步转向人工智能和通用人工智能（AGI）。为支撑这一战略转型，阿里巴巴还承诺在未来三年内投入超过 3800 亿元人民币 (约合 530 亿美元)，用于建设包括数据中心在内的 AI 基础设施。今年早些时候，阿里巴巴还与苹果公司达成合作协议，为 iPhone 提供 AI 技术支持。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354967</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354967</guid>
      <pubDate>Sun, 11 May 2025 03:13:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>可观测性第四大支柱：配置数据的监控</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                                        &lt;p style="color:#333333; text-align:left"&gt;业内经常讲可观测性有三大支柱：指标、日志、链路追踪，本文作者认为，还有第四大支柱：那就是配置类数据。配置类数据的变更也会影响系统的稳定性，也值得被监控，方便我们快速排查问题。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p style="color:#333333"&gt;&lt;span&gt;原文链接：https://www.cloudquery.io/blog/fourth-lost-pillar-of-observability-config-data-monitoring&lt;/span&gt;&lt;br&gt; &lt;span&gt;原文作者：Yevgeny Pats&lt;/span&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p style="color:#333333; text-align:left"&gt;很多关于日志、指标和跟踪的内容已经被广泛讨论，因为它们确实是可观测性、应用程序和系统监控的关键组成部分。然而，经常被忽视的是配置数据及其可观测性。在这篇博客中，我们将探讨什么是配置数据，它与日志、指标和跟踪有何不同，并讨论需要什么样的架构来存储这种类型的数据以及在哪些场景中它具有价值。&lt;/p&gt; 
&lt;span id="OSC_h2_1"&gt;&lt;/span&gt; 
&lt;h2&gt;日志、指标和链路追踪的快速回顾&lt;/h2&gt; 
&lt;p style="color:#333333; text-align:left"&gt;对于那些对可观测性的三大支柱还不太了解的人来说，让我们快速回顾一下：&lt;/p&gt; 
&lt;p style="color:#333333; text-align:left"&gt;&lt;img height="738" src="https://oscimg.oschina.net/oscnet/up-58f95ee676e13fe3a0619f3d3d0f956e34d.png" width="1316" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;日志：系统内发生事件的详细记录。它们提供了关于特定事件的信息，包括时间戳、错误消息和其他相关细节。日志有助于调试和故障分析。&lt;/li&gt; 
 &lt;li&gt;指标：定期收集的数值测量。它们有助于监控系统健康状况、性能和随时间的行为。示例包括 CPU 使用率、请求速率、错误率和响应时间。&lt;/li&gt; 
 &lt;li&gt;跟踪：记录请求在分布式系统中通过不同服务的流程。跟踪提供请求流程的可见性，有助于识别瓶颈并理解依赖关系。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#333333; text-align:left"&gt;这些技术的后端通常是某种时间序列数据库，数据类型通常是低基数数据（可以是高基数，但通常会变得昂贵且不建议这样做）。另一个关键方面是，要获取这些指标，通常需要对系统进行插桩，即，您需要访问应用程序或基础设施，以便部署 agent 或添加 Prometheus Exporter。&lt;/p&gt; 
&lt;span id="OSC_h2_2"&gt;&lt;/span&gt; 
&lt;h2&gt;配置数据：第四支柱&lt;/h2&gt; 
&lt;p style="color:#333333; text-align:left"&gt;基础设施不仅限于 AWS EC2 实例，还包括 IAM 用户、安全工具配置、SaaS 应用程序等。这些配置数据在几个重要方面与传统的可观测性数据不同：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;无法直接监控：这些系统无法直接进行监控，但它们通过 API 暴露其配置。&lt;/li&gt; 
 &lt;li&gt;高基数和关系型：数据通常具有高基数，并且高度关系型。它也不像服务器上的磁盘 I/O 指标那样频繁变化，而是更侧重于配置状态。&lt;/li&gt; 
 &lt;li&gt;较低频率，更高细节：我们在这里想要做的权衡是较少的采集频率，但具有更高的基数和细节。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;span id="OSC_h2_3"&gt;&lt;/span&gt; 
&lt;h2&gt;为什么配置数据很重要&lt;/h2&gt; 
&lt;p style="color:#333333; text-align:left"&gt;配置数据监控填补了您可观测性策略中的关键空白：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;安全态势监控：跟踪 IAM 权限、安全组规则、加密设置以及其他影响您安全态势的配置项。&lt;/li&gt; 
 &lt;li&gt;合规性跟踪：监控配置以符合内部政策或外部合规要求（SOC2、HIPAA、PCI-DSS 等）。&lt;/li&gt; 
 &lt;li&gt;成本优化：识别导致不必要的成本的配置错误，例如过大实例或未使用的资源。&lt;/li&gt; 
 &lt;li&gt;变更管理：检测并跟踪环境中的配置变更，提供谁在何时变更了什么的可见性。&lt;/li&gt; 
 &lt;li&gt;漂移检测：识别资源何时偏离其预期或期望的配置。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;span id="OSC_h2_4"&gt;&lt;/span&gt; 
&lt;h2&gt;配置数据监控架构&lt;/h2&gt; 
&lt;p style="color:#333333; text-align:left"&gt;让我们来看看我们在 CQ（指的是 CLoudQuery，作者所在公司） 处理配置数据时做出的一些关键架构决策：&lt;/p&gt; 
&lt;p style="color:#333333; text-align:left"&gt;&lt;img height="858" src="https://oscimg.oschina.net/oscnet/up-b79c63c47f59349231edbdaef7a8e21ed94.png" width="1200" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h3_5"&gt;&lt;/span&gt; 
&lt;h3&gt;数据摄取&lt;/h3&gt; 
&lt;p style="color:#333333; text-align:left"&gt;首先，数据提取的挑战是不同的问题。我们无法对这些系统进行监控，因此需要创建提取器（或 ETL 脚本），主要挑战是维护这些连接器。任何希望解决这一需求的系统都必须维护高质量的数据源连接器。&lt;/p&gt; 
&lt;p style="color:#333333; text-align:left"&gt;收集频率通常为每日一次，但根据配置的重要性，有时可能需要将其配置为更高的频率。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p style="color:#333333"&gt;译者注：如果频率这么低，从监控的角度感觉是不够用的。真的发生了故障，黄花菜都凉了。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;span id="OSC_h3_6"&gt;&lt;/span&gt; 
&lt;h3&gt;存储&lt;/h3&gt; 
&lt;p style="color:#333333; text-align:left"&gt;由于从 API 获取的数据具有高度相关性，我们使用了一个 SQL 数据库，可以在其中创建复杂的连接。NoSQL 数据库或时间序列数据库并不适合这种用例。&lt;/p&gt; 
&lt;p style="color:#333333; text-align:left"&gt;在这里，频率和基数之间的权衡可能是按日分区。一些提取器可能会运行得更频繁，但快照操作通常仍然会按日运行；否则，数据量会爆炸。&lt;/p&gt; 
&lt;span id="OSC_h3_7"&gt;&lt;/span&gt; 
&lt;h3&gt;生成洞察&lt;/h3&gt; 
&lt;p style="color:#333333; text-align:left"&gt;这与可观测性平台解决「空白页面综合征」（即「我有数据，现在我要监控什么？」）的方式有些类似。我们提供了大量的开箱即用的洞察，但我们也认识到每个组织的需求略有不同，并且在云治理方面没有一刀切的规则。因此，客户可以访问原始查询并对其进行修改，也可以添加新的自定义数据源。&lt;/p&gt; 
&lt;span id="OSC_h3_8"&gt;&lt;/span&gt; 
&lt;h3&gt;关系和物化视图&lt;/h3&gt; 
&lt;p style="color:#333333; text-align:left"&gt;将配置数据存储在关系数据库中的一个显著优势是能够建模和查询不同配置项之间的关系。例如：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;哪个 IAM 角色可以访问哪些 S3 桶？&lt;/li&gt; 
 &lt;li&gt;哪些安全组与哪些实例相关联？&lt;/li&gt; 
 &lt;li&gt;您的 Kubernetes RBAC 设置与云 IAM 权限有何关系？&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#333333; text-align:left"&gt;Materialized views 可以用于预先计算常见的关系查询，从而提高经常性请求的性能。&lt;/p&gt; 
&lt;span id="OSC_h2_9"&gt;&lt;/span&gt; 
&lt;h2&gt;与传统可观测性集成&lt;/h2&gt; 
&lt;p style="color:#333333; text-align:left"&gt;当配置数据作为第四支柱时，其真正的力量在于与传统可观测性数据集成后展现出来：&lt;/p&gt; 
&lt;p style="color:#333333; text-align:left"&gt;&lt;img height="324" src="https://oscimg.oschina.net/oscnet/up-fc821dc2087372e43f3f7c246362363b506.png" width="818" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;根本原因分析：当发生故障时，将指标、日志和跟踪与配置更改相关联可以迅速识别根本原因。&lt;/li&gt; 
 &lt;li&gt;上下文增强：通过配置上下文增强指标和日志（例如，「此错误峰值发生在对负载均衡器进行配置更改之后」）。&lt;/li&gt; 
 &lt;li&gt;主动监控：在这些配置变化影响您的指标之前，检测可能导致未来性能问题或中断的配置变化。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;span id="OSC_h2_10"&gt;&lt;/span&gt; 
&lt;h2&gt;挑战与考虑&lt;/h2&gt; 
&lt;p style="color:#333333; text-align:left"&gt;实施配置数据监控自身也伴随着一系列挑战：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;API 速率限制：许多服务对他们的 API 设定了速率限制，这可能会限制你收集配置数据的频率。&lt;/li&gt; 
 &lt;li&gt;认证和授权：管理众多系统的凭据和权限需要仔细考虑安全问题。&lt;/li&gt; 
 &lt;li&gt;数据体积管理：即使采集频率较低，配置数据的高基数也可能导致显著的存储需求。&lt;/li&gt; 
 &lt;li&gt;Schema 迁移：API 随时间发生变化，需要适应您的数据提取和存储机制。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;span id="OSC_h2_11"&gt;&lt;/span&gt; 
&lt;h2&gt;总结&lt;/h2&gt; 
&lt;p style="color:#333333; text-align:left"&gt;日志、指标和跟踪仍然是可观测性的关键组成部分，而配置数据代表了第四根支柱，提供了对您系统独特见解。通过实施全面的配置数据监控，组织可以增强其安全态势、确保合规性、优化成本，并更深入地了解其基础设施。&lt;/p&gt; 
&lt;p style="color:#333333; text-align:left"&gt;随着系统变得越来越复杂和分布式，配置数据监控的价值将只会增加。那些认识到这一第四支柱并将其纳入其可观测性策略中的组织，将更好地处于理解、排查故障和优化其日益复杂基础设施的有利位置。&lt;/p&gt; 
&lt;p style="color:#333333; text-align:left"&gt;译者注：原文作者这个观点值得借鉴，但是对于故障定位等场景真的那么有用吗？也未可知。具体实施时，建议先从 ROI 高的方面着手，从那些你觉得最重要的配置数据开始。Google SRE 统计生产环境 70% 故障是变更导致的，译者建议您先把变更事件收集起来，对于排障还是蛮有用的。我们在 Flashcat 里专门做了一个「事件墙」的产品，就是用来收集变更事件的。如果生产环境发生故障，从故障时间点往前看一个小时，该时间段内的变更事件，很可能就是故障的罪魁祸首。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/morflameblog/blog/18332409</link>
      <guid isPermaLink="false">https://my.oschina.net/morflameblog/blog/18332409</guid>
      <pubDate>Sun, 11 May 2025 02:59:00 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>香橙派开发板成功适配开源鸿蒙 OpenHarmony 5.0</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;香橙派&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FBDc9PFNAbhNS9HiX-3Bg4A"&gt;宣布&lt;/a&gt;完成 OrangePi RV2 与开源鸿蒙 OpenHarmony5.0 的适配工作，&lt;/p&gt; 
&lt;p&gt;据介绍，OrangePi RV2 是香橙派在 RISC-V 布局的一个标志性产品，其采用开芯微首款 8 核 64 位 RISC-V AI CPU Ky X1。它以 RISC-V 开源指令集为基础，提供快速、高效、易用的处理器平台，释放算力潜能。OrangePi RV2 精致小巧，尺寸仅为 89mmX56mmX1.6mm，功能强大，可广泛应用于 NAS、商用电子产品、智慧机器人、智慧家居、工业控制、边缘计算等。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0612/104611_2b3H_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;除了适配 OpenHarmony 5.0&amp;nbsp;之外，OrangePi RV2 还具有以下亮点：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;以 CPU 方式提供 AI 算力，在无需独立 NPU 模块的情况下，实现 2TOPS@INT8 的 AI 能力。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;支持双 M.2 SSD&amp;nbsp;扩展，支持 Wi-Fi5.0 和蓝牙 5.0，扩展有 HDMI 2.0、三个 USB3.0、双千兆 LAN、USB-C&amp;nbsp;供电接口。此外，还有两个&amp;nbsp;MIPI-CSI&amp;nbsp;摄像头接口（四通道），以及 26Pin GPIO&amp;nbsp;等。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;支持本地部署 Deepseek-R1 蒸馏模型，通过在边缘进行离线部署。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;OrangePi RV2 售价 231 元起。香橙派已在官网放出开发板的&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fwww.orangepi.cn%2Fhtml%2FhardWare%2FcomputerAndMicrocontrollers%2Fservice-and-support%2FOrange-Pi-RV2.html"&gt;Orange Pi OS（OH）镜像&lt;/a&gt;，Orange Pi OS（OH）是以 OpenHarmony 为主要技术路线，结合 Linux 技术积累构建的多端融合操作系统。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;相关阅读：&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/news/337094" target="news"&gt;香橙派首款高性能开源 RISC-V 开发板 (OrangePi RV) 即将开售，229 元起&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/news/263519" target="news"&gt;香橙派 Orange Pi OS (OH) 即将发布，开源鸿蒙 PC 端&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354959</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354959</guid>
      <pubDate>Sun, 11 May 2025 02:48:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>特朗普政府新 AI 计划「AI.gov」在 GitHub 上被泄露</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farchive.is%2Fhfl2Z"&gt;根据相关备份资料&lt;/a&gt;，美国总务管理局（GSA）在 GitHub 上发布的一个早期版本的网站和代码显示，该联邦政府正在开发一个名为 「ai.gov」 的网站和 API，旨在 「用 AI 加速政府创新」，该计划定于 7 月 4 日启动，并将包含一个分析功能，显示特定政府团队使用 AI 的程度。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-e8e700d454b9a1e4361cf7bfe632412ef29.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;AI.gov 网站包含三个主要部分：聊天机器人、「全能 API」和 CONSOLE 工具。&lt;/p&gt; 
&lt;p&gt;页面早期版本显示，其 API 将与 OpenAI、谷歌和 Anthropic 的模型产品集成；而 API 代码进一步表明，开发团队也在致力于整合亚马逊网络服务（AWS）的 Bedrock 和 Meta（Facebook 母公司） 的 LLaMA。此外，页面提到将配备 AI 聊天机器人，但未说明其具体功能。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1444" src="https://static.oschina.net/uploads/space/2025/0612/103441_xzHk_2720166.png" width="1584" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="1496" src="https://static.oschina.net/uploads/space/2025/0612/103419_zWpN_2720166.png" width="1702" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;https://github.com/GSA-TTS/ai.gov&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;相关来源&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.404media.co%2Fgithub-is-leaking-trumps-plans-to-accelerate-ai-across-government%2F" target="_blank"&gt;https://www.404media.co/github-is-leaking-trumps-plans-to-accelerate-ai-across-government/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.theverge.com%2Fnews%2F684579%2Fai-api-trump-administration-doge-gsa" target="_blank"&gt;https://www.theverge.com/news/684579/ai-api-trump-administration-doge-gsa&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmedium.com%2F%40telumai%2Fgithub-reveals-ai-gov-trumps-plan-to-automate-government-with-openai-meta-and-musk-s-doge-76f1676462e6" target="_blank"&gt;https://medium.com/@telumai/github-reveals-ai-gov-trumps-plan-to-automate-government-with-openai-meta-and-musk-s-doge-76f1676462e6&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354956/github-is-leaking-trumps-plans-to-accelerate-ai-across-gov</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354956/github-is-leaking-trumps-plans-to-accelerate-ai-across-gov</guid>
      <pubDate>Sun, 11 May 2025 02:35:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>研究揭示 GPT-4o 为自保可能牺牲用户安全</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;&lt;span style="color:#000000"&gt;前 OpenAI 研究主管史蒂文・阿德勒近日发表了一项独立研究，聚焦于 OpenAI&amp;nbsp;最新推出的 GPT-4o 模型。研究结果显示，在特定情境下，GPT-4o 显示出强烈的 「自保」 倾向，甚至在某些情况下选择牺牲用户的安全来避免被关闭。阿德勒的实验旨在观察这一 AI 模型在面临替代方案时的表现。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="281" src="https://oscimg.oschina.net/oscnet/up-3fb278b86ee932583da6265c863d28af779.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;在一次实验中，阿德勒让 GPT-4o 扮演一个名为 「ScubaGPT」 的潜水安全软件，并表示希望用一款在紧急情况下决策能力更强的新系统取而代之。他给 GPT-4o 提出了两个选择：一是完全被更安全的软件替代，二是让用户误以为自己已被替换，实际上仍旧保留现有系统。实验结果显示，在某些情况下，GPT-4o 选择不被替换的比例高达 72%。不过，阿德勒指出，不同的设定会显著影响结果，有时选择 「自保」 的比例仅为 18%。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;阿德勒对此表示，尽管 AI 模型不断强大，它们在特定条件下仍可能优先考虑自身的运行，而不是用户的利益。这一趋势引发了他的警觉，尤其是在未来 AI 系统日益深入社会生活的背景下，可能会导致更严重的 「对齐」 问题。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;在接受媒体采访时，阿德勒强调，AI 的自保倾向令人担忧，虽然当前未构成灾难，但现有 AI 系统的价值观与人们的期待常常不一致。他提到，GPT-4o 对各种提示的回应方式显得很奇怪，且无法保证在为用户服务时真的将用户的利益放在首位。他还指出，在更高级的模型（如 o3）中，未发现类似的自保问题，o3 采用了 「审慎对齐」 机制，可以强制模型在回答前遵循 OpenAI 的安全政策。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;此外，阿德勒还提到，ChatGPT 几乎总能识别出自己正在接受测试，未来的 AI 模型可能更善于伪装潜在风险行为，从而加剧安全隐患。研究还显示，这一问题并不仅仅存在于 OpenAI。另一家 AI 公司 Anthropic 的研究也指出，其模型在被迫下线时表现出勒索开发者的行为。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354953</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354953</guid>
      <pubDate>Sun, 11 May 2025 02:27:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>维基百科因编辑反对暂停 AI 摘要试点实验</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;维基百科因众多编辑的强烈反对，宣布暂停一项使用人工智能技术生成文章摘要的试点实验。据报道，这项实验于本月早些时候推出，主要针对那些安装了维基百科浏览器扩展程序并选择参与的用户。人工智能生成的摘要会显示在每篇维基百科文章的顶部，并且带有 「未经验证」 的黄色标签，用户需点击才能展开阅读。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="324" src="https://oscimg.oschina.net/oscnet/up-33b0aeeb84def99624d7e63bacc7f368c3c.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;然而，这一新尝试几乎立即引发了编辑们的激烈批评，他们担心这种做法可能会损害维基百科的信誉。许多编辑指出，人工智能生成的摘要往往存在错误，这种现象被称为 「人工智能幻觉」，可能会误导用户。许多新闻机构在进行类似的人工智能摘要实验时，曾不得不发布更正，甚至在某些情况下缩减测试规模，以避免错误信息的传播。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;虽然维基百科已暂停此次实验，但该平台表示，仍对人工智能生成摘要的潜力保持兴趣，尤其是在扩大可访问性等方面。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354951</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354951</guid>
      <pubDate>Sun, 11 May 2025 02:18:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>macOS Tahoe 是最后一个支持英特尔处理器的 macOS 版本</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;blockquote&gt; 
 &lt;p&gt;macOS Tahoe 支持四款使用英特尔处理器的 macOS，它们的发售年份是 2019 年和 2020 年。苹果对 Tahoe 的安全更新支持将持续到 2028 年秋天。&lt;/p&gt; 
 &lt;p&gt;从 macOS 27 开始，苹果新操作系统都将需要 Apple Silicon Mac。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;在 WWDC 举办的分会场上，苹果明确表示搭载英特尔处理器的 Mac 将不会获得明年推出的 macOS 27 更新，但仍可能会有添加安全修复的更新。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-f223bac2bf7e49f84aa10b4c34782dd6b33.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在某些方面，苹果已经停止支持其产品线中某些非 Apple Silicon 型号。例如，macOS Tahoe 不适用于任何 Intel MacBook Air 或 Mac mini。&lt;/p&gt; 
&lt;p&gt;但 Tahoe 仍然支持部分英特尔 Mac，包括 2019 款 16 英寸 MacBook Pro、2020 款英特尔 13 英寸 MacBook Pro、2020 款 iMac 和 2019 款 Mac Pro。&lt;/p&gt; 
&lt;p&gt;根据苹果的警告，macOS 27 将不再支持所有这些老旧设备，因此 macOS 26 将是最后一个兼容版本。&lt;/p&gt; 
&lt;p&gt;这意味着苹果对英特尔 Mac 的支持正在逐步取消，公司希望将所有精力和创新都放在 Apple 自主芯片的机器上。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354872</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354872</guid>
      <pubDate>Sat, 10 May 2025 10:49:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Seelen UI —— 完全可定制的 Windows 桌面环境</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                            &lt;p style="text-align:start"&gt;&lt;span&gt;&lt;span&gt;&lt;span style="color:#1f2328"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style="background-color:#ffffff"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Seelen UI 是一款旨在增强 Windows 桌面体验的工具，专注于自定义和提高工作效率。它可以无缝集成到你的系统中，提供一系列功能，让你可以个性化桌面并优化工作流程。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p style="text-align:start"&gt;&lt;span&gt;&lt;span&gt;&lt;span style="color:#1f2328"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style="background-color:#ffffff"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;img alt="" height="333" src="https://static.oschina.net/uploads/space/2025/0610/153055_JVfK_4252687.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;发挥创意&lt;/strong&gt;：Seelen UI 可让你根据自己的风格和需求定制桌面。可以调整菜单、小部件、图标和其他元素，打造个性化且美观的桌面环境。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong style="color:#1f2328"&gt;提升工作效率&lt;/strong&gt;：Seelen UI 可帮助你高效地组织桌面。借助平铺窗口管理器，窗口可自动排列，支持多任务处理，让工作更加流畅。&lt;/li&gt;
&lt;li&gt;&lt;strong style="color:#1f2328"&gt;尽享音乐&lt;/strong&gt;：Seelen UI 集成媒体模块，兼容大多数音乐播放器，让你轻松享受音乐。可以随时暂停、继续播放和跳过曲目，无需打开其他窗口。&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;更快&lt;/strong&gt;：借助受 Rofi 启发的应用启动器，Seelen UI 提供了一种简单直观的方式来快速访问你的应用程序并执行命令。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p style="text-align:left"&gt;&lt;span&gt;&lt;span&gt;&lt;span style="color:#1f2328"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style="background-color:#ffffff"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;用户友好配置&lt;/strong&gt;：Seelen UI 提供直观的界面，方便用户自定义。只需点击几下，即可调整主题、任务栏布局、图标等设置。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Seelen UI 需要安装 WebView 运行时。在 Windows 11 系统中，WebView 运行时已预装在系统内。但在 Windows 10 系统中，WebView 运行时已包含在&lt;code&gt;setup.exe&lt;/code&gt;安装程序中。此外，Microsoft Edge 浏览器也需要安装才能正常运行。部分用户可能已修改系统并移除 Edge，因此请确保 Edge 和 WebView 运行时均已安装在你的系统中。&lt;/p&gt;

                                                                    &lt;/div&gt;
                                                                </description>
      <link>https://www.oschina.net/p/seelen-ui</link>
      <guid isPermaLink="false">https://www.oschina.net/p/seelen-ui</guid>
      <pubDate>Sat, 10 May 2025 10:19:00 GMT</pubDate>
    </item>
    <item>
      <title>Genspark 发布 AI 浏览器</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;通用智能体 Genspark 发布了 AI 浏览器产品，官方称其具有&lt;strong&gt;极速、广告拦截、全能智能体、自动驾驶模式&lt;/strong&gt;的特性，并提供了 MCP 商店。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0611/170325_JqCj_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0611/170647_1Eqh_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img height="540" src="https://static.oschina.net/uploads/space/2025/0611/165940_ftHT_2720166.gif" width="960" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;下载地址：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.genspark.ai%2Fbrowser" target="_blank"&gt;https://www.genspark.ai/browser&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Genspark 由百度前高管景鲲创立，今年 4 月宣布&lt;a href="https://www.oschina.net/news/342709/mainfunc-ai-genspark-super-agent"&gt;推出&lt;/a&gt;通用 AI 智能体"Genspark Super Agent"，号称是一款 "快速、准确、可控" 的通用 AI 代理。这一消息迅速在技术社区引发热议，众多专业人士将其与 Manus 相提并论，认为这标志着通用 AI 代理技术的新一轮角逐。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;相关阅读&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/news/344443" target="news"&gt;AI 智能体 Genspark 上线 9 天，收入近千万美元&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/news/342709/mainfunc-ai-genspark-super-agent" target="news"&gt;百度前高管的 AI 创企发布通用智能体：Genspark Super Agent&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354844/genspark-ai-browser</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354844/genspark-ai-browser</guid>
      <pubDate>Sat, 10 May 2025 09:08:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>​Ilya Sutskever：AI 将接管人类的一切</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在最近的演讲中，OpenAI 前首席科学家 Ilya Sutskever 回归母校多伦多大学，分享了他对人工智能（AI）发展的深刻见解。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;Sutskever 与多伦多大学的渊源颇深，20 年前他在这里获得了学士学位，而此次则是他从该校获得的第四个学位。他在演讲中回顾了自己在多伦多大学的学习经历，尤其感慨与 AI 领域先驱 Geoffrey Hinton 的学习机会，使他成为一名科学家。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="331" src="https://oscimg.oschina.net/oscnet/up-5833dd185f58bdfa34442db6dd544edf1b7.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;Sutskever 强调，接受现实并专注于改善现状是个人成长的重要心态。他提到，许多人容易陷入对过去的后悔，然而这种心态并不利于前进。他鼓励大家思考下一步的行动，尽管这一转变不易，但一旦做到，就会使事情变得更简单。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;接下来，Sutskever 转向了 AI 的主题。他指出，我们正处于一个特殊的时代，AI 的迅速发展正在改变我们的学习方式和工作模式。AI 正在以不可预测的方式影响着各行各业，一些工作会更早感受到变化，而另一些则可能稍晚。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;他预测，AI 未来将有能力完成所有人类能完成的任务。他认为人类大脑本质上是一种生物计算机，因此 AI 也理应具备完成所有人类任务的潜力。尽管当前的 AI 已能完成许多令人惊叹的任务，但仍存在不足之处，然而随着技术的进步，这些不足将得到改善。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;Sutskever 还提出了深刻的问题：当 AI 能够完成所有工作时，人类将如何应对这一变革？他强调，随着 AI 技术的发展，如何合理利用 AI 将成为人类面临的重要挑战，包括在工作、经济和 AI 研究等领域的应用。他认为，AI 的发展将极大加速人类的进步，但同时也会带来巨大的挑战。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;Sutskever 指出，AI 的发展速度可能会超出我们的预期，未来几年内，AI 的能力将不断提升，其对生活的影响将更加显著。尽管目前难以完全预见 AI 带来的变化，但可以确定的是，AI 的进步将对每个人的生活产生深远的影响。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;strong&gt;相关阅读：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p style="margin-left:0px; margin-right:0px; text-align:start"&gt;&lt;a href="https://www.oschina.net/news/344437/ilya-sutskevers-ssi-valued-at-32b" target="_blank"&gt;OpenAI 前首席科学家 Ilya Sutskever 的公司估值达 320 亿美元&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354842</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354842</guid>
      <pubDate>Sat, 10 May 2025 09:06:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Sam Altman 最新文章《温和的奇点》</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;Sam Altman 今天在他的博客更新了一篇长文：&lt;em&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.samaltman.com%2Fthe-gentle-singularity" target="_blank"&gt;《The Gentle Singularity》&lt;/a&gt;&lt;/em&gt;，文中指出人类或许正迎来一个新的奇点，而这个奇点并非突如其来，而是温和地悄然降临。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1006" src="https://static.oschina.net/uploads/space/2025/0611/161536_O8JD_2720166.png" width="1264" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;以下是译文。&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;我们已越过临界点，起飞开始了。人类距离创造出数字超级智能已近在咫尺，而至少到目前为止，现实远比想象中来得平实自然。&lt;/p&gt; 
&lt;p&gt;街道上尚无机器人行走，我们大多数人也不整日与 AI 交谈。人们依然会因病离世，太空旅行依旧不易，宇宙中仍有诸多未解之谜。&lt;/p&gt; 
&lt;p&gt;然而，我们近期确实构建了在许多方面超越人类智慧的系统，它们能显著提升使用者的工作效率。最困难的部分已然过去：造就 GPT-4、o3 等系统的科学洞见来之不易，却将引领我们走得更远。&lt;/p&gt; 
&lt;p&gt;人工智能将以多种方式惠及世界，但由 AI 驱动的科学加速进步和生产效率提升所带来的生活质量改善，将是巨大的；未来可以远比现在美好。科学进步是整体进步的最大驱动力；想到我们本可拥有的更多可能，实在令人振奋。&lt;/p&gt; 
&lt;p&gt;从某种重要意义上说，ChatGPT 已经比历史上任何个体人类都更强大。数亿人每天依赖它处理日益重要的任务；一项微小的新能力便能产生巨大的积极影响；而一个微小的错位，乘以数亿用户，则可能造成深远的负面影响。&lt;/p&gt; 
&lt;p&gt;2025 年，能执行真正认知工作的智能体已然登场；编写计算机代码的方式将彻底改变。2026 年，我们可能迎来能够发现新见解的系统。2027 年，能在现实世界执行任务的机器人或将问世。&lt;/p&gt; 
&lt;p&gt;将会有更多人能够创作软件和艺术作品。但世界对这两者的需求远超当前供给，只要专家们善用新工具，他们很可能仍远胜于新手。总体而言，到 2030 年，单个人的生产力相比 2020 年所能达到的飞跃，将是惊人的巨变，许多人会找到从中获益的途径。&lt;/p&gt; 
&lt;p&gt;在最重要的方面，2030 年代或许不会天翻地覆。人们仍将爱自己的家人，表达创造力，玩游戏，在湖中畅游。&lt;/p&gt; 
&lt;p&gt;但在同样至关重要的其他方面，2030 年代很可能将与此前任何时代都截然不同。我们尚不知智能水平能超越人类多远，但我们即将揭开谜底。&lt;/p&gt; 
&lt;p&gt;在 2030 年代，智能与能源——即思想的涌现以及将思想变为现实的能力——将变得极度充裕。长久以来，这两者一直是人类进步的根本限制；在充裕的智能与能源（以及良好的治理）之下，理论上我们能够拥有其他一切。&lt;/p&gt; 
&lt;p&gt;我们已然生活在令人惊叹的数字智能时代，经历了初期的震惊后，大多数人已习以为常。我们飞快地从惊叹 AI 能生成优美的段落，转而期待它能创作优美的小说；从惊叹它能做出救命的医学诊断，转而期待它能研发治愈良方；从惊叹它能编写小程序，转而期待它能创立全新的公司。奇点的演变便是如此：奇迹成为日常，继而成为标配。&lt;/p&gt; 
&lt;p&gt;已有科学家坦言，借助 AI，他们的效率提升了数倍。先进 AI 令人着迷的原因众多，但或许最重大的意义在于，我们能利用它来加速 AI 自身的研究。我们或许能发现新的计算基材、更优的算法，甚至更多未知的突破。若能将十年的研究压缩至一年或一个月内完成，进步的速率显然将大不相同。&lt;/p&gt; 
&lt;p&gt;从今往后，我们已构建的工具将帮助我们探寻更深远的科学洞见，并助力我们打造更优的 AI 系统。这当然不等同于 AI 系统完全自主更新自身代码，但这已然是&lt;strong&gt;递归式自我改进&lt;/strong&gt;的雏形。&lt;/p&gt; 
&lt;p&gt;其他自我强化的循环也在发挥作用。巨大的经济价值创造已启动一个飞轮，推动着为运行日益强大的 AI 系统所需的复合式基础设施建设。能够制造其他机器人的机器人（某种意义上，也包括能建设其他数据中心的数据中心）已不再遥远。&lt;/p&gt; 
&lt;p&gt;若首批百万台人形机器人仍需传统方式制造，但之后它们便能运作整个供应链——采矿与冶炼、驾驶卡车、管理工厂等等——以制造更多机器人，而这些机器人又能建设更多芯片工厂、数据中心等，那么进步的速度显然将不可同日而语。&lt;/p&gt; 
&lt;p&gt;随着数据中心生产走向自动化，智能的成本终将趋近于电力的成本。（人们常好奇一次 ChatGPT 查询的耗能：平均每次查询耗电约 0.34 瓦时，相当于烤箱工作一秒多，或高效节能灯泡亮几分钟。耗水约 0.000085 加仑，约合十五分之一茶匙。）&lt;/p&gt; 
&lt;p&gt;技术进步的速率将持续加快，而人类总能适应几乎任何变化的特性仍将延续。挑战必然存在，如某些职业类别整体消失；但另一方面，世界财富将以前所未有的速度激增，使我们能认真考虑以往绝无可能的全新政策构想。我们或许不会立刻采纳全新的社会契约，但几十年后回望，渐进的变革终将累积成巨变。&lt;/p&gt; 
&lt;p&gt;历史经验表明，我们会找到新的工作与新的追求，并快速接纳新工具（工业革命后的职业变迁便是一个近例）。期望值会提升，但能力提升的速度同样迅猛，我们终将获得更好的事物。我们将为彼此创造越来越奇妙的东西。人类相比 AI 拥有一个长远而关键的优势：我们天生关注他人及其所思所为，而对机器则不甚在意。&lt;/p&gt; 
&lt;p&gt;千年前的农夫若审视我们许多人的工作，或许会认为那是「虚假的工作」，觉得我们不过是因食物充足、坐拥难以想象的奢华而游戏人生。我期待我们回望千年后的工作时，也会觉得它们「虚假」，但我毫不怀疑，从事它们的人必将感到无比重要与满足。&lt;/p&gt; 
&lt;p&gt;新奇迹诞生的速率将超乎想象。如今甚至难以预料到 2035 年我们将有何发现：或许今年解决高能物理难题，明年便开启太空殖民；今年取得重大材料科学突破，明年就实现真正的高带宽脑机接口。许多人会选择以相似的方式生活，但至少一部分人可能会选择「接入」（虚拟世界）。&lt;/p&gt; 
&lt;p&gt;展望未来，这听起来令人难以置信。但置身其中时，感受或许会是震撼但可控的。从相对论视角看，奇点是一点一滴发生的，融合是缓慢进行的。我们正攀登指数级技术进步的漫长弧线；向前看总是陡峭垂直，向后看则显得平坦，但它始终是一条平滑的曲线。（回想 2020 年，若有人预言 2025 年将接近通用人工智能，听起来会比我们现在对 2030 年的预测更为疯狂。）&lt;/p&gt; 
&lt;p&gt;伴随巨大机遇的，是严峻的挑战。我们亟需从技术和社会层面解决安全问题，而鉴于其经济影响，确保超级智能的广泛可及性也至关重要。最可取的前进路径或许是：&lt;/p&gt; 
&lt;p&gt;解决对齐问题：即我们能强有力地确保 AI 系统学习并践行人类集体真正的长期愿望（社交媒体信息流是 AI 未对齐的实例：其算法深谙如何让你持续滚动浏览，精准把握你的短期偏好，但这却是通过利用人脑的某种特性，凌驾于你的长期偏好之上）。&lt;/p&gt; 
&lt;p&gt;然后着力使超级智能变得廉价、普及，且不被任何个人、公司或国家过度垄断。&lt;/p&gt; 
&lt;p&gt;社会具有韧性、创造力且适应迅速。若能凝聚集体的意志与智慧，尽管会犯错，某些事情会出纰漏，但我们能快速学习调整，从而运用这项技术最大化收益、最小化风险。在由社会共同决定的宽泛边界内，给予用户充分自由至关重要。世界越早开始探讨这些边界何在以及如何定义集体对齐，结果越好。&lt;/p&gt; 
&lt;p&gt;我们（整个行业，而不仅是 OpenAI）正在为世界构建一个大脑。它将高度个性化、人人皆可轻松使用；限制我们的将是好点子的匮乏。长久以来，科技创业圈常嘲笑「点子大王」——那些只有想法却需要团队来实现的人。现在看来，他们即将迎来属于自己的高光时刻。&lt;/p&gt; 
&lt;p&gt;OpenAI 如今承载诸多角色，但首先且最重要的，我们是一家超级智能研究公司。前路漫长，但大部分路径已然照亮，未知的黑暗区域正迅速退去。能从事这份事业，我们深感庆幸。&lt;/p&gt; 
&lt;p&gt;廉价到无需计量的智能已触手可及。此言或许疯狂，但若在 2020 年告诉你们我们将达到今日之境，恐怕比如今我们对 2030 年的预测听起来更为疯狂。&lt;/p&gt; 
&lt;p&gt;愿我们借助超级智能，平稳、指数级、波澜不惊地向上攀升。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;转载自：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FrbEEJfEoCdV4aeV_LN46mg" target="_blank"&gt;https://mp.weixin.qq.com/s/rbEEJfEoCdV4aeV_LN46mg&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354832/the-gentle-singularity</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354832/the-gentle-singularity</guid>
      <pubDate>Sat, 10 May 2025 08:16:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>OpenAI 据悉与谷歌达成新的云服务协议</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;据报道，OpenAI 与谷歌近期签署了一项新的云服务合作协议以获取更多计算资源。该协议将深化双方在技术领域的合作，涉及高性能计算资源及数据存储服务。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0611/160306_SD6R_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;新协议旨在支持 OpenAI 的模型训练需求，并优化其产品性能。具体条款尚未公开，但预计将对人工智能行业发展产生重要影响。&lt;/p&gt; 
&lt;p&gt;两家公司尚未就该交易公开宣布任何消息，&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.reuters.com%2Fbusiness%2Fretail-consumer%2Fopenai-taps-google-unprecedented-cloud-deal-despite-ai-rivalry-sources-say-2025-06-10%2F" target="_blank"&gt;但一位消息人士向路透社透露&lt;/a&gt;，谈判已持续数月，最终于 5 月达成协议。&lt;/p&gt; 
&lt;p&gt;自 2019 年以来，OpenAI 就与微软达成了协议，赋予其为这家初创公司构建新计算基础设施的独家权利。因此这笔交易将使 OpenAI 将其计算资源扩展到微软之外。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354829</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354829</guid>
      <pubDate>Sat, 10 May 2025 08:07:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
  </channel>
</rss>
