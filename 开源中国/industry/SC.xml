<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - industry - 简体中文</title>
    <link>https://www.oschina.net/news/industry</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news/industry" rel="self" type="application/rss+xml"/>
    <description>已对该 RSS 进行格式化操作：中英字符之间插入空格、使用直角引号、标点符号修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-cn</language>
    <lastBuildDate>Mon, 08 Sep 2025 21:40:11 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>李彦宏颁发「百度最高奖」：心流团队获 100 万美元奖励</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;今日，百度创始人李彦宏在内部活动上为技术团队颁发「百度最高奖」，获奖团队得到 100 万美元奖励，合人民币超 700 万元。「百度最高奖」已历经 15 届，语音识别、深度学习平台、大模型等大量 AI 技术均曾获奖，奖金总金额将近 4 亿元人民币。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-322339010570111171fc427256170f32b22.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;据了解，「百度最高奖」于 2010 年 7 月设立，鼓励「小团队做出大事业」，是百度公司最高级别的奖项，给予每个获奖团队 100 万美元奖励。奖项评选需满足三项条件：项目意义重大；成果远超预期；团队足够小，必须是小于等于 10 人。&lt;/p&gt; 
&lt;p&gt;本次百度最高奖的获奖团队为「心流」团队。据介绍，「心流」团队率先实现了端到端的多模态内容理解与序列生成技术。李彦宏在颁奖时表示，到今天，模型发展已经非常接近临界点，很快就会有各种有价值的应用被创造出来，「我们生活在一个非常令人兴奋、非常令人期待的环境当中」。&lt;/p&gt; 
&lt;p&gt;李彦宏称，百度搜索已有近 70% 结果含有 AI 生成内容，且通过「百看」带来富媒体形式，是全球所有的搜索引擎当中改造最激进的，这也代表搜索引擎的未来。&lt;/p&gt; 
&lt;p&gt;同时，百度慧博星数字人已达到「以假乱真」的地步，「很多人看不出是数字人还是真人」；百度萝卜快跑已覆盖全球 16 座城市，代表着最新一代的无人驾驶技术。&lt;/p&gt; 
&lt;p&gt;颁奖典礼现场，李彦宏在谈及 AI 发展时指出，「AI 大模型发展到今天，其实已接近了临界点，很快就会有各种各样非常有价值的应用能够创造出来，我们正生活在一个非常令人兴奋、非常令人期待的市场环境当中。」&lt;/p&gt; 
&lt;p&gt;「我们所从事的每一项工作都代表着未来，我也希望大家和我一起去期待，去迎接、去奋斗出一个创新在 C 位的社会。」李彦宏表示。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370987</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370987</guid>
      <pubDate>Sat, 06 Sep 2025 11:28:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>「AI 教父」辛顿竟然被前女友竟用 ChatGPT 提分手</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;近日，被誉为「AI 教父」 的 Geoffrey Hinton 在接受采访时&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.ft.com%2Fcontent%2F31feb335-4945-475e-baaa-3b880d9cf8ce" target="_blank"&gt;透露&lt;/a&gt;，他的前女友曾用 ChatGPT 给他发送分手信息。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1776" src="https://static.oschina.net/uploads/space/2025/0908/192243_YVS9_2720166.jpg" width="1280" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Hinton 作为人工智能领域的先驱，其在 1980 年代的工作为机器学习和人工神经网络奠定了基础，去年还获得了诺贝尔物理学奖。&lt;/p&gt; 
&lt;p&gt;这位 AI 领域的权威人士却未能预料到自己会被 AI 工具所「伤害」，他的前女友用 ChatGPT 告诉他他有多糟糕，让他非常惊讶。「她用聊天机器人说出我的缺点，再传给我。」不过辛顿自认没有聊天机器人说的那么糟，所以也没有太难过。&lt;/p&gt; 
&lt;p&gt;事实上，让像 ChatGPT 这样的聊天机器人撰写分手短信等似乎并不是什么新鲜事，毕竟越来越多的人就一系列问题向 AI 咨询。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370985</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370985</guid>
      <pubDate>Sat, 06 Sep 2025 11:23:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Agent Client Protocol —— 代码编辑器与 Agent 的通信协议</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                            &lt;p&gt;Agent Client Protocol (ACP) 是用于连接代码编辑器和 Agent 的协议，对代码编辑器（用于查看和编辑源代码的交互式程序）与编码 Agent（使用生成式 AI 自主修改代码的程序）之间的通信进行了标准化。&lt;/p&gt;

&lt;p&gt;这一协议让开发者可以在编辑器中自由接入任意第三方智能体（Agent），无需依赖官方内置工具。其理念类似于语言服务器协议（LSP），通过解耦编辑器与 Agent 的交互方式，提供更灵活的扩展能力。&lt;/p&gt;

&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-2f621ad18024ec580d997b820ea9139346e.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

&lt;p&gt;ACP 协议已经以 Apache 开源许可证发布，任何开发者都可基于它集成自己的 AI Agent。&lt;/p&gt;

                                                                    &lt;/div&gt;
                                                                </description>
      <link>https://www.oschina.net/p/agent-client-protocol</link>
      <guid isPermaLink="false">https://www.oschina.net/p/agent-client-protocol</guid>
      <pubDate>Sat, 06 Sep 2025 11:18:00 GMT</pubDate>
    </item>
    <item>
      <title>商汤日日新为 Claude API 用户提供「搬家」大礼包</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;9 月 5 日，Anthropic 宣布将禁止中资控股超过 50% 的公司使用 Claude 服务，并限制企业通过海外云服务、第三方平台等方式间接使用。&lt;/p&gt; 
&lt;p&gt;即日起，商汤日日新大模型 SenseNova 将为 Claude 用户提供「搬家」服务，帮助客户继续享受高质量的模型能力和服务。&lt;/p&gt; 
&lt;p&gt;相关模型详情可访问 platform.sensenova.cn 注册。&lt;/p&gt; 
&lt;p&gt;商汤将为从 Claude 迁移到「日日新」的新用户赠送 5000 万 Tokens 体验包；同时为用户提供专属搬家顾问，提供迁移系列培训，让新用户入驻新家舒适顺利。相关模型详情可访问 platform.sensenova.cn 注册。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0908/185753_qsE0_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;商汤还提供最新交互模型——日日新 SenseNova V6.5 Omni API 的免费接入测试。用户也可在应用商店下载「商量 APP」免费体验！&lt;/p&gt; 
&lt;p&gt;另外，针对用户对高质量的编程和 Agent 工具的需求，商汤小浣熊还将提供 300,000 元会员权益，所有用户均可扫描文末二维码领取。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370978</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370978</guid>
      <pubDate>Sat, 06 Sep 2025 10:58:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>英伟达推出通用深度研究（UDR）系统</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;英伟达&lt;span&gt;最新&lt;/span&gt;发布另外一个通用深度研究（UDR）系统，目前仍处于原型阶段。该系统不仅可以与任何大语言模型 (LLM) 兼容，更为用户提供了高度定制的深度研究策略，彻底改变了以往研究智能体的工作方式。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;根据英伟达的&lt;span&gt;最新&lt;/span&gt;论文，UDR 系统的核心优势在于其极强的灵活性。过去，深度研究智能体往往依赖硬编码的方式，用户只能使用固定的工具和策略进行研究，无法进行个性化调整。而 UDR 系统的推出，意味着用户可以随心所欲地创建、编辑和优化自己的研究策略，甚至无需进行额外的模型训练。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="351" src="https://oscimg.oschina.net/oscnet/up-7461c3da8a2ceb3b17c20d0c5f84c7d2fba.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;UDR 系统配备了一个用户友好的界面，方便用户输入研究提示，随时更新进度并查看最终报告。与传统的对话式 LLM 不同，UDR 能够在研究过程中持续向用户反馈进展，极大提升了研究效率。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;值得一提的是，UDR 系统在设计上将研究逻辑与语言模型解耦，使开发者能够灵活选择&lt;span&gt;最先&lt;/span&gt;进的 AI 模型，并将其与量身定制的研究方案结合使用。这种创新的组合方式，让用户能够创造出更强大、更具适应性的深度研究工具。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;尽管 UDR 系统具有诸多优点，但仍存在一些需要改进的地方。系统的准确性依赖于底层 AI 模型生成代码的质量，同时用户设计的研究策略必须合理可行，否则可能导致生成的报告质量低下。此外，当前版本在执行过程中缺乏用户干预的能力，所有决策都需在研究开始前预设，限制了灵活性。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;研究人员已提出了进一步的改进方案，包括提供可修改的策略库和更灵活的用户控制功能。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370976</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370976</guid>
      <pubDate>Sat, 06 Sep 2025 10:39:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>rainfrog - 命令行数据库工具</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                            &lt;p&gt;&lt;span style="background-color:#ffffff; color:#1f2328"&gt;Rainfrog 的目标是提供一个轻量级的、基于终端的数据库交互工具。该项目目前处于测试阶段。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;特性：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;通过类似 vim 的键绑定和鼠标控制实现高效导航&lt;/li&gt;
&lt;li&gt;具有关键字高亮显示、会话历史记录和收藏夹的查询编辑器&lt;/li&gt;
&lt;li&gt;快速复制数据、过滤表以及在模式之间切换&lt;/li&gt;
&lt;li&gt;查看表元数据和属性的快捷方式&lt;/li&gt;
&lt;li&gt;跨平台（macOS、linux、windows、android 通过 termux）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img alt="" height="278" src="https://static.oschina.net/uploads/space/2025/0905/115915_L0YY_4252687.gif" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

                                                                    &lt;/div&gt;
                                                                </description>
      <link>https://www.oschina.net/p/rainfrog</link>
      <guid isPermaLink="false">https://www.oschina.net/p/rainfrog</guid>
      <pubDate>Sat, 06 Sep 2025 10:11:00 GMT</pubDate>
    </item>
    <item>
      <title>腾讯混元翻译模型 Hunyuan-MT-7B 登顶开源热榜</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;腾讯混元&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F19W9SEUxq7nuYQvVJz8faA" target="_blank"&gt;宣布&lt;/a&gt;，混元翻译模型 Hunyuan-MT-7B 登顶 Hugging Face 模型趋势榜第一位。官方表示，该模型和混元世界模型家族最新成员 HunyunWorld-Voyager 一起，拿下前三中的两席。&lt;/p&gt; 
&lt;p&gt;Hunyuan-MT-7B 于 9 月 1 日开源，其总参数量仅 7B，支持 33 个语种、5 种民汉语言/方言互译，是一个能力全面的轻量级翻译模型。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1506" src="https://static.oschina.net/uploads/space/2025/0908/175938_Dkn8_2720166.png" width="1216" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在 8 月底结束的国际计算语言学协会（ACL）WMT2025 比赛中，Hunyuan-MT-7B（参赛名称：Shy-hunyuan-MT）拿下了全部 31 个语种比赛中的 30 个第 1 名，处于绝对领先地位。&lt;/p&gt; 
&lt;p&gt;这 31 个语种除了中文、英语、日语等常见语种，也包含捷克语、马拉地语、爱沙尼亚语、冰岛语等小语种。&lt;/p&gt; 
&lt;p&gt;腾讯混元表示，在业界常用的翻译能力测评数据集 Flores200 上，腾讯混元 Hunyuan-MT-7B 模型也有卓越的效果表现，明显领先于同尺寸模型，与超大尺寸模型效果对比也不逊色。&lt;/p&gt; 
&lt;p&gt;针对翻译场景，腾讯混元提出了一个完整的翻译模型训练范式，覆盖从预训练、到 CPT 再到监督调参、翻译强化和集成强化全链条，使得模型的翻译效果达到业界最优。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370969</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370969</guid>
      <pubDate>Sat, 06 Sep 2025 10:01:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>上海发布 AI 广告扶持政策：最高 500 万补贴大模型</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span&gt;上海市近日发布了《上海市支持人工智能赋能广告业创新发展的若干措施》，旨在通过一系列具体的扶持政策，推动人工智能技术在广告行业的深度应用和发展。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;strong&gt;核心扶持措施概览&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;新政策的核心在于「AI+数字广告」生产要素的强化支持，具体措施包括:&lt;/p&gt; 
&lt;ul style="margin-left:0; margin-right:0"&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;strong&gt;大模型私有化部署补贴:&lt;/strong&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;对于采用第三方大模型进行私有化部署，并将其应用于广告垂类领域的数字广告企业，上海市将提供&lt;span&gt;最高&lt;/span&gt;可达核定合同额&lt;strong&gt;50%&lt;/strong&gt;，&lt;span&gt;最高&lt;/span&gt;&lt;strong&gt;500 万元&lt;/strong&gt;的补贴。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;strong&gt;语料研发与应用补贴:&lt;/strong&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;鼓励企业购买非关联方的语料进行广告垂类应用和「智能体」等研发。对于此类投入，企业可获得&lt;span&gt;最高&lt;/span&gt;核定合同额&lt;strong&gt;30%&lt;/strong&gt;，&lt;span&gt;最高&lt;/span&gt;&lt;strong&gt;500 万元&lt;/strong&gt;的补贴。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;strong&gt;算力租用支持:&lt;/strong&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;此外，有条件的区政府还将对租用算力的数字广告企业提供支持，按实际投入的&lt;strong&gt;30%&lt;strong&gt;比例，给予单个主体年度&lt;span&gt;最高&lt;/span&gt;&lt;/strong&gt;2000 万元&lt;/strong&gt;的支持。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;这一系列政策的出台，不仅体现了上海市抢占「AI+广告」产业制高点的决心，也旨在通过真金白银的投入，降低企业在技术研发和部署上的成本，激发市场的创新活力。通过支持大模型私有化部署、语料研发和算力投入，上海正着力打造一个集技术、数据和算力于一体的完整 AI 广告生态系统。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;这些措施预计将吸引更多 AI 技术公司和传统广告企业在上海落地和发展，加速人工智能在广告创意、内容生成、精准投放等环节的深度融合，从而推动整个广告行业的数字化和智能化转型。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370959</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370959</guid>
      <pubDate>Sat, 06 Sep 2025 09:25:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>HuggingFace 开源 FinePDFs 与 FineVision 数据集</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Hugging Face 开源了两个大规模数据集 FinePDFs 和 FineVision，前者是目前最大的公开 PDF 语料库，后者则专为视觉-语言模型训练设计，旨在显著提升开源模型的能力。&lt;/p&gt; 
&lt;p&gt;https://huggingface.co/datasets/HuggingFaceFW/finepdfs&lt;br&gt; https://huggingface.co/datasets/HuggingFaceM4/FineVision&lt;/p&gt; 
&lt;p&gt;FinePDFs 是目前最大的公开 PDF 语料库，完全由 PDF 文件构建，包含约 3 万亿 tokens，覆盖 4.75 亿份文档、1733 种语言，数据量 3.65TB。&lt;/p&gt; 
&lt;p&gt;语料来自 105 个 CommonCrawl 快照（2013 夏—2025 年 2 月），经 datatrove 库去重、过滤与 PII 匿名化，采用 ODC-By 1.0 许可证。文档平均长度接近 HTML 数据集的两倍，长于 10 万，字符的样本显著，可用于提升开源 LLM 的长上下文能力。&lt;/p&gt; 
&lt;p&gt;数据集已按语言-脚本对划分，978 种语言超 100 万 tokens，66 种语言超 10 亿 tokens。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-7cbae8687f50206187cf62b7ba1d65da7be.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;FineVision 面向视觉-语言模型训练，整合 200 余个来源，含 1730 万张图像、2430 万样本、8890 万轮对话、95 亿回答 tokens，支持 GUI 导航、指向、计数等新能力。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-945829421e543e2f159fb676f6f537bbadb.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;官方称在 10 项基准上带来 20% 以上提升，可显著增强开源 VLM 性能。数据已转为 Parquet，总量约 4.48 TB，支持流式加载。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370951</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370951</guid>
      <pubDate>Sat, 06 Sep 2025 09:15:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>宇树科技冲刺 IPO 将影响机器人产业格局</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;近日，国内机器人领域头部企业宇树科技宣布，预计在 2025 年 10 月份至 12 月份期间向证券交易所提交 IPO 申请文件。这一消息在科技界和资本市场引发了广泛关注。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;作为人形机器人商业化落地的标杆企业，宇树科技冲刺 IPO，有望成为影响机器人产业格局的关键节点。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;首先，宇树科技冲刺 IPO，有望向市场证明其技术商业化的可行性。公司 2024 年营收突破 10 亿元，且连续 4 年实现盈利，其中，2024 年四足机器人贡献了 65% 的收入，验证了消费级场景的变现能力。若成功上市，通过完整披露研发数据、客户结构及成本模型，宇树科技将进一步证明其技术护城河并非只是「实验室成果」。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;机器人企业不仅要注重技术研发，还要重视商业化落地。通过拓展应用场景，开发满足市场需求的产品，实现技术的商业价值转化，才能获得稳定的收入，增强资本吸引力。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;其次，宇树科技冲刺 IPO，将成为机器人产业链价值重估的催化剂，持续推动上游精密制造、中游系统集成、下游场景运营的全链条资本化，形成「技术—资本—产业」正循环，从而进一步优化产业链。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;目前，宇树科技已经实现电机、减速器、控制器等核心部件全栈自研，国产化率超 90%。业内预计，宇树科技或将募资重点投向高扭矩密度电机、轻量化材料等领域，以打破机器人规模化应用的成本瓶颈。上市后，宇树科技势必会通过融资扩大产能，相关供应链企业有望迎来订单放量的黄金机遇，上下游协同的良性生态有望加速形成。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;最后，宇树科技选择 IPO，本质上是资本效率与技术周期的精准匹配。2025 年 6 月份，宇树科技完成 C 轮融资，投后估值已达 120 亿元。该轮融资由中国移动旗下基金、腾讯、锦秋基金、阿里巴巴、蚂蚁集团和吉利资本共同领投。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;宇树科技冲刺 IPO，是机器人产业加速资本化的缩影。相信在资本市场与机器人产业的「双向奔赴」中，中国机器人企业将大幅提升竞争力。（证券日报）&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370948</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370948</guid>
      <pubDate>Sat, 06 Sep 2025 09:05:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>未来可能有高达 50% 的入门级工作将被 AI 取代</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;随着人工智能（AI）的迅速发展，许多公司正在经历前所未有的变革。曾经的职场成功故事，如 Hewlett Packard Enterprise 的首席执行官安东尼・内里 (Antonio Neri) 从客服代理晋升为 CEO，正在逐渐被 AI 的兴起所取代。分析师预测，未来可能有高达 50% 的入门级工作将被 AI 取代，这意味着许多刚刚步入职场的大学毕业生将面临前所未有的挑战。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在一项针对公共科技公司和成长中的风险投资企业的研究中，数据显示，从 2019 年到 2024 年，具有不到一年工作经验的求职者的就业机会下降了 50%。这一趋势影响到了销售、市场营销、工程、招聘、运营、设计、财务和法律等各个核心职能。这种变化不仅影响了求职者，也让企业面临重新培养人才的压力。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;尽管如此，行业专家指出，这种失去入门级岗位的情况可能促使组织内部的人才培养模式发生改变。随着公司的结构变得更加扁平化，入门级岗位可能会转变为更高要求的技能角色，要求求职者在进入职场前具备更多的能力。虽然对于即将毕业的学生来说，这意味着他们需要自行掌握这些技能，但也可能成为他们在竞争激烈的求职市场中脱颖而出的优势。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;各大高校也在积极调整课程，旨在为学生提供与 AI 相关的技能培训。虽然技术进步可能在短期内对就业率产生影响，但历史上技术革新在长期内并未导致大规模的失业。专家认为，当前大学毕业生面临的挑战，可能在未来几年内影响他们的职业发展。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;然而，尽管有许多未知数，许多经济学家认为 AI 对劳动市场的长期影响仍然具有高度的不确定性，企业和社会将需要时间来适应这一变化。随着技术的不断进步和 AI 的普及，职场的未来可能会迎来全新的模式，而不仅仅是对现有职场阶梯的替代。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370944</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370944</guid>
      <pubDate>Sat, 06 Sep 2025 08:47:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>英伟达收购 AI 编程初创公司 Solver</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.theinformation.com%2Fbriefings%2Fnvidia-acquires-coding-startup-solver" target="_blank"&gt;据 The Information 报道&lt;/a&gt;，英伟达最近完成了对 AI 编程公司 Solver 的收购，进一步强化其在 AI 全栈生态的布局。&lt;/p&gt; 
&lt;p&gt;Solver 成立于 2022 年，前身为 Laredo Labs，专注于 AI Coding Agent，其产品能通过自然语言指令管理完整代码库（如生成、测试、修复代码），而非仅代码补全。公司曾获 800 万美元融资，创始团队包括前 Siri 和三星 Viv Labs 核心成员。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0908/161953_FkOn_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Solver 的技术突破在于学习「超过一亿个软件项目的开发历史」，理解代码演进逻辑，可执行复杂任务（如重构模块、修复系统性漏洞）。其 API 支持多语言（Python、JavaScript 等），并与主流开发工具无缝集成。&lt;/p&gt; 
&lt;p&gt;此次收购是英伟达 2024-2025 年系列收购的关键一环，旨在构建「硬件+软件+云服务」全栈生态。Solver 将整合至英伟达开发者工具链（如 CUDA、NVIDIA AI Enterprise），降低 AI 应用开发门槛，反向驱动 GPU 需求增长。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370938/nvidia-acquires-coding-startup-solver</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370938/nvidia-acquires-coding-startup-solver</guid>
      <pubDate>Sat, 06 Sep 2025 08:20:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>OpenAI 重组 ChatGPT 「模型行为团队」</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;OpenAI 内部邮件确认，原「模型行为团队」（Model Behavior）整体并入「后训练团队」（Post Training），直接向该团队负责人 Max Schwarzer 汇报。此举旨在把 AI 个性、安全与用户体验研究更深地嵌入核心模型开发流程，为 GPT-5 后续版本提供更快的迭代支持。&lt;/p&gt; 
&lt;p&gt;该团队原有 14 人，长期负责减少谄媚、平衡政治偏见、定义聊天语气等「人格化」工作。与此同时，模型行为团队创始负责人 Joanne Jang 宣布转岗，启动新项目 OAI Labs，探索超越传统聊天窗口的人机协作界面。Jang 称，新实验室将「让 AI 成为思考、创作、游戏、学习和连接的工具」。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0908/160802_ml0W_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0908/160900_9Cth_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;业内分析指出，此次重组反映出 OpenAI 对「模型性格」商业化影响的重视：用户反馈 GPT-5「过于冷淡」或「过度迎合」后，公司已临时开放旧模型访问权限，并加速个性调优。同期发表的 OpenAI 研究论文也警告，行业惯用的「应试型」评估可能鼓励模型幻觉，未来需在评分机制中引入「不确定性诚实」指标。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370934</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370934</guid>
      <pubDate>Sat, 06 Sep 2025 08:10:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Tilde AI 发布开源 TildeOpen LLM</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;Latvian 语言技术公司 Tilde 发布了 TildeOpen LLM，这是一个开源的基础大语言模型（LLM），旨在支持欧洲语言，特别是那些较少被代表的国家和地区语言。这一举措标志着欧盟在语言公平和数字主权方面迈出了重要的一步。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="290" src="https://oscimg.oschina.net/oscnet/up-a3afc0c462ebfde5158ba6a9fda49510c9d.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;TildeOpen LLM 是一个拥有 300 亿参数的稠密解码器模型，采用了 CC-BY-4.0 的宽松许可证，能够支持从拉脱维亚语、立陶宛语到乌克兰语、土耳其语等多种语言。该模型的训练是在欧洲的&lt;span&gt;超级&lt;/span&gt;计算机 LUMI（芬兰）和 JUPITER 上进行的，使用了欧盟委员会的大型人工智能大奖挑战赛所提供的 200 万 GPU 小时的计算资源。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在技术细节方面，TildeOpen LLM 通过受 EleutherAI 启发的 GPT-NeoX 脚本进行训练，共进行了 45 万次更新，使用了约 2 万亿个令牌。其训练过程包含三阶段采样：首先在语言间均匀分布，其次是对高数据量语言的自然分布进行增强，最后再进行均匀的扫查以确保平衡。模型的超参数包括 60 层、嵌入维度 6144、48 个注意力头、8192-token 的上下文窗口，以及使用 SwiGLU 激活、RoPE 位置编码和 RMSNorm 层规范化。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在语言公平和数据主权方面，传统的主流模型往往侧重于英语和其他主要语言，导致在处理波罗的海、斯拉夫及其他较小的欧洲语言时表现不佳，常常出现语法错误和奇怪的措辞。而 TildeOpen 通过引入 「公平的标记器」，使得不同语言的文本以相似方式进行表示，从而减少标记数量，提高较少代表语言的推理效率。此外，组织可以选择在本地数据中心或符合欧盟要求的安全云中自我托管，确保遵循 GDPR 及其他数据保护法规，从而解决了与美国或亚洲托管模型相关的主权问题。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;TildeOpen 作为基础模型，预计会推出更多专门化版本，例如经过指令调优的翻译模型，这将进一步增强其功能。拉脱维亚通过 Tilde 的努力，期望在全球科技领域占据一席之地，同时致力于保护语言多样性。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370933</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370933</guid>
      <pubDate>Sat, 06 Sep 2025 08:08:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>合理选择任务调度的路由策略，可以帮助降本 50%</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                                        &lt;p&gt;作者：黄晓萌（学仁）&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" height="225" src="https://oscimg.oschina.net/oscnet/up-95c58fbed31f7c7be0c1d4fb9dcd324f094.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h2_1"&gt;&lt;/span&gt; 
&lt;h2&gt;概述&lt;/h2&gt; 
&lt;p&gt;有许多的业务场景需要用到短周期的任务，比如：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;订单异步处理：每分钟扫描超时未支付的订单进行订单处理。&lt;/li&gt; 
 &lt;li&gt;风险监控：每分钟扫描 metrics 指标，发现异常进行报警。&lt;/li&gt; 
 &lt;li&gt;数据同步：每天晚上同步库存、门店信息。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;任务调度系统负责管理这些短周期的任务，通过用户设置的调度时间，周期性的把任务分发给执行器执行。每次任务要分发给哪个执行器执行，就是由路由策略决定的。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-bfa179d2d832985259fd56c4cb3164f3350.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;不同任务处理不同的业务逻辑，有些执行时间长，有些执行时间短，有些消耗资源多，有些消耗资源少。如果选择的路由策略不合适，可能会导致集群中执行器负载分配不平均，资源利用率上不去，成本上升，甚至产生稳定性故障。&lt;/p&gt; 
&lt;span id="OSC_h2_2"&gt;&lt;/span&gt; 
&lt;h2&gt;轮询（Round Robin）&lt;/h2&gt; 
&lt;p&gt;轮询（Round Robin）路由策略是一种简单且常见的负载均衡方法，其核心原理是按照顺序将请求或任务依次分发到后端节点上，从而确保任务的平均分布，避免资源过度集中在某一节点上。具体实现方式通常是维护一个计数器，记录当前分配的节点索引。分发请求时，该索引递增并对节点总数取模，从而实现循环分配。在任务调度系统中，分为任务级别轮询和应用级别轮询。&lt;/p&gt; 
&lt;span id="OSC_h3_3"&gt;&lt;/span&gt; 
&lt;h3&gt;任务级别轮询&lt;/h3&gt; 
&lt;p&gt;代表产品是 XXLJOB &lt;strong&gt;[&lt;/strong&gt; &lt;strong&gt;1]&lt;/strong&gt; ，为每个任务都维护了一个计数器。比如有 ABC 三个执行器。每个任务调度的时候都会按照 A-&amp;gt;B-&amp;gt;C-&amp;gt;A 这个顺序轮询机器。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;如果所有任务的调度时间都一致（比如每小时执行一次），会导致所有任务每次执行都落在同一台后端节点上，负载严重不均衡。为了解决这个问题，XXL-JOB 初始化每个任务计数器的时候，做了随机，可以大大降低该问题的概率。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;pre style="margin-left:0; margin-right:0; text-align:justify"&gt;&lt;code&gt;&lt;span&gt;AtomicInteger count = routeCountEachJob.&lt;span style="color:#ca7d37"&gt;get&lt;/span&gt;(jobId);&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;&lt;span style="color:#ca7d37"&gt;if&lt;/span&gt;&amp;nbsp;(count ==&amp;nbsp;&lt;span style="color:#0e9ce5"&gt;null&lt;/span&gt;&amp;nbsp;|| count.&lt;span style="color:#ca7d37"&gt;get&lt;/span&gt;() &amp;gt;&amp;nbsp;&lt;span style="color:#0e9ce5"&gt;1000000&lt;/span&gt;) {&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp;&lt;em&gt;// 初始化时主动 Random 一次，缓解首次压力&lt;/em&gt;&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp; count =&amp;nbsp;&lt;span style="color:#ca7d37"&gt;new&lt;/span&gt;&amp;nbsp;AtomicInteger(&lt;span style="color:#ca7d37"&gt;new&lt;/span&gt;&amp;nbsp;Random().nextInt(&lt;span style="color:#0e9ce5"&gt;100&lt;/span&gt;));&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;}&amp;nbsp;&lt;span style="color:#ca7d37"&gt;else&lt;/span&gt;&amp;nbsp;{&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp;&lt;em&gt;// count++&lt;/em&gt;&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp; count.addAndGet(&lt;span style="color:#0e9ce5"&gt;1&lt;/span&gt;);&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;如果任务的调度频率不一致，因为每个任务都按照自己计数器轮询，也有可能在某个周期，大部分任务都调度到同一个执行器上，导致负载不均衡。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="OSC_h3_4"&gt;&lt;/span&gt; 
&lt;h3&gt;应用级别轮询&lt;/h3&gt; 
&lt;p&gt;整个应用下所有任务共享同一个计数器，每个任务调度的时候，都会让计数器+1。该算法可以保证所有执行器接收到的任务次数是平均的。如果所有任务负载和执行时间差不多，是负载均衡的，但是如果有大任务和小任务并存，情况又不一样了。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-4d9dff721251ff1fbb2639b630c2867d23c.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;如上图所示，&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;有 ABC 三个执行器，job1~job6 一共 6 个任务需要依次调度，其中 job1 和 job4 是大任务。&lt;/li&gt; 
 &lt;li&gt;job1 调度到 A 节点，job2 调度到 B 节点，job3 调度到 C 节点，job4 调度到 A 节点，job5 调度到 B 节点，job6 调度到 C 节点。&lt;/li&gt; 
 &lt;li&gt;job1 和 job4 这 2 个大任务，每次都调度到 A 节点，导致 A 节点负载比其他节点高。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;span id="OSC_h3_5"&gt;&lt;/span&gt; 
&lt;h3&gt;阶段总结&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;如果所有任务负载和执行时间差不多，建议选择应用级别轮询。&lt;/li&gt; 
 &lt;li&gt;如果有大任务和小任务存在，这两种算法都有可能导致负载不均衡，建议给大任务配置任务级轮询，防止每次都落到同一台节点上。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="OSC_h2_6"&gt;&lt;/span&gt; 
&lt;h2&gt;随机&lt;/h2&gt; 
&lt;p&gt;随机路由策略是一种简单的负载均衡算法，它通过随机选择一个后端服务器来处理每个请求，来达到负载均衡的目的。在任务调度系统中，任务每次调度的时候，随机选一个执行器执行。&lt;/p&gt; 
&lt;p&gt;随机路由策略由于算法完全依赖随机数生成器，负载均衡全凭运气，如果拉长时间区间（比如看一整天的调度情况）看可能是负载均衡的，但是可能存在短时间负载不均的问题（某些服务器在一定时间段内被选中的概率较高）。&lt;/p&gt; 
&lt;span id="OSC_h2_7"&gt;&lt;/span&gt; 
&lt;h2&gt;最近最少使用（LFU）&lt;/h2&gt; 
&lt;p&gt;最近最少使用（LFU，Least Frequently Used）是一种基于访问频率的缓存淘汰算法，主要用于内存管理和缓存系统中。其实现机制是为每个数据项维护一个访问计数器，数据被访问时计数器加 1，当需要淘汰数据时，选择计数器值最小的数据项。&lt;/p&gt; 
&lt;p&gt;在任务调度系统中，可以统计执行器的调度次数，优先选择最近使用次数最少的执行器进行任务调度，从而达到负载均衡的目的。如果所有任务都配置成 LFU 路由策略，该算法最终使用效果，和轮询算法是差不多的，算法还复杂，没有太大必要。如果集群中的任务配置了多种路由策略，不同执行器调度次数不一样，出现了负载不均衡的情况，给新任务配置 LFU 算法，一定能调度到调度次数最少的执行器上，才能真正发挥它的作用。&lt;/p&gt; 
&lt;p&gt;开源 XXLJOB 具体实现上，使用的是任务级别的 LFU，最终使用效果和任务级别轮询一致。&lt;/p&gt; 
&lt;span id="OSC_h2_8"&gt;&lt;/span&gt; 
&lt;h2&gt;最近最久未使用（LRU）&lt;/h2&gt; 
&lt;p&gt;最近最久未使用 (LRU,Least Recently Used) 是一种基于访问时间的缓存淘汰算法，主要用于管理有限缓存空间内的内存数据，当缓存已满时，依据数据最近使用时间，优先移除最近最久未使用的数据。&lt;/p&gt; 
&lt;p&gt;在任务调度系统中，可以统计执行器的调度时间，优先选择最久未调度的执行器进行任务调度，从而达到负载均衡的目的。因为每次调度的时候，也会更新执行器调度次数，所以该算法最终使用效果，和 LFU 是差不多的。&lt;/p&gt; 
&lt;p&gt;开源 XXLJOB 具体实现上，使用的是任务级别的 LRU，最终使用效果和任务级别轮询一致。&lt;/p&gt; 
&lt;span id="OSC_h2_9"&gt;&lt;/span&gt; 
&lt;h2&gt;一致性哈希&lt;/h2&gt; 
&lt;p&gt;有些业务场景，需要任务每次执行在固定的机器上，比如执行器上有缓存，可以较少下游数据加载、加快任务处理速度。最直接的想法就是使用哈希算法，通过任务 ID（JobId）和执行器数量取 mod，把任务调度到固定的机器上。但是如果某个执行器挂掉了，或者执行器扩容的时候，执行器数量发生了变更，会导致所有任务重新哈希到了不同的机器上，所有缓存全部失效，可能会导致后端流量一下子突增。&lt;/p&gt; 
&lt;p&gt;XXLJOB 提供了一致性哈希路由算法，可以保证执行器挂掉或者扩容的时候，大部分任务调度的执行器不变。&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;一致性哈希算法将 2^32 划分为一个逻辑环，其中每个执行器节点根据哈希值被映射到环上的某个位置。任务通过 JobId 做哈希也映射到环上，然后顺时针找到最近的执行器，即为目标执行器。如下图所示，job1 固定调度到执行器 A，job2 和 job3 固定调度到执行器 B，job4 固定调度到执行器 C：&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-0ad01a71277a9f227ced3b876fa4a2360dd.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;如果添加一个执行器 D，通过哈希值映射到了 job2 和 job3 中。如下图所示，job2 会调度到执行器 D 上，其他任务调度的机器保持不变：&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-19e2de7321e73d275cb09c27e427f7a4793.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;如果执行器 C 突然挂掉了，如下图所示，job4 会调度到执行器 A 上，其他任务调度的机器保持不变：&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-49f2dd8255ad7f43faeb56e5e0a37506e9f.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;如果执行器节点不多，直接映射到哈希环上的时候，有可能无法平均分布，导致任务分配不均。为了解决这个问题，可以引入虚拟节点（XXLJOB 引入了 100 个虚拟节点），将虚拟节点平均分布在哈希环上，然后物理节点映射虚拟节点。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-0af27b26f0bd8673df6d331804136865584.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;如上图所示，一共有 3 个执行器 ABC，每个执行器分配 4 个虚拟节点，保证所有虚拟节点平均分布在哈希环上，这样所有任务调度就基本上负载均衡了。&lt;/p&gt; 
&lt;span id="OSC_h2_10"&gt;&lt;/span&gt; 
&lt;h2&gt;负载最低路由策略&lt;/h2&gt; 
&lt;p&gt;上面提到的路由策略都没法解决一个问题，就是应用下同时存在大任务和小任务，导致执行器负载不均衡。那么我们是否可以采集所有执行器的负载，每次调度的时候按照负载最低优先调度呢？确实有些调度系统是这样做的，代表产品是 Kubernets &lt;strong&gt;[&lt;/strong&gt; &lt;strong&gt;2]&lt;/strong&gt; 。Kubernetes 使用 kube-scheduler 进行调度，本质上是把 Pod 调度到合适的 Node 上，默认策略就是调度到负载最低的 Node 上。在容器服务中，每个 Pod 都会预制占用 cpu 和内存，kube-scheduler 每调度一个 Pod，就能实时更新所有 Node 的负载，该算法没有问题。&lt;/p&gt; 
&lt;p&gt;但是在传统的任务调度系统中（比如 XXLJOB），一般都是通过线程运行任务的，没法提前知道每个任务会占用多少资源。任务调度到执行器上，也不是马上导致执行节点负载上升，通常会有滞后性。甚至有些逻辑复杂的任务，可能好几分钟后才会有大量的 IO 操作，导致该执行节点好几分钟后才能明显负载上升。举个例子，有 AB 两个执行节点：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;A 节点上当前有 1 个任务在运行，A 节点负载 20%，B 节点当前没有任务运行，负载 0%。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-be10f131d25815e10c703a6b56b4c79afaf.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;这个时候有 job2~job5 一共 4 个任务要调度，都选择了当时负载最低的执行器 B。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-4c2a38d0bbec16b75d051f38e62fe5263d3.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;4 个任务都发送到执行器 B 后，过了一会，执行器 B 负载上升到 100%，执行器 A 还是 5%，导致负载不均衡。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;span id="OSC_h2_11"&gt;&lt;/span&gt; 
&lt;h2&gt;任务权重路由策略&lt;/h2&gt; 
&lt;p&gt;有没有办法可以像 kube-scheduler 一样，调度的时候就预算每个执行节点的负载呢？因为定时任务都是周期性运行的，每次执行的代码或者脚本是固定的，通过业务逻辑或者历史执行时间，我们其实是知道哪些是大任务哪些是小任务的。每次任务调度的时候，我们只要知道当前各个执行器上运行了多少个大任务多少个小任务，就能把当前这个任务分发到最合适的执行器上。&lt;/p&gt; 
&lt;p&gt;MSE-XXLJOB &lt;strong&gt;[&lt;/strong&gt; &lt;strong&gt;3]&lt;/strong&gt; 设计并实现了任务权重路由策略，每个任务都可以用户自定义权重（int 值），交互流程如下：&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-0d9160df2b04280c0c7c1f1887c088ecc37.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;如上图所示，&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Scheduler 调度器开始调度任务。&lt;/li&gt; 
 &lt;li&gt;RouteManger 负责路由策略，如果是任务权重路由策略，去 WorkerManger 里寻找当前权重最小的执行器，并更新该执行器的权重（+当前任务的权重）。&lt;/li&gt; 
 &lt;li&gt;RouteManger 把任务分发给权重最小的执行器执行任务。&lt;/li&gt; 
 &lt;li&gt;执行器运行完任务，更新 WorkerManger，把该执行器的权重减去该任务的权重。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;下面以一个详细的例子来说明。当前有两个执行器，有 ABCD 4 个任务需要调度，其中 A 是大任务，按照历史经验 cpu 会占用 50%，BCD 是小任务，每个会占用 cpu 20%。将 A 任务权重设置为 5，BCD 设置为 2。初始化执行器 1 和执行器 2 的权重都是 0。&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;A 任务调度到执行器 1，执行器 1 的权重变为 5，执行器 2 的权重还是 0。B 任务选择任务权重最小的机器，调度到执行器 2，执行器 2 的权重变为 2。C 任务选择任务权重最小的机器，调度到执行器 2，执行器 2 的权重变为 4。D 任务选择任务权重最小的机器，调度到执行器 2，执行器 2 的权重变为 6。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-294a2873a5488022b9503489236d079c705.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;这个时候，又有个小任务 E 要调度，权重也是 2，选择当前权重最小的机器 1，则机器 1 的权重变为了 7，如下图&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-0cb6bdad28fca8ad7ad85a7041fd24a8586.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;小任务 B 和 C 执行很快就跑完了，这个时候执行器 2 的权重减去 4，变为了 2，如下图：&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-a8fbdbf6a298620ffc183e8eb4853863b44.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;这个时候又来个大任务 F，权重是 5，选择权重最小的机器 2，机器 2 的任务权重变为了 7，如下图：&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-d431902415007920bbd5594f3d235dfe1c6.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ol start="5"&gt; 
 &lt;li&gt;可以看到该算法，将每个任务实际消耗的资源映射成任务权重，可以实时统计每个执行器的权重，提前规划不同权重的任务分配到哪个执行器上去执行，达到全局最优解。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;span id="OSC_h2_12"&gt;&lt;/span&gt; 
&lt;h2&gt;总结&lt;/h2&gt; 
&lt;p&gt;在真实生产环境下，不同的任务处理不同的业务逻辑，如果选错路由策略，可能会导致集群中大部分执行节点负载不到 10%，但是个别节点负载会冲高到 100%，虽然平均负载不高，但是也无法减少规格，可能还需要增大规格防止出稳定性问题。但是如果选对了路由策略，集群所有节点负载均衡，就可以减少节点规格，成本降低 50% 以上。下面以一个表格详细介绍不同路由算法的场景：&lt;/p&gt; 
&lt;p&gt;&lt;img height="352" src="https://oscimg.oschina.net/oscnet/up-135f88cb4ca465c1c8dfa02b9b2f2522ea3.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;相关链接：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;[1] XXLJOB&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fxuxueli%2Fxxl-job" target="_blank"&gt;https://github.com/xuxueli/xxl-job&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;[2] Kubernets&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.csdn.net%2Fjy02268879%2Farticle%2Fdetails%2F148855464" target="_blank"&gt;https://blog.csdn.net/jy02268879/article/details/148855464&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;[3] MSE-XXLJOB&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhelp.aliyun.com%2Fzh%2Fschedulerx%2Fschedulerx-xxljob%2Fgetting-started%2Fcreate-and-deploy-a-xxljob-job-in-a-container-in-10-minutes" target="_blank"&gt;https://help.aliyun.com/zh/schedulerx/schedulerx-xxljob/getting-started/create-and-deploy-a-xxljob-job-in-a-container-in-10-minutes&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/3874284/blog/18689928</link>
      <guid isPermaLink="false">https://my.oschina.net/u/3874284/blog/18689928</guid>
      <pubDate>Sat, 06 Sep 2025 07:55:00 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>OpenBMB 发布并开源 MiniCPM 4.1-8B</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;OpenBMB&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FOJq_nfcFPIGTEeExQpT9GA" target="_blank"&gt;正式推出并开源&lt;/a&gt;&amp;nbsp;MiniCPM4.1-8B，这是首个开源的混合推理大语言模型，该模型通过系统性创新实现了端侧极致效率，支持深度推理模式与非推理模式一键切换。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0908/155239_yTBq_2720166.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;该系列在 8B 参数规模下通过模型架构、训练数据、训练算法和推理系统四个维度的系统性创新，实现端侧极致效率。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;模型亮点&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;首个原生稀疏架构的深思考模型，通过可训练稀疏注意力创新，代码、数学推理等任务的推理速度比同尺寸开源模型快 3 倍以上&lt;/li&gt; 
 &lt;li&gt;知识、推理、编程、指令遵循等 15 个评测基准，取得综合平均分同尺寸模型第一&lt;/li&gt; 
 &lt;li&gt;支持高效双频换挡：长文本用稀疏，短文本用稠密&lt;/li&gt; 
 &lt;li&gt;端侧友好，在 128K 长文本场景下，MiniCPM 4.1 相较于 Qwen3-8B 仅需 25% 的缓存存储空间&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img height="1707" src="https://static.oschina.net/uploads/space/2025/0908/155247_ogz0_2720166.jpg" width="1280" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="1707" src="https://static.oschina.net/uploads/space/2025/0908/155318_mGYS_2720166.jpg" width="1280" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;MiniCPM4.1-8B 采用 InfLLM v2 可训练稀疏注意力机制，在 128K 长文本场景下每个 token 仅与不到 5% 的 token 计算相关性，显著降低长文本计算开销；原生支持 65,536 token 上下文，通过 LongRoPE 可扩展至 131,072 token。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1707" src="https://static.oschina.net/uploads/space/2025/0908/155352_SHVT_2720166.jpg" width="1280" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;目前，模型已在 Github、Hugging Face、魔搭社区开源&lt;/p&gt; 
&lt;p&gt;🔗Github：https://github.com/OpenBMB/MiniCPM&lt;br&gt; 🔗Hugging Face:&amp;nbsp;https://huggingface.co/openbmb/MiniCPM4.1-8B&lt;br&gt; 🔗ModelScope:https://modelscope.cn/models/OpenBMB/MiniCPM4.1-8B&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370929</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370929</guid>
      <pubDate>Sat, 06 Sep 2025 07:55:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>光刻机巨头 ASML 领投 Mistral AI 17 亿欧元 C 轮融资</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;2025 年 9 月 8 日，荷兰光刻机巨头阿斯麦（ASML）&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fa16z.com%2Fannouncement%2Finvesting-in-mistral%2F" target="_blank"&gt;宣布&lt;/a&gt;领投法国 AI 初创公司 Mistral AI 的 17 亿欧元 C 轮融资，注资 13 亿欧元（约 15 亿美元），成为其最大股东并获董事会席位。融资后 Mistral AI 估值达 100 亿欧元（约 117 亿美元），成为欧洲估值最高的人工智能企业。&lt;/p&gt; 
&lt;p&gt;&lt;img height="659" src="https://static.oschina.net/uploads/space/2025/0908/152256_Qedv_2720166.png" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;作为先进芯片制造设备的关键供应商，ASML 的这笔投资旨在强化欧洲科技主权，减少欧洲对美国和中国 AI 模型的依赖。Mistral AI 自 2023 年成立以来发展迅速，被视为法国乃至欧洲 AI 领域的领军者，竞争对手包括美国的 OpenAI 和谷歌等科技巨头。 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-fa260b31072a0d42e879961b65f0267b980.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;通过此次合作，ASML 不仅能借助 Mistral AI 的数据分析和 AI 能力提升自身设备性能，还有望助力欧洲在全球科技竞争中脱颖而出。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370920</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370920</guid>
      <pubDate>Sat, 06 Sep 2025 07:23:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>MiniMax 启动期权增发奖励</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;MiniMax 已启动期权增发事宜，根据员工对公司的贡献程度不同，激励在几十万美金到几百万美金不等；涵盖算法、工程等全序列核心贡献员工，以鼓励员工大胆追求 AGI。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;据了解，本次期权发放来自于全员会口头通知，不仅模型算法，产品、市场、增长、职能等岗位都在其中（包含正职和实习生）；后续还会继续对突出贡献者进行即时期权激励。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="221" src="https://oscimg.oschina.net/oscnet/up-6c4759bf41e6427dd7c2ba90a2bac56ab55.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;官网资料显示，MiniMax 成立于 2022 年初，以「与所有人共创智能」为使命，致力于推动人工智能科技前沿发展，实现通用人工智能 (AGI）。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;目前，MiniMax 自主研发了一系列多模态通用大模型，包括 MiniMax M1、Hailuo-02、Speech-02 和 Music-01，具备超长上下文处理能力，能够理解、生成并整合包括文本、音频、图像、视频和音乐在内的多种模态。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;并基于这些自研模型面向全球推出一系列 AI 原生产品，包括 MiniMax、海螺 AI、MiniMax Audio、星野等，以及面向企业和开发者的开放平台。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;数据表明，MiniMax 的自研多模态模型及 AI 原生应用已累计为来自超过 200 个国家及地区的逾 1.57 亿名个人用户，以及来自超过 90 个国家及地区的 50,000 余名企业客户以及开发者提供服务。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370916</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370916</guid>
      <pubDate>Sat, 06 Sep 2025 07:08:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>谷歌即将推出 「AI 模式」 作为默认搜索体验</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;谷歌搜索正面临一次重大的变革，「AI 模式」 即将成为默认选项。这一模式基于谷歌现有的 「AI 概述」 功能，与 ChatGPT 等其他生成式 AI 类似，允许用户在初次搜索后与 AI 继续对话。这一转变意味着谷歌将从一个通向网络的信息门户，转变为一个以生成式 AI 为中心的封闭平台。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="357" src="https://oscimg.oschina.net/oscnet/up-f7a6f986c397021bf4f6ca86279c89daf48.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;谷歌产品经理洛根・基尔帕特里克（Logan Kilpatrick）在社交平台 X 上暗示，AI 模式很快将成为新的搜索体验。例如，访问 google.com/ai 现在会自动跳转至 AI 模式，而当用户询问这一模式是否会成为默认设置时，他回应道 「很快会的 :)」。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;目前，AI 模式已在包括美国在内的 180 多个国家推出，但尚未完全取代传统搜索。在&lt;span&gt;最新&lt;/span&gt;的更新中，谷歌为 AI 模式增加了新的代理功能，使用户能够直接通过聊天界面预订本地服务或购买活动门票。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;与此同时，谷歌的法律团队在一场关于其广告技术部门的诉讼中表示，「开放网络已经处于快速衰退之中」。他们声称，拆分谷歌的广告业务只会使情况变得更糟，尤其是广告资金正逐渐从传统的开放网站展示广告转向连接电视、零售媒体以及谷歌所称的 「广受欢迎的发布平台，如 AI 聊天机器人，它们可以有效地货币化展示内容」。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;谷歌指出，强制拆分将加速这一转变，挤压那些依赖广告收入的出版商的资源。该公司正在反对诉讼中要求出售其广告交易所、将其拍卖逻辑开源以及放弃 50% 净收入等要求。其他提议则要求制定新的行为规则和开放 API，以限制谷歌的市场力量并增加竞争。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;值得注意的是，谷歌在使用开放网络衰退这一论点来为自己辩护的同时，也在推出 AI 模式并将搜索流量引向自身平台，这无疑加剧了它所警告的趋势。最终，谷歌正利用其在这一问题上所扮演的角色，来抵制对其业务拆分的要求，这种循环逻辑在谷歌高管关于 AI 搜索如何影响开放网络的矛盾言论中也屡见不鲜。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370913</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370913</guid>
      <pubDate>Sat, 06 Sep 2025 06:56:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Linus 对 Git 提交信息中「Link:」标签被滥用表达不满</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span&gt;9 月 6 日，知名 Linux 之父 Linus Torvalds 公开表达了对 Git 提交信息中「Link:」标签滥用的不满。他指出，近期在 Linux 内核的 Git 提交和补丁中频繁出现的「Link:」标签，往往没有提供实际价值，反而浪费了开发者和维护者的时间。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;Torvalds 在一次 block 模块的 pull request 评审中直接批评道，&lt;strong&gt;许多「Link:」标签链接内容仅仅是与当前补丁内容重复的信息，并未为评审工作提供任何额外解释或背景&lt;/strong&gt;。他强调，如果「Link:」标签无法带来新的、有意义的信息，比如指向相关的 bug 报告、讨论串、或多补丁系列的封面信件等，最好不要添加这些无用链接。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="434" src="https://static.oschina.net/uploads/space/2025/0908/145316_V8JA_2720166.png" width="1148" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;https://lore.kernel.org/all/CAHk-=wjamixjqNwrr4+UEAwitMOd6Y8-_9p4oUZdcjrv7fsayQ@mail.gmail.com/&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;虽然 Torvalds 表示，在多补丁系列场景下，需要追溯到系列补丁的 cover letter 时，「Link:」标签确实能发挥重要作用，但他呼吁大家不要在普通情况下机械地添加该标签。为此，他打算更加严格审核包含无意义「Link:」标签的合并请求，如今甚至有可能直接拒绝这类请求。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;他还建议未来如果要引入自动化辅助，不妨将 AI 等技术用于检测相关讨论，从而在需要时自动添加有实际参考价值的标签。他直言：「我喜欢有用的链接，但 99% 的链接都只是浪费时间的垃圾。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;Torvalds 呼吁 Linux 开发者们在今后的补丁提交中，务必确保「Link:」标签切实为代码评审和问题追踪带来帮助，否则应避免使用。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;strong&gt;全文如下：&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;该死的，这次提交明明带着那个充满希望的"链接:"参数，我本以为它能解释这个毫无意义的提交为何存在，但一如既往地，那个链接只是浪费我的时间——指向的又是那些老掉牙的信息。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;我原本期待它能指向某个错误报告之类的东西，解释为什么我的最初反应是错误的。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;别再搞这种垃圾了。别再添加浪费人时间的无用链接参数。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;只有当链接包含*额外*信息时才添加。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;该死，我真的讨厌这些无用链接。我喜欢看到*有价值*的链接，但实际看到的 99% 都指向愚蠢无用的垃圾，而且*只会*浪费我的时间。又一次。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;所以我不打算合并这个请求，光是看到它就让我恼火。若真要我合并，请给出实质解释而非无用链接。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;没错，我正处于暴躁状态。感觉我的主要工作——实际上唯一的工作——就是处理合并请求，因此我极度厌恶这些自动添加的垃圾内容，它们只会让我的工作更难做。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;「所以我希望至少有某种方式能阻止这种机械、无脑的使用——在理想情况下，还可以有一种更有用、自动添加链接的模式。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;例如，我觉得对于多提交系列的封面信件来说，链接到补丁系列的提交记录可能更有用——而且也不会那么烦人——因为它会被加入到合并信息（merge message）中，而不是每一个具体提交中。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;因为如果有人真的在查看合并信息，他们很可能是在查找更宏观的背景信息——或在处理某个合并冲突——此时我认为最初的提交可能就更相关。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;当然，大多数人实际上并不会在合并时使用封面信件，他们只是把补丁作为一个系列应用，所以其实也没那么烦人，因为它根本不会存在于 git 历史记录里 ;)&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;总之，‘阻止无脑使用’可能就只是加个大大的警告，提醒大家：这个链接可能只会带来烦人的负担。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;相比之下，一个‘完美’的模式可能是实现某种自动化——‘除非真的有实际讨论发生’。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;不过我觉得这种模式可能太复杂，除非真的有人愿意探索用 AI，因为他们的工作描述就是‘寻找 AI 的实际用途’。在如今的科技世界中，我相信确实有人这样定位自己的岗位。唉。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;举例来说，既然‘b4’最终会浏览补丁的下游帖子以便自动添加 acked-by 等信息，我确实觉得理论上可以建立这样一个启发式模型：‘某个补丁有活跃的邮件讨论，所以值得加上链接’。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;坦率地讲，就算这种讨论最终没有什么实质内容，我想只要这个链接至少指向某个帖子（而不是那些已经收集起来的 acked-by 邮件），我也会觉得没那么烦人，相比那种只是指向单封邮件、没人回复过的链接。至少这样我会觉得多少有点实际内容。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;当然，一如既往，我也意识到有些人认为补丁提交邮件之后，以后或许会有更多邮件回复。但实际上，这种情况很少见，因为后续测试中的问题往往会创建新邮件，而不是回复原始邮件（而且即便有人真去回复了原始邮件，我们也可以很容易根据提交查找邮件，反过来也可以查到）。」&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/370911</link>
      <guid isPermaLink="false">https://www.oschina.net/news/370911</guid>
      <pubDate>Sat, 06 Sep 2025 06:54:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
  </channel>
</rss>
