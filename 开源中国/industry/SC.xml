<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>开源中国-综合资讯</title>
        <link>https://www.oschina.net/news/industry</link>
        <atom:link href="http://8.134.148.166:30044/oschina/news/industry" rel="self" type="application/rss+xml"></atom:link>
        <description>开源中国-综合资讯 - Powered by RSSHub</description>
        <generator>RSSHub</generator>
        <webMaster>contact@rsshub.app (RSSHub)</webMaster>
        <language>en</language>
        <lastBuildDate>Thu, 06 Mar 2025 12:38:48 GMT</lastBuildDate>
        <ttl>5</ttl>
        <item>
            <title>开源中国马越：DeepSeek 不是国运级的创新，年轻人才是</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;blockquote&gt; 
 &lt;p&gt;「你很难要求大家还没吃饱喝足的情况下，去做开源。」&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;如果说关于 DeepSeek 的讨论已经过于泛滥，开源也许是当下依然值得讨论的主题。&lt;/p&gt; 
&lt;p&gt;长期以来，在国内谈起「开源」，都会无可避免地陷入一种尴尬的语境。&lt;/p&gt; 
&lt;p&gt;它当然是理想主义的。「开源」背后的自由、开放特性，被普遍认为是互联网精神的外化——源代码向公众开放共享，且允许在遵循特定许可证条款的前提下，对软件进行自由使用、修改和二次分发。&lt;/p&gt; 
&lt;p&gt;最知名的开源项目「Linux」是操作系统的内核，催生了数以千万计的开源软件，这是互联网世界的根基。&lt;/p&gt; 
&lt;p&gt;但它的背后经常跟着一个问题：为什么要开源？怎么考虑开源之后的商业化？哪怕到 DeepSeek 爆火的现在，也很难有人给出完美的答案。&lt;/p&gt; 
&lt;p&gt;开源中国董事长马越，是最有立场谈国内开源历史的人之一，他在这条路上走了 18 年。&lt;/p&gt; 
&lt;p&gt;2008 年，马越从硅谷回国创业，先是成立了「恒拓开源」——用开源软件帮助企业摆脱数据库、ERP 等大型软件的束缚。&lt;/p&gt; 
&lt;p&gt;但很快他就发现，这种方案很难摆脱 To B 项目制的重投入，还很容易做成外包公司。&lt;/p&gt; 
&lt;p&gt;随后，马越选择收购「开源中国」这个社区，开始了一段曲折的创业路——「开源中国」经历过数度转型，从开源社区，拓展到代码托管、代码工具链，在探索商业化期间，经历了从母公司剥离独立发展，2019 年被百度战略控股，最后，又在中美竞争、国产替代浪潮中决定重新独立发展，谋求上市。&lt;/p&gt; 
&lt;p&gt;做开源社区需要大量的资源、资金投入，在开源中国最艰难的时候，马越揹负的个人债务最高达 1.8 亿元。&lt;/p&gt; 
&lt;p&gt;1972 年出生的马越，有着一种老大哥式的坦率。他绝没有卖苦的意思，但你很容易从他的敍述中，体会到经历这些坎坷过后的幽默——他表示，在中国做 To B 就是「城市包围农村」，企业软件就是管理者智慧的固化。当中国的企业发展阶段还在初期，「你很难要求大家还没吃饱喝足的情况下，去做开源。」&lt;/p&gt; 
&lt;p&gt;但这些时刻都已经过去了。开源中国也已经摸索出一条更适合自己的、中国式的开源道路。&lt;/p&gt; 
&lt;p&gt;现在，开源中国已经成为全球第二大的代码托管平台，汇聚了超过 1800 万开发者。其自主研发的 DevOps 工具链已在金融、军工等关键领域，达到 80% 的市场渗透率。2024 年，开源中国的营收已超过 2 亿元。&lt;/p&gt; 
&lt;p&gt;《智能涌现》获悉，&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/news/337301/oschina-series-c-funding-round&quot;&gt;「开源中国」近期正式完成数亿元 C 轮融资&lt;/a&gt;&lt;/u&gt;，由北京信息产业发展投资基金（北京信产基金）领投，深报一本股权投资基金（深报一本）及北京上河动量私募股权基金（上河动量）跟投。&lt;/p&gt; 
&lt;p&gt;至此，开源中国已累计获得超 16 亿元战略投资。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0306/194426_EaKH_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;△开源中国董事长马越&amp;nbsp;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;马越认为，哪怕在全球范围内，开源也不是件容易的事情。他以 GitHub 举例：从 2008 年成立开始直到被微软收购，在 2022 年 ChatGPT 爆发后推出 Copilot，才算是正式证明了商业化潜力。&lt;/p&gt; 
&lt;p&gt;「开源是强者和富人的游戏。」他说，上一代人都成长在物质更短缺的年代——商业社会也是如此，企业要先赚够了钱，才有余裕考虑是否开源，做一些人人为我、我为人人的好事。「吃饱了饭，才能有力气谈开源。」&lt;/p&gt; 
&lt;p&gt;这就不难理解，即使 DeepSeek 的爆火为全中国都打了一记强心针，马越的观点依然是冷静的。他认为，DeepSeek 很难根本性改变国内软件生态的问题，这是一个时代的局限。&lt;/p&gt; 
&lt;p&gt;而想要在开源路线上有所成就，这要求新一代的开发者，从 Day 1 就开始出海，像 DeepSeek 一样去全球市场中竞争。&lt;/p&gt; 
&lt;p&gt;如果说 DeepSeek 改变了什么，更多的都是文化和价值观层面的事情。「十年前大家普遍不理解开源，觉得开源是一群草根做的事，现在全社会都能认识到，开源等于创新。」马越说。&lt;/p&gt; 
&lt;p&gt;以下为《智能涌现》与开源中国董事长马越的对话，经编辑：&lt;/p&gt; 
&lt;h2&gt;&lt;span style=&quot;color:#27ae60&quot;&gt;&lt;strong&gt;DeepSeek 不是国运级产品，年轻人才是&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;《智能涌现》：开源中国现在是第二大代码托管平台，国内最大的开源社区。DeepSeek 的热潮，对你们的直观影响，是从什么时候开始的？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;马越&lt;/strong&gt;&lt;/span&gt;：就是他在 App Store 登顶那会儿。先是 V3，然后是 R1 发布，一下子就火起来了。我们春节一直在加班，让 DeepSeek 首先能在中国生产的 GPU 上运行，这需要大量工作，我们是第一个在沐曦芯片上部署的。&lt;/p&gt; 
&lt;p&gt;我们都在调侃，春节就两件事：DeepSeek、哪吒。DeepSeek 就是开源圈出了个哪吒。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能涌现》：现在已经形成了一种论调：DeepSeek 是一个国运级的产品。但最近你的公开表达里，似乎对这一点不太认同。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;马越&lt;/strong&gt;&lt;/span&gt;：首先，大模型这个事情，你离不开英伟达吧？其次，你离不开 Transformer 架构；第三，你采用了蒸馏的思路，这些都不是国内原创的。&lt;/p&gt; 
&lt;p&gt;DeepSeek 本质上是在现有路线上走得最好，实现了弯道超车，这是值得尊敬的。&lt;/p&gt; 
&lt;p&gt;但是 DeepSeek 能够不依靠外部资金支持，也不做任何 PR，靠技术就能做到全球顶尖——以梁文锋为代表的年轻人崛起，这才是国运级的现象。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能涌现》：那什么才算国运级的产品？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;马越&lt;/strong&gt;&lt;/span&gt;：完全原创的技术创新。谁说 Transformer 就是算法的终局？如果有人用非 Transformer 方案做出比 DeepSeek 强十倍的成果，那才是真正的突破，那是人类级的进步。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能涌现》：DeepSeek 给开源生态最大的启示会是什么？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;马越&lt;/strong&gt;&lt;/span&gt;：让全社会认识到：开源等于创新。&lt;/p&gt; 
&lt;p&gt;DeepSeek 最令人唏嘘的是，在国内两年都默默无闻，也不如打广告的很多大模型公司，直到 2024 年开始，才因为技术，因为开源，被美国人超级关注——虽然一部分人特别支持，一部分人极力贬低，这种关注反而倒逼着国内形成了一种爱国情怀。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能涌现》：以前大家不相信这个观点吗？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;马越&lt;/strong&gt;&lt;/span&gt;：以前很多人认为开源就是一帮草根、乌合之众，很难和大厂这种正规军相比。&lt;/p&gt; 
&lt;p&gt;其实二十年前我就在说这些话：开源约等于创新能力，创新能力和国力是映射关系。正是因为我们有钱了、富足了，才会有 DeepSeek 这样的企业出现。&lt;/p&gt; 
&lt;p&gt;以前没人听，现在有人听了。&lt;/p&gt; 
&lt;p&gt;第二点很重要，就是要对年轻人保持敬畏。不只是尊重，而是要怕年轻人，信任年轻人。每一代人都有自己的时代使命，也有时代局限性。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能涌现》：这群年轻人，或者新一代开源贡献者，为什么能成长起来？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;马越&lt;/strong&gt;&lt;/span&gt;：这种质变是建立在之前充分的量变基础上的。&lt;/p&gt; 
&lt;p&gt;这十年要感谢走在前面的互联网大厂，事实上国内的主要开源力量集中在这些有实力的企业上。包括百度、阿里、腾讯等组织的开源项目，还有华为的鸿蒙、欧拉等等。他们都是领着工资的员工，在搞这些开源工作，不是纯粹基于兴趣。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能涌现》：DeepSeek 证明一件最关键的事：通过底层技术突破，就能吸引大量用户，以及赢得尊重。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;马越&lt;/strong&gt;&lt;/span&gt;：现在我们国家最应该做两件事：一是牵头一起开发中国的 CUDA；二是让所有国产 GPU 都能快速支持这些模型。&lt;/p&gt; 
&lt;p&gt;说到生态，生态就是要有更多的人参与，而且大家都有高度共识。现在最大的问题不是芯片卡脖子，而是 CUDA 这个生态的制约。中国完全可以开发一套类似 CUDA 的系统，就像我们有自己的 GPU 一样。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;&lt;span style=&quot;color:#27ae60&quot;&gt;&lt;strong&gt;开源是富人和强者的游戏&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;《智能涌现》：DeepSeek 爆火之后，找你讨论的人多吗？大家最关心什么话题？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;马越&lt;/strong&gt;&lt;/span&gt;：有人问我，DeepSeek 会不会给中国 To B 市场带来新生机？不可能那么快。&lt;/p&gt; 
&lt;p&gt;IT 外包的人天价格，20 年来的涨幅还不如按摩师。现在外包人天均价一千就算高的了，还有五六百的。你去按摩，现在一小时都要一两百块钱。十年前，IT 的外包时薪就比不上按摩了，现在差距更大，那是因为按摩价格涨得快。&lt;/p&gt; 
&lt;p&gt;中国软件没人愿意花钱，这是行业发展还不行的核心原因。要等这一代年轻人变成决策者，好时代才会来。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能涌现》：本质还是因为国内企业发展阶段还比较早。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;马越&lt;/strong&gt;&lt;/span&gt;：开源本质上是强者和富人的游戏。正是因为我们吃饱喝足了，才会有 DeepSeek 这样的企业出现。上一代互联网用户普遍不愿意为软件和知识付费，腾讯会议掉线了就重连，也不愿意买会员。&lt;/p&gt; 
&lt;p&gt;但这一代年轻人生活富足，你们会改变这个局面。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能涌现》：DeepSeek 会给上一代 To B 创业者带来什么启示吗？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;马越&lt;/strong&gt;&lt;/span&gt;：我觉得他们给创业者带来两个重要的启示。第一是要对钱保持敬畏。创业的目的就是为了挣钱，谈理想和情怀没意义。&lt;/p&gt; 
&lt;p&gt;DeepSeek 不太需要考虑商业化的问题，是因为幻方已经解决了这个事情。&lt;/p&gt; 
&lt;p&gt;上一代的软件创业者有个致命问题，一心想着烧钱，通过标准化产品打市场，这不是中国市场的运行逻辑，中国最有钱的金主都是大型企业，在中国想要赚钱，不做定制化是不现实的。&lt;/p&gt; 
&lt;p&gt;中国软件行业是城市包围农村，而美国是农村包围城市，腰部企业数量很多。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能涌现》：DeepSeek 会改变大家对商业化的看法吗？开源怎么考虑商业化，是这个领域的「天问」。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;马越&lt;/strong&gt;&lt;/span&gt;：如果要开源做创业项目，技术必须得硬邦邦。就是和 DeepSeek 一样，Day 1 就出海，否则在中国太难赚钱了，时代还不够成熟。&lt;/p&gt; 
&lt;p&gt;大家总是会举例，比如红帽那套模式也能商业化，但是想用这种方式在中国做一个上市公司，还不是这个时代的事。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能涌现》：你们自己也经历了很长一段商业化探索的时期，是从什么时候想明白要怎么做的？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;马越&lt;/strong&gt;&lt;/span&gt;：2020 年是个重要转折点。我们那年决定从百度独立出来，重新谋求 IPO。那段时间因为美国开始在很多尖端技术上断供，我们想抓住这个机会，真正成为一个独立的开源平台。&lt;/p&gt; 
&lt;p&gt;想要做真正的本土开源平台，必须要是彻底中立的第三方，这是选择重新独立发展的核心原因。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能涌现》：想明白之后，都做了什么？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;马越&lt;/strong&gt;&lt;/span&gt;：我们现在从社区发展出了三大产品线。&lt;/p&gt; 
&lt;p&gt;开源中国社区（OSChina）现在已经完全进化成一个 AI 教育平台。我们是中国最大的开源社区，有 1000 多万用户。现在我们 24 人的团队能创造约 5000 万收入，还有净利润，这在社区团队中很少见。&lt;/p&gt; 
&lt;p&gt;第二块是代码托管和研发效能平台 Gitee，现在平台有 3600 万个代码仓库，服务 36 万家企业。主要提供代码托管私有化仓库服务，确保很多中小团队的代码安全，客单每年 3000 块左右。&lt;/p&gt; 
&lt;p&gt;从 2020 年到现在，我们已经能够提供 DevOps 全生命周期国产替代方案，在满足开发者需求的同时，也建立起一个自主创新、安全可信的本土开源软件工具与生态。&lt;/p&gt; 
&lt;p&gt;第三块是 AI 大模型平台「模力方舟」，模型体验、推理训练到应用部署等等服务，都会提供。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能涌现》：为什么会从社区拓展到后来的 DevOps，以及 AI 大模型基座？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;马越&lt;/strong&gt;&lt;/span&gt;：一个开源公司想要成功，靠社区是不够的，我们需要找一个闭环的商业模式，像 GitHub 那条路——社区、代码托管是没法达到这个目标的。GitHub 也是在大模型浪潮来了之后，推出 Copilot，才把营收做起来。&lt;/p&gt; 
&lt;p&gt;以后没有净利润的公司很难在国内上市，所以我一直强调看毛利率和人效，这两个指标高了，自然会有净利润。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能涌现》：你们现在的主要收入，来自哪里？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;马越&lt;/strong&gt;&lt;/span&gt;：我们主要收入来自 100 家左右的银行、券商、军工、制造业客户，都走大型私有部署形式。中小客户主要靠 SaaS 服务。&lt;/p&gt; 
&lt;p&gt;2024 年我们全国订单超过 2 亿，是一个突破。前年过 1 亿，2024 年翻了一倍，还实现了盈亏平衡，这很不容易。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能涌现》：主要模式靠服务大型企业的话，怎么避免走到项目制的老路？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;马越&lt;/strong&gt;&lt;/span&gt;：我们的产品做得很复杂，是因为中国的大型企业的场景复杂。我们的流程引擎、角色引擎、交互界面、流水线都是可定制的，还能做各种插件，就是为了保证灵活性。&lt;/p&gt; 
&lt;p&gt;我们会帮客户做定制化配置，但是不做二次开发。我们现在 330 多人，这块业务占 200 多人，但定制化去做开发和交付的不到 10%。&lt;/p&gt; 
&lt;p&gt;第二是我们自己坚决不卖算力，只做第三方，比如给云厂商导流。&lt;/p&gt; 
&lt;p&gt;我们现在的路线很清晰：前端社区承载大流量，做开发者工具卖给企业，先 To C，再 To B，也算是一种产品驱动增长（Product-Driven Growth）的模式。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;&lt;span style=&quot;color:#27ae60&quot;&gt;&lt;strong&gt;一起发展，比单打独斗强&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能涌现》：是否选择开源，企业的考量到底是什么？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;马越&lt;/strong&gt;&lt;/span&gt;：现在大模型不开源很难。苹果为什么到今天 iOS 都不开源？因为硬件生态已经形成垄断。如果没有类似这样的护城河，你不开源，凭什么在市场立足？&lt;/p&gt; 
&lt;p&gt;就像我十几年来一直说的，开源是创新的最佳方法论，也是市场竞争的方法论，是反强权的方法论。你做得好，我们就开源来和你竞争。当年有 Unix 和 Windows，就有 Linux；有 iOS，后来就有 Android，都一样。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能涌现》：你做过很多并购，DeepSeek 的成功会改变投资人对开源项目的看法吗？开源项目的出路会变得宽吗？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;马越&lt;/strong&gt;&lt;/span&gt;：这也是我想问所有投资人和创业者的问题：投资的目的是啥？到底你希望怎么赚钱？&lt;/p&gt; 
&lt;p&gt;上市、被收购、分红都是一种退出方式。但现在在国内，要么 IPO，要么死掉，这很残酷。&lt;/p&gt; 
&lt;p&gt;中国的开源生态很分散，现在很多创业者缺乏一种共识，就是一起发展比单打独斗强。很多人把创业当作获取情绪价值的方式，就想当老大，宁可公司死也不愿意卖给别人。觉得卖了就是投降，这坎儿过不去。&lt;/p&gt; 
&lt;p&gt;如果放不下自己的 ego，最终就会害了自己，也害了客户和投资人。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能涌现》：DeepSeek 大获成功之后，你怎么评估现在我们所处的 AI 发展阶段？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;马越&lt;/strong&gt;&lt;/span&gt;：如果类比互联网那个时代，我们还是在大时代的开端状态，类似当年的拨号上网阶段。我从 1997 年开始上网，下载一张照片要四五天，网速只有 28K。但即便如此，我们也觉得很神奇。&lt;/p&gt; 
&lt;p&gt;现在就像出海探索新大陆，所以创业者只要带着干粮上了船，不淹死，就一定有收获。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能涌现》：DeepSeek 会怎么改变现在国内的创业格局？你觉得更利好大厂还是创业公司？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;马越&lt;/strong&gt;&lt;/span&gt;：很难说，可能还是大厂比较有优势。&lt;/p&gt; 
&lt;p&gt;首先，DeepSeek 不是一个创业公司，人家不用外部资金就能买一万张卡，某种程度上也算个小大厂了。&lt;/p&gt; 
&lt;p&gt;我觉得 DeepSeek 给创业者带来两个重要的启示。第一是要对钱保持敬畏。创业的目的就是为了挣钱，谈理想和情怀没意义。&lt;/p&gt; 
&lt;p&gt;初创公司除非在算法、技术底层有突破，否则在工程层面，很难跟大厂拼数据，拼流量，这是最终商业化的两个要素。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能涌现》：现在的大模型初创的转向都很明显，方向聚焦，专心做底层技术。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;马越&lt;/strong&gt;&lt;/span&gt;：这就是开源的可怕之处。&lt;/p&gt; 
&lt;p&gt;我前年就说，预训练是大厂的游戏，创业公司应该做垂直领域的训练，把更多精力放在推理上，烧钱的事情本来就不该做。&lt;/p&gt; 
&lt;p&gt;历史上都有很多例子，当年开源领域有很多做容器的公司，比如 Docker 刚出来时只是各种容器运行时技术中的一种。结果 K8s 生态起来之后，任何容器技术只要实现 K8s 兼容性，就可以融入云原生技术栈，这种强大的生态整合能力最终使其它技术方案逐渐边缘化，相当于前边都白做了。&lt;/p&gt; 
&lt;p&gt;所以我给大家的建议，包括我们自己的策略，就是产品功能要紧跟随，但要轻投入，商业模式要做轻一点。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;《智能涌现》：对开源中国来说，未来的目标会是什么？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;马越&lt;/strong&gt;&lt;/span&gt;：开源中国这十几年，积累了用户流量护城河，客户品牌美誉度，现在是通过信创找到了快速增长的收入模式。&lt;/p&gt; 
&lt;p&gt;我们在这轮融资之后，也会开始寻求进一步上市，希望成为 A 股人工智能开源第一股。&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;&lt;em&gt;原文：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FEY_NhbMX94bk6op8GDuG6g&quot; target=&quot;_blank&quot;&gt;《对话开源中国马越：DeepSeek 不是国运级的创新，年轻人才是》&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/337320</link>
            <guid isPermaLink="false">https://www.oschina.net/news/337320</guid>
            <pubDate>Thu, 06 Mar 2025 11:52:45 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>开源中国完成数亿元 C 轮融资，领航 AI 新纪元</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#27ae60&quot;&gt;&lt;strong&gt;开源技术生态领军企业开源中国&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;（开源共识（上海）网络技术有限公司）近日完成了数亿元 C 轮融资&lt;/strong&gt;，本轮融资由北京信息产业发展投资基金（北京信产基金）领投，深报一本股权投资基金（深报一本）及北京上河动量私募股权基金（上河动量）跟投。&lt;/p&gt; 
&lt;p&gt;此次融资将加速公司 AI 战略布局：深化现有产品矩阵的扩展、完善与全面 AI 化，构建软硬件协同的智能解决方案体系，促进人工智能在产业领域的 AI 应用落地。&lt;/p&gt; 
&lt;p&gt;至此，开源中国已累计获得超 16 亿元战略投资，投资方包括百度、华为、海望资本、张江科投、中科创星、天际资本、君联资本、上海国际创投、中移和创投资、瑞壹投资、容亿资本、泰达实业、中国互联网投资基金、国调科改、联想创投、上海浦东软件园、上海科创、北京信产基金、深报一本、上河动量等。构建起国有资本、科技大厂、创始团队&quot;3:3:4&quot;的良性股权结构，形成&quot;国家队护航、产业方协同、市场化运作&quot;的创新生态。&lt;/p&gt; 
&lt;h3&gt;&lt;span style=&quot;color:#27ae60&quot;&gt;&lt;strong&gt;开发者生态的厚积薄发&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;p&gt;作为中国开源基础设施奠基者，开源中国运营着 1800 万开发者聚集的 oschina.net 社区及代码托管平台 Gitee，服务 36 万企业级用户。其自主研发的 DevOps 工具链已在金融、军工等关键领域实现 80% 市场渗透率，成为信创替代工程的标杆案例，验证了开源商业化的中国路径。&lt;/p&gt; 
&lt;h3&gt;&lt;span style=&quot;color:#27ae60&quot;&gt;&lt;strong&gt;AI 转型的战略升维&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;2024 年，公司推出对标 HuggingFace 的 AI 大模型平台&quot;模力方舟 (moark.com)&quot;，首创&quot;模型数据-算力调度-应用开发&quot;全栈服务体系。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;平台已实现三大突破：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;生态开放化&lt;/strong&gt;：聚合数千开源模型，打造 AI 应用创新基座；&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;服务一体化&lt;/strong&gt;：提供从模型体验、推理训练到应用部署的全生命周期服务；&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;算力国产化&lt;/strong&gt;：完成多家国产 GPU 深度适配，成功运行 DeepSeek-V3 等千亿级模型。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;开源中国下一步将以模力方舟为核心，打造全方位的 AI 业务布局，助力 AI 应用创新、科技人才培养和新质生产力提升。&lt;/p&gt; 
&lt;h3&gt;&lt;span style=&quot;color:#27ae60&quot;&gt;&lt;strong&gt;起航，迈向「开源 AI 第一股」&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;p&gt;开源中国以开发者生态为基座，构建开源领域‌「用户-流量-盈利」‌三重护城河，率先在信创市场完成‌开源商业化闭环验证‌，实现国产研发工具从技术突破到商业变现的质变。依托本轮战略投资，加速 AI 战略升级扩张市场领域，开辟第二增长曲线‌。&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;目前开源中国已开始进入 IPO 倒计时，计划以&quot;开源 AI 第一股&quot;身份登陆资本市场，通过技术普惠推动新质生产力发展，助力中国在全球 AI 2.0 时代构建核心竞争力。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;北京信息产业发展投资基金表示：&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;开源中国是中国开源生态与 AI 技术融合创新的标杆企业，其以开发者生态为根基、以信创替代为突破、以 AI 战略为驱动的增长路径，高度契合国家科技创新与自主可控的战略方向。领投本轮融资，既是基于对开源中国在国产软件基础设施领域不可替代地位的认可，更是看好其通过「模力方舟」平台推动 AI 技术普惠化、算力国产化和应用场景规模化落地的能力。&lt;/p&gt; 
 &lt;p&gt;我们期待通过资源协同与生态赋能，助力开源中国加速构建 AI 时代的技术底座，为全球 AI 2.0 竞争注入中国开源力量。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;深报一本表示：&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;我们高度关注开源中国在国产软件替代浪潮中展现的商业化前景，凭借其构建的庞大开发者生态体系，公司有望在 AI 应用层持续释放开源技术的创新势能。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;上河动量管理合伙人王欣表示：&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;世界范围内，开源已经成为软件开发和人工智能创新的极其重要的推动力量。&lt;/p&gt; 
 &lt;p&gt;开源中国是服务于中国本土开源生态的先行者和坚守者，在地缘科技竞争的背景下，开源中国已经成为中国软件和人工智能领域具有国家级影响力的科技创新基础设施。相信开源中国的独特价值会得到越来越多的行业参与者和资本市场的认可。&lt;/p&gt; 
&lt;/blockquote&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/337301/oschina-series-c-funding-round</link>
            <guid isPermaLink="false">https://www.oschina.net/news/337301/oschina-series-c-funding-round</guid>
            <pubDate>Thu, 06 Mar 2025 10:11:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>个人开发者也能训练推理模型？GRPO 技术详解</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                    
                                                                                                                                                    &lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;编者按：&lt;/strong&gt; 还在为训练推理模型烧光算力预算而发愁？当开源小模型遇上数学题就&quot;智商掉线&quot;，如何低成本突破性能瓶颈？&lt;/p&gt; 
 &lt;p&gt;传统 RLHF 动辄百万级算力投入，让多少团队在强化学习门前望而却步；格式混乱、逻辑断层、答案偏差------这些模型推理的顽疾是否也在阻碍你的 AI 产品落地？&lt;/p&gt; 
 &lt;p&gt;本文深入解析 DeepSeek 团队突破性的 GRPO（群组相对策略优化）技术，这项创新将强化学习所需计算资源几乎减半，甚至可以结合 LoRA 在普通消费级 GPU 上进行模型训练。作者通过亲身实践，成功在仅需 16GB 显存的环境下将 1B 参数的 Llama 3.2 转化为推理模型（后续文章会分享相关细节），完全颠覆了传统强化学习的资源需求认知。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;作者 | Greg Schoeninger&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;编译 | 岳扬&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-71410d1cd55685fc17beb84605c865f5851.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;不久前，我们深入研究了 DeepSeek-R1 背后的技术原理，但是没有详细介绍其训练流程中采用的一项名为&quot;群组相对策略优化&quot;（Group Relative Policy Optimization, GRPO）的关键技术。&lt;/p&gt; 
&lt;p&gt;GRPO 本质上是一种旨在提升模型推理能力的强化学习算法。该技术最早发表于其研究论文《DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models》[1]，随后也被应用于 DeepSeek-R1 的后训练阶段。&lt;/p&gt; 
&lt;p&gt;在《DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning》这一论文[2]中，研究团队详细阐述了从基础预训练语言模型到最终推理模型的完整构建路径。虽然之前我们未深入探讨 GRPO 的数学原理和代码实现，但今天这篇文章将全面解析 GRPO 的技术细节，助力各位读者掌握这项技术的核心要义并应用于实际工作。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;01 要点回顾：DeepSeek-R1 如何运用 GRPO 技术&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;为帮助理解，我们首先梳理从基础模型到推理模型的完整训练流程。该流程通过监督式微调（SFT）与群组相对策略优化（GRPO）的交替迭代实现模型能力跃升：&lt;/p&gt; 
&lt;p&gt;1.监督式微调（SFT）阶段&lt;/p&gt; 
&lt;p&gt;a.&lt;strong&gt;冷启动训练&lt;/strong&gt;：采用数千条人工标注的高质量数据微调模型&lt;/p&gt; 
&lt;p&gt;b.&lt;strong&gt;数据验证&lt;/strong&gt;：所有样本均通过人工审核确保可靠性&lt;/p&gt; 
&lt;p&gt;2.GRPO 强化学习阶段&lt;/p&gt; 
&lt;p&gt;a.&lt;strong&gt;推理轨迹训练&lt;/strong&gt; ：引导模型生成结构化推理过程（具有标签的推理轨迹）&lt;/p&gt; 
&lt;p&gt;b.&lt;strong&gt;三重确定性奖励&lt;/strong&gt;：基于格式规范性、逻辑一致性、答案正确性设计奖励机制&lt;/p&gt; 
&lt;p&gt;3.增强型 SFT 阶段&lt;/p&gt; 
&lt;p&gt;a.&lt;strong&gt;合成数据生成&lt;/strong&gt;：创建 80 万条合成训练样本并进行筛选&lt;/p&gt; 
&lt;p&gt;b.&lt;strong&gt;模型自检过滤&lt;/strong&gt;：通过&quot;LLM As A Judge&quot;机制剔除错误响应&lt;/p&gt; 
&lt;p&gt;4.最终 GRPO 对齐阶段&lt;/p&gt; 
&lt;p&gt;a.&lt;strong&gt;价值观校准&lt;/strong&gt;：确保模型输出兼具实用性与安全性&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-a1a3d34d2c90323e9660a599d6baf9ba905.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;在这篇文章中，我们将深入探讨 GRPO 的细节，助您掌握这项推动大模型推理能力突破的关键技术。笔者已开展基于 GRPO 的小模型训练实验，后续将发布完整代码与工程实践细节，通过可复现案例串联理论知识与实际应用。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;02 为什么 GRPO 很重要？&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;TLDR ~ 大幅降低了计算需求且简化了强化学习流程。与 ChatGPT（PPO）使用的基于人类反馈的强化学习（RLHF）相比，所需的计算资源几乎减半。当你结合 LoRA 使用时，即使&quot;GPU poor&quot;（译者注：GPU 的性能不足）也能进行强化学习训练。我试过了，确实有效。我成功地将 1B 参数的 Llama 3.2 模型改造成了仅需 16GB 显存的推理模型。后续文章会分享代码和硬件要求细节。&lt;/p&gt; 
&lt;p&gt;我们只需在云 GPU 服务上花不到 100 美元，就能从自家车库训练推理模型。如果用自己的硬件跑小模型，基本上算是&quot;免费&quot;。其底层原理是什么呢？下一节将讨论从 PPO 到 GRPO 的演变过程。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;03 从 PPO 到 GRPO&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;传闻 ChatGPT 背后的强化学习（RL）技术是 PPO（Proximal Policy Optimization，近端策略优化）。该流程在 InstructGPT 论文[3]中被提出，用于创建能够遵循指令而不仅仅是简单预测下一个单词的模型。&lt;/p&gt; 
&lt;p&gt;训练过程需要收集大量标注数据。对于给定的用户查询，模型需生成多个候选响应，然后由人类或 AI 在循环中对输出进行标注并按质量从优到劣排序。这些数据可用于训练&quot;奖励模型&quot;，其职责是为新接收的提示词计算&quot;奖励值&quot;。该奖励值应体现给定用户查询下模型响应的优劣程度。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-7e2d394a6e114e400146d01db6dc67530d8.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;收集完所有这些经过排序和标注的数据后，即可启动 PPO 来训练大语言模型（LLM）。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;问题在于 PPO 的训练成本可能非常高昂。&lt;/strong&gt; GRPO 论文[1]中的相关图表展示了 PPO 和 GRPO 过程中涉及的不同 LLM。下方蓝色和黄色方框中共有 4 个不同的 LLM。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-89e516de6f1f5de6d7805fc499a831b28e8.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;为了帮助大家理解上图的一些术语，我在这里给出了一些简单的定义：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;策略模型（Policy Model）&lt;/strong&gt; - 对当前正在训练的 LLM 的别称&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;参考模型（Reference Model）&lt;/strong&gt; - 被训练原始 LLM 的冻结版本&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;奖励模型（Reward Model）&lt;/strong&gt; - 基于人类偏好训练的模型（来自上文提到的 InstructGPT 技术）&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;价值模型（Value Model）&lt;/strong&gt; - 试图估算特定动作长期奖励的模型&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h1&gt;&lt;strong&gt;04 通过 GRPO 减少内存使用量&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;在 PPO 算法中，策略模型和价值模型都包含需要通过反向传播进行优化的可训练参数。反向传播过程需要消耗大量内存资源。&lt;/strong&gt; 从上面的架构图可以看出，GRPO 算法移除了价值模型模块。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-909a6a3cd9958e6b2b842610e7167f00efc.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;PPO 算法中混合使用了 4 个大语言模型（LLMs），这些模型都需要消耗大量的内存和计算资源。其中价值模型和奖励模型的参数量通常与正在训练的目标语言模型相当。参考模型通常是训练初期的语言模型的冻结副本。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-73a54d4ed060595c351d14255a2a026dcc4.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;这种实现方法不仅带来高昂的计算成本，还存在诸多需要协调的动态组件，而且还有多个模型需要优化。组件数量越多，通常意味着优化难度越大。GRPO 通过精简架构有效降低了系统复杂度。&lt;/p&gt; 
&lt;p&gt;出于兴趣，我在 H100 上测试了不同参数规模的模型，观察使用 GRPO 进行微调的难易程度。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-f30692cc7db7719142a8d0c6d7bf4b34cda.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;如果想了解具体技术细节，可以查阅相关文档：&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.oxen.ai%2Fblog%2Fgrpo-vram-requirements-for-the-gpu-poor&quot; target=&quot;_blank&quot;&gt;https://www.oxen.ai/blog/grpo-vram-requirements-for-the-gpu-poor&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;若您理解了所有系统需求的来源，就可以开始参与开源项目贡献，或像我最近看到的 trl 仓库的这个 PR 那样，动手优化自己的机器学习库：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-46b6550e48276645bc3fb9fe2f1531ef4a2.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;05 群组相对优势（Group Relative Advantages）&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;在强化学习过程中，我们从语言模型（LLMs）中获取的主要信号是代表&quot;优势&quot;（Advantage）的&quot;A&quot;。这个信号为更新原始语言模型的权重提供了方向指导：&lt;strong&gt;当优势值较高时，我们需要鼓励模型重复当前行为；当优势值较低时，则需要引导模型尝试不同的行为。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在 PPO 算法中，传统价值模型的核心任务是评估生成内容的质量，或者说预测这些内容获得高奖励值（high reward）的可能性。为了完成这项评估工作，需要训练大语言模型作为价值判断模块。那么 GRPO 是如何摆脱对价值模型的依赖的呢？&lt;/p&gt; 
&lt;p&gt;第一个技巧是：&lt;strong&gt;GRPO 不再针对单个查询生成单一输出，而是开始生成多个候选回答。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-e4651272806713abc0c71e0e9e4adb90f96.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;具体来说，如果问题是一道数学题，模型可能会尝试几种不同的解题方法。以下面这个数学问题为例：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Mr. Curtis has 325 chickens on his farm where 28 are roosters and the rest are hens. Twenty hens do not lay eggs while the rest of the hens do. How many egg-laying hens does Mr. Curtis have on his farm?&lt;/p&gt; 
 &lt;p&gt;Curtis 先生的农场有 325 只鸡，其中 28 只是公鸡，其余是母鸡。其中有 20 只母鸡不下蛋，问有多少只产蛋母鸡？&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;模型可能会尝试多种解题思路，有的正确（答案为 227），有的不正确（答案为 305）。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-4f8c57fb9d114d8cd19c1c434f42d31ba6e.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;正确推理路径：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;First, let&#39;s find out how many hens there are. The total number of chickens is 325, and 28 are roosters. So, the number of hens is 325 - 28 = 297. Of these 297 hens, 20 do not lay eggs, so the number of egg-laying hens is 297 - 20 = 277.&lt;/p&gt; 
 &lt;p&gt;277&lt;/p&gt; 
 &lt;p&gt;首先，我们来看看有多少只母鸡。鸡的总数是 325 只，公鸡有 28 只。因此，母鸡的数量是 325 - 28 = 297。在这 297 只母鸡中，有 20 只不下蛋，所以下蛋母鸡的数量是 297 - 20 = 277。&lt;/p&gt; 
 &lt;p&gt;277&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;错误推理路径：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;You need to subtract the 20 hens that do not lay eggs from the total number of hens to find the number of egg-laying hens. So, the number of egg-laying hens is 325 - 20 = 305.&lt;/p&gt; 
 &lt;p&gt;305&lt;/p&gt; 
 &lt;p&gt;您需要从母鸡总数中减去不下蛋的 20 只母鸡，才能求出下蛋母鸡的数量。因此，产蛋鸡的数量为 325 - 20 = 305。&lt;/p&gt; 
 &lt;p&gt;305&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;然后我们对每个输出根据其回答质量计算&quot;奖励值&quot;（reward）。可能存在多个评估不同响应属性的奖励函数。我们暂时将奖励函数视为黑盒，但知道它们会返回数值型结果------如果响应质量较好则数值较高，较差则较低，例如：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Formatting（格式规范度）=1.0&lt;/li&gt; 
 &lt;li&gt;Answer（答案正确性）=0.0&lt;/li&gt; 
 &lt;li&gt;Consistency（逻辑一致性）=0.5&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;当获得所有输出的奖励值 (r) 后，GRPO 通过计算奖励值的均值 μ 和标准差 σ，生成群组相对优势 A。具体公式为：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-64e1b0b82899716c405d7c373fad02574d9.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;这个公式在机器学习特征工程中非常实用，它可以将任意数值归一化为更易学习的正负信号。&lt;/strong&gt; &lt;strong&gt;其直观含义是：&quot;这个数据点偏离平均值多少个标准差？&quot;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;让我们来看几个例子。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-b9a698704de3094e3837dd1ab2500a2a14a.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;若用原生 numpy 代码表示可能如下：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-b22895474b1a709438951615cdb36e7923e.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-cba3c65046eea82fccdf170ae3480b63728.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;再试另一组数值：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-cc74f64ff94df742a05791fc54f6d5685c6.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;通过归一化，将奖励值转换为以均值为中心（0.0）的相对优势值。正值表示优于平均水平，负值表示劣于平均水平。这为我们建立了一套基准：&quot;给定当前提示词，平均响应的质量如何？&quot;在训练过程中，强化表现好的输出（提高其概率），抑制表现差的输出（降低其概率），从而引导模型优化方向。&lt;/p&gt; 
&lt;p&gt;这与传统价值模型的目标相似：预测给定响应的奖励值。由于我们现在训练的是语言模型，只需调整 temperature 参数即可生成多个候选回答，所有生成回答的平均奖励值即可作为衡量当前模型表现的良好信号，以及决定是否需要强化该行为。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;06 KL 散度&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;这个方程的最后一项是 KL 散度项。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-31fdc55549c04177692cffd66e30e71153b.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;无需深入数学细节，这就是我们在训练过程中始终保留&quot;参考模型&quot;的原因。我们不希望新模型偏离原始模型太远，对于每个词元（token），都要确保新模型的预测结果不会与原始模型的预测结果产生过大偏差。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-c452d93871b84a20051519ae616d6a561c3.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;强制执行 KL 散度的直接原因是：初始模型已经具备生成连贯语句和遵循指令的能力。我们不希望新模型通过&quot;奖励欺骗&quot;（reward hack）或利用奖励信号中某些与原始模型不匹配的特性来取巧。&lt;strong&gt;例如，如果模型发现使用&quot;pamplemousse&quot;（葡萄柚的法语，发音有趣且较罕见）这个词能获得高奖励，但该词在预训练阶段并不常用，我们就要阻止模型过度依赖这种用词行为。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;将这些要素整合，就得到了完整的最终方程！&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-9c0e6e909d24b741ba1137deb08f79e43bc.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;或者就像我们值得信赖的&quot;牛人 Eric&quot;说的那样... 这个数学公式看起来比实际复杂...&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-856291aa6b9db5cbd46f3cbcf9d349927ab.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;07 奖励信号机制&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;DeepSeek-R1-Zero 研究的突破性在于，他们通过完全弃用&quot;神经奖励模型&quot;进一步大幅降低了内存消耗。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-6f1e8b8d0e5a2080f1c94fa211bdd425e7c.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;这意味着什么？简而言之，他们直接使用正则表达式（regex）和字符串匹配技术生成奖励信号。&lt;strong&gt;研究团队认为，这种方法既能规避&quot;奖励欺骗&quot;（reward hacking）问题，又能简化整个训练流程。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;如果将前文提到的&quot;准确性奖励（Accuracy Rewards）&quot;和&quot;格式奖励（Format Rewards）&quot;规则转化为代码，其代码实现可能如下所示：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-d0b618e57acda02c9b615ec705c4b7b9119.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;reference:&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgist.github.com%2Fwillccbb%2F4676755236bb08cab5f4e54a0475d6fb&quot; target=&quot;_blank&quot;&gt;https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;训练过程中完全无需引入额外的奖励模型 LLM，仅需保留策略模型和参考模型作为主要内存占用源。将所需 LLM 数量从 4 个削减至 2 个，显著降低了 GPU 资源需求。&lt;/p&gt; 
&lt;p&gt;若你的直觉此时感到不对劲，质疑&quot;这种奖励函数是否具备泛化能力？&quot;，那么你是对的。&lt;strong&gt;这类奖励机制仅在预设的特定任务（如数学推理和格式规范）上表现良好，但无法扩展到其他实用场景。&lt;/strong&gt; 例如，模型可能擅长生成格式的数学解题过程，却无法完成开放式对话或创意写作。&lt;/p&gt; 
&lt;p&gt;我的预测是&quot;苦涩的教训&quot;（The Bitter Lesson）[4]将在此重现：当计算资源和数据量足够时，模型更倾向于自主学习。我们越是减少人工编码规则，让模型自主探索，其表现就越优异。当前 GRPO 的奖励机制仍显人工干预痕迹 ------ 为何不让模型自行学习奖励信号的权重呢？&lt;/p&gt; 
&lt;p&gt;尽管如此，尝试不同的奖励机制其实挺有意思的。&lt;strong&gt;GRPO 的亮点在于：&lt;/strong&gt; &lt;strong&gt;只要能用代码定义奖励函数（输入响应、输出数值），即可基于此进行优化。甚至可以通过外部 API 调用其他 LLM 生成奖励信号。&lt;/strong&gt; 我预感未来几周/月内，因为 GRPO 训练门槛的降低，开发者将开始探索各种创意奖励机制的设计。&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Thanks for reading!&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Hope you have enjoyed and learned new things from this blog!&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;END&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;本期互动内容 🍻&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;❓对于文中提到的&quot;不到 100 美元训练推理模型&quot;，你有何看法？欢迎在评论区畅所欲言。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;🔗文中链接🔗&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;[1]&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2402.03300&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2402.03300&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[2]&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2501.12948&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2501.12948&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[3]&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2203.02155&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2203.02155&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[4]&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fwww.incompleteideas.net%2FIncIdeas%2FBitterLesson.html&quot; target=&quot;_blank&quot;&gt;http://www.incompleteideas.net/IncIdeas/BitterLesson.html&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;原文链接：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fghost.oxen.ai%2Fwhy-grpo-is-important-and-how-it-works%2F&quot; target=&quot;_blank&quot;&gt;https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/&lt;/a&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/IDP/blog/17778588</link>
            <guid isPermaLink="false">https://my.oschina.net/IDP/blog/17778588</guid>
            <pubDate>Thu, 06 Mar 2025 09:49:00 GMT</pubDate>
            <author>原创</author>
        </item>
        <item>
            <title>trustcall —— 基于 LangGraph 的强大工具调用库</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                                                                        
                                                                                    &lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:#1f2328&quot;&gt;当被要求生成或修改大型 JSON blob 时，LLM 会遇到困难。&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;color:#1f2328&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;background-color:#ffffff&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;code&gt;trustcall&lt;/code&gt;可通过&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;color:#1f2328&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;background-color:#ffffff&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;要求 LLM 生成&amp;nbsp;&lt;a href=&quot;https://datatracker.ietf.org/doc/html/rfc6902&quot;&gt;JSON 补丁&lt;/a&gt;操作来解决这个问题。这使得：&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;更快、更便宜地生成结构化输出。&lt;/li&gt;
&lt;li&gt;即使对于复杂的嵌套模式（定义为 pydantic、模式字典或常规 python 函数）也可以弹性重试验证错误&lt;/li&gt;
&lt;li&gt;准确更新现有模式，避免不必要的删除。&lt;/li&gt;
&lt;/ul&gt;

&lt;p style=&quot;text-align:start&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;color:#1f2328&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style=&quot;background-color:#ffffff&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;可灵活适用于多种常见的 LLM 工作流程，例如：&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Extraction&lt;/li&gt;
&lt;li&gt;LLM routing&lt;/li&gt;
&lt;li&gt;Multi-step agent tool use&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;415&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0306/173555_bjJK_4252687.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

                                                                    &lt;/div&gt;
                                                                </description>
            <link>https://www.oschina.net/p/trustcall</link>
            <guid isPermaLink="false">https://www.oschina.net/p/trustcall</guid>
            <pubDate>Thu, 06 Mar 2025 09:37:00 GMT</pubDate>
        </item>
        <item>
            <title>腾讯混元发布并开源图生视频模型，支持生成背景音效及 2K 视频</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;3 月 6 日，腾讯混元&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FaOeJoWyQ78o45KlJnAtAkg&quot; target=&quot;_blank&quot;&gt;宣布&lt;/a&gt;推出图生视频模型并对外开源，同时上线对口型与动作驱动等玩法，并支持生成背景音效及 2K 高质量视频。&lt;/p&gt; 
&lt;p&gt;开源内容包含权重、推理代码和 LoRA 训练代码，支持开发者基于混元训练专属 LoRA 等衍生模型。&lt;/p&gt; 
&lt;p&gt;目前在 Github、HuggingFace 等主流开发者社区均可下载体验。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Github: &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FTencent%2FHunyuanVideo-I2V&quot; target=&quot;_blank&quot;&gt;https://github.com/Tencent/HunyuanVideo-I2V&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Huggingface：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Ftencent%2FHunyuanVideo-I2V&quot; target=&quot;_blank&quot;&gt;https://huggingface.co/tencent/HunyuanVideo-I2V&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;据介绍，基于图生视频的能力，用户只需上传一张图片，并简短描述希望画面如何运动、镜头如何调度等，混元即可按要求让图片动起来，变成 5 秒的短视频，还能自动配上背景音效。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-79aaf27253683e0e75fd797b7842f3f77d1.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;此外，上传一张人物图片，并输入希望「对口型」的文字或音频，图片中的人物即可「说话」或「唱歌」；使用「动作驱动」能力，还能一键生成同款跳舞视频。&lt;/p&gt; 
&lt;p&gt;目前用户通过混元 AI 视频官网即可体验（https://video.hunyuan.tencent.com/），企业和开发者可在腾讯云申请使用 API 接口使用。&lt;/p&gt; 
&lt;p&gt;腾讯混元表示，此次开源的图生视频模型，是混元文生视频模型开源工作的延续，模型总参数量保持 130 亿，模型适用于多种类型的角色和场景，包括写实视频制作、动漫角色甚至 CGI 角色制作的生成。&lt;/p&gt; 
&lt;ul&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/337275</link>
            <guid isPermaLink="false">https://www.oschina.net/news/337275</guid>
            <pubDate>Thu, 06 Mar 2025 08:46:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>Manus 邀请码炒至 6 万元，官方称将逐步有序释放</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;自发布以后，Manus 受到了热烈追捧，网友纷纷涌向 Manus 官网，从而导致页面一度因访问量过大而崩溃。目前，试用 Manus 需要输入邀请码，这导致邀请码一码难求。在二手交易平台上，邀请码的价格被炒至几百元到 6 万元不等。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;448&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-556805b52b9544d5e92e8dd8809b514dcf6.png&quot; width=&quot;300&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;针对邀请码炒作问题，Manus AI 合伙人张涛在社交平台做出了回应。他首先感谢了大家对 Manus 的关注，并澄清了几点重要信息：一是公司从未开设任何付费获取邀请码的渠道；二是从未投入任何市场推广预算；三是内测期间系统容量有限，公司将优先保障现有用户的核心体验，并逐步有序释放邀请码。&lt;/p&gt; 
&lt;p&gt;张涛称，「目前采取邀请码机制，是因为此刻服务器容量确实有限，不得已而为之，团队也熬夜搞了一整天了。希望在接下来的时间里能让更多处在 waitlist 中的用户优先体验 Manus。」&lt;/p&gt; 
&lt;p&gt;「恳请大家对一家几十人的创业公司多一点包容和理解，团队正在全力输出，让大家早日体验上更好的产品。」&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;相关阅读：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p style=&quot;margin-left:0px; margin-right:0px; text-align:start&quot;&gt;&lt;a href=&quot;https://www.oschina.net/news/337193/manus-ai-agent&quot; target=&quot;_blank&quot;&gt;Monica.im 发布 AI Agent 产品「Manus」&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/337267</link>
            <guid isPermaLink="false">https://www.oschina.net/news/337267</guid>
            <pubDate>Thu, 06 Mar 2025 08:19:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>AMD 发布完全开源的 3B 参数语言模型 Instella</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;AMD 今天&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Frocm.blogs.amd.com%2Fartificial-intelligence%2Fintroducing-instella-3B%2FREADME.html&quot; target=&quot;_blank&quot;&gt;发布&lt;/a&gt;&lt;/u&gt;了完全开源的 3B 参数语言模型&amp;nbsp;Instella。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;1614&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0306/155518_eNxW_2720166.png&quot; width=&quot;2188&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;GitHub：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FAMD-AIG-AIMA%2FInstella&quot; target=&quot;_blank&quot;&gt;https://github.com/AMD-AIG-AIMA/Instella&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;AMD 宣称 Instella 代表着&quot;完全开放的最先进的 30 亿参数语言模型 (LM)&quot;。这些模型是在 AMD Instinct MI300X GPU 上训练的。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;通过完全开源 Instella 模型，包括权重、训练超参数、数据集和代码，我们旨在促进人工智能社区内的创新与合作。&lt;/p&gt; 
 &lt;p&gt;我们相信，透明度、可重复性和可访问性是人工智能研究与开发取得进展的关键驱动力。&lt;/p&gt; 
 &lt;p&gt;我们邀请开发人员、研究人员和人工智能爱好者探索 Instella，为其不断改进献计献策，并与我们一起推动语言模型的发展。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;根据 AMD 公布的数据，其性能与 Llama 3.2 3B、Gemma-2 2B 和 Qwen 2.5 3B 等同类产品相比具有很强的竞争力。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-6b27824412274c03ca53d1b47afaedf5831.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/337263/amd-instella-3b</link>
            <guid isPermaLink="false">https://www.oschina.net/news/337263/amd-instella-3b</guid>
            <pubDate>Thu, 06 Mar 2025 07:56:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>优刻得支撑全球首款 AI 多智能体开发团队 MGX 上线</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p style=&quot;color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left&quot;&gt;&lt;strong&gt;近日，全球首款 AI 多智能体开发团队 MGX（MetaGPT X）上线，由优刻得云平台提供核心算力支持&lt;/strong&gt;。作为一个极具创新性的 AI 软件开发团队，在 MGX 平台无需写一行代码，仅说出需求，再通过多位 AI Agent 的智能分工，就能帮助企业及个人用户从 0 到 1 完成应用开发。&lt;/p&gt; 
&lt;p style=&quot;color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left&quot;&gt;MGX 是由厦门深度赋智科技有限公司（下称：DeepWisdom）所倾力打造的一款多智能体编程平台，基于 Multi-Agent（多智能体）开源框架 MetaGPT 带来颠覆性行业变革。&lt;/p&gt; 
&lt;p style=&quot;color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left&quot;&gt;MGX 以多智能体解放社会生产力，其爆火的背后，不仅依托优刻得高性能的训练和推理算力能力加速模型迭代，提升 Agent 协作效能。同时受益于优刻得海外数据中心的资源支持，为 MGX 的全球用户提供低延迟、高稳定的应用服务体验。&lt;/p&gt; 
&lt;p style=&quot;color:#191919; margin-left:1.8em; margin-right:.63em; text-align:center&quot;&gt;&lt;img height=&quot;318&quot; src=&quot;https://oscimg.oschina.net/oscnet//14530db6545281959d72eeb99fc54e56.png&quot; width=&quot;640&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left&quot;&gt;&lt;strong&gt;Agent 领域的国产之光&lt;/strong&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left&quot;&gt;&lt;strong&gt;让开发像「对话」一样简单&lt;/strong&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left&quot;&gt;DeepWisdom 成立于 2019 年，专注于 AI Infra 的开发及商业落地，希望通过搭建 AgentStack（智能体底层架构），完成各领域 Agent 的大规模生产。作为全球首个完全模拟人类软件工作流程的多 Al Agent 开发平台，MGX 实现了&lt;strong&gt;&quot;零代码构建全栈应用&quot;&lt;/strong&gt;的创新突破。&lt;/p&gt; 
&lt;p style=&quot;color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left&quot;&gt;MGX 的核心价值在于将专业软件工程能力转化为对话式的自然交互，通过‌多 Agent 智能体架构‌模拟人类软件团队的完整协作流程，覆盖软件开发的‌需求分析、技术设计、编码实现、测试验证和部署上线‌全生命周期‌，极大地降低了软件开发的门槛。&lt;/p&gt; 
&lt;p style=&quot;color:#191919; margin-left:1.8em; margin-right:.63em; text-align:center&quot;&gt;&lt;img height=&quot;329&quot; src=&quot;https://oscimg.oschina.net/oscnet//d0f91ffd65e7680aedba733e17d3065a.png&quot; width=&quot;640&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left&quot;&gt;MGX 的 AI 团队（产品经理、架构师、工程师、数据分析师等）基于多模态大语言模型构建，&lt;strong&gt;深度融合优刻得 GPU 集群与弹性计算，以及 DeepSeek 等核心产品能力&lt;/strong&gt;，助力 MGX 不仅实现全流程编程自动化，更在多模态决策、动态优化等维度达到人类团队级协作水平，成为您的一个私人软件团队。&lt;/p&gt; 
&lt;p style=&quot;color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left&quot;&gt;依托分布式训练框架和优刻得高性能 GPU 集群支持，MGX 在较短时间内完成了复杂智能体协作逻辑的优化，确保团队内每一位 Agent 都坚守在研发标准操作流程（SOP）的关键岗位上，各司其职：&lt;/p&gt; 
&lt;p style=&quot;color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left&quot;&gt;&lt;strong&gt;01、角色化分工与实时协作‌&lt;/strong&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left&quot;&gt;各 Agent 通过自然语言交互共享上下文信息，例如架构师修改技术方案后，工程师 Agent 同步更新代码逻辑，测试 Agent 即时调整验证策略‌。采用 SOP 机制，确保需求→设计→代码→测试的流程一致性，规避人工传递偏差‌。&lt;/p&gt; 
&lt;p style=&quot;color:#191919; margin-left:1.8em; margin-right:.63em; text-align:center&quot;&gt;&lt;img height=&quot;231&quot; src=&quot;https://oscimg.oschina.net/oscnet//2155e14153f515039f522a00f4a5869c.png&quot; width=&quot;640&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left&quot;&gt;&lt;strong&gt;02、动态决策与知识共享&lt;/strong&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left&quot;&gt;智能体基于优刻得 GPU 集群的并行计算能力，实时调用行业知识库，实现跨项目经验复用‌。通过多模态大模型融合文本、代码与数据，工程师 Agent 可自主识别设计文档中的潜在冲突并提出优化建议‌。&lt;/p&gt; 
&lt;p style=&quot;color:#191919; margin-left:1.8em; margin-right:.63em; text-align:center&quot;&gt;&lt;img height=&quot;354&quot; src=&quot;https://oscimg.oschina.net/oscnet//e570d4365e883261c360fbc026a71a88.png&quot; width=&quot;640&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left&quot;&gt;&lt;strong&gt;03、全流程透明化与可控性&lt;/strong&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left&quot;&gt;基于优刻得云平台的数据管理能力，MGX 记录每个 Agent 的操作日志并生成可视化报告，用户可追溯代码生成路径、测试覆盖率等关键指标，满足企业审计要求‌。支持人工介入调整（如手动修改 PRD 或架构图），智能体自动同步变更至下游环节，保持流程连贯性‌。&lt;/p&gt; 
&lt;p style=&quot;color:#191919; margin-left:1.8em; margin-right:.63em; text-align:center&quot;&gt;&lt;img height=&quot;300&quot; src=&quot;https://oscimg.oschina.net/oscnet//cbc50a405f8446132208c92e5345d078.png&quot; width=&quot;640&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left&quot;&gt;优刻得在全球 24 个地域设立了 31 大可用区，遍及东南亚、中东、非洲、欧洲、北美、南美等多个关键区域，打造出一个规模庞大、运行高效的全球智算网络，可靠支持 MGX 全球用户的流畅访问。海内外地区的用户可借助就近的智算中心快速接入，享受本地低延时的模型调用体验，让整个 AI 开发工作的高效与精准，同时保障了多智能体协作场景下的高并发需求。&lt;/p&gt; 
&lt;p style=&quot;color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left&quot;&gt;MGX 的成功上线，让传统软件开发中所面临的编程难、周期长、成本高等痛点被一一化解，同时也标志着优刻得智算平台在 AI 工程化领域的深度实践。未来，双方将进一步探索基于 AI 智算云的智能体开发和协作，为企业提供更智能、更敏捷的 AI 开发新范式。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/337261</link>
            <guid isPermaLink="false">https://www.oschina.net/news/337261</guid>
            <pubDate>Thu, 06 Mar 2025 07:54:00 GMT</pubDate>
            <author>来源: 投稿</author>
        </item>
        <item>
            <title>智源开源多模态向量模型 BGE-VL</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;智源研究院宣布联合多所高校开发了多模态向量模型 BGE-VL，进一步扩充了原有生态体系。BGE-VL 在图文检索、组合图像检索等主要多模态检索任务中均取得了最佳效果。&lt;/p&gt; 
&lt;p&gt;BGE-VL 借助大规模合成数据 MegaPairs 训练而成。这一设计具备以下两大核心优势:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;优异的可扩展性：&lt;/strong&gt;MegaPairs 结合多模态表征模型、多模态大模型和大语言模型，在海量图文语料库中高效挖掘多模态三元组数据。其算法能够以极低成本持续生成多样化且高质量的多模态三元组。本次发布的版本涵盖 2600 万条样本，为多模态检索模型的训练提供了大规模、高价值的数据支持。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;卓越的数据质量：&lt;/strong&gt;相较于传统人工标注数据，MegaPairs 仅需 1/70 的数据量即可实现更优的训练效果。利用该合成数据，智源训练了多模态检索模型 BGE-VL，显著提升了多个主流多模态检索基准的性能。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;BGE-VL 的技术报告已发布，相关数据、模型及代码资源将陆续向社区全面开放。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li style=&quot;text-align:justify&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;论文地址：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2412.14475&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2412.14475&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; 
 &lt;li style=&quot;text-align:justify&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;项目主页：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FVectorSpaceLab%2FMegaPairs&quot; target=&quot;_blank&quot;&gt;https://github.com/VectorSpaceLab/MegaPairs&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; 
 &lt;li style=&quot;text-align:justify&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;模型地址：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2FBAAI%2FBGE-VL-MLLM-S1&quot; target=&quot;_blank&quot;&gt;https://huggingface.co/BAAI/BGE-VL-MLLM-S1&lt;/a&gt;&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/337258</link>
            <guid isPermaLink="false">https://www.oschina.net/news/337258</guid>
            <pubDate>Thu, 06 Mar 2025 07:38:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>vivo OS 部门设立 AI 领域板块</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FbXnSMuj_jA5V9BeIDYuJkw&quot; target=&quot;_blank&quot;&gt;据雷锋网独家消息&lt;/a&gt;&lt;/u&gt;，vivo 近日进行了组织架构调整，其中其 AI 领域有了新的变动。&lt;/p&gt; 
&lt;p&gt;具体来看，vivo 原 OS 产品领域下将设立 AI 领域，人工智能一部、人工智能二部划入 AI 领域。原互联网平台运营领域总经理张飞被调任 AI 领域总经理，并兼管人工智能一部，无考察期，直接向公司副总裁、OS 产品领域负责人周围汇报。而原人工智能一部总经理肖方旭已于 1 月份离职。&lt;/p&gt; 
&lt;p&gt;据 vivo 员工透露，公司在 AI 大模型方面投入巨大，前期管理意志干预很重，可实际看来技术进展缓慢，此事早在去年内部就有过讨论，最终结果是暂时不做商业化考核，但暂停了对资金的投入。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;报道指出，目前 vivo 的大模型训练重心正在向端侧转移，云端的 700 亿参数大语言模型还在微调和优化中，暂停了该模型的预训练工作&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;公开资料显示，vivo 每年都会投入 20-30 亿用于大模型研发。截至 2024 年 10 月，vivo 在 AI 领域的投入已经超过 230 亿元，且 AI 研究院的研发人员数量也从 2019 年的 1 千人增加至 2 千多人，是目前公开披露 AI 投入最高的手机厂商之一。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/337257</link>
            <guid isPermaLink="false">https://www.oschina.net/news/337257</guid>
            <pubDate>Thu, 06 Mar 2025 07:36:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>微信月活突破 10 亿</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;QuestMobile 近日发布了&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F-mdl9gcCNmfLotd87SOIFg&quot; target=&quot;_blank&quot;&gt;2024 年度中国移动互联网实力价值榜&lt;/a&gt;&lt;/u&gt;，TOP50 赛道用户规模 NO.1 APP 如下。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;2284&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0306/152504_luN0_2720166.png&quot; width=&quot;800&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;本榜单体现了互联网行业 50 个细分赛道的第一名，微信在即时通讯位列第一，&lt;strong&gt;月活唯一突破 10 亿级，达到 10.8 亿&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;综合电商方面，淘宝以 9.6 亿月活排名第一。短视频方面的第一是抖音，月活 8.4 亿。&lt;/p&gt; 
&lt;p&gt;从 50 个 APP 所属的集团来看：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;阿里旗下有 8 款：淘宝、高德地图、支付宝、钉钉、闲鱼、饿了么、菜鸟、盒马。&lt;/li&gt; 
 &lt;li&gt;腾讯旗下有 7 款：微信、搜狗输入法、腾讯视频、QQ 浏览器、酷狗音乐、王者荣耀、QQ 邮箱。&lt;/li&gt; 
 &lt;li&gt;字节旗下有 6 款：抖音、今日头条、番茄免费小说、剪映、番茄畅听、豆包。&lt;/li&gt; 
 &lt;li&gt;百度旗下有 2 款：百度、百度网盘。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;App 规模增长千万级榜单如下：&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;1548&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0306/152718_x9Xf_2720166.png&quot; width=&quot;800&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;前十名的千万级体量 APP 增速排行榜中，字节旗下产品占据七夕，分别是：抖音商城、豆包、悟空浏览器、红果免费短剧、抖音精选、汽水音乐、番茄畅听音乐版，可见字节流量之猛。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/337256</link>
            <guid isPermaLink="false">https://www.oschina.net/news/337256</guid>
            <pubDate>Thu, 06 Mar 2025 07:27:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>谷歌搜索测试「AI Mode」：整合多模态和实时信息、一键解答复杂问题</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;谷歌公司昨日&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.google%2Fproducts%2Fsearch%2Fai-mode-search%2F&quot; target=&quot;_blank&quot;&gt;发布博文&lt;/a&gt;，邀请谷歌搜索用户测试全新的&lt;strong&gt;「AI 模式」（AI Mode）&lt;/strong&gt;。用户可以提出更复杂的问题，并基于搜索结果，AI 生成更详细、更直观的答案。&lt;/p&gt; 
&lt;p&gt;谷歌表示，AI 模式将提供更高级的推理、思考和多模态能力，帮助用户更高效地获取信息。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;540&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0306/150029_Fzwi_2720166.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;谷歌表示，以往用户在处理复杂问题时，往往需要多次搜索才能解决，而「AI 模式」能够解决这个痛点。用户只需在桌面或移动设备上输入查询，点击新的「AI 模式」按钮即可体验。&lt;/p&gt; 
&lt;p&gt;此外，AI 模式页面底部还提供了「深入探索」快捷入口，用户可直接跳过常规搜索结果，专注于 AI 生成的内容。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-39952b2fa3ffbf6ae0dc3082083d80b34c1.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;在移动设备上，用户可以通过上传图片或语音输入查询，但目前仅支持文本输出。AI 模式还支持历史搜索记录，方便用户查看过往查询。&lt;/p&gt; 
&lt;p&gt;AI 模式由定制版的 Gemini 2.0 驱动，能够访问实时数据源和知识图谱等资源。它通过「查询扩展」技术，从多个子主题和数据源中提取信息，并综合呈现。如果信息不足，用户将被引导至网页搜索结果。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;案例 1：鸟类迁徙路径&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;用户提问：「候鸟如何知道迁徙路线？」AI 模式会进行多步搜索并组织结果，在移动设备上以轮播形式展示来源网站，随后提供简明答案和相关文章。&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;案例 2：户外拍摄最佳时间&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;用户询问：「本周在波士顿公共花园拍摄户外订婚照的最佳时间是什么？」AI 模式结合实时天气信息，推荐具体日期和黄金时段，并注明日落时间。&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;案例 3：睡眠追踪设备对比&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;用户提问：「智能戒指、智能手表和追踪垫在睡眠追踪功能上有何区别？」AI 模式以对比表格形式呈现答案，并支持后续问题，如「深度睡眠时心率如何变化？」&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;从早期测试来看，AI 模式的查询长度是传统搜索的两倍，用户有 25% 的时间会进行后续提问。谷歌计划逐步向所有用户开放这一功能，目前测试主要面向高级用户。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/337252/google-ai-mode-search</link>
            <guid isPermaLink="false">https://www.oschina.net/news/337252/google-ai-mode-search</guid>
            <pubDate>Thu, 06 Mar 2025 07:07:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>MongoDB 终于实现盈利，但股价因业绩预期不佳而暴跌</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;MongoDB 公司&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Finvestors.mongodb.com%2Fnews-releases%2Fnews-release-details%2Fmongodb-inc-announces-fourth-quarter-and-full-year-fiscal-2025&quot; target=&quot;_blank&quot;&gt;公布&lt;/a&gt;了 2025 财年第四季度业绩，终于实现了季度盈利，超出了华尔街对盈利和收入的目标。但该公司对新财年的预期却令人大失所望，导致投资者纷纷逃离，其股价在尾盘交易中暴跌。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;该公司报告称，扣除股票薪酬等某些成本前的每股收益为 1.28 美元，营收为 5.484 亿美元，比去年同期增长 20%。这些数字远远超出了分析师的预期，分析师此前预计该公司每股收益仅为 60 美分，销售额为 5.21 亿美元。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;本季度订阅收入增长了 19%，服务收入增长了 34%，公司继续以惊人的速度增加新客户，本季度结束时新客户数量已超过 54,500 名。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;总而言之，该公司的净收入为 1580 万美元 —— 虽然利润不高，但要好于一年前的 5550 万美元净亏损。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;270&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-09822091507c54b3e53dd83a09a400a41e2.png&quot; width=&quot;700&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;然而尽管这些数据很稳健，该公司对新财年的预期却令人失望。MongoDB 表示，预计每股收益在 2.44 美元至 2.62 美元之间，远低于华尔街 3.38 美元的目标。在收入方面，该公司预计收入在 22.4 亿美元至 22.8 亿美元之间，低于华尔街 23.3 亿美元的普遍预期。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;在报告发布之前，MongoDB 股价当天早些时候上涨了 3% 以上，但投资者因预期下调而放弃交易。盘后，该股暴跌逾 16%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;MongoDB 最近还进行了一项重要收购，收购了一家名为 Voyage AI Inc. 的初创公司，「支持下一代 AI 应用的先进嵌入和重新排序模型的先驱」。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;MongoDB 总裁兼首席执行官 Dev Ittycheria 在谈到此次收购时表示：「收购 Voyage AI 之后，我们将实时数据、复杂的嵌入和检索模型以及语义搜索直接结合在数据库中，简化了可信赖的人工智能应用程序的开发。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;尽管盘后下跌，但 MongoDB 股价今年迄今仍上涨逾 13%。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/337249/mongodb-fourth-quarter-and-full-year-fiscal-2025</link>
            <guid isPermaLink="false">https://www.oschina.net/news/337249/mongodb-fourth-quarter-and-full-year-fiscal-2025</guid>
            <pubDate>Thu, 06 Mar 2025 06:56:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>微信「史诗级」更新：新增「清理原图 / 视频」功能</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;在最新版本的 iOS 和 Android 微信版本中，微信热更新了一个针对清理存储空间缓存的功能，在「设置 – 通用 – 存储空间」当中，可以查看已接收和已发出的原图、原视频。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-14c220568032f76d5204db7983e2d087abf.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;该界面顶部还有一行小字写着「清理原图、原视频可以节省存储空间」。在清理后，用户仍可在聊天中看到普通画质的图片、视频。&lt;/p&gt; 
&lt;p&gt;在原图、原视频点开之后右侧选项之后，会弹出「按文件大小查看」或「按聊天大小查看」，进而按照大小、时间、类型查看具体的原图、原视频，自行选择需要清理的内容。&lt;/p&gt; 
&lt;p&gt;经网友实测，清理了「原图」之后，被清理的内容仍会在聊天记录中显示，但画质已降低。该功能既能避免「图片已过期」，还能腾出存储空间，实现微信瘦身。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/337248</link>
            <guid isPermaLink="false">https://www.oschina.net/news/337248</guid>
            <pubDate>Thu, 06 Mar 2025 06:44:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>OpenAI 拟推月费 2 万美元的博士级 AI Agent</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.theinformation.com%2Farticles%2Fopenai-plots-charging-20-000-a-month-for-phd-level-agents&quot; target=&quot;_blank&quot;&gt;The Information&lt;/a&gt;&lt;span style=&quot;color:#000000&quot;&gt;&amp;nbsp;消息称，OpenAI 计划对达到博士水平的 AI Agent 每月收取高达 2 万美元的费用。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;282&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-eab847f1c359ba1e53fa20ee5c01f5d983f.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;据悉，OpenAI 计划针对不同应用推出几款不同类型的 AI Agent 产品，包括对销售线索进行分类和排名以及软件工程。除了最昂贵的这款每月 2 万美元，旨在支持 「博士级研究」的；还有一款是「high-income knowledge worker」 agent，每月收费 2,000 美元。另一款是软件开发人员代理，每月收费 10,000 美元。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;目前还不清楚这些代理工具何时推出，也不清楚哪些客户有资格购买这些工具。但 The Information 指出，OpenAI 的投资者软银承诺，仅今年一年就将在 OpenAI 的 agent 产品上投资 30 亿美元。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/337244/openai-20000-phd-level-agents</link>
            <guid isPermaLink="false">https://www.oschina.net/news/337244/openai-20000-phd-level-agents</guid>
            <pubDate>Thu, 06 Mar 2025 06:32:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>3 月 22 日，南京，聊聊生成式 AI 应⽤构建</title>
            <description></description>
            <link>https://www.oschina.net/event/2423811</link>
            <guid isPermaLink="false">https://www.oschina.net/event/2423811</guid>
            <pubDate>Thu, 06 Mar 2025 06:29:00 GMT</pubDate>
        </item>
        <item>
            <title>在线老虎机游戏 UI 漏洞导致博彩公司损失近 100 万英镑</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.bbc.co.uk%2Fnews%2Farticles%2Fcx2gl2n2n14o&quot; target=&quot;_blank&quot;&gt;BBC&lt;/a&gt; 报道称，&lt;/span&gt;&lt;span style=&quot;color:#000000&quot;&gt;2020 年 10 月，一位来自英国英格兰格洛斯特郡的园丁 Corrine Durber 在玩 Wild Hatter（爱尔兰体育博彩公司 Paddy Power 旗下的一款线上老虎机博彩游戏）游戏时，结算页面显示她赢得了「Monster Jackpot」奖项，金额高达 1,097,132.71 英镑。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;但 Paddy Power 却以该用户实际中只是中了&quot;Daily Jackpot&quot;为由，仅支付了 20,265 英镑，称差额归因于游戏显示界面的编程错误。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;对此，Durber 以屏幕上显示的内容为依据，起诉了 Paddy Power 和 Betfair 的母公司 PPB Entertainment Limited，指控其违约并要求支付剩余的奖金。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;在当地时间本周三的判决中，法官作出了有利于 Durber 的简易判决：「当商家因自身的鲁莽、疏忽、错误、数字服务不足和测试不足而将所有风险转嫁给消费者时，这在我看来是不合理的。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;393&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-402d7ef5c362952bcd0a58a6c7077403ca7.png&quot; width=&quot;700&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Corrine Durber 与丈夫 Colin（左）和律师 Peter Coyle（右）在高等法院外合影&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Durber 在接受采访时表示，这笔钱将改变她家人的生活。「显然，这将用于照顾孩子们，我们会帮他们还清抵押贷款，并享受我们的退休生活」。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;PPB 曾辩称，游戏结果是由随机数生成器决定的，该生成器显示 Durber 只赢得了&quot;Daily Jackpot&quot;；但因为 bug 影响了游戏结算动画，导致显示了错误的结果。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;对此，法官则表示，「所见即所得」的理念是游戏的核心。他在一份 62 页的裁决中继续写道：「客观地说，顾客会希望并期待屏幕上显示的内容是准确和正确的。当顾客进入实体赌场玩轮盘赌时，可能也会有同样的期望。如果他们押注 13 号，而球落在 13 号上，他们期望赌场会支付奖金。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;法官发现，由于软件映射中的人为错误，随机数生成器的结果与屏幕上的结果不同，这一 bug 共影响了 48 天内的 14 次游戏。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;裁决作出后，Paddy Power 的一位发言人表示：「我们始终努力提供最佳的客户体验，并以公平为荣。我们对这起不幸的案件深表遗憾，并正在对判决进行复审。」&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/337229</link>
            <guid isPermaLink="false">https://www.oschina.net/news/337229</guid>
            <pubDate>Thu, 06 Mar 2025 05:52:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>deepin 社区 2 月报 | deepin 25 冲刺 Alpha、全面接入 DeepSeek</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p style=&quot;margin-left:0; margin-right:0; text-align:center&quot;&gt;&lt;strong&gt;2025 年 2 月社区数据总览&lt;/strong&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;img height=&quot;608&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-ca62af4f9c2de9f28fa5700668bbecd4660.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:center&quot;&gt;&lt;strong&gt;社区产品&lt;/strong&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0&quot;&gt;&lt;span&gt;&lt;strong&gt;deepin 25 开发进展&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#3e3e3e; margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;自 deepin 25 Preview 版本发布以来，社区用户反响热烈，积极参与测试体验并踊跃提交反馈。通过 deepin 25 Preview 用户访谈活动，我们收集了大量优化建议与 bug 报告，所有内容已分类整理并纳入开发优先级清单。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#3e3e3e; margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;截至 2 月底，社区团队共标记高优先级待修复 bug 418 个，其中 &lt;strong&gt;210 个已成功修复并关闭&lt;/strong&gt;。当前开发团队正全力推进剩余问题攻坚，&lt;strong&gt;计划于 3 月中旬发布 deepin 25 Alpha 版本&lt;/strong&gt;。与此同时在我们的访谈活动上，社区产品经理已同步公布版本迭代计划，明确开发里程碑与目标，确保进度透明化。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;img height=&quot;607&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-5057bc838b32daac87ec9a1302da7f143d2.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;span style=&quot;background-color:#ffffff; color:#000000&quot;&gt;我们将持续以用户反馈为驱动，打磨产品体验，为社区呈现更完善的 deepin 25 操作系统。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0&quot;&gt;&lt;span&gt;&lt;strong&gt;deepin 23 更新&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:#000000&quot;&gt;2025 年 2 月，deepin 23 更新 4 次（内测 3 次、正式更新 1 次），更新聚焦于&lt;strong&gt;系统安全加固、内核与核心组件升级&lt;/strong&gt;，并重点解决了企业微信兼容性、多媒体功能稳定性及用户交互中的高频问题，进一步优化了生产力和娱乐场景下的使用体验。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#3e3e3e; margin-left:0; margin-right:0&quot;&gt;&lt;span style=&quot;background-color:#ffffff; color:#000000&quot;&gt;社区产品 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzA5NzE0Mjg4Ng%3D%3D%26mid%3D2650455294%26idx%3D1%26sn%3Dc865573d9afd351626fa2fe6a7656372%26scene%3D21%23wechat_redirect&quot; target=&quot;_blank&quot;&gt;UOS AI 成功接入 DeepSeek 大模型&lt;/a&gt;，为用户提供&lt;strong&gt;本地离线部署&lt;/strong&gt;与&lt;strong&gt;在线 API 接入&lt;/strong&gt;两种灵活方式，显著提升了 AI 工具的可用性与响应效率。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:#000000&quot;&gt;这一技术升级吸引了社区用户的高度关注与深度参与，用户积极体验新功能，并踊跃分享使用心得与反馈，形成了良好的技术交流氛围。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0&quot;&gt;&lt;span&gt;&lt;strong&gt;深度之家&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#3e3e3e; margin-left:0; margin-right:0&quot;&gt;&lt;span style=&quot;background-color:#ffffff; color:#000000&quot;&gt;2025 年 2 月，深度之家共收到用户 bug 及需求反馈记录共计 190 条，其中：&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#3e3e3e; margin-left:0; margin-right:0&quot;&gt;&lt;span style=&quot;background-color:#ffffff; color:#000000&quot;&gt;需求反馈记录 35 条：已完成需求 2 个，已采纳接受需求 6 个，已流转回复 6 个。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:center&quot;&gt;&lt;strong&gt;社区 SIG 进展&lt;/strong&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0&quot;&gt;&lt;span&gt;&lt;strong&gt;deepin-kernel SIG&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0&quot;&gt;&amp;nbsp;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span&gt;更新双线内核至 6.6.79 （HWE） 及 6.12.13 （Rolling）；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span&gt;新增 Intel Kaby Lake（七代）及更新平台的 uncore-frequency（非核心部件）频率控制支持，可改善能源效率及性能；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span&gt;优化 STMMAC（多见于龙架构及飞腾平台）的性能；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span&gt;提升联发科 （MediaTek） 蓝牙及无线网卡稳定性；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span&gt;精简 ARM 平台配置；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span&gt;修复 ARM、龙架构平台无法从 S4 唤醒的问题；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span&gt;新增龙芯 3C6000 平台支持；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span&gt;修复龙芯 3A6000 部分笔记本型号触摸板不能使用的问题；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span&gt;回合部分来自上游的 Intel 平台支持、Wi-Fi、蓝牙、I2C 及调度器修复；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span&gt;回合部分上游龙芯修复。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0&quot;&gt;&lt;span&gt;&lt;strong&gt;deepin-sysdev-team&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;CVE 安全漏洞更新；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;mesa 24.3 及其相关依赖更新；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;补齐 gcc-12 交叉编译功工具链；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;util-linux 版本更新至 2.40.4；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;新增 usbguard 及其相关依赖包；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;opensbi 更新至 1.6 版本；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;lightdm 更新添加 Qt6 支持；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;glibc 合入兆芯相关 patch；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;sway 更新至 1.10.1；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;新增 gcc-riscv64-unknown-elf、binutils-riscv64-unknown-elf；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;edk2 添加 LoongArch64 架构支持；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;gcc-12 合并上游 LoongArch64 上游补丁；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;修复 mkosi 依赖问题，解决 LoongArch64 安装问题。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0&quot;&gt;&lt;span&gt;&lt;strong&gt;DDE SIG&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0&quot;&gt;&amp;nbsp;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;修正控制中心涉及的部分多语言支持问题；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;启动器自由排序模式拖拽时增加挤动效果；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;启动器增加多音字搜索支持；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;修正部分情况下通知气泡可能不消失的问题；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;修正控制中心蓝牙、网络、更新等模块样式与行为问题；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;修正全屏启动器不跟随 dock 所在屏幕展示的问题；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;修正任务栏应用数量过多时，图标未缩小展示的问题；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;修正任务栏应用图标在特定尺寸情况下，位置可能不正确的问题；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;修正快捷面板主题色未跟随任务栏主题色切换的问题；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;调整托盘区域拖拽时的鼠标偏移，避免鼠标指示遮挡被拖拽的图标；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;修正通知中心通知有时无法及时消失的问题；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;修正控制中心显示、生物认证、个性化、更新等模块的现存界面与稳定性问题。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0&quot;&gt;&lt;span&gt;&lt;strong&gt;dde-port SIG&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0&quot;&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;根据系统底层包更新，维护对应发行版 DDE 相关软件包的可构建状态，改善可移植性。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0&quot;&gt;&lt;span&gt;&lt;strong&gt;deepin LoongArch SIG&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0&quot;&gt;&amp;nbsp;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p style=&quot;color:#3e3e3e; margin-left:0; margin-right:0&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;新增对龙芯 3C6000 平台 AVEC IRQ 控制器的支持，该版更新后 deepin 6.6 内核可配合 3C6000 机型使用；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style=&quot;color:#3e3e3e; margin-left:0; margin-right:0&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;修复攀升 3A6000 笔记本无法使用触摸板的问题；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style=&quot;color:#3e3e3e; margin-left:0; margin-right:0&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;修复 HWMon 驱动对处理器封装数计算不正确的问题；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style=&quot;color:#3e3e3e; margin-left:0; margin-right:0&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;开启 PCIe Port H 的 MSI 支持；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style=&quot;color:#3e3e3e; margin-left:0; margin-right:0&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;修复低电耗模式下 PCI 设备可能无法访问的问题；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style=&quot;color:#3e3e3e; margin-left:0; margin-right:0&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;修复调频驱动未正确刷新处理器频率的问题；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style=&quot;color:#3e3e3e; margin-left:0; margin-right:0&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;修正 DWMAC 驱动重置超时的问题；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style=&quot;color:#3e3e3e; margin-left:0; margin-right:0&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;修复部分未定义 _DMA 方法的 BPI01001 固件（「旧世界」第二版）下的启动问题；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style=&quot;color:#3e3e3e; margin-left:0; margin-right:0&quot;&gt;&lt;span style=&quot;color:rgba(0, 0, 0, 0.9)&quot;&gt;回合来自 6.7-6.14 的所有平台修复补丁。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:center&quot;&gt;&lt;strong&gt;社区生态&lt;/strong&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#3e3e3e; margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;2 月份，应用商店完成近 300 次应用审核测试，成功上架 162 款应用，其中 41 款全新应用，121 款应用完成更新升级。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#3e3e3e; margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;在重点应用更新方面，微信、企业微信、GIMP 等均完成小版本迭代，全方位提升用户体验。应用商店生态共建小组成员积极推进适配工作，对 Cherry Studio 及 Chatbox 多模型 AI 助手进行了更新和玲珑转制。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#3e3e3e; margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;欢迎大家通过&lt;/span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzA5NzE0Mjg4Ng%3D%3D%26mid%3D2650435695%26idx%3D1%26sn%3Dd924d485e64800cd6b4386a5029132ff%26scene%3D21%23wechat_redirect&quot; target=&quot;_blank&quot;&gt; deepin 社区投递平台&lt;/a&gt;&lt;span style=&quot;color:#000000&quot;&gt;进行应用投递，共同维护和丰富我们的社区应用生态。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:center&quot;&gt;&lt;strong&gt;社区论坛&lt;/strong&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#3e3e3e; margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;2 月 deepin 论坛共发布主题帖 764 个、回帖 4548 个；共有 669 位新朋友加入到 deepin 论坛。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:center&quot;&gt;&lt;strong&gt;社区海外发展&lt;/strong&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0&quot;&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0&quot;&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style=&quot;color:#3e3e3e; margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;deepin 厄瓜多尔分支社区持续推进 deepin 在南美洲地区的本地化进程。其中，社区发起的 Killa OS Project（Kichwa 语翻译项目）获得重大进展，该项目不仅获得当地政府的高度重视和支持，已完成在市政厅的法律认证流程，更于近期获得移动实验室的硬件支持。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;接下来，该实验室将全面部署 deepin 操作系统，为项目的后续开发与推广提供坚实保障。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;margin-left:0; margin-right:0; text-align:center&quot;&gt;&lt;strong&gt;社区鸣谢&lt;/strong&gt;&lt;/p&gt; 
&lt;p style=&quot;text-align:center&quot;&gt;&lt;img height=&quot;639&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-6c36fd5108b9409b1c6ec7d7e0f39dbc9c5.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/337227</link>
            <guid isPermaLink="false">https://www.oschina.net/news/337227</guid>
            <pubDate>Thu, 06 Mar 2025 05:43:00 GMT</pubDate>
            <author>来源: 投稿</author>
        </item>
        <item>
            <title>史上最强芯片：苹果 M3 Ultra 支持 512 GB 统一内存、可本地部署满血版 DeepSeek R1</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;苹果昨天&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.apple.com.cn%2Fnewsroom%2F2025%2F03%2Fapple-reveals-m3-ultra-taking-apple-silicon-to-a-new-extreme%2F&quot; target=&quot;_blank&quot;&gt;正式发布&lt;/a&gt;&lt;/u&gt;了迄今打造的最强芯片&amp;nbsp;&lt;span&gt;M3 Ultra —— 将 Apple 芯片性能提升至新极限。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;新芯片性能比 M1 Ultra 提升最多达 2.6 倍，最高支持 512 GB 统一内存，创个人电脑内存新高，此外还&lt;/span&gt;配备了 Mac 性能最强劲的中央处理器和图形处理器，神经网络引擎核心数量翻倍。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-d1d71f48b7a0d7ae41433386bb9e2aa9072.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;M3 Ultra 芯片亮点&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;起步内存 96GB，最高可配置 512GB 内存&lt;/li&gt; 
 &lt;li&gt;内部共集成 1,840 亿个晶体管&lt;/li&gt; 
 &lt;li&gt;支持雷雳 5 连接，数据传输速度最高可达 120 Gb/s，比雷雳 4 提升达 2 倍以上&lt;/li&gt; 
 &lt;li&gt;配备最多 32 核中央处理器，包括 24 颗性能核心和 8 颗能效核心，性能最高可达 M2 Ultra 的 1.5 倍，M1 Ultra 的 1.8 倍&lt;/li&gt; 
 &lt;li&gt;拥有 Apple 芯片中最强的图形处理器，包括最多 80 颗图形处理核心，性能比 M2 Ultra 提升最多达 2 倍，比 M1 Ultra 提升最多达 2.6 倍&lt;/li&gt; 
 &lt;li&gt;采用创新的 UltraFusion 封装架构，通过超过 10,000 个高速连接点，将两枚 M3 Max 晶粒整合在一起，可同时传输超过 10,000 个信号，带来超过 2.5TB/s 的低延迟片间带宽，提供低延迟和高带宽的传输能力&lt;/li&gt; 
 &lt;li&gt;提供了专属的硬件加速 H.264、HEVC 与四个 ProRes 编解码引擎，能够播放最多可达 22 条 8K ProRes 422 视频流。&lt;/li&gt; 
 &lt;li&gt;显示引擎支持最多 8 台 Pro Display XDR，呈现超过 1.6 亿颗像素&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.apple.com.cn%2Fnewsroom%2F2025%2F03%2Fapple-reveals-m3-ultra-taking-apple-silicon-to-a-new-extreme%2F&quot; target=&quot;_blank&quot;&gt;详情查看官方介绍&lt;/a&gt;&lt;/u&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;相关阅读：&lt;a href=&quot;https://www.oschina.net/news/337201/apple-unveils-new-mac-studio-m3-ultra&quot; target=&quot;news&quot;&gt;苹果发布「核弹级」 Mac Studio：顶配售价超 10 万、最强处理器 M3 Ultra 正式亮相&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/337215/apple-m3-ultra-new-extreme</link>
            <guid isPermaLink="false">https://www.oschina.net/news/337215/apple-m3-ultra-new-extreme</guid>
            <pubDate>Thu, 06 Mar 2025 03:56:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>LSM-TREE 从入门到入魔：从零开始实现一个高性能键值存储</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                    
                                                                                                                                                    &lt;h1&gt;一、引，言&lt;/h1&gt; 
&lt;p&gt;LSM-Tree（Log-Structured Merge Tree）是一种高效的键值存储数据结构，广泛应用于 NoSQL 数据库和大数据处理系统中。其核心思想是通过分层、有序地利用磁盘顺序写入的性能优势，优化写入操作，同时牺牲部分读取性能以换取更高的写入吞吐量。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//3983af5521bbb144f98321026a45d75a.jpeg&quot; alt=&quot;引言.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt; &lt;img src=&quot;https://oscimg.oschina.net/oscnet//e78c25208e08de7c918d0a3ff99efdfe.jpeg&quot; alt=&quot;引言 2.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 在互联网的各个基础设施中，不论是数据库还是缓存亦或是大数据框架，LSM-Tree 这个数据结构都是很常见的身影。&lt;/p&gt; 
&lt;p&gt;我每天都在使用这个存储引擎，但是对它的了解还流于表面，所以我想要自己实现一次 LSM-Tree 加深理解。&lt;/p&gt; 
&lt;p&gt;本次实现我们采用了 Zig 语言，简要的实现 LSM-Tree 的核心功能（读写、数据压缩、持久化，不包含 MVCC 的内容）。&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Zig 是一种新兴的系统编程语言，其设计目标是提供现代特性的同时保持低复杂性。&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;本项目极大的受到了 Mini-Lsm 这个项目的启发，强烈推荐大家学习这个项目！&lt;/em&gt;&lt;/p&gt; 
&lt;h1&gt;二、LSM-Treee 核心功能概述&lt;/h1&gt; 
&lt;p&gt;在开始自己编写之前，我先简单介绍一下 LSM-Tree（&lt;strong&gt;Log-Structured Merge Tree&lt;/strong&gt;）的架构以及读写流程。&lt;/p&gt; 
&lt;p&gt;LSM-Tree 它结合了日志和索引的特点，优化了写入和读取性能。每次写入都是采用 append-only 的方式，所以写入性能很高。&lt;/p&gt; 
&lt;p&gt;而作为代价，追加写入会造成存储放大，LSM-Tree 时采用了多层 SSTable 的方式将数据堆叠在硬盘上。所以需要一个合并压缩的过程来回收过多的空间。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//d49f94616047080aabc5ff8f4d80024f.jpeg&quot; alt=&quot;合并压缩的过程.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;写流程&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;预写日志&lt;/strong&gt; （&lt;strong&gt;WAL&lt;/strong&gt;） ：写操作首先写入预写日志（WAL），用于记录未提交的数据，确保数据的持久性和一致性。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MemTable&lt;/strong&gt;：随后将数据写入内存中的 MemTable，MemTable 是一个平衡树（如 skiplist），支持快速插入和删除操作。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;触发 Compaction&lt;/strong&gt;：当 MemTable 达到一定阈值时，会触发后台线程将 MemTable 中的数据刷入磁盘，生成 SSTable 文件。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SSTable&lt;/strong&gt;：生成的 SSTable 文件是不可变的，存储在磁盘上，用于后续读取操作。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;合并操作&lt;/strong&gt; （&lt;strong&gt;Merge&lt;/strong&gt;） ：当多个 SSTable 文件达到一定数量时，会触发合并操作，将它们合并为一个更大的 SSTable 文件，以减少文件数量。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;读流程&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;MemTable 优先&lt;/strong&gt;：读取操作首先从 MemTable 中查找数据，因为 MemTable 是按升序排列的，查找效率较高。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Block Cache&lt;/strong&gt;：如果 MemTable 中未找到数据，则从 Block Cache 中查找。Block Cache 存储了预先加载到内存中的 SSTable 块，以提高读取性能。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SSTable 查找&lt;/strong&gt;：如果 Block Cache 中也未找到数据，则从磁盘上的 SSTable 文件中查找。Lsm-tree 会从最低层（L0）开始查找，逐层向上查找，直到找到目标数据。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;多版本并发控制&lt;/strong&gt; （&lt;strong&gt;MVCC&lt;/strong&gt;） ：Lsm-tree 支持多版本并发控制，允许同时访问不同版本的数据，从而提高并发性能。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;三、核心功能实现&lt;/h1&gt; 
&lt;h2&gt;MemTable 实现&lt;/h2&gt; 
&lt;p&gt;首先，我们先实现 LSM 存储引擎的内存结构---Memtable。我们选择&lt;strong&gt;跳表&lt;/strong&gt;实现作为 Memtable 的数据结构，因为它支持无锁的并发读写。我们不会深入介绍跳表的工作原理 (Redis 的同学应该不陌生这个东西)，简单来说，它是一个易于实现的有序键值映射。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//adc116878e613d568440d87fb5500640.jpeg&quot; alt=&quot;有序健值.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt; Skiplist 的实现非常简单，这里我利用 Zig 编译时的能力实现了一个泛型版本的跳表 src/skiplist.zig，有兴趣的小伙伴可以直接去仓库中参观代码。&lt;/p&gt; 
&lt;p&gt;基于 SkipList 的能力，我们即可包装出 Memtable 的基本功能。&lt;/p&gt; 
&lt;p&gt;我们这个 LSM 支持 WAL 功能的，即写入内存表之前要先写入磁盘日志，方便在意外宕机重启后可以恢复数据。&lt;/p&gt; 
&lt;p&gt;WAL 的能力我就不想自己再实现了，于是从网上扒了一个 C 的实现（Zig 集成 C 语言非常便捷，可以参考与 C 交互）。&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;map: Map,
lock: RwLock,
wal: ?Wal,
id: usize,
allocator: std.mem.Allocator,
arena: std.heap.ArenaAllocator,
approximate_size: atomic.Value(usize) = atomic.Value(usize).init(0),

fn putToList(self: *Self, key: []const u8, value: []const u8) !void {
    {
        self.lock.lock();
        defer self.lock.unlock();
        try self.map.insert(kk, vv);
    }

    _ = self.approximate_size.fetchAdd(@intCast(key.len + value.len), .monotonic);
}

fn putToWal(self: *Self, key: []const u8, value: []const u8) !void {
    // [key-size: 4bytes][key][value-size: 4bytes][value]

    if (self.wal) |w| {
        var buf = std.ArrayList(u8).init(self.arena.allocator());

        var bw = buf.writer();
        try bw.writeInt(u32, @intCast(key.len), .big);
        _ = try bw.write(key);
        try bw.writeInt(u32, @intCast(value.len), .big);
        _ = try bw.write(value);
        try w.append(buf.items);
    }
}

// 写入 Memtable，先写 WAL，再写 skiplist table
pub fn put(self: *Self, key: []const u8, value: []const u8) !void {
    try self.putToWal(key, value);
    try self.putToList(key, value);
}

pub fn get(self: *Self, key: []const u8, val: *[]const u8) !bool {
    self.lock.lockShared();
    defer self.lock.unlockShared();
    var vv: []const u8 =     ;
    if (try self.map.get(key, &amp;amp;vv)) {
        val.* = vv;
        return true;
    }
    return false;
}

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;注意到这里我们没有实现删除的功能，这里我仿照了 RocksDB 中的墓碑机制，用空值代表删除，所以删除被 put(key, &quot;&quot;) 取代。&lt;/p&gt; 
&lt;h2&gt;SSTable&lt;/h2&gt; 
&lt;p&gt;接下来，我们就着手开始实现 LSM 中另外一个重要元素 --- SSTable。&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;SSTable（Sorted String Table）是一种不可变的（Immutable）磁盘文件，内部按 Key 有序排列，存储键值对数据。每个 SSTable 文件生成后不再修改，更新和删除操作通过追加新记录或标记删除，最终通过合并（Compaction）清理冗余数据。 每当 LSM-Tree 中的 MemTable 体积超出阈值，就会将内存中的数据写入 SsTable。&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//441435c815897a51832a32a1157637ae.jpeg&quot; alt=&quot;内存中的数据.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 每个 SSTable 由多个 Block 组成，每个 Block 是一组 KV 的 package。&lt;/p&gt; 
&lt;p&gt;Block 的&lt;strong&gt;编码格式&lt;/strong&gt;如下：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//ec2724e561fe943e071d00506663be9d.jpeg&quot; alt=&quot;block 的健码格式.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 为了构建一个 Block，我们实现了一个&lt;strong&gt;BlockBuilder&lt;/strong&gt;的模块，这部分代码见 src/block.zig：&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;pub const Block = struct {
    data_v: std.ArrayList(u8),
    offset_v: std.ArrayList(u16),
}

pub const BlockBuilder = struct {
    allocator: std.mem.Allocator,
    offset_v: std.ArrayList(u16),
    data_v: std.ArrayList(u8),
    block_size: usize,
    first_key: []u8,
    
    pub fn add(self: *Self, key: []const u8, value: ?[]const u8) !bool {
        std.debug.assert(key.len &amp;gt; 0); // key must not be empty

        const vSize = if (value) |v| v.len else 0;
        
        if ((self.estimated_size() + key.len + vSize + 3 * @sizeOf(u16) &amp;gt; self.block_size) and !self.is_empty()) {
            return false;
        }
        try self.doAdd(key, value);

        if (self.first_key.len == 0) {
            self.first_key = try self.allocator.dupe(u8, key);
        }
        return true;
    }

    fn doAdd(self: *Self, key: []const u8, value: ?[]const u8) !void {
        // add the offset of the data into the offset array
        try self.offset_v.append(@intCast(self.data_v.items.len));
        const overlap = calculate_overlap(self.first_key, key);

        var dw = self.data_v.writer();
        // encode key overlap
        try dw.writeInt(u16, @intCast(overlap), .big);
        // encode key length
        try dw.writeInt(u16, @intCast(key.len - overlap), .big);

        // encode key content
        _ = try dw.write(key[overlap..]);
        // encode value length
        if (value) |v| {
            try dw.writeInt(u16, @intCast(v.len), .big);
            // encode value content
            _ = try dw.write(v);
        } else {
            try dw.writeInt(u16, 0, .big);
        }
    }

    pub fn build(self: *Self) !Block {
        if (self.isEmpty()) {
            @panic(&quot;block is empty&quot;);
        }
        return Block.init(
            try self.data_v.clone(),
            try self.offset_v.clone(),
        );
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;可能有同学注意到，我们写 key 的时候没有直接将 key 值写入，而且只写了 key 与当前 block 的第一个 key 不重叠的 suffix 部分。由于 block 中的 key 都是有序的，所以一个 block 中的 key 有很大概率是前缀类似的，所以这里是一个空间优化的小技巧，例如：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Key: foo, foo1, foo2, foo3 ....&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;我们写入 block 时，只需要写：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;foo|1|2|3|....&lt;/strong&gt; 很多有序表的实现中都会用到这个小技巧。&lt;/p&gt; 
&lt;p&gt;有了 block 的实现，我们可以进一步来定义 SSTable 的格式。一个 SSTable 由多个 Block、block 元数据以及布隆过滤器构成。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//e183c694bf3c0e2a6ec6b15651d5fbee.jpeg&quot; alt=&quot;布隆过滤器.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt; &lt;em&gt;布隆过滤器是一种概率性数据结构，用于维护一组键。您可以向布隆过滤器中添加键，并且可以知道在添加到布隆过滤器中的键集中可能存在或必须不存在的键。&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;在 SSTable 中添加布隆过滤器可以有效提升查询 key 的效率。&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;元数据包含了 block 的&lt;strong&gt;第一个与最后一个 key 以及 block 在 sst 中的 offset 信息&lt;/strong&gt;，记录元数据主要为了在后续的检索中可以快速定位某个 key 落在哪个 block 中。&lt;/p&gt; 
&lt;p&gt;同样的套路，为了构建 SSTable，我们先实现一个&lt;strong&gt;SSTableBuilder&lt;/strong&gt;，部分代码见 src/ss_table.zig&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;pub const SsTableBuilder = struct {
    allocator: std.mem.Allocator,
    builder: BlockBuilder, // 刚才实现的 block 构建装置
    first_key: ?[]const u8,
    last_key: ?[]const u8,
    meta: std.ArrayList(BlockMeta),
    block_size: usize,
    data: std.ArrayList(u8),
    bloom: BloomFilterPtr, // 布隆过滤器
    
    pub fn add(self: *Self, key: []const u8, value: []const u8) !void {
        try self.setFirstKey(key);
        try self.bloom.get().insert(key); // 写入布隆过滤器

        if (try self.builder.add(key, value)) {
            try self.setLastKey(key);
            return;
        }
        // block is full
        try self.finishBlock();
        std.debug.assert(try self.builder.add(key, value));
        try self.resetFirstKey(key);
        try self.setLastKey(key);
    }
    
    // 写入一个 block 的数据
    fn finishBlock(self: *Self) !void {
        if (self.builder.isEmpty()) {
            return;
        }
        var bo = self.builder;
        // reset block
        defer bo.reset();

        self.builder = BlockBuilder.init(self.allocator, self.block_size);
        var blk = try bo.build();
        defer blk.deinit();
        const encoded_block = try blk.encode(self.allocator); // block 序列化
        defer self.allocator.free(encoded_block);
        
        // 记录 block 的元数据
        try self.meta.append(.{
            .allocator = self.allocator,
            .offset = self.data.items.len,
            .first_key = try self.allocator.dupe(u8, self.first_key.?),
            .last_key = try self.allocator.dupe(u8, self.last_key.?),
        });
        const cksm = hash.Crc32.hash(encoded_block); // 写入 4b 的校验值
        try self.data.appendSlice(encoded_block);
        try self.data.writer().writeInt(u32, cksm, .big);
    }
    
    // 构建为一个 SSTable
    pub fn build(
        self: *Self,
        id: usize,
        block_cache: ?BlockCachePtr, // 读取 block 数据的缓存，减少 block 的反序列化成本
        path: []const u8,
    ) !SsTable {
        var arena = std.heap.ArenaAllocator.init(self.allocator);
        defer arena.deinit();
        const allocator = arena.allocator();

        try self.finishBlock();
        const w = self.data.writer();
        
        // 写入元数据及其 offset
        const meta_offset = self.data.items.len;
        const meta_b = try BlockMeta.batchEncode(self.meta.items, allocator);
        _ = try w.write(meta_b);
        try w.writeInt(u32, @intCast(meta_offset), .big);

        // 写入布隆过滤器及其 offset
        const bloom_offset = self.data.items.len;
        const encoded_bloom = try self.bloom.get().encode(allocator);
        _ = try w.write(encoded_bloom);
        try w.writeInt(u32, @intCast(bloom_offset), .big);
        
        
        const file = try FileObject.init(path, self.data.items);
        errdefer file.deinit();

        const fk = self.meta.items[0].first_key;
        const lk = self.meta.getLast().last_key;

        return .{
            .allocator = self.allocator,
            .file = file,
            .block_metas = try self.meta.toOwnedSlice(),
            .meta_offset = meta_offset,
            .block_cache = block_cache,
            .bloom = self.bloom.clone(),
            .id = id,
            .first_key = try self.allocator.dupe(u8, fk),
            .last_key = try self.allocator.dupe(u8, lk),
            .max_ts = 0,
        };
    }
}

&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Write&lt;/h2&gt; 
&lt;p&gt;有了 SSTable 和 MemTable，我们就有了 LSM-Tree 需要的两个最重要的材料，后续的读写不过是对这两类材料的组合拼装。&lt;/p&gt; 
&lt;p&gt;在实现写操作之前，我们先假想一下 LSM-Tree 的数据结构: &lt;img src=&quot;https://oscimg.oschina.net/oscnet//e8b35c69dad1058161979d0761a78d91.jpeg&quot; alt=&quot;lsmtree 的数据结构.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;首先我们需要一个数据结构存储当前 MemTable、冷 MemTables 和多层的 SST，如下图所示。 图片&lt;/li&gt; 
 &lt;li&gt;其次我们需要一个锁用于同步上述数据结构的读写行为。&lt;/li&gt; 
 &lt;li&gt;我们还需要一个 SSTable 的自增 id。&lt;/li&gt; 
 &lt;li&gt;最后还需要一些必要的配置，例如存储路径、线程管理器等。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;最终，我们实现的 LSM 数据结构如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;pub const StorageState = struct {
    allocator: std.mem.Allocator,
    mem_table: MemTablePtr, // 当前正在写的 MemTable
    imm_mem_tables: std.ArrayList(MemTablePtr), // 冷 MemTable 数组
    l0_sstables: std.ArrayList(usize), // 第一层的 SSTable 数组
    levels: std.ArrayList(std.ArrayList(usize)), // 后续多层的 SSTable 数组
    sstables: std.AutoHashMap(usize, SsTablePtr), // sst_id =&amp;gt; SSTable
}

pub const StorageInner = struct {
    const Self = @This();

    allocator: std.mem.Allocator,
    state: StorageState,
    state_lock: std.Thread.RwLock = .{},
    next_sst_id: atomic.Value(usize),
    path: []const u8,
    options: StorageOptions,
    compaction_controller: CompactionController,
    block_cache: BlockCachePtr,
    terminate: std.Thread.ResetEvent = .{},
    wg: std.Thread.WaitGroup = .{},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;先不考虑逐层压缩的逻辑，只考虑一层 SSTable 的简单情况，写逻辑可以简化为如下流程：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//f5e96cf0290f96c4ce85ccfb3f369951.jpeg&quot; alt=&quot;简化为如下流程.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;写入 State 中的 MemTable&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;pub fn writeBatch(self: *Self, records: []const WriteBatchRecord) !void {
   for (records) |record| {
       switch (record) {
           .put =&amp;gt; |pp| {
               try self.state.getMemTable().put(pp.key, pp.value);
           },
           .delete =&amp;gt; |dd| {
               // we use &quot;&quot; as the tombstone value
               try self.state.getMemTable().put(dd, &quot;&quot;);
           },
       }
       // 尝试把当前 MemTable 压入冷数据
       try self.tryFreeze(self.state.getMemTable().getApproximateSize());
   }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;当 MemTable 体积超出阈值，压入冷 MemTable 数组，重置当前 MemTable&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;fn forceFreezeMemtable(self: *Self) !void {
    const next_sst_id = self.getNextSstId();
    
    // 生成一个新的 MemTable
    var new_mm: MemTable =     ;
    {
        if (self.options.enable_wal) {
            const mm_path = try pathOfWal(self.allocator, self.path, next_sst_id);
            defer self.allocator.free(mm_path);
            new_mm = MemTable.init(next_sst_id, self.allocator, mm_path);
        } else {
            new_mm = MemTable.init(next_sst_id, self.allocator, null);
        }
    }
    errdefer new_mm.deinit();

    var old_mm: *MemTable =     ;
    {
        self.state_lock.lock();
        defer self.state_lock.unlock();
        var old_mm_ptr = self.state.mem_table;
        old_mm = old_mm_ptr.get();
        defer old_mm_ptr.release();
        self.state.mem_table = try MemTablePtr.create(self.allocator, new_mm);
        
        // 将写满的 MemTable 压入冷数据
        try self.state.imm_mem_tables.append(old_mm_ptr.clone()); // newer memtable is inserted at the end
    }
    // TIPS：把磁盘同步放在锁的范围外面，降低锁的覆盖
    try old_mm.syncWal();
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;当冷 MemTable 数组大小超出配置阈值，触发 SSTable 落盘，弹出最冷的 MemTable，写入磁盘 SSTable，并记录在 L0 的 SSTable 数组中。这一过程是在一个线程中定时触发&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;pub fn flushNextMemtable(self: *Self) !void {
    std.debug.assert(self.state.imm_mem_tables.items.len &amp;gt; 0);
    var to_flush_table: *MemTable =     ;
    {
        self.state_lock.lockShared();
        defer self.state_lock.unlockShared();
        // oldest memtable is at the index 0
        to_flush_table = self.state.imm_mem_tables.items[0].load();
    }

    // 将最冷的 MemTable 构建为 SSTable
    var builder = try SsTableBuilder.init(self.allocator, self.options.block_size);
    defer builder.deinit();

    const sst_id = to_flush_table.id;
    try to_flush_table.flush(&amp;amp;builder);

    const sst_path = try self.pathOfSst(sst_id);
    defer self.allocator.free(sst_path);
    var sst = try builder.build(sst_id, self.block_cache.clone(), sst_path);
    errdefer sst.deinit();

    // add the flushed table to l0_sstables
    {
        self.state_lock.lock();
        defer self.state_lock.unlock();

        var m = self.state.imm_mem_tables.orderedRemove(0);
        defer m.deinit();
        std.debug.assert(m.load().id == sst_id);

        // newest sstable is at the end
        try self.state.l0_sstables.append(sst_id);
        try self.state.sstables.put(sst.id, try SsTablePtr.create(self.allocator, sst));
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;当然，这里只实现了一半的写逻辑，数据停留在 L0 的 SST 中，后续的多层 SST 还没有使用。&lt;/p&gt; 
&lt;p&gt;剩下一半的写逻辑会在数据压缩的章节中介绍。&lt;/p&gt; 
&lt;h2&gt;Iterators&lt;/h2&gt; 
&lt;p&gt;写入的过程比较好理解，但是读就略微复杂了，以上面我们实现的写结果为例子，最终我们的数据沉淀在一个 3 层的数据结构中，要如何高效的从其中检索数据呢？&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//21d6f9e0f2c5dcbdd5570ac5da94ee77.jpeg&quot; alt=&quot;高效检索数据.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 如同写过程一般，读过程也是对各个基础单元 (MemTable、SSTable、Block) 读过程的组合，为了方便组合逻辑，我们要先统一各个模块的读行为。&lt;/p&gt; 
&lt;p&gt;在 LSM-Tree 中，所有的读行为都定义为了如下的 Interface（Zig 中没 trait 或者 Interface，所以这里实例代码我用 Rust 描述）：&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;pub trait StorageIterator {
    /// Get the current value.
    fn value(&amp;amp;self) -&amp;gt; &amp;amp;[u8];

    /// Get the current key.
    fn key(&amp;amp;self) -&amp;gt; &amp;amp;[u8];

    /// Check if the current iterator is empty.
    fn is_empty(&amp;amp;self) -&amp;gt; bool;

    /// Move to the next position.
    fn next(&amp;amp;mut self) -&amp;gt; anyhow::Result&amp;lt;()&amp;gt;;

    /// Number of underlying active iterators for this iterator.
    fn num_active_iterators(&amp;amp;self) -&amp;gt; usize {
        1
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;我们首先对 MemTable、SSTable、Block 这些模块实现读接口，代码可见：src/MemTable.zig，src/block.zig，src/ss_table.zig，这里单独简单介绍下 SSTable 的读接口实现思路，其他的模块实现思路类似，感兴趣的直接阅读源码即可。&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;pub const SsTableIterator = struct {
    allocator: std.mem.Allocator,
    table: SsTablePtr,
    blk: BlockPtr,
    blk_iterator: BlockIteratorPtr,
    blk_idx: usize,

    const Self = @This();


    pub fn initAndSeekToFirst(allocator: std.mem.Allocator, table: SsTablePtr) !Self {
        const s = try seekToFirstInner(allocator, table);
        return .{
            .allocator = allocator,
            .table = table,
            .blk_iterator = s.blk_iter,
            .blk = s.blk,
            .blk_idx = 0,
        };
    }

    pub fn initAndSeekToKey(allocator: std.mem.Allocator, table: SsTablePtr, k: []const u8) !Self {
        const b = try seekToKeyInner(allocator, table, k);
        return .{
            .allocator = allocator,
            .table = table,
            .blk_iterator = b.blk_iter,
            .blk_idx = b.blk_idx,
            .blk = b.blk,
        };
    }

    fn seekToFirstInner(allocator: std.mem.Allocator, table: SsTablePtr) !struct {
        blk: BlockPtr,
        blk_iter: BlockIteratorPtr,
    } {
        var blk = try table.get().readBlockCached(0, allocator); // 读取第一个 block
        errdefer blk.release();
        var blk_iter = try BlockIterator.createAndSeekToFirst(allocator, blk.clone());
        errdefer blk_iter.deinit();

        return .{
            .blk = blk,
            .blk_iter = try BlockIteratorPtr.create(allocator, blk_iter), // 从 SSTable 的读接口转换为 Block 的读接口
        };
    }

    fn seekToKeyInner(allocator: std.mem.Allocator, table: SsTablePtr, k: []const u8) !struct {
        blk_idx: usize,
        blk: BlockPtr,
        blk_iter: BlockIteratorPtr,
    } {
        const table_ptr = table.get();
        var blk_idx = try table_ptr.findBlockIndex(k);
        var blk = try table_ptr.readBlockCached(blk_idx, allocator);
        errdefer blk.deinit();
        var blk_iter = try BlockIterator.createAndSeekToKey(allocator, blk.clone(), k);
        errdefer blk_iter.deinit();
        var blk_iter_ptr = try BlockIteratorPtr.create(allocator, blk_iter);
        errdefer blk_iter_ptr.release();

        // 如果当前 block 读完了，跳到下一个 block，并生成 block 的读接口
        if (blk_iter.isEmpty()) {
            blk_idx += 1;
            if (blk_idx &amp;lt; table_ptr.numBlocks()) {
                {
                    blk.deinit();
                    blk_iter.deinit();
                }
                var blk2 = try table_ptr.readBlockCached(blk_idx, allocator);
                errdefer blk2.deinit();
                var blk_iter2 = try BlockIterator.createAndSeekToFirst(allocator, blk2.clone());
                errdefer blk_iter2.deinit();

                return .{
                    .blk_idx = blk_idx,
                    .blk_iter = try BlockIteratorPtr.create(allocator, blk_iter2),
                    .blk = blk2,
                };
            }
        }
        return .{
            .blk_idx = blk_idx,
            .blk_iter = blk_iter_ptr,
            .blk = blk,
        };
    }

    pub fn key(self: Self) []const u8 {
        return self.blk_iterator.get().key();
    }

    pub fn value(self: Self) []const u8 {
        return self.blk_iterator.get().value();
    }

    pub fn isEmpty(self: Self) bool {
        return self.blk_iterator.get().isEmpty();
    }

    pub fn next(self: *Self) !void {
        try self.blk_iterator.get().next();
        // 若当前的 Block 读完了，就跳到下一个 block，并生成 Block 读接口。
        if (self.blk_iterator.get().isEmpty()) {
            self.blk_idx += 1;
            if (self.blk_idx &amp;lt; self.table.get().numBlocks()) {
                self.reset();
                const blk = try self.table.get().readBlockCached(self.blk_idx, self.allocator);
                const blk_iter = try BlockIterator.createAndSeekToFirst(self.allocator, blk.clone());
                self.blk = blk;
                self.blk_iterator = try BlockIteratorPtr.create(self.allocator, blk_iter);
            }
        }
    }
};

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;有了几个基本元素的读接口之后，我们便遇到第一个问题：&lt;strong&gt;我们如何对多个 MemTable 做读检索？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//32630b4228aeed16aa135674cfff65a1.jpeg&quot; alt=&quot;如何对多个 m 做检索.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 这个时候，我们需要一个新的数据结构来实现多个读实例的合并检索---- &lt;strong&gt;MergeIterator&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;MergeIterator 在内部维护一个二叉堆。堆中数据的优先级如下：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;当各个迭代器 key 不同时，具有最小 key 的迭代器最优。当多个迭代器有相同的当前 key 时，最新的迭代器一个最优。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;假设我们有如下 MemTable（iter1 最新，iter3 最旧）:&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;iter1: b-&amp;gt;del, c-&amp;gt;4, d-&amp;gt;5 iter2: a-&amp;gt;1, b-&amp;gt;2, c-&amp;gt;3 iter3: e-&amp;gt;4&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;经过合并后迭代器结果应该为：&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;a 最小，iter2 优先迭代 iter2 迭代一次后，iter1 与 iter2 key 相同，iter1 优先迭代，b-&amp;gt;2 跳过 c 最小，iter1 优先迭代，iter2 中 c-&amp;gt;3 跳过 d 最小，iter1 优先迭代，只剩 iter3，迭代 iter3&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;最终结果：a-&amp;gt;1, b-&amp;gt;del, c-&amp;gt;4, d-&amp;gt;5, e-&amp;gt;4&lt;/p&gt; 
&lt;p&gt;实现代码如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;// 标准库中有二叉堆实现
const IteratorHeap = std.PriorityQueue(*HeapWrapper, Comparer.Context, Comparer.cmp);

allocator: std.mem.Allocator,
q: IteratorHeap,
current: ?*HeapWrapper,

pub fn init(allocator: std.mem.Allocator, iters: std.ArrayList(StorageIteratorPtr)) !Self {
    var q = IteratorHeap.init(allocator, .{});
    if (iters.items.len == 0) {
        return Self{
            .allocator = allocator,
            .q = q,
            .current = null,
        };
    }

    // PS: the last iter has the highest priority
    // 按顺序写入二叉堆
    for (iters.items, 0..) |sp, i| {
        if (!sp.load().isEmpty()) {
            const hw = try allocator.create(HeapWrapper);
            errdefer allocator.destroy(hw);
            hw.* = HeapWrapper.init(i, sp.clone());
            try q.add(hw);
        }
    }

    const cc = q.removeOrNull();
    return Self{
        .allocator = allocator,
        .q = q,
        .current = cc,
    };
}

pub fn key(self: Self) []const u8 {
    return self.current.?.key();
}

pub fn value(self: Self) []const u8 {
    return self.current.?.value();
}

pub fn isEmpty(self: Self) bool {
    if (self.current) |cc| {
        return cc.isEmpty();
    }
    return true;
}

pub fn next(self: *Self) !void {
    const cc = self.current.?;
    while (true) {
        if (self.q.peek()) |ii| {
            std.debug.assert(!ii.isEmpty());
            // 如果优先堆头部迭代器 A 和当前正在生效的迭代器 B 的 key 相同，让迭代器 A 跳过重复 key
            if (std.mem.eql(u8, cc.key(), ii.key())) {
                try ii.next();
                if (ii.isEmpty()) {
                    _ = self.q.remove();
                    ii.deinit();
                    self.allocator.destroy(ii);
                }
            } else {
                break;
            }
        }
        break;
    }

    try cc.next(); // 迭代当前迭代器

    // 如果当前优先迭代器迭代完了，就从堆中弹出最优迭代器
    if (cc.isEmpty()) {
        defer {
            cc.deinit();
            self.allocator.destroy(cc);
        }
        if (self.q.removeOrNull()) |h| {
            self.current = h;
        } else {
            self.current = null;
        }
        return;
    }

    // 将当前迭代器写回二叉堆，重新计算最优迭代器
    try self.q.add(cc); 
    self.current = self.q.removeOrNull();
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;有了 MergeIterator 这个工具，我们具备了在多个 MemTable 和多个 SSTable 中迭代检索的能力，但是还有个问题，我们当前有两个 MergeIterator，应该如何在两个迭代器中执行迭代任务？&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//d030afd564de14e802e21ca38a1e58ce.jpeg&quot; alt=&quot;执行迭代任务.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 此时，我们再引入一个新的数据结构：&lt;strong&gt;TwoMergeIterator&lt;/strong&gt;，这个是 MergeIterator 在元素只有两个的情况下的简化版。&lt;/p&gt; 
&lt;p&gt;TwoMergeIterator 由两个迭代器构成，一个高优一个低优，每次迭代优先迭代高优，当 key 相同时，优先迭代高优。实现如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;pub const TwoMergeIterator = struct {
    a: StorageIteratorPtr,
    b: StorageIteratorPtr,
    choose_a: bool,

    // 选择两个迭代器中 key 更小的迭代器
    fn chooseA(a: *StorageIterator, b: *StorageIterator) bool {
        if (a.isEmpty()) {
            return false;
        }
        if (b.isEmpty()) {
            return true;
        }
        return std.mem.lessThan(u8, a.key(), b.key());
    }

    // key 相同时，跳过低优中的 key
    fn skipB(self: *TwoMergeIterator) !void {
        const ap = self.a.load();
        const bp = self.b.load();
        if (!ap.isEmpty() and !bp.isEmpty() and std.mem.eql(u8, ap.key(), bp.key())) try bp.next();
    }

    pub fn init(a: StorageIteratorPtr, b: StorageIteratorPtr) !TwoMergeIterator {
        var iter = TwoMergeIterator{
            .a = a,
            .b = b,
            .choose_a = false,
        };
        try iter.skipB();
        iter.choose_a = chooseA(iter.a.load(), iter.b.load());
        return iter;
    }

    pub fn deinit(self: *TwoMergeIterator) void {
        self.a.release();
        self.b.release();
    }

    pub fn key(self: TwoMergeIterator) []const u8 {
        if (self.choose_a) {
            std.debug.assert(!self.a.load().isEmpty());
            return self.a.load().key();
        }
        std.debug.assert(!self.b.load().isEmpty());
        return self.b.load().key();
    }

    pub fn value(self: TwoMergeIterator) []const u8 {
        if (self.choose_a) {
            std.debug.assert(!self.a.load().isEmpty());
            return self.a.load().value();
        }
        std.debug.assert(!self.b.load().isEmpty());
        return self.b.load().value();
    }

    pub fn isEmpty(self: TwoMergeIterator) bool {
        if (self.choose_a) {
            return self.a.load().isEmpty();
        }
        return self.b.load().isEmpty();
    }

    pub fn next(self: *TwoMergeIterator) !void {
        if (self.choose_a) {
            try self.a.load().next();
        } else {
            try self.b.load().next();
        }
        try self.skipB();
        self.choose_a = chooseA(self.a.load(), self.b.load());
    }
};

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;至此，我们读行为所需要的武器就完备了！&lt;/p&gt; 
&lt;h2&gt;Read/Scan&lt;/h2&gt; 
&lt;p&gt;让我们再来看看 LSM 的架构图：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//04dd1b2d4cde764a23b28d78259cf343.jpeg&quot; alt=&quot;LSM 的架构图.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 我们将每个数据层中的数据标上优先级，由于 LSM-Tree 是 append-only 的，所以优先级越高的数据层中数据越新。&lt;/p&gt; 
&lt;p&gt;所以我们的读策略也很明显：按照上图中 P0 至 P2 依次检索，这部分代码实现见 src/storage.zig。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;读 MemTable&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;// search in memtable
if (try self.state.getMemTable().get(key, value)) {
    if (value.*.len == 0) {
        // tomestone
        return false;
    }
    return true;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;读 Immutable MemTable&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;// search in imm_memtables

self.state_lock.lockShared();
defer self.state_lock.unlockShared();
for (self.state.imm_mem_tables.items) |imm_table| {
    if (try imm_table.load().get(key, value)) {
        if (value.*.len == 0) {
            // tomestone
            return false;
        }
        return true;
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;读 LV0~LVmax SSTables&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;// 收集 L0 中的迭代器
var l0_iters = std.ArrayList(StorageIteratorPtr).init(self.allocator);
defer {
    for (l0_iters.items) |iter| {
        var ii = iter;
        ii.release();
    }
    l0_iters.deinit();
}
{
    self.state_lock.lockShared();
    defer self.state_lock.unlockShared();
    for (self.state.l0_sstables.items) |sst_id| {
        const sst = self.state.sstables.get(sst_id).?;
        if (try sst.load().mayContain(key)) {
            var ss_iter = try SsTableIterator.initAndSeekToKey(self.allocator, sst.clone(), key);
            errdefer ss_iter.deinit();
            try l0_iters.append(try StorageIteratorPtr.create(self.allocator, .{ .ss_table_iter = ss_iter }));
        }
    }
}

// 收集 Levels 中的迭代器
var level_iters: std.ArrayList(StorageIteratorPtr) =     ;
{
    self.state_lock.lockShared();
    defer self.state_lock.unlockShared();
    level_iters = try std.ArrayList(StorageIteratorPtr).initCapacity(
        self.allocator,
        self.state.levels.items.len,
    );
    for (self.state.levels.items) |level| {
        var level_ssts = try std.ArrayList(SsTablePtr).initCapacity(self.allocator, level.items.len);
        errdefer level_ssts.deinit();
        for (level.items) |sst_id| {
            const sst = self.state.sstables.get(sst_id).?;
            if (try mayWithinTable(key, sst)) {
                try level_ssts.append(sst.clone());
            }
        }
        if (level_ssts.items.len &amp;gt; 0) {
            var level_iter = try SstConcatIterator.initAndSeekToKey(
                self.allocator,
                level_ssts,
                key,
            );
            errdefer level_iter.deinit();
            try level_iters.append(try StorageIteratorPtr.create(self.allocator, .{ .sst_concat_iter = level_iter }));
        }
    }
}

// 将多个迭代器合并为一个 TwoMergeIterator
var l0_merge_iter = try MergeIterators.init(self.allocator, l0_iters);
errdefer l0_merge_iter.deinit();

var levels_merge_iter = try MergeIterators.init(self.allocator, level_iters);
errdefer levels_merge_iter.deinit();

var iter = try TwoMergeIterator.init(
    try StorageIteratorPtr.create(self.allocator, .{ .merge_iterators = l0_merge_iter }),
    try StorageIteratorPtr.create(self.allocator, .{ .merge_iterators = levels_merge_iter }),
);
defer iter.deinit();

if (iter.isEmpty()) {
    return false;
}

if (std.mem.eql(u8, iter.key(), key) and iter.value().len &amp;gt; 0) {
    value.* = iter.value();
    return true;
}

&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;压缩&lt;/h2&gt; 
&lt;p&gt;在上一节的写过程中，我们实现了从内存表到 Level0 的 SSTable 堆叠。&lt;/p&gt; 
&lt;p&gt;随着写入的持续，Lv0 的 SSTable 会越来越多，这个时候就需要我们将 Lv0 中的数据合并写入至 Lv2，并依次类推重复这个过程，直到堆叠到最深的层数，这个逐层合并数据的过程就是&lt;strong&gt;数据压缩&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//982033f774dbf9693963f20f07b42b5b.jpeg&quot; alt=&quot;数据压缩.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt; LSM-Tree 中数据压缩的过程大致如下：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//cb153ea8c68e5038d9c6b02da7837645.jpeg&quot; alt=&quot;过程大致如下.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 具体的实现代码可见 src/compact.zig，src/storage.zig。&lt;/p&gt; 
&lt;p&gt;简单分层压缩与原始 LSM 论文中的压缩策略相似。它为 LSM 树维护多个层级。当一个层级太大时，它会将此层级的所有 SST 与下一层合并。压缩策略由 3 个参数控制：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;size_ratio_percent：【文件低级数量/文件高级数量】，当实际计算的值低于此阈值时触发压缩。假设这里我们设置为 60%，当 L0 中 SST 数量为 2，L1 中 SST 数量为 1，此时 ratio 为 1/2 = 50% &amp;lt; 60%，此时我们应该将 L0 压缩合并至 L1。&lt;/li&gt; 
 &lt;li&gt;level0_file_num_compaction_trigger: 第一层 SSTable 达到多少后触发压缩。因为这是最高层，没法与更高层比较，只能固定触发压缩。&lt;/li&gt; 
 &lt;li&gt;max_levels: 顾名思义，最大的层数限制。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;做好这些准备工作，我们可以逐步实现压缩逻辑：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;生成压缩任务：&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;pub const SimpleLeveledCompactionController = struct {
    options: SimpleLeveledCompactionOptions,

    pub fn generateCompactionTask(self: SimpleLeveledCompactionController, state: *storage.StorageState) !?SimpleLeveledCompactionTask {
        if (self.options.max_levels == 1) {
            return null;
        }

        var level_sizes = std.ArrayList(usize).init(state.allocator);
        defer level_sizes.deinit();

        try level_sizes.append(state.l0_sstables.items.len);
        for (state.levels.items) |level| {
            try level_sizes.append(level.items.len);
        }

        // 如果 Lv0 中 SST 数量超出阈值，触发 L0 级别压缩
        if (state.l0_sstables.items.len &amp;gt;= self.options.level0_file_num_compaction_trigger) {
            std.debug.print(&quot;compaction of L0 to L1 because L0 has {d} SSTS &amp;gt;= {d}\n&quot;, .{ state.l0_sstables.items.len, self.options.level0_file_num_compaction_trigger });
            return .{
                .upper_level = null,
                .upper_level_sst_ids = try state.l0_sstables.clone(),
                .lower_level = 1,
                .lower_level_sst_ids = try state.levels.items[0].clone(),
                .is_lower_level_bottom = false,
            };
        }

        // 计算 Lv[n+1]/lv[n]，如果比例小于阈值，触发 Lv[n]级别压缩
        for (1..self.options.max_levels) |level| {
            const lower_level = level + 1;
            if (level_sizes.items[level] == 0) {
                continue;
            }
            const size_ration = level_sizes.items[lower_level] * 100 / level_sizes.items[level];
            if (size_ration &amp;lt; self.options.size_ration_percent) {
                std.debug.print(&quot;compaction of L{d} to L{d} because L{d} size ratio {d} &amp;lt; {d}\n&quot;, .{ level, lower_level, level, size_ration, self.options.size_ration_percent });
                return .{
                    .upper_level = level,
                    .upper_level_sst_ids = try state.levels.items[level - 1].clone(),
                    .lower_level = lower_level,
                    .lower_level_sst_ids = try state.levels.items[lower_level - 1].clone(),
                    .is_lower_level_bottom = lower_level == self.options.max_levels,
                };
            }
        }

        return null;
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;执行压缩任务：&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;有了上一小节中读过程的介绍，多层数据的压缩过程就很好理解了。&lt;/p&gt; 
&lt;p&gt;例如我们想将 L1 与 L2 的 SSTable 合并压缩至 L2，我们只需要把 L1 和 L2 的数据放在一起创造一个迭代器，再持续从该迭代器中读出数据写入新的 SSTable 中，这个过程保证了新的 SSTable 中数据不重复且有序。&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;fn compactSimple(self: *Self, task: SimpleLeveledCompactionTask) !std.ArrayList(SsTablePtr) {
    if (task.upper_level) |_| {
        var upper_ssts = try std.ArrayList(SsTablePtr).initCapacity(
            self.allocator,
            task.upper_level_sst_ids.items.len,
        );
        var lower_ssts = try std.ArrayList(SsTablePtr).initCapacity(
            self.allocator,
            task.lower_level_sst_ids.items.len,
        );

        self.state_lock.lockShared();
        for (task.upper_level_sst_ids.items) |sst_id| {
            const sst = self.state.sstables.get(sst_id).?;
            try upper_ssts.append(sst.clone());
        }
        for (task.lower_level_sst_ids.items) |sst_id| {
            const sst = self.state.sstables.get(sst_id).?;
            try lower_ssts.append(sst.clone());
        }
        self.state_lock.unlockShared();

        var upper_iter = try SstConcatIterator.initAndSeekToFirst(self.allocator, upper_ssts);
        errdefer upper_iter.deinit();

        var lower_iter = try SstConcatIterator.initAndSeekToFirst(self.allocator, lower_ssts);
        errdefer lower_iter.deinit();

        var iter = try TwoMergeIterator.init(
            try StorageIteratorPtr.create(self.allocator, .{ .sst_concat_iter = upper_iter }),
            try StorageIteratorPtr.create(self.allocator, .{ .sst_concat_iter = lower_iter }),
        );
        defer iter.deinit();
        return self.compactGenerateSstFromIter(&amp;amp;iter, task.is_lower_level_bottom);
    } else {
        // compact l0_sstables to l1_sstables
        // ..... 代码逻辑大致与上面 LvN 层压缩一致，只是 Lv0 层的 SSTable 是无序的需要特殊考虑
        return self.compactGenerateSstFromIter(&amp;amp;iter, task.is_lower_level_bottom);
    }
}


fn compactGenerateSstFromIter(self: *Self, iter: *TwoMergeIterator, compact_to_bottom_level: bool) !std.ArrayList(SsTablePtr) {
    var builder: SsTableBuilder = try SsTableBuilder.init(self.allocator, self.options.block_size);
    defer builder.deinit();
    var new_ssts = std.ArrayList(SsTablePtr).init(self.allocator);
    
    // 持续迭代此迭代器
    while (!iter.isEmpty()) {
        // 如果压缩至最后一层，可以不保留墓碑值 key 了
        if (compact_to_bottom_level) {
            if (iter.value().len &amp;gt; 0) {
                try builder.add(iter.key(), iter.value());
            }
        } else {
            try builder.add(iter.key(), iter.value());
        }
        // 当写满一个 SSTable 后，就清空 builder，把写满的 SSTable 入列
        if (builder.estimatedSize() &amp;gt;= self.options.target_sst_size) {
            // reset builder
            defer builder.reset() catch unreachable;
            const sst_id = self.getNextSstId();
            const path = try self.pathOfSst(sst_id);
            defer self.allocator.free(path);
            var sst = try builder.build(sst_id, self.block_cache.clone(), path);
            errdefer sst.deinit();

            var sst_ptr = try SsTablePtr.create(self.allocator, sst);
            errdefer sst_ptr.deinit();

            try new_ssts.append(sst_ptr);
        }
        try iter.next();
    }
    // 剩余的数据单独一个 SSTable
    if (builder.estimatedSize() &amp;gt; 0) {
        const sst_id = self.getNextSstId();
        const path = try self.pathOfSst(sst_id);
        defer self.allocator.free(path);
        var sst = try builder.build(sst_id, self.block_cache.clone(), path);
        errdefer sst.deinit();
        var sst_ptr = try SsTablePtr.create(self.allocator, sst);
        errdefer sst_ptr.deinit();
        try new_ssts.append(sst_ptr);
    }
    return new_ssts;
}

&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;替换压缩后的 SST&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;这部分逻辑并不复杂，即删除此次压缩任务中的原有两层数据，用新合并的 SSTable 替换至较低层数据。&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;这里有个需要注意的点，即压缩过程是在一个线程中单独执行的，压缩过程中 LSM-Tree 的原数据可能发生了改变，所以这里执行 SSTable 删除时要注意过滤掉新数据，不能覆盖了有效数据。&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;并发问题是软件中的 Bug 集散地！&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;pub fn applyCompactionResult(
    _: SimpleLeveledCompactionController,
    state: *storage.StorageState,
    task: SimpleLeveledCompactionTask,
    output: []usize,
) !std.ArrayList(usize) {
    var files_to_remove = std.ArrayList(usize).init(state.allocator);
    errdefer files_to_remove.deinit();

    if (task.upper_level) |upper_level| {
        // 删除高层 SSTable 数据，这层数据不会在压缩过程中变更，放心删
        std.debug.assert(sliceEquals(
            task.upper_level_sst_ids.items,
            state.levels.items[upper_level - 1].items,
        ));
        try files_to_remove.appendSlice(task.upper_level_sst_ids.items);
        state.levels.items[upper_level - 1].clearAndFree();
    } else {
        // 删除 L0 数据，需要小心
        try files_to_remove.appendSlice(task.upper_level_sst_ids.items);
        var new_l0_sstables = std.ArrayList(usize).init(state.allocator);
        errdefer new_l0_sstables.deinit();

        {
            var l0_sst_compacted = std.AutoHashMap(usize, struct {}).init(state.allocator);
            defer l0_sst_compacted.deinit();
            for (task.upper_level_sst_ids.items) |sst_id| {
                try l0_sst_compacted.put(sst_id, .{});
            }

            for (state.l0_sstables.items) |sst_id| {
                if (!l0_sst_compacted.remove(sst_id)) { // 不在压缩任务中的 SST 不能删除
                    try new_l0_sstables.append(sst_id);
                }
            }
            std.debug.assert(l0_sst_compacted.count() == 0);
        }
        state.l0_sstables.deinit();
        state.l0_sstables = new_l0_sstables;
    }
    // 低层 SSTable 数据，直接删除
    try files_to_remove.appendSlice(task.lower_level_sst_ids.items);
    state.levels.items[task.lower_level - 1].clearAndFree();
    try state.levels.items[task.lower_level - 1].appendSlice(output);

    return files_to_remove;
}


// sst to remove
var ssts_to_remove = std.ArrayList(SsTablePtr).init(self.allocator);

{
    var new_sst_ids = std.ArrayList(usize).init(self.allocator);
    defer new_sst_ids.deinit();

    self.state_lock.lock();
    defer self.state_lock.unlock();

    for (sstables.items) |sst| {
        const id: usize = @intCast(sst.get().sstId());
        try new_sst_ids.append(id);
        try self.state.sstables.put(id, sst.clone());
    }

    var file_to_remove = try self.compaction_controller.applyCompactionResult(
        &amp;amp;self.state,
        task,
        output.items,
    );
    defer file_to_remove.deinit();

    for (file_to_remove.items) |id| {
        if (self.state.sstables.fetchRemove(id)) |kv| {
            try ssts_to_remove.append(kv.value);
        }
    }
    try self.syncDir();
}

for (ssts_to_remove.items) |sst| {
    const path = try self.pathOfSst(sst.get().sstId());
    defer self.allocator.free(path);
    try std.fs.cwd().deleteFile(path);
}
try self.syncDir();
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;四、总结&lt;/h1&gt; 
&lt;p&gt;我们使用 Zig 语言实现了一个 LSM-Tree 的核心功能，包括 MemTable、SSTable、写流程、各类 Iterator 与数据压缩能力。通过这个项目，我收获了很多心得体会。&lt;/p&gt; 
&lt;h3&gt;了解了 LSM-Tree 的核心流程&lt;/h3&gt; 
&lt;p&gt;以往对 LSM 这个数据结构的多层 SST 设计与写过程早有耳闻，但是读流程的实现不太理解。这个项目解答了我疑惑很久的读流程的实现，特别是 MergeIterator 的算法设计非常巧妙。&lt;/p&gt; 
&lt;h3&gt;摸索了个 zig 语言的智能指针&lt;/h3&gt; 
&lt;p&gt;Zig 语言没有内存安全的保证，为了不想指针乱飞到处泄露，在 Deepseek 的帮助下实现了一个简单的智能指针，极大降低了内存管理的心智负担。&lt;/p&gt; 
&lt;h3&gt;工程经验&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;尽可能多的做 assertion 的工作，可以提前暴露很多 bug。&lt;/li&gt; 
 &lt;li&gt;大型多模块的项目，一定要写单元测试，不然出了 bug 无法分块定位问题。&lt;/li&gt; 
 &lt;li&gt;千万不要把 IO 过程放在锁的范围里，极大的影响性能！&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;文 / 酒米&lt;/p&gt; 
&lt;p&gt;关注得物技术，每周更新技术干货&lt;/p&gt; 
&lt;p&gt;要是觉得文章对你有帮助的话，欢迎评论转发点赞～&lt;/p&gt; 
&lt;p&gt;未经得物技术许可严禁转载，否则依法追究法律责任。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/u/5783135/blog/17821037</link>
            <guid isPermaLink="false">https://my.oschina.net/u/5783135/blog/17821037</guid>
            <pubDate>Thu, 06 Mar 2025 03:24:00 GMT</pubDate>
            <author>原创</author>
        </item>
    </channel>
</rss>