<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - industry - 简体中文</title>
    <link>https://www.oschina.net/news/industry</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news/industry" rel="self" type="application/rss+xml"/>
    <description>已对该 RSS 进行格式化操作：中英字符之间插入空格、使用直角引号、标点符号修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 05 Aug 2025 02:40:53 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>Chrome 139 开发者工具增强 AI 辅助功能</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;Chrome 139 开发者工具&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.chrome.com%2Fblog%2Fnew-in-devtools-139%3Fhl%3Dzh-cn" target="_blank"&gt;已发布&lt;/a&gt;，此次更新重点在于提升产品的可靠性和效率，解决了大量已知问题——从长期存在的视觉故障、可用性问题和设计不一致问题到性能和功能问题。总体而言，将未结问题的数量减少了 27%。&lt;/p&gt; 
&lt;p&gt;在 AI 辅助功能方面，新版本增强了样式设置的交互性。用户现在不仅可以截取屏幕截图，还可以上传任意图片到「AI 辅助」面板与 Gemini 的对话中，以提供更直观的视觉上下文。&lt;/p&gt; 
&lt;p&gt;&lt;img height="682" src="https://static.oschina.net/uploads/space/2025/0805/103609_kGTN_2720166.png" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;此外，「网络」面板也得到了改进，用户现在可以右键点击请求表格的列标题，选择并添加多个请求标头作为新的列，方便查看和分析。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-58bc3a421212513fb74c256cf34c5fc155a.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364232</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364232</guid>
      <pubDate>Tue, 05 Aug 2025 02:37:50 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>智谱推出 Zread.ai 开发效率工具，搭载 GLM-4.5</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span&gt;智谱许&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FkgkxULD3SOu5f9gS4XQH1g" target="_blank"&gt;宣布&lt;/a&gt;推出基于大模型的开发效率工具 Zread.ai，旨在通过 AI 技术一站式解决开发者在接手旧项目、文档撰写以及理解开源项目时的常见痛点。Zread.ai 的核心功能包括一键理解代码、生成知识以及促进协作，能够显著提升开发效率。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;Zread.ai 的核心功能主要体现在三个方面：源项目的深度学习、快速接手历史代码库以及构建团队知识协作系统。开发者可以通过输入任意 GitHub 仓库链接，让 Zread 生成包含架构解析、模块说明、设计模式的 Guide，同时支持多仓库对比、分层解读与 GitHub Trending 项目逻辑拆解。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;此外，Zread 还能够自动梳理项目结构、模块依赖，生成系统性文档，帮助开发者快速进入状态，即便面对复杂的代码也能快速上手。Zread 还提供贡献者图谱、社区评论聚合、交互式批注与问答，支持上传私有项目，构建团队内部的知识库和技术文档体系。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="282" src="https://oscimg.oschina.net/oscnet/up-b39df6faff0e30bca10a982d65b27f2c4bc.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在构建 Zread.ai 的过程中，智谱公司评估了多种大语言模型，最终选择了 GLM-4.5 作为代码分析与文档生成的核心底座。GLM-4.5 在模型代码理解能力、低幻觉、支持 Deep Research 以及 Agent 能力适配等方面表现出色。它能够准确识别代码模块之间的调用关系、架构层级与依赖结构，为生成高质量技术文档和项目导读提供了坚实基础。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在复杂代码场景下，GLM-4.5 的输出稳定性较高，误解代码意图或编造逻辑的情况明显减少，尤其适合用于代码解读和技术问答类任务。针对大型代码库，GLM-4.5 能够进行多轮深入解析，结合上下文与语义线索，对关键技术设计进行追问与深挖，帮助开发者获取更具洞察力的解答。在长上下文理解与技术问答的响应速度、准确率方面，GLM-4.5 也表现稳定，提升了整体交互体验。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;使用 Zread.ai 非常简单，只需四步即可快速上手。首先，打开 Zread.ai，输入 GitHub 仓库链接，系统将自动识别代码结构与核心组件。接着，系统会自动生成项目导读（Guide），包含架构拆解、模块说明与设计范式。然后，开发者可以使用「Ask」功能提问关键技术细节，支持深入代码问答与跨模块追踪。最后，上传私有项目，生成团队专属知识库，为项目构建可持续的文档资产。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;智谱公司表示，GLM-4.5 不仅是模型提供方，更是 Zread.ai 实现「读懂代码、生成知识、服务协作」的核心支撑。未来，智谱将继续探索 GLM-4.5 在智能体集成、团队知识协同等场景下的深度应用，为开发者提供更强大的工具，提升开发效率。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364230</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364230</guid>
      <pubDate>Tue, 05 Aug 2025 02:35:50 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>疑似回应「全员裁员」传言，硅基智能称预计全年新增岗位数百个</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;近日，一家成立 8 年的 AI 独角兽南京硅基智能科技有限公司（下称「硅基智能」）卷入「全员裁员」传言。脉脉平台显示，当前「硅基智能 CEO:准备全员裁员，养不起你们」占据热榜第四。&lt;/p&gt; 
&lt;p&gt;8 月 3 日，硅基智能在微信公众号发出&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fmq8xKJFLo6gCwvG_Vn_rEA" target="_blank"&gt;声明&lt;/a&gt;，称「目前拥有一支稳定的产研与销售团队，且持续在全球范围内扩大招聘规模。2025 年，我们将重点布局杭州、嘉兴、香港、新加坡等地，预计全年新增岗位数百个，2026 年将达到新增数千人的扩张节奏。」&lt;/p&gt; 
&lt;p&gt;&lt;img height="293" src="https://oscimg.oschina.net/oscnet/up-e9194ccac4d60e4e961661160dfc6b4e061.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;几天前，一则疑似硅基智能创始人司马华鹏在公司工作群发言的截图在业内流传，引发关注。&lt;/p&gt; 
&lt;p&gt;截图内文文字显示，司马华鹏@所有人并表示：「各位，昨天我去看研发，只有徐超一个人在加班，公司今天已经做好了全员裁员的计划，算法给港科大和清华做，工程化留几个骨干，其他的都自寻出路，硅基养不起这样的团队，请大家见谅。」&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;针对传言及相关声明，记者于 8 月 4 日向硅基智能求证。硅基智能相关人士表示，「我们暂时不对外发声。团队专心做好产品和业务，团队稳定且在扩展。」&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;根据前述声明，硅基智能称，「2025 年上半年，公司超额完成销售目标，盈利能力逐步增强；下半年刚刚开始一个月，我们已锁定超 3 亿元的 AIGC 订单，并将加快推动「新质生产力赋能中心」在浙江等地落地。同时，新加坡、香港的出海团队也已初具规模。」&lt;/p&gt; 
&lt;p&gt;据了解，硅基智能已于 2025 年 6 月完成来自浙江嘉兴的新一轮数亿元融资，同时也拿到各大银行提供的数亿元授信额度。目前其账面现金可支持 120 个月以上的工资发放，同时拥有近亿元规模的算力硬件资产。&lt;/p&gt; 
&lt;p&gt;硅基智能成立于 2017 年，提供企业级 AIGC 数字人解决方案。其总部位于南京，属于专精特新小巨人企业、高新技术企业，当前已拥有发明授权专利一百多项，其中包括二十多项海外专利。股东包括腾讯、招银国际、国新央企、海松资本、红杉资本、浦信资本、奇虎 360 等，最新估值近 10 亿美元。&lt;/p&gt; 
&lt;p&gt;硅基智能联合创始人、高级副总裁孙凯彼时在 WAIC 某沙龙上谈及公司战略时透露，硅基智能正在从传统的工具收费向」按结果付费」转型。「我们的海外合作伙伴依靠硅基智能提供 AI 数字人技术接口，今年收入就已超过 1 亿美元。真正优秀的 AI 员工，不仅要赚月薪，更应成为客户的合伙人。」&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364227</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364227</guid>
      <pubDate>Tue, 05 Aug 2025 02:28:50 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>深度解析 RocketMQ 核心组件：ConsumeQueue 的设计与实现</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                                        &lt;h2&gt;导语&lt;/h2&gt; 
&lt;p&gt;在分布式消息队列 RocketMQ 中，ConsumeQueue（消费队列） 是消息消费的核心组件之一。它作为 &amp;nbsp;CommitLog 的索引机制，帮助消费者快速定位并拉取消息。如果没有 ConsumeQueue，消费者将无法高效地从海量消息中筛选出自己订阅的数据。&lt;/p&gt; 
&lt;p&gt;本文将基于 RocketMQ 5.0 源码，深入探讨 ConsumeQueue 的设计原理与实现细节。&lt;/p&gt; 
&lt;h2&gt;为什么需要 ConsumeQueue？&lt;/h2&gt; 
&lt;p&gt;在深入探讨 ConsumeQueue 之前，我们有必要先了解 RocketMQ 的消息写入和存储方式。&lt;/p&gt; 
&lt;p&gt;CommitLog 是 RocketMQ 的消息存储模块，用户生产的所有消息都持久化存储在该模块中，它具备两个特点：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;使用的持久化存储是一个文件队列，文件保存于指定目录下，每个文件的大小是固定的，通常是 1GB。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;只有一个文件可写入，且仅支持追加写，文件写满后自动切换至新的文件。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;RocketMQ 设计者出于写入优先的考虑，没有为不同 Topic 队列的消息分配不同的存储文件，而是将消息直接写入 CommitLog，不同 Topic 的消息混合分布在 CommitLog 的文件中。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/f7014d6f-45a9-4e8c-aca4-4eca355e5e5b.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;从上图中可以看出，尽管消息的写入非常高效，但是消费者需要按照其订阅的 Topic 来从 CommitLog 中读取该 Topic 的消息，显而易见，RocketMQ 需要一种索引机制来快速读取指定 Topic 队列的消息，这正是 ConsumeQueue 要做的事情。&lt;/p&gt; 
&lt;h2&gt;ConsumeQueue 的设计原理&lt;/h2&gt; 
&lt;p&gt;ConsumeQueue 作为 RocketMQ 的消息索引枢纽，其设计核心在于高效映射逻辑队列与物理存储。我们通过下面的图示来介绍 ConsumeQueue 的核心设计：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/36fce070-c2fd-4a3d-9449-0650f6c9946f.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;每个 Topic 队列有其对应的唯一的 ConsumeQueue，当一条消息写入到 CommitLog 后，RocketMQ 会构建该消息的索引，按异步方式将其写入到对应 Topic 队列的 ConsumeQueue 中。使用索引可以快速定位到消息在 CommitLog 文件的位置并读取它。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;消息索引对象在 ConsumeQueue 中的位置被称为 Offset，是个从 0 开始的序号数，maxOffset 即 ConsumeQueue 索引的最大 Offset，会随着新消息的写入递增。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;基于这个设计，消费者通过与 ConsumeQueue 的 Offset 交互来实现消息的消费。最常见的场景就是，我们记录消费组在 ConsumeQueue 上当前消费的 Offset，那么消费者下线后再上线仍然可从上次消费的位置继续消费。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;基于文件的传统实现方案&lt;/h2&gt; 
&lt;h3&gt;数据存储与格式&lt;/h3&gt; 
&lt;p&gt;与 CommitLog 类似，ConsumeQueue 使用文件队列来持久化存储消息索引。ConsumeQueue 使用的文件目录所在路径由其对应的 Topic 队列确定，举例说明，一个名为 ABC 的 Topic，其队列 0 所在的文件目录路径是 /data/rocketmq_data/store/consumequeue/abc/0/。消息的索引对象是固定的 20 个字节大小，其内部格式定义见下图。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/9c1e2702-a9f4-4d84-9bd8-3c5f1b5d7b18.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;为了方便描述，从这里开始我们将索引对象叫作 CqUnit。ConsumeQueue 的文件队列中每个文件的大小是固定的，默认配置可存储 30 万个 CqUnit，当文件写满后，会切换到新文件进行写入。文件名称的命名方式是有讲究的，它以文件存储的第一个 CqUnit 的 Offset 作为名称，这样做的好处是，按 Offset 查询 CqUnit 时，可以根据文件名称，快速定位到该 Offset 所在的文件，大幅减少对文件的读取操作频次。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/dc6e56f6-83c4-4e99-92f5-4c2702207472.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;构建过程&lt;/h3&gt; 
&lt;p&gt;当消息写入到 CommitLog 后，该消息对于消费者是不可见的，只有在 ConsumeQueue 中增加这条消息的 CqUnit 后，消费者才能消费到这条消息，因此写入消息时须立刻往 ConsuemQueue 写入消息的 CqUnit。我们需要给每一条消息指定其在 ConsumeQueue 中的 Offset，QueueOffsetOperator 类维护了一个 Topic 队列与其当前 &amp;nbsp;Offset 的表，当写入一条新消息时，DefaultMessageStore 从 QueueOffsetOperator 中取出该 Topic 队列的当前 Offset，将其写入到消息体中，在消息成功写入到 CommitLog 后，指示 QueueOffsetOperator 更新为当前 Offset + 1。为了防止其他写入线程并发访问 Topic 队列的当前 Offset，在读取和修改 Offset 期间，会使用一个 ReentrantLock 锁定该 Topic 队列。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/bd666fd0-892a-4923-8f29-f459737f4981.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;ReputMessageService 作为异步任务，会不停的读取 CommitLog，当有新的消息写入，它会立即读取到该消息，然后根据消息体构建一个 DispatchRequest 对象，CommitLogDispatcherBuildConsumeQueue 处理 DispatchRequest 对象，最终将 CqUnit 写入到 ConsumeQueue 的存储中。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/89cd3d56-7582-4cbe-ae34-1478e76b2a7c.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;按 Offset 查找消息&lt;/h3&gt; 
&lt;p&gt;消费者通常是从某个 Offset 开始消费消息的，比如消费者下线后再次上线会从上次消费的 Offset 开始消费。DefaultMessageStore 的 GetMessage 方法实现从一个 Topic 队列中拉取一批消息的功能，每次拉取要指定读取的起始 Offset 以及该批次读取的最大消息数量。下面截取了部分源码展示实现的基本思路：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp;@Override
    public GetMessageResult getMessage(final String group, final String topic, final int queueId, final long offset,
        final int maxMsgNums, final int maxTotalMsgSize, final MessageFilter messageFilter) {
        long beginTime = this.getSystemClock().now();
        GetMessageStatus status = GetMessageStatus.NO_MESSAGE_IN_QUEUE;
        long nextBeginOffset = offset;
        long minOffset = 0;
        long maxOffset = 0;
        GetMessageResult getResult = new GetMessageResult();
        final long maxOffsetPy = this.commitLog.getMaxOffset();
        ConsumeQueueInterface consumeQueue = findConsumeQueue(topic, queueId);
        if (consumeQueue != null) {
            minOffset = consumeQueue.getMinOffsetInQueue();
            maxOffset = consumeQueue.getMaxOffsetInQueue();
            if (maxOffset == 0) {
            //             
            } else {
                long maxPullSize = Math.max(maxTotalMsgSize, 100);
                if (maxPullSize &amp;gt; MAX_PULL_MSG_SIZE) {
                    LOGGER.warn("The max pull size is too large maxPullSize={} topic={} queueId={}", maxPullSize, topic, queueId);
                    maxPullSize = MAX_PULL_MSG_SIZE;
                }
                status = GetMessageStatus.NO_MATCHED_MESSAGE;
                long maxPhyOffsetPulling = 0;
                int cqFileNum = 0;
                while (getResult.getBufferTotalSize() &amp;lt;= 0
                    &amp;amp;&amp;amp; nextBeginOffset &amp;lt; maxOffset
                    &amp;amp;&amp;amp; cqFileNum++ &amp;lt; this.messageStoreConfig.getTravelCqFileNumWhenGetMessage()) {
                    ReferredIterator&amp;lt;CqUnit&amp;gt; bufferConsumeQueue = null;
                    try {
                        bufferConsumeQueue = consumeQueue.iterateFrom(group, nextBeginOffset, maxMsgNums);
                        long nextPhyFileStartOffset = Long.MIN_VALUE;
                        long expiredTtlOffset = -1;
                        while (bufferConsumeQueue.hasNext() &amp;amp;&amp;amp; nextBeginOffset &amp;lt; maxOffset) {
                            CqUnit cqUnit = bufferConsumeQueue.next();
                            long offsetPy = cqUnit.getPos();
                            int sizePy = cqUnit.getSize();
                            SelectMappedBufferResult selectResult = this.commitLog.getMessage(offsetPy, sizePy);
                            getResult.addMessage(selectResult, cqUnit.getQueueOffset(), cqUnit.getBatchNum());
                            status = GetMessageStatus.FOUND;
                            nextPhyFileStartOffset = Long.MIN_VALUE;
                        }
                    } catch (RocksDBException e) {
                        ERROR_LOG.error("getMessage Failed. cid: {}, topic: {}, queueId: {}, offset: {}, minOffset: {}, maxOffset: {}, {}",
                            group, topic, queueId, offset, minOffset, maxOffset, e.getMessage());
                    } finally {
                        if (bufferConsumeQueue != null) {
                            bufferConsumeQueue.release();
                        }
                    }
                }
                long diff = maxOffsetPy - maxPhyOffsetPulling;
                long memory = (long) (StoreUtil.TOTAL_PHYSICAL_MEMORY_SIZE
                    * (this.messageStoreConfig.getAccessMessageInMemoryMaxRatio() / 100.0));
                getResult.setSuggestPullingFromSlave(diff &amp;gt; memory);
            }
        } else {
            status = GetMessageStatus.NO_MATCHED_LOGIC_QUEUE;
            nextBeginOffset = nextOffsetCorrection(offset, 0);
        }
        getResult.setStatus(status);
        getResult.setNextBeginOffset(nextBeginOffset);
        getResult.setMaxOffset(maxOffset);
        getResult.setMinOffset(minOffset);
        return getResult;
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;上述代码片段的要点：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Topic 队列的 ConsumeQueue 的 IterateFrom 方法依据 Offset 生成一个 Iterator 对象。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;在 Iterator 有效的情况，不断从 Iterator 拉取 CqUnit 对象，即按 Offset 顺序读取 CqUnit。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;使用 CqUnit 对象中的 OffsetPy 和 SizePy 从 CommitLog 中读取消息内容，返回给消费者。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;接下来，我们介绍 ConsumeQueue 的 IterateFrom 方法是如何读取 CqUnit 的。从下面的源码中可以看到，GetIndexBuffer 方法先从 MappedFileQueue 中找到 Offset 所在的 MappedFile，然后找到 Offset 在 MappedFile 中的位置，从该位置读取文件剩余的内容。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;public SelectMappedBufferResult getIndexBuffer(final long startIndex) {
        int mappedFileSize = this.mappedFileSize;
        long offset = startIndex * CQ_STORE_UNIT_SIZE;
        if (offset &amp;gt;= this.getMinLogicOffset()) {
            MappedFile mappedFile = this.mappedFileQueue.findMappedFileByOffset(offset);
            if (mappedFile != null) {
                return mappedFile.selectMappedBuffer((int) (offset % mappedFileSize));
            }
        }
        return null;
    }
    @Override
    public ReferredIterator&amp;lt;CqUnit&amp;gt; iterateFrom(long startOffset) {
        SelectMappedBufferResult sbr = getIndexBuffer(startOffset);
        if (sbr == null) {
            return null;
        }
        return new ConsumeQueueIterator(sbr);
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ConsumeQueueIterator 的 Next 方法和 hasNext 方法是对 getIndexBuffer 方法返回的 SelectMappedBufferResult 对象，即文件内容的 ByteBuffer，进行访问。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp;private class ConsumeQueueIterator implements ReferredIterator&amp;lt;CqUnit&amp;gt; {
        private SelectMappedBufferResult sbr;
        private int relativePos = 0;
        public ConsumeQueueIterator(SelectMappedBufferResult sbr) {
            this.sbr = sbr;
            if (sbr != null &amp;amp;&amp;amp; sbr.getByteBuffer() != null) {
                relativePos = sbr.getByteBuffer().position();
            }
        }
        @Override
        public boolean hasNext() {
            if (sbr == null || sbr.getByteBuffer() == null) {
                return false;
            }
            return sbr.getByteBuffer().hasRemaining();
        }
        @Override
        public CqUnit next() {
            if (!hasNext()) {
                return null;
            }
            long queueOffset = (sbr.getStartOffset() + sbr.getByteBuffer().position() - relativePos) / CQ_STORE_UNIT_SIZE;
            CqUnit cqUnit = new CqUnit(queueOffset,
                sbr.getByteBuffer().getLong(),
                sbr.getByteBuffer().getInt(),
                sbr.getByteBuffer().getLong());
            return cqUnit;
        }
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;我们再讲下 MappedFileQueue 的 FindMappedFileByOffset 方法，该方法从其维护的文件队列中查找到 Offset 所在的文件。前面我们介绍过，ConsumeQueue 的文件队列中的文件是按 Offset 命名的，MappedFile 的 GetFileFromOffset 就是文件的名称，那么只需要按照 Offset 除以文件的大小便可得文件在队列中的位置。这里要注意的是，这个位置必须要先减去 FirstMappedFile 的位置后才是有效的，因为 ConsumeQueue 会定期清除过期的文件，所以 ConsumeQueue 管理的 MappedFileQueue 的第一个文件对应的 Offset 未必是 0。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp;public MappedFile findMappedFileByOffset(final long offset, final boolean returnFirstOnNotFound) {
        try {
            MappedFile firstMappedFile = this.getFirstMappedFile();
            MappedFile lastMappedFile = this.getLastMappedFile();
            if (firstMappedFile != null &amp;amp;&amp;amp; lastMappedFile != null) {
                if (offset &amp;lt; firstMappedFile.getFileFromOffset() || offset &amp;gt;= lastMappedFile.getFileFromOffset() + this.mappedFileSize) {
                    LOG_ERROR.warn("Offset not matched. Request offset: {}, firstOffset: {}, lastOffset: {}, mappedFileSize: {}, mappedFiles count: {}",
                        offset,
                        firstMappedFile.getFileFromOffset(),
                        lastMappedFile.getFileFromOffset() + this.mappedFileSize,
                        this.mappedFileSize,
                        this.mappedFiles.size());
                } else {
                    int index = (int) ((offset / this.mappedFileSize) - (firstMappedFile.getFileFromOffset() / this.mappedFileSize));
                    MappedFile targetFile = null;
                    try {
                        targetFile = this.mappedFiles.get(index);
                    } catch (Exception ignored) {
                    }
                    if (targetFile != null &amp;amp;&amp;amp; offset &amp;gt;= targetFile.getFileFromOffset()
                        &amp;amp;&amp;amp; offset &amp;lt; targetFile.getFileFromOffset() + this.mappedFileSize) {
                        return targetFile;
                    }
                    for (MappedFile tmpMappedFile : this.mappedFiles) {
                        if (offset &amp;gt;= tmpMappedFile.getFileFromOffset()
                            &amp;amp;&amp;amp; offset &amp;lt; tmpMappedFile.getFileFromOffset() + this.mappedFileSize) {
                            return tmpMappedFile;
                        }
                    }
                }
                if (returnFirstOnNotFound) {
                    return firstMappedFile;
                }
            }
        } catch (Exception e) {
            log.error("findMappedFileByOffset Exception", e);
        }
        return null;
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;按时间戳查找消息&lt;/h3&gt; 
&lt;p&gt;除了从指定 Offset 消费消息这种方式，消费者还有回溯到某个时间点开始消费的需求，这要求 RocketMQ 支持查询指定的 Timestamp 所在的 Offset，然后从这个 Offset 开始消费消息。&lt;/p&gt; 
&lt;p&gt;我们可以从 ConsumeQueue 的 GetOffsetInQueueByTime 方法直接了解按时间戳查找消息的具体实现。&lt;/p&gt; 
&lt;p&gt;消息是按时间先后写入的，ConsumeQueue 文件队列中的 CqUnit 也是按时间先后排列的，那么每个 MappedFile 都对应一段时间区间内的 CqUnit。从下面代码可以看出，我们可以先根据 Timestamp 找到其落在时间区间的 MappedFile，然后在该 MappedFile 里查找最接近该 Timestamp 的 CqUnit。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@Override
    public long getOffsetInQueueByTime(final long timestamp, final BoundaryType boundaryType) {
        MappedFile mappedFile = this.mappedFileQueue.getConsumeQueueMappedFileByTime(timestamp,
            messageStore.getCommitLog(), boundaryType);
        return binarySearchInQueueByTime(mappedFile, timestamp, boundaryType);
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;GetConsumeQueueMappedFileByTime 的具体实现主要分为两个部分：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;找到每个 MappedFile 的 StartTimestamp 和 StopTimestamp，即 MappedFile 里第一个 CqUnit 对应消息的时间戳和最后一个 CqUnit 对应消息的时间戳，需要访问两次 CommitLog 来得到消息内容。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;使用 Timestamp 和每个 MappedFile 的 StartTimestamp 和 StopTimestamp 比较。当 Timestamp 落在某个 MappedFile 的 StartTimestamp 和 StopTimestamp 区间内时，那么该 MappedFile 是下一步查找 CqUnit 的目标。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;接下来，要按照二分查找法在该 MappedFile 中找到最接近 Timestamp 的 CqUnit。根据二分查找的法则，每次查找需要比较中间位置的 CqUnit 引用消息的存储时间和目标 Timestamp 以确定下一个查找区间，直至 CqUnit 满足最接近目标 Timestamp 的条件。要注意的是，获取 CqUnit 引用消息的存储时间需从 CommitLog 中读取消息。&lt;/p&gt; 
&lt;h2&gt;基于 RocksDB 的优化方案&lt;/h2&gt; 
&lt;p&gt;尽管基于文件的实现比较直观，但是当 Topic 队列达到一定数量后，会出现明显的性能和可用性问题。Topic 队列数量越多，代表着 ConsumeQueue 文件越多，产生的随机读写也就越多，这会影响系统整体的 IO 性能，导致出现生产消费 TPS 不断下降，延迟不断增高的趋势。在我们内部的测试环境和客户的生产环境中，我们都发现使用的队列数过多直接影响系统的可用性，而且我们无法通过不断升级 Broker 节点配置来消除这种影响，因此我们腾讯云 TDMQ RocketMQ 版在产品控制枱上会限制客户可创建的 Topic 数量以确保消息服务的稳定性。&lt;/p&gt; 
&lt;p&gt;那么有没有办法能够解决上面的问题让服务能够承载更多的 Topic 呢？我们可以把 ConsumeQueue 提供的功能理解为使用 Topic 队列的 Offset 来找到 CqUnit，那么 Topic 队列和 Offset 构成了 Key，CqUnit 是 Value，是一个典型的 KV 使用场景。在单机 KV 存储的软件里，最著名的莫过于 RocksDB 了，它被广泛使用于 Facebook，LinkedIn 等互联网公司的业务中。从下面的设计图看，RocksDB 基于 SSTable + MemTable 的实现能够提供高效写入和查找 KV 的能力，有兴趣的读者可以研究下 RocksDB 的具体实现 (&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Ffacebook%2Frocksdb%2Fwiki%2FRocksDB-Overview" target="_blank"&gt;https://github.com/facebook/rocksdb/wiki/RocksDB-Overview&lt;/a&gt;)，这里不展开说明。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/097e131f-0b3f-41fa-9973-57cab02459f0.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;如果我们使用 RocksDB 读写 CqUnit，那么 ConsumeQueue 文件数量不会随着 Topic 队列的数量线性增长，便不必担心由此带来的 IO 开销。&lt;/p&gt; 
&lt;p&gt;下面我们来介绍如何使用 RocksDB 来实现 ConsumeQueue。&lt;/p&gt; 
&lt;h3&gt;数据存储与格式&lt;/h3&gt; 
&lt;p&gt;在基于 RocksDB 的实现里，RocketMQ 使用两个 ColumnFamily 来管理不同类型的数据，这里不熟悉 RocksDB 的读者可以将 ColumnFamily 视作 MySQL 里的 Table。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;第一个 ColumnFamiliy，简称为 DefaultColumnFamily，用于管理 CqUnit 数据。&lt;/p&gt; &lt;p&gt;Key 的内容格式定义参考下图，其包含 Topic 名称、QueueId 和 ConsumeQueue 的 Offset。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/831d29f2-7280-4583-8cf7-794df3d6d057.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Value 的内容格式，与前文中文件实现里的索引对象定义类似，但是多了一个消息存储时间的字段。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/cbba8329-dbd2-4dd0-9557-a8e455bcb831.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;第二个 ColumnFamily，简称为 OffsetColumnFamily，用于管理 Topic 队列的 MaxOffset 和 MinOffset。&lt;/p&gt; &lt;p&gt;MaxOffset 是指 Topic 队列最新一条消息在 ConsumeQueue 中的 Offset，随着消息的新增而变化。MinOffset 是指 Topic 队列最早一条消息在 ConsumeQueue 中的 Offset，当消息过期被删除后发生变化。MaxOffset 和 MinOffset 确定消费者可读取消息的范围，在基于文件的实现里，通过访问 ConsumeQueue 文件队列里的队尾和队首文件得到这两个数值。而在 RocksDB 的实现里，我们单独保存这两个数值。&lt;/p&gt; &lt;p&gt;下图是 Key 的格式定义，其包含 Topic 名称、QueueId 以及用于标记是 MaxOffset 或 MinOffset 的字段。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/84dd4df8-a509-4f23-8467-cd67f7b5be20.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Value 保存 ConsumeQueue 的 Offset，以及该 Offset 对应消息在 CommitLog 的位置。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/0e4c53a0-12b1-44fb-a457-3694f870b0c9.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;构建过程&lt;/h3&gt; 
&lt;p&gt;ConsumeQueue 的 CqUnit 的构建过程与前文中基于文件的实现的过程一致，此处不再赘述，不同的是前文中 ReputMessageService 使用的 ConsumeQueueStore 被替换为 RocksDBConsumeQueueStore。在这个过程中，RocksDBConsumeQueueStore 主要完成两件事：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;往 DefaultColumnFamily 写入消息对应的 CqUnit。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;往 OffsetColumnFamily 更新消息对应 Topic 队列的 maxOffset。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp;private boolean putMessagePosition0(List&amp;lt;DispatchRequest&amp;gt; requests) {
        if (!this.rocksDBStorage.hold()) {
            return false;
        }
        try (WriteBatch writeBatch = new WriteBatch(); WriteBatch lmqTopicMappingWriteBatch = new WriteBatch()) {
            final int size = requests.size();
            if (size == 0) {
                return true;
            }
            long maxPhyOffset = 0;
            for (int i = size - 1; i &amp;gt;= 0; i--) {
                final DispatchRequest request = requests.get(i);
                DispatchEntry entry = DispatchEntry.from(request);
                dispatch(entry, writeBatch, lmqTopicMappingWriteBatch);
                dispatchLMQ(request, writeBatch, lmqTopicMappingWriteBatch);
                final int msgSize = request.getMsgSize();
                final long phyOffset = request.getCommitLogOffset();
                if (phyOffset + msgSize &amp;gt;= maxPhyOffset) {
                    maxPhyOffset = phyOffset + msgSize;
                }
            }
            // put lmq topic Mapping to DB if there has mapping exist
            if (lmqTopicMappingWriteBatch.count() &amp;gt; 0) {
                // write max topicId and all the topicMapping as atomic write
                ConfigHelperV2.stampMaxTopicSeqId(lmqTopicMappingWriteBatch, this.topicSeqIdCounter.get());
                this.configStorage.write(lmqTopicMappingWriteBatch);
                this.configStorage.flushWAL();
            }
            this.rocksDBConsumeQueueOffsetTable.putMaxPhyAndCqOffset(tempTopicQueueMaxOffsetMap, writeBatch, maxPhyOffset);
            this.rocksDBStorage.batchPut(writeBatch);
            this.rocksDBConsumeQueueOffsetTable.putHeapMaxCqOffset(tempTopicQueueMaxOffsetMap);
            long storeTimeStamp = requests.get(size - 1).getStoreTimestamp();
            if (this.messageStore.getMessageStoreConfig().getBrokerRole() == BrokerRole.SLAVE
                || this.messageStore.getMessageStoreConfig().isEnableDLegerCommitLog()) {
                this.messageStore.getStoreCheckpoint().setPhysicMsgTimestamp(storeTimeStamp);
            }
            this.messageStore.getStoreCheckpoint().setLogicsMsgTimestamp(storeTimeStamp);
            notifyMessageArriveAndClear(requests);
            return true;
        } catch (Exception e) {
            ERROR_LOG.error("putMessagePosition0 failed.", e);
            return false;
        } finally {
            tempTopicQueueMaxOffsetMap.clear();
            consumeQueueByteBufferCacheIndex = 0;
            offsetBufferCacheIndex = 0;
            this.rocksDBStorage.release();
        }
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;按 offset 查找消息&lt;/h3&gt; 
&lt;p&gt;在前文中我们已介绍过按 Offset 查找消息的流程，RocksDB 的实现里，DefaultMessageStore 的 GetMessage 方法中使用的 ConsumeQueue 被替换成了 RocksDBConsumeQueue。这里我们只关注其 IterateFrom 方法的实现，以下是该方法的代码片段。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;public ReferredIterator&amp;lt;CqUnit&amp;gt; iterateFrom(String group, long startIndex, int count) throws RocksDBException {
        long maxCqOffset = getMaxOffsetInQueue();
        if (startIndex &amp;lt; maxCqOffset) {
            int num = Math.min((int) (maxCqOffset - startIndex), count);
            if (MixAll.isLmq(topic) || PopAckConstants.isStartWithRevivePrefix(topic)) {
                return iterateUseMultiGet(startIndex, num);
            }
            if (num &amp;lt;= messageStore.getMessageStoreConfig().getUseScanThreshold()) {
                return iterateUseMultiGet(startIndex, num);
            }
            if (!messageStore.getMessageStoreConfig().isEnableScanIterator()) {
                return iterateUseMultiGet(startIndex, num);
            }
            final String scannerIterKey = group + "-" + Thread.currentThread().getId();
            ScanRocksDBConsumeQueueIterator scanRocksDBConsumeQueueIterator = scanIterators.get(scannerIterKey);
            if (scanRocksDBConsumeQueueIterator == null) {
                if (RocksDBConsumeQueue.this.messageStore.getMessageStoreConfig().isEnableRocksDBLog()) {
                    LOG.info("new ScanIterator Group-threadId{} Topic:{}, queueId:{},startIndex:{}, count:{}",
                        scannerIterKey, topic, queueId, startIndex, num);
                }
                ScanRocksDBConsumeQueueIterator newScanIterator = new ScanRocksDBConsumeQueueIterator(startIndex, num);
                scanRocksDBConsumeQueueIterator = scanIterators.putIfAbsent(scannerIterKey, newScanIterator);
                if (scanRocksDBConsumeQueueIterator == null) {
                    scanRocksDBConsumeQueueIterator = newScanIterator;
                } else {
                    newScanIterator.closeRocksIterator();
                }
                return scanRocksDBConsumeQueueIterator;
            }
            if (!scanRocksDBConsumeQueueIterator.isValid()) {
                scanRocksDBConsumeQueueIterator.closeRocksIterator();
                if (RocksDBConsumeQueue.this.messageStore.getMessageStoreConfig().isEnableRocksDBLog()) {
                    LOG.info("new ScanIterator not valid Group-threadId{} Topic:{}, queueId:{},startIndex:{}, count:{}",
                        scannerIterKey, topic, queueId, startIndex, count);
                }
                ScanRocksDBConsumeQueueIterator newScanIterator = new ScanRocksDBConsumeQueueIterator(startIndex, num);
                scanIterators.put(scannerIterKey, newScanIterator);
                return newScanIterator;
            } else {
                if (RocksDBConsumeQueue.this.messageStore.getMessageStoreConfig().isEnableRocksDBLog()) {
                    LOG.info("new ScanIterator valid then reuse Group-threadId{} Topic:{}, queueId:{},startIndex:{}, count:{}",
                        scannerIterKey, topic, queueId, startIndex, count);
                }
                scanRocksDBConsumeQueueIterator.reuse(startIndex, num);
                return scanRocksDBConsumeQueueIterator;
            }
        }
        return null;
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;在上面的代码中，首先通过 GetMaxOffsetInQueue 方法获取该 Topic 队列 ConsumeQueue 的 MaxOffset，MaxOffset 结合 Count 参数共同指定 Iterator 扫描的 Offset 区间。&lt;/p&gt; 
&lt;p&gt;然后，我们可以看到 IterateFrom 方法中根据不同的条件判断分支返回不同类型的 Iterator 类对象，RocksDBConsumeQueueIterator 和 ScanRocksDBConsumeQueueIterator。下面是 &amp;nbsp;IteratorUseMultiGet 方法中创建 RocksDBConsumeQueueIterator 对象的调用链中最核心的代码， RangeQuery 方法根据 StartIndex 和 Num 构建了要查询的 Key 列表，然后调用 RocksDB 的 MultiGet 方法查询到 Key 列表对应的 Value 列表，RocksDBConsumeQueueIterator 使用该 Value 列表上提供迭代器的功能。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;nbsp; &amp;nbsp;public List&amp;lt;ByteBuffer&amp;gt; rangeQuery(final String topic, final int queueId, final long startIndex,
        final int num) throws RocksDBException {
        final byte[] topicBytes = topic.getBytes(StandardCharsets.UTF_8);
        final List&amp;lt;ColumnFamilyHandle&amp;gt; defaultCFHList = new ArrayList&amp;lt;&amp;gt;(num);
        final ByteBuffer[] resultList = new ByteBuffer[num];
        final List&amp;lt;Integer&amp;gt; kvIndexList = new ArrayList&amp;lt;&amp;gt;(num);
        final List&amp;lt;byte[]&amp;gt; kvKeyList = new ArrayList&amp;lt;&amp;gt;(num);
        for (int i = 0; i &amp;lt; num; i++) {
            ByteBuffer keyBB;
            // must have used topicMapping
            if (this.topicMappingTable != null) {
                Long topicId = topicMappingTable.get(topic);
                if (topicId == null) {
                    throw new RocksDBException("topic: " + topic + " topicMapping not existed error when rangeQuery");
                }
                keyBB = buildCQFixKeyByteBuffer(topicId, queueId, startIndex + i);
            } else {
                keyBB = buildCQKeyByteBuffer(topicBytes, queueId, startIndex + i);
            }
            kvIndexList.add(i);
            kvKeyList.add(keyBB.array());
            defaultCFHList.add(this.defaultCFH);
        }
        int keyNum = kvIndexList.size();
        if (keyNum &amp;gt; 0) {
            List&amp;lt;byte[]&amp;gt; kvValueList = this.rocksDBStorage.multiGet(defaultCFHList, kvKeyList);
            final int valueNum = kvValueList.size();
            if (keyNum != valueNum) {
                throw new RocksDBException("rocksdb bug, multiGet");
            }
            for (int i = 0; i &amp;lt; valueNum; i++) {
                byte[] value = kvValueList.get(i);
                if (value == null) {
                    continue;
                }
                ByteBuffer byteBuffer = ByteBuffer.wrap(value);
                resultList[kvIndexList.get(i)] = byteBuffer;
            }
        }
        final int resultSize = resultList.length;
        List&amp;lt;ByteBuffer&amp;gt; bbValueList = new ArrayList&amp;lt;&amp;gt;(resultSize);
        for (int i = 0; i &amp;lt; resultSize; i++) {
            ByteBuffer byteBuffer = resultList[i];
            if (byteBuffer == null) {
                break;
            }
            bbValueList.add(byteBuffer);
        }
        return bbValueList;
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ScanRocksDBConsumeQueueIterator 则是使用了 RocksDB 的 Iterator 特性（&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Ffacebook%2Frocksdb%2Fwiki%2FIterator%EF%BC%89%EF%BC%8C%E7%9B%B8%E6%AF%94" target="_blank"&gt;https://github.com/facebook/rocksdb/wiki/Iterator），相比&lt;/a&gt; MultiGet，其拥有更好的性能。&lt;/p&gt; 
&lt;p&gt;下面是 ScanQuery 的实现，代码比较简洁，指定 Iterator 的 BeginKey 和 UpperKey，再调用 RocksDB 的 API 返回 Iterator 对象。&lt;/p&gt; 
&lt;p&gt;BeginKey 是通过 Topic 队列信息和 StartIndex 参数构造的 Key。UpperKey 的构造比较精妙，还记得在 DefaultColumnFamily 介绍里 Key 的格式吧，Key 的倒数第二个部分是 CTRL_1，作为 CqUnit 的 Key 时是个常量，Unicode 值为 1。构造 UpperKey 时，CTRL_1 被替换为 CTRL_2， Uinicode 值为 2，这样能保证 Iterator 扫描区间的上限不超过 Topic 队列 Offset 的理论最大值。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;public RocksIterator scanQuery(final String topic, final int queueId, final long startIndex,
        ReadOptions scanReadOptions) throws RocksDBException {
        final ByteBuffer beginKeyBuf = getSeekKey(topic, queueId, startIndex);
        if (scanReadOptions.iterateUpperBound() == null) {
            ByteBuffer upperKeyForInitScanner = getUpperKeyForInitScanner(topic, queueId);
            byte[] buf = new byte[upperKeyForInitScanner.remaining()];
            upperKeyForInitScanner.slice().get(buf);
            scanReadOptions.setIterateUpperBound(new Slice(buf));
        }
        RocksIterator iterator = this.rocksDBStorage.scan(scanReadOptions);
        iterator.seek(beginKeyBuf.slice());
        return iterator;
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;按时间戳查找消息&lt;/h3&gt; 
&lt;p&gt;与基于文件的实现类似，使用 RocksDB 来按时间戳查找消息，首先也需要确定 Topic 队列 ConsumeQueue 的 MinOffset 和 MaxOffset，然后使用二分查找法查找到最接近指定时间戳的 CqUnit。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp;@Override
    public long getOffsetInQueueByTime(String topic, int queueId, long timestamp,
        BoundaryType boundaryType) throws RocksDBException {
        final long minPhysicOffset = this.messageStore.getMinPhyOffset();
        long low = this.rocksDBConsumeQueueOffsetTable.getMinCqOffset(topic, queueId);
        Long high = this.rocksDBConsumeQueueOffsetTable.getMaxCqOffset(topic, queueId);
        if (high == null || high == -1) {
            return 0;
        }
        return this.rocksDBConsumeQueueTable.binarySearchInCQByTime(topic, queueId, high, low, timestamp,
            minPhysicOffset, boundaryType);
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;与基于文件的实现不同的是，由于 RocksDB 的 CqUnit 里保存了消息存储的时间，比较时间戳时不必再读取 CommitLog 获取消息的存储时间，这样提升了查找的时间效率。&lt;/p&gt; 
&lt;h2&gt;总结及展望&lt;/h2&gt; 
&lt;p&gt;本文和读者分享了 ConsumeQueue 的设计与实现，着重介绍其在消息消费场景的应用。鉴于篇幅限制，仍有许多细节未涉及，比如 ConsumeQueue 的容错恢复、过期清理机制等。近些年，RocketMQ 往 Serveless 化方向发展，在 5.0 的架构里，已经将计算和存储分离，Proxy 作为计算集群，Broker 作为存储集群。从实际应用上来讲，Broker 作为存储角色，从计算的角色释放出来之后，多出的性能和资源应该用于承载更多的 Topic，而基于文件的 ConsumeQueue 实现限制了 Broker 的上限，因此我们需要 RocksDB 的实现方案来解决这个问题。&lt;/p&gt; 
&lt;p&gt;目前，腾讯云的 TDMQ RabbitMQ Serveless、MQTT 产品均基于 RocketMQ 5.0 的架构部署运行，Broker 集群已采用 RocksDB 的方案支持百万级的 Topic 队列，满足 RabbitMQ 和 MQTT 协议需要大量 Topic 支持的场景。在腾讯云 RocketMQ 5.0 的产品上，我们开始逐渐在新版本中灰度开启该方案，为客户提供更好性能更稳定的消息队列服务。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/4587289/blog/18499646</link>
      <guid isPermaLink="false">https://my.oschina.net/u/4587289/blog/18499646</guid>
      <pubDate>Tue, 05 Aug 2025 02:23:50 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>阿里通义发布开源文生图模型 Qwen-Image</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;阿里通义千问团队开源了其首个图像生成基础模型 Qwen-Image。该模型是一个拥有 200 亿参数的 MMDiT（多模态扩散 Transformer）模型，基于 Apache 2.0 许可证开源。&lt;/p&gt; 
&lt;p&gt;Qwen-Image 在复杂文本渲染和精确图像编辑方面取得了显著进展，尤其在中文文本渲染上表现卓越。&lt;/p&gt; 
&lt;p&gt;Qwen-Image 的主要特性包括：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;卓越的文本渲染能力:&amp;nbsp;Qwen-Image 在复杂文本渲染方面表现出色，支持多行布局、段落级文本生成以及细粒度细节呈现。无论是英语还是中文，均能实现高保真输出。&lt;/li&gt; 
 &lt;li&gt;一致性的图像编辑能力:&amp;nbsp;通过增强的多任务训练范式，Qwen-Image 在编辑过程中能出色地保持编辑的一致性。&lt;/li&gt; 
 &lt;li&gt;强大的跨基准性能表现:&amp;nbsp;在多个公开基准测试中的评估表明，Qwen-Image 在各类生成与编辑任务中均获得 SOTA，是一个强大的图像生成基础模型。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;如需体验 Qwen-Image，访问 QwenChat（chat.qwen.ai) 并选择「图像生成」功能。同时该模型已在魔搭社区与 Hugging Face 开源。&lt;/p&gt; 
&lt;p&gt;ModelScope：https://modelscope.cn/models/Qwen/Qwen-Image&lt;br&gt; Hugging Face：https://huggingface.co/Qwen/Qwen-Image&lt;br&gt; GitHub：https://github.com/QwenLM/Qwen-Image&lt;br&gt; Technical report：https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/Qwen_Image.pdf&lt;br&gt; Demo:&amp;nbsp;https://modelscope.cn/aigc/imageGeneration?tab=advanced&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;示例展示&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;宫崎骏的动漫风格。平视角拍摄，阳光下的古街热闹非凡。一个穿着青衫、手里拿着写着「阿里云」卡片的逍遥派弟子站在中间。旁边两个小孩惊讶的看着他。左边有一家店铺挂着「云存储」的牌子，里面摆放着发光的服务器机箱，门口两个侍衞守护者。右边有两家店铺，其中一家挂着「云计算」的牌子，一个穿着旗袍的美丽女子正看着里面闪闪发光的电脑屏幕；另一家店铺挂着「云模型」的牌子，门口放着一个大酒缸，上面写着「千问」，一位老板娘正在往里面倒发光的代码溶液。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0805/101844_6QPa_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364222</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364222</guid>
      <pubDate>Tue, 05 Aug 2025 02:19:50 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>KiCad - 开源免费电子设计自动化（EDA）套件</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                        
                                                                                    &lt;p style="color:#000000; margin-left:auto; margin-right:auto; text-align:start"&gt;KiCad EDA 是一款开源的电子设计自动化（EDA）软件，基于 GPLv3 开源协议，最初由法国人 Jean-Pierre Charras 于 1992 年推出，现由 KiCad 开源社区维护。&lt;/p&gt;

&lt;p style="color:#000000; margin-left:auto; margin-right:auto; text-align:start"&gt;KiCad 提供了一个完整的设计流程，从原理图到 PCB 布局，以及 3D 模型和 BOM 生成。KiCad 支持多种文件格式，可以与其他 EDA 软件兼容，并且可以在多种操作系统上运行，包括 Windows，Linux 和 Mac OS X，软件包含工程项目管理、原理图设计、线路板绘制、符号库设计、封装库设计、线路板 3D 显示、Gerber 查看、线路板实用计算等工具。&lt;/p&gt;

&lt;p style="color:#000000; margin-left:auto; margin-right:auto; text-align:start"&gt;&lt;img alt="" height="1080" src="https://oscimg.oschina.net/oscnet/up-416b95962f155bd5c9c0b3172706d97017f.png" width="1920" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

&lt;p style="color:#000000; margin-left:auto; margin-right:auto; text-align:start"&gt;&lt;img alt="" height="1080" src="https://oscimg.oschina.net/oscnet/up-a671e278e4aae67095a053103e543bc62a3.png" width="1920" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

&lt;p style="color:#000000; margin-left:auto; margin-right:auto; text-align:start"&gt;&lt;img alt="" height="1080" src="https://oscimg.oschina.net/oscnet/up-6501cd15c8e91e73f511b9edeaeed9ac221.png" width="1920" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

&lt;p&gt;KiCad EDA 官网：&lt;a href="https://www.kicad.org/"&gt;https://www.kicad.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;KiCad EDA 开源中国&amp;nbsp;&lt;a href="https://gitee.com/kicad-eda"&gt;https://gitee.com/kicad-eda&lt;/a&gt;&lt;/p&gt;

                                                                    &lt;/div&gt;
                                                                </description>
      <link>https://www.oschina.net/p/kicad</link>
      <guid isPermaLink="false">https://www.oschina.net/p/kicad</guid>
      <pubDate>Tue, 05 Aug 2025 02:11:50 GMT</pubDate>
    </item>
    <item>
      <title>​Perplexity AI 被指控秘密抓取被禁止的网站内容</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p style="margin-left:0; margin-right:0"&gt;根据互联网基础设施提供商 Cloudflare 的&lt;span&gt;最新&lt;/span&gt;研究&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.cloudflare.com%2Fperplexity-is-using-stealth-undeclared-crawlers-to-evade-website-no-crawl-directives%2F" target="_blank"&gt;报告&lt;/a&gt;，人工智能初创公司 Perplexity 被指控在抓取网站内容时忽视了明确的阻止指令。Cloudflare 表示，他们观察到 Perplexity 在尝试抓取网页时隐藏了自己的身份，以此规避网站的偏好设置。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&lt;img height="312" src="https://oscimg.oschina.net/oscnet/up-cc5a99ff2f39168f9451614222dc2d73118.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Perplexity 等人工智能产品通常依赖于从互联网收集大量数据，而这些初创公司长期以来在未获得许可的情况下抓取文本、图像和视频，以便支持其产品的正常运作。近年来，许多网站通过使用标准的 Robots.txt 文件来应对这一问题，该文件指示搜索引擎和 AI 公司哪些页面可以被索引，哪些页面不可以。然而，当前这些努力的成效并不显著。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;根据 Cloudflare 的分析，Perplexity 似乎通过更改其机器人的 「用户代理」 来绕过这些限制。「用户代理」 是指用于识别网站访问者的设备和版本类型的信号。Cloudflare 还提到，Perplexity 更改了其自治系统网络（ASN），这是一个识别互联网上大型网络的数字标识。Cloudflare 在数万个域名和数百万个请求中观察到了这一行为，凭借机器学习和网络信号的结合成功识别了这一爬虫。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Perplexity 的发言人 Jesse Dwyer 对 Cloudflare 的指控表示反驳，并称其博客文章为 「推销」。他补充称，文中截图显示并没有访问内容。他进一步声称，Cloudflare 所提到的爬虫并非其所拥有的。Cloudflare 表示，他们最初注意到这些问题是由于客户投诉 Perplexity 仍在抓取其网站内容，尽管这些网站已通过 Robots 文件阻止了该爬虫的访问。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Cloudflare 的分析表明，Perplexity 不仅使用了其声明的用户代理，还在其被阻止时利用一个模拟 Google Chrome 的通用浏览器。最终，Cloudflare 决定将 Perplexity 的爬虫从其验证列表中移除，并采取新的技术来阻止其活动。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;值得注意的是，Cloudflare 最近对人工智能爬虫表示反对，并推出了一个市场，允许网站所有者向访问其网站的 AI 爬虫收费。Cloudflare 的首席执行官马修・普林斯曾警告称，人工智能正在破坏互联网的商业模式，尤其是出版商的盈利模式。这并非 Perplexity&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;第一&lt;/span&gt;次面临未经授权抓取的指控，早在去年，《连线》杂志等媒体就曾指控 Perplexity 抄袭其内容。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364217</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364217</guid>
      <pubDate>Tue, 05 Aug 2025 01:59:50 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>「香山」实现业界首个开源芯片的产品级交付与首次规模化应用</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 开源高性能 RISC-V 处理器核「香山」产业落地取得里程碑式突破。7 月 16-19 日，在上海举办的 2025 RISC-V 中国峰会期间，北京开源芯片研究院（以下简称开芯院）在大会报告中宣布第三代「香山」（昆明湖）IP 核已实现了首批量产客户的产品级交付。7 月 26-28 日，世界人工智能大会期间，集成了第二代「香山」（南湖）IP 核的某国产量产 GPGPU 芯片正式亮相，基于该芯片的智能加速卡出货量已上万——「香山」（南湖）IP 核实现规模化应用。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 香山 IP 核在业界首次实现了产品级交付与规模化应用，标志着开源高性能处理器 IP 核正式进入产业落地阶段，为 RISC-V 产业技术研发、商业落地开辟了一条不同于传统 ARM 模式、基于开源模式的新路径。香山 IP 核的首次产品级交付与规模化应用，就如 1990 年代中期开源操作系统 Linux 首次在企业中部署应用，具有重要的里程碑意义，必将产生深远影响。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:center"&gt;&lt;strong&gt;开源高性能 RISC-V 处理器核&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:center"&gt;&lt;strong&gt;第二代「香山」（南湖）已实现首次规模化应用&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 开源高性能 RISC-V 处理器核「香山」，源于中国科学院在 2019 年的前瞻布局。中国科学院计算技术研究所（以下简称计算所）于 2021 年 6 月成功研制了第一代开源高性能 RISC-V 处理器核「香山（雁栖湖）」，性能对标 ARM A73，SPECINT2006 7 分/GHz，是同期全球性能最高的开源处理器核。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 为了加速香山的技术演进和应用落地，加快 RISC-V 生态建设, 2021 年北京市与中国科学院达成战略合作，发挥北京市应用牵引和芯片定义的优势，组织 18 家行业龙头企业和国内顶尖科研单位共同发起成立开芯院。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 2023 年 5 月 26 日，开芯院在中关村论坛上正式对外发布由多家单位联合开发的第二代「香山」（南湖）。这是一款性能对标 ARM Cortex-A76 的高性能开源 RISC-V 处理器核，主频 2GHz@14nm，SPECCPU2006 分值达到 10 分/GHz，专门针对工业控制、汽车、通信等泛工业领域。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 「香山」（南湖）成功带动一批企业加速布局 RISC-V 产品线，开始在一些芯片产品中集成「香山」（南湖）IP 核，取得积极成效。近日，在上海举办的世界人工智能大会上，某国产 GPU 芯片厂商展示的自研智算加速卡中成功集成了「香山」（南湖）IP 核。据了解，该国产 GPU 公司已经实现全国产千卡千亿模型算力集群的交付，正朝着万卡智算集群加速迭代。这标志着「香山」（南湖）IP 核首次实现规模化应用，也推动了 RISC-V 在人工智能智算集群中的产业化落地。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 「香山」(南湖)IP 核作为高性能片内主控 CPU，也被用于芯动科技高性能全功能 GPGPU 芯片「风华 3 号"中,实现 CPU+AI 强强联手。其中，香山 CPU 核负责从主 CPU 卸载的一些关键功能，包括处理跨芯片通讯与数据搬运、启动控制及片内 IP 配置、实现低功耗与动态功耗控制、确保系统稳定运行、提供异常处理能力等。香山 CPU 核与高性能风华 GPU 的结合，在重度负载渲染、高性能 AI 计算、多芯片集群互联等使用场景中能发挥各自的优势，提供了高性能低功耗、灵活定制和成本效益的处理器的解决方案。据悉，该产品即将面市。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:center"&gt;&lt;strong&gt;基于开源模式联合研发&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:center"&gt;&lt;strong&gt;第三代「香山」（昆明湖）&lt;/strong&gt;&lt;strong&gt;已&lt;/strong&gt;&lt;strong&gt;实现产品级交付&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 2022 年 8 月，开芯院牵头联合计算所、腾讯、阿里、中兴通讯、中科创达、奕斯伟、算能等形成了联合研发团队，在全球首次采用基于开源的处理器核联合研发模式，共同研制第三代「香山」（昆明湖）开源高性能 RISC-V 处理器核，性能对标 ARM N2。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 2024 年 4 月，「香山」（昆明湖）正式发布，SPECCPU2006 分值达到 15 分/GHz，符合 RVA23 标准，性能进入全球 RISC-V 处理器第一梯队。同时，「香山」开源芯片项目在全球最大的开源项目托管平台 GitHub 上获得超过 6500 个星标（Star），形成超过 780 个分支（Fork），远超其他开源硬件项目，成为国际开源社区性能最强、最活跃的 RISC-V 处理器核。「香山」及其敏捷开发基础设施入选「计算机体系结构领域年度全球十二大亮点成果」，连续两年入选「2024 中关村论坛 10 项重大科技成果」和 2025 中关村论坛「北京重大开源成果」。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 在发布后的一年多时间中，开芯院联合企业共同完成了针对「香山」（昆明湖）的产品级验证工作，包括按规模量产芯片企业要求构建了一套严格的测试验证流程，形成了「单元级测试 UT集成级测试 IT系统测试 ST原型系统测试 Prototype」四个层次的验证规范，开发了超过 2 万个测试用例，建立了一套包含数十个商业工具、开源工具、形式化工具等验证工具箱等等。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 实践表明，基于开源的联合开发模式，平摊了验证成本，提升了验证效率——用户/企业贡献了近 1600 个测试用例，发现的 1470 项 BUG 中企业累计提交了 492 项。通过开芯院与多家企业的共同努力， 「香山」（昆明湖）最终实现了验证覆盖率近 100%，同时大幅降低了企业获得性能对标 ARM N2 的产品级 RISC-V IP 核的成本。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 目前，「香山」（昆明湖）已实现首批量产客户的产品级交付。进迭时空正基于「香山」（昆明湖）自研 X200 核，并研发其第三代旗舰 RISC-V AI CPU 芯片，预计 2026 年底进入量产。同时，进迭时空研发的首款 RISC-V 服务器芯片将于近期流片，其中内置 6 个「香山」（昆明湖）核。在双方团队的努力下，进迭时空服务器芯片已在 FPGA 平台上稳定地运行 Linux 操作系统及虚拟机。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364198</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364198</guid>
      <pubDate>Sun, 03 Aug 2025 01:06:00 GMT</pubDate>
      <author>作者: 开源科技</author>
    </item>
    <item>
      <title>中国开源 AI 社区 7 月高亮时刻回顾</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;Hugging Face&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2FAdinaYakup%2Fstatus%2F1951020254269939964" target="_blank"&gt;发布&lt;/a&gt;了中国 AI 社区 7 月高亮时刻，回溯这一个月来令人眼花缭乱的开源浪潮。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1504" src="https://static.oschina.net/uploads/space/2025/0804/184858_39sJ_2720166.png" width="962" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;包括：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;✨ 另一个「DeepSeek 时刻」——Kimi K2&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;✨ Qwen 完全矩阵化- Instruct / Thinking / Coder 模型跨越 30B - 480B 参数规模&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;✨ 多模态浪潮：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;GLM-4.1V-Thinking: Image+Text &amp;gt; Text&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Intern-S1: Image+Text &amp;gt; Text&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Wan 2.2 - Text +Image &amp;gt; video&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Skywork-R1V3: Image+Text &amp;gt; Text&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Skywork-UniPic: Text &amp;gt; Image / Image &amp;gt; Text&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Tar-7B: Any-to-Any&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Ming-Lite-Omni-1.5: Any-to-Any&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Step3: Image+Text &amp;gt; Text&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;HunyuanWorld-1: Image &amp;gt; 3D&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ThinkSound: Video &amp;gt; Audio&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Neta-Lumina: Text &amp;gt; Image&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;✨ 轻量级、可部署的模型&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;SmallThinker runs on 1GB RAM&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;✨ Agentic 编程成为主流&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Qwen3-Coder: fully spec'd tool calling&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;GLM-4.5: browser agents, IDE assistant&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Qwen3 WebDev demo: text-to-frontend code&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;✨特定领域和实用的模型/工具/数据集&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Science one S1: Scientific model&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Agentar DeepFinance: Finance dataset&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ObjectClear: Interactive Vision Tool&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Qwen3 MT Demo: Machine Translation Tool&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;其中回顾的 7 月 31 个亮眼开源模型、1 个框架、1 个数据集，来自 16 家企业、高校或研究机构：&lt;/p&gt; 
&lt;p&gt;阿里（9 个）、月之暗面（2 个）、智谱（2 个）、阶跃星辰（1 个）、字节跳动（2 个）、昆仑万维（2 个）、智源研究院（1 个）、中国电信人工智能研究院（1 个）、蚂蚁集团（4 个）、快手（1 个）、捏 Ta（1 个）、磐石（3 个）、上海交通大学（1 个）、腾讯（1 个）、上海人工智能实验室（1 个）、复旦大学（1 个）。&lt;/p&gt; 
&lt;p&gt;1、阿里（9 个）：编程模型 Qwen3-Coder-30B-A3B-Instruct、Qwen3-Coder-480B-A35B-Instruct，深度思考模型 Qwen3-30B-A3B-Thinking-2507、Qwen3-235B-A22B-Thinking-2507，基础模型 Qwen3-235B-A22B-Instruct-2507、Qwen3-30B-A3B-Instruct-2507，CoT 音频模型 ThinkSound，统一视频生成模型 Wan2.2-TI2V-5B，文生视频 Wan2.2-T2V-A14B。&lt;br&gt; 2、月之暗面（2 个）：MoE 基础模型 Kimi-K2-Base，与 Numina 团队联合研发的数学定理证明模型 Kimina-Prover-72B。&lt;br&gt; 3、智谱（2 个）：多模态大模型 GLM-4.1V-9B-Thinking，基础模型 GLM-4.5。&lt;br&gt; 4、阶跃星辰（1 个）：基础模型 Step3。&lt;br&gt; 5、字节跳动（2 个）：智能体模型 Tar-7B，多语言翻译模型 Seed-X-Instruct-7B。&lt;br&gt; 6、昆仑万维（2 个）：多模态推理大模型 Skywork-R1V3-38B，多模态统一模型 Skywork-UniPic-1.5B。&lt;br&gt; 7、智源研究院（1 个）：文生配音视频框架 MTVCraft。&lt;br&gt; 8、中国电信人工智能研究院（1 个）：AI-Flow-Ruyi-7B-Preview0704。&lt;br&gt; 9、蚂蚁集团（4 个）：多模态推理模型 M2-Reasoning，多模态大模型 Ming-Lite-Omni-1.5，金融训练数据集 Agentar-DeepFinance-100K，交互式深度推理模型 KAG-Thinker-en-ch-7b-instruct。&lt;br&gt; 10、快手（1 个）：自适应思考模型 KAT-V1-40B。&lt;br&gt; 11、捏 Ta（1 个）：动漫风格图像生成模型 Neta-Lumina。&lt;br&gt; 12、磐石（3 个）：科学基础大模型 S1-Base-671B、S1-Base-8B、S1-Base-32B。&lt;br&gt; 13、上海交通大学（1 个）：端侧原生大模型 SmallThinker-4BA0.6B-Instruct。&lt;br&gt; 14、腾讯（1 个）：3D 世界生成模型 HunyuanWorld-1。&lt;br&gt; 15、上海人工智能实验室（1 个）：科学多模态大模型 Intern-S1。&lt;br&gt; 16、复旦大学（1 个）：语音生成模型 MOSS-TTSD-v0.5。&lt;/p&gt; 
&lt;p&gt;更多内容查看：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fcollections%2Fzh-ai-community%2Fjuly-2025-open-works-from-the-chinese-community-686586f1a8840797e477ae5a" target="_blank"&gt;https://huggingface.co/collections/zh-ai-community/july-2025-open-works-from-the-chinese-community-686586f1a8840797e477ae5a&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364138</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364138</guid>
      <pubDate>Sat, 02 Aug 2025 10:50:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>应用多点开花，AI 大模型从「炫技」走向「实干」</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;&lt;span style="color:#000000"&gt;京东宣布旗下言犀大模型品牌全新升级为 JoyAI，并推出行业首个 100% 开源的企业级智能体 JoyAgent；由钉钉 AI 平台训练的垂类妇科大模型通过主任医师考试；网易灵动发布行业首个工程机械具身智能模型「灵掘」……近期，国产大模型频频「上新」，并不断刷新应用「进度条」。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;而在近日举行的 2025 世界人工智能大会（WAIC）期间，AI 大模型也是格外引人关注。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在物流领域，仓内无人机、无人车等智能物流设施惊艳亮相；工业场景中，AR 眼镜可以辅助产业工人精准质检并推荐维修方案；零售体验台前，系统可以自动个性化推荐商品、瞬间生成海量商品广告素材……位于上海世博展览馆一号馆的京东展区内，展示了全新升级的 JoyAI 大模型深度应用的诸多场景。与此同时，京东云还正式开源 JoyAgent 智能体。作为行业首个 100% 开源的企业级智能体，JoyAgent 依托多智能体协同引擎实现高效协作，并融合大小模型优势，打通 AI 落地「最后一公里」。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;今年以来，国内大模型迭代速度提升，加快赋能千行百业。其中，不少企业着力打造垂类大模型，推动大模型快速走向「实干」。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;日前，壹生检康（杭州）生命科技有限公司研发的「豆蔻妇科大模型」成功通过国家妇产科衞生高级职称（正高）笔试考试。钉钉 CTO 朱鸿介绍，豆蔻妇科大模型是钉钉 AI 平台上诞生的第一个专业垂类大模型，双方团队只经过短短一个多月的协作，就将模型准确率提升到了 90.2%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;「通过正高职称考试，意味着该模型已具备主任级医师的专业判断力。」壹生检康创始人王强宇表示，大模型的核心价值在于，为女性用户提供居家自诊断支持，实现「术前分流」与「院外健康管理」；针对无需就诊的情况提供科普指导与生活建议；为医疗、医美等行业机构提供专业支撑，同时可通过机构的数据训练专科模型，提升医疗效率。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;「这一突破为 AI 在妇产科临床决策辅助、循证医学研究、患者健康教育、医学生学习考试等场景的深度应用开辟了新路径。」浙江大学医学院附属妇产科医院妇科周博士表示。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;据悉，随着技术的不断完善和推广，豆蔻妇科大模型不仅有望在更多医疗场景中发挥重要作用，还将进一步优化医疗资源配置。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在工业领域，垂类模型也有不少突破性进展。WAIC 期间，网易旗下工程机械智能化品牌网易灵动推出全球首个专为露天矿山挖掘机装车场景打造的具身智能模型——「灵掘」。在网易灵动展位，观众通过智能座舱可以实时体验内蒙古矿山的无人挖掘机自动装车功能。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在内蒙古霍林河北露天煤矿的严苛环境中，「灵掘」单机装车效率已达人工的 80%，近 70% 的作业时间无需人为干预，成功适配极寒、高粉尘等严苛环境与多型号矿卡。「这项技术让 AI 成为矿山的‘铁臂战友’，装车 3 精度和连续性远超预期，为行业安全与效率提升开辟了新路径。」内蒙古某露天煤矿代表在实测后评价道。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;据了解，网易灵动将首次开源「灵掘」数据集，并向全行业发起「2027 产业协同计划」。该计划将联合徐工、三一、山河智能等主机厂及各露天煤矿企业，通过技术共享平台推动联合研发、场景共创与标准制定。作为「灵掘」技术基石的端到端训练框架——「机械智心」，已支撑「灵掘」在矿山场景的成功实践，并快速向港口清舱、混凝土拌合站、地销煤等十余个场景迁移，未来将延伸至农业、智能制造等领域。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;数据显示，中国目前已发布 1509 个大模型，在全球已发布的 3755 个大模型中数量位居首位。业内指出，AI 大模型正从「炫技」走向「实干」，2025 年成为大模型应用全面落地的关键转折点。这场由技术驱动、场景牵引的深度应用革命，正在重塑千行百业的生产力图谱。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364136</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364136</guid>
      <pubDate>Sat, 02 Aug 2025 10:39:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Taro on HarmonyOS 技术架构深度解析</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                                        &lt;blockquote&gt; 
 &lt;p&gt;2025 年 6 月，在华为开发者大会 2025 开发者场景技术共建分论坛，本文作者进行了《京东 Taro 框架鸿蒙版本正式开源，助力鸿蒙版三方应用开发》专题演讲。期间阐述了 Taro on HarmonyOS 的技术实现方案、核心优化策略，以及开源版本的主要特性。&lt;/p&gt; 
 &lt;p&gt;本文将详细介绍 Taro on HarmonyOS 的技术架构、性能优化实践和开源进展，分享我们在跨端开发中遇到的问题和解决思路。&lt;/p&gt; 
 &lt;p&gt;期待更多人可以参与开源共建，一起交流讨论！&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr&gt; 
&lt;span id="OSC_h2_1"&gt;&lt;/span&gt; 
&lt;h2&gt;背景&lt;/h2&gt; 
&lt;p&gt;回顾 Taro 的发展历程，从 2018 年 6 月开源至今，作为开放式的跨端跨框架解决方案在众多热心开源贡献者的支持下，从初出茅庐逐步迈向成熟。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="taro_background" src="https://oscimg.oschina.net/oscnet//e99ecb394150ea01d8d0df9310c36e30.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;从最初仅支持面向编译时的小程序端解决方案，到如今拥有支持多种前端框架和 UI 库的强大能力；从单一的构建工具，到通过开放生态为开发者提供 &lt;code&gt;Webpack&lt;/code&gt;、&lt;code&gt;Vite&lt;/code&gt;、&lt;code&gt;ESBuild&lt;/code&gt; 等丰富的工具选择，让团队能够定制专属的研发流程；从专注小程序开发，到覆盖各大小程序平台以及 Web、iOS、Android、HarmonyOS 等移动端场景——Taro 的每一步成长都离不开社区的力量。&lt;/p&gt; 
&lt;p&gt;这些年来，我们在 GitHub 上收获了 &lt;strong&gt;36,000+ star&lt;/strong&gt; 和&lt;strong&gt;近 5,000 fork&lt;/strong&gt;，更重要的是得到了众多企业团队和个人开发者贡献的宝贵功能特性。在此，我们要向所有支持 Taro 发展的朋友们表示衷心的感谢！&lt;/p&gt; 
&lt;span id="OSC_h3_2"&gt;&lt;/span&gt; 
&lt;h3&gt;技术架构演进&lt;/h3&gt; 
&lt;p&gt;说到 &lt;code&gt;HarmonyOS&lt;/code&gt;，Taro 从 2022 年开始布局鸿蒙适配，走过了一条持续演进的技术路径。最初我们推出了 &lt;code&gt;JSUI&lt;/code&gt; 版本的端平台插件，为鸿蒙支持打下基础；2023 年开源了 &lt;code&gt;ETS&lt;/code&gt; 版本的端平台插件，大幅提升了开发体验和业务性能；而在最近释出的 4.1 版本中，&lt;code&gt;C-API&lt;/code&gt; 版本的 Harmony 端平台插件也正式发布了，这标志着 Taro 鸿蒙支持能力的重要突破。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="taro_on_harmonyos" src="https://oscimg.oschina.net/oscnet//b251d41c504ba0e92bd0ce7994398d6f.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;目前，我们仍在持续优化 Harmony C-API 插件的性能表现。团队正在推进多线程以及更多版本特性的内部验证，期待在验证完成后能够将其开源，为开发者在鸿蒙端带来更优秀的研发体验。&lt;/p&gt; 
&lt;span id="OSC_h3_3"&gt;&lt;/span&gt; 
&lt;h3&gt;面向多端研发&lt;/h3&gt; 
&lt;p&gt;面向多端的复杂场景，从来都不是一件容易的事情。在传统的多端开发中，开发者往往需要面对各端语法标准不统一、组件和 API 接口各异、开发环境复杂多样等诸多挑战。当业务逻辑需要调整时，开发者必须在多个平台上重复实现相同功能，代码复用率极低，维护工作量成倍增长。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="taro_cross_platform" src="https://oscimg.oschina.net/oscnet//35b595f2278f17a13dd02df5716f8266.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;如图所示，Taro 现已成功在 HarmonyOS 平台上实现了与 Web 端、小程序及其他平台一致的 UI 呈现效果。&lt;/p&gt; 
&lt;p&gt;基于 Taro 跨端研发标准推进业务实现，开发者只需编写一套代码，就能够在多个平台上获得统一的用户体验，最大限度地节省多端业务的研发成本，让团队能够将更多精力投入到业务创新上。&lt;/p&gt; 
&lt;span id="OSC_h3_4"&gt;&lt;/span&gt; 
&lt;h3&gt;京东鸿蒙版&lt;/h3&gt; 
&lt;p&gt;&lt;img alt="taro_harmony_jd" src="https://oscimg.oschina.net/oscnet//473a9f8879d8cddefdbac83f93802a3a.gif" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;以京东鸿蒙版本为例，基于 Taro on HarmonyOS 解决方案，成功在研发效率与应用性能之间达成了理想平衡，其性能表现和稳定性均位居行业前列。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="taro_jd_detail" src="https://oscimg.oschina.net/oscnet//e92b9ee175f09086507c7a5526832104.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;对于商品详情页等高复杂度、高数据量的核心业务场景，该方案展现出强大的技术适配能力。仅是在单线程 C-API 架构的支持下，这些重载业务场景的运行性能已达到与原生应用相当的水准，充分验证了跨端技术在复杂场景下的可行性。&lt;/p&gt; 
&lt;span id="OSC_h2_5"&gt;&lt;/span&gt; 
&lt;h2&gt;技术架构&lt;/h2&gt; 
&lt;p&gt;为了达成这一目标，我们需要在技术架构层面进行深度优化。&lt;/p&gt; 
&lt;p&gt;Taro 在各平台的适配逻辑保持高度一致性。开发者通过统一的 &lt;code&gt;DSL&lt;/code&gt;以及标准化的组件和 API 库即可完成全部代码开发，样式规范完全遵循 &lt;code&gt;W3C&lt;/code&gt; 标准，使前端开发者能够以极低的学习成本快速上手。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="taro_2025" src="https://oscimg.oschina.net/oscnet//ee72a5129bf8027a11211bcd29c13fa8.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在编译层面，Taro 通过 CLI 工具和插件系统实现各端的差异化处理。各个端平台插件可以在编译核心中选择基于 &lt;code&gt;Webpack&lt;/code&gt;、&lt;code&gt;Vite&lt;/code&gt; 或 &lt;code&gt;Metro&lt;/code&gt; 为基础的编译流程，将开发者的源代码高效转换为各目标平台的可执行代码。&lt;/p&gt; 
&lt;p&gt;在运行时中，通过集成语法适配器、&lt;code&gt;DOM&lt;/code&gt;、&lt;code&gt;BOM&lt;/code&gt; 模拟实现以及其他核心模块，确保开发者项目能够在 HarmonyOS 等各类平台上稳定运行，真正实现一码多端的开发愿景。&lt;/p&gt; 
&lt;span id="OSC_h3_6"&gt;&lt;/span&gt; 
&lt;h3&gt;渲染层适配&lt;/h3&gt; 
&lt;p&gt;尽管 Taro 在 HarmonyOS 平台的插件架构历经多轮重大版本升级，但其核心架构设计依旧可从以下几个维度来理解：&lt;/p&gt; 
&lt;p&gt;&lt;img alt="taro_harmonyos_rendering" src="https://oscimg.oschina.net/oscnet//73d229191f933afdedd47c60cc69d5cc.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;代码转换流程&lt;/strong&gt;：从开发者编写的 &lt;code&gt;React&lt;/code&gt; 代码出发，通过与 &lt;code&gt;React Reconciler&lt;/code&gt; 的深度集成，系统构建出完整的虚拟节点树。随后，运行时环境通过模拟的 &lt;code&gt;DOM&lt;/code&gt; 和 &lt;code&gt;BOM&lt;/code&gt; API，实现 &lt;code&gt;React&lt;/code&gt; 节点树与 Taro 内部节点树的精确映射。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;平台适配实现&lt;/strong&gt;：结合标准化的组件库和 API 库，系统将抽象的节点结构转换为 HarmonyOS 平台的原生原子组件，最终构建出完整的 &lt;code&gt;ArkUI&lt;/code&gt; 渲染树，并呈现在用户界面上。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;扩展能力支持&lt;/strong&gt;：除了核心渲染流程外，运行时还集成了布局计算、事件处理、动画效果等关键模块，并持续接入更多 HarmonyOS 平台特有能力，为开发者提供完整的跨平台开发体验。&lt;/p&gt; 
&lt;span id="OSC_h3_7"&gt;&lt;/span&gt; 
&lt;h3&gt;架构方案迭代&lt;/h3&gt; 
&lt;p&gt;在技术架构层面，&lt;code&gt;ETS&lt;/code&gt; 方案与 &lt;code&gt;C-API&lt;/code&gt; 方案本质上都遵循着相同的设计理念。两者均构建了一套完整的三层节点树体系：应用层的 &lt;code&gt;React&lt;/code&gt; 节点树首先转换为中间层的 Taro 节点树，随后进一步映射到底层的 &lt;code&gt;ArkUI&lt;/code&gt; 节点树，最终实现界面的完整渲染。&lt;/p&gt; 
&lt;p&gt;然而，尽管在宏观架构上两种方案展现出高度的相似性，我们仍然坚定地推进从 &lt;code&gt;ETS&lt;/code&gt; 向 &lt;code&gt;C-API&lt;/code&gt; 的技术转型。这一决策的背后，是团队对性能极致追求的不懈努力。在移动应用开发的激烈竞争中，每一毫秒的性能提升都可能成为用户体验的关键差异点，而 C-API 方案正是在这样的背景下应运而生的技术选择。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="taro_harmonyos_architecture" src="https://oscimg.oschina.net/oscnet//d56195a048781803635354a4f73e5b0d.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;C-API&lt;/code&gt; 方案带来的性能提升是全方位的。在节点操作层面，我们彻底摒弃了传统的声明式递归构建模式，转而采用更加灵活的实现方式，这为底层节点 API 的深度优化创造了前所未有的空间。同时，通过引入指令式节点操作机制，不同节点树之间的数据交互效率得到了显著改善，原本复杂的跨树通信变得更加高效流畅。&lt;/p&gt; 
&lt;p&gt;更为重要的是，我们将样式处理、布局计算、事件管理等核心功能模块全面下沉至 &lt;code&gt;C++&lt;/code&gt; 原生层。这一架构调整不仅大幅减少了跨语言调用的频次和开销，更从根本上提升了系统的执行效率。通过这些多维度的优化措施，整个应用的性能表现实现了质的飞跃。&lt;/p&gt; 
&lt;span id="OSC_h3_8"&gt;&lt;/span&gt; 
&lt;h3&gt;跨端研发标准&lt;/h3&gt; 
&lt;p&gt;在适配鸿蒙和其他各端能力的基础上，Taro 正在构建一套完整的跨端研发标准体系。这套标准不仅能够最大限度地节约不同端之间的适配成本，更重要的是能够充分兼容现有的前端生态系统，让团队多年积累的组件库、工具链和技术沉淀得以无缝复用。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="taro_cross_standard" src="https://oscimg.oschina.net/oscnet//2e9017e41ce210f8f74ed721a42b25bf.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;目前，以 &lt;code&gt;React&lt;/code&gt; 作为 UI 基础库，该标准已涵盖了 &lt;code&gt;View&lt;/code&gt;、&lt;code&gt;Text&lt;/code&gt; 等 26 个常用组件和网络请求、图片等 88 个常用 API。在样式规范方面，我们遵循 W3C 标准实现了包含 93 条常用规范的样式子集。与此同时，我们正在持续努力扩充这套标准体系，不断增加新的组件类型、API 接口和样式规范，以满足日益复杂的业务场景需求。&lt;/p&gt; 
&lt;p&gt;更为关键的是，这套不断完善的标准体系具备良好的扩展性和兼容性，能够与团队现有的 UI 组件库、业务组件以及各类前端工具库形成有机整合。我们致力于通过标准的持续演进，确保开发团队能够在跨端开发中充分发挥既有技术资产的价值，避免重复建设带来的资源浪费，同时为未来更多端侧适配需求预留充足的扩展空间。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="taro_harmony_style" src="https://oscimg.oschina.net/oscnet//ba286db72609455a07cb982564d36146.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;为实现跨平台开发的一致性标准，我们设计了 &lt;code&gt;C++&lt;/code&gt; 底层样式处理架构。该架构整合了包括 &lt;code&gt;Yoga&lt;/code&gt; 这类成熟布局引擎，构建统一的布局计算体系，保障各端样式渲染的视觉一致性。通过将样式计算逻辑完全迁移至 &lt;code&gt;C-API&lt;/code&gt; 底层，系统获得了显著的性能优化潜力——不仅消除了对主渲染线程和业务逻辑的性能干扰，还通过 &lt;code&gt;C++&lt;/code&gt; 的高效执行特性实现了跨端样式处理的统一化管理，从根本上提升了整体渲染效率。&lt;/p&gt; 
&lt;p&gt;针对鸿蒙端的特殊需求，我们在编译阶段引入了创新的预处理机制。通过在编译流程中的 &lt;code&gt;Rust&lt;/code&gt; 插件集成 &lt;code&gt;lightingCSS&lt;/code&gt;，我们能够将标准样式预先转换为鸿蒙平台可以识别的样式，进一步节省运行时运算的负担。这一机制不仅实现了 W3C 标准属性到各端专用单位和属性值的智能转换，更为跨端样式的统一管理奠定了坚实的底层基础。&lt;/p&gt; 
&lt;p&gt;基于这套完善的 &lt;code&gt;C++&lt;/code&gt; 样式处理体系，UI 库和业务团队能够轻松应对各种复杂场景的适配需求。无论是折叠屏的多形态展示、关怀模式的无障碍优化，还是暗黑模式的主题切换，都可以通过灵活的样式选择器机制实现精准控制。同时，动画效果和过渡转场也能够通过高效的样式更新和节点刷新机制，呈现出极为流畅的用户体验。&lt;/p&gt; 
&lt;span id="OSC_h2_9"&gt;&lt;/span&gt; 
&lt;h2&gt;方案特性&lt;/h2&gt; 
&lt;p&gt;基于此架构，Taro on HarmonyOS 方案积累了丰富的核心特性。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;研发效能层面&lt;/strong&gt;：通过兼容 React 生态体系和 W3C 样式规范，开发者能够充分利用前端成熟的工具链和生态资源，高效完成业务功能迭代与开发调试工作，完善鸿蒙端的开发体验。同时，开发者编写的样式代码可在鸿蒙、小程序和 Web 端无缝复用，实现真正的"一次编写，多端运行"。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;生态扩展层面&lt;/strong&gt;：提供灵活的组件和 API 扩展机制，支持业务团队根据实际需求定制运行时环境。更重要的是，通过跨端统一的原生混合调用方案，Taro C++ 模块与 ArkTS 原生模块可实现双向互调，为团队间协作提供了更多可能性，有效避免重复开发，提升整体研发效率。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="taro_harmony_tech" src="https://oscimg.oschina.net/oscnet//2f6fbb8105c9d720737b1c639d96f421.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h3_10"&gt;&lt;/span&gt; 
&lt;h3&gt;性能体验&lt;/h3&gt; 
&lt;p&gt;在 C-API 方案中，我们围绕卓越性能体验实现了多项核心特性：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;运行时性能优化&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;将 DOM Tree、事件处理、样式计算等高频操作模块完全下沉至 C++ 层，显著提升运行时执行效率。通过底层优化，减少了 JavaScript 与原生层之间的频繁通信开销，避免了数据序列化/反序列化的性能损耗。同时，C++ 层的内存管理更加高效，能够更好地控制对象生命周期，减少内存碎片，为复杂应用场景提供更稳定的性能表现。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;高阶组件能力&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;img alt="taro_harmony_list" src="https://oscimg.oschina.net/oscnet//3edfae841c90d8ab8bb6b83a5fd4701b.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; &lt;p&gt;基于 HarmonyOS 原生的 List、WaterFlow 等组件特性，深度定制实现虚拟列表、瀑布流等高性能组件，充分发挥系统级优势。&lt;/p&gt; &lt;p&gt;这些高阶组件不仅继承了系统组件的原生性能，还针对前端开发习惯进行了接口封装，支持动态数据加载、智能缓存策略、滚动性能优化等特性。开发者可以像使用传统前端组件一样轻松实现大数据量的列表展示，无需关心底层的复杂优化逻辑。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;图片处理模块&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;构建专门服务于样式渲染、背景绘制、Image 组件的图片处理系统，实现更优秀的图片加载性能和内存管理。该模块集成了多级缓存机制，支持内存缓存、磁盘缓存和网络缓存的智能调度，大幅减少重复加载时间。&lt;/p&gt; &lt;p&gt;&lt;img alt="jd_image" src="https://oscimg.oschina.net/oscnet//e1c0bf07e61ef7b88ac2337b49f4d197.gif" referrerpolicy="no-referrer"&gt;&lt;/p&gt; &lt;p&gt;同时提供了图片压缩、格式转换、尺寸适配等功能，能够根据设备性能和网络状况自动选择最优的图片处理策略，有效降低内存占用和网络带宽消耗。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;文字与绘图支持&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;通过 PixelMap 技术为文字组件提供丰富的字体属性和渲染能力，同时为 Canvas 组件及相关 API 提供底层支持，覆盖分享海报生成等复杂业务绘制场景。文字渲染支持多种字体格式、文字效果（阴影、描边、渐变等）和排版布局，满足不同设计需求。&lt;/p&gt; &lt;p&gt;&lt;img alt="taro_harmony_shared" src="https://oscimg.oschina.net/oscnet//443eb38b5216494391fcaf0ddc35e649.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; &lt;p&gt;Canvas 绘图能力则支持路径绘制、图形变换、滤镜效果等高级功能，为数据可视化、游戏开发、创意设计等场景提供强大的图形处理能力。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;视频播放能力&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;基于 AVPlayer 重构 Video 组件和相关 API 实现，在 C-API 层直接接入，减少调用链路，为业务提供更灵活的视频适配方案。新的视频播放架构支持多种视频格式和编码标准，提供了精确的播放控制、实时进度反馈、音视频同步等核心功能。&lt;/p&gt; &lt;p&gt;&lt;img alt="taro_harmony_video" src="https://oscimg.oschina.net/oscnet//9f5a8a4b269af1f5b7354aaf25169c9e.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;span id="OSC_h2_11"&gt;&lt;/span&gt; 
&lt;h2&gt;总结展望&lt;/h2&gt; 
&lt;p&gt;Taro 在 HarmonyOS 平台的深度适配，旨在为全场景应用开发开辟新的技术路径。通过构建完善的鸿蒙端能力体系，我们致力于为更广泛的业务场景提供技术支撑，推动跨平台开发在鸿蒙生态中的创新应用。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="taro_harmony_jd_all" src="https://oscimg.oschina.net/oscnet//61ac5ecfa77950e92954c28304b10de7.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在实际应用中，Taro 成功支撑了京东鸿蒙 APP 的商业化落地。该应用的首页、搜索推荐以及核心购物流程等关键业务模块均基于 Taro 技术栈开发，在确保快速迭代交付的同时，实现了业界领先的性能表现和系统稳定性。应用上线后迅速在华为应用市场购物类别中登顶，充分验证了技术方案的商业价值。&lt;/p&gt; 
&lt;span id="OSC_h3_12"&gt;&lt;/span&gt; 
&lt;h3&gt;生态建设与合作拓展&lt;/h3&gt; 
&lt;p&gt;基于成功实践的示范效应，更多京东生态应用正在加速鸿蒙化进程，包括一号会员店、七鲜等重要业务线的鸿蒙版本已上架鸿蒙应用市场或者进入开发阶段。同时，我们的技术方案也获得了外部合作伙伴的认可，58 同城、朴朴超市等知名企业均选择采用 Taro 相关的鸿蒙开发解决方案，共同构建更加繁荣的鸿蒙应用生态。&lt;/p&gt; 
&lt;span id="OSC_h3_13"&gt;&lt;/span&gt; 
&lt;h3&gt;未来展望&lt;/h3&gt; 
&lt;p&gt;我们将持续深化开源战略，在内部版本充分验证后，逐步向社区开放多线程等更多核心技术特性。同时不断扩展跨端标准覆盖范围，让更多组件和 API 实现跨平台一致性，为开发者提供更优质的开发体验和更完善的调试工具链，也为动态化能力构建更坚实的技术基础。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="taro_harmony_jd_devtools" src="https://oscimg.oschina.net/oscnet//2aaf8134cf5e8255e8d878eacb2805a8.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在性能优化方面，我们将持续推进更多核心模块向 C++ 层迁移，包括 React 的 C++ 版本实现和高频 API 运行时模块优化，同时积极借鉴节点树扁平化等社区验证的优秀实践。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="taro_harmony_jd_c_react" src="https://oscimg.oschina.net/oscnet//21bfd7a53be846ef5a0b4bc0437f6180.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;虽然 Taro on HarmonyOS 的 C-API 版本插件开源时间不长，但已经吸引了众多开发者的积极参与。我们期待更多技术同仁能够加入这个充满活力的开源生态，共同推动 Taro on HarmonyOS 方案的不断完善，在开源共建的道路上续写跨端开发的新篇章。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/4090830/blog/18686949</link>
      <guid isPermaLink="false">https://my.oschina.net/u/4090830/blog/18686949</guid>
      <pubDate>Sat, 02 Aug 2025 10:20:00 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>我国连续 12 年保持全球最大工业机器人市场</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;&lt;span style="color:#000000"&gt;2024 年，我国工业机器人市场销量达 30.2 万套，连续 12 年保持全球最大工业机器人市场。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;中国电子学会理事长徐晓兰介绍，自 2015 年首届世界机器人大会在北京召开以来，我国机器人产业实现一系列科技创新突破。2024 年，我国机器人专利申请量占全球机器人专利申请总量的 2/3。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;产业发展方面，我国是全球第一大机器人生产国，工业机器人产量由 2015 年的 3.3 万套增长至 2024 年的 55.6 万套，服务机器人产量为 1051.9 万套，同比增长 34.3%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;北京、上海分别成立国家地方共建具身智能机器人创新中心、国家地方共建人形机器人创新中心，浙江、安徽、湖北、广东、四川等地均成立省级机器人创新中心，集聚区域产业优势力量，推动技术共享与联合攻关。机器人整机企业充分发挥引领作用，带动产业链上下游零部件企业配套发展，形成大中小协同、上下游联动的良好生态。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;应用场景方面，工业机器人已应用于国民经济 71 个行业大类、236 个行业中类，制造业机器人密度已跃升至全球第三位。服务机器人在家用服务、仓储物流、商用服务、养老助残、医疗康复等领域的渗透率显著提升。国际数据公司数据显示，2024 年，中国厂商在全球商用服务机器人市场中占据主导地位，出货量占比高达 84.7%，规模优势明显。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;「人形机器人是人工智能与机器人深度融合的产物，是机器人的高阶形态和具身智能的良好载体。」徐晓兰表示，人形机器人有望在家政服务、生产制造、仓储物流、边防海防、教育医疗等场景发挥作用，拉动新消费、催生新产业、扩大新就业，推动新质生产力加快发展。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;2025 世界机器人大会将于 8 月 8 日至 12 日在北京经济技术开发区北人亦创国际会展中心举办。大会期间，200 余家国内外优秀机器人企业的 1500 余件展品将亮相，企业数量较去年增长 25%。其中，首发新品 100 余款，数量是去年的近 2 倍。（人民日报）&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364133</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364133</guid>
      <pubDate>Sat, 02 Aug 2025 10:09:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>苹果组建新 AI 团队「AKI」，打造类似 ChatGPT 的 AI 搜索工具</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;彭博社记者&amp;nbsp;Mark Gurman &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.bloomberg.com%2Fnews%2Fnewsletters%2F2025-08-03%2Fapple-s-chatgpt-rival-from-new-answers-team-iphone-17-spotted-in-the-wild-mdvmqs6g" target="_blank"&gt;报道称&lt;/a&gt;，苹果正开发一款类似 ChatGPT、能够直接回答用户广泛问题的搜索引擎。&lt;/p&gt; 
&lt;p&gt;该项目由一个新成立的&lt;strong&gt;「Answers, Knowledge, and Information，简称 AKI」&lt;/strong&gt;（答案、知识与信息）内部团队负责。领导该新项目的是高级总监 Robby Walker，他曾负责 Siri 的研发工作。虽然该项目仍处于早期阶段，但该团队正在构建所谓的 「答案引擎」—— 一个能够抓取网页以回应通用知识问题的系统。目前正在探索开发一款独立应用，同时也在搭建新的后端基础设施，旨在为未来版本的 Siri、Spotlight 和 Safari 提供搜索功能。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0804/175856_2Ke0_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;苹果最近已在其招聘网站上为该团队发布了职位空缺，他们正在为该团队招募具有搜索算法和引擎开发经验的工程师。招聘信息称：「我们的工作为苹果一些最具标志性的产品（包括 Siri、Spotlight、Safari、Messages、Lookup 等）提供直观的信息体验。加入我们，共同塑造全球与信息连接方式的未来！」&lt;/p&gt; 
&lt;p&gt;此举标志着苹果在 AI 上的重大转变，因为此前苹果高管曾多次表示，无意开发自有聊天机器人，而是选择集成第三方服务。&lt;/p&gt; 
&lt;p&gt;目前，Siri 在处理复杂问题时表现不佳，苹果与谷歌之间价值约 200 亿美元的默认搜索引擎协议正面临美国司法部的严格审查，苹果可能因此感到有必要开发自主引擎。不过与此同时，苹果正面临 AI 人才流失问题，负责开发大语言模型的团队在一个月内已有四名关键研究员离职，转投竞争对手 Meta。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364130</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364130</guid>
      <pubDate>Sat, 02 Aug 2025 10:01:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>AI 编程工具 Roo Code 支持通过对话历史提供更智能的建议</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;AI 编程助手 Roo Code 发布了&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.roocode.com%2Fupdate-notes%2Fv3.25.4" target="_blank"&gt;v3.25.4&lt;/a&gt;更新，支持基于最近 10 条消息作为上下文来增强其代码建议，从而提供更智能、更少幻觉的响应。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0804/174428_JrSy_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;用户可以完全控制 API 路由和历史记录的开关，以平衡上下文和隐私需求。&lt;/p&gt; 
&lt;hr&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;Roo Code 是一个 AI 驱动的开源自主编码 Agent，它存在于您的编辑器中。&lt;/p&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0618/193229_CZTb_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;功能&lt;/p&gt; 
&lt;ul style="list-style-type:disc; margin-left:0; margin-right:0"&gt; 
 &lt;li&gt;用自然语言沟通&lt;/li&gt; 
 &lt;li&gt;直接在您的工作区读写文件&lt;/li&gt; 
 &lt;li&gt;运行终端命令&lt;/li&gt; 
 &lt;li&gt;自动化浏览器操作&lt;/li&gt; 
 &lt;li&gt;与任何 OpenAI 兼容或自定义的 API / 模型集成&lt;/li&gt; 
 &lt;li&gt;通过&lt;strong&gt;自定义模式&lt;/strong&gt;调整其 "个性" 和能力&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364125</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364125</guid>
      <pubDate>Sat, 02 Aug 2025 09:46:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>辩证看待 KubeSphere 闭源删库，前核心团队成员的解读</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;ul&gt; 
 &lt;li&gt;&lt;em&gt;文章来源：微信公众号 Cloud Native Fun&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;作者：周鹏飞&lt;/em&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong style="color:#000000"&gt;2025 年 8 月 1 日，青云科技在 KubeSphere 社区发布消息，宣布暂停 KubeSphere 的开源版本支持（&lt;/strong&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fkubesphere%2Fkubesphere%2Fissues%2F6550" target="_blank"&gt;&lt;u&gt;https://github.com/kubesphere/kubesphere/issues/6550&lt;/u&gt;&lt;/a&gt;）。这一消息如同投下一颗重磅炸弹，引发了全球开源社区的强烈反响。&lt;/span&gt;&lt;/p&gt; 
 &lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;作为前青云科技的高级社区经理、KubeSphere 项目的前核心维护者，我曾参与这个项目从零到一的过程。在这篇文章中，我站在一个长期开源从业者的角度，尝试用辩证的视角去还原事件背后更深层的逻辑，并回答一些几个普遍关注的问题。&lt;/span&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;KubeSphere 自 2018 年 4 月份开始在 GItHub 写下第一行代码并开源，至今七年多的时间，曾被全球数以万计的大小企业所使用，与众多知名开源项目集成，被全球各大云厂商认可和合作，不可否认的事实是，KubeSphere 是一个非常优秀的开源项目和云原生产品，项目创始人 Ray 也是一个很有开源情怀的人。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;我曾在 2018-2022 年担任青云科技的高级社区经理，在全球不遗余力地推广 KubeSphere，&lt;/span&gt;&lt;span&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzA4MDcyOTc2Nw%3D%3D%26mid%3D2247485443%26idx%3D1%26sn%3D0d66089771b9ea44006666eeb4471af8%26scene%3D21%23wechat_redirect" target="_blank"&gt;从零到一构建了活跃多元化的开源社区&lt;/a&gt;&lt;/span&gt;&lt;span style="color:#000000"&gt;，创作了官网、用户文档、博客、视频教程，在全国各大城市办过几十场活动，并将 KubeSphere 孵化的 3 个子项目捐给 CNCF 基金会。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="324" src="https://oscimg.oschina.net/oscnet/up-a83df931bfba4277ce01f9eb46797ef1d7d.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;2020-2022 年几乎是 KubeSphere 在全球飞速发展的三年，我记得曾经几乎每天都能看到新的用户增长和使用反馈，以及来自全球不同公司的贡献者参与。说实话，那曾是我最有工作成就感和成长飞快的时光，在开源社区认识了很多优秀的开发者，也给公司带来过一些商业机会。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="330" src="https://oscimg.oschina.net/oscnet/up-49bfe588b4f1c2be1102d667fadac349a14.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;h2 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong style="color:#773098"&gt;一次不留余地的急转弯&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;令人遗憾的是，这次青云的决定来了个 180 度急转弯，不仅将前端代码闭源，还删除所有已发布的文档和安装镜像，彻底将 KubeSphere 推向了深渊，在全球开源社区引发了巨大的舆论和开源信任危机。巧合的是，消息发布当天正是项目创始人 Ray 宣布离职的时间点。我不知道这是否是有意为之，但属实是不讲「情面」的一个选择，也引发了国内外很多开发者对「中国式开源」的发问和深层焦虑。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="319" src="https://oscimg.oschina.net/oscnet/up-1e8382254228e4b54d873e3631e7c2654cd.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;h2 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong style="color:#773098"&gt;墙倒众人推，并不能让中国开源更好&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;事实上，我刚开始看到消息时也有些情绪化，因为 「KubeSphere 闭源删库」后，互联网舆论呈现了墙倒众人推的趋势，很多博主的文章以及吃瓜群众的评论，站在制高点一味地批判和指责商业公司「闭源」的行为，诋毁该开源项目的价值，甚至还有很多人直接全盘否定 「中国开源」 的努力和价值。这些带有偏见的观点，很容易误导大众的认知和情绪，并且不会让这个行业变得更好。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;我也看到了很多曾经的用户、贡献者、前同事们表示唏嘘和惋惜，感叹自己曾经参与过的开源明星项目的陨落。我没有选择在第一时间发布这篇文章，在阅读了国内外很多博主的文章、讨论和用户在 GitHub 上的回复后，带着辩证的角度来分享自己的一些观点。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;截止目前，我看到的可能有价值的一个讨论是，有一些用户在 GitHub 上希望通过投票和众筹的方式重新组建纯社区自发驱动的 KubeSphere 开源贡献者团队，Issue 下也有众多用户和贡献者表示支持，这让我感受到了些许欣慰，因为它展现了开源社区开发者的集体力量，但这个目标要实现的难度不亚于去经营一家公司。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="229" src="https://oscimg.oschina.net/oscnet/up-bab13eb637d429b475fd818bbd06bc869b5.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;h2 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong style="color:#773098"&gt;维护开源是商业公司应有的社会责任吗？&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;这是一个值得每位开源参与者深思的问题。开源项目由商业公司主导，并不意味着它对社区有「义务」无限维护。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;显然 「KubeSphere 闭源删库」 是一个唐突而又糟糕的决策，但我想说，&lt;strong style="color:#000000"&gt;选择「闭源」或核心团队撤离开源项目这件事情本身从来不是某一家商业公司的过错，&lt;/strong&gt;任何一家商业公司主导的开源项目都存在这样的「闭源」风险，这在开源界也已有多个案例。客观来说，在当前的经济下行的市场环境下，几乎所有商业公司都在收缩投资或调整战略，对于营收增长慢的项目和人力都有被优化的可能。&lt;strong style="color:#000000"&gt;维护开源项目并不是一家商业公司应有的社会责任或义务。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;但是，开源项目背后的主导公司如果希望撤资开源投入，&lt;strong style="color:#000000"&gt;需要遵循开源项目客观存在的生命周期，&lt;/strong&gt;而不是把开源社区一刀切，用开源断供来「绑架」用户，转到自家商业产品，这并不是一项精明的生意决策。实际上，&lt;strong&gt;如果青云选择将 KubeSphere 项目归档 （Archive）并选择提前一段时间在社区发布项目 Retire 的公告，同时致谢所有参与贡献的开发者和用户，那就不会出现今天的危机局面&lt;/strong&gt;。&lt;/span&gt;&lt;/p&gt; 
&lt;h2 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong style="color:#773098"&gt;失去开发者=失去企业服务信任&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;「得开发者得天下」 是一个很通俗的道理，很多大小企业选择走开源的策略，也正是因为开源是一个能够低成本地快速地获取全球开发者用户的机会和建立跨企业的社区合作模式，比传统的闭源软件开发和 Marketing 传播更快，通过开源社区协作开发也更容易建立规模与行业标准。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;但很多商业公司错误地把开源作为低成本「获客」的渠道，&lt;strong style="color:#000000"&gt;把开源作为一项「免费广告」的福利，利用开发者对开源技术的关注来获取销售商机，这样的做法是对开源社区最大的伤害，也是对商业公司口碑的破坏&lt;/strong&gt;。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;实际上，很多开源项目的开发者用户是一些企业的运维开发负责人或内部决策影响者，他们虽然可能不能直接决定公司的软件采购，但他们提供的观点和意见会直接或间接地影响企业管理层的决策。很可惜，很多开源商业化公司的 CXO 们并不懂开发者的重要性，忽视了开源社区的规模影响力。&lt;/span&gt;&lt;/p&gt; 
&lt;h2 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong style="color:#773098"&gt;青云做些什么能补救？&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;答案是能。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;青云作为一家上市公司，公司层面一定还是会关注自己在业内的口碑以及客户的信任。这里我不适合作为局外人指点江山，但提供思路作为参考，&lt;strong style="color:#000000"&gt;毕竟在社区里还有很多用户在关注 KubeSphere 后续是否会有好的转机。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;让我们来看一个正面的例子，CNCF 毕业项目 Flux 背后的核心维护公司 WeaveWorks 在去年 2 月份虽然宣布了公司关闭停止运营，但 CEO 第一时间在联系一些大公司的用户和贡献者参与 Flux 项目的维护，并积极寻求 CNCF 的帮助，确保该项目在即使没有了 WeaveWorks 公司的支持，还能继续在社区维护和迭代。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="504" src="https://oscimg.oschina.net/oscnet/up-5c4d3b4efba258bd1ad215ef4c196d40c65.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;青云是否会考虑做一个 「git revert」 的回滚操作，来响应社区目前呼声最高的提议&lt;/strong&gt;：恢复闭源的仓库、下线的文档、历史镜像，并将开源项目交由给中立的社区自发去维护？青云作为商业公司继续去做自己的商业产品，或许还有重建信任的机会。如果公司层面担心开源项目抢了自己的商业产品的蛋糕，那必然是商业产品与服务做得还不够好。&lt;/span&gt;&lt;/p&gt; 
&lt;h2 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong style="color:#773098"&gt;对 KubeSphere 开源协议的疑问？&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;KubeSphere 有两个子项目的开源协议值得探讨，第一个是此次被闭源的前端 Console 项目开源协议是 AGPL-3.0，这个协议除了要求二次分发必须开源没有其它问题，一些知名项目如 Grafana 也采用该协议。还记得 KubeSphere 曾多次被国内和海外的一些大公司换 Logo 后改个名字后就拿去卖钱，AGPL-3.0 的协议某种程度上也有一定的品牌保护作用。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;但值得注意的是，后端开源是在 Apache 2.0 基础上加入了「附加条款」的开源协议，例如：&lt;/span&gt;&lt;/p&gt; 
&lt;ul style="margin-left:0; margin-right:0"&gt; 
 &lt;li&gt; &lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;禁止用于商业化 SaaS 服务&lt;/strong&gt;；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;禁止移除或修改 KubeSphere 品牌和 logo&lt;/strong&gt;；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;要求贡献者接受维护者可以在未来&lt;strong&gt;改变授权方式&lt;/strong&gt;的条款；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;对 fork 或商用做出限制。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;虽然它的目的是出于保护青云的商业产品利益和 KubeSphere 品牌，但这个修改后的协议是不符合 OSI（Open Source Initiative）定义的开源标准的，破坏了 Apache 2.0 的开放性和自由性。所以青云回复 KubeSphere 将继续保持核心代码开源的声明，是需要推敲的。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;开源协议的选择，以及&lt;strong style="color:#000000"&gt;所选开源协议的开放程度和商业友好程度，会从本质上影响和决定这个项目最终是否会有众多企业级贡献者来参与&lt;/strong&gt;。&lt;/span&gt;&lt;/p&gt; 
&lt;h2 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong style="color:#773098"&gt;关于「K8s 上游贡献」的误解&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;圈子里有开源从业者提出了一个问题：&lt;strong style="color:#000000"&gt;KubeSphere 作为 K8s 发行版，秉着 「Upstream First」 的原则， KubeSphere 项目维护者在上游 K8s 社区的贡献有多少？&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;实际上，研究这个问题对于分析这个事件的的意义并不大，因为 KubeSphere 不算严格意义上 K8s 发行版，因为它没有改 K8s 一行代码，没有对 K8s 进行二次分发，它是一个构建于 Kubernetes 之上的平台型项目，用户可以使用自己已有的 K8s 对接 KubeSphere，所以从产品定义层面这个说法不是非常准确；&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;其次，KubeSphere 团队对 K8s 项目的贡献多少，并不会直接影响到 KubeSphere 开源项目本身的可持续性发展，上游贡献的指标仅对下游企业在 K8s 上游社区的话语权和影响力能产生影响，或对企业内部基于 K8s 做了扩展或二次修改的厂商，或是直接提供托管 K8s 云服务的云厂商，能体现技术实力。&lt;/span&gt;&lt;/p&gt; 
&lt;h2 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong style="color:#773098"&gt;可持续的开源项目，有哪些共同特征？&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;em&gt;「一个人可以走得很快，但一群人可以走得更远。」&lt;/em&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;很多用户评估开源项目是否值得在企业内部特别是生产环境采用，习惯先去看这个项目的 GitHub Star 数量来评估这个项目的流行度，从而判断该项目的可持续性。实际上，这样的方式太过于片面和业余。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;我如果作为用户，我认为最可靠的方式是去关注这些指标：&lt;/span&gt;&lt;/p&gt; 
&lt;ul style="list-style-type:disc; margin-left:0; margin-right:0"&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;有多少家公司参与了开源项目的贡献与维护？贡献比例分别是多少？（单一厂商控制的开源项目属于高风险）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;开源社区治理和开发流程是否公开和透明？&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;有多少大的企业已经采用了该开源项目？（这通常在官网或 README 中能找到）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;该开源项目对商业化是否友好？业内是否已有多家商业公司集成和提供商业支持？&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span style="color:#000000"&gt;…&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;我的观点是，真正强大的开源项目，往往拥有更丰富的「社区股东」和更清晰的合作模型。&lt;strong style="color:#000000"&gt;一个开源项目的贡献者和商业生态越多元化，拥抱它的企业越多，它的可持续性将会越强大。&lt;/strong&gt;如果只有单一厂商在维护，并且只有用户生态，缺乏不同组织的贡献者，并且用户市场还局限在国内，这样的开源项目大概率是走不远的。&lt;/span&gt;&lt;/p&gt; 
&lt;h2 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong style="color:#773098"&gt;我们从该事件能学到什么？&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong style="color:#000000"&gt;对于希望构建开源商业化的公司&lt;/strong&gt;：开源的核心价值并不仅仅是代码和技术，而是围绕这些能力构建起来的社区生态与信任体系。如果把开源仅仅视为一种「获客渠道」，通过免费开放源码吸引用户，再通过商用版本进行转化，却忽略了社区治理、合作机制、开发者关系和中立性建设，最终可能得到短期流量，但失去长期信任。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong style="color:#000000"&gt;对开源项目使用者的启示：&lt;/strong&gt;&amp;nbsp;如何选型一个更有可持续性的项目是非常关键的，上面提到的一些指标可以作为参考。作为用户在有余力的情况下，可以思考自己作为用户如何参与到开源社区中，毕竟 「众人拾柴火焰高」。&lt;/span&gt;&lt;/p&gt; 
&lt;h2 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong style="color:#773098"&gt;写在最后&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt; 
&lt;p style="color:#5a5a5a; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;我在西雅图的周日晚午夜时分写完这篇文章，虽然我已经离开了前司青云三年多时间，但依然对曾经一起参与 KubeSphere 维护的前同事、社区贡献者和用户们合作的时光非常怀念，当时 KubeSphere 项目也确实在四海大地聚集了很有有热情和信仰的开源爱好者。如今的局面，个人还是衷心希望能有一些好的转机！&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364121</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364121</guid>
      <pubDate>Sat, 02 Aug 2025 09:36:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>谷歌变更 goo.gl 短链接服务「停用」计划，会保留活跃链接</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;谷歌去年&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdevelopers.googleblog.com%2Fen%2Fgoogle-url-shortener-links-will-no-longer-be-available%2F" target="_blank"&gt;宣布&lt;/a&gt;，它将于 2025 年 8 月 25 日关闭 Google URL Shortener 短链接服务（goo.gl/*），届时所有 goo.gl 链接将会&lt;a href="https://www.oschina.net/news/362402"&gt;停止响应&lt;/a&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-3de2e977a212c4b76a27a10e76c003b1a75.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;距离关闭日期不到一个月时间，在依赖于 goo.gl 短链接的开发者、教育工作者和记者等表达担忧之后，&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.google%2Ftechnology%2Fdevelopers%2Fgoogl-link-shortening-update%2F" target="_blank"&gt;谷歌改变了主意&lt;/a&gt;，采取了更温和的立场：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0804/173324_wscd_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;它将只禁用自 2024 年底以来没有任何活动的 goo.gl 链接，如果 goo.gl 链接在活跃使用或点击，这些链接将能继续使用&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-c970c162854ec8999ffbf88f6e05480f82b.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364120/google-googl-shutdown-reversal</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364120/google-googl-shutdown-reversal</guid>
      <pubDate>Sat, 02 Aug 2025 09:29:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>揭秘字节跳动内部流量调度与容灾实践</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                                        &lt;div&gt; 
 &lt;div&gt;
   资料来源： 
  &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.volcengine.com%2F" target="_blank"&gt;火山引擎-开发者社区&lt;/a&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; 摘要： 在字节跳动，平衡超大规模流量的稳定性、性能、容量与成本，是一系列产品共同面临的挑战，其中， Trafficroute GTM 起到了不可忽视的作用&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;Trafficroute GTM 承载了字节跳动亿级流量、覆盖了大规模场景，是一款基于 DNS 的流量路由服务，我们将通过两期文章，揭秘字节跳动如何通过 Trafficroute GTM 巧妙应对以上挑战，实现高效流量管理！&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;上期内容中，我们主要介绍了基于 TrafficRoute GTM 的 GEO-基础路由模式进行自定义流量编排，感兴趣的小伙伴可以点击了解：《&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzkyNzY0OTE5Ng%3D%3D%26mid%3D2247487345%26idx%3D1%26sn%3D8240962635c24ae6f065a483be88d8eb%26scene%3D21%23wechat_redirect" target="_blank"&gt;揭秘字节跳动内部流量调度与容灾实践【上】&lt;/a&gt;》。&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;本文为下期，主要介绍基于 TrafficRoute GTM 的 Perf-智能路由模式落地全智能、可观测、可微调的流量调度，主要内容包括：&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;1.TrafficRoute GTM 介绍&lt;br&gt; 2.TrafficRoute GTM 的 Perf-智能路由关键技术&lt;br&gt; 3.字节跳动智能流量调度内部实践&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;面临超大规模流量时，平衡好稳定性、性能、容量、成本，能确保用户在访问服务时获得流畅、快速且可靠的体验，这对于提高用户满意度和粘性至关重要。TrafficRoute GTM 为业务提供基于 DNS 的全球流量负载均衡、智能调度、自动容灾服务，可以帮助业务提升连续性，实现资源优化，获取更多竞争优势。&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;strong&gt;1.火山引擎 Trafficroute GTM 简介&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;火山引擎 Trafficroute GTM 是基于 DNS 的流量路由服务。它依托全球 1100+ 分布式探测节点及 IDC 质量数据等，构建出强大的网络质量感知能力，实现了对「端-边-云」全链路流量的质量感知，从而根据 APP 应用的实时访问质量、节点负载和健康状况作出动态流量调度。&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;此外，Trafficroute GTM 还提供灵活的调度策略，其中 GEO-基础，路由功能丰富，包括负载均衡、会话粘性（内部使用中，暂未对外开放）和故障转移等多种特性。而 Perf-智能路由则在基础路由的基础上，进一步提供性能优先，容量优先和负载反馈等智能调度能力，以满足更高层次的调度需求。&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//27f3f1197212e22ab45afebdef1b5d94.jpg" width="4000" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; 一图看懂 TrafficRoute GTM&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;在字节跳动内部业务中，诸多业务基于 TrafficRoute GTM 的 Perf-智能路由，借助 GTM 的全球网络质量地图、APP 全链路可用性、APP 实时负载等感知能力落地了全智能、可观测、可微调的流量调度。&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;strong&gt;2.Perf-智能路由，实现流量智能调度&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;TrafficRoute GTM 的 Perf-智能路由旨在为边缘计算、IoT 物联网、多云混合等大规模分布式场景提供智能化的流量调度方案。&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;用户无需人工进行流量编排，只需在 GTM 中输入目标节点地址，GTM 即刻呈现最优的流量调度策略；同时，GTM 会根据全球网络质量，目标节点健康状况等动态的更新流量调度规则，真正地实现自动、智能的流量调度。&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//51866a5a42c1752611bb4008749e8321.jpg" width="4000" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; 凭借以下关键技术，Perf-智能路由实现了更智能、更动态的流量调度。&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;2.1 感知中心&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;GTM 感知中心通过分布于全球 1100+ 的节点实时采集：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;全球网络链路质量，反映网络链路的连通性/时延/抖动等&lt;/li&gt; 
 &lt;li&gt;目标资源健康情况，反映业务的资源节点当前健康程度&lt;/li&gt; 
 &lt;li&gt;目标资源实时负载，反映业务的资源节点当前工作负荷&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;这些数据经过预处理、转换、分析后作为策略中心的决策依据。&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//755a66782dbc63a888425e5bdb229491.jpg" width="4000" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; 以感知中心生成的中国大陆的网络质量地图为例：&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//46181b2fb4b81bfa8d7342a946b61e71.jpg" width="1005" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; 该有向图表达了 6 个省份-运营商之间的网络质量，节点代表省份-运营商，边表示节点之间的连通性&amp;amp;时延&amp;amp;抖动。&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;在实际应用中，GTM 的策略中心亦可根据业务需求，对该有向图施加【成本系数、ISP 亲和、GEO 亲和】等约束，这些约束最终会影响到流量调度。&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;2.2 策略中心&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;策略中心根据感知中心上报的事件，利用实例设定的策略算法进行路由计算，进而生成动态的调度拓扑。Perf-智能路由主要有 3 种模式，分别面向对性能、容量、成本、稳定性等有不同诉求的业务场景。&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//3a18a1d64b5cf0c4990422b915b0ddde.jpg" width="3412" referrerpolicy="no-referrer"&gt; 
 &lt;br&gt; 
 &lt;br&gt; 性能优先 | Perf 
&lt;/div&gt; 
&lt;div&gt;
  &amp;nbsp; 
&lt;/div&gt; 
&lt;div&gt;
  适用于量级可控、资源容量充沛、追求极致性能的业务 
&lt;/div&gt; 
&lt;ul&gt; 
 &lt;li&gt;核心原理：据全球网络质量，动态的将各地区的客户端调度至其访问最快的资源节点&lt;/li&gt; 
 &lt;li&gt;核心特色：以数据 （ 网络质量 ） 驱动调度而非经验，将流量调度变得更加智能、实时、精确&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//8829efb01af6d09d6e03a63deb50b9cc.jpg" width="1482" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; &lt;br&gt; 容量优先 | Perf-Cap&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;适用于量级中等，资源分布不均，要求在资源约束下实现最高性能的业务&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;核心原理：根据全球网络质量，在容量限制的前提下，动态地将各地区客户端调度至其访问最快的资源节点&lt;/li&gt; 
 &lt;li&gt;核心特色：在 Perf 性能优先的基础上，引入资源节点容量的约束，能够更加智能的实现容量，性能的平衡&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//fc0b6086dc2a8b0704f5e7b93fbe5d64.jpg" width="936" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; 负载，反馈 | Perf-Feedback&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;适用于量级波动大，资源分布广且不均，追求容量&amp;amp;性能&amp;amp;成本的平衡，尤其适合边缘下沉场景&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;核心原理：根据全局和节点负载，动态的将流量在可用节点中分配，同时兼顾性能最优和容量安全&lt;/li&gt; 
 &lt;li&gt;核心特色：以最合理的资源成本，稳定支撑量级&amp;amp;波动大的业务，实现容量/性能/成本/稳定性的平衡&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//4f0e9bd329249ed0ebe8ec044f0a7a95.jpg" width="4000" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; Perf-Feedback 内置两种调度倾向：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;当全局平均负载较低时，GTM 倾向于性能，将客户端流量调度至其访问最快的资源节点&lt;/li&gt; 
 &lt;li&gt;当全局平均负载较高时，GTM 倾向于稳定，确保每个节点的水位不高于全局平均负载水位&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;如下图所示，相比于 Perf-Cap，GTM 的调度输入中引入了实时负载的数据。&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//fbb51d8fe9e5aeec563326aafa3b54a5.jpg" width="936" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;Perf 自定义路由&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;适用于用户需要对 Perf 智能路由流量进行微调，以满足特定场景的业务&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;核心原理：自定义路由规则的优先级高于 Perf 智能路由生成的路由规则优先级&lt;/li&gt; 
 &lt;li&gt;核心特色：在智能化的同时也为业务方提供更多的灵活性，满足特定业务需求&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//0f2575e23ad2499c9ac0ab47b58bb6d6.jpg" width="2850" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; 2.3 流量可视化&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;Perf-智能调度智慧透明，配备全面工具集，助力业务深入分析流量动态，通过 Perf-智能调度，可以观测到实时流量拓扑、客户端请求趋势、客户端地区分布等流量动态。&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//725ec82c907b637a27e73d5ab3598418.jpg" width="2722" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; 实时流量拓扑&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//9fc1792f281616eeef95de1b8fdb01d2.jpg" width="1864" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; 客户端请求趋势&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//3da45538936e9772728e8057243a3548.jpg" width="1872" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; 客户端地区分布&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;strong&gt;3.字节跳动智能流量调度内部实践&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;在字节跳动，越来越多的业务正通过边缘计算将服务去中心化，从而实现更优的用户体验和更低的基建成本。面对边缘节点分布广泛、数量庞大、能力参差不齐的挑战，TrafficRoute GTM 的 Perf 智能路由展现出天然优势。通过 Perf -智能路由的三种调度模式，帮助字节跳动内部多个业务落地了边缘下沉，在成本、性能和稳定性上取得较大收益。&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;3.1 RTC 实时音频，访问时延降低 10%+&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;字节跳动某款 APP 的 RTC 实时音频服务，在全国三个城市部署了 9 个接入节点。通过采用 TrafficRoute GTM 的 Perf 性能优先模式，确保全国的企业用户在不同工作场所均能体验到极低延迟的音频接入服务，保障了通信的高效与流畅。&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//b46e6cf10f0195428eb729c7d2c6b570.jpg" width="1048" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//717de0c554e9a316da09e95cae8b0522.jpg" width="3752" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; GTM 感知中心实时感知全国网络质量，智能地为不同地区客户端动态制定调度规则，确保用户始终连接到最健康、速度最快的音频接入点，以优化通信体验。整个应用过程中，GTM 的 Perf 性能优先模式充分发挥了独特功能，涵盖了智能动态调度策略、显著降低了接入成本以及显著提升了应用性能，展现出其卓越的技术优势。&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//22ae0e72f459a8d877bbd5c14643deb5.jpg" width="2442" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; 采用 TrafficRoute GTM 的 Perf 性能优先模式，相比较 GEO 基础路由，最终业务实现了如下收益：&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//0e9a9657ca6bffdeb34296935aadd2ec.jpg" width="4000" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;成本收益：智能调度代替了人工维护，每月降低了 3 人天以上；&lt;/li&gt; 
 &lt;li&gt;性能收益：访问时延 avg 降低 10%+，p95 降低 25%；请求成功率 avg 提升 0.05%；&lt;/li&gt; 
 &lt;li&gt;稳定性收益：业务实现了分钟级全链路自动容灾，最快做到 3 分钟全国 95%+ 流量收敛。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; 3.2 千万 QPS 业务，成本降低 35%，性能提升 20%&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;在边缘计算浪潮的推动下，能否有效驾驭大规模边缘算力，成为业务边缘下沉成功的关键。TrafficRoute GTM 深度参与了一个超 1500 万 QPS 的业务边缘下沉项目，通过使用 Perf-Cap 容量优先模式，助力其在字节内部率先落地端-边-云一体化的架构，成为先行者。&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//e7deae9af5c6d09fccc0c1904c31f291.jpg" width="3132" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; 通过将中心 Region 数据面服务下沉至全国 30+ 省份、50+ 边缘节点，来实现提升用户访问体验 (边缘节点距离终端客户端更近) 和降低带宽&amp;amp;算力成本 (边缘资源成本约为中心的 20%~60%)。GTM 的 P erf-Cap 容量优先模式，根据业务的客户端请求分布、全国网络质量地图 ， 在满足各边缘节点容量约束的前提下，生成全局总时延最低的流量调度规则。&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//ad93894a8582bd02fd6e4d9c63309094.jpg" width="2596" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; GTM 上实际配置如下图：&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//433b27fc81232ee1a9ec89b390e0f5ed.jpg" width="1532" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; 此时，用户无需繁琐的容量规划、节点统筹、流量调度，只需在 console 上填入边缘节点的元信息 (IP 地址+容量)，GTM 即刻生成&lt;em&gt;智能&lt;/em&gt;、&lt;em&gt;动态&lt;/em&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;em&gt;的调度&lt;/em&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;*， *&lt;em&gt;时刻保证最终客户的体验最优。&lt;/em&gt;&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;通过抖音客户端 AB 数据分析，该业务边缘下沉带来的整体收益如下：&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//d60771c32ba3fa11f057436c4149292f.jpg" width="2074" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; 其中，边缘下沉 x GTM Perf-Cap 模式，额外取得的收益如下：&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//c2dd755e546b20a5daee185333db42d1.jpg" width="2092" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; 3.3 302 服务，端上播放质量显著提升&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;字节跳动 302 服务承担了抖音、头条、西瓜等 APP 点播&amp;amp;下载的重定向功能，其流量呈现明显的波峰波谷特征，日内 QPS 在 30-350 万范围波动。&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;为实现最优访问性能和最低基建成本，要求 TrafficRoute GTM 将动态波动的流量在最小资源冗余的火山引擎边缘节点上合理调度，既要保证性能全局最优，又要保证全局水位健康。&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//4fcd16dcc51697e3731fb733c9e6a59c.jpg" width="4000" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; 采用 Perf-Feedback 负载反馈模式，302 服务实现了如下收益：&lt;/p&gt; 
&lt;div&gt; 
 &lt;img src="https://oscimg.oschina.net/oscnet//69cfbed876af215db19663285d846b05.jpg" width="4000" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;ul&gt; 
 &lt;li&gt;100+ 边缘节点的负载更加可控，资源利用率更加合理，节点负载跑超率从 20% 降至 0%；&lt;/li&gt; 
 &lt;li&gt;TCP 建联失败率下降明显：晚高峰 19%-&amp;gt;16.5% ，午高峰 18%-&amp;gt; 14%；&lt;/li&gt; 
 &lt;li&gt;客户端 7 层负面指标均下降 ： 其中播放 error 错误率、播放 play_break 中断率降幅超 50%。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;&lt;br&gt; END&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;Trafficroute GTM 通过 Perf -智能路由的三种调度模式，帮助字节跳动内部 RTC 实时音频业务、千万 QPS 业务、302 服务实现了在成本、性能和稳定性上的收益，进一步助力字节跳动内部业务经受超大规模流量考验，确保始终为用户提供稳定服务。&lt;/p&gt; 
&lt;p style="color:#191b1f; margin-left:0; margin-right:0; text-align:start"&gt;最后，给大家预告番外篇，后续我们将聚焦更新的 GTM 调度功能，详细阐述技术思路、关键技术和实践经验，感兴趣的小伙伴记得持续关注~&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/6800876/blog/18684814</link>
      <guid isPermaLink="false">https://my.oschina.net/u/6800876/blog/18684814</guid>
      <pubDate>Sat, 02 Aug 2025 09:24:00 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>B 站上线「AI 原声翻译功能」，将加入日语等语言</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p style="color:#2b2b2b; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;B 站今年 5 月下架国际版 App，与国内版合并为一个统一 App。为解决海内外内容互通问题，B 站现公布一项自研的「AI 原声翻译功能」，号称可以帮助海外用户更好体验游戏、科技、二次元等主推内容。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#2b2b2b; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;据 B 站介绍，&lt;strong&gt;目前相应功能已向海外用户开放，暂仅支持英语&lt;/strong&gt;，主要提供画面和音频两大翻译能力，在画面方面支持自动擦除原中文字幕改为英文、自动翻译弹幕、各类按钮语言。在音频方面号称可以还原 UP 主的声线、音色、气口，而非传统的机器音翻译。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#2b2b2b; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;img height="259" src="https://oscimg.oschina.net/oscnet/up-a4c2bba74fb87025c787b7a83000654e41b.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2b2b2b; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;B 站表示，相应翻译功能的技术难点&lt;strong&gt;在于游戏、二次元等专有名词梗的密集领域「如何实现原风格精准保留与语音时长完美对应」&lt;/strong&gt;。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#2b2b2b; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;为此，相应技术团队基于大语言模型（LLM）构建翻译引擎，采用对抗式强化学习（RL）训练驱动模型；并引入 Deep Research 深度挖掘技术，专攻专有名词与流行梗点的翻译难点，确保最终译文准确传神。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#2b2b2b; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;后续，B 站还将视需求为「AI 原声翻译」功能新增日语等更多语言，持续扩展在海外市场的适配能力。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364111</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364111</guid>
      <pubDate>Sat, 02 Aug 2025 09:13:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>高德发布全球首个地图 AI 原生智能体</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;&lt;span style="color:#000000"&gt;高德地图正式&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F4qsBRg16CnO1ayKe6sHiFA" target="_blank"&gt;宣布&lt;/a&gt;其全面 AI 化，结合前沿的空间智能技术，推出了全球首个 AI 原生地图应用 —— 高德地图 2025。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;公告称，高德地图 2025 旨在打造具备深度时空理解和自主推理决策能力的一体化出行生活智能体，以及 AI 领航、AI 即刻、AI 探索、AR 打卡等创新场景工具，为用户提炼一个更加符合习惯和喜好的个性化数字孪生世界，基于空间智能架构解决一切出行生活需求。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="228" src="https://oscimg.oschina.net/oscnet/up-f16397c20425bb4922c0dbd519e0b8bed5e.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;「空间智能是在三维空间和时间中感知、推理、和行动的能力，能够让地图实现被动感知到主动预判的跨越。」高德地图 CEO 郭宁表示，希望从高德地图 2025 开始，推动 AI 从 「对话工具」蜕变为「行动伙伴」。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;即日起用户升级高德地图 APP 至最新版，搜索「空间智能」，即可体验。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#3e3e3e; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;根据介绍，用户可与高德主智能体「小高老师」的语音交流。该语音技术以自然语言交互为核心，通过全双工语音技术实现流畅交流，支持用户随时打断指令、动态调整规划；内置的回声消除算法与异常语义拒识模型，可智能去噪以精准聆听用户声音；而情感计算模块赋予对话温度，不仅提供清晰的路线指引，更能在旅途中提供情绪陪伴。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#3e3e3e; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;语音感知之后，即进入思考决策环节。基于高德与通义深度共建的大模型簇，小高老师能够进行基于空间智能的推理、计划、反思和行动，并通过 MCP 协同调用出行服务、生活服务、空间服务等子智能体和工具链，整合内外部知识库来制定最优方案。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364108</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364108</guid>
      <pubDate>Sat, 02 Aug 2025 09:01:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>「问小白」发布第四代开源大模型 XBai o4，擅长复杂推理</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;「问小白」发布了第四代开源大模型&lt;strong&gt;XBai o4&lt;/strong&gt;（其中「o」代表「open」），该模型在复杂推理能力方面表现出色，在 Medium 模式下已全面超越&lt;strong&gt;OpenAI-o3-mini&lt;/strong&gt;，并在部分基准测试中优于&lt;strong&gt;Anthropic Claude Opus&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0804/163409_xSmx_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;XBai o4 基于创新的&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FRxWjzZe5WWVGKKOk2JbfJQ" target="_blank"&gt;「反思型生成范式」&lt;strong&gt;（reflective generative form）&lt;/strong&gt;&lt;/a&gt;，融合了 Long-CoT 强化学习&lt;strong&gt;与&lt;/strong&gt;过程评分学习（Process Reward Learning），使单个模型同时具备深度推理和高质量推理链路筛选的能力。通过共享过程评分模型（PRMs）和策略模型的主干网络，XBai o4 显著降低了 99% 的过程评分推理耗时。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-8840d76d6357daa77dda0a67b9e0727ff4c.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;该模型提供三种模式（low、medium、high），在多个基准测试（如 AIME24、AIME25、LiveCodeBench v5、C-EVAL 等）中均展现出强大性能，相关训练和评估代码已在 GitHub 开源。&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FMetaStone-AI%2FXBai-o4" target="_blank"&gt;https://github.com/MetaStone-AI/XBai-o4&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364101</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364101</guid>
      <pubDate>Sat, 02 Aug 2025 08:35:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
  </channel>
</rss>
