<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>开源中国-综合资讯</title>
        <link>https://www.oschina.net/news/industry</link>
        <atom:link href="http://8.134.148.166:30044/oschina/news/industry" rel="self" type="application/rss+xml"></atom:link>
        <description>开源中国-综合资讯 - Powered by RSSHub</description>
        <generator>RSSHub</generator>
        <webMaster>contact@rsshub.app (RSSHub)</webMaster>
        <language>en</language>
        <lastBuildDate>Wed, 12 Mar 2025 02:36:31 GMT</lastBuildDate>
        <ttl>5</ttl>
        <item>
            <title>TypeScript 编译器和工具链将移植到 Go：性能提升 10 倍</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;TypeScript、C#语言、Delphi 语言之父 Anders Hejlsberg 今日在 Microsoft 开发者博客&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdevblogs.microsoft.com%2Ftypescript%2Ftypescript-native-port%2F&quot; target=&quot;_blank&quot;&gt;宣布重大消息&lt;/a&gt;，TypeScript 编译器以及工具链将移植到 Go 语言，性能提升高达 10 倍！&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0312/102953_R1VQ_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;这一举动旨在解决 TypeScript 在大型代码库中性能瓶颈的问题，为开发者带来更流畅、更高效的开发体验。&lt;/p&gt; 
&lt;p&gt;根据官方公布的数据，新的原生实现将带来以下惊人的改进：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;编辑器启动的项目加载速度提升 8 倍！&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;大多数构建时间缩短 10 倍！&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;内存使用量大幅减少！&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Anders Hejlsberg 和 TypeScript 团队在 GitHub 仓库的讨论区解释了为何采用 Go，主要原因有以下几点：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;代码结构相似性：TypeScript 现有代码库采用函数式编程风格，很少使用类。而 Go 语言也以函数和数据结构为中心，与现有代码结构高度相似，这使得移植工作更加容易。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;内存管理：Go 语言提供自动垃圾回收（GC），无需开发者手动管理内存，这大大简化了移植过程，降低了代码复杂度。同时，Go 的 GC 对 TypeScript 编译器这类批处理任务影响很小。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;内存布局控制：Go 语言允许对内存布局和分配进行精细控制，这对于优化性能至关重要。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;图处理能力：TypeScript 编译器涉及大量的树遍历和多态节点处理，Go 语言在这方面表现出色。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Anders Hejlsberg 强调，&lt;strong&gt;这是一次「移植」而非「重写」&lt;/strong&gt;，目标是尽可能保留现有代码库的结构和语义，确保兼容性。Go 语言的特性与 TypeScript 现有代码库的契合度最高，是「阻力最小」的路径。&lt;/p&gt; 
&lt;p&gt;详情查看：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Ftypescript-go%2Fdiscussions%2F411&quot; target=&quot;_blank&quot;&gt;https://github.com/microsoft/typescript-go/discussions/411&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338304/typescript-native-port</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338304/typescript-native-port</guid>
            <pubDate>Wed, 12 Mar 2025 02:34:27 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>两大存储龙头 NAND 下月或将提价</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p style=&quot;color:#222222; margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;继美光、闪迪等接连官宣涨价决定后，又有新的 NAND 涨价预期逐一涌现。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#222222; margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;据美光、闪迪此前宣布，两者均自 4 月 1 日起对渠道及消费类产品实施全面涨价，整体涨幅超 10%，并计划后续季度进一步调价。而如今这一行业复苏信号或被进一步放大，据今日 TrendForce 援引消息人士称，&lt;strong&gt;三星和 SK 海力士等韩国内存巨头同样有望在下个月提高 NAND 报价&lt;/strong&gt;。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#222222; margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;与此同时，NAND Flash 控制芯片厂群联也于近日估计，2025 年第一季度将是 NAND 报价全年低点，随着 NAND 原厂涨价以及 DeepSeek 带动 AI 硬件等效应显现，NAND 供需有望在下半年趋于紧张，全年营运表现将优于去年。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#222222; margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;群联 CEO 潘健成在此前的法说会中表示，受备货需求提升和大厂减产影响，NAND Flash 涨价已是进行式，&lt;strong&gt;预估第三季度价格仍看涨&lt;/strong&gt;。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#222222; margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;此次存储涨价潮的原因，一方面或许在于减产效应。公开资料显示，三星、SK 海力士、美光、西部数据和铠侠五大原厂均从 2024 年第四季度就开始减产 NAND，而今年第二季度来势汹涌的涨价潮则被视作此前减产效果的体现。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#222222; margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;日前《科创板日报》记者从一名国内存储企业高管处得知，几家存储芯片原厂已发布 15%-25% 的减产计划，按这个计划施行，供应量减少，相应价格就会上涨。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#222222; margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;TrendForce 调查指出，自 2023 年起各家 NAND Flash 原厂已深刻体认到供给过剩对产业造成的严重冲击，特别是 NAND Flash 需求年增率自 30% 大幅下修至 10-15%，唯有供应商积极调整生产策略，方能避免价格下行周期持续扩大。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#222222; margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;因此，2025 年初，各 NAND Flash 原厂均采取更为坚决的减产措施，缩减全年投产规模，期待有效降低供应位增长率。TrendForce 认为，随着减产加上价格于第一季逐步触底，预期 NAND Flash 产业下半年可望重回上升轨道。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#222222; margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;除上所述外，华安证券将涨价原因归结为 NAND Flash 需求旺盛。该机构表示，随着 AI 算力建设浪潮涌现，服务器存储需求旺盛。另一方面，手机厂库存去化，加上下半年新机发布，预计手机存储需求将逐渐恢复到常态，而 AI 眼镜带来存储新增量。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#222222; margin-left:0; margin-right:0; text-align:justify&quot;&gt;&lt;span&gt;天风证券 3 月 10 日研报指出，存储涨价趋势已明确。从存储下游来看，字节、阿里加大数据中心建设投入，端侧 AI 百花齐放带动各类存储需求高速增长。当前大语言模型（LLM）参数规模的迅速增长已经超越了传统存储系统所能轻松处理的范围。这些模型的参数量由数十亿转向数千亿，甚至上万亿，导致对存储系统的带宽和响应时间方面的要求大幅提升。（科创板日报，张真）&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338302</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338302</guid>
            <pubDate>Wed, 12 Mar 2025 02:32:27 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>Manus 中文版与通义千问达成战略合作</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Manus &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmanus.monica.cn%2Fnews%2Fmanus-cn&quot; target=&quot;_blank&quot;&gt;宣布&lt;/a&gt;，为满足中文用户需求，与阿里通义千问团队正式达成战略合作。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;双方将基于通义千问系列开源模型，在国产模型和算力平台上实现 Manus 的全部功能。目前两家技术团队已展开紧密协作，致力于为用户打造更具创造力的通用智能体产品。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;「我们期待通过此次合作，尽快将 Manus 的创新体验带给广大中文用户，敬请期待。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;img height=&quot;267&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-a622562d4ce7b313198b4d76914506a4aef.png&quot; width=&quot;700&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;对此，阿里通义回应称：Manus 和通义千问确实在进行开源模型方面的合作。我们期待与更多全球 AI 创新者开展合作。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Manus 创始人兼 CEO 肖弘近期接受媒体采访时称，Manus 母公司蝴蝶效应共完成两轮融资，总规模超过 1000 万美元，投资人包括真格基金、红杉中国、腾讯和美团联合创始人王慧文。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338296</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338296</guid>
            <pubDate>Wed, 12 Mar 2025 02:16:27 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>硅基流动：DeepSeek-R1&amp;V3 API 支持批量推理 R1 价格直降 75%</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;硅基流动昨日晚间宣布，即刻起，硅基流动 SiliconCloud 平台的 DeepSeek-R1&amp;amp;V3API 支持批量推理（BatchInference）。用户通过批量 API 发送请求到 SiliconCloud，不受实时推理速率限制的影响，预期可在 24 小时内完成任务。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;相比实时推理，DeepSeek-V3 批量推理价格直降 50%，其中，3 月 11 日至 3 月 18 日，DeepSeek-R1 批量推理优惠价格直降 75%，输入价格为 1 元/百万 Tokens、输出价格为 4 元/百万 Tokens。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;批量推理可帮助用户更高效处理生成报告、数据清洗等大批量数据处理任务，享受更低成本的 DeepSeek-R1 &amp;amp; V3 API 服务，适用于无需实时响应的数据分析、模型性能评估等场景。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;251&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-bb8c779853fd6654ea525539cf3488720a3.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338293</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338293</guid>
            <pubDate>Wed, 12 Mar 2025 02:04:27 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>DevEco Studio 联合小艺接入 DeepSeek，步骤更简单开发鸿蒙更专业</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:left&quot;&gt;随着小艺接入了 DeepSeek，智能体的问答变得更加丝滑流畅，让人不禁想到鸿蒙原生应用开发如果接入这个智能体会产生什么样的效果？确实，当我们把负责开发原生鸿蒙应用的 DevEco Studio 联合小艺接入 DeepSeek 后，这 4 个亮点迫不及待要和大家分享：&lt;/p&gt; 
&lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:center&quot;&gt;&lt;img alt=&quot;111.jpg&quot; height=&quot;179&quot; src=&quot;https://oscimg.oschina.net/oscnet//0a2c5184bd5cec66f8a5e358d0d64d96.jpg&quot; width=&quot;600&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:left&quot;&gt;只需轻松两步，就可以在 DevEco Studio 中使用 DeepSeek！&lt;/p&gt; 
&lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;strong&gt;第一步：环境准备&lt;/strong&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;strong&gt;&lt;span&gt;1、下载并安装 DevEco Studio&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:left&quot;&gt;如果尚未安装 DevEco Studio，请访问华为开发者官网下载并安装最新版本 DevEco Studio 5.0.3 Beta2，本次指导就以 5.0.3 Beta2 进行演示。根据指导安装完成后，打开 DevEco Studio，创建一个新的鸿蒙应用项目，选择适合的模板（如 Empty Ability）。&lt;/p&gt; 
&lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:center&quot;&gt;&lt;img alt=&quot;222.jpg&quot; height=&quot;395&quot; src=&quot;https://oscimg.oschina.net/oscnet//a5fb544214738530955257b8b3249982.jpg&quot; width=&quot;600&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;strong&gt;&lt;span&gt;2、下载并安装 CodeGenie 工具&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:left&quot;&gt;如果你安装的是官网最新的 DevEco Studio 5.0.3 Beta2 可以直接跳过这一步，该版本已经内置了最新版本的 CodeGenie。&lt;/p&gt; 
&lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:left&quot;&gt;CodeGenie 是华为开发者官网推出的官方 DevEco Studio AI 辅助编程工具，提供智能知识问答、代码生成、元服务卡片生成的能力，新增支持对接小艺 DeepSeek，可以帮助开发者高效开发鸿蒙应用及元服务。&lt;/p&gt; 
&lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:left&quot;&gt;在 DevEco Studio 下载页向下翻，就可以看到 CodeGenie 的下载链接，点击链接进行下载：&lt;/p&gt; 
&lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:center&quot;&gt;&lt;img alt=&quot;333.jpg&quot; height=&quot;207&quot; src=&quot;https://oscimg.oschina.net/oscnet//610c7328b3615d26d8f89c6247683ffe.jpg&quot; width=&quot;600&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span&gt;CodeGenie 下载完成后，在右上方打开 DevEco Studio 中的插件市场，我们可以看到 DevEco Studio 已经为我们内置了一个 CodeGenie，但是该版本不是最新的，所以我们手动再安装这一步下载的最新的 CodeGenie。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span&gt;点击右上方的设置按钮，选择其中的 Install plugin from Disk...&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:center&quot;&gt;&lt;img alt=&quot;444.jpg&quot; height=&quot;268&quot; src=&quot;https://oscimg.oschina.net/oscnet//fc25f14715c5290ac56666a09c606253.jpg&quot; width=&quot;600&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:center&quot;&gt;&lt;img alt=&quot;555.jpg&quot; height=&quot;398&quot; src=&quot;https://oscimg.oschina.net/oscnet//712e023ff09b4a0a4393dfdc4d855ea9.jpg&quot; width=&quot;600&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:left&quot;&gt;选择这一步下载的 CodeGenie 路径，点击 OK：&lt;/p&gt; 
&lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:center&quot;&gt;&lt;img alt=&quot;666.jpg&quot; height=&quot;411&quot; src=&quot;https://oscimg.oschina.net/oscnet//f04615f19acf657db03d1c12dbe7e68d.jpg&quot; width=&quot;600&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:left&quot;&gt;在列表顶部就会出现最新的 CodeGenie 工具，然后我们选择 Restart IDE 重启 DevEco Studio：&lt;/p&gt; 
&lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:center&quot;&gt;&lt;img alt=&quot;777.jpg&quot; height=&quot;392&quot; src=&quot;https://oscimg.oschina.net/oscnet//b28dd8f3476776fc62ab340a446b36fa.jpg&quot; width=&quot;600&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;strong&gt;&lt;span&gt;第二步：登录华为账号&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;strong&gt;&lt;span&gt;1&lt;/span&gt;&lt;span&gt;、登录华为账号&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:left&quot;&gt;点击 DevEco Studio 右侧栏中的 CodeGenie，可以看到弹出框中显示的登录按钮，点击登录按钮进行华为开发者账号的登录。&lt;/p&gt; 
&lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:center&quot;&gt;&lt;img alt=&quot;888.jpg&quot; height=&quot;406&quot; src=&quot;https://oscimg.oschina.net/oscnet//2141f8fbad4aff2afcc474092fcb3ac9.jpg&quot; width=&quot;600&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;strong&gt;&lt;span&gt;2&lt;/span&gt;&lt;span&gt;、即刻开启智能原生鸿蒙开发之旅&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:left&quot;&gt;登录后再次打开 CodeGenie，在底部 Agents 中选择 DeepSeek-R1(Beta)，就可以直接开始使用 DeepSeek 的能力辅助开发原生鸿蒙应用了！&lt;/p&gt; 
&lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:center&quot;&gt;&lt;img alt=&quot;999.jpg&quot; height=&quot;392&quot; src=&quot;https://oscimg.oschina.net/oscnet//e3a11024f6c427d1a868b7d7bcb3d5df.jpg&quot; width=&quot;600&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:left&quot;&gt;我们来结合鸿蒙原生应用开发+应用流行趋势+鸿蒙特性来做一些复杂的示例，一起来看看他的表现吧！&lt;/p&gt; 
&lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:left&quot;&gt;1） 我先给他一个网站，让他帮我分析出这个网站中包含的流行的设计元素，并在应用首页中结合这些流行设计元素做一个个人 IP 的展示，让他先给出我整体设计说明&lt;/p&gt; 
&lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:center&quot;&gt;&lt;img alt=&quot;100.jpg&quot; height=&quot;404&quot; src=&quot;https://oscimg.oschina.net/oscnet//cba411bc060e254789dae8feb95878d9.jpg&quot; width=&quot;600&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:left&quot;&gt;刚将诉求发出，马上获得反馈，并且明确给出了对网站的设计元素分析和整体架构设计，甚至给出了性能优化方案和设计验收指标。简直是又快又清晰又准确。&lt;/p&gt; 
&lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:left&quot;&gt;2） 现在让他结合鸿蒙最佳实践，按照他的设计，生成这个首页所有的代码，并且希望快点看到效果&lt;/p&gt; 
&lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:center&quot;&gt;&lt;img alt=&quot;11.jpg&quot; height=&quot;405&quot; src=&quot;https://oscimg.oschina.net/oscnet//74044a3f8aeebd52ba6aac0eaba5c564.jpg&quot; width=&quot;600&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:left&quot;&gt;按照提供的代码，将整个工程进行代码和资源的填充和简单修改，马上一个富有设计元素，拥有点击动效的个人 IP 首页就可以马上运行了，整个过程不到半个小时，是不是很快！&lt;/p&gt; 
&lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:center&quot;&gt;&lt;img alt=&quot;12.jpg&quot; height=&quot;406&quot; src=&quot;https://oscimg.oschina.net/oscnet//12bcc374a63fa3fb6db7913a9795f213.jpg&quot; width=&quot;600&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#555555; margin-left:0; margin-right:0; text-align:left&quot;&gt;无需等待，DeepSeek 的反馈即刻呈现，帮助开发者高效便捷的进行原生鸿蒙应用的开发！大家快去试试吧！&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338286</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338286</guid>
            <pubDate>Thu, 06 Mar 2025 01:23:00 GMT</pubDate>
            <author>来源: 投稿</author>
        </item>
        <item>
            <title>梁文锋拒绝用 DeepSeek 赚快钱，腾讯、阿里近期都曾与其接触</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcn.wsj.com%2Farticles%2Finvestors-want-a-piece-of-deepseek-its-founder-says-not-now-724386a5%3Fmod%3Dcn_hp_lead_pos2&quot; target=&quot;_blank&quot;&gt;据《华尔街日报》报道&lt;/a&gt;&lt;/u&gt;，&lt;strong&gt;DeepSeek 创始人梁文锋已经拒绝了通过其大模型赚快钱的投资提议。他告诉潜在投资者，自己希望保持那种致力于科学项目研究的精神&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;702&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0311/184106_oQoG_2720166.png&quot; width=&quot;1800&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;据知情人士透露，梁文锋告诉身边的人，他并不急于获得投资，因为担心外部投资者会干预 DeepSeek 的决策。最近几周，包括腾讯和阿里巴巴在内的中国科技公司高管曾与梁文锋会面，讨论潜在合作机会。&lt;/p&gt; 
&lt;p&gt;知情人士称，自 2023 年底以来，DeepSeek 曾向多个风险投资基金进行推介，包括一些外国公司，但是这些公司拒绝投资，因为他们看不到明确的资金回报途径。&lt;/p&gt; 
&lt;p&gt;尽管对 DeepSeek 缺乏明确创收计划表示担忧，但最近更多潜在投资者表达了对投资 DeepSeek 的兴趣。然而，梁文锋着眼于公司的长期战略，拒绝了他们的投资提议。DeepSeek 正在研究如何帮助科技巨头利用 AI 开发商业应用并分享其中的收益。&lt;/p&gt; 
&lt;p&gt;目前，梁文锋似乎在坚持他在 2023 年一次罕见采访中表达的理念。他当时说：「我们不做应用，我们只做研究和探索。」记者问他为什么这样做，梁文锋回答说，原因是好奇心驱动。&lt;/p&gt; 
&lt;p&gt;知情人士还透露，梁文锋不想对 DeepSeek 的核心 AI 模型收费。这些模型目前是免费的。该公司计划最早在 4 月发布其下一个推理模型，旨在解决复杂问题。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338220</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338220</guid>
            <pubDate>Wed, 05 Mar 2025 10:41:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>微信小程序「聊天工具」模式开始内测</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;微信团队今日发布公告称，为更好地支持开发者在微信群聊场景内服务用户&lt;strong&gt;，小程序「聊天工具」模式开始内测&lt;/strong&gt;。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;功能介绍&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;聊天工具模式是为了帮助小程序更好与微信聊天结合而推出的模式，可用于实现群问卷、群拼单、群任务等功能。其与小程序普通模式相比开放更多与聊天紧密结合的能力：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;聊天成员相关能力：开发者可调用聊天成员选择器并获取成员相关 id，通过开放数据域渲染聊天成员的头像暱称&lt;/li&gt; 
  &lt;li&gt;发送内容到聊天能力：开发者可发送文本、提醒、图片、表情、视频等内容类型到聊天中&lt;/li&gt; 
  &lt;li&gt;动态消息能力：小程序卡片上的辅标题可以动态更新，在用户完成/参与了活动后下发系统消息&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;同时，聊天工具模式需要使用独立分包基于 skyline 开发，该分包也可被普通模式打开。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;小程序「聊天工具」模式的能力介绍如下：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;聊天成员相关能力&lt;/strong&gt;：开发者可调用聊天成员选择器并获取成员相关 id，通过开放数据域渲染聊天成员的头像暱称&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://img.ithome.com/newsuploadfiles/2025/3/70dbe39d-824d-4504-9929-aca7c109a807.png?x-bce-process=image/format,f_avif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-681fa9ded30fbe32ccdcc7d158863ac4818.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;发送内容到聊天能力&lt;/strong&gt;：开发者可发送文本、提醒、图片、表情、视频等内容类型到聊天中，用户通过内容可进入小程序&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-59fa2aecb7ab97672121ab713e31a28004e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;https://img.ithome.com/newsuploadfiles/2025/3/b38d2ebf-68ab-4a08-ac94-60d09d79681d.png?x-bce-process=image/format,f_avif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;动态消息能力&lt;/strong&gt;：小程序卡片上的辅标题可以动态更新，在用户完成 / 参与了活动后下发系统消息&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-03294ca6c0c39d56a7d7127e664c8534b88.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;详情查看文档：&lt;img alt=&quot;&quot; src=&quot;https://img.ithome.com/newsuploadfiles/2025/3/3e6a2e0e-35a8-48c6-bb62-6be791f89764.png?x-bce-process=image/format,f_avif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdevelopers.weixin.qq.com%2Fminiprogram%2Fdev%2Fframework%2Fopen-ability%2FchatTool.html&quot; target=&quot;_blank&quot;&gt;https://developers.weixin.qq.com/miniprogram/dev/framework/open-ability/chatTool.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338202</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338202</guid>
            <pubDate>Wed, 05 Mar 2025 09:04:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>AI 的「脸」该怎么管？</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;em&gt;人民网记者，赵竹青&lt;/em&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;「靳东两会建议 AI 换脸立法」「雷军回应国庆 7 天被 AI 雷军骂了 8 天」……今年两会期间，AI 话题频频登上热搜，「AI 换脸」引发的风险问题更是受到代表委员的热议。 事实上，苦于被 AI 偷走面孔和声音的，远不止靳东与雷军。前有「假张文宏」深夜直播带货，后有「假刘晓庆」分享人生鸡汤，以及「假古天乐」代言游戏平台，深度伪造 (Deepfake) 技术自 2017 年问世以来，正以指数级速度渗透日常生活。随着技术持续进化、门槛不断降低，「AI 换脸」的不当滥用已成为违法侵权重灾区，并有愈演愈烈之势。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;面对新型侵权形态，现行法律体系正面临现实挑战。全国人大代表、小米集团创始人雷军坦言，现有法律体系仍将 AI 侵权嵌套在隐私权、肖像权、名誉权等传统框架中，而「被骂 8 天」「因 AI 谣言导致股价下跌」等损失，却因无法量化举证而难以维权。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;「这些损害涉及众多不特定个体，仅靠私益诉讼难以实现有效治理。」最高人民检察院公益诉讼检察厅厅长徐向春的观察一针见血。显然，面对新技术引发的新问题，仍沿用传统追责机制，就好比用渔网去拦截数据洪流，是力所不逮的。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;近年来，我国《互联网信息服务深度合成管理规定》《生成式人工智能服务管理暂行办法》等法律法规已颁布实施，《人工智能生成合成内容标识办法 (征求意见稿)》完成社会意见征集，对人工智能治理进行了有益探索。然而，鉴于技术的迅猛发展，治理的「工具箱」仍需不断扩容。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;如何管好 AI 这张「脸」？全国两会上，代表委员们围绕如何更好把握人工智能发展机遇、更好应对新技术带来的风险挑战，积极建言献策。 雷军建议，加快单行法立法进程，提升立法位阶；强化行业自律共治，压实平台企业等各方的责任和义务；加大普法宣传的广度力度，增强民众的警惕性和鉴别力。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;全国人大代表、TCL 创始人李东生建议，加快人工智能深度合成内容标识管理规章制度的出台，明确惩罚制度，同时还需加强国际合作，形成人工智能生成合成内容的有效监管。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;全国政协委员、中华全国律师协会监事长吕红兵认为，应当加快推进「小、快、灵」立法，尽早出台行政法规，规范相关领域发展。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;在全国人大代表、四川省律师协会会长李世亮看来，随着 DeepSeek 在全世界范围内引发轰动，「AI+」快速融入各行各业，AI 立法「条件已经成熟」。他期待 AI 全面系统性法案的出台，填补我国 AI 法律空白。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;记者注意到，8 日提请审议的全国人大常委会工作报告明确，今年将围绕人工智能、数字经济、大数据等新兴领域加强立法研究。这也意味着，我国 AI 治理正逐步从「补丁式规范」向「体系化建构」转型。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;技术狂飙的时代，规则不能只是追赶者。今年全国两会关于 AI 治理的讨论中，一个共识逐渐清晰：唯有构建起「法律划界、技术鉴伪、平台担责、全民共治」的多维防护网，才能让 AI 的「脸」既绽放科技之美，又不失人性之真与善。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338192</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338192</guid>
            <pubDate>Wed, 05 Mar 2025 08:33:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>百度上线 AI 陪伴产品「月匣」App</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;百度近期低调推出情感陪伴类 App「月匣」，主打高自由度 AI 对话与沉浸式剧本互动两大核心功能。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;img height=&quot;544&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-2b1b8422cc0ee7f9d5a53332bf2f668b7d0.jpg&quot; width=&quot;300&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;根据介绍，这款产品通过构建虚拟角色生态，试图在泛娱乐社交领域开辟新赛道。与以往百度推出的 AI 社交产品不同的是，百度的这款全新 AI 社交产品，不仅搭载自研的文心一言大模型，还整合了 DeepSeek、豆包、MiniMax abab 三大外部的大模型，构建起「四核驱动」的 AI 社交引擎。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338176</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338176</guid>
            <pubDate>Wed, 05 Mar 2025 07:39:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>TIOBE 3 月榜单：Fortran 和 Delphi 争夺前十</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;TIOBE 公布了 2025&amp;nbsp;年 3 月的&lt;/span&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.tiobe.com%2Ftiobe-index%2F&quot; target=&quot;_blank&quot;&gt;编程语言排行榜&lt;/a&gt;&lt;span style=&quot;color:#000000&quot;&gt;。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;66&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-538dc8bd513430c8df55d4d61c23929ab13.png&quot; width=&quot;700&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;本月榜单中，一些古老的语言正在悄然上升。其中，Fortran 和 Delphi 正在争夺前十名的位置，COBOL 和本月新加入的 Ada 则稍稍靠后，但也均成功挤入了 Top 20。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;TIOBE CEO&amp;nbsp;Paul Jansen 认为，这一趋势与维持世界运转的许多重要遗留系统有关。&lt;/span&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;它们中的大多数都是借助这些「dinosaur languages」开发出来的。现在，这些系统的最后一批核心开发人员即将退休，公司为了避免任何风险，选择保留现有系统，甚至对其进行扩展，而不是用基于更现代语言的更新系统来取代它们。&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;请注意，我们把这些语言称为「dinosaurs」，但它们已经随着时间的推移而不断发展，而且已经相当先进。它们都有新的语言定义。譬如 Fortran 2023、Delphi 12（2024 年发布）、Ada 2023 和 COBOL 2023。我们可能会对这些语言进入 TIOBE 指数前 20 名感到惊讶，但它们绝对是有作用的，值得称赞。&lt;/span&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;strong style=&quot;color:#333333&quot;&gt;TIOBE 3 月 TOP 20 编程语言&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;400&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-204e07ccd60ab4b3c568070f91028f34963.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;strong style=&quot;color:#333333&quot;&gt;TOP 10 编程语言 TIOBE 指数走势（2002-2024）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;221&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-33bfc79079a90aaf3dd4ea7c65b16937581.png&quot; width=&quot;700&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;strong style=&quot;color:#333333&quot;&gt;第 21-50 名编程语言排行&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;413&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-270d625073c800a18b1102408d7f43cbb57.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;第 51-100 名如下，由于它们之间的数值差异较小，仅以文本形式列出（按字母排序）：&lt;/span&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;ActionScript, Algol, Alice, Apex, APL, CFML, CHILL, Clipper, CLIPS, Clojure, Curl, Eiffel, Elm, Erlang, F#, Forth, Groovy, Hack, Icon, Inform, Io, J, JScript, LabVIEW, Ladder Logic, Logo, Maple, Modula-2, Mojo, MQL5, NATURAL, Nim, OCaml, Occam, OpenCL, OpenEdge ABL, PL/I, Q, Raku, Ring, S, Scheme, Simulink, Smalltalk, SPARK, Tcl, Vala/Genie, VHDL, Wolfram, Xojo&lt;/span&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;TIOBE 编程社区指数（The TIOBE Programming Community index）是一个衡量编程语言受欢迎程度的指标，该指数每月更新一次。评判的依据来自世界范围内的工程师、课程和第三方供应商，包括流行的搜索引擎，如 Google、必应、雅虎、维基百科、亚马逊、YouTube 和百度都被用于指数计算。值得注意的是，TIOBE 指数并不代表编程语言的好坏或编写代码的多少。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span style=&quot;color:#000000&quot;&gt;该指数可以用来检查你的编程技能是否还能跟上时代的步伐，或者在开始建立一个新的软件系统时，基于指数对采用何种编程语言做出决策。&lt;/span&gt;&lt;/p&gt; 
&lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.tiobe.com%2Ftiobe-index%2Fprogramminglanguages_definition%2F&quot; target=&quot;_blank&quot;&gt;TIOBE 指数&lt;/a&gt;&lt;span style=&quot;color:#000000&quot;&gt;的定义方式，以及详细榜单信息&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.tiobe.com%2Ftiobe-index%2F&quot; target=&quot;_blank&quot;&gt;均可查看官网&lt;/a&gt;。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338172/tiobe-index-202503</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338172/tiobe-index-202503</guid>
            <pubDate>Wed, 05 Mar 2025 07:30:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>最新消息：DeepSeek-R2 AI 模型将于 3 月 17 日发布</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#d35400&quot;&gt;&lt;strong&gt;3 月 11 日晚更新&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;针对 DeepSeek 将在 3 月 17 日发布下一代 R2 模型的传闻，DeepSeek 官方企业咨询账号在用户群中回应称，「辟谣：R2 发布为假消息」。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;1124&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0311/231933_8Rg2_2720166.png&quot; width=&quot;1280&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;据智通财经援引消息人士透露，&lt;strong&gt;DeepSeek 下一代 AI 模型 DeepSeek-R2 或提前于下周一 (3 月 17 日) 正式发布&lt;/strong&gt;。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;img height=&quot;948&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0311/153001_YozB_2720166.png&quot; width=&quot;1220&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;DeepSeek-R2 在多个关键领域实现突破，包括更出色的编程能力、多语言推理能力，以及以更低的成本提供更高的准确性。专业人士认为，这些特性若得以兑现，可能使其在全球 AI 竞赛中占据显著优势。这对于资产价格而言，可能又是一次重估机会。&lt;/p&gt; 
&lt;p&gt;据悉，截至目前，DeepSeek 官方尚未正式公布 R2 的具体日期及技术细节等。早前市场预期 DeepSeek-R2 模型于 5 月发布。&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;相关阅读：&lt;a href=&quot;https://www.oschina.net/news/335778/deepseek-rushes-launch-new-ai-model&quot; target=&quot;news&quot;&gt;DeepSeek R2 将提前推出&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338171</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338171</guid>
            <pubDate>Wed, 05 Mar 2025 07:25:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>基于 Megatron 的多模态大模型训练加速技术解析</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                    
                                                                                                                                                    &lt;p&gt;作者：胡凯文，李鹏，黄俊&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;单位：阿里云智能集团人工智能平台 PAI 算法团队&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_1&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;引言&lt;/h1&gt; 
&lt;p&gt;多模态大模型是近期业界关注的热点，OpenAI 的 GPT4O 以及谷歌 Gemini 等多模态大模型的出现让人机交互变得更加简单和自然。这种模型在多种下游任务上表现优异，比如图文检索、视觉问答等。通过结合语言理解和视觉感知能力，它能为用户提供更加丰富和自然的人机交互体验。Pai-Megatron-Patch 是一款由阿里云人工智能平台 PAI 研发的围绕英伟达 Megatron 的大模型训练配套工具，旨在帮助开发者快速上手大模型，打通大模型相关的高效分布式训练、有监督指令微调、下游任务评估等大模型开发链路。在 Megatron 的基础之上，Pai-Megatron-Patch 及时追踪社区最新需求，搭建了包括 LLaMA3、Qwen2 等在内的多种热门大语言模型，并在此基础上持续拓展新特性，如支持 Optimizer Offloading 功能的 Distributed Optimizer， 可与 Transformer Engine、MoE 以及流水并行等模块协同使用，以适应更多场景需求。Pai-Megatron-Patch 的整体技术栈如下图所示：&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//0965cecdadad37ae040df3b73c232297.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;本文以 Qwen2-VL 为例，从易用性和训练性能优化两个方面介绍基于 Megatron 构建的 Pai-Megatron-Patch 多模态大模型训练的关键技术。在易用性方面，最新的 Pai-Megatron-Patch 实现了基于 Mcore 的多模态编码器和 LLM 解码器，同时实现了支持高精度低损耗的 Huggingface 和 MCore 多模态模型权重互转转换以及并行加载，极大简化了不同框架间迁移的学习成本和技术障碍。在性能方面，实现了支持高性能的文本/图像/视频数据统一加载，实现了高性能的流水并行切分，另外通过引入 CPU 卸载技术以及 Sequence Packing 技术，进一步支持多模态长序列训练的显存优化，有效缓解了 GPU 的显存压力，提升了大模型训练的效率与稳定性。这些改进共同作用，为用户提供更加流畅且高效的多模态大模型开发体验。&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_2&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;支持模型并行的权重转换&lt;/h1&gt; 
&lt;p&gt;作为开源主流高效大模型训练工具，用户在尝试使用 Megatron 训练开源模型时首先遇到的障碍是 Huggingface 与 Megatron 模型权重格式的差异。下面将以 Qwen2-VL 为例，详解从 Huggingface 到 Megatron 的模型权重转换技术。如下图所示，Qwen2-VL[2] 主要由多模态编码器和 LLM 解码器两个模块组成。Qwen2-VL 采用了专门设计的多模态编码器架构，可以同时接受文本/图像/视频作为输入，并将它们转换成统一形式的表示向量后送入 LLM 解码器。这种设计使得模型能够在同一个空间内有效地捕捉到跨模态的信息关联。与 LLaVA 等多模态模型不同，Qwen2-VL 的视觉输入不需要缩放到固定分辨率大小，而是根据图像/视频数据的原始分辨率，动态生成不同长度的视觉表示向量，这一特性被称为动态分辨率。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//1eb93737ae6dbd05ddba2e152c5a393b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;通过将 Huggingface 模型转换到 Megatron 格式，用户可在已经充分训练好的 Qwen2-VL 基础大模型或者微调大模型上进行二次训练。所谓支持模型并行的权重转换，其核心原理是在转换时将大型模型权重分割成多个部分，并分配给不同的 GPU 进行并行加载。通过这种方式可以显著减少单个设备上的显存占用量，加快训练速度。然而，在实际应用中这一技术面临着若干挑战。在开发过程中，我们遇到的最常见问题之一是在完成权重转换后重新加载进行训练时，step 0 loss 比预期高。造成这种情况的原因主要有两个方面：一方面是模型实现风格不一致，结构映射关系十分复杂；另一方面是模型权重并行切分算法实现容易出错。因此，针对上述两种原因，Pai-Megatron-Patch 分别进行了优化。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;在 Pai-Megatron-Patch 的模型转换中，针对模型结构映射关系十分复杂的问题，建模分析并构建映射表能有效对比 Huggingface 模型与 Megatron 实现的联系。下表总结了两者分别在各个算子类型上的命名对应关系。在支持模型并行的权重转换实现过程中，我们首先将 Huggingface 模型的 ckpt 赋值给 Megatron 模型，然后再在 Megatron 模型上进行 TP/PP 切分。这种先转换再切分的好处是可以提升代码的可维护性，减少转换出错的概率。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//ef715495d37cfd035b51f1b0d64e4bbb.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;除了模块层面的实现差异，在算子层，由于 3D 并行的存在，Megatron 与 Huggingface 的权重及计算方式也存在很大区别，需要根据实际情况进行转换。针对模型权重并行切分算法实现容易出错的问题，逐算子精细化切分保证了自底向上的准确转换。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//0a7d46cfbd1f0e2bdb39037f639960a9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;上图表示了一个 Huggingface 实现的 GQA 权重转换到 Megatron 的正确/错误流程，其中每个块表示一个 head 的权重，边框和内部的颜色分别代表这个 head 对应的类型及被理解成的类型。在 transformers 库中，Huggingface 采用 q_proj, k_proj, v_proj 这三个算子来计算 QKV status, 随后再调用&lt;code&gt;repeat_kv()&lt;/code&gt;函数将同一个 Query Group 内的 KV status 数量与 Query 对齐，进而调用 attention 函数。在 Megatron 中，由于张量并行的存在，Attention Module 采用了 ColumnLinear(linear_qkv) 对 QKV 统一计算，为了降低节点间通信量，这个并行 Linear 将 Query Group 均匀切分到各个 Node 上，使每个节点内就能计算一部分 attention 结果，不需要对 QKV status 进行同步。在这个 case 下，如果直接拼接 QKV 的权重张量（如图上半部分）再进行 TP 切分，可以明显看到大部分 head 位置出现问题，因此，我们需要将 QKV 权重进行一定转换，按 Query Group 的顺序进行拼接（如图下半部分）后再切分才能保证 Attention 的正确性。具体地说，在进行 TP=2 切分时，Pai-Megatron-Patch 先将 query，key 和 value 这三个 tensor 分别 reshape 成一个 4D 的 tensor，reshape 后的 shape 是&amp;lt;num_query_groups, -1, head_dim, hidden_dim&amp;gt;。接着沿着第二个维度拼接在一起，然后沿着 num_query_groups 维度进行算子拆分。才能确保并行加载 ckpt 后的起步 loss 值误差偏移在合理范围内。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//13b6f29e544514a59b0fa5f932487c7e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_3&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;用户友好的多模态数据加载&lt;/h1&gt; 
&lt;p&gt;相对于 LLM 预训练，在数据加载方面，由于引入了多个类型的数据，多模态模型的 DataLoader 尤其复杂。为了支持 LLaVA 等模型，英伟达的 Megatron 进一步推出了 Energon 来实现多模态数据的加载逻辑。尽管如此，由于目前多模态输入并无统一的格式，在目前的 Megatron 中，对于多模态数据的支持仍比较有限，具体表现在 (1) 仅支持部分格式简单的数据加载，例如每个样本至多包含一张图像或一个视频，应用场景有限;(2) 现有 TaskEncoder 输出格式固定，灵活性不大，难以高效支持模型训练。在实际开发中，Qwen2-VL 同时遇到了上述两类障碍:与已有的 LLaVA 相比，Qwen2-VL 基于 ChatML 格式设计输入，应当支持单个样本中多张图片视频、多轮对话等复杂功能，但这类数据无法使用当前 energon 的代码进行读取;此外，现有 TaskEncoder 仅支持将图片缩放到固定大小，再传入模型训练，因此难以支持 Qwen2-VL 独有的动态分辨率特性。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;为了解决上述功能性问题，我们首先对内置的 DataLoader 代码做了大量扩展，使其能基于输入的原始多模态数据构造用于动态分辨率训练的图像切片序列以及位置信息，同时支持可自定义 prompt、多轮、包含任意数量图像或视频的对话样本的数据处理。在此基础上，我们设计了一套自动化脚本，能基于用户给定的 sharegpt 格式数据集路径自动转换成 energon 可读取的 webdataset 数据集，绕过当前 energon 数据集准备的交互流程，加快数据转换效率，同时降低用户学习成本。此外，对于数量不定的图像，为了能在 energon 侧自动解码，webdataset 支持将其以 numpy array 的形式保存，然而，由于 jpg 到 ndarray 间的数据格式差异，在实际测试中，我们发现这将造成最终数据集文件体积出现几倍到几十倍的增加。为了解决这个问题，我们在运行时向 webdataset 的编解码模块增加钩子函数，使其支持图像文件列表的自动编解码，实现在按原始二进制数据保存的同时能被 energon 自动转换为图像张量的需求。通过上述优化，在 Pai-Megatron-Patch 中，经过转换的 sharegpt 格式数据相对于其原始总大小几乎没有变化。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//6d2c8ff40024e1e3d09c02ef056fc475.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_4&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;视觉特征处理流程优化&lt;/h1&gt; 
&lt;p&gt;与 LLM 相比，多模态模型先对视觉数据进行编码，再与文本特征拼接后送入 LLM 进行推理。与基于静态分辨率的多模态大模型不同，复杂的多模态数据格式以及动态分辨率共同使得 Qwen2-VL 的这一过程更加复杂，不仅显著影响训练效率，也与能否应用 TP Comm Overlap 等技术相关。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;与静态分辨率使用固定数量视觉 token 不同，动态分辨率技术允许模型依据输入图像大小，将其编码成不定数量的视觉特征。这一改进使模型对于高分辨率图像的细节捕捉能力获得显著提升，同时也优化了低分辨率图像的推理性能。下图是 Qwen2-VL 的训练数据的简单示例，针对不同长度的视觉 token，最终它们会被嵌入到文本序列中，与同一个 batch 的数据拼合后送入到 LLM 解码器。在这个过程中主要存在两个难点:(1) 如何对同一批次内的多个视觉输入高效获得特征表示;(2) 如何拼合数据以支持原生性能优化开关。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//739d529e86745f50251962405174bef3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;由于视觉输入的分辨率具有很大不确定性，为了将同一 batch 内的不同长度视觉数据转换成推理所需的特征表示，常见的做法是将每个视觉数据填充到相同长度后统一送入视觉模型。尽管这一方式实现简洁，但填充的 token 不仅造成了显存的浪费，同时也影响了视觉编码器的吞吐，当同一批次数据中同时包含高分辨率视频及低分辨率图像，由于视觉 token 数量的显著差异，将造成极为显著的性能浪费。对此，Pai-Megatron-Patch 借鉴了 Sequence Packing 的做法，将同一批次内的所有视觉输入打包后调用 varlen attention，来避免填充操作带来的显存/性能损耗。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//939334d510ecedf2be878901ae2b15b7.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;在生成视觉特征表示后，基于预先构造好的掩膜张量，视觉编码器的输出会被依次填入文本表示的对应位置，替换掉默认的占位符。在不使用任何性能优化技术的情况下，这一实现并不会带来什么问题。然而，当开启序列并行后，LanguageEmbedding 模块的输出会自动 reduce scatter 到各个 TP rank，导致基于原始输入构造的完整掩膜张量无法使用。为了解决这一问题，同时避免冗余通信，Pai-Megatron-Patch 针对性修改了 MCore 中实现的 LanguageEmbedding 模块，在输入序列拆分前增加利用掩膜张量替换文本表示的操作 ，不干扰序列并行特性的正常运行。&lt;strong&gt;在实现序列并行机制的基础上，通过进一步应用 TP Communication Overlap 特性，对于 Qwen2-VL-70B 我们在 4 机 32 卡 a100 上观察到了 6% 性能提升。（详见实验部分）&lt;/strong&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_5&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;基于优化器卸载的多模态长序列显存优化&lt;/h1&gt; 
&lt;p&gt;相对于大语言模型训练微调，由于视频、音频等模态存在，多模态大模型的输入序列长度将远远大于纯的语言模型。因此，在通常情况下，为了完成这类模型的训练微调，往往需要更多的计算资源。而当获取更多资源这条路径不可行时，在资源受限的情况下拉起任务成为这一场景下的首要挑战。在之前 Pai-Megatron-Patch 实现的优化器卸载特性基础上，最近，PAI 团队与英伟达 Megatron 团队深度合作，共建了一套原生基于 Megatron 的权重卸载优化器。在此基础上，通过与多种正交的卸载技术结合，Pai-Megatron-Patch 将四机 32 卡 A100 上的 Qwen2-VL 的最长上下文从 4K 提升至 32K。在本章节中，我们将以 BF16 训练为例，详细描述最新版权重卸载优化器的原理、实现及我们引入的最新性能优化特性--Optimizer Overlapping。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;如下图，与激活重计算或激活卸载等显存优化技术不同，优化器卸载的目的在于将参数更新这一计算负载较低但显存占用相对高的步骤全部或部分放置到 CPU 上进行，以达到降低显存占用的目的。对于一个参数量为 M 的模型，Adam 会默认分配大小为 12M 字节的显存以保存全精度的参数 (4M) 及优化器状态 (8M)，当使用优化器卸载时，优化器则会将这 12M 大小的张量全部分配到内存上，同时额外分配 4M 的内存用于拷贝 GPU 上的全精度梯度数据。因此，当打开 Megatron 的分布式优化器时，以 4 机 32 卡训练 70B 为例，每张卡的显存能进一步减少约 26GB。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//d9895494e5ede2ef2bfd1f6e0b119cdc.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;如下图所示，在最新的原生优化器卸载特性实现中，我们设计了一个混合设备优化器类 (Hybrid Device Optimizer, HDO) 来支持静态优化器卸载功能。与先前的深度修改 DistributedOptimizer，难以快速迁移到其他训练代码不同，通过在顶层沿用 PyTorch 优化器的 API，HDO 能无缝替换 TorchAdam，同时仍保留一定的优化器卸载能力。我们希望这一设计能使用户在更多场景下体验到优化器卸载带来的资源需求下降*注。整个 HDO 大致分为两部分，包括针对单个参数张量更新的底层优化器集合，以及用于管理优化器参数映射关系、优化器保存/读取、以及更新过程中子优化器间同步的 HDO 类。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//f44b34f0fc6d5d9c7efc682cbcd49061.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;由于在先前版本的 Megatron-Core 中，优化器内全精度参数创建部分由 DistributedOptimizer 默认分配到 GPU 上，并不能进行卸载，导致最初 HDO 的实际显存优化仅有 8M 而非旧版深度定制 DO 的 12M。为了解决这一问题，我们修改了 DO 的参数创建逻辑，当识别到用户启用优化器卸载特性时，DO 自动将全精度参数创建这一步骤交由 HDO 进行，避免了冗余显存分配。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;注: 由于参数已经被外部创建，为了不对这些参数进行修改，这一场景下仅能将 8M 的 Adam/AdamW 优化器状态分配及参数更新移动到 CPU 上进行，对于 SGD 优化器，由于无额外的固定显存分配，HDO 没有卸载效果。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;优化器初始化&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;与支持单一设备张量的优化器不同，为了实现位于不同设备上的多个张量的参数更新及性能优化，HDO 被实现为多个子优化器的集合，每个子优化器负责单一设备上的数个张量的参数更新，而 HDO 用于实现主优化器与子优化器间的优化状态同步、Device/Host 间的参数/梯度传递以及由此引发的 CUDA 同步问题。在优化器初始化阶段，相对于普通的 Adam，HDO 额外要求用户指定 CPU/CUDA 上的子优化器类型以及相应的卸载比例，随后基于这一比例对 param_groups 进行拆分，再重新构造对应设备上的子优化器。在训练阶段，由于 lr_scheduler 等外部因素可能会对优化器参数产生影响，在子优化器参数更新前，HDO 会将所有状态同步到子优化器，如果子优化器在 CPU 上更新，也会将 GPU 梯度复制到 CPU 上。由于子优化器也可能修改参数组的状态，在更新结束后，每个子优化器也会将这些状态同步到主优化器，从而保证与非优化器卸载训练的收敛一致性。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;优化器保存/读取，及 卸载比例变换&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;相对于 Pai-Megatron-Patch 内的优化器卸载模块，HDO 支持完整的优化器保存/读取特性，同时也支持加载优化器时切换优化器卸载比例，具有更大的灵活性。如前文所述，HDO 内部由多个子优化器负责实际参数更新，在每个参数更新阶段开始前/结束后分别进行 HDO 及子优化器间参数的同步。在保存优化器状态时，由于 HDO 内数据与子优化器一致，HDO 的 state_dict 即包含恢复优化器所需的全部信息。在加载权重时，由于子优化器需要基于 fp32 参数进行更新，我们引入了 pre_load_state_dict_hook 以及 post_load_state_dict_hook，将 HDO 的 state 中的半精度参数数据临时用保存的 fp32 替换后同步到子优化器，再替换回来，随后调用&lt;code&gt;_move_new_state_to_right_device&lt;/code&gt;，将每个子优化器的状态移动到其对应的设备。通过在运行时更新子优化器的对应设备，我们能将在一个卸载比例下保存的模型在另一个优化器卸载配置上拉起训练。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Naive Optimizer Overlapping 技术&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;与 CUDA 上的参数更新相比，基于 CPU 的参数更新性能成为优化器卸载技术是否可用的关键因素。通常，优化器卸载的时间包含三个部分：将 CUDA 梯度同步到 CPU(D2H)、在 CPU 上进行参数更新 (C)、将更新后的 CPU 参数同步到 CUDA(H2D)。为了尽可能提升性能，我们提出了一项 Naive optimizer Overlapping 技术，通过 PyTorch 的 CUDA Stream 机制，实现通信与计算重叠，将 CPU-CUDA 间数据拷贝的时间尽可能掩盖在 CPU 计算下。在 Naive optimizer Overlapping 中，我们的一个主要改进是将 CPU 子优化器进一步打散，使一个 CPU 参数对应一个 CPU 优化器，同时令所有拷贝非阻塞化。这一改进允许 HDO 在单个梯度张量同步完成后立即调用 CPU 参数更新，无需等待所有梯度完成拷贝，也允许 CPU 参数更新完成后立刻非阻塞拷贝到 GPU，从而实现重叠。为了保证多个 CUDA Stream 以及 CPU 之间正确的数据同步关系，同时尽可能避免影响性能，我们仅在关键位置引入 CUDAEvent，来避免额外流同步带来的开销。下图表示了一个理想的优化器重叠场景，其中每个参数张量的更新时间与两次数据传输接近。当流水线热身结束后（time step 2），可以看到 HDO 在两个 CUDA Stream 上进行双向的数据拷贝同时，CPU 也在同时进行参数更新。这一方式最大化掩盖了冗长的数据传输时间，&lt;strong&gt;在我们测试的 LLaMA2-70B 训练中，我们观察到该优化能减少约 1s 的端到端时间。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//5ca7cdd30c2c3af2cf816eeb23f0e253.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_6&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;多模态流水并行性能优化&lt;/h1&gt; 
&lt;p&gt;多模态大模型训练微调作为英伟达 Megatron 的新特性，目前仍在持续开发中。在 3D 并行方面，目前以 LLaVA 为代表的多模态模型的类型均为 ModelType.encoder_and_decoder，它对 encoder（视觉编码器）与 decoder（LLM 解码器）分别引入了一套 TP/PP 参数，同时支持特殊的 Encoder PP 配置：当 Encoder PP&amp;gt;0 时，Megatron 会将 Encoder 放在独立的 GPU 上，允许两部分模型使用不同的 TP/PP 参数初始化；当 Encoder PP=0 时，Megatron 将 Encoder 部分按照 Decoder 的 TP 切分后，与 Decoder 的第一个 PP Stage 合并，放置在同一组 GPU 上。其中，前种切分方式一般适用于编码器/解码器结构类似、参数量接近的情况；后者则适用于视觉编码器参数量较少，计算量不大的场景。考虑到 Qwen2-VL 的视觉编码器大小以及用户体验，我们最初采用了编码器与解码器部分合并的切分方式，来保持和其他 LLM 相似的切分参数配置，减少学习成本。但在实际测试中，通过与基模型 Qwen2 对比，我们观察到这一合并显著降低了 Qwen2-VL 的训练吞吐。这主要由两方面因素导致：(1) 由于模型结构不一致，视觉编码器与 LLM 解码器的前向速度有差异，导致 micro batch 内哥 GPU 间存在负载不均衡;(2) 由于视觉编码器支持动态分辨率特性，对于每个 micro batch，实际前向的 token 数并不一致，使得每个 mciro batch 计算量略有差异，出现各 GPU 间存在负载不均衡的情况。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;为了解决上述负载不均衡的问题，进一步提升训练吞吐，在原生 Megatron 的基础上，我们从两个方向对现有框架进行改进尝试：(1) 基于非均匀切分策略间的负载均衡;(2) 拓展模型实现以支持虚拟流水线并行特性。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;基于非均匀切分策略间的负载均衡：在模型总参数量较小或 PP 较大的并行配置下，pp rank 0 包含的视觉编码器带来的额外计算量相对于原有的 LLM 解码器的计算量比例有显著提升。由于在流水线中，最慢的步骤决定了整体的性能，应用 megatron 内的非均匀切分特性，将部分计算量转移到其他 pp rank 上，能有效提升整体的训练性能。为了支持基于非均匀切分的继续预训练或微调功能，我们同步更新了 Qwen2-VL 的转换模型，用户可以通过控制 MP_PP0_LAYERS 环境变量，获得非均匀切分的模型权重文件。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//32be076a3fe4ab8d995a8aa30ddcf555.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;虚拟流水线并行特性：虚拟流水线并行，或 interleaved 1F1B PP 是 Megatron-LM 在 1F1B 的流水线并行实现上的重要改进。它通过将同一个 pp stage 内的 transformer 层分配到多个 GPU，进一步打碎计算，通过增大通信量的方式使负载更加均衡，降低 Global Batch 内的 bubble 比例。然而， 对于 encoder_and_decoder 类型的模型（即英伟达内置的所有多模态模型），MCore 不支持启用 VPP。考虑到视觉编码器的整体计算量不需要独立 GPU，我们进一步改进了 Qwen2-VL 实现，移除了 Encoder PP 的支持，并将 Qwen2-VL 的模型类型设置为 LLM 的 encoder_or_decoder 来启用 VPP 选项。&lt;strong&gt;与官方论文的结果不同，我们观察到虚拟流水线并行也能起到训练加速效果，甚至在 H20 上，我们观察到相对于非均匀切分，虚拟流水线并行的加速效果更加明显。&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id=&quot;OSC_h1_7&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;实验分析&lt;/h1&gt; 
&lt;p&gt;在本节中，我们对上述提到的技术对 Qwen2-VL 模型的性能及收敛情况展开了全面的分析，主要包括三方面：1. 权重转换精度分析：通过对比转换前后的模型测试集表现，我们验证了权重转换模块的正确性；2.多模态长序列显存优化分析：通过引入 HDO 对优化器进行卸载，在相同 GPU 数量的情形下，Pai-Megatron-Patch 大大提升了长序列训练的可用性；3.多模态模型训练性能优化：全面测试了各训练加速技术对于不同设备/模型大小的影响，论证了技术的有效性，同时为用户提供开关设置的指引。&lt;/p&gt; 
&lt;span id=&quot;OSC_h3_8&quot;&gt;&lt;/span&gt; 
&lt;h3&gt;权重转换精度分析&lt;/h3&gt; 
&lt;p&gt;为了验证 PAI-Megatron-Patch 内多模态模型转换的正确性，我们采用 VLMEvalKit 对转换前后的 Qwen2-VL-7B 模型在多个数据集上的表现进行评估。结果如下图，其中 Ref 是 VLMEvalKit 提供的在 SEEDBench 及 MMBench 上的评估分数，Official 是官方权重在测试环境内的实测分数，Convert 为将官方原始权重进行两次转换得到的新 Huggingface 权重的分数。可以看到，转换前后的模型评估分数完全一致，且均与开源 leaderboard 指标接近，有效说明了转换代码的正确性。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//03efee772c4e46b60b013c2d364ef1c2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;span id=&quot;OSC_h3_9&quot;&gt;&lt;/span&gt; 
&lt;h3&gt;基于优化器卸载的多模态长序列显存优化&lt;/h3&gt; 
&lt;p&gt;在本节中，我们探索了通过 HDO 与其他显存优化技术复合，进一步提升有限机器场景下可训练上下文长度上限的可能性。我们分别测试了 Qwen2-VL 7B/72B 在单机 8 卡、四机 32 卡的无 offload 情况下的最长上下文，以及打开所有优化后的上下文长度，结果如下表。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//e1dc0526f3b5d0fc7ccb86254065117d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;可以看到，对于 Qwen2-VL 7B 模型，通过结合激活重计算技术及优化器卸载技术，我们将原本 64K 上下文长度的训练上限进一步拓展到 128K，且没有 OOM 风险。*对于 72B 模型，结论类似，通过结合多种正交的显存优化技术，我们也将四机 32 卡 A100 的可训练上下文长度从 16K 提升到 128K。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;*注：在单独开启重计算的情况下我们也成功在 demo 数据集上实现了 128K 上下文长度的训练，此时显存空闲不到 1G，考虑到实际场景下视觉模型占用的显存会随数据变化等原因，不推荐使用这一配置训练。&lt;/p&gt; 
&lt;span id=&quot;OSC_h3_10&quot;&gt;&lt;/span&gt; 
&lt;h3&gt;多模态模型训练性能优化&lt;/h3&gt; 
&lt;p&gt;在本节中，我们针对两种大小的 Qwen2-VL 模型的训练场景，测试了 PAI-Megatron-Patch 当前支持的训练优化技术对吞吐的影响，来说明其在不同场景下的优势。具体地，我们采用预处理的 LLaVA-Pretrain 数据集 (参见 Qwen2-VL 的最佳实践文档) 在 4K 上下文长度下对 Qwen2-VL-7B/70B 进行训练，在最佳并行配置的基础上，进一步打开各类优化技术以比较模型性能的变化。其中，&lt;code&gt;PP0_layers&lt;/code&gt;表示开启非均匀切分时，PP Rank0 上的 LLM Decode r 层数，&lt;code&gt;TGP&lt;/code&gt;内的三个字母依次表示&lt;code&gt;tp-comm-overlap&lt;/code&gt;、&lt;code&gt;overlap-grad-reduce&lt;/code&gt;、&lt;code&gt;overlap-param-gather&lt;/code&gt;三个优化开关的启用状态。对于每个实验，我们记录了其训练时的每秒 token 数 (TGS) 及 MFU。需要注意的是，由于 megatron-core 暂不支持估计视觉编码器的 TFLOPs，记录的 MFU 仅基于 LLM 的运算量计算得到，因此略低于实际值。&lt;/p&gt; 
&lt;span id=&quot;OSC_h4_11&quot;&gt;&lt;/span&gt; 
&lt;h4&gt;overlap 开关对训练吞吐的影响&lt;/h4&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//8ccde1833962a155ee78229e63d7572e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;相对于 LLM，由于存在多个不同结构的 transformer，多模态模型并不原生支持 megatron-core 的某些 overlap 技术，同时差异化的结构也可能导致 overlap 的收益与 LLM 不同。在本节中，我们在 A100 上测试了&lt;code&gt;tp-comm-overlap&lt;/code&gt;、&lt;code&gt;overlap-grad-reduce&lt;/code&gt;、&lt;code&gt;overlap-param-gather&lt;/code&gt;三个优化开关对 Qwen2-VL 7B/72B 训练吞吐的影响，结果如上表所示。主要结论如下：&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;对于 Qwen2-VL，可以观察到&lt;code&gt;overlap-grad-reduce&lt;/code&gt;及&lt;code&gt;overlap-param-gather&lt;/code&gt;无论在 7B/72B 模型规模上均导致了性能出现略微下降。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;相对于 72B 模型 (~8%),7B 模型开启&lt;code&gt;tp-comm-overlap&lt;/code&gt;的直接收益可以忽略不计。这主要是因为 7B 模型实际运行时的低 TP 数以及未开启 overlap 的 vision model 计算量占比相对较高。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;span id=&quot;OSC_h4_12&quot;&gt;&lt;/span&gt; 
&lt;h4&gt;虚拟流水并行及非均匀切分对训练吞吐的影响&lt;/h4&gt; 
&lt;p&gt;基于 megatron-core 的核心能力，Pai-Megatron-Patch 为 Qwen2-VL 提供了两种性能优化方式：多模态大模型的虚拟流水并行及解码器非均匀切分技术。为了比较这两者对训练吞吐的影响差异，我们在最佳并行配置的基础上，比较了两种技术的性能上限，结果如下表。其中，VP 表示每个虚拟并行块中的解码器 Transformer 层数（区别于通常意义的 VPP Size）。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet//e941277896d9cc4003214357264a1e77.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;从表中我们能得出的主要结论如下：&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;尽管&lt;code&gt;tp-comm-overlap&lt;/code&gt;仅能为 Qwen2-VL 72B 模型直接带来约 8% 的性能提升，然而，通过序列并行带来的显存优化，模型有进一步降低并行数来提升吞吐的潜力。例如在 H20 上，我们观察到，通过打开 SP 及 TP Comm overlap 开关，模型可以采用 TP4PP4 的配置进行训练，通过调整虚拟流水并行，相对于 TP8PP4 的最佳性能有额外 15% 的性能提升。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;由于各 GPU 的机内、机间带宽差异，在不同的 GPU 上，最佳优化技术有所不同。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;与 Qwen2-VL 论文的结果不同，我们的 VPP（或 1F1B interleaved）实现在 A100/H20 机器上均有一定的性能提升。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;span id=&quot;OSC_h1_13&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;总结&lt;/h1&gt; 
&lt;p&gt;在基于 Megatron 的 Qwen2-VL 多模态大模型最佳实践开发过程中，我们围绕大模型训练测试了以下核心技术的性能：&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;支持模型并行的权重转换。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;基于 CPU 卸载的多模态长序列显存优化的鲁棒性。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;基于 Megatron 的多重训练加速技术的稳定性。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;运用综合加速技术来训练 Qwen2-VL 过程中的易用性和稳定性。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;后续在 Pai-Megatron-Patch 中还会陆续放出更多高质量的大模型最佳实践以及最新的训练加速技术应用示例，敬请期待。&lt;/p&gt; 
&lt;span id=&quot;OSC_h1_14&quot;&gt;&lt;/span&gt; 
&lt;h1&gt;参考文献&lt;/h1&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Qwen2-VL: Enhancing Vision-Language Model&#39;s Perception of the World at Any Resolution&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;MMBench: Is Your Multi-modal Model an All-around Player?&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fopen-compass%2FVLMEvalKit&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot;&gt;https://github.com/open-compass/VLMEvalKit&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/u/5583868/blog/17876695</link>
            <guid isPermaLink="false">https://my.oschina.net/u/5583868/blog/17876695</guid>
            <pubDate>Wed, 05 Mar 2025 07:09:00 GMT</pubDate>
            <author>原创</author>
        </item>
        <item>
            <title>奇安信：攻击 X 平台的僵尸网络与春节攻击 DeepSeek 的为同一组织</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;美国东部时间 3 月 10 日，埃隆·马斯克一天内遭遇了双重打击：除了特斯拉股价下跌 15%、自高点几乎腰斩之外，其旗下的社交媒体 X 平台（原 Twitter）还遭遇了大规模网络攻击，导致全球范围内多次宕机，许多用户无法正常使用该应用程序。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-3d867334583b4a067b5d5ac7e28903e2b6f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-c22e078be8628892b35aceafc606813c59c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FBSbnMTxG_piE0Mi_bH4v0A&quot; target=&quot;_blank&quot;&gt;奇安信 Xlab 实验室监测发现&lt;/a&gt;&lt;/u&gt;，&lt;strong&gt;此次攻击所使用的僵尸网络为 Mirai 变种僵尸网络 RapperBot，&lt;/strong&gt;攻击时间与 X 平台宕机时间完全吻合，主要集中在北京时间 3 月 10 日晚 10 点至 11 日凌晨 2 点。&lt;strong&gt;与 2025 年春节期间攻击 DeepSeek 的属于同一组僵尸网络&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;据称该僵尸网络常年活跃，以高强度的流量攻击闻名，平均每天攻击上百个目标，高峰时期指令上千条，能够迅速瘫痪目标服务器。攻击目标分布在巴西、白俄罗斯、俄罗斯、中国、瑞典等地区。此次攻击规模之大、烈度之猛，直接导致 X 平台在美国东部时间 3 月 10 日遭遇三次明显服务中断，最高时有 20538 名用户报告故障。&lt;/p&gt; 
&lt;p&gt;RapperBot 僵尸网络并非普通黑客组织，而是对外提供有偿攻击服务的「职业打手」。其攻击规模和资源投入远超普通网络攻击，可能涉及大型组织甚至国家层面的支持。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338164</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338164</guid>
            <pubDate>Wed, 05 Mar 2025 07:04:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>RWKV7-G1 0.1B 推理模型发布，最适合嵌入式的纯血 RNN 模型</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;2025 年 3 月 10 日，RWKV 基金会发布第一个 &lt;strong&gt;RWKV-7 推理模型&lt;/strong&gt;（Reasoning Model）： &lt;strong&gt;RWKV7-G1&lt;/strong&gt; 0.1B。&lt;/p&gt; 
&lt;p&gt;RWKV7-G1 系列模型拥有&lt;strong&gt;杰出的推理能力&lt;/strong&gt;，且原生支持世界 100+ 种语言和代码。即使是最小的 0.1B 也能回答&lt;strong&gt;开放性和创造性问题&lt;/strong&gt;。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;RWKV7-G1（&quot;GooseOne&quot;）系列推理模型是基于 World v3.5 数据集&lt;strong&gt;继续训练&lt;/strong&gt; RWKV-7 &quot;Goose&quot; World 系列模型。&lt;/p&gt; 
 &lt;p&gt;World v3.5 数据集包含更多小说、网页、数学、代码和 reasoning 数据，总数据为 5.16T tokens。对于 0.1B 模型，我们会随机采样其中的 1T tokens 训练。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;目前我们已能在手机高通 8gen3 以 62 token/s 推理 RWKV-7 1.5B 模型，而 0.1B 模型在树莓派也能跑得挺快，欢迎做嵌入式的朋友加入 RWKV 技术群讨论。&lt;/p&gt; 
&lt;h2&gt;模型表现&lt;/h2&gt; 
&lt;p&gt;RWKV7-G1 0.1B 模型回答 &lt;code&gt;simulate SpaceX mars landing using python&lt;/code&gt;（使用 python 模拟 SpaceX 火星着陆）」：&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;RWKV G1 0.1B simulate SpaceX mars landing using python&quot; height=&quot;2276&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-a90a626d3589d084f50ce2836131854b642.jpg&quot; width=&quot;650&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;日本开发者测试 RWKV7-G1 0.1B 的多语言能力：&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;RWKV7-G1-0.1B-jpn&quot; height=&quot;480&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-30c2c03d927be963bcead808ee26ffd2196.png&quot; width=&quot;600&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;如此小的 0.1B 模型，也能同时支持世界 100+ 种语言和代码。更大参数的 RWKV7-G1 0.4B/1.5B/2.9B &lt;strong&gt;正在同时训练中&lt;/strong&gt;。&lt;/p&gt; 
&lt;h2&gt;英文和多语言测评&lt;/h2&gt; 
&lt;p&gt;RWKV7-G1 0.1B 的英文和多语言能力相比 RWKV-7-World 0.1B 继续提升：&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;141&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-c2badec5fa9dba560aa79f167930e033418.png&quot; width=&quot;1441&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;我们也对 RWKV7-G1 0.1B 进行了 「无法作弊的模型评测」 Uncheatable Eval，可见 RWKV7-G1 0.1B 对于多种新数据的压缩率，显著超越所有其它同尺寸的开源模型：&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;154&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-d64cc13d9c6b94f4752c404005bbf9b83bb.png&quot; width=&quot;700&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Uncheatable Eval：&lt;/strong&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fspaces%2FJellyfish042%2FUncheatableEval&quot; target=&quot;_blank&quot;&gt;https://huggingface.co/spaces/Jellyfish042/UncheatableEval&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;训练中的 RWKV7-G1 1.5B 模型&lt;/h2&gt; 
&lt;p&gt;以下示例基于 &lt;code&gt;RWKV7-G1-1.5B-16%trained&lt;/code&gt;模型，注意这个模型目前只训练了 16%。后续 100% 训练完成的 RWKV7-G1 1.5B 会显著更强：&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;1556&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-4b81465fc759bfe4e90f4c9969ce95b71d2.png&quot; width=&quot;812&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;RWKV7-G1-1.5B-16%trained 的示例二：&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;541&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-a300e868682a29ea61efa75a25523cee5fd.png&quot; width=&quot;660&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;训练中的 RWKV 模型可在 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2FBlinkDL%2Ftemp-latest-training-models%2Ftree%2Fmain&quot; target=&quot;_blank&quot;&gt;https://huggingface.co/BlinkDL/temp-latest-training-models/tree/main&lt;/a&gt; 下载。&lt;/p&gt; 
&lt;h2&gt;模型试用&lt;/h2&gt; 
&lt;p&gt;可以在 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fspaces%2FBlinkDL%2FRWKV-Gradio-2&quot; target=&quot;_blank&quot;&gt;Hugging Face Gradio Demo&lt;/a&gt; 试用 RWKV7-G1 0.1B 模型。&lt;/p&gt; 
&lt;p&gt;G1 的整体 prompt 格式与 RWKV-7 模型类似，可选使用 &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; 标签开启 reasoning 功能：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;User: 你不许参加学术派对！

Assistant: &amp;lt;think&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;RWKV Runner 和 Ai00 等 RWKV 推理工具&lt;strong&gt;正在适配 reasoning 聊天模式&lt;/strong&gt;，因此目前只能在&lt;strong&gt;续写模式中&lt;/strong&gt;体验 reasoning 功能。&lt;/p&gt; 
&lt;h2&gt;模型下载&lt;/h2&gt; 
&lt;p&gt;下载已完成训练的 RWKV7-G1 0.1B 模型：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Hugging Face：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2FBlinkDL%2Frwkv7-g1%2Ftree%2Fmain&quot; target=&quot;_blank&quot;&gt;https://huggingface.co/BlinkDL/rwkv7-g1/tree/main&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;魔搭社区：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmodelscope.cn%2Fmodels%2FRWKV%2Frwkv7-g1%2Ffiles&quot; target=&quot;_blank&quot;&gt;https://modelscope.cn/models/RWKV/rwkv7-g1/files&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;WiseModel：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwisemodel.cn%2Fmodels%2Frwkv4fun%2FRWKV-7-G1%2Ffile&quot; target=&quot;_blank&quot;&gt;https://wisemodel.cn/models/rwkv4fun/RWKV-7-G1/file&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;下载其他训练中的 RWKV7-G1 模型：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Hugging Face：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2FBlinkDL%2Ftemp-latest-training-models%2Ftree%2Fmain&quot; target=&quot;_blank&quot;&gt;https://huggingface.co/BlinkDL/temp-latest-training-models/tree/main &lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;魔搭社区：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmodelscope.cn%2Fmodels%2FRWKV%2Ftemp-latest-training-models%2Ffiles&quot; target=&quot;_blank&quot;&gt;https://modelscope.cn/models/RWKV/temp-latest-training-models/files&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;G1 模型发布计划&lt;/h2&gt; 
&lt;p&gt;当前已发布 G1 0.1B 模型，正在训练 G1 0.4B/1.5B/2.9B，具体发布计划如下：&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;th&gt;模型&lt;/th&gt; 
   &lt;th&gt;发布计划&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;G1 0.1B&lt;/td&gt; 
   &lt;td&gt;3 月 8 日（已发布）&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;G1 0.4B&lt;/td&gt; 
   &lt;td&gt;3 月下旬&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;G1 1.6B&lt;/td&gt; 
   &lt;td&gt;4 月&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;G1 2.9B&lt;/td&gt; 
   &lt;td&gt;5 月&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;我们也在同时准备更大更优的数据集 &lt;strong&gt;World v3.7&lt;/strong&gt;，用于 G1 7B 训练。&lt;/p&gt; 
&lt;h2&gt;RWKV-7 学术支持&lt;/h2&gt; 
&lt;p&gt;RWKV 社区近期新增了大量 RWKV 学术研究论文，以下是截至 2025 年 2 月的 RWKV 论文数量统计表格：&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;546&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-ad88e36470c390ace32e587452140fa813c.png&quot; width=&quot;650&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;欢迎大家基于 RWKV-7 进行创业、科研，我们也会为基于 RWKV 的项目提供技术支持。&lt;/p&gt; 
&lt;p&gt;如果您的团队正在基于 RWKV 创业或开展研究，请联系我们！（在「RWKV 元始智能」微信公众号留言您的联系方式，或发送邮件到「contact@rwkvos.com」。）&lt;/p&gt; 
&lt;h2&gt;加入 RWKV 社区&lt;/h2&gt; 
&lt;p&gt;欢迎大家加入 RWKV 社区，可以从 RWKV 中文官网了解 RWKV 模型，也可以加入 RWKV 论坛、QQ 频道和 QQ 群聊，一起探讨 RWKV 模型。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;📖 RWKV 中文文档：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.rwkv.cn&quot; target=&quot;_blank&quot;&gt;https://www.rwkv.cn&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;💬 RWKV 论坛：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcommunity.rwkv.cn%2F&quot; target=&quot;_blank&quot;&gt;https://community.rwkv.cn/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;🐧 QQ 频道：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fpd.qq.com%2Fs%2F9n21eravc&quot; target=&quot;_blank&quot;&gt;https://pd.qq.com/s/9n21eravc&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;📺 BiliBili 视频教程：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fspace.bilibili.com%2F3546689096910933&quot; target=&quot;_blank&quot;&gt;https://space.bilibili.com/3546689096910933&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338160</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338160</guid>
            <pubDate>Wed, 05 Mar 2025 06:53:00 GMT</pubDate>
            <author>来源: 投稿</author>
        </item>
        <item>
            <title>李想：AGI 投资远超互联网、一两年内还不具备更好的商业模式</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;理想汽车 CEO 李想近日在朋友圈分享了对 AGI 的一些观点，他表示，AGI 所需的投资远超互联网，一两年内还不具备好的商业模式。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;1612&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0311/144714_pwWM_2720166.png&quot; width=&quot;1002&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;李想还说道，&lt;strong&gt;AGI 早期虽然自己不赚钱，但会破坏传统商业模式赚钱&lt;/strong&gt;。他还提到，新技术往往从通缩开始，AGI 的发展趋势不可阻挡。&lt;/p&gt; 
&lt;p&gt;理想汽车在 2024 年底举行 2024 理想 AI Talk ，李想当时曾就 AI 等话题展开对话。&lt;/p&gt; 
&lt;p&gt;他表示，理想汽车一年一百亿的研发，一半投在了人工智能上。大模型出现以后，人类会发生根本性的改变。互联网让信息平权，人工智能帮助实现认知和知识的平权。&lt;/p&gt; 
&lt;p&gt;李想还谈到了人工智能的三个阶段：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;第一阶段，「增强能力」，主要起辅助作用，决策权在用户。例如 L3 自动驾驶，需要用户监督，并且负责任。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;第二阶段，「我的助手」，助手角色，布置给它任务，它就可以独立完成，并对结果承担责任。例如 L4 自动驾驶，可以让它到学校帮忙接孩子等。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;第三阶段，终极阶段，「硅基家人」，不需要指示、分配任务，它就是我们的家庭成员，甚至是家庭重要的组织者。它不但了解我，它还了解我的孩子，了解我身边的朋友，甚至比我还了解。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;p&gt;相关阅读&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/news/285159&quot; target=&quot;news&quot;&gt;理想汽车多模态认知大模型 Mind GPT 正式上线&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/news/270426&quot; target=&quot;news&quot;&gt;理想汽车全自研多模态认知大模型 —— Mind GPT&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338155</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338155</guid>
            <pubDate>Wed, 05 Mar 2025 06:49:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>iPhone 17 系列机模曝光</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;博主 MajinBuOfficial 根据供应链获得的 CAD 数据，以 3D 打印的方式获得了 iPhone 17 全系列的机模。这批机模包括了 iPhone 17、iPhone 17 Air、iPhone 17 Pro、iPhone 17 Pro Max 四款机型，而外观信息与此前透露的谍照基本一致。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-2fecc3d4611b83757346306026632dfd575.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;除了 iPhone 17 的后置摄像头排列没有太大变化之外，新的 iPhone 17 Air 采用了后置单摄的配置，并将它放置于一个长条形的横向模块当中，整机的厚度也有了明显缩小。&lt;/p&gt; 
&lt;p&gt;Pro 版本的摄像头同样变化较大，采用横向的大尺寸模组，将后置三摄、LiDAR 传感器、闪光灯、后置麦克风等元器件都收纳于其中。至于为什么苹果要预留如此巨大的模组位置，还有待观察。&lt;/p&gt; 
&lt;p&gt;此前，分析师郭明𫓹透露，新的 iPhone 17 Air 或将采用高密度电池，使其可以在紧凑尺寸之中获得更胜于以往的电池续航表现。&lt;/p&gt; 
&lt;p&gt;此外，彭博社记者 Mark Gurman 今日发文&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/news/338104/apple-ios-ipados-macos-design-overhaul&quot;&gt;透露&lt;/a&gt;&lt;/u&gt;，苹果将会统一 iPhone、iPad 以及 Mac 三个设备的系统界面。据 Gurman 预测，本次系统界面统一将涉及图标、菜单、界面窗口样式等内容。值得关注的是，统一后的系统界面设计将会与 visionOS 的风格保持一致，同时简化用户使用的操作步骤、方式。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0311/103415_RHy0_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;据悉，本次改动将会在 iOS19、iPadOS19 和 MacOS16 中出现。报道指出，这也是自 iOS7 后，时隔十年，苹果再一次为 iPhone 进行 UI 大改变；而 Mac 方面，MacOS16 将是自 2020 年 MacOS Big Sur 发布以来，苹果对 MacOS 最大的一次升级。&lt;/p&gt; 
&lt;p&gt;相关阅读&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/news/335621&quot; target=&quot;news&quot;&gt;iPhone 17 系列 CAD 图曝光&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/news/338104/apple-ios-ipados-macos-design-overhaul&quot; target=&quot;news&quot;&gt;苹果计划对 iOS、iPadOS 和 macOS 系统外观进行大幅重新设计&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338152</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338152</guid>
            <pubDate>Wed, 05 Mar 2025 06:40:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>越疆发布 Dobot Atom：全球首款「灵巧操作+直膝行走」具身智能人形机器人</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;深圳越疆科技今日&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.bilibili.com%2Fvideo%2FBV1EkREYbEsg%2F&quot; target=&quot;_blank&quot;&gt;发布&lt;/a&gt;&lt;/u&gt;了全球首款「灵巧操作 + 直膝行走」具身智能人形机器人 Dobot Atom，可实现跨场景、多台协同胜任复杂操作泛化任务。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;1502&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0311/143353_pHFB_2720166.png&quot; width=&quot;2430&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;据介绍，该款机器人是面向工业级精细操作全尺寸仿生人形机器人，搭载自研操作技能模型 ROM-1，具有神经驱动灵巧操作和仿人直膝步态行走两大特征。&lt;/p&gt; 
&lt;p&gt;区别于传统机器人，该产品通过「脑-手协同」技术突破，结合视觉感知与五指灵巧手闭环操作，无需预编程即可自主完成上百种复杂任务。&lt;/p&gt; 
&lt;p&gt;官方介绍称，这台为「打工」而生的工业级操作类人形机器人，身高 1.53 米，体重 62 公斤，采用 1:1 仿人手臂构型设计，全身配置 41 个自由度，搭载重复定位精度 ±0.05mm 的 7 自由度工业级仿生协作臂，适应常见 700-1000mm 工作台高度灵巧作业，并具有工业现场稳定通过能力。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-fbf5c225d2fa360687b55084e48cf17f3d8.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;作为中国首款具备工业级双臂操作与仿人直膝行走能力的人形机器人，其核心搭载越疆自研的神经驱动灵巧操作系统（NDS）与仿人直膝行走系统（AWS），双臂协同灵巧操作，高还原度仿人直膝行走，这种同时拥有上下肢体高水平运动和控制表现的能力，标志着人形机器人操作领域取得重要进展。&lt;/p&gt; 
&lt;p&gt;NDS 系统通过端到端自主推理，赋予机器人 28 个上肢自由度及伺服级抖动抑制能力，可完成工具制造、脆弱物料无损抓取（如车厘子）等复杂任务，操作精度高达 ±0.05mm。&lt;/p&gt; 
&lt;p&gt;AWS 系统则基于类人生物力学与强化学习技术，实现高度拟人的直膝行走和灵活转身，适配短程狭小空间作业需求。其自适应泛化能力尤为突出，仅需 2 小时采集少量数据即可掌握新技能，显著提升非结构化场景下的操作效率。&lt;/p&gt; 
&lt;p&gt;据官方介绍，这台人形机器人的核心零部件、软硬件系统采用了自主研发的工业级方案，主要面向数以千计用工的车厂组装备料环节、咖啡店制饮多台设备的流程操作、连锁药店夜间取药等场景，即设备位置不固定、产品多规格、操作相似度高，并有短程狭小空间通过、灵活转身操作需求的工业商业连续重复工作场景。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338149</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338149</guid>
            <pubDate>Wed, 05 Mar 2025 06:35:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>马斯克尝试用 AI 取代美国公务员</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.theatlantic.com%2Ftechnology%2Farchive%2F2025%2F03%2Fgsa-chat-doge-ai%2F681987%2F&quot; target=&quot;_blank&quot;&gt;大西洋月刊报道称&lt;/a&gt;&lt;/u&gt;，马斯克领导的政府效率部（DOGE）正在努力缩减和重组美国公务员队伍，这一努力已进入新阶段。其理念很简单：&lt;strong&gt;利用生成式人工智能来自动化以前由人完成的工作&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0311/142816_kg1u_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;美国政府正在与美国总务管理局（GSA）的 1,500 名联邦雇员一起测试一款新型聊天机器人 「GSA Chat」&amp;nbsp;，并可能最早于本周五向整个机构发布，这意味着超过 10,000 名负责超过 1,000 亿美元合同和服务的工作人员可以使用这款机器人。&lt;/p&gt; 
&lt;p&gt;这款聊天机器人被 GSA 领导层视为提升联邦工作人员生产力的工具，是政府效率部及其盟友更大行动方案的一部分。谈到 GSA 的整体计划时，最近被任命为 GSA 信息技术部门 —— 技术转型服务局局长的前特斯拉工程师托马斯・谢德上个月在全体员工会议上表示，&lt;strong&gt;该机构正在推进「人工智能优先战略」&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;谢德称，「正如大家所知，随着我们缩减联邦政府的整体规模，仍有大量项目需要保留，这为技术和自动化全面发挥作用提供了巨大机遇」。他提出，可以在政府范围内提供「编码代理」—— 指的是能够代替人类编写并可能部署代码的 AI 程序。此外，谢德还表示，AI 可以「对合同进行分析」，软件可用于「自动化」GSA 的「财务职能」。&lt;/p&gt; 
&lt;p&gt;目前，在工作中使用人工智能很常见，GSA 的聊天机器人可能不会对政府运作产生巨大影响。但这只是政府效率部继续大幅削减公务员体系的一个小例子。据报道，在教育部，政府效率部顾问将有关机构支出的敏感数据输入 AI 程序，以确定削减开支的方向。据说政府效率部打算利用 AI 来帮助决定政府各部门的员工是否应保住工作。&lt;/p&gt; 
&lt;p&gt;在上周晚些时候的另一场 TTS 会议上，谢德表示，他预计该部门在几周内规模将「至少缩小 50%」。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338147/musk-replacing-workers-with-ai</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338147/musk-replacing-workers-with-ai</guid>
            <pubDate>Wed, 05 Mar 2025 06:28:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>RISC-V 工委会：征集《RISC-V 指令集架构矩阵扩展（ME）指令集》等三项团体标准参编单位</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;中国电子工业标准化技术协会 RISC-V 工委会发布「关于公开征集《RISC-V 指令集架构矩阵扩展（ME）指令集》等三项团体标准参编单位的通知」。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;通知内容指出，由阿里巴巴达摩院（杭州）科技发起制定的《RISC-V 指令集架构矩阵扩展（ME）指令集》、进迭时空（杭州）科技发起制定的《开放精简指令集（RISC-V）配置文件》，以及由芯升科技发起制定的《RISC-V 指令集架构无线矢量扩展（Zvw）指令集》三项团体标准已获批立项。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;为更高质量地完成标准编制工作，保障标准的广泛性、科学性和实用性，现向全行业及 RISC-V 工委会成员征集上述三项团体标准参编单位，共同完成标准的制定工作。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;383&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0311/142732_nWVQ_4252687.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;&lt;strong&gt;报名条件&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;1、参编单位具备相关领域工作基础，具有较高的社会影响力，重视标准化工作，能够提供技术专家作为参编人员参与标准编制。&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;2、参编单位应指定固定的起草人员，能够确保参与标准制修订过程中的各项会议，按时完成标准编制组分配的工作任务。&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;3、起草人应具备相应的专业知识、经验和能力，了解产业现状和技术发展水平。同时较熟悉标准化工作流程，具有标准制修订相关工作经验的优先。起草人应及时对标准提出建设性意见建议。&lt;/span&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;4、在可以公开的前提下，参编单位向标准编制工作组提供相关研究成果、经典案例和数据，供编制标准组参考。&lt;/span&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;更多详情可&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FjAQEyU7kzEWgp9HefHji_g&quot; target=&quot;_blank&quot;&gt;查看官方公告&lt;/a&gt;。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/338146</link>
            <guid isPermaLink="false">https://www.oschina.net/news/338146</guid>
            <pubDate>Wed, 05 Mar 2025 06:28:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>Composio —— 适用于 AI 代理的生产就绪工具集</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                                                                        
                                                                                    &lt;p style=&quot;color:#1f2328; text-align:start&quot;&gt;&lt;strong&gt;Composio 为 AI 代理提供可用于生产的工具集&lt;/strong&gt;，提供：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;支持多个类别的 250 多种工具：
&lt;ul&gt;
&lt;li&gt;GitHub、Notion、Linear、Gmail、Slack、Hubspot、Salesforce 等软件工具 &amp;amp;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;a href=&quot;https://app.composio.dev/apps&quot;&gt;更多&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;操作系统操作, 包括文件工具、shell 工具、代码分析工具 &amp;amp;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;a href=&quot;https://app.composio.dev/apps&quot;&gt;更多&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;通过 Google、Perplexity、Tavily 和 Exa 实现搜索功能 &amp;amp;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;a href=&quot;https://app.composio.dev/apps&quot;&gt;更多&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;全面的框架支持，包括 OpenAI、 Groq、Claude、LlamaIndex、Langchain、CrewAI、Autogen、Gemini 以及&lt;a href=&quot;https://docs.composio.dev/framework&quot;&gt;更多&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;支持多种协议 (OAuth、API 密钥、Basic JWT) 的托管身份验证&lt;/li&gt;
&lt;li&gt;通过优化设计将工具调用准确率提高高达 40%&lt;/li&gt;
&lt;li&gt;用于后端集成的白标解决方案&lt;/li&gt;
&lt;li&gt;支持自定义工具和扩展的可插拔架构&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;500&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0310/163052_S7VK_4252687.gif&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

                                                                    &lt;/div&gt;
                                                                </description>
            <link>https://www.oschina.net/p/composio</link>
            <guid isPermaLink="false">https://www.oschina.net/p/composio</guid>
            <pubDate>Wed, 05 Mar 2025 06:15:00 GMT</pubDate>
        </item>
    </channel>
</rss>