<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - industry - 简体中文</title>
    <link>https://www.oschina.net/news/industry</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news/industry" rel="self" type="application/rss+xml"/>
    <description>已对该 RSS 进行格式化操作：中英字符之间插入空格、使用直角引号、标点符号修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 29 Jul 2025 07:45:01 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>蚂蚁 inclusionAI 团队发布 Ming-lite-omni v1.5</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;蚂蚁集团 inclusionAI 团队发布了全面升级版的全模态模型 &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Finclusionai.github.io%2Fzh%2Fblog%2Fming-lite-omni-1_5%2F" target="_blank"&gt;&lt;strong&gt;Ming-Lite-Omni v1.5&lt;/strong&gt;&lt;/a&gt;，基于 &lt;strong&gt;Ling-lite-1.5&lt;/strong&gt; 构建，总参数量为 &lt;strong&gt;203 亿&lt;/strong&gt;（其中 MoE 部分活跃参数为 &lt;strong&gt;30 亿&lt;/strong&gt;），在图像-文本理解、文档理解、视频理解、语音理解与合成、图像生成与编辑等全模态能力上显著提升。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-95012b461af8180f5480bac2f4c85b95949.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Ming-lite-omni v1.5 模型架构如下，主题参考了 Ming-lite-omni v1 版本的结构，区别在于为了增强图像编辑人物和场景一致性，升级 Vision head 支持参考图特征输入。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-6c48bdf68ea2bcdfc0e8deb440f605297fb.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;关键优化&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;增强视频理解&lt;/strong&gt;：通过 &lt;strong&gt;MRoPE 3D 时空编码&lt;/strong&gt; 和针对长视频的 &lt;strong&gt;课程学习策略&lt;/strong&gt;，显著提升对复杂视觉序列的理解能力 。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;优化多模态生成&lt;/strong&gt;：采用双分支图像生成（ID 与场景一致性损失）和新的音频解码器及 BPE 编码，提升生成一致性与感知控制，实现高质量实时语音合成。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;数据全面升级&lt;/strong&gt;：新增结构化文本数据、高质量产品信息及包括方言（如普通话、粤语、四川话等）在内的精细化视觉与语音感知数据。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;性能表现&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;在 &lt;strong&gt;MMVet&lt;/strong&gt;、&lt;strong&gt;MathVista&lt;/strong&gt;、&lt;strong&gt;OCRBench&lt;/strong&gt; 等数据集上表现突出，文档理解任务（如 &lt;strong&gt;ChartQA&lt;/strong&gt;、&lt;strong&gt;OCRBench&lt;/strong&gt;）取得 10B 以下参数模型中的 &lt;strong&gt;SOTA&lt;/strong&gt; 成绩。&lt;/li&gt; 
 &lt;li&gt;视频理解、语音理解与生成（支持多种方言）及图像生成（保持人物 ID 一致性编辑）均处于行业领先地位。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;该模型已在 &lt;strong&gt;Hugging Face&lt;/strong&gt; 和 &lt;strong&gt;ModelScope&lt;/strong&gt; 上开放下载，并提供详细安装指南、代码示例和 &lt;strong&gt;Gradio&lt;/strong&gt; 演示。&lt;/p&gt; 
&lt;p&gt;Hugging Face: https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5&lt;br&gt; ModelScope: https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362971</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362971</guid>
      <pubDate>Tue, 29 Jul 2025 07:39:59 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>eBPF 助力 NAS 分钟级别 Pod 实例溯源</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;span id="OSC_h1_1"&gt;&lt;/span&gt; 
&lt;h1&gt;一、背景&lt;/h1&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;云存储 NAS 产品是一个可共享访问、弹性扩展、高可靠、高性能的分布式文件系统。 NAS 兼容了 POSIX 文件接口，可支持数千台计算节点共享访问，可挂载到弹性计算 ECS、容器实例等计算业务上，提供高性能的共享存储服务。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;鉴于多主机间共享的便利性和高性能， NAS 在得物的算法训练、应用构建等场景中均成为了基础支撑。&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img src="https://static001.geekbang.org/infoq/36/36274710c18533ce5fc246ee82e640c2.jpeg" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;在多业务共享的场景中，单个业务流量异常容易引发全局故障。目前，异常发生后需依赖&lt;strong&gt;云服务厂商 NAS &lt;/strong&gt;的溯源能力，&lt;strong&gt;但只能定位到主机级别，无法识别具体异常服务&lt;/strong&gt;。要定位到服务级别，仍需依赖所有使用方协同排查，并由 SRE 多轮统计分析，&lt;strong&gt;效率低下&lt;/strong&gt;&lt;/span&gt;&lt;span style="color:#6a6a6a"&gt;（若服务实例发生迁移或重建，排查难度进一步增加）&lt;/span&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;&lt;strong&gt;为避免因 NAS 异常或带宽占满导致模型训练任务受阻&lt;/strong&gt;，因此需构建支持服务级流量监控、快速溯源及 NAS 异常实时感知的能力，以提升问题定位效率并减少业务中断。&lt;/span&gt;&lt;/p&gt; 
&lt;span id="OSC_h1_2"&gt;&lt;/span&gt; 
&lt;h1&gt;二、流量溯源方案调研和验证&lt;/h1&gt; 
&lt;span id="OSC_h2_3"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span style="color:#000000"&gt;NAS 工作原理&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;NAS 本地挂载原理&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;在 Linux 平台上，NAS 的产品底层是基于标准网络文件系统 NFS（Network File System），通过将远端文件系统挂载到本地，实现用户对远端文件的透明访问。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;NFS 协议（主要支持 NFS v3 和 v4，通常以 v3 为主）允许将远端服务挂载到本地，使用户能够像访问本地文件目录一样操作远端文件。文件访问请求通过 RPC 协议发送到远端进行处理，其整体流程如下：&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img height="465" src="https://oscimg.oschina.net/oscnet/up-3f3abf4fce2a08639688cf370284cf62cdd.png" width="620" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#6a6a6a"&gt;文件系统访问时的数据流向示意&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img height="552" src="https://oscimg.oschina.net/oscnet/up-a5b7a533fbca51c726053b4598e79c9786f.jpg" width="507" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#6a6a6a"&gt;Linux 内核中 NFS 文件系统&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;NFS 文件系统读/写流程&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;在 Linux NFS 文件系统的实现中，文件操作接口由 nfs_file_operations 结构体定义，其读取操作对应的函数为:&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;//NFS 文件系统的 VFS 层实现的函数如下所示：
const&amp;nbsp;struct&amp;nbsp;file_operations nfs_file_operations = {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; .llseek &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; = nfs_file_llseek,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; .read_iter &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;= nfs_file_read,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; .write_iter &amp;nbsp; &amp;nbsp; &amp;nbsp; = nfs_file_write,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;// ...
};&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;针对 NFS 文件系统的读操作涉及到 2 个阶段（写流程类似，只是函数名字有所差异，本文仅以读取为例介绍）。由于文件读取涉及到网络操作因此这两个阶段涉及为异步操作：&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;&lt;u&gt;※ 两个阶段&lt;/u&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;读取请求阶段：&lt;/strong&gt;当应用程序针对 NFS 文件系统发起 read() 读操作时，内核会在 VFS 层调用 nfs_file_read 函数，然后调用 NFS 层的 nfs_initiate_read 函数，通过 RPC 的 rpc_task_begin 函数将读请求发送到 NFS Server，至此向 NFS Server 发起的请求工作完成。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;读响应阶段：&lt;/strong&gt;在 NFS Server 返回消息后，会调用 rpc_task_end 和 nfs_page_read_done 等函数，将数据返回到用户空间的应用程序。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img height="415" src="https://oscimg.oschina.net/oscnet/up-fd2c800299c65096f8d6eba7de108c0581f.jpg" width="1280" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;在了解 NFS 文件系统的读流程后，我们回顾一下 NFS Server 为什么无法区分单机访问的容器实例或进程实例。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;这是因为 NFS 文件系统的读写操作是在内核空间实现的。当容器 A/B 和主机上的进程 C 发起读请求时，这些请求在进入内核空间后，统一使用主机 IP（如 192.168.1.2）作为客户端 IP 地址。因此，NFS Server 端的统计信息只能定位到主机维度，无法进一步区分主机内具体的容器或进程。&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img height="516" src="https://oscimg.oschina.net/oscnet/up-91366119d285327518f226735b34227dddd.jpg" width="1080" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#6a6a6a"&gt;内核空间实现示意&lt;/span&gt;&lt;/p&gt; 
&lt;span id="OSC_h2_4"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span style="color:#000000"&gt;方案调研和验证&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;进程对应容器上下文信息关联&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;内核中进程以 PID 作为唯一编号，与此同时，内核会建立一个 struct task_struct 对象与之关联，在 struct task_struct 结构会保存进程对应的上下文信息。如实现 PID 信息与用户空间容器上下文的对应（进程 PID 1000 的进程属于哪个 Pod 哪个 Container 容器实例），我们需基于内核 task_struct 结构获取到容器相关的信息。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;通过分析内核代码和资料确认，发现可以通过 task_struct 结构中对应的 cgroup 信息获取到进程对应的 cgroup_name 的信息，而该信息中包含了容器 ID 信息，例如&lt;strong&gt; docker-2b3b0ba12e92...983.scope &lt;/strong&gt;，完整路径较长，使用 .... 省略。基于容器 ID 信息，我们可进一步管理到进程所归属的 Pod 信息，如 Pod NameSpace 、 Pod Name 、 Container Name 等元信息，最终完成进程 PID 与容器上下文信息元数据关联。&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;struct&amp;nbsp;task_struct&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;struct&amp;nbsp;css_set&amp;nbsp;__rcu &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;*cgroups;
}


struct&amp;nbsp;css_set&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;struct&amp;nbsp;cgroup_subsys_state&amp;nbsp;*subsys[CGROUP_SUBSYS_COUNT];
}


struct&amp;nbsp;cgroup_subsys_state&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;struct&amp;nbsp;cgroup&amp;nbsp;*cgroup;
}


struct&amp;nbsp;cgroup&amp;nbsp;{
&amp;nbsp;&amp;nbsp;struct&amp;nbsp;kernfs_node&amp;nbsp;*kn; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;/* cgroup kernfs entry */
}


struct&amp;nbsp;kernfs_node&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;const&amp;nbsp;char&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; *name; &amp;nbsp;// docker-2b3b0ba12e92...983.scope
}&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;以某容器进程为例，该进程在 Docker 容器环境中的 cgroup 路径完整为 /sys/fs/cgroup/cpu/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podefeb3229_4ecb_413a_8715_5300a427db26.slice/docker-2b3b0ba12e925820ac8545f67c8cadee864e5b4033b3d5004d8a3aa742cde2ca.scope 。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;经验证，我们在内核中读取 task-&amp;gt;cgroups-&amp;gt;subsys[0]-&amp;gt;kn-&amp;gt;name 的值为 docker-2b3b0ba12e925820ac8545f67c8cadee864e5b4033b3d5004d8a3aa742cde2ca.scope 。&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img src="https://static001.geekbang.org/infoq/92/92735ea140e6e0021584e2c7cc21b0b4.png" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;其中容器 ID 字段为 docker- 与 .scope 间的字段信息，在 Docker 环境中一般取前 12 个字符作为短 ID，如 2b3b0ba12e92 ，可通过 docker 命令进行验证，结果如下：&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;docker&amp;nbsp;ps -a|grep&amp;nbsp;2b3b0ba
2b3b0ba12e92&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; registry-cn-hangzhou-vpc.ack.aliyuncs.com/acs/pause:3.5&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;NAS 上下文信息关联&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;NAS 产品的访问通过挂载命令完成本地文件路径的挂载。我们可以通过 mount 命令将 NAS 手工挂载到本地文件系统中。&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;mount&amp;nbsp;-t nfs -o vers=3,nolock,proto=tcp,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport \
&amp;nbsp;&amp;nbsp;3f0f3489aa-xxxx.cn-shanghai.nas.aliyuncs.com:/test /mnt/nas&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;执行上述挂载命令成功后，通过 mount 命令则可查询到类似的挂载记录：&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;5368 47 0:660 / /mnt/nas rw,relatime shared:1175 \
&amp;nbsp; &amp;nbsp; &amp;nbsp;- nfs 3f0f3489aa-xxxx.cn-shanghai.nas.aliyuncs.com:/test \ &amp;nbsp; &amp;nbsp;
&amp;nbsp; &amp;nbsp; &amp;nbsp;rw,vers=3,rsize=1048576,wsize=1048576,namlen=255,hard,nolock,\
&amp;nbsp; &amp;nbsp; &amp;nbsp;noresvport,proto=tcp,timeo=600,retrans=2,sec=sys, \
&amp;nbsp; &amp;nbsp; &amp;nbsp;mountaddr=192.168.0.91,mountvers=3,mountport=2049,mountproto=tcp,\
&amp;nbsp; &amp;nbsp; &amp;nbsp;local_lock=all,addr=192.168.0.92&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;核心信息分析如下：&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;# 挂载点，父挂载点，挂载设备号 &amp;nbsp; 目录 &amp;nbsp; &amp;nbsp; 挂载到本机目录 &amp;nbsp;协议 &amp;nbsp; NAS 地址
5368&amp;nbsp; &amp;nbsp; &amp;nbsp;47&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;0:660&amp;nbsp; &amp;nbsp; &amp;nbsp;/ &amp;nbsp; &amp;nbsp; &amp;nbsp; /mnt/nas &amp;nbsp; &amp;nbsp; nfs &amp;nbsp; &amp;nbsp;3f0f3489aa-xxxx.cn-shanghai.nas.aliyuncs.com:/test
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;maror:minor&amp;nbsp;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;挂载记录中的&lt;/span&gt;&lt;span style="color:#d92142"&gt;&lt;strong&gt; 0:660 &lt;/strong&gt;&lt;/span&gt;为本地设备编号，格式为 major:minor ， 0 为 major 编号， 660 为 minor 编号，系统主要以 minor 为主。在系统的 NFS 跟踪点 nfs_initiate_read 的信息中的 dev 字段则为在挂载记录中的 minor 编号。&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;cat /sys/kernel/debug/tracing/events/nfs/nfs_initiate_read/format
format:
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; field:dev_t dev; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;offset:8; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; size:4; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;signed:0;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;...
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; field:u32 count; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;offset:32; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;size:4; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;signed:0;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;通过用户空间 mount 信息和跟踪点中 dev_id 信息，则可实现内核空间设备编号与 NAS 详情的关联。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;内核空间信息获取&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;如容器中进程针对挂载到本地的目录 /mnt/nas 下的文件读取时，会调用到 nfs_file_read() 和 nfs_initiate_read 函数。通过 nfs_initiate_read 跟踪点我们可以实现进程容器信息和访问 NFS 服务器的信息关联。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;通过编写 eBPF 程序针对跟踪点 tracepoint/nfs/nfs_initiate_read 触发事件进行数据获取，我们可获取到访问进程所对应的 cgroup_name 信息和访问 NFS Server 在本机的设备 dev_id 编号。&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img height="673" src="https://oscimg.oschina.net/oscnet/up-b7e2eac3eeba85b51ed94f7a84d28a096ea.jpg" width="1280" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#6a6a6a"&gt;获取 cgroup_name 信息&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;进程容器上下文获取：&lt;/strong&gt; 通过 cgroup_name 信息，如样例中的 docker-2b3b0ba12e92...983.scope ，后续可以基于 container_id 查询到容器对应的 Pod NameSpace 、 Pod Name 和 Container Name 等信息，从而定位到访问进程关联的 Pod 信息。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;NAS 上下文信息获取：&lt;/strong&gt; 通过 dev 信息，样例中的 660 ，通过挂载到本地的记录，可以通过 660 查询到对应的 NAS 产品的地址，比如 3f0f3489aa-xxxx.cn-shanghai.nas.aliyuncs.com 。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;用户空间元信息缓存&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img src="https://static001.geekbang.org/infoq/aa/aa252b079e6ce3cb1e52e02ab5b4052a.png" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;在用户空间中，可以通过解析挂载记录来获取 DEV 信息，并将其与 NAS 信息关联，从而建立以 DevID 为索引的查询缓存。如此，后续便可以基于内核获取到 dev_id 进行关联，进一步补全 NAS 地址及相关详细信息。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;对于本地容器上下文的信息获取，最直接的方式是通过 K8s kube-apiserver 通过 list-watch 方法进行访问。然而，这种方式会在每个节点上启动一个客户端与 kube-apiserver 通信，显著增加 K8s 管控面的负担。因此，我们选择通过本地容器引擎进行访问，直接在本地获取主机的容器详情。通过解析容器注解中的 Pod 信息，可以建立容器实例缓存。后续在处理指标数据时，则可以通过 container-id 实现信息的关联与补全。&lt;/span&gt;&lt;/p&gt; 
&lt;span id="OSC_h1_5"&gt;&lt;/span&gt; 
&lt;h1&gt;三、架构设计和实现&lt;/h1&gt; 
&lt;span id="OSC_h2_6"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span style="color:#000000"&gt;整体架构设计&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;内核空间的信息采集采用 Linux eBPF 技术实现，这是一种安全且高效的内核数据采集方式。简单来说，eBPF 的原理是在内核中基于事件运行用户自定义程序，并通过内置的 map 和 perf 等机制实现用户空间与内核空间之间的双向数据交换。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;在 NFS 和 RPC 调用事件触发的基础上，可以通过编写内核空间的 eBPF 程序来获取必要的原始信息。当用户空间程序搜集到内核指标数据后，会对这些原始信息进行二次处理，并在用户空间的采集程序中补充容器进程信息（如 NameSpace、Pod 和 Container 名称）以及 NFS 地址信息（包括 NFS 远端地址）。&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img src="https://static001.geekbang.org/infoq/c6/c687b3f4df8a2ce5d0ab5cd9f287dfd4.png" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;span id="OSC_h2_7"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span style="color:#000000"&gt;内核 eBPF 程序流程&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;以 NFS 文件读为例，通过编写 eBPF 程序跟踪 nfs_initiate_read / rpc_task_begin / rpc_task_end / nfs_page_read_done 等关键链路上的函数，用于获取到 NFS 读取的数据量和延时数据，并将访问链路中的进程上下文等信息保存到内核中的指标缓存中。&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img height="471" src="https://oscimg.oschina.net/oscnet/up-f854a1a8cdf429eff044e715772dc96dfb6.jpg" width="1280" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;如上图所示， nfs_initate_read 和 rpc_task_begin 发生在同一进程上下文中，而 rpc_task_begin 与 rpc_task_end 是异步操作，尽管两者不处于同一进程上下文，但可以通过 task_id 进行关联。同时， page_read_done 和 rpc_task_end 则发生在同一进程上下文中。&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img height="627" src="https://oscimg.oschina.net/oscnet/up-70f989400d6710f021cfa7577375a709030.jpg" width="1280" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;nfs_initiate_read 函数调用触发的 eBPF 代码示例如下所示：&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;

SEC("tracepoint/nfs/nfs_initiate_read")
int&amp;nbsp;tp_nfs_init_read(struct&amp;nbsp;trace_event_raw_nfs_initiate_read *ctx)
&amp;nbsp; &amp;nbsp;&amp;nbsp;// 步骤 1 获取到 nfs 访问的设备号信息，比如 3f0f3489aa-xxxx.cn-shanghai.nas.aliyuncs.com
&amp;nbsp; &amp;nbsp;&amp;nbsp;// dev_id 则为： 660&amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;dev_t&amp;nbsp;dev_id =&amp;nbsp;BPF_CORE_READ(ctx, dev);
&amp;nbsp; &amp;nbsp; u64 file_id =&amp;nbsp;BPF_CORE_READ(ctx, fileid);
&amp;nbsp; &amp;nbsp; u32 count =&amp;nbsp;BPF_CORE_READ(ctx, count);
&amp;nbsp; &amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;struct&amp;nbsp;task_struct&amp;nbsp;*task = (struct&amp;nbsp;task_struct *)bpf_get_current_task();


&amp;nbsp; &amp;nbsp;&amp;nbsp;// 步骤 2 获取进程上下文所在的容器 cgroup_name 信息
&amp;nbsp; &amp;nbsp;&amp;nbsp;// docker-2b3b0ba12e925820ac8545f67c8cadee864e5b4033b3d5004d8a3aa742cde2ca.scope
&amp;nbsp; &amp;nbsp;&amp;nbsp;const&amp;nbsp;char&amp;nbsp;*cname =&amp;nbsp;BPF_CORE_READ(task, cgroups, subsys[0], cgroup, kn, name);
&amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(cname)
&amp;nbsp; &amp;nbsp; {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;bpf_core_read_str(&amp;amp;info.container, MAX_PATH_LEN, cname);
&amp;nbsp; &amp;nbsp; }


&amp;nbsp; &amp;nbsp;&amp;nbsp;bpf_map_update_elem(&amp;amp;link_begin, &amp;amp;tid, &amp;amp;info, BPF_ANY);
}


SEC("tracepoint/nfs/nfs_readpage_done")
int&amp;nbsp;tp_nfs_read_done(struct&amp;nbsp;trace_event_raw_nfs_readpage_done *ctx)
{
&amp;nbsp; &amp;nbsp;//... 省略
}


SEC("tracepoint/sunrpc/rpc_task_begin")
int&amp;nbsp;tp_rpc_task_begin(struct&amp;nbsp;trace_event_raw_rpc_task_running *ctx)
{
&amp;nbsp; &amp;nbsp;&amp;nbsp;//... 省略
}


SEC("tracepoint/sunrpc/rpc_task_end")
int&amp;nbsp;tp_rpc_task_done(struct&amp;nbsp;trace_event_raw_rpc_task_running *ctx)
{
&amp;nbsp; &amp;nbsp;//... 省略
}&lt;/code&gt;&lt;/pre&gt; 
&lt;span id="OSC_h2_8"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span style="color:#000000"&gt;用户空间程序架构&lt;/span&gt;&lt;/h2&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img height="606" src="https://oscimg.oschina.net/oscnet/up-b9cf6d19b68dcceaa9f6f459645264baaa8.jpg" width="1280" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;元数据缓存&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;&lt;u&gt;※ NAS 挂载信息缓存&lt;/u&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;通过解析挂载记录，可以获取 DEV 信息与 NAS 信息的关联关系。以下是实现该功能的关键代码详情：&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;scanner := bufio.NewScanner(mountInfoFile)
count :=&amp;nbsp;0
for&amp;nbsp;scanner.Scan() {
&amp;nbsp; &amp;nbsp; line := scanner.Text()
&amp;nbsp; &amp;nbsp; devID,remoteDir, localDir, NASAddr = parseMountInfo(line)


&amp;nbsp; &amp;nbsp; mountInfo := MountInfo{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;DevID: &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; devID,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;RemoteDir: &amp;nbsp; &amp;nbsp; remoteDir,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;LocalMountDir: localDir,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;NASAddr： NASAddr,
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; mountInfos =&amp;nbsp;append(mountInfos, mountInfo)

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;&lt;u&gt;※ 容器元信息缓存&lt;/u&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;通过 Docker 或 Containerd 客户端，从本地读取单机的容器实例信息，并将容器的上下文数据保存到本地缓存中，以便后续查询使用。&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;podInfo := PodInfo{
&amp;nbsp; &amp;nbsp; NameSpace: &amp;nbsp; &amp;nbsp; labels["io.kubernetes.pod.namespace"],
&amp;nbsp; &amp;nbsp; PodName: &amp;nbsp; &amp;nbsp; &amp;nbsp; labels["io.kubernetes.pod.name"],
&amp;nbsp; &amp;nbsp; ContainerName: labels["io.kubernetes.container.name"],
&amp;nbsp; &amp;nbsp; UID: &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; labels["io.kubernetes.pod.uid"],
&amp;nbsp; &amp;nbsp; ContainerID: &amp;nbsp; conShortID,
}&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;数据处置流程&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;用户空间程序的主要任务是持续读取内核 eBPF 程序生成的指标数据，并对读取到的原始数据进行处理，提取访问设备的 dev_id 和 container_id 。随后，通过查询已建立的元数据缓存，分别获取 NAS 信息和容器 Pod 的上下文数据。最终，经过数据合并与处理，生成指标数据缓存供后续使用。&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;func&amp;nbsp;(m *BPFEventMgr)&amp;nbsp;ProcessIOMetric() {
&amp;nbsp; &amp;nbsp;&amp;nbsp;// ...
&amp;nbsp; &amp;nbsp; events := m.ioMetricMap
&amp;nbsp; &amp;nbsp; iter := events.Iterate()


&amp;nbsp; &amp;nbsp;&amp;nbsp;for&amp;nbsp;iter.Next(&amp;amp;nextKey, &amp;amp;event) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;// ① 读取到的 dev_id 转化为对应的完整 NAS 信息
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;devId := nextKey.DevId
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;mountInfo, ok := m.mountMgr.Find(int(devId))


&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;// ② 读取 containerID 格式化并查询对应的 Pod 上下文信息
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;containerId := getContainerID(nextKey.Container)
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;podInfo, ok = m.criMgr.Find(containerId)
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;// ③ 基于事件信息、NAS 挂载信息和 Pod 上下文信息，生成指标数据缓存&amp;nbsp;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;metricKey, metricValue := formatMetricData(nextKey， mountInfo, podInfo)
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;value, loaded := metricCache.LoadOrStore(metricKey, metricValue)
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp;&amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;// ④ 指标数据缓存，生成最终的 Metrics 指标并更新&amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;var&amp;nbsp;ioMetrics []metric.Counter
&amp;nbsp; &amp;nbsp; metricCache.Range(func(key, value&amp;nbsp;interface{})&amp;nbsp;bool&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;k := key.(metric.IOKey)
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;v := value.(metric.IOValue)


&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;ioMetrics =&amp;nbsp;append(ioMetrics, metric.Counter{"read_count",&amp;nbsp;float64(v.ReadCount),
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;[]string{k.NfsServer, v.NameSpace, v.Pod, v.Container})
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;// ...
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;}
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;return&amp;nbsp;true
&amp;nbsp; &amp;nbsp; })
&amp;nbsp; &amp;nbsp;&amp;nbsp;
&amp;nbsp; &amp;nbsp; m.metricMgr.UpdateIOStat(ioMetrics)
}&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;启动 Goroutine 处理指标数据：通过启动一个 Goroutine，循环读取内核存储的指标数据，并对数据进行处理和信息补齐，最终生成符合导出格式的 Metrics 指标。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;&lt;u&gt;※ 具体步骤&lt;/u&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;获取 NAS 信息：&lt;/strong&gt;从读取的原始数据中提取 dev_id ，并通过 dev_id 查询挂载的 NAS 信息，例如远端访问地址等相关数据。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;查询 Pod 上下文：&lt;/strong&gt;对 containerID 进行格式化处理，并查询对应的容器 Pod 上下文信息。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;生成指标数据缓存：&lt;/strong&gt;基于事件数据、NAS 挂载信息和 Pod 上下文信息，生成指标数据缓存。此过程主要包括对相同容器上下文的数据进行合并和累加。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;导出 Metrics 指标：&lt;/strong&gt;根据指标数据缓存，生成最终的 Metrics 指标，并更新到指标管理器。随后，通过自定义的 Collector 接口对外导出数据。当 Prometheus 拉取数据时，指标会被转换为最终的 Metrics 格式。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;通过上述步骤，用户空间能够高效地处理内核 eBPF 程序生成的原始数据，并结合 NAS 挂载信息和容器上下文信息，生成符合 Prometheus 标准的 Metrics 指标，为后续的监控和分析提供了可靠的数据基础。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;自定义指标导出器&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;在导出指标的场景中，我们需要基于保存在 Go 语言中的 map 结构中的动态数据实时生成，因此需要实现自定义的 Collector 接口。自定义 Collector 接口需要实现元数据描述函数 Describe() 和指标搜集的函数 Collect() ，其中 Collect() 函数可以并发拉取，因此需要通过加锁实现线程安全。该接口需要实现以下两个核心函数：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Describe() ：用于定义指标的元数据描述，向 Prometheus 注册指标的基本信息。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Collect() ：用于搜集指标数据，该函数支持并发拉取，因此需要通过加锁机制确保线程安全。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;type&amp;nbsp;Collector&amp;nbsp;interface&amp;nbsp;{
&amp;nbsp; &amp;nbsp;&amp;nbsp;// 指标的定义描述符
&amp;nbsp; &amp;nbsp; Describe(chan&amp;lt;- *Desc)
&amp;nbsp; &amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;// 并将收集的数据传递到 Channel 中返回
&amp;nbsp; &amp;nbsp; Collect(chan&amp;lt;- Metric)
}&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;我们在指标管理器中实现 Collector 接口， 部分实现代码，如下所示：&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;nfsIOMetric := prometheus.NewDesc(
&amp;nbsp; &amp;nbsp; prometheus.BuildFQName(prometheusNamespace,&amp;nbsp;"",&amp;nbsp;"io_metric"),
&amp;nbsp; &amp;nbsp;&amp;nbsp;"nfs io metrics by cgroup",
&amp;nbsp; &amp;nbsp; []string{"nfs_server",&amp;nbsp;"ns",&amp;nbsp;"pod",&amp;nbsp;"container",&amp;nbsp;"op",&amp;nbsp;"type"},
&amp;nbsp; &amp;nbsp;&amp;nbsp;nil,
)


// Describe and Collect implement prometheus collect interface
func&amp;nbsp;(m *MetricMgr)&amp;nbsp;Describe(ch&amp;nbsp;chan&amp;lt;- *prometheus.Desc) {
&amp;nbsp; &amp;nbsp; ch &amp;lt;- m.nfsIOMetric
}


func&amp;nbsp;(m *MetricMgr)&amp;nbsp;Collect(ch&amp;nbsp;chan&amp;lt;- prometheus.Metric) {
&amp;nbsp; &amp;nbsp;&amp;nbsp;// Note：加锁保障线程并发安全
&amp;nbsp; &amp;nbsp; m.activeMutex.Lock()
&amp;nbsp; &amp;nbsp;&amp;nbsp;defer&amp;nbsp;m.activeMutex.Unlock()
&amp;nbsp; &amp;nbsp;&amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;for&amp;nbsp;_, v :=&amp;nbsp;range&amp;nbsp;m.ioMetricCounters {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;ch &amp;lt;- prometheus.MustNewConstMetric(m.nfsIOMetric, prometheus.GaugeValue, v.Count, v.Labels...)
&amp;nbsp; &amp;nbsp; }

&lt;/code&gt;&lt;/pre&gt; 
&lt;span id="OSC_h1_9"&gt;&lt;/span&gt; 
&lt;h1&gt;四、总结&lt;/h1&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;当前 NAS 溯源能力已正式上线，以下是主要功能和视图介绍：&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;&lt;u&gt;※ 单 NAS 实例整体趋势&lt;/u&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;支持基于环境和 NAS 访问地址过滤，展示 NAS 产品的读写 IOPS 和吞吐趋势图。同时，基于内核空间统计的延时数据，提供 P95 读写延时指标，用于判断读写延时情况，辅助问题分析和定位。&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img src="https://static001.geekbang.org/infoq/14/14577d6fe8b2876ca7f5138680fc3667.png" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img src="https://static001.geekbang.org/infoq/9c/9c091aeaecc284956b851416de5d313a.png" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;在 NAS 流量溯源方面，我们结合业务场景设计了基于任务和 Pod 实例维度的流量分析视图：&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;&lt;u&gt;※ 任务维度流量溯源&lt;/u&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;通过聚合具有共同属性的一组 Pod 实例，展示任务级别的整体流量情况。该视图支持快速定位任务级别的流量分布，帮助用户进行流量溯源和多任务错峰使用的依据。&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img src="https://static001.geekbang.org/infoq/0a/0a504606b05345b52061d2f754c15b51.png" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;&lt;u&gt;※ Pod 实例维度流量溯源&lt;/u&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;以 Pod 为单位进行流量分析和汇总，提供 Pod NameSpace 和 Name 信息，支持快速定位和分析实例级别的流量趋势，帮助细粒度监控和异常流量的精准定位。&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img src="https://static001.geekbang.org/infoq/90/90765c51493906beaf530c64494abc6a.png" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;在整体能力建设完成后，我们成功构建了 NAS 实例级别的 IOPS、吞吐和读写延时数据监控大盘。通过该能力，进一步实现了 NAS 实例的 IOPS 和吞吐可以快速溯源到任务级别和 Pod 实例级别，流量溯源时效从小时级别缩短至分钟级别，有效提升了异常问题定位与解决的效率。同时，基于任务流量视图，我们为后续带宽错峰复用提供了直观的数据支持。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;往期回顾&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#586c90"&gt;1.&lt;/span&gt;正品库拍照 PWA 应用的实现与性能优化｜得物技术&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#586c90"&gt;2.&lt;/span&gt;汇金资损防控体系建设及实践 | 得物技术&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#586c90"&gt;3.&lt;/span&gt;一致性框架：供应链分布式事务问题解决方案｜得物技术&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#586c90"&gt;4.&lt;/span&gt;得物社区活动：组件化的演进与实践&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#586c90"&gt;5.&lt;/span&gt;从 CPU 冒烟到丝滑体验：算法 SRE 性能优化实战全揭秘｜得物技术&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;文 / 泊明&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;关注得物技术，每周更新技术干货&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;要是觉得文章对你有帮助的话，欢迎评论转发点赞～&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;未经得物技术许可严禁转载，否则依法追究法律责任。&lt;/span&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/5783135/blog/18683994</link>
      <guid isPermaLink="false">https://my.oschina.net/u/5783135/blog/18683994</guid>
      <pubDate>Tue, 29 Jul 2025 07:35:59 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>英伟达开源 Llama-3.3-Nemotron-Super-49B-v1.5 模型</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;英伟达发布了 Llama-3.3-Nemotron-Super-49B-v1.5，这是一款专为推理和 Agentic 任务优化的开源模型，在单个 H100 GPU 上实现高吞吐量。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-1eb8efcbf4188aaa81c53fbda0c23259d86.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;模型介绍&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Llama Nemotron Super v1.5 是 Llama-3.3-Nemotron-Super-49B-V1.5 的简称。它是 Llama-3.3-Nemotron-Super-49B-V1 的升级版本（该模型是 Meta 的 Llama-3.3-70B-Instruct 的衍生模型），专为复杂推理和智能体任务设计，支持 128K tokens 的上下文长度。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;模型架构&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Llama Nemotron Super v1.5 采用神经架构搜索（Neural Architecture Search，NAS），使该模型在准确率和效率之间实现了良好的平衡，将吞吐量的提升有效转化为更低的运行成本。&lt;/p&gt; 
&lt;p&gt;（注：NAS 的目标是通过搜索算法从大量的可能架构中找到最优的神经网络结构，利用自动化方法替代人工设计神经网络架构，从而提高模型的性能和效率。）&lt;/p&gt; 
&lt;p&gt;模型经过了多阶段后训练，包括针对数学、代码、科学和工具调用的监督微调 (SFT)，以及用于聊天对齐的奖励感知偏好优化 (RPO)、用于推理的带可验证奖励的强化学习 (RLVR) 和用于工具调用能力增强的迭代直接偏好优化 (DPO)。&lt;/p&gt; 
&lt;p&gt;在多个基准测试中，该模型表现出色。例如，在 MATH500 上 pass@1 达到 97.4，在 AIME 2024 上达到 87.5，在 GPQA 上达到 71.97。模型支持 Reasoning On/Off 模式，用户可通过在系统提示中设置 /no_think 来关闭推理模式。官方推荐在推理开启时使用 temperature=0.6 和 Top P=0.95，在关闭时使用贪心解码。&lt;/p&gt; 
&lt;p&gt;该模型已准备好用于商业用途，遵循 NVIDIA Open Model License 和 Llama 3.3 社区许可协议。开发者可以通过 NVIDIA build.nvidia.com 或 Hugging Face 下载和试用该模型，并可使用 vLLM（推荐 v0.9.2）进行部署，官方仓库中提供了支持工具调用的解析器插件。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362966</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362966</guid>
      <pubDate>Tue, 29 Jul 2025 07:26:59 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>GitHub 出现大范围服务中断：目前已全部恢复，影响超 8 小时</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;代码托管平台 GitHub 从 2025 年 7 月 28 日 16:50 UTC（北京时间 7 月 29 日 00:50）起突发&lt;strong&gt;大规模服务中断&lt;/strong&gt;，受影响服务包括 Git 操作、API 请求、Pull 请求和 Issues 跟踪等核心功能 。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0729/150643_ZtSu_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;尽管 GitHub 工程团队&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.githubstatus.com%2Fincidents%2Fs6d4x8c6cvv5" target="_blank"&gt;尝试了多种修复措施&lt;/a&gt;（如增设服务器容量、调整限流措施），初期效果不佳，直到北京时间 &lt;strong&gt;7 月 29 日 9:23 左右&lt;/strong&gt; 才取得实质性进展。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1416" src="https://static.oschina.net/uploads/space/2025/0729/150606_wHAe_2720166.png" width="1890" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;最终，相关问题已逐步解决，截至目前，API 请求、Pull 请求等服务已全面恢复，整体中断时间超过 &lt;strong&gt;8 小时&lt;/strong&gt;。GitHub 官方表示正在深入调查具体原因，后续将发布详细技术分析报告。&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;https://www.githubstatus.com/history&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362956</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362956</guid>
      <pubDate>Tue, 29 Jul 2025 07:07:59 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Julius AI 完成 1000 万美元种子轮融资</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        ********************************************************************************************************************
    ********************************************************************************************************************
    ********************************************************************************************************************
    ********************************************************************************************************************
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362957</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362957</guid>
      <pubDate>Tue, 29 Jul 2025 07:07:59 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>中国移动「九天」3.0 发布，多项核心技术同步开源</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;中国移动发布了其自主研发的 「九天」基础大模型 3.0。根据介绍，「九天众擎语言大模型」实现了架构上的突破性创新，采用可扩展至万亿级的&amp;nbsp;&lt;strong&gt;MoE 架构&lt;/strong&gt;。通过 15T token 的多阶段配比预训练数据与全流程治理体系，其推理能力得到显著强化。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;该模型还创新构建了 113 域 ×53 能力的二维分级后训练框架，结合动态强化学习策略，使复杂推理能力提升了&amp;nbsp;&lt;strong&gt;35%&lt;/strong&gt;。测评结果显示，「九天」语言大模型：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;在&amp;nbsp;&lt;strong&gt;GPQA-Diamond&lt;/strong&gt;&amp;nbsp;评测中，以&amp;nbsp;&lt;strong&gt;77.67 分&lt;/strong&gt;斩获全球第二，超越 DeepSeekR1 和 Qwen3。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;在&amp;nbsp;&lt;strong&gt;ArenaHard V1.0&lt;/strong&gt;&amp;nbsp;中，以&amp;nbsp;&lt;strong&gt;67.2 分&lt;/strong&gt;位居全球第一。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;在&amp;nbsp;&lt;strong&gt;BFCL V3&lt;/strong&gt;&amp;nbsp;评测中，达到&amp;nbsp;&lt;strong&gt;68 分&lt;/strong&gt;。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;在性能大幅跃升的同时，模型进一步强化了可控生成能力，通过精确流程内置等技术细节，实现了专业场景下的&lt;strong&gt;零幻觉&lt;/strong&gt;，破解了沉浸式角色演绎难题。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;基于最新的语言大模型，中国移动还同步推出了多个专项模型:&lt;/span&gt;&lt;/p&gt; 
&lt;ul style="margin-left:0; margin-right:0"&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;九天代码大模型：&lt;/strong&gt;采用两阶段持续训练技术，支持代码生成、注释生成、单元测试生成、代码智能问答等任务，覆盖 Python、Java、JS、TS、Go、C++ 等 10 余种主流编程语言。在 EvalPlus、MHPP、LivecodeBenchv6 等多个代码生成榜单上表现领先。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;九天数学大模型：&lt;/strong&gt;在短思考、长思考模式下均达到业界 SOTA 水平，多项指标超越 Qwen2.5Math、Qwen3、DeepSeek Math、DeepSeek R1-Distill 等同参数量级模型。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;「九天善智多模态大模型」引入复杂时空建模、流匹配图片视频渐进式联合训练、端到端局部可控注意力机制等创新技术。同时，通过融合多模态理解信息和联合图文交织数据训练，显著提升了模型对文本指令和输入条件图像视频的感知能力。这意味着模型不仅能生成高质量的图像视频，还能进行多轮对话式高可控精确编辑操作，大幅提升了视觉生成的灵活便利性。例如，在图片生成方面可支持多轮精准局部修改，如修改文字、修改背景、增加元素等。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;模型的图理解和视频理解性能也得到了全面提升：&lt;/span&gt;&lt;/p&gt; 
&lt;ul style="margin-left:0; margin-right:0"&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;图理解方面：&lt;/strong&gt;在 MMStar、HallusionBench 和 OCRBench 等图理解任务中，九天模型分别获得了&amp;nbsp;&lt;strong&gt;82.2、64.3 和 94.9 的高分&lt;/strong&gt;，处于业界领先水平。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;视频理解方面：&lt;/strong&gt;在 Videomme 和 MVbench 两个任务中均表现领先，超越 Qwen2-VL 和 InternVideo2。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;目前，中国移动已将多项模型及核心技术进行开源：&lt;/span&gt;&lt;/p&gt; 
&lt;ol style="margin-left:0; margin-right:0"&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;开源九天数童结构化数据大模型&lt;/strong&gt;：包括 JT-DA-8B 模型及后续演进版本，支持下载模型权重、微调代码、推理代码等。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;开源九天数学大模型&lt;/strong&gt;：包括 JT-Math-8B 系列模型，支持下载模型权重、推理代码、技术报告。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;开源九天代码大模型&lt;/strong&gt;：包括 JT-Coder-8B 系列模型，支持下载模型权重、推理代码、技术报告。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;开源业界首创的结构化数据模型评测数据及 TReB 评测体系&lt;/strong&gt;：涵盖 6 大任务、34 个能力，包括高质量、全面的数据、推理模式及评价指标，支持下载评测数据集、测试代码。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;开源 CCR-Bench 行业场景复杂指令遵循评测数据集&lt;/strong&gt;：包含 174 条高质量、多样化、高难度复杂指令数据，高度模拟健康专家、智能客服、医疗助手等典型工业场景，支持下载数据集。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362949</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362949</guid>
      <pubDate>Tue, 29 Jul 2025 06:45:59 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>特斯拉与三星签订 165 亿美元 AI 芯片制造协议</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;7 月 28 日，三星电子在提交给监管机构的文件中表示，三星电子与一家全球大型公司签署了价值 22.8 万亿韩元（注：现汇率约合 1181.72 亿元人民币，约合 165 亿美元）的芯片制造协议，但未透露具体客户名称。&lt;/p&gt; 
&lt;p&gt;据消息人士透露，特斯拉正是这家客户，该公司目前与三星的合同芯片制造部门已有业务往来。&lt;/p&gt; 
&lt;p&gt;有外媒表示，三星电子公司将就新达成的 165 亿美元协议，为特斯拉公司生产半导体，这将为其表现不佳的晶圆代工部门提供助力。该合作的合同期 2025 年 7 月 24 日-2033 年 12 月 31 日。&lt;/p&gt; 
&lt;p&gt;对此，特斯拉 CEO 埃隆・马斯克确认了合作爆料，三星在美国得克萨斯州新建的巨型工厂将专门用于生产特斯拉的下一代 AI6 芯片（注：特斯拉汽车智驾芯片），并称「其战略重要性毋庸置疑」。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-99e430011e2ff79c31d84adca382c598c73.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;马斯克还称，三星目前正在生产 AI4 芯片。台积电将首先在中国台湾地区生产刚刚完成设计的 AI5 芯片，然后在美国亚利桑那州生产。&lt;/p&gt; 
&lt;p&gt;根据外媒今年 6 月报道，台积电在全球第三方晶圆代工市场的市占比为 67%，而排名第二的三星则仅占 11%。另外，有消息人士称，三星电子 2025 上半年晶圆代工部门获零奖金。此前，外媒援引供应链消息称，三星已启动「精选和聚焦」战略，集中资源提升 2nm 工艺良率，希望通过产量和成本优势来挑战台积电。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362947</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362947</guid>
      <pubDate>Thu, 17 Jul 2025 06:41:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>端侧原生大模型 SmallThinker 正式开源</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;上海交通大学 IPADS 研究所、上海交通大学人工智能学院联合初创公司本智激活（Zenergize AI），发布了开源端侧原生大模型 SmallThinker。&lt;/p&gt; 
&lt;p&gt;该系列模型采用为端侧算力、内存、存储特性而原生设计的模型架构，并从零开始预训练，具体包含两个尺寸的稀疏模型，分别是 SmallThinker-4B-A0.6B 和 SmallThinker-21B-A3B。&lt;/p&gt; 
&lt;ul&gt; 
&lt;/ul&gt; 
&lt;p&gt;SmallThinker 专为低成本硬件设计，可在百元级国产开发板（如瑞芯微 RK3588）上流畅运行百亿参数模型，旨在为资源受限的个人设备带来强大、私密且低延迟的 AI 能力，无需依赖云端。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1578" src="https://static.oschina.net/uploads/space/2025/0729/143436_ewWq_2720166.png" width="2044" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;用户可以通过 Transformers（版本需 &amp;gt;= 4.53.3）或 ModelScope 来运行该模型。官方 GitHub 仓库提供了详细的设置、模型转换和运行指南。官方提示，模型使用了稀疏的 lm_head，可能会导致一定的精度损失，但用户可以手动修改代码禁用此特性。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct&lt;/li&gt; 
 &lt;li&gt;https://github.com/SJTU-IPADS/PowerInfer/tree/main/smallthinker&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362945</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362945</guid>
      <pubDate>Thu, 17 Jul 2025 06:36:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>🔥造物分享：AnyShake Project 开源地震监测系统</title>
      <description/>
      <link>https://www.oschina.net/ai-creation/details/2112</link>
      <guid isPermaLink="false">https://www.oschina.net/ai-creation/details/2112</guid>
      <pubDate>Thu, 17 Jul 2025 06:18:00 GMT</pubDate>
    </item>
    <item>
      <title>谷歌 NotebookLM 即将推出「视频概览」功能</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;谷歌 NotebookLM 即将推出一项名为「视频概览 (Video Overviews)」的新功能，能以视频幻灯片形式呈现内容摘要。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0729/141652_Z58B_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Ftestingcatalog%2Fstatus%2F1949120138373914737" target="_blank"&gt;相关爆料称&lt;/a&gt;，这些视频概览将以视频幻灯片的形式呈现，内容包含文本、图像和其他视觉元素，并由女性声音进行旁白解说。&lt;/p&gt; 
&lt;p&gt;谷歌 NotebookLM 功能于去年推出，旨在通过 AI 虚拟主持人根据用户上传到 NotebookLM 的文档（如课程阅读材料或法律摘要）生成播客，帮助用户以另一种方式理解和消化文档中的信息。&lt;/p&gt; 
&lt;p&gt;用户可以上传中文 PDF、Google Docs、网页链接或文本，NotebookLM 会生成中文总结或回答基于中文来源的问题。支持高达 50 万字的单个来源，适合处理长篇中文文档。NotebookLM 目前支持 130 种语言的输入来源和聊天功能，包括中文（简体和繁体）。用户可以上传中文文档、网页链接或文本，并以中文与 AI 进行交互。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362937</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362937</guid>
      <pubDate>Thu, 17 Jul 2025 06:18:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>全球首家机器人 6S 店深圳开业</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;全球首家机器人「6S 店」于 7 月 28 日在深圳龙岗星河 WORLD 园区机器人剧场开业，店内集聚数百种机器人及配套零部件。多家企业带来的产品涵盖了家庭服务、医疗辅助、工业巡检、教育陪伴等多个领域。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="358" src="https://oscimg.oschina.net/oscnet/up-5d0777ddd254f3732d7201680e7a9e95e67.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;「机器人 6S 店究竟是什么？」深圳未来时代机器人有限公司 CEO 兼 6S 店店长林枫解释称，其在传统汽车 4S 店「销售（Sale）、零配件供应（Sparepart）、售后服务（Service）、信息反馈（Survey）」的基础上，新增「租赁（Lease）、个性化订制（Customized）」两大功能，形成独特的「6S」模式。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;活动现场数据显示，已有超 200 家产业链上下游企业表达进驻意向，其中人形及服务机器人企业近 50 家，涵盖从核心零部件研发到场景应用的全产业链环节。该店紧扣机器人产业特性，一方面将建立实时数据反馈机制，精准收集用户需求反哺研发；另一方面提供租赁服务，让用户无需购买即可体验。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;据介绍，机器人 6S 店内置「机器人零配件超市」，汇聚伺服电机、减速器等核心元器件，能满足主流机器人维修需求，实现「快速响应、即时维修」。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362936</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362936</guid>
      <pubDate>Thu, 17 Jul 2025 06:12:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>网信办整治自媒体发布不实信息，平台需优化 AI 生成内容标识</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;为持续深入整治「自媒体」发布不实信息乱象，进一步规范「自媒体」信息发布行为，按照 2025 年「清朗」系列专项行动总体安排，中央网信办决定自 7 月 24 日起，在全国范围内启动为期 2 个月的「清朗·整治‘自媒体’发布不实信息」专项行动。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="221" src="https://oscimg.oschina.net/oscnet/up-fcdd633462b44ba4d48a4e73cc8356abdca.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;专项行动重点整治四类突出问题：&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;1.恶意蹭炒误导公众问题。涉热点舆情或公众人物时，假冒当事人、近亲属等，通过账号名称、简介等方式编造身份，蹭炒热点，混淆视听。涉重大舆情、突发事件时，假冒知情人士，编造起因进展、伤亡人数等，无中生有，干扰舆论。发布财经、军事、外交等重要领域信息时，虚构所谓「权威报道」「一手数据」「深度揭秘」等信息，胡编乱造，误导认知。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;2.多种手段歪曲事实问题。利用人工智能生成合成技术，仿冒他人，或编造社会民生等领域虚假信息，欺骗公众。通过剧情摆拍、拼凑剪辑等方式，编造事件、虚构或夸大情节，引起关注。歪曲解读关乎公众利益的政策方针、法规文件，宣扬「即将取消」「重大变动」等不实信息，制造噱头。对往年社会新闻、政策发布等旧闻旧事摘头去尾，掩盖时间、地点、结果等关键要素，恶意炒作。借助网络黑灰产等渠道，以刷榜打榜买榜方式，通过热搜榜单呈现不实信息，操纵榜单。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;3.不做标注以假乱真问题。对涉及国内外时事、公共政策、社会事件等相关信息，未标注或未准确标注信息来源。以「网传」「网友表示」「来源于互联网」等方式发布信息，模糊标注信息来源，发布无实际依据内容。标注错误信息来源，或矩阵账号互相引用标注，导致公众无法追溯真实来源。以过小字号、隐蔽位置、进度条遮挡等方式标注，刻意弱化标注标识。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;4.专业领域信息不实问题。不进行专业资质认证，或以虚假认证、过期认证方式，冒用财经专家、医生、律师等身份。歪曲解读专业内容，如杜撰或篡改真实案例细节，发布未经科学验证或明显违背科学常识的信息，将不同的历史人物与事件张冠李戴、篡改史实。借专业知识分享名义，编造同质化文案或虚假故事，借机引流带货。发布教程，教授通过虚假摆拍、蹭热引流等方式打造「网红专家」人设，扰乱传播秩序。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;中央网信办要求网站平台建立三大机制：在信息发布环节强制设置来源标注选项，未标注内容不得进入算法推荐池；细化专业资质认证流程，动态核验账号身份与运营业务匹配度;畅通举报渠道，对首次违规账号采取提示引导，对恶意编造重点领域信息、仿冒热点当事人的账号实施长期禁言或封号。同时，平台需完善负面清单、营利权限管理等制度，对存在突出问题的平台依法采取处罚措施。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;此次行动强调「标本兼治」，既通过技术手段压缩不实信息生存空间，如优化 AI 生成内容标识功能，又压实平台主体责任，要求其定期排查隐形变异问题。据网信办负责人介绍，专项行动将与日常监管形成合力，推动建立「自媒体」行业信用评价体系，引导内容创作者回归真实、专业的传播轨道，为公众营造可信、有序的网络环境。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362926</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362926</guid>
      <pubDate>Thu, 17 Jul 2025 05:47:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>智象未来亮相 WAIC：多模态智能体，重塑创作的未来版图</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#000000; text-align:left"&gt;2025 世界人工智能大会（WAIC）期间，智象未来（HiDream.ai）联合创始人兼首席技术官姚霆发表主题演讲，系统阐释了多模态智能体在内容创作领域的技术突破与商业化实践。作为聚焦多模态生成的 AI 创新企业，智象未来期待通过探索多模态大模型的有效落地形式， 「让创作回归灵感，让时间忠于故事」 ，推动内容创作从工具效率提升向生产力革命跨越。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:center"&gt;&lt;img alt="图 1.jpg" height="500" src="https://oscimg.oschina.net/oscnet//85051c6c86d90e89291a7bfae9e6a83f.jpg" width="750" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;AI 技术的爆发式发展，正从实验室快速走向产业应用。智象未来始终以「解决真实创作痛点」为导向，在商业化落地中探索出一条「技术筑基、场景破局、价值闭环」的路径。智象未来认为，真正的 AI 商业化不是单点技术的炫耀，而是从模型能力到服务形态，再到最终成果的全链路赋能。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;智象未来持续致力于从技术到价值的产品化思路，在这一过程中，智象构建了「MaaS-SaaS-RaaS」的递进商业化体系。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:center"&gt;&lt;img alt="图 2.jpg" height="422" src="https://oscimg.oschina.net/oscnet//9b90e81b026c2f731020661813f929b2.jpg" width="750" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;&lt;strong&gt;MaaS&lt;/strong&gt;&lt;strong&gt;（&lt;/strong&gt;&lt;strong&gt;Model as a Service&lt;/strong&gt;&lt;strong&gt;）&lt;/strong&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;是根基。打造百亿级多模态基础模型，支持图像、视频、音频、文本等多模态的生成与理解。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;&lt;strong&gt;SaaS&lt;/strong&gt;&lt;strong&gt;（&lt;/strong&gt;&lt;strong&gt;Service as a Service&lt;/strong&gt;&lt;strong&gt;）&lt;/strong&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;是桥梁。基于基础模型，开发面向垂直场景的产品，建设个人创作者平台和社区，将技术能力转化为开箱即用的服务，降低创作门槛。 &amp;nbsp;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;&lt;strong&gt;RaaS&lt;/strong&gt;&lt;strong&gt;（&lt;/strong&gt;&lt;strong&gt;Result as a Service&lt;/strong&gt;&lt;strong&gt;）&lt;/strong&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;是终局。通过商业视频营销服务、新媒体创作智能体，直接为客户交付「可落地的成果」，让 AI 真正成为创作的「生产力工具」而非「技术概念」。 &amp;nbsp;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;这种 「模型支撑服务，服务落地场景」 的逻辑，已在实际应用中验证：智象多模态生成平台已服务于影视制作、产品营销、文旅互娱等领域，实现从技术研发到商业价值的闭环。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;&lt;strong&gt;多模态技术突破：从&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;「&lt;/strong&gt;&lt;strong&gt;能生成&lt;/strong&gt;&lt;strong&gt;」&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;到&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;「&lt;/strong&gt;&lt;strong&gt;生成优&lt;/strong&gt;&lt;strong&gt;」&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;技术实力是商业化的底气。智象多模态模型以「高维理解、精准生成」为核心，构建了覆盖图像、视频、编辑的全栈能力矩阵。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;技术层面，智象多模态基础模型历经三次重要迭代，构建起 「理解深、控制准、画质高」 的核心优势。模型从 2023 年 8 月的 1.0 版本（扩散模型 DiT，实现多模态对齐），到 2024 年 6 月 2.0 版本（扩散自回归模型 DiT+AR，强化时空建模），再到 2024 年 12 月 3.0 版本（MoE 多场景学习，记忆增强），持续突破生成技术瓶颈。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;这些能力转化为三大核心价值：语义一致性（如 IP 故事活化时保持风格统一）、精准可控性（支持个性化定制与元素自由调整）、影视级画质（4K 分辨率、长时序稳定输出），为专业创作提供技术保障。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:center"&gt;&lt;img alt="图片 3.png" height="421" src="https://oscimg.oschina.net/oscnet//bc0b074c8a15ca0e37dd98a7a08fe241.png" width="750" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;在图像生成领域，HiDream 系列开源模型表现亮眼，累计下载量超 60 万次，被 Diffusers 库、ComfyUI 、Recraft 等主流工具集成。智象多模态全系列模型均在国际权威榜单排名前列。HiDream-I1 全面开源后 24 小时内即登顶 Artificial Analysis 榜单，成为首个问鼎榜首的中国自研模型，Hugging Face 实时排名全球第一，下载量与点赞数持续攀升。此外，智象大模型家族已实现文本、图像、视频的联合建模，其视频生成产品支持 4K 高清画质、全局 / 局部可控及剧本多镜头生成，被行业专家评价为「重新定义 AIGC 的美学标准」。同时，结合其开源的交互式编辑模型 HiDream-E1，用户通过自然语言指令即可完成图像生成及编辑，直接降低创作门槛，助力全球开发者与创作者实现「所想即所得」。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;7 月，继问鼎图像生成开源模型竞技场榜单后，最新开源模型 HiDream E1.1 再次强势跻身 Artificial Analysis 图像编辑智能体榜单第一梯队，作为领先的开源图像编辑模型，性能全面超越 Flux.1 Kontext 等主流模型，支持自然语言驱动的图像编辑 —— 用户通过文字指令即可完成背景替换、颜色修改、局部重绘等操作。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:center"&gt;&lt;img alt="图片 4.png" height="420" src="https://oscimg.oschina.net/oscnet//9dddc0cdea26950d1a5dfe9cd240357f.png" width="750" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;在视频生成领域，模型支持文生视频、图生视频、首尾帧生成，可精准复刻国漫、吉卜力等风格，实现镜头运动与画面运动的联合学习。通过扩散自回归模型（DiT+AR），我们解决了视频生成中「时空一致性」难题，让生成内容更贴近真实物理世界的规律。 &amp;nbsp;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;在创作工具箱层面，AI 口播、视频模板、运动笔刷、虚拟换衣、图像超分等功能，形成了「生成-编辑-优化」的完整闭环，满足从个人创作者到企业客户的全场景需求。 &amp;nbsp;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:center"&gt;&lt;img alt="图片 5.png" height="422" src="https://oscimg.oschina.net/oscnet//9d53f9a7a31deb70f0db3d482a02fec7.png" width="750" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;&lt;strong&gt;产品形态：&lt;/strong&gt;&lt;strong&gt;agent&lt;/strong&gt;&lt;strong&gt;驱动的&lt;/strong&gt;&lt;strong&gt;「&lt;/strong&gt;&lt;strong&gt;创作革命&lt;/strong&gt;&lt;strong&gt;」&lt;/strong&gt;&lt;strong&gt;，重构内容创作全流程&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;在产品形态上，智象以 「智能体」 为核心形态，构建覆盖图像生成、视频创作、营销传播的工具链。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;作为面向短视频二创的智能体，vivago agent 以「多模态输入、智能拆解、交互式生成」为核心优势。用户只需提供图像、视频、音频、文本等素材（例如咖啡馆的 logo、照片、宣传语），即可自动分析需求、拆解任务（分镜设计、剧本生成、素材检索），调用图像/视频生成模型补全内容，并通过智能剪辑工具整合输出。它不仅能理解「棕色线条勾勒的火焰+波浪 logo」的视觉特征，还能捕捉「静谧奢华的吧枱场景」的氛围，让短视频创作从「从零开始」变为「按需生成」。 &amp;nbsp;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;智象未来即将正式发布长视频编辑智能体-HiClip。针对长视频「内容过载、分发低效、回报周期长」的痛点，HiClip 通过多模态语义理解，精准解构内容核心（如提取高光片段、生成音频摘要），实现「一次创作、全域适配」的二次传播。无论是影视片段的高光剪辑，还是教育课程的知识点拆解，HiClip 都能让长视频内容焕发新的流量生命力。 &amp;nbsp;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;产品化落地实现了创作方面的互补：vivago agent 聚焦短视频二创，通过模板检索、智能剪辑、多模态生成，帮助用户快速制作个性化内容，解决传统模板化创作的同质化问题；HiClip 则针对长视频 「内容过载、分发低效」 的痛点，以多模态语义理解解构长视频核心信息，实现高光片段提取、跨平台适配剪辑，激发长视频二次传播价值。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;&lt;strong&gt;生态共创：链接全产业链的价值网络&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;AI 的价值，在于连接与赋能；技术与产品的落地，离不开生态的协同支撑。目前，智象未来正携手跨境、互联网、影视、新媒体、文旅等多领域伙伴，构建覆盖多领域的生态网络，形成 「技术-场景-生态」 的共赢格局。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:center"&gt;&lt;img alt="图片 6.png" height="431" src="https://oscimg.oschina.net/oscnet//a8763d0a57e911cb5a981b70e0c97fde.png" width="750" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;让每个创作者都能更好释放创意潜力，是智象的始终坚持。让 AI 真正 「理解创作、辅助创作」，让内容产业的生产力革新正加速到来。智象未来期待以多模态智能体为支点，与行业伙伴共同探索「技术为笔，创意为墨」的新可能——让每个创作者都能聚焦灵感，让每个故事都能抵达更远的地方。 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362906</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362906</guid>
      <pubDate>Thu, 17 Jul 2025 04:00:00 GMT</pubDate>
      <author>作者: 开源科技</author>
    </item>
    <item>
      <title>微软为 Edge 浏览器推出新的 Copilot 模式，支持实时分析屏幕内容</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;微软正在为其 Edge 浏览器推出一种名为&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.microsoft.com%2Fzh-cn%2Fedge%2Fai-powered%2Fcopilot-mode" target="_blank"&gt;「Copilot Mode」&lt;/a&gt;的全新实验性模式，旨在提供一种由 AI 驱动的网页浏览体验。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1856" src="https://static.oschina.net/uploads/space/2025/0729/113855_QJwL_2720166.png" width="3360" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;该模式包含多项新功能，包括全新的现代化主页（New Modern Homepage）、快速撰写（Quick Compose）、简单的任务切换（Simple Task Handoff）以及语音导航（Voice Navigation）。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-4177c9b14685e66cd78d1978e425461b2a9.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;其中一个核心功能是「Copilot Vision」，它允许 Copilot 「看到」用户的屏幕，即时扫描和分析屏幕上的内容，并实时提供相关的建议和见解。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362895/microsoft-edge-copilot-mode</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362895/microsoft-edge-copilot-mode</guid>
      <pubDate>Thu, 17 Jul 2025 03:39:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>国内首个农业智能大模型上线，每亩地增收可达 200 元</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;中国中化集团推出了国内首个 「农业种植综合大模型」。这个大模型不仅依托于全国数百座农业技术服务中心的支持，更整合了超过千万条农业知识资源，为农民提供精准、科学的种植指导。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;该农业种植综合大模型贯穿了 「耕、种、管、收」 的整个过程，能够进行高效、可靠的复杂任务处理。通过大模型的应用，农艺师只需在手机或平板电脑上就能实现线上智能决策，线下为农民提供贴身服务。这意味着，农民能够通过实时监测作物的生长情况、土壤湿度，以及气象和病虫害等重要因素，获取及时的建议，例如 「每亩需要多少肥料、何时浇水」 等信息。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="282" src="https://oscimg.oschina.net/oscnet/up-b29e4fd6f3ee345ed36aa8353bfd5d8e80a.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;许多网友对此表示兴奋，甚至戏称这项技术让他们想起了小时候的 QQ 农场。科技的进步，不仅为农事决策带来了高效和便利，还能够让农民在增加收入的同时，享受到更多的乐趣和成就感。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;据悉，使用这一大模型后，农事决策的时间可以缩短 75%，每亩地的增收可达 150 到 200 元。这对广大农民而言，无疑是一个好消息。农业种植综合大模型的上线，不仅提升了农业生产的智能化水平，也为全国的农业发展注入了新动力。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362892</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362892</guid>
      <pubDate>Thu, 17 Jul 2025 03:37:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>上海 AI 实验室开源科学多模态大模型『书生』Intern-S1</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;上海人工智能实验室（上海 AI 实验室）&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fd7DfDz4yw_5ktewSpjzVDA" target="_blank"&gt;发布&lt;/a&gt;并开源『书生』科学多模态大模型 Intern-S1，声称多模态能力全球开源第一，文本能力比肩国内外一流模型，科学能力全模态达到国际领先，作为融合科学专业能力的基础模型，Intern-S1 综合性能为当前开源模型中最优。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;公告介绍称，Intern-S1 在同一模型内实现了语言和多模态性能的高水平均衡发展，具备「全能高手」的实力；同时，作为「科学明星」，它还富集多学科专业知识，重点强化了科学能力，在化学、材料、地球等多学科专业任务基准上超越了顶尖闭源模型 Grok-4；此外，Intern-S1 还开创了「多任务的通专融合」的新范式，支持大规模多任务强化学习齐头并进，在保持能力全面的同时实现专业精通。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="319" src="https://oscimg.oschina.net/oscnet/up-e8b2a446e9dc17a212b7fc9fe2fd02a0f70.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="304" src="https://oscimg.oschina.net/oscnet/up-8d4b59325c224883c411d165c948180e543.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="153" src="https://oscimg.oschina.net/oscnet/up-99ef18ed34310abfc6028e4dc8aa28294d1.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;为了更好地适应科学数据，Intern-S1 新增了动态 Tokenizer 和时序信号编码器，可支持多种复杂科学模态数据，实现了材料科学与化学分子式、生物制药领域的蛋白质序列、天文巡天中的光变曲线、天体碰撞产生的引力波信号、地震台网记录的地震波形等多种科学模态的深度融合。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;Intern-S1 还实现了对科学模态数据的深入理解与高效处理，例如，其对化学分子式的压缩率相比 DeepSeek-R1 提升 70% 以上；在一系列基于科学模态的专业任务上消耗的算力更少，同时性能表现更优。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;更多详情可&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fd7DfDz4yw_5ktewSpjzVDA" target="_blank"&gt;查看官方公告&lt;/a&gt;。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362877</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362877</guid>
      <pubDate>Thu, 17 Jul 2025 03:12:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>GPT-5 即将发布！相关参数、功能与展望预测汇总</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;随着人工智能领域的竞争日益加剧，OpenAI 的下一代大语言模型 GPT-5 备受关注。根据最新信息，GPT-5 预计将于 2025 年年中至晚些时候发布，具体时间可能在 8 月或更晚。本文综合网络信息，整理了关于 GPT-5 的参数、功能及潜在影响的最新动态，为您呈现最全面的预览。&lt;/span&gt;&lt;/p&gt; 
&lt;h3 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;发布日期与开发进展&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;根据 OpenAI 首席执行官 Sam Altman 在 2025 年 2 月发布的路线图，GPT-5 预计在 2025 年年中推出，具体可能在 8 月或稍晚。Altman 在近期采访中表示，GPT-5 的发布将晚于 GPT-4.5（代号 Orion，已于 2025 年 2 月 27 日发布），并强调其为「前沿模型」，代表重大技术飞跃。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;然而，开发过程中面临的技术与资源挑战可能导致进一步延迟。据报道，GPT-5（代号可能为 Orion 或 Arrakis）的训练成本高达 5 亿美元以上，且需要大规模数据中心支持，训练时间至少 6 个月。OpenAI 内部也经历了高管离职等动荡，可能影响开发进度。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;img height="241" src="https://oscimg.oschina.net/oscnet/up-44d549d72cc75a7aa62b1b9593f9f177949.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;h3 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;参数规模与技术架构&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;虽然 OpenAI 尚未公开 GPT-5 的具体参数数量，但业界推测其参数规模将显著超越前代模型。以下是关键信息:&lt;/span&gt;&lt;/p&gt; 
&lt;ul style="margin-left:0; margin-right:0"&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;参数规模&lt;/strong&gt;：GPT-4 据估拥有约 1.5 万亿参数，而 GPT-5 可能达到 3 至 50 万亿参数，具体取决于是否采用混合专家模型（MoE）。有报道称，GPT-5 可能利用 20，000 个 NVIDIA GB200 芯片或 150，000 个 H100 芯片进行训练，支持高达 80 万亿参数的模型。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;架构创新&lt;/strong&gt;：GPT-5 将整合 GPT 系列与 o 系列（如 o1、o3）的能力，采用统一架构，消除用户在不同任务间切换模型的需求。可能引入图神经网络 (GNN) 和增强注意力机制，提升语言处理效率和复杂情境理解能力。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;训练数据&lt;/strong&gt;：GPT-5 预计使用更大规模的多样化数据集，包括公开网络数据和私有企业数据，可能结合合成数据以应对数据短缺问题。然而，合成数据可能引发反馈循环，增加「幻觉」风险。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;核心功能与改进&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;GPT-5 被设计为多模态、统一智能系统，旨在提供更高效、准确的 AI 体验。以下是其核心功能的预期亮点:&lt;/span&gt;&lt;/p&gt; 
&lt;ul style="margin-left:0; margin-right:0"&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;多模态能力&lt;/strong&gt;：GPT-5 将进一步增强多模态处理能力，支持文本、图像、语音和视频输入输出。基于 GPT-4o 的语音和图像处理基础，GPT-5 可能集成视频处理功能，例如通过 OpenAI 的 SORA 技术实现文本到视频的生成。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;高级推理能力&lt;/strong&gt;：OpenAI 强调 GPT-5 将显著提升链式推理（Chain-of-Thought）能力，擅长多步骤逻辑和决策制定。相比 GPT-4o 的快速响应，GPT-5 将更擅长处理复杂问题，如软件工程中的代码生成与调试、数学和物理等科学任务。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;上下文窗口扩展&lt;/strong&gt;：GPT-4o 的上下文窗口为 128，000 个 token，而 GPT-5 可能支持高达 500 万个 token，足以处理整本书籍或大型企业数据，提升长文本处理能力。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;减少幻觉&lt;/strong&gt;：GPT-5 预计将「幻觉」率降至 10% 以下，显著提高输出的准确性和可靠性，特别是在科学和编程领域。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;自主 AI 代理&lt;/strong&gt;：GPT-5 可能引入自主 AI 代理功能，能够执行现实世界的任务，如管理邮件、预订日程或根据用户偏好完成购物，减少人工干预。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;Canvas 工作空间&lt;/strong&gt;：基于 GPT-4o 的 Canvas 功能，GPT-5 将提供更强大的交互式工作空间，优化编码、数学和分步工作流程的体验。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;行业影响与应用前景&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;GPT-5 的发布将对多个领域产生深远影响:&lt;/span&gt;&lt;/p&gt; 
&lt;ul style="margin-left:0; margin-right:0"&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;软件开发&lt;/strong&gt;：测试者反馈，GPT-5 在复杂软件项目中的代码生成和调试能力超越了 Anthropic 的 Claude4Sonnet，可能成为开发者首选工具。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;科学研究&lt;/strong&gt;：在数学、物理和生物学等学科中，GPT-5 的高级推理能力将加速研究进程，支持复杂数据分析和假设验证。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;商业与生产力&lt;/strong&gt;：通过自主 AI 代理和个性化功能，GPT-5 可优化客户服务、内容创作和日常任务自动化，提升企业效率。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;教育与医疗&lt;/strong&gt;：GPT-5 的多模态能力和上下文理解将革新教育领域的个性化学习，以及医疗领域的患者交互和文档处理。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;挑战与伦理考量&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;尽管前景光明，GPT-5 的开发和部署面临多重挑战:&lt;/span&gt;&lt;/p&gt; 
&lt;ul style="margin-left:0; margin-right:0"&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;成本与资源&lt;/strong&gt;：训练成本高昂，数据中心建设周期长，可能限制 OpenAI 的并行开发能力。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;伦理与安全&lt;/strong&gt;：大规模模型可能引发误用风险，如生成虚假信息或模拟人类行为。OpenAI 正在进行严格的安全测试，推迟了部分功能的发布。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;竞争压力&lt;/strong&gt;：Anthropic 的 Claude 系列、Google 的 Gemini 和 Meta 的 LLaMA 等竞品正在迅速追赶，迫使 OpenAI 在性能与可靠性之间找到平衡。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;用户反馈与社区期待&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;社交媒体上，开发者对 GPT-5 的期待集中在编程能力和推理性能的提升。部分用户在 X 平台上提到，GPT-5 的早期测试版在软件工程任务中表现优异，超越 Claude Sonnet4。然而，也有用户担忧其高昂的订阅成本和潜在的使用限制，类似 Anthropic 对 Claude Code Max 计划的调整可能引发的不满。&lt;/span&gt;&lt;/p&gt; 
&lt;h3 style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;总结&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;GPT-5 作为 OpenAI 的下一代旗舰模型，预计将通过更大的参数规模、统一的架构和多模态能力，显著提升 AI 的推理、准确性和实用性。尽管面临成本、安全和竞争等多重挑战，其在编程、科研和商业领域的潜力不容忽视。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362874</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362874</guid>
      <pubDate>Thu, 17 Jul 2025 03:01:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Anthropic 将在 8 月底面向 Claude Pro 和 Max 订阅用户推出新的每周使用限制</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Anthropic &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2FAnthropicAI%2Fstatus%2F1949898502688903593" target="_blank"&gt;宣布&lt;/a&gt;，由于 Claude Code 的需求空前增长，将从 8 月 28 日起为 Claude Pro 和 Max 订阅计划引入新的每周使用量限制。此举旨在解决因少数极端使用案例和违反服务条款的行为（如账户共享和转售）导致的系统容量问题，以确保为所有用户提供更公平、可靠的服务。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0729/105318_5uv3_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;新的限制措施将在现有的每 5 小时重置限制的基础上，增加一个每 7 天重置的总体每周限制，以及一个针对 Claude Opus 4 的特定每周限制。Anthropic 估计，根据当前使用情况，新规将影响不到 5% 的订阅用户。&lt;/p&gt; 
&lt;p&gt;公司透露，一些重度用户在后台 24/7 连续运行模型，其中一个案例是在每月 200 美元的套餐上消耗了价值数万美元的模型用量。新限制旨在缓解此类高成本使用情况。&lt;/p&gt; 
&lt;p&gt;对于受影响的用户，Anthropic 给出了一些预期使用时长的参考：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;大多数 Pro 用户在每周限制内预计可使用 40-80 小时的 Sonnet 4；&lt;/li&gt; 
 &lt;li&gt;大多数 Max 20x 用户则可使用 240-480 小时的 Sonnet 4 和 24-40 小时的 Opus 4。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;对于超出限制的 Max 计划用户，Anthropic 将提供以标准 API 价格购买额外用量的选项。公司表示仍在探索支持长期使用案例的最佳方式，并欢迎重度用户提供反馈。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362871</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362871</guid>
      <pubDate>Thu, 17 Jul 2025 02:53:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>快手可灵发布 Kling Lab</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;快手的 Kling AI 团队&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2FKling_ai%2Fstatus%2F1949760383692255518" target="_blank"&gt;宣布&lt;/a&gt;推出 Kling Lab，这是一个旨在简化创作流程、提高效率并促进协作的新工作空间。该产品目前正处于 Beta 测试阶段。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0729/104711_dOxb_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;此外，在 2025 世界人工智能大会（WAIC）上，可灵 AI 还披露了最新用户数据，在全球拥有超过 4500 万创作者，产品自发布以来迭代升级 30 余次，累计生成超 2 亿个视频和 4 亿张图片。&lt;/p&gt; 
&lt;p&gt;可灵 AI 产品及运营负责人李杨表示，4 月可灵 2.0 发布以来，服务的 B 端商家数量迎来爆发式增长。截至目前，全球范围内已有超过两万企业客户及开发者接入了可灵 AI 的 API（应用程序编程接口）接口，覆盖全球 149 个国家和地区。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362870</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362870</guid>
      <pubDate>Thu, 17 Jul 2025 02:48:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>阿里开源通义万相 Wan2.2</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;阿里&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FiPL7OLQhwYdoFelHt41N6Q" target="_blank"&gt;宣布&lt;/a&gt;开源视频生成模型「通义万相 Wan2.2」，此次共开源文生视频（Wan2.2-T2V-A14B）、图生视频（Wan2.2-I2V-A14B）和统一视频生成（Wan2.2-IT2V-5B）三款模型。其中文生视频模型和图生视频模型均为业界首个使用 MoE 架构的视频生成模型，总参数量为 27B，激活参数 14B。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;根据介绍，通义万相 2.2 率先在视频生成扩散模型中引入 MoE 架构，有效解决视频生成处理 Token 过长导致的计算资源消耗大问题。Wan2.2-T2V-A14B、Wan2.2-I2V-A14B 两款模型均由高噪声专家模型和低噪专家模型组成，分别负责视频的整体布局和细节完善，在同参数规模下，可节省约 50% 的计算资源消耗，在模型能上，通义万相 2.2 在复杂运动生成、人物交互、美学表达、复杂运动等维度上也取得了显著提升。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;Wan2.2 还首创了「电影美学控制系统」，光影、色彩、构图、微表情等能力媲美专业电影水平。例如，用户输入「黄昏」、「柔光」、「边缘光」、「暖色调」「中心构图」等关键词，模型可自动生成金色的落日余晖的浪漫画面；使用「冷色调」、「硬光」、「平衡图」、「低角度」的组合，则可以生成接近科幻片的画面效果。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" height="213" src="https://oscimg.oschina.net/oscnet/up-9384df8eada31bdbc196286746847bfe546.gif" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img alt="" height="177" src="https://oscimg.oschina.net/oscnet/up-58b00afb9376a3a0972e10d973fdb82ab5f.gif" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img alt="" height="213" src="https://oscimg.oschina.net/oscnet/up-7c4a2896d24586d24ff41491862261a2aeb.gif" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;通义万相还开源了一款 5B 小尺寸的统一视频生成模型，单一模型同时支持文生视频和图生视频，可在消费级显卡部署。该模型采用了高压缩率 3D VAE 架构，时间与空间压缩比达到高达 4×16×16，信息压缩率提升至 64，均实现了开源模型的最高水平，仅需 22G 显存（单张消费级显卡）即可在数分钟内生成 5 秒高清视频，是目前 24 帧每秒、720P 像素级的生成速度最快的基础模型。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/362867</link>
      <guid isPermaLink="false">https://www.oschina.net/news/362867</guid>
      <pubDate>Thu, 17 Jul 2025 02:13:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
  </channel>
</rss>
