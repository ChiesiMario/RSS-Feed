<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - industry - 简体中文</title>
    <link>https://www.oschina.net/news/industry</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news/industry" rel="self" type="application/rss+xml"/>
    <description>已对该 RSS 进行格式化操作：中英字符之间插入空格、使用直角引号、标点符号修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-cn</language>
    <lastBuildDate>Thu, 11 Sep 2025 12:41:44 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>快手发布开源多模态大模型 Kwai Keye-VL-1.5</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;快手近日正式发布多模态大语言模型 Keye-VL-1.5-8B。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0911/195624_2HB4_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;https://huggingface.co/Kwai-Keye/Keye-VL-1_5-8B&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;据介绍，与之前的版本相比，Keye-VL-1.5 的综合性能实现显著提升，尤其在基础视觉理解能力方面，包括视觉元素识别、推理能力以及对时序信息的理—表现尤为突出。Keye-VL-1.5 在同等规模的模型中表现出色，甚至超越了一些闭源模型如 GPT-4o。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-99ae906bd3166733efda4ecc20d5895a63d.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Keye-VL-1.5 采用四阶段渐进式训练流水线，以系统化方式提升模型性能。在视觉编码器预训练阶段，使用 SigLIP-400M 权重初始化 ViT，并通过 SigLIP 对比损失持续预训练以适应内部数据分布。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-09d1b4cf72c51b253843c3541b0130725c4.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;第一阶段重点优化投影 MLP 层，实现跨模态特征的稳固对齐；第二阶段解冻全部参数进行端到端多任务预训练，显著增强基础视觉理解能力；第三阶段进行退火训练，利用高质量数据微调模型，弥补上一阶段中高质量样本接触不足的问题，同时将序列长度扩展至 128K、调整 RoPE 逆频率配置，并引入长视频、长文本和大尺度图像等长上下文数据。 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;最终，通过同质-异质融合技术对不同数据混合比例下的模型权重进行平均，减少固定数据比例带来的内在偏差，在保持多样化能力的同时提升模型的鲁棒性。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371648</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371648</guid>
      <pubDate>Thu, 11 Sep 2025 12:00:41 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>AI 编程公司 Replit 发布第三代自主编码 Agent</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Replit&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Freplit.com%2Fagent3" target="_blank"&gt;宣布&lt;/a&gt;推出第三代自主编码 Agent（Agent 3），官方称其自主性提升至前代的 10 倍，单次可连续运行 200 分钟，全程无需人工干预。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;自主性增强&lt;/strong&gt;：Agent 3 可以自主测试和修复代码，甚至在后台持续改进用户的应用，将用户从重复性工作中解放出来。它能够像人类一样在浏览器中 「点击」 和 「操作」，检查应用中的按钮、表单和 API，确保一切正常运行。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;持续运行能力&lt;/strong&gt;：该版本能够持续自主运行超过三小时，相比之前的版本有了很大的进步。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;提升开发效率&lt;/strong&gt;：Agent 3 能够根据用户需求生成高质量代码，并主动提供优化建议，从而提升开发效率。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0911/194603_LoIb_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;新系统通过自研测试框架在浏览器内自动点击按钮、填写表单、调用 API 并修复错误，其速度比主流 Computer Use 模型快 3 倍，成本则降低了 90%。&lt;/p&gt; 
&lt;p&gt;Agent 3 支持自然语言提示，用户可以用简单描述启动复杂项目，并在手机端通过 Live Monitoring 实时查看进度。其另一项突破是能够生成子 Agent 与自动化流程，成品可直接接入 Slack、Notion、邮件等平台，进一步扩展工作流。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371645/replit-agent3</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371645/replit-agent3</guid>
      <pubDate>Thu, 11 Sep 2025 11:49:41 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>谷歌推迟发布 Android 16 QPR1 的 AOSP 源代码，引发担忧</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.androidauthority.com%2Fandroid-16-qpr1-source-code-delay-3596650%2F"&gt;根据科技媒体 Android Authority 的报道&lt;/a&gt;，谷歌近期已向 Pixel 设备推送 Android 16 QPR1 更新，但迟迟未按惯例在 48 小时内同步开放 AOSP 源码，引发第三方 ROM 开发者担忧。&lt;/p&gt; 
&lt;p&gt;Android 16 QPR1（Quarterly Platform Release 1）是谷歌 Android 系统最近发布的一个更新，其中包含了 Material 3 Expressive 等新特性。&amp;nbsp;通常在发布类似更新后，谷歌会在 1-2 天内将对应的源代码上传至 AOSP，方便第三方定制 ROM 的开发人员同步或使用这些更新特性。&lt;/p&gt; 
&lt;p&gt;但截至目前，一周过去了，AOSP 上的源代码还没能找到。 谷歌对外表示，源代码会在 「接下来几周内」（「in the coming weeks」）发布，但未给出更具体的时间表也没有解释延迟原因。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-f069cf8e6cb319a979b8b5816b2b9f523c7.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;定制 ROM 社区（如 LineageOS 等）以及使用 AOSP 的开发者对此延迟表示担忧，因为他们依赖谷歌的及时源代码发布来更新他们的系统、添加新特性或修复兼容性问题。&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/news/341310/google-android-development-aosp" target="_blank"&gt;谷歌在年初已经将部分 Android 的开发流程 「完全私有化」&lt;/a&gt;（即不再在公共视线中进行部分开发）以简化流程。虽然该公司曾多次公开承诺 AOSP 不会取消，但这些动作加剧了对其未来战略走向的猜测。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371641/android-16-qpr1-source-code-delay</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371641/android-16-qpr1-source-code-delay</guid>
      <pubDate>Thu, 11 Sep 2025 11:31:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>宇树科技创始人王兴兴：AI 时代，小组织的爆发力会越来越强</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;9 月 11 日，宇树科技创始人兼 CEO 王兴兴出席了 2025 外滩大会，这也是宇树官宣 IPO 计划后王兴兴首次公开发声。&lt;/p&gt; 
&lt;p&gt;他认为&lt;span&gt;AI 时代的组织管理是一门新课题。王兴兴表示，宇树科技是一家以硬件为主要产品的公司，随着业务快速发展，人员规模更大之后，可能会带来协作效率的降低，需要花时间探索更高效的组织管理方式。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0911/192019_miGZ_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="font-family:-apple-system,BlinkMacSystemFont,&amp;quot;Apple Color Emoji&amp;quot;,&amp;quot;Segoe UI Emoji&amp;quot;,&amp;quot;Segoe UI Symbol&amp;quot;,&amp;quot;Segoe UI&amp;quot;,&amp;quot;PingFang SC&amp;quot;,&amp;quot;Hiragino Sans GB&amp;quot;,&amp;quot;Microsoft YaHei&amp;quot;,&amp;quot;Helvetica Neue&amp;quot;,Helvetica,Arial,sans-serif"&gt;尽管存在挑战，但王兴兴对未来依旧十分乐观，他认为，现在创新创业的门槛已经大幅降低，年轻创新者迎来了好时代。真正可以用 AI 工具去实现新创意，并且在 AI 时代，小组织的爆发力会越来越强。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;王兴兴还提到：「顶尖的 AI 人才肯定是缺的，我相信这是每个大公司共同的渴求。」&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371637</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371637</guid>
      <pubDate>Thu, 11 Sep 2025 11:21:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>AgentFly —— 基于记忆增强的在线强化学习框架</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                        
                                                                                    &lt;p&gt;AgentFly 是基于记忆增强的在线强化学习框架，通过记忆库存储经验轨迹并利用神经案例选择策略实现 LLM 代理的持续适应能力，无需对底层 LLM 参数进行微调。&lt;/p&gt;

&lt;p&gt;该方法将决策过程建模为记忆增强的马尔可夫决策过程（M-MDP），通过非参数或参数化记忆模块存储过往经验，并基于软 Q 学习优化案例检索策略。&lt;/p&gt;

&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-070739b87c472bf439392dc3bc00bb25ddc.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-3e6c7c875e8356c92b9a62c600cca2f9f9e.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

&lt;p&gt;实验表明，该方法通过记忆库的持续更新实现高效在线学习，在复杂工具调用和多轮推理任务中展现出显著优势，为构建具备持续学习能力的通用型 LLM 代理提供了新范式。&lt;/p&gt;

                                                                    &lt;/div&gt;
                                                                </description>
      <link>https://www.oschina.net/p/agentfly</link>
      <guid isPermaLink="false">https://www.oschina.net/p/agentfly</guid>
      <pubDate>Thu, 11 Sep 2025 11:18:00 GMT</pubDate>
    </item>
    <item>
      <title>小米 Kaldi 团队开源零样本语音合成模型模型 ZipVoice</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;近日，小米集团新一代 Kaldi 团队发布了基于 Flow Matching 架构的 ZipVoice 系列语音合成（TTS）模型——ZipVoice（零样本单说话人语音合成模型）与 ZipVoice-Dialog（零样本对话语音合成模型）。&lt;/p&gt; 
&lt;p&gt;作为 zipformer 在语音生成任务上的应用和探索，ZipVoice 解决了现有零样本语音合成模型的参数量大、合成速度慢的痛点，在轻量化建模和推理加速上取得了重要突破。ZipVoice-Dialog 则解决了现有对话语音合成模型在稳定性和推理速度上的瓶颈，实现了又快又稳又自然的语音对话合成。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-207702be1087c5d602185a495fa12acd620.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;ZipVoice 系列的模型文件、训练代码和推理代码以及 6.8k 小时的语音对话数据集 OpenDialog 已全部开源：https://github.com/k2-fsa/ZipVoice&lt;/p&gt; 
&lt;p&gt;Zipvoice 论文：https://arxiv.org/pdf/2506.13053&lt;/p&gt; 
&lt;p&gt;样例体验请访问：https://zipvoice.github.io&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371625</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371625</guid>
      <pubDate>Thu, 11 Sep 2025 10:56:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>AI 编程公司 Replit 融资 2.5 亿美元，估值达 30 亿美元</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;AI 编程公司 Replit &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Freplit.com%2Fnews%2Ffunding-announcement" target="_blank"&gt;宣布&lt;/a&gt;完成了一轮 2.5 亿美元融资，使其估值达到了约 30 亿美元，较 2023 年上一轮融资增长近三倍。在过去一年中，Replit 的年收入从 280 万美元飙升至 1.5 亿美元，目前。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;公告称，Replit 的用户数量已达到 4000 万。此次融资正值该公司年化收入在不到一年的时间里从 280 万美元增长至 1.5 亿美元之际（增幅超过 50 倍）。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="291" src="https://oscimg.oschina.net/oscnet/up-37388b8353eaedc1a8426cc5a02204bd9d6.png" width="600" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;此次融资由 Prysm Capital 主导，参与投资的还有 Amex Ventures 和谷歌的 AI Futures Fund。此外，Replit 的现有投资者，包括 Y Combinator、Craft Ventures、Andreessen Horowitz、Coatue Management 和 Paul Graham 等也参与了这一轮融资。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;与此同时，Replit 还宣布推出了 Agent 3，并声称是其迄今为止自主性最强的 agent。Agent 3 的自主性比之前的版本提高了十倍，能够测试和修复代码，并构建自定义代理和工作流，从而能够自动执行任何类型的复杂或重复性任务，而不仅仅是软件工程。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371618</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371618</guid>
      <pubDate>Thu, 11 Sep 2025 10:31:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>聚焦结构化注意力，探索提升多模态大模型文档问答性能</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;blockquote&gt; 
 &lt;p&gt;作者：vivo 互联网算法团队&lt;/p&gt; 
 &lt;p&gt;本文聚焦多模态大语言模型（MLLMs）在文档问答（DocQA）任务中的性能提升，提出无需改动模型架构或额外训练的结构化输入方法，通过保留文档层次结构与空间关系（如标题、表格、图像位置）优化理解能力。研究发现，传统无结构 OCR 输入导致注意力分散，性能下降，而 LaTeX 范式结构化输入显著提升表现。注意力分析揭示其诱导"结构化注意力"，减少无关区域干扰，聚焦语义核心。在 MMLongBench、PaperTab 等四个数据集上验证，该方法尤其在复杂图表任务中效果显著，为智能文档处理与自动问答提供高效的解决方案。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;本文提供配套演示代码，可下载体验：&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fvivo%2FStructureMatters" target="_blank"&gt;Github |&lt;/a&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fvivo%2FStructureMatters" target="_blank"&gt;StructureMatters&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;一、引言&lt;/h1&gt; 
&lt;p&gt;多模态大语言模型（Multimodal Large Language Models, MLLMs）蓬勃发展的今天，文档理解（Document Understanding）作为一项涉及文本、图表和图像的复杂任务，依然面临诸多挑战。如何高效整合多源信息、理解文档的层次结构，成为提升 MLLMs 性能的关键问题。研究发现了一种无需修改模型架构或额外训练的新方法：仅通过结构化输入提升 MLLMs 在文档问答（DocQA）任务中的表现，同时通过注意力分析实践探寻结构化输入带来性能提升的深层原因。&lt;/p&gt; 
&lt;h1&gt;二、文档理解的核心挑战&lt;/h1&gt; 
&lt;p&gt;文档理解要求模型同时处理文本、图表、图像等多模态信息，并准确回答问题。然而，现有方法多依赖于扩展上下文窗口或优化检索增强生成（RAG），忽略了一个关键问题：输入格式如何影响模型的理解能力？&lt;/p&gt; 
&lt;p&gt;研究发现，传统的无结构 OCR 文本输入在某些 case 下未提升模型性能，反而因注意力分散和结构丢失导致性能下降。例如，在 MMLongBench 数据集上，加入无结构 OCR 文本后，模型准确率从 0.389 下降至 0.370。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-12cc41cc30fcd9ed83f9ed46f2f8fc29448.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;当前主流多模态大模型已经具备处理多模态信息的能力，其中 Qwen2.5-VL-7B-Instruct，Phi-3.5-Vision-Instruct，SmolVLM-Instruct 等在多个多模态任务上达到了 SOTA，但在文档阅读任务中仍表现不佳。以往文档阅读模型通过训练得到专用模型来进行文档阅读理解，并基于文档回答问题，如 mPLUG-DocOwl，Textmonkey 等模型。但随着 RAG 的快速发展，像 ColBERT 和 ColPali 这样的 RAG 方法在分别检索文本或视觉信息方面已被证明有效，当前主流方法通常基于 RAG 检索证据页面，然后将证据信息直接输入多模态大模型中以便回答 DocQAs。但当问题需要整合来自两种模态的信息时，它们通常表现不佳。&lt;/p&gt; 
&lt;p&gt;随着通用大模型的发展和 AGI 概念的普及，如何直接利用通用多模态大模型达到目的，不额外进行训练成为研究热点。改变输入结构能否帮助多模态大模型进行高效推理为本文探讨的重点。本文致力于探寻通用多模态大模型在何种条件下能够具有更加高效的推理理解能力，能否具备在 trainning free 的条件下达到较高的多元素文档理解能力。&lt;/p&gt; 
&lt;h1&gt;三、创新方法：结构化输入与注意力分析&lt;/h1&gt; 
&lt;p&gt;为解决这一问题，提出了一种基于 LaTeX 范式的结构保留方法。该方法通过保留文档的层次结构和空间关系（如标题、表格、图像的位置），从而为模型提供更清晰的语义引导。&lt;/p&gt; 
&lt;p&gt;具体流程包括：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;结构化编码&lt;/strong&gt;：将 OCR 文本和图像输入 MLLMs，提示模型尽可能保留图表、表格和文本的结构，生成 LaTeX 格式的表示。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;联合输入&lt;/strong&gt;：将结构化文本与原始图像一同输入模型，指导其在回答问题时关注关键区域。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;注意力分析&lt;/strong&gt;：通过比较仅图像输入、图像加无结构文本、图像加结构化文本三种情况的注意力分布，发现结构化输入显著减少了注意力浪费，引导模型聚焦于语义相关的文本和图像区域。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;实验结果表明，该方法在多个文档理解基准数据集上显著提升了模型性能。例如，在 MMLongBench 上，QWEN2.5-VL-7B-INSTRUCT 的准确率从 0.389 提升至 0.435；在 PaperTab 数据集上，准确率提升高达 20%，得益于 LaTeX 格式对表格和图表的精准解析。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-51b199d7da4f2b8e482da26b791c77d6d76.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h1&gt;四、通过注意力机制进行深层原因探究&lt;/h1&gt; 
&lt;p&gt;进一步的，通过注意力分析揭示了结构化输入的内在机制。无结构文本输入导致模型注意力分布散乱，浪费在图像边缘或无关区域；而结构化文本添加了结构化约束，诱导模型形成"结构化注意力"模式，聚焦于文档的核心内容（如图表、文本块）。例如，在一个案例中，模型需根据图表回答"西德居民对美俄关系的看法比例"。无结构输入下，注意力分散在图像空白区域；结构化输入后，注意力集中于图表和相关文本，显著提高答案准确性。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-c1cf6c7f984dc7670fa8d60b87bd138c1bf.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;结构化输入帮助减少 MLLMs 对于图片边界 token 的关注度，提高了模型对于文章主体部分的注意力得分。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-4266ab22bf20fc556f2d3de92b379f90a11.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;具体实例分析，证明结构化输入的重要意义。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-8d93f9bca6600aaabfd3cab20c1a73d141c.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h1&gt;五、实验验证与数据支持&lt;/h1&gt; 
&lt;p&gt;在四个文档理解基准数据集（MMLongBench、LongDocUrl、PaperTab、FetaTab）上测试 4 种 MLLMs 模型（如 QWEN2-VL-7B-INSTRUCT、Phi-3.5-Vision-Instruct）。结果显示，结构化输入在所有数据集上均提升了模型性能，尤其在包含复杂图表的 PaperTab 数据集上效果显著。消融实验进一步证明，仅用结构化文本或仅用图像的性能均低于两者结合，验证了结构化输入与图像联合使用的必要性。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-41cf00ff446e8353ac63034f7b316b625ac.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-b172ced302ebcd0de4814ef3027cb3a7f41.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h1&gt;六、总结与展望&lt;/h1&gt; 
&lt;p&gt;实践研究揭示了输入格式对 MLLMs 文档理解能力的关键影响，提出了一种简单而高效的结构化输入方法。未来可进一步探索更先进的结构提取技术或设计注意力控制插件，以进一步释放 MLLMs 在文档理解中的潜力。该研究提供了一种无需重训模型即可提升性能的实用方案，适用于智能文档处理、自动问答等场景。在没有额外训练和架构修改的前提下，通过简单的结构化文本输入，可以提升现有多模态大模型在文档理解任务中的表现。此项研究可以帮助用户分析、工作解析等场景中更准确地提取信息，提升工作效率。同时，RAG（检索增强生成）系统也能结合结构化输入来降低信息检索中的噪声，从而更高效地利用检索到的证据页面，为未来文档处理与分析提供了新的实践路径。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/vivotech/blog/18691398</link>
      <guid isPermaLink="false">https://my.oschina.net/vivotech/blog/18691398</guid>
      <pubDate>Thu, 11 Sep 2025 10:08:00 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>支付宝推出国内首个 AI 付</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;支付宝宣布推出国内首个「AI 付」服务，面向 AI 时代为智能体提供支付服务，并率先在瑞幸咖啡的 AI 点单助手「Lucky AI」上线，用户可在瑞幸支付宝小程序或瑞幸咖啡 App，用说话的方式完成下单并支付。这也是行业首次打通智能体内的下单与支付全链路，实现无缝的 AI 服务体验。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;以点咖啡为例，此前用户通过 AI 智能体下单，仍需跳转至支付页面再予手动确认。现在，用户在瑞幸支付宝小程序或瑞幸 App 唤起「Lucky AI」，不仅可以说句话点咖啡，还可以直接说句「下单」，完成身份核验后即支付。整个过程用户无需离开 AI 对话界面，像日常和店员聊天一样简单自然。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img height="496" src="https://oscimg.oschina.net/oscnet/up-ed3ccbc5548251ac7a86f54e281b209e58a.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;蚂蚁集团数字支付事业群首席技术官朱林表示：支付宝始终致力于通过产品创新，解决支付中存在的安全信任和便捷两大课题，此次为智能体提供「AI 下单+支付」解决方案，旨在服务好用户、产业和时代的需求，做好激活 AI 产业生态的一把钥匙。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;朱林认为，伴随 AI 产业的爆发式增长和智能终端的普及，预计未来 5 年内更自然的新交互支付占比或超 50%，多样化的智能设备支付将增长 10 倍，而更智能的 AI 支付市场规模可达万亿级。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;支付宝正着力打造面向 AI 时代的支付服务，构筑融入 AI 交互服务的「支付新基建」。目前已推出包括:&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;国内第一个「支付 MCP Server」，让 AI 智能体可一键接入支付宝支付服务;&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;国内第一个「AI 打赏」服务，为在 AI 智能体内有收取赞赏、小费等需求的开发者提供便捷收款能力;&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;国内第一个「AI 订阅付费」功能，支持开发者在智能体中便捷接入，按服务次数或时长定价并收款;&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;全球第一个智能眼镜支付「看一下支付」服务，用户佩戴眼镜看商家收款码或收款设备即可支付;&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;国内第一个智能体支付服务「支付宝 AI 付」，用户说说话可点单又能支付。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371606</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371606</guid>
      <pubDate>Thu, 11 Sep 2025 09:41:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>微软再次提醒 Windows 将逐步淘汰 VBScript，分三阶段进行</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;近期微软再次提醒用户，VBScript 将被逐步淘汰，并分享了相关建议。&lt;/p&gt; 
&lt;p&gt;VBScript，即 Visual Basic Script，是微软在近三十年前开发的脚本语言，曾默认包含在 Windows 中，主要用于自动化任务，不过近年来，该语言也常被黑客利用。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-9c905bf5e0ab3ab7646889588b6b63a0c65.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;早在 2023 年 10 月，微软就&lt;a href="https://www.oschina.net/news/261322/microsoft-deprecated-vbscript-in-window" target="_blank"&gt;宣布&lt;/a&gt;将在未来的 Windows 版本中淘汰 VBScript，并在 2024 年 5 月发布了详细的时间表，如今微软又提供了更多细节。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-589957b28bdb1eaa90fcbc1849da2ea8216.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;VBScript 主要用于通过执行外部.vbs 脚本或引用正则表达式库来扩展 Office 应用程序的功能，不过，随着 VBScript 的淘汰，这些开发人员和用处将受到影响。&lt;/p&gt; 
&lt;p&gt;微软将淘汰 VBScript 分为三个阶段，第一阶段已经启动，可能会持续到 2026 年或 2027 年。在此期间，VBScript 将作为「按需功能」（FOD）默认启用，现有项目不会受到影响。&lt;/p&gt; 
&lt;p&gt;当第二阶段开始时，该 FOD 将被禁用，最后，在第三阶段，VBScript 将被完全移除，这将直接影响之前提到的用处。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;微软建议客户使用 Office 2508 版本中包含的 RegExp 类，并默认使用 Microsoft 365 订阅，这将使开发人员能够在 VBScript 中继续使用 RegExp。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;微软还建议客户通过 Microsoft 365 升级到最新版本的 Office，以利用新的 RegExp 实现，这将允许开发人员在 Visual Basic 编辑器（VBE）中使用该功能，而无需添加 vbscript.dll。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371602</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371602</guid>
      <pubDate>Thu, 11 Sep 2025 09:28:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Stability AI 发布专业音频生成模型 Stable Audio 2.5</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Stability AI 推出专业音频生成模型&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fstability.ai%2Fnews%2Fstability-ai-introduces-stable-audio-25-the-first-audio-model-built-for-enterprise-sound-production-at-scale" target="_blank"&gt;Stable Audio 2.5&lt;/a&gt;，借助 Adversarial Relativistic-Contrastive（ARC）后训练技术，实现复杂音乐结构的高效生成。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-e2eb31cef809fb7d09a49502cdb9a67f522.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在英伟达 H100 GPU 上，模型可在 2 秒内完成最长 3 分钟的音频创作，支持前奏、发展、尾声等多段落结构，并集成音频修复功能，允许用户上传现有音频进行续写。&lt;/p&gt; 
&lt;p&gt;该模型同步推出移动端轻量版 Stable Audio Open Small，可在手机端 7 秒内生成 11 秒立体声。为确保商用合规，Stable Audio 2.5 基于 licensed 数据集训练，并通过版权识别系统限制用户上传版权受限内容。&lt;/p&gt; 
&lt;p&gt;Stability AI 希望该技术能应用于广告、零售、品牌音效等多个领域，与 WPP 旗下的音效品牌代理机构 Amp 合作，为大型客户提供一致的音频识别服务。&lt;/p&gt; 
&lt;p&gt;Stability AI 的音频团队还可以根据公司的音效库调整模型，打造独特的音频标识。Stable Audio2.5 将通过 WPP Open 平台面向 WPP 的全球客户开放。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371600/stability-ai-introduces-stable-audio-2-5</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371600/stability-ai-introduces-stable-audio-2-5</guid>
      <pubDate>Thu, 11 Sep 2025 09:21:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>字节跳动发布 Seedream 4.0 图像创作模型，并免费上线豆包</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="text-align:left"&gt;近日，字节跳动 Seed 团队宣布推出豆包图像创作模型 Seedream 4.0。该模型支持文生图、图像编辑及多图参考等功能，多模态生图效果、速度和可用性在专业评测中达到业界领先水平。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;目前，Seedream 4.0 已在豆包 App、即梦 AI、扣子等产品正式上线，用户可以免费体验。该模型也已通过火山引擎开放给企业客户。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;Seed 团队表示，「Seedream 4.0 不仅仅是一个图像生成模型，更是一个具备知识和思考能力的多模态创意引擎。」&lt;/p&gt; 
&lt;p style="text-align:left"&gt;测试案例显示，Seedream 4.0 不仅能理解物理规律与时间约束、三维空间等复杂语境，还能在解谜、填字、续写漫画等任务中保持风格一致与细节精致，逻辑推理和创意生成能力表现出色。&lt;/p&gt; 
&lt;p style="text-align:center"&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//3284d12cbee79faafba2468e30b8a639.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="text-align:center"&gt;Seedream 4.0 测试效果（Prompt：六个小时后这个图片的场景是什么样子）&lt;/p&gt; 
&lt;p style="text-align:left"&gt;据介绍，Seedream 4.0 可灵活支持文本、图像的组合输入，抽取不同图片元素进行创作，还可一次生成角色连贯、风格统一的组图，实现表情包、连环画等各类创意玩法。&lt;/p&gt; 
&lt;p style="text-align:left"&gt;同时，该模型支持高度自由的艺术风格迁移，最高可生成 4K 分辨率的商用级图像，并具备出色的文字渲染能力，还可处理基础的公式、表格、统计图等复杂排版，广泛适用于教育、电商、广告设计、影视后期等应用场景。&lt;/p&gt; 
&lt;p style="text-align:center"&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//808dcef2ea27c4e500493eeb6ed82c6e.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="text-align:center"&gt;Seedream 4.0 测试效果（Prompt：参考图 2 的风格，将图 1 做风格转换）&lt;/p&gt; 
&lt;p style="text-align:left"&gt;基于高效的模型架构和多层推理加速，Seedream 4.0 实现了高质量和高效生成的平衡。Seed 官网显示，Seedream 4.0 在各维度专业评测的综合表现排名业界前列，视觉美感、速度等关键指标成绩突出，并展现出较强的可靠性。&lt;/p&gt; 
&lt;p style="text-align:center"&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//7f85c62003df2d8193e4830a17209985.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="text-align:center"&gt;图：MagicBench 「文生图」及「单图编辑」人工评测基准（数据来源：Seed 官网）&lt;/p&gt; 
&lt;p style="text-align:left"&gt;Seed 团队表示，图像创作正在从文生图进入多模态交互的新阶段，Seedream 4.0 已具备通用多模态创意引擎的雏形。团队将继续探索更实时的交互式生成体验，进一步深度融合多模态推理与世界知识，更好地帮助用户激发灵感、实现创意。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371598</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371598</guid>
      <pubDate>Thu, 11 Sep 2025 09:17:00 GMT</pubDate>
      <author>作者: 开源科技</author>
    </item>
    <item>
      <title>zebra-bpm 一款用户友好、快速上手的工作流系统</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#3c3c43; margin-left:0; margin-right:0; text-align:start"&gt;zebra-bpm 在 Flowable 基础上进行研发而成，其中拖拽式表单和可视化流程设计器，区别于传统工作流系统，传统工作流系统表单设计器和流程设计器晦涩难懂，对于普通企业用户使用门槛偏高，没有经过专业培训根本无从下手，需要相关专业人员辅助来创建流程。而本设计器界面简单，符合普通大众的思维逻辑，易于理解和上手使用。&lt;/p&gt; 
&lt;h3&gt;优势&lt;a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fzebra.zhanghongbin.xyz%2Fproduct%2Fworkflow.html%23%25E4%25BC%2598%25E5%258A%25BF" target="_blank"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;‌&lt;strong&gt;✓ 简化复杂的流程设计&lt;/strong&gt;‌&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;可视化的流程设计器，让流程设计更直观。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;‌&lt;strong&gt;✓ 符合用户操作习惯&lt;/strong&gt;‌&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;优化用户操作体验，快速掌握工作流设计工具。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;‌&lt;strong&gt;✓ 更灵活的与第三方系统对接&lt;/strong&gt;‌&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;提供众多接口，方便对接三方系统如：用户，部门，流转条件等。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#3c3c43; margin-left:0; margin-right:0; text-align:start"&gt;演示地址:&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fbpm.zhanghongbin.xyz%2F" target="_blank"&gt;http://bpm.zhanghongbin.xyz&lt;/a&gt;&lt;/p&gt; 
&lt;p style="color:#3c3c43; margin-left:0; margin-right:0; text-align:start"&gt;文档地址&amp;nbsp;http://zebra.zhanghongbin.xyz/product/workflow.html&lt;/p&gt; 
&lt;table cellspacing="0" style="-webkit-text-stroke-width:0px; background-color:#ffffff; border-collapse:collapse; box-sizing:border-box; color:#3c3c43; display:block; font-family:&amp;quot;LXGW WenKai Lite&amp;quot;,&amp;quot;Inter var experimental&amp;quot;,&amp;quot;Inter var&amp;quot;,-apple-system,BlinkMacSystemFont,&amp;quot;Segoe UI&amp;quot;,Roboto,Oxygen,Ubuntu,Cantarell,&amp;quot;Fira Sans&amp;quot;,&amp;quot;Droid Sans&amp;quot;,&amp;quot;Helvetica Neue&amp;quot;,sans-serif; font-size:16px; font-style:normal; font-variant-caps:normal; font-variant-ligatures:normal; font-weight:400; letter-spacing:normal; margin:20px 0px; orphans:2; overflow-x:auto; text-align:start; text-decoration-color:initial; text-decoration-style:initial; text-decoration-thickness:initial; text-transform:none; white-space:normal; widows:2; word-spacing:0px"&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;th style="background-color:var(--vp-c-bg-soft); text-align:left"&gt;用户&lt;/th&gt; 
   &lt;th style="background-color:var(--vp-c-bg-soft); text-align:left"&gt;账号&lt;/th&gt; 
   &lt;th style="background-color:var(--vp-c-bg-soft); text-align:left"&gt;密码&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td style="border-style:solid; border-width:1px"&gt;超级管理员&lt;/td&gt; 
   &lt;td style="border-style:solid; border-width:1px"&gt;admin&lt;/td&gt; 
   &lt;td style="border-style:solid; border-width:1px"&gt;admin123&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371595</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371595</guid>
      <pubDate>Thu, 11 Sep 2025 09:05:00 GMT</pubDate>
      <author>来源: 资讯</author>
    </item>
    <item>
      <title>​字节 Seed 推出全新 AgentGym-RL 框架</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;字节跳动 Seed 研究团队推出了名为 AgentGym-RL 的新框架，专注于通过强化学习训练 LLM 代理，使其能够进行多轮互动决策。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;该框架具有模块化和解耦的架构，提供了极高的灵活性和扩展性。AgentGym-RL 覆盖了多种真实场景，能够支持主流的强化学习算法，帮助代理全面提升其决策能力。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="372" src="https://oscimg.oschina.net/oscnet/up-6607f56f1b2f55c4cb23f3482def33cf8f5.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;为了进一步优化训练效果，研究团队还提出了一种名为 ScalingInter-RL 的训练方法。该方法通过阶段性调整交互次数，帮助代理在早期专注于掌握基本技能，随后逐渐增加交互次数，以鼓励更多样化的问题解决策略。这种探索与利用的平衡设计，有助于代理在面对复杂任务时保持稳定的学习和决策能力。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;在实验过程中，研究者们采用了 Qwen2.5-3B 和 Qwen2.5-7B 作为基础模型，评估了 AgentGym-RL 和 ScalingInter-RL 在五个不同场景中的表现。结果显示，使用 AgentGym-RL 的代理在 27 个任务中，表现优于多个商业模型。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;研究团队计划将整个 AgentGym-RL 框架，包括代码和数据集，开源，以支持更多研究者开发智能代理。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;AgentGym-RL 框架涉及的多种场景包括网络导航、深度搜索、数字游戏、体感任务和科学实验等，代理在这些场景中需具备强大的决策能力和适应能力，才能完成复杂的任务。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371591</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371591</guid>
      <pubDate>Thu, 11 Sep 2025 08:40:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Salesforce 开源深度研究 Agent：SFR-DeepResearch (SFR-DR)</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Salesforce &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2FCaimingXiong%2Fstatus%2F1965440617334685886" target="_blank"&gt;发布&lt;/a&gt;了开源深度研究 Agent：SFR-DeepResearch（SFR-DR）。该模型基于 OpenAI 的小型开源权重模型，通过强化学习进行训练，具备推理、搜索与代码执行能力，可自主完成深度研究任务。&lt;/p&gt; 
&lt;p&gt;SFR-DR-20B 版本仅依靠网页搜索、浏览器和 Python 解释器，在纯文本的 Humanity's Last Exam 基准测试中取得了 28.7% 的成绩。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-1e558bc3cb2f0faa6a5717b108de0484740.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;SFR-DR 亮点特性如下&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;核心能力&lt;/strong&gt;：基于强化学习（RL）训练的自主研究代理，能够独立推理、搜索和编程，完成深度研究任务。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;性能表现&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;SFR-DR-20B&lt;/strong&gt; 在 Humanity's Last Exam（纯文本）上取得 &lt;strong&gt;28.7%&lt;/strong&gt; 的成绩&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;仅依赖网络搜索、网页浏览和 Python 解释器&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;超越 OpenAI o3 DeepResearch 和 Kimi Researcher&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;训练方式&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;端到端 RL，从优化推理能力的基础模型开始训练&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;保留推理能力的同时提升研究执行能力&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;自主性&lt;/strong&gt;：无需预定义多代理工作流，可自主规划、推理、提出方案并执行行动&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;论文地址：&lt;em&gt;https://arxiv.org/abs/2509.06283&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371589</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371589</guid>
      <pubDate>Thu, 11 Sep 2025 08:33:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>香港科技大学（广州）&amp;点动科技行业智能体联合实验室签约仪式圆满举行</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#2c3e50; margin-left:0px; margin-right:0px; text-align:center"&gt;&lt;img alt="https://img2.danews.cc/upload/images/20250911/335597000129451c4b99c38ebaa51987.png" src="https://oscimg.oschina.net/oscnet//e243bde9227fd33846dcf43e9ba49f7f.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;2025 年 9 月 8 日，香港科技大学（广州）与广州点动信息科技股份有限公司（下文简称：点动科技）成功举行了行业智能体联合实验室签约暨揭牌仪式。&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;香港科技大学（广州）校长倪明选教授、署理副校长（研究）李世玮教授、协理副校长（知识转移）熊辉教授、点动科技董事长陈科斌先生、点动科技深圳公司总经理陈永洁女士、点动科技董事张培扬先生、智算中心总经理杨雪峰先生以及联合实验室技术管理委员会成员、人工智能学域师生代表出席了本次启动仪式。&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0px; margin-right:0px; text-align:center"&gt;&lt;img alt="https://img2.danews.cc/upload/images/20250911/00efd1cf1afe47160e9910ec586193fe.jpg" src="https://oscimg.oschina.net/oscnet//0e4e7d4a02895df18bd7c5a1c4579c06.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:center"&gt;&lt;img alt="https://img2.danews.cc/upload/images/20250911/a1b9ac73ebad1147fe14334994c23956.jpg" src="https://oscimg.oschina.net/oscnet//d3773435a576065839820477223e18a1.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;启动仪式现场&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;香港科技大学（广州）-点动科技行业智能体联合实验室，旨在依托双方优势，聚焦行业智能体领域的前沿技术研发，推动相关理论创新和应用落地，并积极培养高素质人才。实验室的成立，不仅标志着学校在推动研究与实践深度结合上迈出了坚实一步，更彰显了对交叉学科融合理念的积极践行与深入探索。&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;香港科技大学（广州）校长倪明选教授在致辞中谈到，学校自成立以来，始终致力于创新和交叉学科的发展，鼓励教师与企业开展合作，&lt;strong&gt;目前全校已设立 15 个校企联合实验室&lt;/strong&gt;。&amp;nbsp;此次联合实验室的成立，不仅有助于科研成果的产业化转化，也为学生提供了更多的实践机会。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:center"&gt;&lt;img alt="https://img2.danews.cc/upload/images/20250911/81f40384ad8fe10f8cfbf1eeb1a520e5.jpg" src="https://oscimg.oschina.net/oscnet//35337ec52cdbf3341d93028cd47a6ee2.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;香港科技大学（广州）校长，倪明选教授&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;广州点动信息科技股份有限公司董事长&amp;amp;总裁陈科斌先生在发言中提出，本次合作标志着人工智能领域基础算力与前沿研究的深度融合。联合实验室将围绕量化金融、数字文娱与智能调度三大战略性课题展开攻关，充分依托香港科技大学（广州）在前沿科技领域的创新优势，加速核心技术突破与产业转化，切实服务社会高质量发展。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:center"&gt;&lt;img alt="https://img2.danews.cc/upload/images/20250911/9b9746bf77e32c6f7ab4be61a7f32786.jpg" src="https://oscimg.oschina.net/oscnet//eeb06cd2a0691b1940013b5c55160fe4.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;广州点动信息科技股份有限公司董事长&amp;amp;总裁，陈科斌先生&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;香港科技大学（广州）-点动科技行业智能体联合实验室主任熊辉教授在发言中形象地称联合实验室是「 AI 装修队」，能让年轻老师发挥知识力量，并诚邀合作伙伴携手共进，实现资源互补，推动项目规模化、高质量发展。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:center"&gt;&lt;img alt="https://img2.danews.cc/upload/images/20250911/50fd0a81584472e270821f4e7dddb8ca.jpg" src="https://oscimg.oschina.net/oscnet//74ad049f14755f16a9425e0ad2283acf.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;联合实验室主任，熊辉教授&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;随后，进入期待已久的签约及揭牌仪式，在全场嘉宾的见证下，双方正式签署合作协议，标志着联合实验室的正式成立。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:center"&gt;&lt;img alt="https://img2.danews.cc/upload/images/20250911/ef091a3ce3e4df72572c2df5b4886dff.jpg" src="https://oscimg.oschina.net/oscnet//fa2e7288b8492592dde810e224d1088e.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;签约及揭牌仪式仪式现场&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;多位领导和嘉宾共同为联合实验室揭牌，随着全场倒计时的结束，开启了联合实验室的新篇章，香港科技大学（广州）-点动科技行业智能体联合实验室正式揭幕！这一仪式不仅象征着开启校企合作的新序幕，更是通往未来技术创新的窗口。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:center"&gt;&lt;img alt="https://img2.danews.cc/upload/images/20250911/d6f2434b538f024f5724f6a84b9feb6d.jpg" src="https://oscimg.oschina.net/oscnet//ad1117d54c26c4260f5f6fe48e0b765f.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;签约及揭牌仪式仪式现场&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;香港科技大学（广州）校长倪明选教授和广州点动信息科技股份有限公司董事长&amp;amp; 总裁陈科斌先生为联合实验室技术管理委员会成员颁发聘书。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:center"&gt;&lt;img alt="https://img2.danews.cc/upload/images/20250911/7caa75654b24a333f0602f3213db6189.jpg" src="https://oscimg.oschina.net/oscnet//25570379565719828d20f36d678e7dfe.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;颁发聘书&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;仪式圆满结束，活动最后进入了青年学者论坛环节，联合实验室成员袁子轩助理教授（张译助理教授代为讲解）、于佳冬助理教授、岳玉涛副教授、刘李助理教授、王泽宇助理教授、杨萌林助理教授，分享了他们在人工智能和物联网领域的最新研究成果。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:center"&gt;&lt;img alt="https://img2.danews.cc/upload/images/20250911/5792908e2187918dd7a8b87ce56565ea.jpg" src="https://oscimg.oschina.net/oscnet//41306b6555cd6b9e3bea9eff78911364.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;张译助理教授&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;《自然语言处理在量化金融里的应用》&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;张译指出，量化投资以数据为燃料、算力为引擎，AI 与大模型的接入把信号维度从「万」推往「亿」，以海量数据为基、靠统计模型追求风险可控收益，兼具高度分散、策略长期稳定等特点，可借 AI 处理各类数据信号。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:center"&gt;&lt;img alt="https://img2.danews.cc/upload/images/20250911/9051c590a4d358fedd40d8a43b34a1a0.jpg" src="https://oscimg.oschina.net/oscnet//2a46f18eb324c459a2abf6770b60c73c.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;于佳冬助理教授&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;《云边端协同:计算与通信智能资源分配研究》&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;于佳冬分享，其题组聚焦数据中心异构算力调度，以「让算力像电力一样即取即用」为愿景，用强化学习在边端协同的异构、高动态、受限环境中做序贯决策，统一优化通信、计算与多目标权衡，已在多模态 AI 训练与 VR 渲染场景验证，显著提效。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:center"&gt;&lt;img alt="https://img2.danews.cc/upload/images/20250911/e13ff6df53ef81ad3cff381f72ba5cea.jpg" src="https://oscimg.oschina.net/oscnet//293cc339e8187874c620002320105687.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;岳玉涛副教授&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;《面向能够「懂人」和「拟人」的情感陪伴多模态智能体》&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;岳玉涛分享其团队深耕多模态领域，工作涉及多模态在情绪智能中的应用、情感互动内容生成、多模态情绪感知系统研发等方向，相关成果可应用于艺术创作、心理治疗、医疗、公安及人车行为预测等多个领域。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:center"&gt;&lt;img alt="https://img2.danews.cc/upload/images/20250911/6f0417df1290c78dc0dc499c9485b5c9.jpg" src="https://oscimg.oschina.net/oscnet//427b413b05031df5bfddbd72fe82d139.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;刘李助理教授&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;《情智一体的视听内容生成研究进展》&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;刘李分享，现有语音生成在情感感染力和拟人化方面不足，团队开发多层系统&lt;strong&gt;，目标是生成像真人一样的语音对话&lt;/strong&gt;；提出自动评估系统，展示了根据图片、文本生成有声读物和对话的效果。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:center"&gt;&lt;img alt="https://img2.danews.cc/upload/images/20250911/b9c1e5a6928ca130176da727fd6a5200.jpg" src="https://oscimg.oschina.net/oscnet//69a0979c6621649c222f32752d258cf3.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;王泽宇助理教授&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;《智能三维数字人的保真重建与可控生成》&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;王泽宇分享，其团队在智能三维数字人领域取得重要突破，&lt;strong&gt;研发出 SplattingAvatar、HeadEvolver 等创新方案&lt;/strong&gt;。这些成果解决了三维属性兼容编辑、动态渲染等难题，支持艺术家直观操作，推动数字资产创作与三维动画发展，为该领域研究提供新方向。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:center"&gt;&lt;img alt="https://img2.danews.cc/upload/images/20250911/5996133cf75d0cff3d2bd82d80606773.jpg" src="https://oscimg.oschina.net/oscnet//5ee1241e889768049f2258caeae4cd0d.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;杨萌林助理教授&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;《个性化推荐与社交智能化匹配 》&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;杨萌林在分享中重点介绍了&amp;nbsp;AI 宠物项目的个性化推荐工作，并表达了与点动深化合作的期待。他谈到，社交场景序列推荐方面，用户与宠物的行为序列对个性化推荐的指导意义；并讨论到结合大模型如何筛选最佳结果，对产品功能垂直化起到重要作用。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0; text-align:center"&gt;&lt;img alt="https://img2.danews.cc/upload/images/20250911/1ceee3424990260776c1054226c6c4e5.jpg" src="https://oscimg.oschina.net/oscnet//578edd7a3a0e9ec7fef547c2fd6b9968.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:center"&gt;全体大合照&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;到此联合实验室签约仪式暨揭牌仪式就画上了圆满的句号，这场仪式不仅是双方产学研协同的全新起点，也标志着高校前沿学术智慧与产业实践需求的深度耦合正式启航，更打破了 AI 技术从 「理论研究」 到 「场景落地」 的壁垒。期待未来双方以实验室为纽带，共同为人工智能推动区域产业高质量发展注入持久活力，书写产学研协同创新的精彩新篇章！&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;活动主办：香港科技大学（广州）&lt;/p&gt; 
&lt;p style="color:#2c3e50; margin-left:0; margin-right:0; text-align:left"&gt;广州点动信息科技股份有限公司&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371573</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371573</guid>
      <pubDate>Thu, 11 Sep 2025 07:58:00 GMT</pubDate>
      <author>作者: 开源科技</author>
    </item>
    <item>
      <title>腾讯开源图检索增强生成框架 Youtu-GraphRAG</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;腾讯优图实验室开源了 Youtu-GraphRAG，这是一个全新的图检索增强生成框架，旨在通过大语言模型+RAG 模式，将知识组织成图谱，再交给大语言模型进行检索和推理，从而提高模型在处理复杂问答任务时的准确性和可追溯性。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;Youtu-GraphRAG 特别适用于企业知识库问答、科研文档解析、个人知识管理等知识密集型场景。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;Youtu-GraphRAG 通过三大创新实现了从图构建到索引、再到检索的垂直统一和认知闭环。首先，它采用了四层知识树结构，将知识拆解成属性、关系、关键词和社区四个层次，使得大模型在回答问题时能够沿着知识树定位信息，推理路径清晰可见。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;其次，社区检测升级不仅关注「谁和谁有关」，还结合语义理解「为什么它们有关」，生成简明摘要，帮助用户快速抓住问题本质。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;最后，智能迭代检索机制允许用户提出复杂问题时，将其拆解成多个子问题并行检索，并通过迭代反思机制对结果进行补充和修正，最终给出更完整、更可靠的回答。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="314" src="https://oscimg.oschina.net/oscnet/up-ab798039306f65590d7249203cb4394d0d9.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;Youtu-GraphRAG 在实践检验中表现出色。在六个&lt;span&gt;权威&lt;/span&gt;基准测试中，&lt;span&gt;最高&lt;/span&gt;可节省 90.71% 的 Token 成本，复杂推理任务的准确率&lt;span&gt;最高&lt;/span&gt;提升 16.62%。此外，该框架支持中英文双语，跨领域应用无需重构，具有很高的灵活性。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;使用 Youtu-GraphRAG 非常简单，只需四步即可上手。首先，通过命令行获取项目代码。其次，进行环境配置，包括获取远程调用模型的凭证 API key 并创建配置文件。然后，一键部署项目。最后，通过 curl 命令体验交互。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371566</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371566</guid>
      <pubDate>Thu, 11 Sep 2025 07:30:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>字节跳动发布开源多模态模型 Mini-o3</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;字节跳动发布开源多模态模型 Mini-o3，通过扩展推理模式和交互轮次提升视觉搜索性能，在复杂场景中实现显著突破。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0911/151240_xzZ9_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;https://mini-o3.github.io/&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;据介绍，Mini-o3 是一个完全开源的多模态模型，专为「边看边想」的视觉搜索任务设计。它通过强化学习将工具调用次数扩展到数十轮，在 VisualProbe、V* Bench、HR-Bench、MME-Realworld 等基准上取得了 7B 量级的最佳成绩。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-a111377d25b371aba3ccf675445d406ca6b.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-71b6b52f2c739864eda55692e83e9bbe7ce.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;项目公开了训练代码、模型权重以及包含 4,500 条数据的 Visual Probe 数据集，允许研究者在非商业许可下复现 OpenAI o3 风格的深度推理行为。&lt;/p&gt; 
&lt;p&gt;Mini-o3 支持深度优先搜索、试错等多样化推理模式，测试时交互轮次可扩展至 32 轮以上，准确率随轮次增加显著提升（如 VisualProbe-Hard 任务准确率从 35.1% 提升至 48.0%）。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;核心创新&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;挑战性数据集构建&lt;/strong&gt;：推出 VisualProbe 数据集，包含高分辨率图像、小目标和密集干扰物场景，强制模型进行多轮探索。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;迭代数据收集&lt;/strong&gt;：通过冷启动数据生成多样化推理轨迹，覆盖回溯、假设验证等策略，解决预训练模型缺乏多轮交互能力的问题。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Over-Turn Masking 策略&lt;/strong&gt;：在强化学习中避免对超轮次响应的惩罚，支持模型深度探索，训练时轮次上限设为 6 轮，测试时可扩展至 32 轮以上。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;应用案例&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-32f28f4c28db5eb7a911b5d6fe714e2b11b.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/371562</link>
      <guid isPermaLink="false">https://www.oschina.net/news/371562</guid>
      <pubDate>Thu, 11 Sep 2025 07:18:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>🔥🔥AI 能打造盲人的第三只眼？</title>
      <description/>
      <link>https://www.oschina.net/ai-creation/details/2194</link>
      <guid isPermaLink="false">https://www.oschina.net/ai-creation/details/2194</guid>
      <pubDate>Thu, 11 Sep 2025 07:03:00 GMT</pubDate>
    </item>
    <item>
      <title>🔥🔥智能植物收割？能割韭菜不？</title>
      <description/>
      <link>https://www.oschina.net/ai-creation/details/2195</link>
      <guid isPermaLink="false">https://www.oschina.net/ai-creation/details/2195</guid>
      <pubDate>Thu, 11 Sep 2025 07:03:00 GMT</pubDate>
    </item>
  </channel>
</rss>
