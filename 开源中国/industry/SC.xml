<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - industry - 简体中文</title>
    <link>https://www.oschina.net/news/industry</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news/industry" rel="self" type="application/rss+xml"/>
    <description>已对该 RSS 进行格式化操作：中英字符之间插入空格、使用直角引号、标点符号修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-cn</language>
    <lastBuildDate>Thu, 12 Jun 2025 12:41:53 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>Stalwart —— 一体化邮件和协作服务器</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                            &lt;p style="text-align:start"&gt;&lt;span&gt;&lt;span&gt;&lt;span style="color:#1f2328"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style="background-color:#ffffff"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;Stalwart&lt;/strong&gt;是一款开源邮件和协作服务器，支持 JMAP、IMAP4、POP3、SMTP、CalDAV、CardDAV 和 WebDAV，并具备丰富的现代功能。它采用 Rust 编写，安全、快速、健壮且可扩展。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p style="text-align:start"&gt;&lt;span&gt;&lt;span&gt;&lt;span style="color:#1f2328"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style="background-color:#ffffff"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;主要特点：&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;具有完整协议支持的&amp;nbsp;&lt;strong&gt;电子邮件服务器：&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;JMAP：
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://datatracker.ietf.org/doc/html/rfc8621"&gt;JMAP 用于邮件&lt;/a&gt;服务器。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.ietf.org/archive/id/draft-ietf-jmap-sieve-22.html"&gt;用于 Sieve 脚本的 JMAP&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://datatracker.ietf.org/doc/html/rfc8887"&gt;WebSocket&lt;/a&gt;、&lt;a href="https://www.rfc-editor.org/rfc/rfc9404.html"&gt;Blob 管理&lt;/a&gt;和&lt;a href="https://www.rfc-editor.org/rfc/rfc9425.html"&gt;配额&lt;/a&gt;扩展。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;IMAP：
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://datatracker.ietf.org/doc/html/rfc9051"&gt;IMAP4rev2&lt;/a&gt;和&lt;a href="https://datatracker.ietf.org/doc/html/rfc3501"&gt;IMAP4rev1&lt;/a&gt;服务器。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://datatracker.ietf.org/doc/html/rfc5804"&gt;ManageSieve&lt;/a&gt;服务器。&lt;/li&gt;
&lt;li&gt;支持多种&lt;a href="https://stalw.art/docs/development/rfcs#imap4-and-extensions"&gt;扩展&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;POP3：
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://datatracker.ietf.org/doc/html/rfc1939"&gt;POP3&lt;/a&gt;&amp;nbsp;服务器。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://datatracker.ietf.org/doc/html/rfc2595"&gt;STLS&lt;/a&gt;和&lt;a href="https://datatracker.ietf.org/doc/html/rfc5034"&gt;SASL&lt;/a&gt;支持以及其他&lt;a href="https://datatracker.ietf.org/doc/html/rfc2449"&gt;扩展&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;SMTP：
&lt;ul&gt;
&lt;li&gt;SMTP 服务器内置&lt;a href="https://datatracker.ietf.org/doc/html/rfc7489"&gt;DMARC&lt;/a&gt;、&lt;a href="https://datatracker.ietf.org/doc/html/rfc6376"&gt;DKIM&lt;/a&gt;、&lt;a href="https://datatracker.ietf.org/doc/html/rfc7208"&gt;SPF&lt;/a&gt;和&lt;a href="https://datatracker.ietf.org/doc/html/rfc8617"&gt;ARC&lt;/a&gt;支持消息认证。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://datatracker.ietf.org/doc/html/rfc6698"&gt;通过 DANE&lt;/a&gt;、&lt;a href="https://datatracker.ietf.org/doc/html/rfc8461"&gt;MTA-STS&lt;/a&gt;和&lt;a href="https://datatracker.ietf.org/doc/html/rfc8460"&gt;SMTP TLS&lt;/a&gt;报告实现强大的传输安全性。&lt;/li&gt;
&lt;li&gt;通过细粒度的配置规则、筛选脚本、MTA 挂钩和 milter 集成进行入站限制和过滤。&lt;/li&gt;
&lt;li&gt;具有延迟传送、优先传送、配额、路由规则和节流支持的分布式虚拟队列。&lt;/li&gt;
&lt;li&gt;信封重写和消息修改。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;协作&lt;/strong&gt;服务器：
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://datatracker.ietf.org/doc/html/rfc4791"&gt;使用 CalDAV&lt;/a&gt;进行日历和日程安排。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://datatracker.ietf.org/doc/html/rfc6352"&gt;使用 CardDAV&lt;/a&gt;进行联系人管理。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://datatracker.ietf.org/doc/html/rfc4918"&gt;使用 WebDAV&lt;/a&gt;进行文件存储和共享。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;垃圾邮件&lt;/strong&gt;和&lt;strong&gt;网络钓鱼&lt;/strong&gt;内置过滤器：
&lt;ul&gt;
&lt;li&gt;与流行解决方案相当的一套全面的过滤&lt;strong&gt;规则。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;LLM 驱动的垃圾邮件过滤和消息分析。&lt;/li&gt;
&lt;li&gt;具有自动训练功能和地址簿集成的统计&lt;strong&gt;垃圾邮件分类器。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;DNS 黑名单 (&amp;nbsp;&lt;strong&gt;DNSBL&lt;/strong&gt;&amp;nbsp;) 检查 IP 地址、域和哈希。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;使用 Pyzor&lt;/strong&gt;进行基于协作摘要的垃圾邮件过滤。&lt;/li&gt;
&lt;li&gt;针对同形异义 URL 攻击、发件人欺骗和其他技术的网络&lt;strong&gt;钓鱼保护。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;可信&lt;strong&gt;回复&lt;/strong&gt;跟踪，用于识别和优先处理真实的电子邮件回复。&lt;/li&gt;
&lt;li&gt;通过 IP 地址、ASN、域和电子邮件地址监控发件人&lt;strong&gt;信誉。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;灰名单&lt;/strong&gt;可暂时延迟未知发件人。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;垃圾邮件陷阱&lt;/strong&gt;用于设置诱饵电子邮件地址来捕获和分析垃圾邮件。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;灵活的&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;可插入存储后端，支持&lt;strong&gt;RocksDB&lt;/strong&gt;、&lt;strong&gt;FoundationDB&lt;/strong&gt;、&lt;strong&gt;PostgreSQL&lt;/strong&gt;、&lt;strong&gt;mySQL&lt;/strong&gt;、&lt;strong&gt;SQLite&lt;/strong&gt;、&lt;strong&gt;S3-Compatible&lt;/strong&gt;、&lt;strong&gt;Azure&lt;/strong&gt;、&lt;strong&gt;Redis&lt;/strong&gt;和&lt;strong&gt;ElasticSearch&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;提供 17 种语言的全文搜索。&lt;/li&gt;
&lt;li&gt;Sieve 脚本语言支持所有&lt;a href="https://www.iana.org/assignments/sieve-extensions/sieve-extensions.xhtml"&gt;注册的扩展&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;电子邮件别名、邮件列表、子地址和全部地址支持。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.ietf.org/id/draft-bucksch-autoconfig-02.html"&gt;使用 autoconfig&lt;/a&gt;和&lt;a href="https://learn.microsoft.com/en-us/exchange/architecture/client-access/autodiscover?view=exchserver-2019"&gt;autodiscover&lt;/a&gt;自动配置和发现帐户。&lt;/li&gt;
&lt;li&gt;通过域和租户隔离实现多租户支持。&lt;/li&gt;
&lt;li&gt;每个用户和租户的磁盘配额。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;安全且强大&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;使用 S/MIME&lt;/strong&gt;或&lt;strong&gt;OpenPGP&lt;/strong&gt;进行静态加密。&lt;/li&gt;
&lt;li&gt;使用、或挑战通过&lt;a href="https://datatracker.ietf.org/doc/html/rfc8555"&gt;ACME&lt;/a&gt;自动配置 TLS 证书。&lt;code&gt;TLS-ALPN-01&lt;/code&gt;&lt;code&gt;DNS-01&lt;/code&gt;&lt;code&gt;HTTP-01&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;自动阻止攻击、滥用或扫描服务器漏洞的 IP 地址。&lt;/li&gt;
&lt;li&gt;速率限制。&lt;/li&gt;
&lt;li&gt;安全审计（阅读&lt;a href="https://stalw.art/blog/security-audit"&gt;报告&lt;/a&gt;）。&lt;/li&gt;
&lt;li&gt;内存安全（感谢 Rust）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;可扩展且容错&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;设计用于无缝处理增长，从小型设置到数千个节点的大规模部署。&lt;/li&gt;
&lt;li&gt;构建时考虑了&lt;strong&gt;容错&lt;/strong&gt;和&lt;strong&gt;高可用性&lt;/strong&gt;，可以从硬件或软件故障中恢复，同时最大程度地减少对操作的影响。&lt;/li&gt;
&lt;li&gt;对等集群协调或与&lt;strong&gt;Kafka&lt;/strong&gt;、&lt;strong&gt;Redpanda&lt;/strong&gt;、&lt;strong&gt;NATS&lt;/strong&gt;或&lt;strong&gt;Redis&lt;/strong&gt;协调。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kubernetes&lt;/strong&gt;、&lt;strong&gt;Apache Mesos&lt;/strong&gt;和&lt;strong&gt;Docker Swarm&lt;/strong&gt;支持自动扩展和容器编排。&lt;/li&gt;
&lt;li&gt;读取副本、分片 Blob 存储和内存数据存储，实现高性能和低延迟。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;身份验证和授权&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;OpenID Connect&lt;/strong&gt;身份验证。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.rfc-editor.org/rfc/rfc8628"&gt;带有授权码&lt;/a&gt;和&lt;a href="https://www.rfc-editor.org/rfc/rfc8628"&gt;设备授权&lt;/a&gt;流程的 OAuth 2.0 授权。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LDAP&lt;/strong&gt;、&lt;strong&gt;OIDC&lt;/strong&gt;、&lt;strong&gt;SQL&lt;/strong&gt;或内置身份验证后端支持。&lt;/li&gt;
&lt;li&gt;使用基于时间的一次性密码进行双因素身份验证（&lt;code&gt;2FA-TOTP&lt;/code&gt;）&lt;/li&gt;
&lt;li&gt;应用程序密码（App Passwords）。&lt;/li&gt;
&lt;li&gt;角色和权限。&lt;/li&gt;
&lt;li&gt;访问控制列表 (ACL)。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;可观察性&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;使用 OpenTelemetry&lt;/strong&gt;、journald、日志文件和控制枱支持进行日志记录和跟踪。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;具有 OpenTelemetry&lt;/strong&gt;和&lt;strong&gt;Prometheus&lt;/strong&gt;集成的指标。&lt;/li&gt;
&lt;li&gt;用于事件驱动自动化的 Webhook。&lt;/li&gt;
&lt;li&gt;通过电子邮件和 webhook 通知发出警报。&lt;/li&gt;
&lt;li&gt;实时追踪和指标。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;基于 Web 的管理&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;具有实时统计和监控功能的仪表板。&lt;/li&gt;
&lt;li&gt;帐户、域、组和邮件列表管理。&lt;/li&gt;
&lt;li&gt;用于消息和出站 DMARC 和 TLS 报告的 SMTP 队列管理。&lt;/li&gt;
&lt;li&gt;用于接收 DMARC、TLS-RPT 和故障（ARF）报告的报告可视化界面。&lt;/li&gt;
&lt;li&gt;邮件服务器各个方面的配置。&lt;/li&gt;
&lt;li&gt;具有搜索和过滤功能的日志查看器。&lt;/li&gt;
&lt;li&gt;用于密码重置和静态加密密钥管理的自助服务门户。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;截图&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt="" height="299" src="https://static.oschina.net/uploads/space/2025/0528/141541_x6Xx_4252687.gif" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

                                                                    &lt;/div&gt;
                                                                </description>
      <link>https://www.oschina.net/p/stalwart</link>
      <guid isPermaLink="false">https://www.oschina.net/p/stalwart</guid>
      <pubDate>Sun, 11 May 2025 10:20:00 GMT</pubDate>
    </item>
    <item>
      <title>Hugging Face 发布 ScreenSuite：开源 GUI 智能体评测套件</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;Hugging Face&lt;u&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fblog%2Fscreensuite" target="_blank"&gt;&amp;nbsp;开源&lt;/a&gt;&lt;/u&gt;了专用于评估 GUI 智能体的综合测试套件&amp;nbsp;ScreenSuite。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-b8a681e7b26b82daaa084cba75737cf15d0.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;什么是 GUI Agent？简单说，就是「能像人一样操作屏幕」的 AI！它能识别界面内容、点击按钮、输入文字、滚动页面……实现真实的「虚拟助手」体验。&lt;/p&gt; 
&lt;p&gt;现在，Hugging Face 推出了全新的开源工具 ScreenSuite，帮助开发者和研究者评估这类视觉语言模型的实际操作能力！&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ScreenSuite 能做什么？&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;它整合了 13 个评测集，覆盖：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;屏幕感知与 UI 定位&lt;/li&gt; 
 &lt;li&gt;单步操作指令执行&lt;/li&gt; 
 &lt;li&gt;多步骤任务规划与完成&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;ScreenSuite 已在多个主流 VLM 上完成评测，包括：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Qwen2.5-VL 系列（3B~72B）&lt;/li&gt; 
 &lt;li&gt;UI-TARS、Holo1 等优秀开源模型&lt;/li&gt; 
 &lt;li&gt;GPT-4o 等闭源模型&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;ScreenSuite 支持 Ubuntu、Android、Windows 多平台评测，结合虚拟机环境还原真实交互场景，适用于科研评估与模型迭代。与其他评测不同的是，ScreenSuite 完全基于视觉输入，不依赖 DOM 或辅助树，更贴近人类的使用方式，挑战也更真实。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/355075/huggingface-screensuite</link>
      <guid isPermaLink="false">https://www.oschina.net/news/355075/huggingface-screensuite</guid>
      <pubDate>Sun, 11 May 2025 10:02:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>阿里发布开源数字人框架 Mnn3dAvatar</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;阿里开源了名为「Mnn3dAvatar」的数字人框架，项目可以做到实时面捕然后映射到 3D 虚拟角色脸上（注意不是 Live2D 的，是 3D 的），甚至还能帮助创建一个 3D 虚拟角色。 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-752d4b90686f31d92cffa705f0a4666ba61.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;https://github.com/alibaba/MNN/blob/master/apps/Android/Mnn3dAvatar/README.md&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;据介绍，Mnn3dAvatar 是基于自研 3D 高斯溅射技术的 3D 数字人实时面捕方案，通过 AI 驱动实现高精度面部动作捕捉与实时渲染，支持语音、表情、手势等多模态驱动，可在手机、AR 设备等终端以 90FPS 帧率流畅运行。&lt;/p&gt; 
&lt;p&gt;其核心优势在于将影视级数字人效果落地到消费级硬件，仅需普通手机摄像头即可替代传统动捕设备，显著降低直播场景的部署门槛与成本，制作周期缩短至一周内，成本仅为传统 CG 方案的 1/30。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/355070</link>
      <guid isPermaLink="false">https://www.oschina.net/news/355070</guid>
      <pubDate>Sun, 11 May 2025 09:53:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Meta 发布强化学习框架 LlamaRL</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;Meta 发布了 LlamaRL 强化学习框架，基于 PyTorch 构建全异步分布式架构，通过独立执行器并行处理生成、训练和评分任务，并整合 DDMA 和 NVLink 技术实现高效数据传输。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0612/174259_0GLJ_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;实测显示，该框架在 4050 亿参数模型中，将强化学习步骤耗时从 635.8 秒缩减至 59.5 秒，效率提升 10.7 倍，80 亿、700 亿参数模型训练时间分别缩短至 8.90 秒、20.67 秒。其突破内存瓶颈与 GPU 利用率难题，同时在 MATH 和 GSM8K 等标准测试中模型表现稳定甚至增强，为未来更大规模模型训练提供可扩展解决方案。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-586f91769fd68bac924768946985aabdcfc.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;论文地址：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2505.24034" target="_blank"&gt;https://arxiv.org/abs/2505.24034&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/355068</link>
      <guid isPermaLink="false">https://www.oschina.net/news/355068</guid>
      <pubDate>Sun, 11 May 2025 09:45:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>谷歌发布高效运行语言模型的 C++ 库：LiteRT-LM</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;谷歌发布了 LiteRT-LM 早期版本，这是一个 C++库，用于在边缘平台上高效运行语言模型。&lt;/p&gt; 
&lt;p&gt;LiteRT-LM 支持跨平台高效运行 Gemma-3N 系列模型，支持 2B 和 4B 参数模型，适用于桌面环境（Mac/Windows/Linux）及物联网设备。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0612/173748_cLZX_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;LiteRT-LM 的 README 写道：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;语言模型不再是一个单一模型，而是一个由多个模型和组件协同工作的流水线。LiteRT-LM 基于 LiteRT 构建，以支持这些流水线，包括：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;C++ API 高效运行语言模型&lt;/li&gt; 
  &lt;li&gt;跨平台支持，通过便携式 C++实现广泛部署场景&lt;/li&gt; 
  &lt;li&gt;灵活可定制，满足您的特定功能需求&lt;/li&gt; 
  &lt;li&gt;硬件加速，释放设备硬件的全部潜能&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;开源地址&lt;/p&gt; 
&lt;p&gt;https://github.com/google-ai-edge/LiteRT-LM&lt;/p&gt; 
&lt;p&gt;https://huggingface.co/google/gemma-3n-E2B-it-litert-lm-preview&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/355066/google-litert-lm-preview</link>
      <guid isPermaLink="false">https://www.oschina.net/news/355066/google-litert-lm-preview</guid>
      <pubDate>Sun, 11 May 2025 09:38:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>通用型 AI 智能体 Manus 推出全新的聊天模式</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;Manus AI &lt;u&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2FManusAI_HQ%2Fstatus%2F1932862389717995710" target="_blank"&gt;宣布&lt;/a&gt;&lt;/u&gt;推出全新的聊天模式，并面向所有的用户免费开放。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0612/165305_UkfX_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;用户可通过简洁界面进行即时问答，覆盖日常咨询、知识查询等场景，并可无缝切换至代理模式，执行网页设计、数据分析等复杂任务。&lt;/p&gt; 
&lt;p&gt;&lt;img height="580" src="https://static.oschina.net/uploads/space/2025/0612/165427_PwLc_2720166.png" width="1496" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="662" src="https://static.oschina.net/uploads/space/2025/0612/165349_LikS_2720166.png" width="1126" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;此前，代理模式需订阅或消耗点数，而此次更新后，用户无需付费即可体验基础功能，显著降低使用门槛。&lt;/p&gt; 
&lt;p&gt;自 2025 年 3 月上线以来，Manus 已吸引超 200 万用户注册，而在今年 5 月，还获得 Benchmark Capital 和红杉中国 7500 万美元融资，估值达 5 亿美元。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;相关阅读：&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/news/353537/manus-video-generation" target="news"&gt;通用型 AI 智能体 Manus 新增「文字生成视频」功能&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/news/349561" target="news"&gt;Manus 开放注册，用户每天可免费执行一项任务&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/355049/manus-chat-mode</link>
      <guid isPermaLink="false">https://www.oschina.net/news/355049/manus-chat-mode</guid>
      <pubDate>Sun, 11 May 2025 08:56:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Android「 闭源」进度更新：Google 不再提供 Pixel 固件编译</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;「爱范儿」昨日发文介绍了 Android 闭源进度更新，称 Google 最新放出的 AOSP 代码当中，没有像往年那样一并提供 Pixel 设备的 vendor binary，也即必要驱动程序等文件。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0612/161620_QpYg_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;原文：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.ifanr.com%2F1626765" target="_blank"&gt;Android 闭源进度更新：Google 不再提供 Pixel 固件编译&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;下文为该报道的总结：&lt;/p&gt; 
&lt;p&gt;Google 在发布 Android 16 版本 AOSP 代码时未提供 Pixel 设备的 vendor binary。&lt;/p&gt; 
&lt;p&gt;目前 Pixel 设备驱动程序二进制文件停留在 Android 15.0.0 版本，且此次提供的 Android 16 版本 AOSP 代码只能构建为通用系统映像，用于硬件兼容性验证，非完整系统。&lt;/p&gt; 
&lt;p&gt;这一变化对第三方操作系统开发造成障碍，过去 Google 会同时提供 Pixel 设备 vendor binary，方便 ROM 开发者打包，而从 Android 16 开始，开发者无法自行修改 AOSP 代码打包成固件安装到 Pixel 设备上，只能通过逆向工程拆解 Pixel 升级包做适配，且每款 Pixel 手机都要逆向工程一次，限制第三方 ROM 适配范围和市场表现。&lt;/p&gt; 
&lt;p&gt;只有与 Google 签订 GMS 协议的 OEM 合作伙伴能第一时间获得全量 AOSP 代码，这意味着基于 AOSP 开发的 ROM 很难开发 Android 16 版本，除非开发团队与 Google 签订授权协议或从已签约 OEM 获取代码。&lt;/p&gt; 
&lt;p&gt;知名 ROM 团队 GrapheneOS 确认了这一情况，其因无法第一时间拿到完整 AOSP 代码，开发进度受阻，需大量逆向工程且移植工作变得困难。&lt;/p&gt; 
&lt;p&gt;Google 这样做的原因可能是为了节约开支和增加收入，减少对不能带来利益的第三方 ROM、非认证 Android 设备等市场的免费支持。&lt;/p&gt; 
&lt;p&gt;Google 早在 2025 年初就有了收窄 Android 开源属性的想法，策略执行将延续数年，直至 AOSP 彻底失去开源属性。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/355037</link>
      <guid isPermaLink="false">https://www.oschina.net/news/355037</guid>
      <pubDate>Sun, 11 May 2025 08:16:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>字节 Trae 宣布月活突破 100 万，交付超 60 亿行代码</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;&lt;span style="color:#000000"&gt;由字节跳动开发的 AI 原生集成开发环境（IDE）Trae 迎来重要里程碑：截至 2025 年 5 月，月活跃用户已达 100 万，累计帮助开发者交付超过 60 亿行代码。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;Trae 于 2025 年 1 月正式推出，3 月推出国内版本，集成了豆包 1.5-pro 及 DeepSeek R1 和 V3 等先进模型，为中国开发者提供定制化支持。Trae 的多模态功能尤为突出，能够根据 Figma 设计文件或手绘草图生成前端代码，代码生成准确率高达 91%，复杂系统（如电子商务平台）的开发效率提升高达 400%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="344" src="https://oscimg.oschina.net/oscnet/up-e841ba3308d0602d72ac4e45d0a1c2c3980.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;Trae 成功的核心在于其高效生成和管理海量代码的能力。平台已助力开发者交付超 60 亿行代码，充分展示了其强大的自动化功能和与现代开发流程的无缝整合。Trae 的 Builder 模式支持用户通过自然语言提示生成完整的项目框架，例如输入「构建一个带有 Redis 缓存和 JWT 认证的购物车系统」，即可自动生成包含 Dockerfile 和 CI/CD 脚本的完整项目。此外，Chat 模式提供实时代码调试和优化功能，进一步提升了开发体验。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;2025 年 5 月，Trae 推出国际付费订阅计划，首月定价 3 美元，随后每月 10 美元，为用户提供更快访问高级模型和无限代码补全等高级功能。支持支付宝的订阅模式进一步推动了国际用户的增长。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;span style="color:#000000"&gt;相关阅读：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.oschina.net/news/336617" target="_blank"&gt;中国首款 AI IDE：Trae 国内版发布&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.oschina.net/news/352294/trae-pro-plan" target="news"&gt;Trae IDE 海外版上线付费订阅服务，Pro 版每月 10 美金&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/355035</link>
      <guid isPermaLink="false">https://www.oschina.net/news/355035</guid>
      <pubDate>Sun, 11 May 2025 08:13:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>vivo Pulsar 万亿级消息处理实践（1）- 数据发送原理解析和性能调优</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                                        &lt;blockquote&gt; 
 &lt;p&gt;作者：vivo 互联网大数据团队- Quan Limin&lt;/p&gt; 
 &lt;p&gt;本文是 vivo 互联网大数据团队《vivo Pulsar 万亿级消息处理实践》系列文章第 1 篇。&lt;/p&gt; 
 &lt;p&gt;文章以 Pulsar client 模块中的 Producer 为解析对象，通过对 Producer 数据发送原理进行逐层分析，以及分享参数调优实战案例，帮助读者理解与使用好 Producer，并体会到 Producer 对消息中间件系统稳定性以及处理性能所起到的关键作用。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;一、Pulsar 简要介绍&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://static001.geekbang.org/infoq/fb/fbcab3bc275f1a6943f142e8a110d07c.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Pulsar 是新一代的云原生消息中间件，由 Apache 软件基金会孵化和开源。它的设计目的是为了满足现代数据处理和计算应用程序对可扩展性、可靠性和高性能的需求，具备存储与计算分离、节点对等、独立扩展、实时均衡、节点故障快速恢复等特性。&lt;/p&gt; 
&lt;p&gt;Pulsar 由四个核心模块组成：broker、bookKeeper 和 client（Producer 和 Consumer）、zk（元数据管理和节点协调）。broker 接受来自 Producer 的消息，将消息路由到对应的 topic；bookKeeper 用于数据持久化存储和数据复制；Consumer 消费 topic 上的数据。Pulsar 支持多种编程语言和协议（如 Java、C++、Go、Python 等），可以运行在云、本地和混合环境中，扩展性好，支持多租户和跨数据中心复制等特性。因此，Pulsar 被广泛应用于云计算、大数据、物联网等领域的实时消息传递和处理应用中。&lt;/p&gt; 
&lt;h1&gt;二、Pulsar Producer 解析&lt;/h1&gt; 
&lt;p&gt;首先需要了解 Producer 的数据发送流程，这里以「开启压缩、batch 发送消息给 partitioned topic「这样的一个线上常规场景为例，解析数据的发送的关键环节。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;tips：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在 Pulsar 中有无分区（Non-Partitioned）Topic 和有分区 (Partitioned) 的 Topic 之分，Partitioned topic 最小分区数为 1，为满足任务的拓展性，在线上一般使用 Partitioned topic。&lt;/p&gt; 
&lt;h2&gt;2.1 消息生产与发送的详细流程&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://static001.geekbang.org/infoq/66/6663c40abe8c2e1562b1ef8e8393a55d.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Producer 发送数据主要分为&lt;strong&gt;12 个步骤：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;① 创建 Producer：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;partitioned topic 创建的是一个 Partitioned-&lt;/p&gt; 
&lt;p&gt;ProducerImpl 对象，该对象包含了所有分区及其对应的 ProducerImpl 对象，ProducerImpl 对象负责所对应分区数据的维护和发送。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;② 构造消息：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;一条消息被发送前首先会被封装成为一个 Message 对象，对象中包含了所发送的 topic name、消息体、消息大小、schema 类型、metadata（是否指定 key 等）等信息。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;③ 确定目标分区：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在发送消息前需要通过路由策略决定发往哪一个分区，选择对应分区的 ProducerImpl 对象进行进一步处理。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;④ 拦截器：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Producer 可以设置自定义的拦截器，拦截器需要实现 producerInterceptor 接口，在消息发送前可对消息进行拦截修改。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;⑤ 消息堆积控制：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Producer 可以处理的消息是有限的，接收新的消息时会分别进行信号量和内存使用率校验，控制接收消息的速率，防止消息无限在本地堆积。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;⑥ batch 容器管理：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;默认情况下分好区的消息不是直接被发送，而是放入了生产者的一个 batch 缓存容器中里面。在这个缓存里面，多条消息会被封装成为一个批次（batch）。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;⑦ 消息序列化：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;Pulsar 的消息需要从客户端传到服务端，涉及到网络传输，因此 Producer 将 batch 缓冲区中的所有消息逐一进行序列化。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;⑧ 压缩：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Pulsar 内置了多种压缩算法，在发送前会根据所选择的压缩算法对 batch 整体进行压缩，这将优化网络传输以提高 Pulsar 消息传输的性能。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;⑨ 构建消息发送对象：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;无论是开启 batch 的批次消息，还是关闭 batch 的单条消息，都会被包装为一个 OpSendMsg 对象，OpSendMsg 也是 Producer 发送和 pulsar broker 接收处理的最小单位。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;⑩ pending 队列：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;所有构建好的 OpSendMsg 在发送前都会被放入 pendingMessages 队列中，消息处理完成后才会从队列中移除。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;⑪&amp;nbsp;消息传输：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Pulsar 使用 netty 将消息异步的从客户端发送到服务端，Broker 节点将在收到消息后对其进行确认，并将其存储在指定主题的持久存储中。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;⑫ 响应处理：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Pulsar Broker 在收到消息时会返回一个响应，如果写入成功，消息将会从 pendingMessages 队列中移除。如果写入失败，会返回一个错误，生产者在收到可重试错误之后会尝试重新发送消息，直到重试成功或超时。&lt;/p&gt; 
&lt;h2&gt;2.2 关键环节原理分析&lt;/h2&gt; 
&lt;p&gt;接下来会对上述流程中关键环节的设计和原理作进一步的剖析，帮助读者更好的理解 Producer。&lt;/p&gt; 
&lt;h3&gt;2.2.1 创建 Producer&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://static001.geekbang.org/infoq/e5/e52d9588e782f913a00d092f1cdd80f5.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在 Pulsar 中，PartitionedProducerImpl 用于将多个 ProducerImpl 对象包装成为一个逻辑生产者，以便向 Partitioned Topic 发送消息时能够批量操作。其中，PartitionedProducerImpl.producers 成员变量维护了每个分区及其对应的 ProducerImpl 对象，该设计主要有以下&lt;strong&gt;3 个好处：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;① 每个分区对应一个单独的生产者：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在 Pulsar 中，Partitioned Topic 按照分区（Partition）将多个 ProducerImpl 对象进行分配，以便能够同时发往多个 Broker 节点，因此对于每个分区，需要拥有一个单独的生产者以便进行发送操作。在 PartitionedProducerImpl 类中，需要为每个分区维护一个 ProducerImpl 对象，以便在消息被分配好「目标分区」后可以调用对应的 ProducerImpl 进行处理。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;②简化代码逻辑：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在 PartitionedProducerImpl 中，将每个分区及其对应的 ProducerImpl 对象维护在一个 HashMap 中，能够更加方便的维护并管理不同分区的生产者，使得代码逻辑更加清晰简明。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;③ 提高容错性：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;当某个分区的 ProducerImpl 对象无法工作时，可以选择其他可用的 ProducerImpl 对象，从而保证系统整体的可用性。由于将不同分区的 ProducerImpl 对象分别进行维护，因此具备更加灵活的容错处理策略。&lt;/p&gt; 
&lt;p&gt;在线上实践中我们也基于该设计，在 PartitionedProducerImpl 层做了进一步优化，通过感知下一层每个 ProducerImpl 的阻塞状态（信号量的使用情况）来决定新的消息发送，避免将消息持续发往阻塞较为严重的分区，规避了 topic 被某一个分区阻塞而影响到整体发送性能的情况，也提高了线上系统的稳定性，具体的实现可以详见这篇文章《&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzI4NjY4MTU5Nw%3D%3D%26mid%3D2247494958%26idx%3D3%26sn%3Db2f02d545627a1457958289d8f623af3%26scene%3D21%23wechat_redirect" target="_blank"&gt;构建下一代万亿级云原生消息架构：Apache Pulsar 在 vivo 的探索与实践&lt;/a&gt;》。&lt;/p&gt; 
&lt;p&gt;关键代码：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;//对每一个分区都创建一个 ProducerImpl 对象
  private void start(List&amp;lt;Integer&amp;gt; indexList) {
        AtomicReference&amp;lt;Throwable&amp;gt; createFail = new AtomicReference&amp;lt;Throwable&amp;gt;();
        AtomicInteger completed = new AtomicInteger();
 
        for (int partitionIndex : indexList) {
            createProducer(partitionIndex).producerCreatedFuture().handle((prod, createException) -&amp;gt; {
.......
            });
        }
    }
 
    private ProducerImpl&amp;lt;T&amp;gt; createProducer(final int partitionIndex) {
        return producers.computeIfAbsent(partitionIndex, (idx) -&amp;gt; {
            String partitionName = TopicName.get(topic).getPartition(idx).toString();
            return client.newProducerImpl(partitionName, idx,
                    conf, schema, interceptors, new CompletableFuture&amp;lt;&amp;gt;());
        });
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2.2.2 确定目标分区&lt;/h3&gt; 
&lt;p&gt;在发送消息前需要决定发往哪一个分区，确定好分区后便调用对应分区的 ProducerImpl 对象进一步处理，目标分区的确定主要跟「路由策略」和「是否指定 key」有关：&lt;/p&gt; 
&lt;p&gt;**（1）如果消息没有指定 key：**则按照三种路由策略的效果选择分区进行发送，三种路由策略如下：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;SinglePartition：&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;如果消息没有指定 Key，Producer 会随机选择一个 Partition，然后把所有的消息都发送到这个 Partition 上。&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;RoundRobinPartition：&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;生产者将以轮询方式在所有 Partition 之间发布消息，以实现最大吞吐量。需要注意的是如果开启了 batch 发送，则轮询将会以批为单位进行消息发送，批次发送时每隔 partitionSwitchMs 会轮询一个 Partition。如果关闭了批量发送，那么每条消息发送都会轮询一个 Partition。（partitionSwitchMs 至少为一个 batchingMaxPublishDelay 时间）。&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;CustomPartition：&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;使用用户自定义的消息路由实现，根据自定义的 Router 实现决定消息要发往哪个分区。用户自定义的 Router 可以通过 messageRoute 参数设置。自定义的 Router 需要实现 MessageRouter 接口的 choosePartition 方法。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;**（2）如果消息指定 key：**则会对 Key 做哈希处理,然后找到对应的 Partition，把 key 所对应的消息都发送到同一个分区：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;对消息的 Key 进行哈希处理后如何找到对应的 Partition 的？即使用 Key 的哈希值对总的 Partition 数取模：N=(Key 的哈希值% 总的 Partition 数)，得到的 N 就是第 N 个 Partition，Producer 可以通过设置 hashingscheme 来使用不同的哈希算法 ，现在已经支持 JavastringHash 和 Murmur3_32Hash 两种哈希算法，前者直接调用 String.hash.Code()，后者使用 Murmur3。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;路由策略的关键代码如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;//SinglePartition 路由策略：
public int choosePartition(Message&amp;lt;?&amp;gt; msg, TopicMetadata metadata) {
    // If the message has a key, it supersedes the single partition routing policy
    if (msg.hasKey()) {
        return signSafeMod(hash.makeHash(msg.getKey()), metadata.numPartitions());
    }
 
    return partitionIndex;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;//RoundRobin 路由策略：
public int choosePartition(Message&amp;lt;?&amp;gt; msg, TopicMetadata topicMetadata) {
    // If the message has a key, it supersedes the round robin routing policy
    if (msg.hasKey()) {
        return signSafeMod(hash.makeHash(msg.getKey()), topicMetadata.numPartitions());
    }
 
    if (isBatchingEnabled) { // if batching is enabled, choose partition on `partitionSwitchMs` boundary.
        long currentMs = clock.millis();
        return signSafeMod(currentMs / partitionSwitchMs + startPtnIdx, topicMetadata.numPartitions());
    } else {
        return signSafeMod(PARTITION_INDEX_UPDATER.getAndIncrement(this), topicMetadata.numPartitions());
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2.2.3 消息堆积控制&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://static001.geekbang.org/infoq/2f/2fe4f4750d448407920510d97421b11b.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Producer 不可能无限接收新的消息，如果某些分区数据发送较慢，消息就会堆积在 Prouducer 缓存中，导致已经阻塞的分区堆积大量的消息，又无法重新发往其他分区，同时也可能因为无限堆积的消息占用了大量的内存，使得任务频繁 GC 甚至 OOM。&lt;/p&gt; 
&lt;p&gt;在 Pulsar 提供了两个核心的速率限制策略和一个阻塞时的消息处理策略：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;消息数量限制：&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;maxPendingMessages 控制每个分区某一时刻最大可处理消息数量，通过信号量的方式控制「新进入的消息」的信号量分配和「处理完成消息「的信号量释放，防止某个分区的消息严重堆积。&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;消息占用内存大小限制：&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;memoryLimit 控制整个 Pulsar client 的消息最大占用内存大小，通过计数器方式控制「新进入的消息」有效载荷的内存分配和「处理完成消息「有效载荷的内存释放，这里需要特殊说明的是 memoryLimit 是 client 的参数，针对的是该 client 对象下的所有 topic，因此并不建议一个 Pulsar client 对象 new 多个 Producer topic ，因为很容易出现某一个 topic 占用内存过多，导致另一个 topic 无空间可分配的情况。&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;阻塞处理策略：&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;由 blockIfQueueFull 进行控制，当 blockIfQueueFull 为 true 时，代表阻塞等待，Producer 会等待获取信号量；当 blockIfQueueFull 为 false 时，一旦获取不到信号量，就会立刻失败，需要注意的是如果 blockIfQueueFull 为 false，业务需要处理好消息失败后的回调策略，否则会导致数据在 Producer 上「丢失」。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;关键代码如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;public void sendAsync(Message&amp;lt;?&amp;gt; message, SendCallback callback) {
......
        MessageImpl&amp;lt;?&amp;gt; msg = (MessageImpl&amp;lt;?&amp;gt;) message;
        MessageMetadata msgMetadata = msg.getMessageBuilder();
        ByteBuf payload = msg.getDataBuffer();
        int uncompressedSize = payload.readableBytes();
        //对发送队列大小以及 client memory 进行判断是否有空间放入新的消息
        if (!canEnqueueRequest(callback, message.getSequenceId(), uncompressedSize)) {
            return;
        }
......
    }
 
    private boolean canEnqueueRequest(SendCallback callback, long sequenceId, int payloadSize) {
        try {
            if (conf.isBlockIfQueueFull()) {
                //当 blockIfQueueFull 为 true 时，等待获取信号量
                if (semaphore.isPresent()) {
                    semaphore.get().acquire();
                }
                //分配消息有效载荷所需要的内存空间
                client.getMemoryLimitController().reserveMemory(payloadSize);
            } else {
                //当 blockIfQueueFull 为 false 时，如果无法获取到信号量，则快速失败
                if (!semaphore.map(Semaphore::tryAcquire).orElse(true)) {
                    callback.sendComplete(new PulsarClientException.ProducerQueueIsFullError("Producer send queue is full", sequenceId));
                    return false;
                }
                //如果没有如何的内存空间用于消息分配，则报错
                if (!client.getMemoryLimitController().tryReserveMemory(payloadSize)) {
                    semaphore.ifPresent(Semaphore::release);
                    callback.sendComplete(new PulsarClientException.MemoryBufferIsFullError("Client memory buffer is full", sequenceId));
                    return false;
                }
            }
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            callback.sendComplete(new PulsarClientException(e, sequenceId));
            return false;
        }
 
        return true;
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2.2.4 消息 batch 容器打包&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://static001.geekbang.org/infoq/57/57e8428ca0b0093978799d8411231e60.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;（1）batch 关键组成信息&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Messages:&lt;/strong&gt; 保存消息的 list，保存跟这个 batch 相关所有的 MessageImpl 对象。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;**Metadata：**保存 batch 相关的元数据，如批量消息的序列号、消息发送的时间戳等信息。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;**Callback：**保存消息回调逻辑的集合，记录了每一条消息对应的 callback 策略，在 batch 消息发送并等到服务端响应后，依次对消息的回调进行处理。&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;（2）batch 打包条件&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;batch 打包条件的三个关键参数：满足其一数据就会被打包发送出去。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;**批次大小：**batchingMaxBytes&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;**批次条数：**batchingMaxMessages&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;批次延迟发送时间：&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;batchingMaxPublishDelay&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Pulsar 使用两个模块设计来实现上面的参数控制：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;**accumulator：**在 BatchMessage-&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;ContainerImpl 中通过计数器的方式去控制 batch 的大小和条数，numMessages-&lt;/p&gt; 
 &lt;p&gt;InBatch 记录已经缓存的消息数量，currentBatchSizeBytes 用于记录已缓存的消息的大小。当这些变量达到阈值时，BatchMessageContainerImpl 将会触发批量消息的发送。&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;**batchTimerTask：**当生产者使用批量消息发送模式时，Producer 将会创建一个定时器任务（batchTimerTask），并通过计时器的方式定时将 BatchMessageContainer 容器中的消息进行批量发送。&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;2.2.5 消息压缩&lt;/h3&gt; 
&lt;p&gt;如果开启了消息压缩，在发送前都需要进行压缩处理。对於单条消息发送的场景，是对每一条消息进行单独压缩后进行发送；而如果开启了 batch 则是对整个 batch 进行压缩后再整个进行发送。&lt;/p&gt; 
&lt;p&gt;在线上实践中，推荐在不影响业务延迟的情况下 batch 越大越好，主要有&lt;strong&gt;两个理由&lt;/strong&gt;：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;可以优化网络 IO 降低 CPU 负载：&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;不论 Producer 发送的是一条消息还是一批消息，在 pulsar 客户端都会被构建为一个 OpSendMsg 对象，同时 pulsar broker 接收到消息进行写入处理时，也是按照 OpSendMsg 为一个处理单位将消息写入磁盘，因此当消息数量一定时，batch 越大，则代表需要处理的 OpSendMsg 越少。&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;batch 越大「压缩效果则越好」：&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;压缩算法对应的压缩率并不固定，它通常取决于所要压缩的数据对象的内容和压缩算法本身，压缩的本质在于通过消除或利用数据中存在的冗余来实现数据的压缩和重构。而 Pulsar 是以 batch 来进行打包的，batch 越大，压缩的目标包体越大压缩效果则可能越好，同时也能够尽可能避免单条消息因为包体较小导致越压缩后包体越大的情况出现。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;以下是开启了 batch 情况下，构建发送消息和压缩的关键代码：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    public OpSendMsg createOpSendMsg() throws IOException {
        //对数据进行压缩、加密等操作
        ByteBuf encryptedPayload = producer.encryptMessage(messageMetadata, getCompressedBatchMetadataAndPayload());
......
 
        ByteBufPair cmd = producer.sendMessage(producer.producerId, messageMetadata.getSequenceId(),
                messageMetadata.getHighestSequenceId(), numMessagesInBatch, messageMetadata, encryptedPayload);
        //对整个 batch 构建一个 OpSendMsg
        OpSendMsg op = OpSendMsg.create(messages, cmd, messageMetadata.getSequenceId(),
                messageMetadata.getHighestSequenceId(), firstCallback);
......
        return op;
    }
 
    //对 batch 进行压缩，并将压缩后信息更新到 messageMetadata 中
    private ByteBuf getCompressedBatchMetadataAndPayload() {
......
        int uncompressedSize = batchedMessageMetadataAndPayload.readableBytes();
        ByteBuf compressedPayload = compressor.encode(batchedMessageMetadataAndPayload);
        batchedMessageMetadataAndPayload.release();
......
        return compressedPayload;
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2.2.6 pending 队列&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://static001.geekbang.org/infoq/64/6470193ab1e788eb19b49948423a9665.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Pulsar 中的 pendingMessages 队列是客户端用来暂存「未处理完成的消息」的一个缓存队列。用于存储当 Producer 连接到 Broker 服务器后，还未发送或尚未得到 Broker 系统的 ACK 确认的所有生产者（Producer）的消息。在发送消息之前，Producer 首先会将消息缓存到 pendingMessages 队列中，并记录保存缓存消息的 OpSendMsg 对象，直到它被成功发送到了 Broker 端并收到 Broker 发送的 ACK 确认之后，相关的元信息和消息信息才会从队列中移除。&lt;/p&gt; 
&lt;p&gt;需要注意的是：&lt;strong&gt;pending 队列的本质是一个回调处理队列，而不是发送队列&lt;/strong&gt;，消息在放入 pending 队列的同时就被异步发送到服务端了，所以这里需要重点理解什么是「未处理完成的消息」。&lt;/p&gt; 
&lt;p&gt;pendingMessages 队列的&lt;strong&gt;作用在于&lt;/strong&gt;：对于已经发送但尚未收到 ACK 确认的消息，防止在连接出现异常时丢失消息。当连接中断时，缓存在 pendingMessages 队列中的未确认消息将被认为是需要重发的，当连接恢复时，缓存的消息将重新发送到 Broker 端，以确保生产者生产的消息不会丢失。&lt;/p&gt; 
&lt;p&gt;**总的来说，**pendingMessages 队列是 Pulsar 客户端保证消息可靠性和一致性的关键功能组件，在 Pulsar 的生产者（Producer）和消息确认的机制中担任着非常重要的角色。&lt;/p&gt; 
&lt;p&gt;关键代码如下：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;add() 方法用于在追加消息时将指定元素插入队列中的队尾，remove() 用于消息在完成后移除队列头部的元素。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code&gt;    protected void processOpSendMsg(OpSendMsg op) {
        if (op == null) {
            return;
        }
        try {
            if (op.msg != null &amp;amp;&amp;amp; isBatchMessagingEnabled()) {
                batchMessageAndSend();
            }
            //将消息放入「待处理消息队列」
            pendingMessages.add(op);
......
                // If we do have a connection, the message is sent immediately, otherwise we'll try again once a new
                // connection is established
                op.cmd.retain();
                cnx.ctx().channel().eventLoop().execute(WriteInEventLoopCallback.create(this, cnx, op));
                stats.updateNumMsgsSent(op.numMessagesInBatch, op.batchSizeByte);
...... 
    }
 
       //添加消息到 pendingMessages 队列
       public boolean add(OpSendMsg o) {
            // postpone adding to the queue while forEach iteration is in progress
            //batch 的计数是按照 batch 中消息的总量进行计数
            messagesCount.addAndGet(o.numMessagesInBatch);
            if (forEachDepth &amp;gt; 0) {
                if (postponedOpSendMgs == null) {
                    postponedOpSendMgs = new ArrayList&amp;lt;&amp;gt;();
                }
                return postponedOpSendMgs.add(o);
            } else {
                return delegate.add(o);
            }
        }
        //将消息从 pendingMessages 队列移除
        public void remove() {
            OpSendMsg op = delegate.remove();
            if (op != null) {
                messagesCount.addAndGet(-op.numMessagesInBatch);
            }
        }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2.2.7 消息传输&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://static001.geekbang.org/infoq/2d/2d4bd56bfcbf7bbe62118828ee858a69.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Producer 和 broker 都维护了分区维度的 pending 队列来保证消息处理的顺序性，以及实现消息重新发送、重新写入持久化存储的能力。在 Producer 端，消息被顺序追加到 pending 队列并异步发送到服务端，服务端的 pending 队列在接收到消息后，按照顺序追加到队列中，并按照顺序将数据写入 bookie 进行持久化处理，处理完成后按照顺序返回响应 Producer，并将消息从 broker pending 和 producer pending 队列中移除。&lt;/p&gt; 
&lt;p&gt;另外在数据传输过程中，无论是使用 Pulsar Producer 的同步发送还是异步发送，在消息传输环节本质上都是使用 netty 将消息异步的从客户端发送到服务端，区别在于 send() 方法封装了 sendAsync() 方法，使其可以在向服务器发送 Pulsar 消息时阻塞等待 Broker 的响应，直到确认消息已经被 Broker 成功处理后才会返回，常规情况下，建议使用异步的方式发送 Pulsar 消息，因为同步方式必须在 Broker 端成功接收到消息之后才会返回，因此会带来较大的性能损耗和延迟。但是在部分场景下，需要使用同步方式来保证可靠性，以防 Broker 端接收失败，可以考虑使用 send() 方法实现同步方式的方式发送 Pulsar 消息。&lt;/p&gt; 
&lt;p&gt;使用 netty 执行的代码：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    private static final class WriteInEventLoopCallback implements Runnable {  
......
        @Override
        public void run() {
            if (log.isDebugEnabled()) {
                log.debug("[{}] [{}] Sending message cnx {}, sequenceId {}", producer.topic, producer.producerName, cnx,
                        sequenceId);
            }
 
            try {
                cnx.ctx().writeAndFlush(cmd, cnx.ctx().voidPromise());
                op.updateSentTimestamp();
            } finally {
                recycle();
            }
        }
......
    
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2.2.8 处理响应&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://static001.geekbang.org/infoq/28/28dd6d8c28f93ecae7372ea40b5c372b.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Pulsar Producer 使用「ACK 跟踪机制」来实现对 Broker 返回的 ACK 确认消息的处理，用于检测和处理到达生产者的全部消息状态信息。&lt;/p&gt; 
&lt;p&gt;对于 Producer 发送的消息，Pulsar 会对每个消息分配一个唯一的 sequenceId 序号，并记录该消息的创建时间（createdAt）等元数据信息。当 Broker 确认收到某个消息时，Producer 会依据返回的 ACK 序号和 Broker 返回的确认时间来判断当前 ACK 是否有效，并从已缓存的 pendingMessages 队列中找到对应的消息元数据信息，以进行确认处理，在 Broker 确认消息接收成功时，Producer 将从等待确认的消息队列中删除对应的消息元数据信息，如果 Broker 返回的 ACK 消息不符合生产者预期的消息状态信息，Producer 将会重发消息，直到重试成功或多次重试失败后抛出异常后再从队列中移除对应消息元数据信息并释放对应内存、信号量等资源。&lt;/p&gt; 
&lt;p&gt;消息重发的关键代码如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    private void resendMessages(ClientCnx cnx, long expectedEpoch) {
        cnx.ctx().channel().eventLoop().execute(() -&amp;gt; {
            synchronized (this) {
                //判断连接状态：当连接正在关闭或者已经关闭则不进行重发
                if (getState() == State.Closing || getState() == State.Closed) {
                    // Producer was closed while reconnecting, close the connection to make sure the broker
                    // drops the producer on its side
                    cnx.channel().close();
                    return;
                }
......
                //调用重发消息方法
                recoverProcessOpSendMsgFrom(cnx, null, expectedEpoch);
            }
        });
    }
 
 
   // Must acquire a lock on ProducerImpl.this before calling method.
    private void recoverProcessOpSendMsgFrom(ClientCnx cnx, MessageImpl from, long expectedEpoch) {
......
        final boolean stripChecksum = cnx.getRemoteEndpointProtocolVersion() &amp;lt; brokerChecksumSupportedVersion();
        Iterator&amp;lt;OpSendMsg&amp;gt; msgIterator = pendingMessages.iterator();
        OpSendMsg pendingRegisteringOp = null;
        while (msgIterator.hasNext()) {
            OpSendMsg op = msgIterator.next();
......
            op.cmd.retain();
            if (log.isDebugEnabled()) {
                log.debug("[{}] [{}] Re-Sending message in cnx {}, sequenceId {}", topic, producerName,
                          cnx.channel(), op.sequenceId);
            }
            //发送消息
            cnx.ctx().write(op.cmd, cnx.ctx().voidPromise());
            op.updateSentTimestamp();
            stats.updateNumMsgsSent(op.numMessagesInBatch, op.batchSizeByte);
        }
        cnx.ctx().flush();
......
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;三、Pulsar 数据发送端参数调优实践&lt;/h1&gt; 
&lt;p&gt;根据以上对原理解析，我们对 Producer 已经有了一个大致理解，下面通过一个 Producer 参数调优实践案例来帮助读者基于原理进一步理解客户端参数之间的联系。&lt;/p&gt; 
&lt;h2&gt;3.1 调优目的&lt;/h2&gt; 
&lt;p&gt;首先要清楚为什么要进行参数调优，有以下两个目的：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;降低参数使用门槛：&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Pulsar client 和 Producer 的几十个配置参数，参数多且联系紧密，需要花费较多的时间成本去理解，同时参数之间存在协同生效互相影响的情况，对普通使用者而言场景复杂理解门槛高，我们希望能够有一套较为通用的参数配置，或有公式化的参数配置方法论。&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;提升单机处理性能：&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;站在客户端的角度，相同时间内处理的数据量越多，则认为单机处理性能更强。作为中间件系统的提供者，我们经常认为性能提升是服务端的事情，想尽办法在 pulsar 的 broker 和 bookie 上去提升单机处理性能，但 pulsar client 作为整个消息中间件系统的核心组件，它能否发送好一份数据，对整个消息中间件系统的性能和稳定性也发挥着至关重要的作用。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;3.2 调优实践&lt;/h2&gt; 
&lt;p&gt;下面就围绕「参数通用模版化」和「提升单机处理性能」两个目的出发并结合上述讲解的数据发送原理，来分享一些实践经验。&lt;/p&gt; 
&lt;h3&gt;3.2.1 关联与场景相关的重点参数&lt;/h3&gt; 
&lt;p&gt;Pulsar 客户端参数虽多但都提供了默认值，不需要一一调整。只需要对业务场景相关的针对性的去调整即可，如我们本次的参数调优目的是提升单机处理性能，则重点关注哪些场景哪些参数可以提升客户端的发送速率、降低服务端的压力，让服务端可以处理更多的数据，有以下四点最为关键：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;batch 打包发送：&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;消息多条批次发送，在降低客户端和服务端网络 IO 的同时也降低了两者的 cpu 的负载。这里需强调的是我们希望 batch 是一个均匀的、「完整」的包，如 pending 队列被打满，batch 只能空等到延迟发送时间过后被发送，没有构建出预期中的 batch，那么可以认为这个 batch 是一个不完整的包，这种 batch 包含的数据量少，对发送效率有着极大的影响。&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;数据压缩：&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Pulsar 是 IO 密集型系统，常规情况下磁盘是系统的主要瓶颈，开启压缩可以有效降低网络 I/O，提升处理相同数据量下的读写能力。由于压缩是针对 batch 的，在发送时间一定的情况下，batch 越大其压缩效果也越好，代表着处理的消息量也更多。&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;RoundRobin 发送：&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;将数据均匀地分配到多个分区中。它的基本思想是轮询将新的数据写入到不同的分区中，以均衡地分散负载。&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;消息堆积控制：&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;maxPendingMessages 信号量和 memoryLimit 限制不直接提升发送速率，但它能够有效保障我们客户端的稳定，也是控制或限制发送效率的重要参数之一。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;涉及的客户端关键参数以及默认值和我们线上调优后设置的数值如下表：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static001.geekbang.org/infoq/fa/fa59c582553e31ab2ecc8027cdce5f13.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;3.2.2 结合 Producer 发送原理分析参数的效果&lt;/h3&gt; 
&lt;p&gt;接下来我们以参数的效用角度来描述一条消息从构建到发送的过程，进一步解释参数如此设置的意义：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;（1）选择分区&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;构建消息后，通过 messageRoutingMode 参数所设置的路由策略来选择分区，这里以 RoundRobinPartition 为路由策略，开启 batch 时则每间隔 partitionSwitchMs 时间换一个分区进行数据发送，partitionSwitchMs 的值为「batchingPartitionSwitchFrequencyByPublish&lt;/p&gt; 
&lt;p&gt;-Delay、batchingMaxPublishDelayMicros」这两个 Producer 参数之积，也就是每 batchingPartition&lt;/p&gt; 
&lt;p&gt;-SwitchFrequencyByPublishDelay 个 batch 的最大打包时间，消息就会轮换一个分区发送。&lt;/p&gt; 
&lt;p&gt;为了能在 batchingMaxPublishDelayMicros 内得到一个较大的包，我们希望这个 batch 接收的消息是连续的，因此 batchingPartitionSwitchFrequency-&lt;/p&gt; 
&lt;p&gt;ByPublishDelay 不能小于 1，同时也希望一个分区之间数据是较为均匀的，所以 batchingPartition-&lt;/p&gt; 
&lt;p&gt;SwitchFrequencyByPublishDelay 也要尽量小，否则分区对应的信号量 maxPendingMessages 耗尽还没有切换分区，就会导致 batch 必须等待一个 batchingMaxPublishDelayMicros 时间。因此将 batchingPartitionSwitchFrequencyByPublishDelay 修改成了 1，保证打包了一个 batch 之后就切换分区，这也极大的避免了分区信号量耗尽，出现发送阻塞。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;（2）消息堆积控制&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;maxPendingMessages 作为分区的信号量，也是「pending 队列」的大小，代表着每个分区能够同时处理的最大消息上限，而 maxPendingMessages-&lt;/p&gt; 
&lt;p&gt;AcrossPartitions 则是针对整个 topic 生效的，maxPendingMessages=min( maxPending-&lt;/p&gt; 
&lt;p&gt;Messages,maxPendingMessagesAcrossPartitions/Partition），由于线上分区可能会变化，有不确定性，因此就使用上而言除非有特殊的使用场景，建议将 maxPendingMessagesAcrossPartitions 设置的比较大，让 maxPendingMessages 生效即可。&lt;/p&gt; 
&lt;p&gt;除了 maxPendingMessages 以外，消息能否接收被放入 pending 队列中，还要看当前正在处理的消息体大小总和是否超过了 memoryLimit 参数的限制，memoryLimit 控制了消息待处理队列中未压缩前的消息有效荷载总和，可以避免在消息有效荷载非常大时，还未触发 maxPendingMessages 限制，就导致内存占用过多出现频繁 GC 和 oom 的问题。由于 memoryLimit 是 client 级别的策略，因此也建议一个 client 对应一个 Poducer。&lt;/p&gt; 
&lt;p&gt;总而言之 maxPendingMessages 控制了每个分区可以处理消息数量的上限，memoryLimit 控制了所有分区可以消息占用内存的上限，两者相辅相成。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;（3）消息 batch 容器打包&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;决定一个 batch 是否打包完成有三个条件控制，batchingMaxBytes、batchingMaxMessages、batchingMaxPublishDelayMicros 满足其一即可，根据这三个参数的含义去设置值看似是容易的，但容易忽略的是 batch 中用来打包的消息也是受 memoryLimit 和 maxPendingMessages 制约的，应该避免出现 batch 中消息的数量超过 memoryLimit 和 maxPendingMessages 导致 batch 打包效率受影响。举个例子，当 maxPendingMessages 设置为 500，而 batchingMaxMessages 设置 1000 时，batch 就永远无法满足消息条数达到 1000 的条件，只能空等 batchingMaxPublishDelayMicros 或者 batchingMaxBytes 两者生效。&lt;/p&gt; 
&lt;h3&gt;3.2.3 公式化模版&lt;/h3&gt; 
&lt;p&gt;通过上述分析，大致了解了关键参数的生效效果，且彼此相互关联，根据这些关系就能够输出一个较为简单的参数调优模版。&lt;/p&gt; 
&lt;p&gt;假设我们发送的单条消息大小为：messageByte；分区数量为：partitionNum。&lt;/p&gt; 
&lt;p&gt;那么对应参数调整公式如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;//业务发送速率越大，这里设置的值越大
maxPendingMessages：一般 1000-2000 之间
 
//这里值也可以设置大一些，让 maxPendingMessages 生效即可
maxPendingMessagesAcrossPartitions = maxPendingMessages * partitionNum
 
//memoryLimit 的值就是打算阻塞总消息大小，这与消息体和 maxPendingMessages 有关
memoryLimit=(maxPendingMessages * partitionNum * messageByte)
 
//batch 的条数不超过「待处理消息队列」大小的一半
batchingMaxMessages=maxPendingMessages/2，这样可以保证在消息发送等待 ack 的时候，该分区剩下一半的空间还能用来构建一个 batch
 
//batch 大小同理，batch 大小不超过「待处理消息队列」消息大小的一半
batchingMaxBytes= Math.min(memoryLimit * 1024 * 1024 /partitionNum/2,1048576)
 
//业务能够接受的延迟大小，一般延迟时间越大，batch 越大
batchingMaxPublishDelayMicros=1ms-100m 皆可
 
//每构建一个 batch 就转换一个分区
batchingPartitionSwitchFrequencyByPublishDelay=1
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;可以看到根据上面的分析，参数之间是有一个模版化的公式，但这也不是唯一的，读者可以根据自己的业务场景进行调整。在真实使用过程中线上的消息大小以及分区数量实际上是会变化的，因此真正的参数设置还需要根据实际情况来确定，比如我们线上通常的做法是根据机器配置将 memoryLimit 直接设置为 64M-256M，分区数量我们线上不会超过 1000，那么这里就假设为 1000，确定了这两个参数，其他的参数的值也就确定了。&lt;/p&gt; 
&lt;h3&gt;3.2.4 效果对比&lt;/h3&gt; 
&lt;p&gt;以线上一个业务参数调优为例，前后都开启压缩的情况下调整上述参数后的一个效果。&lt;/p&gt; 
&lt;p&gt;服务端（Pulsar）：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static001.geekbang.org/infoq/c4/c45ed239c62c651c39708935958c9416.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static001.geekbang.org/infoq/3c/3c0534426c68d4af315d6d436ea2fcdc.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;优化前后对比数据：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static001.geekbang.org/infoq/3b/3bf425c9885d7cb1c6c6f50f6afde351.jpeg" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;相同的写入速率，Pulsar 服务端网卡流量缩减约 50%（batch 包体增加，压缩效果提升），cpu 负载降低约 90%，Pulsar 服务端总体成本相较优化前至少可降低 50% 以上，客户端也有一定程度的负载降低。&lt;/p&gt; 
&lt;p&gt;参数调整后，CPU 负载得到明显降低，一定程度上避免了 CPU 成为系统的瓶颈，同时由于压缩效果的提升，Pulsar 的磁盘 IO 负载得到显著降低，可以用更少的机器处理更多的数据。&lt;/p&gt; 
&lt;h1&gt;四、总结&lt;/h1&gt; 
&lt;p&gt;理解 Producer 发送原理以及核心参数是写好数据发送程序最为有效的手段，最简单的客户端参数优化反而隐藏了巨大的收益。本文通过对 Producer 原理进行剖析、对消息的流转过程中参数效用进行讲解，并配合参数调优实践案例，介绍了具体的分析思路和调优的方法，在实际使用过程中通过对核心的几个上游系统进行调优，服务端单机处理能力至少提升了一倍以上，成本得到了极大的降低。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;参考文章：&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fpulsar.apache.org%2Fdocs%2F4.0.x%2Fconcepts-overview%2F" target="_blank"&gt;https://pulsar.apache.org/docs/4.0.x/concepts-overview/&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/vivotech/blog/18619282</link>
      <guid isPermaLink="false">https://my.oschina.net/vivotech/blog/18619282</guid>
      <pubDate>Sun, 11 May 2025 07:17:00 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>《智能体网络协议技术报告》发布</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;W3C&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzA3OTY5NDI0OA%3D%3D%26mid%3D2247489930%26idx%3D1%26sn%3Dc3c4ed0e725c88f03e2e6cb6c82c13dc%26scene%3D21%23wechat_redirect" target="_blank"&gt;&lt;strong&gt;AI Agent Protocol 社区组&lt;/strong&gt;&lt;/a&gt;于今年 5 月成立，致力于孵化下一代智能体之间的交互协议，让智能体能够在互联网上使用协议进行高效的连接与协作，推动智能体在 Web 上的安全、高效、可信连接与协作。&lt;/p&gt; 
&lt;p&gt;小组现发布《智能体网络协议技术报告》：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fw3c-cg.github.io%2Fai-agent-protocol%2F" target="_blank"&gt;https://w3c-cg.github.io/ai-agent-protocol/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;另见该报告的中文翻译参考：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fw3c-cg.github.io%2Fai-agent-protocol%2Findex_cn.html" target="_blank"&gt;https://w3c-cg.github.io/ai-agent-protocol/index_cn.html&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0612/145659_Z93C_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0612/145939_LjvL_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;这份报告探讨了从语义网（Semantic Web）的未竟愿景到智能体网络（Agentic Web）的演进历程，并分析了构建标准化智能体网络协议的必要性。&lt;/p&gt; 
&lt;p&gt;尽管二十年前提出的语义网构想极具前瞻性，但受限于当时人工智能技术的能力不足，未能充分实现。随着大型语言模型（LLMs）等现代 AI 技术的飞速发展，智能体已具备自主执行任务、进行复杂推理和解决多步骤问题的能力，从而催生了 Agentic Web 的出现。&lt;/p&gt; 
&lt;p&gt;通过系统分析，该报告给出智能体网络的&lt;strong&gt;四大核心趋势&lt;/strong&gt;：&lt;strong&gt;智能体取代传统软件成为互联网基础设施&lt;/strong&gt;、&lt;strong&gt;智能体间实现普遍互联互通&lt;/strong&gt;、&lt;strong&gt;基于协议的原生连接模式&lt;/strong&gt;、以及&lt;strong&gt;智能体的自主组织与协作能力&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;同时，研究揭示了当前互联网架构对 Agentic Web 发展的&lt;strong&gt;三大挑战&lt;/strong&gt;：&lt;strong&gt;数据孤岛限制智能体决策质量、人机界面阻碍智能体交互效率、以及标准协议缺失阻碍智能体协作&lt;/strong&gt;。针对这些挑战，报告详细阐述了智能体网络协议的设计原则与核心需求，并对当前主要智能体网络协议倡议（MCP、A2A、ACP、ANP 等）进行了系统比较与分析。&lt;/p&gt; 
&lt;p&gt;报告强调，建立标准化智能体网络协议对于打破数据孤岛、实现异构智能体协作、构建 AI 原生数据网络，以及最终实现开放、高效的 Agentic Web 具有关键作用，并呼吁各利益相关方积极参与 W3C 的标准化进程。这是一个塑造未来网络的机会：一个更智能、更协作、更赋能的网络，建立在开放和可信的基础之上。一个精心设计的 Agentic Web 具有巨大的变革潜力，而现在正是为其奠定坚实基础的关键时刻。&lt;/p&gt; 
&lt;p&gt;欢迎参与 AI Agent Protocol 社区组，共同定义 AI Agent 的 Web 通信标准，携手构建可信、安全的智能体互联网生态！&lt;/p&gt; 
&lt;p&gt;参与方式详见：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.w3.org%2Fcommunity%2Fagentprotocol%2Fjoin" target="_blank"&gt;https://www.w3.org/community/agentprotocol/join&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;关于 W3C 社区组&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;W3C 通过社区组（Community Groups）为全球社区提供一个广泛交流 Web 技术进而探索孵化未来新标准的开放平台，从而满足日益增长的各方 Web 参与者的技术交流需求。W3C 社区组面向公众开放，任何感兴趣的单位及个人均可参与。&lt;/p&gt; 
&lt;p&gt;W3C 目前设有 144 个社区组：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.w3.org%2Fgroups%2Fcg%2F" target="_blank"&gt;https://www.w3.org/groups/cg/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;欢迎了解如何参与社区组讨论（中文指南）：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.chinaw3c.org%2Fhowtocg.html" target="_blank"&gt;https://www.chinaw3c.org/howtocg.html&lt;/a&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;来源：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FlSPl8HprLBPuGmkJYvEuBA" target="_blank"&gt;https://mp.weixin.qq.com/s/lSPl8HprLBPuGmkJYvEuBA&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/355009/w3-org-agent-network-protocol-whiter-paper</link>
      <guid isPermaLink="false">https://www.oschina.net/news/355009/w3-org-agent-network-protocol-whiter-paper</guid>
      <pubDate>Sun, 11 May 2025 07:00:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>黄仁勋论 AI 与量子技术驱动新浪潮，微美全息正加速量子计算产业化融合</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;北京时间 6 月 12 日，英伟达 CEO 黄仁勋在法国巴黎召开的 VivaTech2025 上表示，对量子计算越来越看好。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;「量子计算正迎来拐点，我们即将能够在一些有趣的领域应用量子计算。」黄仁勋在在本次演讲中表示，英伟达会以多种方式与世界各地的量子计算公司合作。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;div&gt; 
   &lt;div&gt; 
    &lt;div&gt; 
     &lt;div&gt; 
      &lt;div&gt;
       &lt;img src="https://oscimg.oschina.net/oscnet//95030e6360550155a4bc4588318030aa.png" width="719" referrerpolicy="no-referrer"&gt; 
       &lt;div&gt;
        &amp;nbsp;
       &lt;/div&gt; 
      &lt;/div&gt; 
     &lt;/div&gt; 
    &lt;/div&gt; 
   &lt;/div&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;此前，在 GTC 2025 大会上，英伟达举办了首个「量子日」活动，黄仁勋公开表示，「对时间表判断错误」，并宣布设立量子研究中心，旨在帮助量子计算公司利用英伟达硬件助力其工作。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;现如今，黄仁勋称，未来几年或至少下一代超级计算机中，它们中的每一个都将拥有连接到 GPU 的 QPU（量子处理器），QPU 将进行量子计算，而 GPU 将用于预处理、控制、纠错、后处理。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;div&gt; 
   &lt;div&gt; 
    &lt;div&gt; 
     &lt;div&gt; 
      &lt;div&gt;
       &lt;img src="https://oscimg.oschina.net/oscnet//2723d8c890a73ac2a0a84317244e87dc.png" width="719" referrerpolicy="no-referrer"&gt; 
       &lt;div&gt;
        &amp;nbsp;
       &lt;/div&gt; 
      &lt;/div&gt; 
     &lt;/div&gt; 
    &lt;/div&gt; 
   &lt;/div&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;更重要的是，黄仁勋在开场中还表示，GB200 NVL72 系统将加速量子计算产业发展。英伟达正借助 GB200 NVL72 平台与 CUDA-Q 软件栈，推动 AI 与量子计算的协同发展。比如，GB200 NVL72 输出量子训练数据的速度比基于 CPU 的技术快 4000 倍，有助于将最新的 AI 进展引入量子计算。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;事实上，量子计算机是利用量子力学定律，解决对经典计算机来说过于复杂的问题的机器，其目的是处理更多的数据量，以促进医学、科学和金融等领域的突破。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt; 
  &lt;div&gt; 
   &lt;div&gt; 
    &lt;div&gt; 
     &lt;div&gt; 
      &lt;div&gt;
       &lt;img src="https://oscimg.oschina.net/oscnet//e6c8dec8e124671c4d3a31f832222047.png" width="719" referrerpolicy="no-referrer"&gt; 
       &lt;div&gt;
        &amp;nbsp;
       &lt;/div&gt; 
       &lt;div&gt;
        &lt;span&gt;&lt;span&gt;业内人士分析，过去 5 年，人工智能技术特别是生成式 AI 的爆发，看到计算模式出现了很多颠覆性的发展。未来 5 年，量子计算很可能从实验室走向应用，所以，人工智能与量子计算的融合有望成为必然趋势。&lt;/span&gt;&lt;/span&gt;
       &lt;/div&gt; 
      &lt;/div&gt; 
     &lt;/div&gt; 
    &lt;/div&gt; 
   &lt;/div&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;显然，越来越多的企业在加入开拓「量子计算」未来产业新赛道的行列，资料显示，量子计算概念股微美全息（WIMI.US），积极推动量子计算融入 AI 生态，正加速「量子+AI」技术落地，通过构建 AI 平台基础设施、布局技术研究中心及推进量子技术融合，以全栈式布局加码量子计算+人工智能市场。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;当前，微美全息深刻认识到人工智能是底座，量子科技是跃迁力，而两者的融合正是抢占未来产业、未来话语权的关键路径。因此，微美全息以人工智能为基座，聚焦量子技术、人形机器人、人工智能三大风口，扩展布局量子产业前沿领域，未来，将重点关注量子算法加速 AI 训练、神经拟态计算等融合赛道，让更多成果涌现。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div&gt; 
 &lt;div&gt;
  &lt;span&gt;&lt;span&gt;整体而言，量子计算与经典计算的融合将成为未来技术发展的重要方向，许多企业在这一领域的布局显示了其对前沿技术的敏锐洞察。不过也要意识到一点，目前量子计算的产业格局仍处于早期阶段，格局尚未成型，全球量子计算机数量较少，量子计算芯片将是整个领域未来发展的重点和难点。&lt;/span&gt;&lt;/span&gt;
 &lt;/div&gt; 
&lt;/div&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/355008</link>
      <guid isPermaLink="false">https://www.oschina.net/news/355008</guid>
      <pubDate>Sun, 11 May 2025 06:56:00 GMT</pubDate>
      <author>来源: 投稿</author>
    </item>
    <item>
      <title>Meta 发布开源世界模型 V-JEPA 2</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;Meta 发布了最新的开源世界模型&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fai.meta.com%2Fblog%2Fv-jepa-2-world-model-benchmarks%2F" target="_blank"&gt;V-JEPA 2&lt;/a&gt;，称其在物理世界中实现了最先进的视觉理解和预测，从而提高了 AI agents 的物理推理能力。&lt;/p&gt; 
&lt;p&gt;&lt;img height="559" src="https://static.oschina.net/uploads/space/2025/0612/144127_0qiP_2720166.png" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;开源地址：https://github.com/facebookresearch/vjepa2&lt;/em&gt;&lt;br&gt; &lt;em&gt;官网地址&lt;/em&gt;&lt;em&gt;：https://ai.meta.com/vjepa/&lt;br&gt; 论文地址：https://ai.meta.com/research/publications/v-jepa-2-self-supervised-video-models-enable-understanding-prediction-and-planning/&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;V-JEPA 2 是一种&lt;strong&gt;联合嵌入预测架构&lt;/strong&gt;（&lt;strong&gt;Joint Embedding Predictive Architecture&lt;/strong&gt;）模型，这也是「JEPA」的名称由来。&lt;/p&gt; 
&lt;p&gt;模型包括两个主要组成部分：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;一个编码器，负责接收原始视频，并输出包含对于观察世界状态语义上有用的内容的嵌入（embeddings）。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img height="597" src="https://static.oschina.net/uploads/space/2025/0612/150421_f1zo_2720166.png" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;一个预测器，负责接收视频嵌入和关于要预测的额外内容，并输出预测的嵌入。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0612/150628_SYFp_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;V-JEPA 2 跟传统预测像素的生成式模型有很大性能差异，根据 Meta 测试数据，V-JEPA 2 执行任务时每个步骤的规划用时缩短至 Cosmos 模型的三十分之一，不仅用时短，V-JEPA 2 的成功率还更高。&lt;/p&gt; 
&lt;p&gt;V-JEPA 2 的能力对现实世界 agents 理解复杂运动和时间动态（temporal dynamics），以及根据上下文线索预测动作都非常关键。基于这种预测能力，世界模型对于规划给定目标的动作顺序非常有用，比如从一个杯子在桌子上的状态到杯子在桌子边上的状态，中间要经历怎样的动作。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0612/150646_SoAo_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;据介绍，V-JEPA 2 的核心架构是一个自监督学习框架，通过互联网规模的视频数据来训练模型，使其能够学习到视频中的动态和静态信息。预训练阶段使用了超过 100 万小时的视频和 100 万张图像，这些数据涵盖了各种动作和场景。预训练的目标是让模型能够通过观察学习到世界的背景知识，而无需依赖于大量的标注数据。&lt;/p&gt; 
&lt;p&gt;值得一提的是，图灵奖获得者、Meta 首席科学家杨立昆（Yann LeCun）参与了该模型的开发，这在 Meta 开源的众多大模型中很罕见。他在官方视频中提到，在世界模型的帮助下，AI 不再需要数百万次的训练才能掌握一项新的能力，世界模型直接告诉了 AI 世界是怎样运行的，这可以极大提升效率。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/355002/meta-vjepa-2-world-model</link>
      <guid isPermaLink="false">https://www.oschina.net/news/355002/meta-vjepa-2-world-model</guid>
      <pubDate>Sun, 11 May 2025 06:45:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Liquid Glass React —— 「液态玻璃」的 React 实现</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                            &lt;p&gt;Liquid Glass React 是苹果「液态玻璃（Liquid Glass）」设计语言的 React 实现。&lt;/p&gt;

&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-09f93126aa8a24944a023d1e74e2ab9d9a5.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-f90f27edf2a257be2d13ededab62bc276f0.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;特性&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;良好的边缘弯曲和折射&lt;/li&gt;
&lt;li&gt;多种折射模式&lt;/li&gt;
&lt;li&gt;可配置的冰霜级别&lt;/li&gt;
&lt;li&gt;支持任意子元素&lt;/li&gt;
&lt;li&gt;配置的填充&lt;/li&gt;
&lt;li&gt;修正悬停和点击效果&lt;/li&gt;
&lt;li&gt;边缘和高亮会像苹果一样呈现底层光线&lt;/li&gt;
&lt;li&gt;可配置的色差&lt;/li&gt;
&lt;li&gt;可配置的弹性参数，以模仿苹果的"液体"触感&lt;/li&gt;
&lt;/ul&gt;

                                                                    &lt;/div&gt;
                                                                </description>
      <link>https://www.oschina.net/p/liquid-glass-react</link>
      <guid isPermaLink="false">https://www.oschina.net/p/liquid-glass-react</guid>
      <pubDate>Sun, 11 May 2025 06:21:00 GMT</pubDate>
    </item>
    <item>
      <title>小红书、B 站等平台清理违规 AI 产品营销信息</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;&lt;span style="color:#000000"&gt;网信上海微信公号&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fqmfe8tiUlXJDnt525ucA-Q" target="_blank"&gt;发文称&lt;/a&gt;，为贯彻落实中央网信办「清朗·整治 AI 技术滥用」工作部署，4 月下旬以来，上海市委网信办聚焦 6 类突出问题深入开展第一阶段专项行动。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="329" src="https://oscimg.oschina.net/oscnet/up-11e6dea0c86ec77bcd50df467bb75257ff6.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;上海市委网信办指导小红书、哔哩哔哩、拼多多等 15 家重点网站平台，集中清理「一键脱衣」、未经授权的人脸或人声克隆编辑、未备案等违规 AI 产品、商品及相关营销、炒作、推广、教程信息。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;小红书、哔哩哔哩主动发布专项行动治理公告，开通了有害 AI 内容的举报受理处置渠道；星野开展智能体全面排查清理。各重点网站和 AI 平台共拦截清理相关违法违规信息 82 万余条，处置违规账号 1400 余个，下线违规智能体 2700 余个。经整治，网络违规 AI 信息显著减少。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;目前，上海市已完成 82 款大模型备案，87 款应用登记。对 3 款未履行备案或登记程序提供服务且存在风险的应用，上海市委网信办依法约谈并给予行政处罚。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354996</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354996</guid>
      <pubDate>Sun, 11 May 2025 06:19:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>10+ PG 国际大咖助阵！IvorySQL 生态大会 6.27 开启</title>
      <description>IvorySQL 2025 生态大会将于 6 月 27 日在济南开幕，汇集来自开源和 PostgreSQL 社区的采用者和国内外技术专家。本次大会将以 PostgreSQL 技术生态为核心，聚焦全球数据库技术发展趋势、开源创新与行...</description>
      <link>https://howconf.cn/</link>
      <guid isPermaLink="false">https://howconf.cn/</guid>
      <pubDate>Sun, 11 May 2025 05:52:00 GMT</pubDate>
    </item>
    <item>
      <title>苹果高管回应「个性化版 Siri」延期：技术架构限制导致未达到预期标准</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;在今年的 WWDC25 主题演讲中，苹果的 AI 智能助手 Siri 显然没有占据重要位置。苹果仅简要提到它，并重申了开发进展比预期更慢，集成苹果智能（Apple Intelligence）需要更长时间，预计将在 「明年」 推出。&lt;/p&gt; 
&lt;p&gt;演讲结束后，苹果软件工程高级副总裁克雷格・费德里吉（Craig Federighi）和全球营销副总裁格雷格・乔斯维亚克（Greg Joswiak）&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.tomsguide.com%2Fai%2Fapple-intelligence%2Fwwdc-interview-apples-craig-federighi-and-greg-joswiak-on-siri-delay-voice-ai-as-therapist-and-whats-next-for-apple-intelligence"&gt;进行了深度对话&lt;/a&gt;，解释了苹果在 Apple Intelligence、Siri 和 AI 领域的战略思考。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-fcd007add07610a84ea1a7e78288a2d830c.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;苹果曾承诺在 2024 年底发布集成 Apple Intelligence 的 Siri 更新，但最终未能如期交付，并在 2025 年春季承认该功能还需要更多时间。至于原因，外界一直未能搞清楚，而苹果一向不会展示那些无法按时交付的技术或产品。&lt;/p&gt; 
&lt;p&gt;访谈中，费德里吉详细解释了问题的所在，并说明了苹果如何继续推进这一计划。&lt;/p&gt; 
&lt;p&gt;费德里吉提到在开发过程中意识到可以&lt;strong&gt;基于设备端的大语言模型、私人云计算基础设施和设备端的语义索引技术提升 Siri 的智能化水平&lt;/strong&gt;，&lt;strong&gt;并设想通过 V1 架构协调应用意图来触发设备上的更多操作&lt;/strong&gt;，让 Siri 执行更多任务。例如，利用语义索引中的个人知识，当用户询问特定问题时，Siri 能从消息或邮件中找到相关内容，并通过应用意图执行相关操作。但这些功能在 V1 架构下尚未完全交付。&lt;/p&gt; 
&lt;p&gt;在苹果致力于开发 Siri 架构 V1 的同时，它也在打造 V2 架构 —— 费德里吉称其为 「&lt;strong&gt;更深层次的端到端架构&lt;/strong&gt;，我们知道这才是最终要实现的架构，是让 Siri 具备完整功能的架构。」&lt;/p&gt; 
&lt;p&gt;费德里吉说道：「我们花了几个月的时间，不断优化 V1 架构在更多应用意图和搜索功能方面的表现。但从根本上看，我们发现该架构的局限性没有办法达到客户所期望的质量水平。因此，我们决定转向 V2 架构。但当我们意识到这一点时，已经是春季了，于是我们向外界说明，无法按计划发布，并将继续转向新架构。」&lt;/p&gt; 
&lt;p&gt;费德里吉进一步表示，即便采用第二代架构，苹果仍在不断优化 Siri 功能，确保其达到最佳状态。&lt;/p&gt; 
&lt;p&gt;苹果市场负责人 Greg Joswiak 在访谈中确认，「未来一年」 指向 2026 年，科技媒体 MacRumors 推测个性化 Siri 功能很可能随 iOS 26.4 版本于 2026 年春季面世。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;相关阅读：&lt;a href="https://www.oschina.net/news/337918/apple-says-some-ai-improvements-siri-delayed" target="news"&gt;苹果推迟上线 Siri 中的 AI 相关功能&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354969/apples-on-siri-delay</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354969/apples-on-siri-delay</guid>
      <pubDate>Sun, 11 May 2025 03:31:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>阿里蔡崇信：被 DeepSeek 逼急，工程师春节彻夜留守搞研发</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;&lt;span style="color:#000000"&gt;彭博社报道称，DeepSeek 在，今年 1 月推出低成本、功能强大的人工智能模型震惊全球科技行业后，也给阿里巴巴带来了巨大的紧迫感。为迅速追赶这一技术突破，阿里巴巴的工程师们甚至取消了最重要的中国传统节日——春节的休假，彻夜留守公司，全力投入 AI 研发。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="325" src="https://oscimg.oschina.net/oscnet/up-a5dcc123152fe1c8b0935e049dda3c97f2b.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;阿里巴巴董事长蔡崇信在本周三举行的巴黎 VivaTech 科技大会上，生动地讲述了这段「争分夺秒」的经历。他表示，当 DeepSeek 发布 R1 模型时，阿里巴巴内部意识到自身在 AI 领域已然落后。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;面对这一挑战，公司的工程负责人当机立断，决定取消春节假期，要求所有工程师留在公司，甚至睡在办公室里，以最快的速度进行开发。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;「几周之内，我们推出了自己的版本——通义（Qwen）系列模型，表现相当不错，竞争力很强。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;此后，阿里巴巴推出了一系列新的 AI 模型，并将公司的业务重心进一步转向人工智能和通用人工智能（AGI）。为支撑这一战略转型，阿里巴巴还承诺在未来三年内投入超过 3800 亿元人民币 (约合 530 亿美元)，用于建设包括数据中心在内的 AI 基础设施。今年早些时候，阿里巴巴还与苹果公司达成合作协议，为 iPhone 提供 AI 技术支持。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354967</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354967</guid>
      <pubDate>Sun, 11 May 2025 03:13:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>可观测性第四大支柱：配置数据的监控</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                                        &lt;p style="color:#333333; text-align:left"&gt;业内经常讲可观测性有三大支柱：指标、日志、链路追踪，本文作者认为，还有第四大支柱：那就是配置类数据。配置类数据的变更也会影响系统的稳定性，也值得被监控，方便我们快速排查问题。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p style="color:#333333"&gt;&lt;span&gt;原文链接：https://www.cloudquery.io/blog/fourth-lost-pillar-of-observability-config-data-monitoring&lt;/span&gt;&lt;br&gt; &lt;span&gt;原文作者：Yevgeny Pats&lt;/span&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p style="color:#333333; text-align:left"&gt;很多关于日志、指标和跟踪的内容已经被广泛讨论，因为它们确实是可观测性、应用程序和系统监控的关键组成部分。然而，经常被忽视的是配置数据及其可观测性。在这篇博客中，我们将探讨什么是配置数据，它与日志、指标和跟踪有何不同，并讨论需要什么样的架构来存储这种类型的数据以及在哪些场景中它具有价值。&lt;/p&gt; 
&lt;span id="OSC_h2_1"&gt;&lt;/span&gt; 
&lt;h2&gt;日志、指标和链路追踪的快速回顾&lt;/h2&gt; 
&lt;p style="color:#333333; text-align:left"&gt;对于那些对可观测性的三大支柱还不太了解的人来说，让我们快速回顾一下：&lt;/p&gt; 
&lt;p style="color:#333333; text-align:left"&gt;&lt;img height="738" src="https://oscimg.oschina.net/oscnet/up-58f95ee676e13fe3a0619f3d3d0f956e34d.png" width="1316" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;日志：系统内发生事件的详细记录。它们提供了关于特定事件的信息，包括时间戳、错误消息和其他相关细节。日志有助于调试和故障分析。&lt;/li&gt; 
 &lt;li&gt;指标：定期收集的数值测量。它们有助于监控系统健康状况、性能和随时间的行为。示例包括 CPU 使用率、请求速率、错误率和响应时间。&lt;/li&gt; 
 &lt;li&gt;跟踪：记录请求在分布式系统中通过不同服务的流程。跟踪提供请求流程的可见性，有助于识别瓶颈并理解依赖关系。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#333333; text-align:left"&gt;这些技术的后端通常是某种时间序列数据库，数据类型通常是低基数数据（可以是高基数，但通常会变得昂贵且不建议这样做）。另一个关键方面是，要获取这些指标，通常需要对系统进行插桩，即，您需要访问应用程序或基础设施，以便部署 agent 或添加 Prometheus Exporter。&lt;/p&gt; 
&lt;span id="OSC_h2_2"&gt;&lt;/span&gt; 
&lt;h2&gt;配置数据：第四支柱&lt;/h2&gt; 
&lt;p style="color:#333333; text-align:left"&gt;基础设施不仅限于 AWS EC2 实例，还包括 IAM 用户、安全工具配置、SaaS 应用程序等。这些配置数据在几个重要方面与传统的可观测性数据不同：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;无法直接监控：这些系统无法直接进行监控，但它们通过 API 暴露其配置。&lt;/li&gt; 
 &lt;li&gt;高基数和关系型：数据通常具有高基数，并且高度关系型。它也不像服务器上的磁盘 I/O 指标那样频繁变化，而是更侧重于配置状态。&lt;/li&gt; 
 &lt;li&gt;较低频率，更高细节：我们在这里想要做的权衡是较少的采集频率，但具有更高的基数和细节。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;span id="OSC_h2_3"&gt;&lt;/span&gt; 
&lt;h2&gt;为什么配置数据很重要&lt;/h2&gt; 
&lt;p style="color:#333333; text-align:left"&gt;配置数据监控填补了您可观测性策略中的关键空白：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;安全态势监控：跟踪 IAM 权限、安全组规则、加密设置以及其他影响您安全态势的配置项。&lt;/li&gt; 
 &lt;li&gt;合规性跟踪：监控配置以符合内部政策或外部合规要求（SOC2、HIPAA、PCI-DSS 等）。&lt;/li&gt; 
 &lt;li&gt;成本优化：识别导致不必要的成本的配置错误，例如过大实例或未使用的资源。&lt;/li&gt; 
 &lt;li&gt;变更管理：检测并跟踪环境中的配置变更，提供谁在何时变更了什么的可见性。&lt;/li&gt; 
 &lt;li&gt;漂移检测：识别资源何时偏离其预期或期望的配置。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;span id="OSC_h2_4"&gt;&lt;/span&gt; 
&lt;h2&gt;配置数据监控架构&lt;/h2&gt; 
&lt;p style="color:#333333; text-align:left"&gt;让我们来看看我们在 CQ（指的是 CLoudQuery，作者所在公司） 处理配置数据时做出的一些关键架构决策：&lt;/p&gt; 
&lt;p style="color:#333333; text-align:left"&gt;&lt;img height="858" src="https://oscimg.oschina.net/oscnet/up-b79c63c47f59349231edbdaef7a8e21ed94.png" width="1200" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h3_5"&gt;&lt;/span&gt; 
&lt;h3&gt;数据摄取&lt;/h3&gt; 
&lt;p style="color:#333333; text-align:left"&gt;首先，数据提取的挑战是不同的问题。我们无法对这些系统进行监控，因此需要创建提取器（或 ETL 脚本），主要挑战是维护这些连接器。任何希望解决这一需求的系统都必须维护高质量的数据源连接器。&lt;/p&gt; 
&lt;p style="color:#333333; text-align:left"&gt;收集频率通常为每日一次，但根据配置的重要性，有时可能需要将其配置为更高的频率。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p style="color:#333333"&gt;译者注：如果频率这么低，从监控的角度感觉是不够用的。真的发生了故障，黄花菜都凉了。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;span id="OSC_h3_6"&gt;&lt;/span&gt; 
&lt;h3&gt;存储&lt;/h3&gt; 
&lt;p style="color:#333333; text-align:left"&gt;由于从 API 获取的数据具有高度相关性，我们使用了一个 SQL 数据库，可以在其中创建复杂的连接。NoSQL 数据库或时间序列数据库并不适合这种用例。&lt;/p&gt; 
&lt;p style="color:#333333; text-align:left"&gt;在这里，频率和基数之间的权衡可能是按日分区。一些提取器可能会运行得更频繁，但快照操作通常仍然会按日运行；否则，数据量会爆炸。&lt;/p&gt; 
&lt;span id="OSC_h3_7"&gt;&lt;/span&gt; 
&lt;h3&gt;生成洞察&lt;/h3&gt; 
&lt;p style="color:#333333; text-align:left"&gt;这与可观测性平台解决「空白页面综合征」（即「我有数据，现在我要监控什么？」）的方式有些类似。我们提供了大量的开箱即用的洞察，但我们也认识到每个组织的需求略有不同，并且在云治理方面没有一刀切的规则。因此，客户可以访问原始查询并对其进行修改，也可以添加新的自定义数据源。&lt;/p&gt; 
&lt;span id="OSC_h3_8"&gt;&lt;/span&gt; 
&lt;h3&gt;关系和物化视图&lt;/h3&gt; 
&lt;p style="color:#333333; text-align:left"&gt;将配置数据存储在关系数据库中的一个显著优势是能够建模和查询不同配置项之间的关系。例如：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;哪个 IAM 角色可以访问哪些 S3 桶？&lt;/li&gt; 
 &lt;li&gt;哪些安全组与哪些实例相关联？&lt;/li&gt; 
 &lt;li&gt;您的 Kubernetes RBAC 设置与云 IAM 权限有何关系？&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#333333; text-align:left"&gt;Materialized views 可以用于预先计算常见的关系查询，从而提高经常性请求的性能。&lt;/p&gt; 
&lt;span id="OSC_h2_9"&gt;&lt;/span&gt; 
&lt;h2&gt;与传统可观测性集成&lt;/h2&gt; 
&lt;p style="color:#333333; text-align:left"&gt;当配置数据作为第四支柱时，其真正的力量在于与传统可观测性数据集成后展现出来：&lt;/p&gt; 
&lt;p style="color:#333333; text-align:left"&gt;&lt;img height="324" src="https://oscimg.oschina.net/oscnet/up-fc821dc2087372e43f3f7c246362363b506.png" width="818" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;根本原因分析：当发生故障时，将指标、日志和跟踪与配置更改相关联可以迅速识别根本原因。&lt;/li&gt; 
 &lt;li&gt;上下文增强：通过配置上下文增强指标和日志（例如，「此错误峰值发生在对负载均衡器进行配置更改之后」）。&lt;/li&gt; 
 &lt;li&gt;主动监控：在这些配置变化影响您的指标之前，检测可能导致未来性能问题或中断的配置变化。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;span id="OSC_h2_10"&gt;&lt;/span&gt; 
&lt;h2&gt;挑战与考虑&lt;/h2&gt; 
&lt;p style="color:#333333; text-align:left"&gt;实施配置数据监控自身也伴随着一系列挑战：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;API 速率限制：许多服务对他们的 API 设定了速率限制，这可能会限制你收集配置数据的频率。&lt;/li&gt; 
 &lt;li&gt;认证和授权：管理众多系统的凭据和权限需要仔细考虑安全问题。&lt;/li&gt; 
 &lt;li&gt;数据体积管理：即使采集频率较低，配置数据的高基数也可能导致显著的存储需求。&lt;/li&gt; 
 &lt;li&gt;Schema 迁移：API 随时间发生变化，需要适应您的数据提取和存储机制。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;span id="OSC_h2_11"&gt;&lt;/span&gt; 
&lt;h2&gt;总结&lt;/h2&gt; 
&lt;p style="color:#333333; text-align:left"&gt;日志、指标和跟踪仍然是可观测性的关键组成部分，而配置数据代表了第四根支柱，提供了对您系统独特见解。通过实施全面的配置数据监控，组织可以增强其安全态势、确保合规性、优化成本，并更深入地了解其基础设施。&lt;/p&gt; 
&lt;p style="color:#333333; text-align:left"&gt;随着系统变得越来越复杂和分布式，配置数据监控的价值将只会增加。那些认识到这一第四支柱并将其纳入其可观测性策略中的组织，将更好地处于理解、排查故障和优化其日益复杂基础设施的有利位置。&lt;/p&gt; 
&lt;p style="color:#333333; text-align:left"&gt;译者注：原文作者这个观点值得借鉴，但是对于故障定位等场景真的那么有用吗？也未可知。具体实施时，建议先从 ROI 高的方面着手，从那些你觉得最重要的配置数据开始。Google SRE 统计生产环境 70% 故障是变更导致的，译者建议您先把变更事件收集起来，对于排障还是蛮有用的。我们在 Flashcat 里专门做了一个「事件墙」的产品，就是用来收集变更事件的。如果生产环境发生故障，从故障时间点往前看一个小时，该时间段内的变更事件，很可能就是故障的罪魁祸首。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/morflameblog/blog/18332409</link>
      <guid isPermaLink="false">https://my.oschina.net/morflameblog/blog/18332409</guid>
      <pubDate>Sun, 11 May 2025 02:59:00 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>香橙派开发板成功适配开源鸿蒙 OpenHarmony 5.0</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;香橙派&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FBDc9PFNAbhNS9HiX-3Bg4A"&gt;宣布&lt;/a&gt;完成 OrangePi RV2 与开源鸿蒙 OpenHarmony5.0 的适配工作，&lt;/p&gt; 
&lt;p&gt;据介绍，OrangePi RV2 是香橙派在 RISC-V 布局的一个标志性产品，其采用开芯微首款 8 核 64 位 RISC-V AI CPU Ky X1。它以 RISC-V 开源指令集为基础，提供快速、高效、易用的处理器平台，释放算力潜能。OrangePi RV2 精致小巧，尺寸仅为 89mmX56mmX1.6mm，功能强大，可广泛应用于 NAS、商用电子产品、智慧机器人、智慧家居、工业控制、边缘计算等。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0612/104611_2b3H_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;除了适配 OpenHarmony 5.0&amp;nbsp;之外，OrangePi RV2 还具有以下亮点：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;以 CPU 方式提供 AI 算力，在无需独立 NPU 模块的情况下，实现 2TOPS@INT8 的 AI 能力。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;支持双 M.2 SSD&amp;nbsp;扩展，支持 Wi-Fi5.0 和蓝牙 5.0，扩展有 HDMI 2.0、三个 USB3.0、双千兆 LAN、USB-C&amp;nbsp;供电接口。此外，还有两个&amp;nbsp;MIPI-CSI&amp;nbsp;摄像头接口（四通道），以及 26Pin GPIO&amp;nbsp;等。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;支持本地部署 Deepseek-R1 蒸馏模型，通过在边缘进行离线部署。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;OrangePi RV2 售价 231 元起。香橙派已在官网放出开发板的&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fwww.orangepi.cn%2Fhtml%2FhardWare%2FcomputerAndMicrocontrollers%2Fservice-and-support%2FOrange-Pi-RV2.html"&gt;Orange Pi OS（OH）镜像&lt;/a&gt;，Orange Pi OS（OH）是以 OpenHarmony 为主要技术路线，结合 Linux 技术积累构建的多端融合操作系统。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;相关阅读：&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/news/337094" target="news"&gt;香橙派首款高性能开源 RISC-V 开发板 (OrangePi RV) 即将开售，229 元起&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/news/263519" target="news"&gt;香橙派 Orange Pi OS (OH) 即将发布，开源鸿蒙 PC 端&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354959</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354959</guid>
      <pubDate>Sun, 11 May 2025 02:48:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>特朗普政府新 AI 计划「AI.gov」在 GitHub 上被泄露</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farchive.is%2Fhfl2Z"&gt;根据相关备份资料&lt;/a&gt;，美国总务管理局（GSA）在 GitHub 上发布的一个早期版本的网站和代码显示，该联邦政府正在开发一个名为 「ai.gov」 的网站和 API，旨在 「用 AI 加速政府创新」，该计划定于 7 月 4 日启动，并将包含一个分析功能，显示特定政府团队使用 AI 的程度。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-e8e700d454b9a1e4361cf7bfe632412ef29.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;AI.gov 网站包含三个主要部分：聊天机器人、「全能 API」和 CONSOLE 工具。&lt;/p&gt; 
&lt;p&gt;页面早期版本显示，其 API 将与 OpenAI、谷歌和 Anthropic 的模型产品集成；而 API 代码进一步表明，开发团队也在致力于整合亚马逊网络服务（AWS）的 Bedrock 和 Meta（Facebook 母公司） 的 LLaMA。此外，页面提到将配备 AI 聊天机器人，但未说明其具体功能。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1444" src="https://static.oschina.net/uploads/space/2025/0612/103441_xzHk_2720166.png" width="1584" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="1496" src="https://static.oschina.net/uploads/space/2025/0612/103419_zWpN_2720166.png" width="1702" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;https://github.com/GSA-TTS/ai.gov&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;相关来源&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.404media.co%2Fgithub-is-leaking-trumps-plans-to-accelerate-ai-across-government%2F" target="_blank"&gt;https://www.404media.co/github-is-leaking-trumps-plans-to-accelerate-ai-across-government/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.theverge.com%2Fnews%2F684579%2Fai-api-trump-administration-doge-gsa" target="_blank"&gt;https://www.theverge.com/news/684579/ai-api-trump-administration-doge-gsa&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmedium.com%2F%40telumai%2Fgithub-reveals-ai-gov-trumps-plan-to-automate-government-with-openai-meta-and-musk-s-doge-76f1676462e6" target="_blank"&gt;https://medium.com/@telumai/github-reveals-ai-gov-trumps-plan-to-automate-government-with-openai-meta-and-musk-s-doge-76f1676462e6&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/354956/github-is-leaking-trumps-plans-to-accelerate-ai-across-gov</link>
      <guid isPermaLink="false">https://www.oschina.net/news/354956/github-is-leaking-trumps-plans-to-accelerate-ai-across-gov</guid>
      <pubDate>Sun, 11 May 2025 02:35:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
  </channel>
</rss>
