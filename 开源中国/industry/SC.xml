<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - industry - 简体中文</title>
    <link>https://www.oschina.net/news/industry</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news/industry" rel="self" type="application/rss+xml"/>
    <description>已对该 RSS 进行格式化操作：中英字符之间插入空格、使用直角引号、标点符号修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 05 Aug 2025 07:40:54 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>谷歌 AI 编程 Agent 「Jules」 支持创建 PR</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;谷歌 AI 编程 Agent 「Jules」&amp;nbsp;新增创建拉取请求（PR）的功能，实现了从编码到提交的完整开发闭环。&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fjules.google%2Fdocs%2Fchangelog%2F" target="_blank"&gt;根据 Jules 的更新日志&lt;/a&gt;，它现在可以将代码变更整合并创建拉取请求（Pull Request）。完成任务后，用户可以要求 Jules 打包变更、撰写摘要并开启一个待审查的 PR，实现了从规划、编码、提交到 PR 的完整闭环。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-be3ad0bacfe87f8715b643becb4e4d845d2.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="636" src="https://static.oschina.net/uploads/space/2025/0805/152238_a4vc_2720166.png" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Jules 官方称，本周是「Jules 发布周」，预计将会有更多功能升级。Google AI 开发者关系负责人 Logan Kilpatrick 也在社交媒体上发帖称「big week ahead!」（未来将是重要的一周！），不确定是否仅与 Jules 有关。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0805/152248_VoyB_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364327</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364327</guid>
      <pubDate>Tue, 05 Aug 2025 07:24:52 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>警惕 AI 数据投毒，0.01% 虚假训练文本可致有害内容增加 11.2%</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;&lt;span style="color:#000000"&gt;国家安全部发布安全提示&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FjJ33UUZzgVn3LD9BpJe1bA" target="_blank"&gt;文章&lt;/a&gt;指出，当前，人工智能已深度融入经济社会发展的方方面面，在深刻改变人类生产生活方式的同时，也成为关乎高质量发展和高水平安全的关键领域。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;然而，人工智能的训练数据存在良莠不齐的问题，其中不乏虚假信息、虚构内容和偏见性观点，造成数据源污染，给人工智能安全带来新的挑战。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;人工智能的三大核心要素是算法、算力和数据，其中数据是训练 AI 模型的基础要素，也是 AI 应用的核心资源。&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;提供 AI 模型的原料。&lt;/strong&gt;海量数据为 AI 模型提供了充足的训练素材，使其得以学习数据的内在规律和模式，实现语义理解、智能决策和内容生成。同时，数据也驱动人工智能不断优化性能和精度，实现模型的迭代升级，以适应新需求。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;影响 AI 模型的性能。&lt;/strong&gt;AI 模型对数据的数量、质量及多样性要求极高。充足的数据量是充分训练大规模模型的前提；高准确性、完整性和一致性的数据能有效避免误导模型；覆盖多个领域的多样化数据，则能提升模型应对实际复杂场景的能力。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;促进 AI 模型的应用。&lt;/strong&gt;数据资源的日益丰富，加速了「人工智能+」行动的落地，有力促进了人工智能与经济社会各领域的深度融合。这不仅培育和发展了新质生产力，更推动我国科技跨越式发展、产业优化升级、生产力整体跃升。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;高质量的数据能够显著提升模型的准确性和可靠性，但数据一旦受到污染，则可能导致模型决策失误甚至 AI 系统失效，存在一定的安全隐患。&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;投放有害内容。&lt;/strong&gt;通过篡改、虚构和重复等「数据投毒」行为产生的污染数据，将干扰模型在训练阶段的参数调整，削弱模型性能、降低其准确性，甚至诱发有害输出。研究显示，当训练数据集中仅有 0.01% 的虚假文本时，模型输出的有害内容会增加 11.2%；即使是 0.001% 的虚假文本，其有害输出也会相应上升 7.2%。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;造成递归污染。&lt;/strong&gt;受到数据污染的人工智能生成的虚假内容，可能成为后续模型训练的数据源，形成具有延续性的「污染遗留效应」。当前，互联网 AI 生成内容在数量上已远超人类生产的真实内容，大量低质量及非客观数据充斥其中，导致 AI 训练数据集中的错误信息逐代累积，最终扭曲模型本身的认知能力。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;引发现实风险。&lt;/strong&gt;数据污染还可能引发一系列现实风险，尤其在金融市场、公共安全和医疗健康等领域。在金融领域，不法分子利用 AI 炮制虚假信息，造成数据污染，可能引发股价异常波动，构成新型市场操纵风险；在公共安全领域，数据污染容易扰动公众认知、误导社会舆论，诱发社会恐慌情绪；在医疗健康领域，数据污染则可能致使模型生成错误诊疗建议，不仅危及患者生命安全，也加剧伪科学的传播。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;为了应对数据污染带来的威胁，国家安全部建议：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;加强源头监管，防止污染数据的产生。&lt;/strong&gt;以《网络安全法》《数据安全法》《个人信息保护法》等法律法规为依据，建立 AI 数据分类分级保护制度，从根本上防范污染数据的产生，助力有效防范 AI 数据安全威胁。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;强化风险评估，保障数据流通。&lt;/strong&gt;加强对人工智能数据安全风险的整体评估，确保数据在采集、存储、传输、使用、交换和备份等全生命周期环节安全。同步加快构建人工智能安全风险分类管理体系，不断提高数据安全综合保障能力。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;末端清洗修复，构建治理框架。&lt;/strong&gt;定期依据法规标准清洗修复受污数据。依据相关法律法规及行业标准，制定数据清洗的具体规则。逐步构建模块化、可监测、可扩展的数据治理框架，实现持续管理与质量把控。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364320</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364320</guid>
      <pubDate>Tue, 05 Aug 2025 07:06:52 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>阿里巴巴 2026 秋季校招计划超 6 成 AI 相关岗位</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p style="color:#2a3840; margin-left:.2rem; margin-right:0; text-align:justify"&gt;阿里巴巴 2026 届秋季校园招聘正式&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftalent-holding.alibaba.com%2Fcampus%2Fhome%3Flang%3Dzh" target="_blank"&gt;启动&lt;/a&gt;，计划发出超过 7000 个录用通知。此次招聘涵盖阿里巴巴控股集团、淘天、阿里云、阿里国际、通义实验室、智能信息、钉钉、高德等 15 个业务集团和公司。&lt;/p&gt; 
&lt;p style="color:#2a3840; margin-left:.2rem; margin-right:0; text-align:justify"&gt;与春季招聘相比，秋季校招显著加大了 AI 人才招聘力度。AI 相关岗位占比超过六成。部分 AI 业务部门的招聘比例更为突出，阿里云、阿里国际、钉钉的 AI 岗位占比达到 80%，高德的相关比例也达到 75%。&lt;/p&gt; 
&lt;p style="color:#2a3840; margin-left:.2rem; margin-right:0; text-align:justify"&gt;&lt;img height="341" src="https://oscimg.oschina.net/oscnet/up-fcbb54eb5d1a7776ad186909f3ec872638f.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#2a3840; margin-left:.2rem; margin-right:0; text-align:justify"&gt;据悉，阿里国际在 2026 届校招中，80% 的职位均为 AI 岗位。这些职位包括 AI 算法工程师、研发工程师以及 AI 产品经理等关键技术岗位。与此同时，阿里国际启动了面向全球的头部 AI 科技人才培养计划 Bravo102。&lt;/p&gt; 
&lt;p style="color:#2a3840; margin-left:.2rem; margin-right:0; text-align:justify"&gt;阿里巴巴集团董事会主席蔡崇信此前在香港举行的汇丰全球投资峰会上表示，阿里巴巴的员工数量已触底。公司将重新启动并重新招聘，标志着人才战略的重要转向。&lt;/p&gt; 
&lt;p style="color:#2a3840; margin-left:.2rem; margin-right:0; text-align:justify"&gt;集团 CEO 吴泳铭在财报分析师会上明确表态，未来三年将围绕 AI 战略核心加大投入。投入重点包括 AI 和云计算的基础设施建设、AI 基础模型平台及 AI 原生应用，以及现有业务的 AI 转型升级三个方面。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364315</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364315</guid>
      <pubDate>Tue, 05 Aug 2025 06:56:52 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Google Kaggle 举办 AI 国际象棋锦标赛，评估领先模型的推理能力</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;OpenAI 的 o3 和 04-mini、Google LLC 的 Gemini 2.5 Pro 和 Gemini 2.5 Flash、Anthropic 的 Claude Opus 4 以及 xAI Corp. 的 Grok 4 等全球性能最强的人工智能模型将在棋盘上展开正面交锋。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-559fc85f92024cf582bdb7dd5bb9b495a9f.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fsiliconangle.com%2F2019%2F11%2F04%2Fgoogle-makes-cloud-automl-service-available-kaggle%2F" target="_blank"&gt;这场为期三天的人工智能象棋对决是 Google 数据科学社区 Kaggle&lt;/a&gt;&amp;nbsp;即将在新开发的 Game Arena 举办的一系列锦标赛的首场。在那里，模型将在一系列旨在评估其思维和推理能力的战略游戏中相互竞争。&lt;img alt="" src="https://static.cnbetacdn.com/article/2025/0805/8cbf492b712fcfe.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Google DeepMind 和 Kaggle 将与 Chess.com、国际象棋应用程序 Take Take Take 以及传奇国际象棋直播主播 Levy Rozman 和 Hikaru Nakamura 合作举办此次比赛，首场模拟比赛将于明天开始。&lt;/p&gt; 
&lt;p&gt;Kaggle&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fkaggle.com%2Fgame-arena" target="_blank"&gt;Game Arena&lt;/a&gt;是一个全新的 AI 基准测试平台，旨在测试大型语言模型在围棋和狼人杀等一系列战略游戏中的竞争力。首先登场的是 AI 国际象棋表演赛，该表演赛将于 8 月 5 日至 7 日举行，模拟比赛将在 Kaggle.com 上进行直播。&lt;/p&gt; 
&lt;p&gt;Hikaru Nakamura 将对每场比赛进行评论，而 Levy Rozman 将在 GothamChess&amp;nbsp;YouTube 频道上提供每日比赛的回顾和分析。比赛结束时，Magnus Carlsen 将在&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.youtube.com%2F%40TakeTakeTakeApp" target="_blank"&gt;Take Take Take YouTube 频道&lt;/a&gt;上直播冠军对决和赛事回顾。&lt;/p&gt; 
&lt;p&gt;八位选手将角逐国际象棋霸主地位：Gemini 2.5 Pro、Gemini 2.5 Flash、Claude Opus 4、DeepSeek-R1、Moonshot 的 Kimi 2-K2-Instruct、o3、o4-mini 和 Grok 4。比赛将采用标准的单败淘汰赛制，每场比赛的胜负将通过四局两胜制决出。Kaggle Game Arena 每天将直播一轮比赛，因此第一轮四分之一决赛将进行四场八个模型的对决，第二天将进行两场半决赛，第三天将进行一场决赛。&lt;/p&gt; 
&lt;p&gt;Google 在一篇博客文章中概述了一系列&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.kaggle.com%2Fbenchmarks%2Fkaggle%2Fchess-fen%2Fversions%2F1%2Ftournament" target="_blank"&gt;规则&lt;/a&gt;，称这些模型将响应基于文本的输入。所有参赛模型都不得访问任何第三方工具，因此它们无法直接使用 Stockfish 国际象棋引擎来识别任何情况下的最佳走法。相反，它们必须自行思考。&lt;/p&gt; 
&lt;p&gt;模型不会获得所有可能的合法走法列表，如果模型尝试走法，则允许重试三次。如果模型未能走法，则将弃权。此外，每步走法都有 60 分钟的时间限制。&lt;/p&gt; 
&lt;p&gt;直播将尝试展示每个竞争模型如何「推理」其下一步行动，以及对任何失败行动的反应。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.cnbetacdn.com/article/2025/0805/3c8d2544fd8be20.png" referrerpolicy="no-referrer"&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-b875aad4f4c3af6d1e66faa049d0decc0e4.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;除了比赛之外，Kaggle 还将创建一个更全面的排行榜，根据每个模型在数百场非直播的「幕后」比赛中的表现进行排名。每个模型将与竞争对手进行多次对决，对决双方随机选择。此举旨在帮助 Kaggle 创建一个更强大的排行榜，作为衡量每个模型棋艺的综合基准。&lt;/p&gt; 
&lt;p&gt;Kaggle 产品经理 Meg Risdal 表示：「虽然比赛是一种有趣的方式，可以观看并了解不同模型在游戏竞技场环境中如何下棋，但最终的排行榜将代表我们长期以来对模型下棋能力的严格基准。」&lt;/p&gt; 
&lt;p&gt;Google 表示，推出 Kaggle 游戏竞技场是因为国际象棋等游戏是评估法学硕士推理能力的最佳方式之一。&lt;/p&gt; 
&lt;p&gt;这是因为游戏能够抵御 Google 所谓的「饱和度」，换句话说，可以用标准公式来解决。国际象棋、围棋和其他游戏极其复杂，每场比赛都是独一无二的，这意味着随着每个参赛者的进步，难度也会随之增加。而狼人杀游戏则能够考验企业的基本技能，例如在不完整信息中导航，以及在合作与竞争之间取得平衡。&lt;/p&gt; 
&lt;p&gt;此外，Google 表示，游戏就像现实世界技能的代理，可以测试模型在战略规划、记忆、推理、适应、欺骗和「心智理论」（即预测对手想法的能力）方面的能力。同时，像「狼人杀」这样的团队游戏可以帮助评估每个模型的沟通和协调能力。&lt;/p&gt; 
&lt;p&gt;Kaggle 的全新 Game Arena 将展示当前和即将举行的直播比赛，每场比赛都将拥有专属页面，列出排名模型的排行榜、比赛结果以及开源游戏环境及其规则的具体细节。随着每个模型玩更多比赛，以及更新的模型添加到排名中，排行榜将动态更新。&lt;/p&gt; 
&lt;p&gt;未来，Kaggle Game Arena 将扩展到包括更复杂的多人视频游戏和真实世界模拟，以生成更全面的基准来评估不断扩展的 AI 模型技能。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364296</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364296</guid>
      <pubDate>Sun, 03 Aug 2025 06:16:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>deepin 技术双周报 | DDE 稳定性显著提升，6.6 内核大量优化</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;&lt;span&gt;&lt;span&gt;2025 年第 10 期 deepin 双周技术进展报告现已正式&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fy0-tetQxAjDGl7ooLrAmTw" target="_blank"&gt;发布&lt;/a&gt;，详细梳理了 deepin 各技术组在过去两周内的工作成果，并对未来两周的工作计划进行简要说明。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;h4 style="margin-left:0px; margin-right:0px; text-align:justify"&gt;&lt;strong&gt;&lt;span&gt;01&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;DDE&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt; 
&lt;p style="margin-left:0px; margin-right:0px"&gt;&lt;strong&gt;&lt;span&gt;进展&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul style="list-style-type:disc; margin-left:0; margin-right:0"&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;提升 dde-shell 的稳定性，避免部分场景下偶现的更新过程中任务栏崩溃的现象；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;修复拖拽未驻留在任务栏的图标导致图标被驻留的问题；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;修复部分场景中，任务栏驻留的图标可能重复的问题；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;修正通知中心空白图标的问题，并增加无通知场景的相应状态；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;针对全屏启动器的应用右键菜单支持跟随主题色变化；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;为启动器增加 F1 帮助快捷键；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;修正部分场景下切换小窗口启动器的分类模式可能导致启动器崩溃的问题；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;针对 wine 程序提供更好的卸载功能集成支持；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;修正控制中心时区菜单激活色不正确的问题，以及诸多其他类似 UI 问题调整；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;修正控制中心调整音量时可能产生的音频反馈问题；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;修正控制中心蓝牙界面展示的设备排序问题；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;其他各类琐碎的问题修正和功能开发。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&lt;strong&gt;&lt;span&gt;计划&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul style="list-style-type:disc; margin-left:0; margin-right:0"&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;继续针对已发现问题进行修正；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;针对 TreeLand 环境进行积极适配。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4 style="margin-left:0px; margin-right:0px; text-align:justify"&gt;&lt;strong&gt;&lt;span&gt;02&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;系统研发&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt; 
&lt;p style="margin-left:0px; margin-right:0px"&gt;&lt;strong&gt;&lt;span&gt;进展&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul style="list-style-type:disc; margin-left:0; margin-right:0"&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;SW64 架构工具链 patch 合入主线；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;usb.ids 版本更新；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;dh-builtusing 版本更新；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;apache2 版本更新；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;git-buildpackage 版本更新；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;box64 版本更新；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;db5.3 版本更新；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;djvulibre 版本更新；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;fastfetch 版本更新；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;jq 版本更新；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;redis 版本更新；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;usbutils 版本更新；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;sane-airscan 版本更新；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;sqlite3 版本更新；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;CVE 漏洞修复。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&lt;strong&gt;&lt;span&gt;计划&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul style="list-style-type:disc; margin-left:0; margin-right:0"&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;软件包更新；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;推进 CVE 安全漏洞修复。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4 style="margin-left:0px; margin-right:0px; text-align:justify"&gt;&lt;strong&gt;&lt;span&gt;03&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;strong&gt;&lt;span&gt;内核&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&lt;strong&gt;&lt;span&gt;进展&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul style="list-style-type:disc; margin-left:0; margin-right:0"&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;小版本更新补丁合入到上游 6.6.100 内核版本和 6.12.40 内核版本；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;6.6 内核分支合入并在 x86 和 ARM64 上启用 ashmem 功能；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;6.6 内核分支合入补丁，优化对 RISC-V 上的 amdgpu 支持；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;6.6 内核分支合入来自申威的补丁 (kvm,acpi,pci 等等)，优化对申威架构的支持；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;6.6 内核分支合入来自飞腾的补丁，修复当 SMMU 事件类型为 0x10 且故障转换地址为 0x0 时，跳过该错误信息的打印；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;6.6 内核分支合入来自海光的补丁，优化对海光 tdm、ccp 功能的支持；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;6.6 内核分支合入上游补丁，移除了部分上游已移除的内容，例如 wait bookmarks；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;6.6 内核分支合入上游补丁，优化 pipe 锁的性能；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;6.6 内核分支合入上游补丁，优化 udp 的性能；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;6.6 内核分支支持成都华瑞数鑫 D3100s sas/sata raid 卡驱动；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;6.6 内核分支上进行对 IEE 的优化，加入 SLAB_NO_MERGE 标志，防止缓存合并，确保使用独立的内存池；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;6.6 内核回合 mt7925 网卡驱动的修复，修复了 reset 进程可能失败的问题和硬件扫描中可能存在不合法的数组下标问题；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;修正了来自 Debian 固件加载失败补丁的错误，只有在所有压缩方式尝试都失败的情况下打印日志，而不是在不压缩的情况下尝试失败就打印日志，导致大量误报问题；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;调整 x86 的编译配置，防止非 root 用户可以访问内核日志以增强安全性；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;在 ARM64 架构上打开内核的 wireguard 编译。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&lt;strong&gt;&lt;span&gt;计划&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul style="list-style-type:disc; margin-left:0; margin-right:0"&gt; 
 &lt;li&gt; &lt;p style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;代码评审合入厂商的提交。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364294</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364294</guid>
      <pubDate>Sun, 03 Aug 2025 06:12:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>GitHub CEO：开发者必须积极拥抱 AI，否则可能被淘汰</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在近期的一篇博客文章中，微软旗下 GitHub 的首席执行官托马斯・多姆克（Thomas Dohmke）对全球的软件工程师发出了重要警告。他指出，开发者必须积极拥抱人工智能 (AI)，否则可能面临被行业淘汰的风险。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;多姆克在文章中提到，软件开发不仅仅是在编写代码，而是正在经历身份的转变。他提到，已经有 22 位开发者分享了他们在工作中深度融入 AI 工具的经历，表明 AI 已成为他们日常工作中不可或缺的伙伴。其中一些开发者最初对 AI 持怀疑态度，但随着时间的推移，他们逐渐认识到 AI 工具的价值，并开始将其视为协作的关键。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;「要么拥抱 AI，要么就离开这个行业。」 这句引人注目的话来自于一位开发者，体现了当前开发者面临的压力和挑战。多姆克进一步强调，开发者的角色正在转变，从传统的编码者变成 AI 战略家，他们不仅负责代码的编写，还需要管理和审核 AI 生成的代码。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;随着 AI 技术的迅速发展，预计未来 90% 的代码编写将实现自动化。这种变化意味着开发者需要掌握新的技能，包括系统设计、AI 应用的熟练度以及任务分配等。多姆克认为，早期采用 AI 工具的开发者已经获得了先发优势，而不是被取代。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;这种转型不仅涉及技术的更新，更是思维方式的变化。开发者们需要把重心从单纯追求速度和效率转向如何利用 AI 提升工作的质量和创意。尽管改变是困难的，许多人可能对此感到抗拒，但多姆克认为，未来只有那些能够适应这一变化的人才能在行业中立足。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在文章的最后，多姆克直言不讳地表示，不愿意改变的人应该考虑寻找其他职业道路。这一观点无疑对许多开发者提出了挑战，同时也在呼唤着未来软件开发的新标准。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364283</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364283</guid>
      <pubDate>Sun, 03 Aug 2025 05:49:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Anthropic 已开始内部测试 Claude Opus 4.1</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Fbtibor91%2Fstatus%2F1952366658326036781" target="_blank"&gt;根据社交媒体上流传的截图&lt;/a&gt;，Anthropic 已开始对其下一代大模型 Claude Opus 4.1 进行内部测试。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1477" src="https://static.oschina.net/uploads/space/2025/0805/115400_Jq4N_2720166.png" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;泄露的信息显示，该模型的内部代号为 claude-leopard-v2-02-prod。一张截图中的宣传语写道：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Opus 4.1 is here - Try our latest model for more problem solving power.&lt;/p&gt; 
 &lt;p&gt;Opus 4.1 来了——试试我们最新的模型，获得更强的问题解决能力。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;这预示着新模型可能在推理和解决复杂问题的能力上会有显著提升。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364260</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364260</guid>
      <pubDate>Sun, 03 Aug 2025 03:55:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>谷歌称其基于 AI 开发的漏洞猎手「Big Sleep」已报告 20 个安全漏洞</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;谷歌安全副总裁 Heather Adkins 周一&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Fargvee%2Fstatus%2F1952390039700431184" target="_blank"&gt;宣布&lt;/a&gt;，其大语言模型漏洞研究员「Big Sleep」在多种流行开源软件中发现并报告了 20 个漏洞。这些漏洞主要存在于音频和视频库 FFmpeg 和图像编辑套件 ImageMagick 等开源软件中。「Big Sleep」由谷歌人工智能部门 DeepMind 及其精英黑客团队 Project Zero 开发。&lt;/p&gt; 
&lt;p&gt;鉴于这些漏洞尚未修复，我们目前尚不清楚其影响或严重程度，因为谷歌&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgoogleprojectzero.blogspot.com%2F2025%2F07%2Freporting-transparency.html" target="_blank"&gt;目前不愿提供详细信息&lt;/a&gt;，而这在等待漏洞修复时是常规做法。但 Big Sleep 发现这些漏洞这一简单事实意义重大，因为它表明这些工具开始取得实际成效，即使此案例中涉及人为因素。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0805/113208_FaQ8_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fgoo.gle%2Fbigsleep" target="_blank"&gt;http://goo.gle/bigsleep&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;谷歌发言人金伯利·萨姆拉 (Kimberly Samra) 表示：「为了确保报告的高质量和可操作性，我们在报告之前会聘请一位人类专家参与，但每个漏洞都是由人工智能代理发现并重现的，无需人工干预。」&lt;/p&gt; 
&lt;p&gt;谷歌工程副总裁 Royal Hansen&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Froyalhansen%2Fstatus%2F1952424018663162235" target="_blank"&gt;在 X 上写道&lt;/a&gt;，这一发现表明「自动化漏洞发现领域开辟了新领域」。 基于 LLM 的工具能够查找和发现漏洞已经成为现实。除了 Big Sleep，还有&amp;nbsp;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.runsybil.com%2F" target="_blank"&gt;RunSybil&lt;/a&gt;&amp;nbsp;和 XBOW 等。&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fxbow.com%2Fblog%2Ftop-1-how-xbow-did-it" target="_blank"&gt;XBOW 在漏洞赏金平台 HackerOne 的美国排行榜上名列前茅&lt;/a&gt;后，引起了媒体的关注。值得注意的是，在大多数情况下，这些报告在流程的某个阶段都会有人工参与，以验证人工智能漏洞猎人是否发现了合法的漏洞，Big Sleep 就是这种情况。&lt;/p&gt; 
&lt;p&gt;RunSybil 是一家开发人工智能漏洞猎手的初创公司，其联合创始人兼首席技术官 Vlad Ionescu 介绍说，Big Sleep 是一个「合法」的项目，因为它「设计精良，背后的人知道自己在做什么，Project Zero 拥有漏洞查找经验，而 DeepMind 拥有强大的资源来支持它」。&lt;/p&gt; 
&lt;p&gt;这些工具显然前景光明，但也存在一些明显的缺陷。一些维护不同软件项目的人抱怨说，他们的错误报告实际上是幻觉，有些人甚至称其为「漏洞赏金计划」版的人工智能垃圾。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364254</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364254</guid>
      <pubDate>Sun, 03 Aug 2025 03:33:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>ChatGPT 用户数暴涨至 7 亿，OpenAI 年化收入飙升至 120 亿美元</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;&lt;span style="color:#000000"&gt;OpenAI 正在经历前所未有的爆炸式增长。该公司周一宣布，其旗舰产品 ChatGPT 的周活跃用户数已达到 7 亿，同比增长超过四倍。除了周活跃用户数的大幅攀升，其日均用户消息量也突破了 30 亿条大关。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="279" src="https://oscimg.oschina.net/oscnet/up-1c5aba8b9f39a0fcea4f5232267cb5fc41e.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在商业化层面，OpenAI 付费商业用户数量从今年 6 月的 300 万激增至 500 万，增长幅度超过 66%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;财务表现方面，OpenAI 今年前七个月的收入实现了翻番增长，年化收入达到 120 亿美元。相比 2024 年约 40 亿美元的收入水平，这一增长幅度堪称惊人，甚至有望超越此前预期的 2025 年 127 亿美元收入目标。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;OpenAI 产品副总裁 Nick Turley 在宣布用户数据时表示："每天，个人和团队都在学习、创造和解决更棘手的问题。接下来将是一周重要的时刻。"这一表态引发了业界的广泛猜测。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;分析人士普遍认为，Turley 所提及的"重要的一周"很可能暗示着 GPT- 5 的即将发布。根据此前的报道，该版本原计划于 8 月初发布，预计将集成推理功能（o3）、推出迷你和纳米版本，并在编码和性能方面实现显著提升。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;如果 GPT- 5 确实即将面世，这将是 OpenAI 在技术能力和市场竞争力方面的又一次重大升级。新版本的发布有望进一步巩固 ChatGPT 在 AI 聊天机器人领域的领先地位，并可能带来新一轮的用户增长和收入提升。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;尽管增长迅速，但在与科技巨头的竞争中，ChatGPT 仍面临不小的挑战。谷歌母公司 Alphabet 首席执行官 Sundar Pichai 在最近的财报电话会议上透露，其 AI 搜索摘要产品 AI Overviews 目前在 200 多个国家拥有约 20 亿月度用户，而 AI 聊天机器人 Gemini App 的月活跃用户也超过了 4. 5 亿。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;从用户规模对比来看，谷歌的 AI 产品在覆盖面上仍然具有明显优势。不过，ChatGPT 在用户活跃度和付费转化方面的表现更为突出，这反映出两家公司在 AI 产品策略上的不同侧重点。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在用户基数快速扩张的同时，OpenAI 也在积极优化产品体验。本周 ChatGPT 推出的更新中，新增了休息提醒功能，旨在为用户提供更健康、更有目标的使用方式。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;此外，新版本还致力于改善用户的情绪和精神状态，为重大个人决策提供指导，并整合了来自医生、研究人员和心理健康顾问的专业意见。这些功能的加入使 ChatGPT 从单纯的对话工具向更全面的 AI 助手发展。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364252</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364252</guid>
      <pubDate>Sun, 03 Aug 2025 03:27:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>OpenAI 为 ChatGPT 增加长时间使用提醒</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;OpenAI&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopenai.com%2Findex%2Fhow-we%27re-optimizing-chatgpt%2F" target="_blank"&gt;宣布&lt;/a&gt;正在为 ChatGPT 推出一系列新功能，旨在促进用户健康和目标驱动的使用。其中一项已经推出的功能是「休息提醒」，当用户与 ChatGPT 进行长时间对话后，系统会弹出窗口建议用户休息。&lt;/p&gt; 
&lt;p&gt;据介绍，ChatGPT 将出于用户健康考虑，为 ChatGPT 增加温和的长时间使用提醒，当用户在某一对话中沉浸过长时间时会有弹窗提示。同时，ChatGPT 将加强对幻觉与情感依赖等对话内容的识别，并与专业人士、研究团队合作，以更好回复精神与情感困难问题。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1080" src="https://static.oschina.net/uploads/space/2025/0805/111637_278T_2720166.png" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;OpenAI 表示，其目标是帮助用户善用注意力，而非仅仅吸引注意力。除了休息提醒，公司还在改进对处于困境中用户的支持，并开发更好的生活建议功能。所有这些更新都是在医生、研究人员和心理健康顾问等专家的指导下进行的。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364246</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364246</guid>
      <pubDate>Sun, 03 Aug 2025 03:17:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>WAIC 2025 超擎数智圆满收官！AI 全栈火爆出圈，加速 AI 应用变革新引擎</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:justify"&gt;7 月 29 日，为期四天的 2025 世界人工智能大会（WAIC）在上海世博展览馆圆满落幕。本届大会以「智能时代，同球共济」为主题，吸引了全球 800 多家企业参展，线下参观人数达 30.5 万人次，全面呈现 AI 在核心技术、行业应用、智能终端与生态链接的全景落地。&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:justify"&gt;作为领先的人工智能核心产品与整体解决方案提供商，NVIDIA Compute（GPU）、Networking（网络）的双 Elite 精英级合作伙伴，超擎数智以「AI 全栈 · 数智赋能」为主题，携 AI 应用全栈方案、AI+教科研、医疗健康、金融服务、具身智能应用、联 NVIDIA 建设交付的中国首个 L20 千卡灯塔集群等惊艳亮相，开启了一场 AI 加速千行百业应用「成果爆炸」之旅，让我们一起回顾超擎数智在 WAIC 2025 的高光时刻吧！&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:justify"&gt;&lt;strong&gt;AI 应用全栈方案重磅亮相&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left"&gt;超擎数智携自主品牌的元景系列 H20 AI 服务器（8U8 卡 NVLink）、擎天系列 L20 AI 服务器（4U8 卡 PCle）、锋锐系列 L20 AI 服务器（2U4 卡 PCIe）系列 AI 服务器，集计算、存储、网络于一体的超擎数智 AI 一体机柜，NVIDIA DGX Spark 桌面级 AI 超级计算机、800G 交换机、ConnectX-8 网卡等多样 AI 算力产品集体亮相，为大规模数据训练和推理提供强劲性能，帮助 AI 用户高效构建 AI 基础设施和应用环境，满足 AI 场景下的多元算力需求，为 AI 新质生产力提供强劲引擎。&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:center"&gt;&lt;img height="427" src="https://oscimg.oschina.net/oscnet//61d24d3709bd8a4fd6818e160a14d0de.jpeg" width="640" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left"&gt;前沿尖端网络产品现场集结，1.6T XDR IB 交换机、800G 交换机、400G RoCE 交换机、NVIDIA ConnectX-8 800G 网卡、BlueField-3 DPU 等众多前沿网络产品集体亮相，尽现超擎数智构建高性能、安全、可扩展的下一代 AI 无损网络的领先方案与技术服务能力！&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:center"&gt;&lt;img height="427" src="https://oscimg.oschina.net/oscnet//2649b35637292fa1a4ff96d31623761b.jpeg" width="640" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left"&gt;光电零损，算力全开。现场展示 1.6T、800G 光模块、浸没式液冷光模块以及 AOC/DAC/AEC/ACC 高速线缆等领先产品，以多样化 AI 零损光电联接方案以及领先液冷部署实践展示，助力多场景应用落地，赋能下一代 AI 集群。&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:center"&gt;&lt;img height="425" src="https://oscimg.oschina.net/oscnet//6cb0d5b6bb0d3c8d0285a8c82797184c.jpeg" width="640" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left"&gt;超擎数智自主研发的 Al Engine 人工智能开发平台，提供从数据处理、模型开发、训练到部署的全流程支持，以强大的 AI 开发和推理环境，助力各行业加速智能化转型。&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:center"&gt;&lt;img height="424" src="https://oscimg.oschina.net/oscnet//67f196af6522b41186e60992bb458d41.jpeg" width="640" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left"&gt;超擎数智拥有专业的审计调优和交付验收技术服务团队，超擎数智 CQIS 服务，保障用户 AI 集群实现卓越性能、高度可靠性和严密的安全性，可显著缩短模型的训练周期，提升模型的性能和精度，体验从算力集群、存储、智算网络到安全的最优算力整体方案，确保用户的 AI 集群始终处于最佳状态。&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:center"&gt;&lt;img height="433" src="https://oscimg.oschina.net/oscnet//ec1d2f9cd2c60be70caf2cebb04eb973.jpeg" width="640" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left"&gt;算力设计与运营服务是超擎数智的核心优势，现场呈现超擎数智从 0 到 1，为用户构建适合其业务需求的算力系统的整体方案，从需求分析、方案设计、成本评估、风险预估与管理到实施交付、算力调优，帮助用户实现算力资源的最大化利用和业务价值的提升。&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:center"&gt;&lt;img height="479" src="https://oscimg.oschina.net/oscnet//d1e6e40a19d1744dfceedcaa6b6fa0fd.jpeg" width="640" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:justify"&gt;&lt;strong&gt;成果展示：超擎数智建设交付的中国首个 L20 千卡灯塔集群项目引关注热潮&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left"&gt;超擎数智联合 NVIDIA 建设交付的中国首个 L20 千卡灯塔集群项目在本次 WAIC 重磅亮相，现场向观众展示了超擎数智的 AI 集群方案建设及高效交付能力，引得一众围观，现场嘉宾与观众纷纷与超擎业务及技术团队交流探讨集群方案设计、整体方案交付、产品技术支持、调优、运维服务等经验。项目一期规模为 L20 千卡集群，二期将扩展到 L20 万卡集群，已经为众多模型厂商、游戏企业、短视频产业链、工业制造、生物制药、高校科研机构及金融数据客户提供高效的算力服务。&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:center"&gt;&lt;img height="427" src="https://oscimg.oschina.net/oscnet//f72aae4e489c964fcf72e06b4705849c.jpeg" width="640" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left"&gt;&lt;strong&gt;AI+行业，解锁数智赋能无限可能&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:center"&gt;&lt;img height="427" src="https://oscimg.oschina.net/oscnet//8fb6817bb5fec4cfad9ddc84f7e7ea30.jpeg" width="640" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left"&gt;超擎数智全栈场景落地，赋能千行百业，现场展示面向高等教育和研究、医疗健康、金融服务、具身智能等行业应用方案，面向不同行业打造全新场景化落地方案，洞见未来 AI 业务场景。&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left"&gt;&lt;strong&gt;六场 AI 大咖演讲，深度解析 AI 技术应用突破与全栈赋能&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left"&gt;WAIC 期间，超擎数智邀请人工智能行业领导企业多位 AI 大咖亲临展台，现场分享 AI 加速行业变革的真知灼见和实践经验，引发现场观众的热情关注和驻足观摩。&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:center"&gt;&lt;img height="480" src="https://oscimg.oschina.net/oscnet//c8df7ec8d4b7299b29b57e9382f0f0eb.jpeg" width="640" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left"&gt;超擎数智产品技术负责人带来《NVIDIA InfiniBand XDR 高性能 AI 网络平台》、《超擎数智 AI 应用全栈产品与落地方案》报告分享，从 AI 全栈产品与落地方案，重新定义 AI 时代的数据中心网络与应用加速路径。&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left"&gt;思科大中华区首席架构师蒋星带来《思科 AI 创新与实践》的主题报告，深度探讨 AI 创新与实践的高效变革之路，与现场观众一起沉浸式洞悉 AI 无界未来。&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left"&gt;思科大中华区首席架构师魏航现场分享《AI 体验跃升：迈向高效、可观察与安全之路》，系统介绍了 AI 时代，借助值得信赖的基础设施为 AI 赋能并保驾护航，助力客户充分发挥 AI 的价值。&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:center"&gt;&lt;img height="421" src="https://oscimg.oschina.net/oscnet//4da469dfa1c336726b81e65bc8dac777.jpeg" width="640" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:center"&gt;&lt;img height="427" src="https://oscimg.oschina.net/oscnet//ab114c8611f5812317ef12cb739328aa.jpeg" width="640" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left"&gt;超擎数智解决方案技术部负责人带来《Al Engine 人工智能开发平台加速 AI 应用落地》主题分享，为加速 AI 的开发与部署，提供从数据处理、模型开发、训练到部署的全流程支持解决方案。&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left"&gt;福鑫科创 CEO 吴笛介绍《GenAl 医疗场景化实践》，深入解读了生成式人工智能（GenAI）在医疗行业的场景应用与系统性落地路径，为行业带来一场聚焦技术与实践的思想碰撞。&lt;/p&gt; 
&lt;p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left"&gt;超擎数智本次亮相 2025 世界人工智能大会，向行业客户全面展示了 AI 赋能千行百业的前瞻技术、创新方案、成功实践与生态构建。未来，超擎数智立足开放格局，链接全球前沿智慧，聚焦应用场景突破，创新解决方案，深化产业协作，共建 AI 繁荣生态，携手行业伙伴共绘智能时代新图景！&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364243</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364243</guid>
      <pubDate>Sun, 03 Aug 2025 03:13:00 GMT</pubDate>
      <author>作者: 开源科技</author>
    </item>
    <item>
      <title>微软正在分阶段开源 Windows 11 用户界面框架 WinUI</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;微软拥有大量开源项目，如今，该公司对社区贡献的态度也更加开放。尽管如此，仍有大量代码保持闭源，公司也不断收到更多开放请求。其中之一就是 Windows 11 的用户界面框架 WinUI。尽管微软虽然尚未完全开放，但该公司分享了未来六个月计划的细节，其中包括「产品工作和基础变革，以支持更加开放和协作的未来」。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-a574d1f8d2589c1f12d0d92889eb094019d.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fmicrosoft-ui-xaml%2Fdiscussions%2F10700" target="_blank"&gt;微软表示&lt;/a&gt;，由于其复杂性和连接性，开源 WinUI 不可能轻而易举地完成。Windows 11 的用户界面利用了操作系统的许多专有层，这些层无法直接发布。因此，微软需要区分哪些内容可以与社区共享，哪些内容不能共享：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;你们中很多人问到关于真正开源这个代码库的问题。虽然我们还没有准备好承诺完成所有里程碑的具体截止日期，但我们正在积极努力。这不是一个瞬间就能实现的计划，而是一个经过深思熟虑的过程。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;此外，团队需要优先考虑其他事项，包括安全性、稳定性和对现有产品的支持。&lt;/p&gt; 
&lt;p&gt;微软计划分阶段开放 WinUI 的 GitHub 存储库：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;第一阶段：提高镜像频率。WASDK 1.8 发布（8 月底）后，我们将开始更频繁地将内部提交镜像到 GitHub，以提高透明度并显示进度。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;第二阶段：第三方开发者本地构建。外部开发者将能够在本地克隆和构建 repo，并提供文档来指导设置和依赖关系。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;第三阶段：第三方开发者贡献并运行测试。贡献者将能够在本地提交 PR 并运行测试。我们正在努力理清私有依赖关系，并将测试基础设施开放给公众访问。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;第四阶段：GitHub 成为重心。GitHub 将成为开发、问题跟踪和社区参与的主要平台。内部镜像将被逐步淘汰。&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0805/104430_LP4V_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;让 WinUI 更加开放将是一个渐进的过程，您可以在&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Forgs%2Fmicrosoft%2Fprojects%2F1868%2Fviews%2F1" target="_blank"&gt;GitHub 上的这个页面上&lt;/a&gt;跟踪它。同时，开发人员可以通过分享反馈、提交清晰且写得好的问题以及点赞现有反馈来做出贡献。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364233</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364233</guid>
      <pubDate>Sun, 03 Aug 2025 02:44:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Chrome 139 开发者工具增强 AI 辅助功能</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;Chrome 139 开发者工具&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.chrome.com%2Fblog%2Fnew-in-devtools-139%3Fhl%3Dzh-cn" target="_blank"&gt;已发布&lt;/a&gt;，此次更新重点在于提升产品的可靠性和效率，解决了大量已知问题——从长期存在的视觉故障、可用性问题和设计不一致问题到性能和功能问题。总体而言，将未结问题的数量减少了 27%。&lt;/p&gt; 
&lt;p&gt;在 AI 辅助功能方面，新版本增强了样式设置的交互性。用户现在不仅可以截取屏幕截图，还可以上传任意图片到「AI 辅助」面板与 Gemini 的对话中，以提供更直观的视觉上下文。&lt;/p&gt; 
&lt;p&gt;&lt;img height="682" src="https://static.oschina.net/uploads/space/2025/0805/103609_kGTN_2720166.png" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;此外，「网络」面板也得到了改进，用户现在可以右键点击请求表格的列标题，选择并添加多个请求标头作为新的列，方便查看和分析。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-58bc3a421212513fb74c256cf34c5fc155a.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364232</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364232</guid>
      <pubDate>Sun, 03 Aug 2025 02:37:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>智谱推出 Zread.ai 开发效率工具，搭载 GLM-4.5</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span&gt;智谱许&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FkgkxULD3SOu5f9gS4XQH1g" target="_blank"&gt;宣布&lt;/a&gt;推出基于大模型的开发效率工具 Zread.ai，旨在通过 AI 技术一站式解决开发者在接手旧项目、文档撰写以及理解开源项目时的常见痛点。Zread.ai 的核心功能包括一键理解代码、生成知识以及促进协作，能够显著提升开发效率。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;Zread.ai 的核心功能主要体现在三个方面：源项目的深度学习、快速接手历史代码库以及构建团队知识协作系统。开发者可以通过输入任意 GitHub 仓库链接，让 Zread 生成包含架构解析、模块说明、设计模式的 Guide，同时支持多仓库对比、分层解读与 GitHub Trending 项目逻辑拆解。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;此外，Zread 还能够自动梳理项目结构、模块依赖，生成系统性文档，帮助开发者快速进入状态，即便面对复杂的代码也能快速上手。Zread 还提供贡献者图谱、社区评论聚合、交互式批注与问答，支持上传私有项目，构建团队内部的知识库和技术文档体系。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="282" src="https://oscimg.oschina.net/oscnet/up-b39df6faff0e30bca10a982d65b27f2c4bc.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在构建 Zread.ai 的过程中，智谱公司评估了多种大语言模型，最终选择了 GLM-4.5 作为代码分析与文档生成的核心底座。GLM-4.5 在模型代码理解能力、低幻觉、支持 Deep Research 以及 Agent 能力适配等方面表现出色。它能够准确识别代码模块之间的调用关系、架构层级与依赖结构，为生成高质量技术文档和项目导读提供了坚实基础。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;在复杂代码场景下，GLM-4.5 的输出稳定性较高，误解代码意图或编造逻辑的情况明显减少，尤其适合用于代码解读和技术问答类任务。针对大型代码库，GLM-4.5 能够进行多轮深入解析，结合上下文与语义线索，对关键技术设计进行追问与深挖，帮助开发者获取更具洞察力的解答。在长上下文理解与技术问答的响应速度、准确率方面，GLM-4.5 也表现稳定，提升了整体交互体验。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;使用 Zread.ai 非常简单，只需四步即可快速上手。首先，打开 Zread.ai，输入 GitHub 仓库链接，系统将自动识别代码结构与核心组件。接着，系统会自动生成项目导读（Guide），包含架构拆解、模块说明与设计范式。然后，开发者可以使用「Ask」功能提问关键技术细节，支持深入代码问答与跨模块追踪。最后，上传私有项目，生成团队专属知识库，为项目构建可持续的文档资产。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;智谱公司表示，GLM-4.5 不仅是模型提供方，更是 Zread.ai 实现「读懂代码、生成知识、服务协作」的核心支撑。未来，智谱将继续探索 GLM-4.5 在智能体集成、团队知识协同等场景下的深度应用，为开发者提供更强大的工具，提升开发效率。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364230</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364230</guid>
      <pubDate>Sun, 03 Aug 2025 02:35:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>疑似回应「全员裁员」传言，硅基智能称预计全年新增岗位数百个</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;近日，一家成立 8 年的 AI 独角兽南京硅基智能科技有限公司（下称「硅基智能」）卷入「全员裁员」传言。脉脉平台显示，当前「硅基智能 CEO:准备全员裁员，养不起你们」占据热榜第四。&lt;/p&gt; 
&lt;p&gt;8 月 3 日，硅基智能在微信公众号发出&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fmq8xKJFLo6gCwvG_Vn_rEA" target="_blank"&gt;声明&lt;/a&gt;，称「目前拥有一支稳定的产研与销售团队，且持续在全球范围内扩大招聘规模。2025 年，我们将重点布局杭州、嘉兴、香港、新加坡等地，预计全年新增岗位数百个，2026 年将达到新增数千人的扩张节奏。」&lt;/p&gt; 
&lt;p&gt;&lt;img height="293" src="https://oscimg.oschina.net/oscnet/up-e9194ccac4d60e4e961661160dfc6b4e061.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;几天前，一则疑似硅基智能创始人司马华鹏在公司工作群发言的截图在业内流传，引发关注。&lt;/p&gt; 
&lt;p&gt;截图内文文字显示，司马华鹏@所有人并表示：「各位，昨天我去看研发，只有徐超一个人在加班，公司今天已经做好了全员裁员的计划，算法给港科大和清华做，工程化留几个骨干，其他的都自寻出路，硅基养不起这样的团队，请大家见谅。」&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;针对传言及相关声明，记者于 8 月 4 日向硅基智能求证。硅基智能相关人士表示，「我们暂时不对外发声。团队专心做好产品和业务，团队稳定且在扩展。」&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;根据前述声明，硅基智能称，「2025 年上半年，公司超额完成销售目标，盈利能力逐步增强；下半年刚刚开始一个月，我们已锁定超 3 亿元的 AIGC 订单，并将加快推动「新质生产力赋能中心」在浙江等地落地。同时，新加坡、香港的出海团队也已初具规模。」&lt;/p&gt; 
&lt;p&gt;据了解，硅基智能已于 2025 年 6 月完成来自浙江嘉兴的新一轮数亿元融资，同时也拿到各大银行提供的数亿元授信额度。目前其账面现金可支持 120 个月以上的工资发放，同时拥有近亿元规模的算力硬件资产。&lt;/p&gt; 
&lt;p&gt;硅基智能成立于 2017 年，提供企业级 AIGC 数字人解决方案。其总部位于南京，属于专精特新小巨人企业、高新技术企业，当前已拥有发明授权专利一百多项，其中包括二十多项海外专利。股东包括腾讯、招银国际、国新央企、海松资本、红杉资本、浦信资本、奇虎 360 等，最新估值近 10 亿美元。&lt;/p&gt; 
&lt;p&gt;硅基智能联合创始人、高级副总裁孙凯彼时在 WAIC 某沙龙上谈及公司战略时透露，硅基智能正在从传统的工具收费向」按结果付费」转型。「我们的海外合作伙伴依靠硅基智能提供 AI 数字人技术接口，今年收入就已超过 1 亿美元。真正优秀的 AI 员工，不仅要赚月薪，更应成为客户的合伙人。」&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364227</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364227</guid>
      <pubDate>Sun, 03 Aug 2025 02:28:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>深度解析 RocketMQ 核心组件：ConsumeQueue 的设计与实现</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                                        &lt;h2&gt;导语&lt;/h2&gt; 
&lt;p&gt;在分布式消息队列 RocketMQ 中，ConsumeQueue（消费队列） 是消息消费的核心组件之一。它作为 &amp;nbsp;CommitLog 的索引机制，帮助消费者快速定位并拉取消息。如果没有 ConsumeQueue，消费者将无法高效地从海量消息中筛选出自己订阅的数据。&lt;/p&gt; 
&lt;p&gt;本文将基于 RocketMQ 5.0 源码，深入探讨 ConsumeQueue 的设计原理与实现细节。&lt;/p&gt; 
&lt;h2&gt;为什么需要 ConsumeQueue？&lt;/h2&gt; 
&lt;p&gt;在深入探讨 ConsumeQueue 之前，我们有必要先了解 RocketMQ 的消息写入和存储方式。&lt;/p&gt; 
&lt;p&gt;CommitLog 是 RocketMQ 的消息存储模块，用户生产的所有消息都持久化存储在该模块中，它具备两个特点：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;使用的持久化存储是一个文件队列，文件保存于指定目录下，每个文件的大小是固定的，通常是 1GB。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;只有一个文件可写入，且仅支持追加写，文件写满后自动切换至新的文件。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;RocketMQ 设计者出于写入优先的考虑，没有为不同 Topic 队列的消息分配不同的存储文件，而是将消息直接写入 CommitLog，不同 Topic 的消息混合分布在 CommitLog 的文件中。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/f7014d6f-45a9-4e8c-aca4-4eca355e5e5b.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;从上图中可以看出，尽管消息的写入非常高效，但是消费者需要按照其订阅的 Topic 来从 CommitLog 中读取该 Topic 的消息，显而易见，RocketMQ 需要一种索引机制来快速读取指定 Topic 队列的消息，这正是 ConsumeQueue 要做的事情。&lt;/p&gt; 
&lt;h2&gt;ConsumeQueue 的设计原理&lt;/h2&gt; 
&lt;p&gt;ConsumeQueue 作为 RocketMQ 的消息索引枢纽，其设计核心在于高效映射逻辑队列与物理存储。我们通过下面的图示来介绍 ConsumeQueue 的核心设计：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/36fce070-c2fd-4a3d-9449-0650f6c9946f.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;每个 Topic 队列有其对应的唯一的 ConsumeQueue，当一条消息写入到 CommitLog 后，RocketMQ 会构建该消息的索引，按异步方式将其写入到对应 Topic 队列的 ConsumeQueue 中。使用索引可以快速定位到消息在 CommitLog 文件的位置并读取它。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;消息索引对象在 ConsumeQueue 中的位置被称为 Offset，是个从 0 开始的序号数，maxOffset 即 ConsumeQueue 索引的最大 Offset，会随着新消息的写入递增。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;基于这个设计，消费者通过与 ConsumeQueue 的 Offset 交互来实现消息的消费。最常见的场景就是，我们记录消费组在 ConsumeQueue 上当前消费的 Offset，那么消费者下线后再上线仍然可从上次消费的位置继续消费。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;基于文件的传统实现方案&lt;/h2&gt; 
&lt;h3&gt;数据存储与格式&lt;/h3&gt; 
&lt;p&gt;与 CommitLog 类似，ConsumeQueue 使用文件队列来持久化存储消息索引。ConsumeQueue 使用的文件目录所在路径由其对应的 Topic 队列确定，举例说明，一个名为 ABC 的 Topic，其队列 0 所在的文件目录路径是 /data/rocketmq_data/store/consumequeue/abc/0/。消息的索引对象是固定的 20 个字节大小，其内部格式定义见下图。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/9c1e2702-a9f4-4d84-9bd8-3c5f1b5d7b18.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;为了方便描述，从这里开始我们将索引对象叫作 CqUnit。ConsumeQueue 的文件队列中每个文件的大小是固定的，默认配置可存储 30 万个 CqUnit，当文件写满后，会切换到新文件进行写入。文件名称的命名方式是有讲究的，它以文件存储的第一个 CqUnit 的 Offset 作为名称，这样做的好处是，按 Offset 查询 CqUnit 时，可以根据文件名称，快速定位到该 Offset 所在的文件，大幅减少对文件的读取操作频次。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/dc6e56f6-83c4-4e99-92f5-4c2702207472.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;构建过程&lt;/h3&gt; 
&lt;p&gt;当消息写入到 CommitLog 后，该消息对于消费者是不可见的，只有在 ConsumeQueue 中增加这条消息的 CqUnit 后，消费者才能消费到这条消息，因此写入消息时须立刻往 ConsuemQueue 写入消息的 CqUnit。我们需要给每一条消息指定其在 ConsumeQueue 中的 Offset，QueueOffsetOperator 类维护了一个 Topic 队列与其当前 &amp;nbsp;Offset 的表，当写入一条新消息时，DefaultMessageStore 从 QueueOffsetOperator 中取出该 Topic 队列的当前 Offset，将其写入到消息体中，在消息成功写入到 CommitLog 后，指示 QueueOffsetOperator 更新为当前 Offset + 1。为了防止其他写入线程并发访问 Topic 队列的当前 Offset，在读取和修改 Offset 期间，会使用一个 ReentrantLock 锁定该 Topic 队列。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/bd666fd0-892a-4923-8f29-f459737f4981.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;ReputMessageService 作为异步任务，会不停的读取 CommitLog，当有新的消息写入，它会立即读取到该消息，然后根据消息体构建一个 DispatchRequest 对象，CommitLogDispatcherBuildConsumeQueue 处理 DispatchRequest 对象，最终将 CqUnit 写入到 ConsumeQueue 的存储中。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/89cd3d56-7582-4cbe-ae34-1478e76b2a7c.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;按 Offset 查找消息&lt;/h3&gt; 
&lt;p&gt;消费者通常是从某个 Offset 开始消费消息的，比如消费者下线后再次上线会从上次消费的 Offset 开始消费。DefaultMessageStore 的 GetMessage 方法实现从一个 Topic 队列中拉取一批消息的功能，每次拉取要指定读取的起始 Offset 以及该批次读取的最大消息数量。下面截取了部分源码展示实现的基本思路：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp;@Override
    public GetMessageResult getMessage(final String group, final String topic, final int queueId, final long offset,
        final int maxMsgNums, final int maxTotalMsgSize, final MessageFilter messageFilter) {
        long beginTime = this.getSystemClock().now();
        GetMessageStatus status = GetMessageStatus.NO_MESSAGE_IN_QUEUE;
        long nextBeginOffset = offset;
        long minOffset = 0;
        long maxOffset = 0;
        GetMessageResult getResult = new GetMessageResult();
        final long maxOffsetPy = this.commitLog.getMaxOffset();
        ConsumeQueueInterface consumeQueue = findConsumeQueue(topic, queueId);
        if (consumeQueue != null) {
            minOffset = consumeQueue.getMinOffsetInQueue();
            maxOffset = consumeQueue.getMaxOffsetInQueue();
            if (maxOffset == 0) {
            //             
            } else {
                long maxPullSize = Math.max(maxTotalMsgSize, 100);
                if (maxPullSize &amp;gt; MAX_PULL_MSG_SIZE) {
                    LOGGER.warn("The max pull size is too large maxPullSize={} topic={} queueId={}", maxPullSize, topic, queueId);
                    maxPullSize = MAX_PULL_MSG_SIZE;
                }
                status = GetMessageStatus.NO_MATCHED_MESSAGE;
                long maxPhyOffsetPulling = 0;
                int cqFileNum = 0;
                while (getResult.getBufferTotalSize() &amp;lt;= 0
                    &amp;amp;&amp;amp; nextBeginOffset &amp;lt; maxOffset
                    &amp;amp;&amp;amp; cqFileNum++ &amp;lt; this.messageStoreConfig.getTravelCqFileNumWhenGetMessage()) {
                    ReferredIterator&amp;lt;CqUnit&amp;gt; bufferConsumeQueue = null;
                    try {
                        bufferConsumeQueue = consumeQueue.iterateFrom(group, nextBeginOffset, maxMsgNums);
                        long nextPhyFileStartOffset = Long.MIN_VALUE;
                        long expiredTtlOffset = -1;
                        while (bufferConsumeQueue.hasNext() &amp;amp;&amp;amp; nextBeginOffset &amp;lt; maxOffset) {
                            CqUnit cqUnit = bufferConsumeQueue.next();
                            long offsetPy = cqUnit.getPos();
                            int sizePy = cqUnit.getSize();
                            SelectMappedBufferResult selectResult = this.commitLog.getMessage(offsetPy, sizePy);
                            getResult.addMessage(selectResult, cqUnit.getQueueOffset(), cqUnit.getBatchNum());
                            status = GetMessageStatus.FOUND;
                            nextPhyFileStartOffset = Long.MIN_VALUE;
                        }
                    } catch (RocksDBException e) {
                        ERROR_LOG.error("getMessage Failed. cid: {}, topic: {}, queueId: {}, offset: {}, minOffset: {}, maxOffset: {}, {}",
                            group, topic, queueId, offset, minOffset, maxOffset, e.getMessage());
                    } finally {
                        if (bufferConsumeQueue != null) {
                            bufferConsumeQueue.release();
                        }
                    }
                }
                long diff = maxOffsetPy - maxPhyOffsetPulling;
                long memory = (long) (StoreUtil.TOTAL_PHYSICAL_MEMORY_SIZE
                    * (this.messageStoreConfig.getAccessMessageInMemoryMaxRatio() / 100.0));
                getResult.setSuggestPullingFromSlave(diff &amp;gt; memory);
            }
        } else {
            status = GetMessageStatus.NO_MATCHED_LOGIC_QUEUE;
            nextBeginOffset = nextOffsetCorrection(offset, 0);
        }
        getResult.setStatus(status);
        getResult.setNextBeginOffset(nextBeginOffset);
        getResult.setMaxOffset(maxOffset);
        getResult.setMinOffset(minOffset);
        return getResult;
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;上述代码片段的要点：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Topic 队列的 ConsumeQueue 的 IterateFrom 方法依据 Offset 生成一个 Iterator 对象。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;在 Iterator 有效的情况，不断从 Iterator 拉取 CqUnit 对象，即按 Offset 顺序读取 CqUnit。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;使用 CqUnit 对象中的 OffsetPy 和 SizePy 从 CommitLog 中读取消息内容，返回给消费者。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;接下来，我们介绍 ConsumeQueue 的 IterateFrom 方法是如何读取 CqUnit 的。从下面的源码中可以看到，GetIndexBuffer 方法先从 MappedFileQueue 中找到 Offset 所在的 MappedFile，然后找到 Offset 在 MappedFile 中的位置，从该位置读取文件剩余的内容。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;public SelectMappedBufferResult getIndexBuffer(final long startIndex) {
        int mappedFileSize = this.mappedFileSize;
        long offset = startIndex * CQ_STORE_UNIT_SIZE;
        if (offset &amp;gt;= this.getMinLogicOffset()) {
            MappedFile mappedFile = this.mappedFileQueue.findMappedFileByOffset(offset);
            if (mappedFile != null) {
                return mappedFile.selectMappedBuffer((int) (offset % mappedFileSize));
            }
        }
        return null;
    }
    @Override
    public ReferredIterator&amp;lt;CqUnit&amp;gt; iterateFrom(long startOffset) {
        SelectMappedBufferResult sbr = getIndexBuffer(startOffset);
        if (sbr == null) {
            return null;
        }
        return new ConsumeQueueIterator(sbr);
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ConsumeQueueIterator 的 Next 方法和 hasNext 方法是对 getIndexBuffer 方法返回的 SelectMappedBufferResult 对象，即文件内容的 ByteBuffer，进行访问。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp;private class ConsumeQueueIterator implements ReferredIterator&amp;lt;CqUnit&amp;gt; {
        private SelectMappedBufferResult sbr;
        private int relativePos = 0;
        public ConsumeQueueIterator(SelectMappedBufferResult sbr) {
            this.sbr = sbr;
            if (sbr != null &amp;amp;&amp;amp; sbr.getByteBuffer() != null) {
                relativePos = sbr.getByteBuffer().position();
            }
        }
        @Override
        public boolean hasNext() {
            if (sbr == null || sbr.getByteBuffer() == null) {
                return false;
            }
            return sbr.getByteBuffer().hasRemaining();
        }
        @Override
        public CqUnit next() {
            if (!hasNext()) {
                return null;
            }
            long queueOffset = (sbr.getStartOffset() + sbr.getByteBuffer().position() - relativePos) / CQ_STORE_UNIT_SIZE;
            CqUnit cqUnit = new CqUnit(queueOffset,
                sbr.getByteBuffer().getLong(),
                sbr.getByteBuffer().getInt(),
                sbr.getByteBuffer().getLong());
            return cqUnit;
        }
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;我们再讲下 MappedFileQueue 的 FindMappedFileByOffset 方法，该方法从其维护的文件队列中查找到 Offset 所在的文件。前面我们介绍过，ConsumeQueue 的文件队列中的文件是按 Offset 命名的，MappedFile 的 GetFileFromOffset 就是文件的名称，那么只需要按照 Offset 除以文件的大小便可得文件在队列中的位置。这里要注意的是，这个位置必须要先减去 FirstMappedFile 的位置后才是有效的，因为 ConsumeQueue 会定期清除过期的文件，所以 ConsumeQueue 管理的 MappedFileQueue 的第一个文件对应的 Offset 未必是 0。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp;public MappedFile findMappedFileByOffset(final long offset, final boolean returnFirstOnNotFound) {
        try {
            MappedFile firstMappedFile = this.getFirstMappedFile();
            MappedFile lastMappedFile = this.getLastMappedFile();
            if (firstMappedFile != null &amp;amp;&amp;amp; lastMappedFile != null) {
                if (offset &amp;lt; firstMappedFile.getFileFromOffset() || offset &amp;gt;= lastMappedFile.getFileFromOffset() + this.mappedFileSize) {
                    LOG_ERROR.warn("Offset not matched. Request offset: {}, firstOffset: {}, lastOffset: {}, mappedFileSize: {}, mappedFiles count: {}",
                        offset,
                        firstMappedFile.getFileFromOffset(),
                        lastMappedFile.getFileFromOffset() + this.mappedFileSize,
                        this.mappedFileSize,
                        this.mappedFiles.size());
                } else {
                    int index = (int) ((offset / this.mappedFileSize) - (firstMappedFile.getFileFromOffset() / this.mappedFileSize));
                    MappedFile targetFile = null;
                    try {
                        targetFile = this.mappedFiles.get(index);
                    } catch (Exception ignored) {
                    }
                    if (targetFile != null &amp;amp;&amp;amp; offset &amp;gt;= targetFile.getFileFromOffset()
                        &amp;amp;&amp;amp; offset &amp;lt; targetFile.getFileFromOffset() + this.mappedFileSize) {
                        return targetFile;
                    }
                    for (MappedFile tmpMappedFile : this.mappedFiles) {
                        if (offset &amp;gt;= tmpMappedFile.getFileFromOffset()
                            &amp;amp;&amp;amp; offset &amp;lt; tmpMappedFile.getFileFromOffset() + this.mappedFileSize) {
                            return tmpMappedFile;
                        }
                    }
                }
                if (returnFirstOnNotFound) {
                    return firstMappedFile;
                }
            }
        } catch (Exception e) {
            log.error("findMappedFileByOffset Exception", e);
        }
        return null;
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;按时间戳查找消息&lt;/h3&gt; 
&lt;p&gt;除了从指定 Offset 消费消息这种方式，消费者还有回溯到某个时间点开始消费的需求，这要求 RocketMQ 支持查询指定的 Timestamp 所在的 Offset，然后从这个 Offset 开始消费消息。&lt;/p&gt; 
&lt;p&gt;我们可以从 ConsumeQueue 的 GetOffsetInQueueByTime 方法直接了解按时间戳查找消息的具体实现。&lt;/p&gt; 
&lt;p&gt;消息是按时间先后写入的，ConsumeQueue 文件队列中的 CqUnit 也是按时间先后排列的，那么每个 MappedFile 都对应一段时间区间内的 CqUnit。从下面代码可以看出，我们可以先根据 Timestamp 找到其落在时间区间的 MappedFile，然后在该 MappedFile 里查找最接近该 Timestamp 的 CqUnit。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@Override
    public long getOffsetInQueueByTime(final long timestamp, final BoundaryType boundaryType) {
        MappedFile mappedFile = this.mappedFileQueue.getConsumeQueueMappedFileByTime(timestamp,
            messageStore.getCommitLog(), boundaryType);
        return binarySearchInQueueByTime(mappedFile, timestamp, boundaryType);
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;GetConsumeQueueMappedFileByTime 的具体实现主要分为两个部分：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;找到每个 MappedFile 的 StartTimestamp 和 StopTimestamp，即 MappedFile 里第一个 CqUnit 对应消息的时间戳和最后一个 CqUnit 对应消息的时间戳，需要访问两次 CommitLog 来得到消息内容。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;使用 Timestamp 和每个 MappedFile 的 StartTimestamp 和 StopTimestamp 比较。当 Timestamp 落在某个 MappedFile 的 StartTimestamp 和 StopTimestamp 区间内时，那么该 MappedFile 是下一步查找 CqUnit 的目标。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;接下来，要按照二分查找法在该 MappedFile 中找到最接近 Timestamp 的 CqUnit。根据二分查找的法则，每次查找需要比较中间位置的 CqUnit 引用消息的存储时间和目标 Timestamp 以确定下一个查找区间，直至 CqUnit 满足最接近目标 Timestamp 的条件。要注意的是，获取 CqUnit 引用消息的存储时间需从 CommitLog 中读取消息。&lt;/p&gt; 
&lt;h2&gt;基于 RocksDB 的优化方案&lt;/h2&gt; 
&lt;p&gt;尽管基于文件的实现比较直观，但是当 Topic 队列达到一定数量后，会出现明显的性能和可用性问题。Topic 队列数量越多，代表着 ConsumeQueue 文件越多，产生的随机读写也就越多，这会影响系统整体的 IO 性能，导致出现生产消费 TPS 不断下降，延迟不断增高的趋势。在我们内部的测试环境和客户的生产环境中，我们都发现使用的队列数过多直接影响系统的可用性，而且我们无法通过不断升级 Broker 节点配置来消除这种影响，因此我们腾讯云 TDMQ RocketMQ 版在产品控制枱上会限制客户可创建的 Topic 数量以确保消息服务的稳定性。&lt;/p&gt; 
&lt;p&gt;那么有没有办法能够解决上面的问题让服务能够承载更多的 Topic 呢？我们可以把 ConsumeQueue 提供的功能理解为使用 Topic 队列的 Offset 来找到 CqUnit，那么 Topic 队列和 Offset 构成了 Key，CqUnit 是 Value，是一个典型的 KV 使用场景。在单机 KV 存储的软件里，最著名的莫过于 RocksDB 了，它被广泛使用于 Facebook，LinkedIn 等互联网公司的业务中。从下面的设计图看，RocksDB 基于 SSTable + MemTable 的实现能够提供高效写入和查找 KV 的能力，有兴趣的读者可以研究下 RocksDB 的具体实现 (&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Ffacebook%2Frocksdb%2Fwiki%2FRocksDB-Overview" target="_blank"&gt;https://github.com/facebook/rocksdb/wiki/RocksDB-Overview&lt;/a&gt;)，这里不展开说明。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/097e131f-0b3f-41fa-9973-57cab02459f0.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;如果我们使用 RocksDB 读写 CqUnit，那么 ConsumeQueue 文件数量不会随着 Topic 队列的数量线性增长，便不必担心由此带来的 IO 开销。&lt;/p&gt; 
&lt;p&gt;下面我们来介绍如何使用 RocksDB 来实现 ConsumeQueue。&lt;/p&gt; 
&lt;h3&gt;数据存储与格式&lt;/h3&gt; 
&lt;p&gt;在基于 RocksDB 的实现里，RocketMQ 使用两个 ColumnFamily 来管理不同类型的数据，这里不熟悉 RocksDB 的读者可以将 ColumnFamily 视作 MySQL 里的 Table。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;第一个 ColumnFamiliy，简称为 DefaultColumnFamily，用于管理 CqUnit 数据。&lt;/p&gt; &lt;p&gt;Key 的内容格式定义参考下图，其包含 Topic 名称、QueueId 和 ConsumeQueue 的 Offset。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/831d29f2-7280-4583-8cf7-794df3d6d057.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Value 的内容格式，与前文中文件实现里的索引对象定义类似，但是多了一个消息存储时间的字段。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/cbba8329-dbd2-4dd0-9557-a8e455bcb831.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;第二个 ColumnFamily，简称为 OffsetColumnFamily，用于管理 Topic 队列的 MaxOffset 和 MinOffset。&lt;/p&gt; &lt;p&gt;MaxOffset 是指 Topic 队列最新一条消息在 ConsumeQueue 中的 Offset，随着消息的新增而变化。MinOffset 是指 Topic 队列最早一条消息在 ConsumeQueue 中的 Offset，当消息过期被删除后发生变化。MaxOffset 和 MinOffset 确定消费者可读取消息的范围，在基于文件的实现里，通过访问 ConsumeQueue 文件队列里的队尾和队首文件得到这两个数值。而在 RocksDB 的实现里，我们单独保存这两个数值。&lt;/p&gt; &lt;p&gt;下图是 Key 的格式定义，其包含 Topic 名称、QueueId 以及用于标记是 MaxOffset 或 MinOffset 的字段。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/84dd4df8-a509-4f23-8467-cd67f7b5be20.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Value 保存 ConsumeQueue 的 Offset，以及该 Offset 对应消息在 CommitLog 的位置。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/0e4c53a0-12b1-44fb-a457-3694f870b0c9.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h3&gt;构建过程&lt;/h3&gt; 
&lt;p&gt;ConsumeQueue 的 CqUnit 的构建过程与前文中基于文件的实现的过程一致，此处不再赘述，不同的是前文中 ReputMessageService 使用的 ConsumeQueueStore 被替换为 RocksDBConsumeQueueStore。在这个过程中，RocksDBConsumeQueueStore 主要完成两件事：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;往 DefaultColumnFamily 写入消息对应的 CqUnit。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;往 OffsetColumnFamily 更新消息对应 Topic 队列的 maxOffset。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp;private boolean putMessagePosition0(List&amp;lt;DispatchRequest&amp;gt; requests) {
        if (!this.rocksDBStorage.hold()) {
            return false;
        }
        try (WriteBatch writeBatch = new WriteBatch(); WriteBatch lmqTopicMappingWriteBatch = new WriteBatch()) {
            final int size = requests.size();
            if (size == 0) {
                return true;
            }
            long maxPhyOffset = 0;
            for (int i = size - 1; i &amp;gt;= 0; i--) {
                final DispatchRequest request = requests.get(i);
                DispatchEntry entry = DispatchEntry.from(request);
                dispatch(entry, writeBatch, lmqTopicMappingWriteBatch);
                dispatchLMQ(request, writeBatch, lmqTopicMappingWriteBatch);
                final int msgSize = request.getMsgSize();
                final long phyOffset = request.getCommitLogOffset();
                if (phyOffset + msgSize &amp;gt;= maxPhyOffset) {
                    maxPhyOffset = phyOffset + msgSize;
                }
            }
            // put lmq topic Mapping to DB if there has mapping exist
            if (lmqTopicMappingWriteBatch.count() &amp;gt; 0) {
                // write max topicId and all the topicMapping as atomic write
                ConfigHelperV2.stampMaxTopicSeqId(lmqTopicMappingWriteBatch, this.topicSeqIdCounter.get());
                this.configStorage.write(lmqTopicMappingWriteBatch);
                this.configStorage.flushWAL();
            }
            this.rocksDBConsumeQueueOffsetTable.putMaxPhyAndCqOffset(tempTopicQueueMaxOffsetMap, writeBatch, maxPhyOffset);
            this.rocksDBStorage.batchPut(writeBatch);
            this.rocksDBConsumeQueueOffsetTable.putHeapMaxCqOffset(tempTopicQueueMaxOffsetMap);
            long storeTimeStamp = requests.get(size - 1).getStoreTimestamp();
            if (this.messageStore.getMessageStoreConfig().getBrokerRole() == BrokerRole.SLAVE
                || this.messageStore.getMessageStoreConfig().isEnableDLegerCommitLog()) {
                this.messageStore.getStoreCheckpoint().setPhysicMsgTimestamp(storeTimeStamp);
            }
            this.messageStore.getStoreCheckpoint().setLogicsMsgTimestamp(storeTimeStamp);
            notifyMessageArriveAndClear(requests);
            return true;
        } catch (Exception e) {
            ERROR_LOG.error("putMessagePosition0 failed.", e);
            return false;
        } finally {
            tempTopicQueueMaxOffsetMap.clear();
            consumeQueueByteBufferCacheIndex = 0;
            offsetBufferCacheIndex = 0;
            this.rocksDBStorage.release();
        }
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;按 offset 查找消息&lt;/h3&gt; 
&lt;p&gt;在前文中我们已介绍过按 Offset 查找消息的流程，RocksDB 的实现里，DefaultMessageStore 的 GetMessage 方法中使用的 ConsumeQueue 被替换成了 RocksDBConsumeQueue。这里我们只关注其 IterateFrom 方法的实现，以下是该方法的代码片段。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;public ReferredIterator&amp;lt;CqUnit&amp;gt; iterateFrom(String group, long startIndex, int count) throws RocksDBException {
        long maxCqOffset = getMaxOffsetInQueue();
        if (startIndex &amp;lt; maxCqOffset) {
            int num = Math.min((int) (maxCqOffset - startIndex), count);
            if (MixAll.isLmq(topic) || PopAckConstants.isStartWithRevivePrefix(topic)) {
                return iterateUseMultiGet(startIndex, num);
            }
            if (num &amp;lt;= messageStore.getMessageStoreConfig().getUseScanThreshold()) {
                return iterateUseMultiGet(startIndex, num);
            }
            if (!messageStore.getMessageStoreConfig().isEnableScanIterator()) {
                return iterateUseMultiGet(startIndex, num);
            }
            final String scannerIterKey = group + "-" + Thread.currentThread().getId();
            ScanRocksDBConsumeQueueIterator scanRocksDBConsumeQueueIterator = scanIterators.get(scannerIterKey);
            if (scanRocksDBConsumeQueueIterator == null) {
                if (RocksDBConsumeQueue.this.messageStore.getMessageStoreConfig().isEnableRocksDBLog()) {
                    LOG.info("new ScanIterator Group-threadId{} Topic:{}, queueId:{},startIndex:{}, count:{}",
                        scannerIterKey, topic, queueId, startIndex, num);
                }
                ScanRocksDBConsumeQueueIterator newScanIterator = new ScanRocksDBConsumeQueueIterator(startIndex, num);
                scanRocksDBConsumeQueueIterator = scanIterators.putIfAbsent(scannerIterKey, newScanIterator);
                if (scanRocksDBConsumeQueueIterator == null) {
                    scanRocksDBConsumeQueueIterator = newScanIterator;
                } else {
                    newScanIterator.closeRocksIterator();
                }
                return scanRocksDBConsumeQueueIterator;
            }
            if (!scanRocksDBConsumeQueueIterator.isValid()) {
                scanRocksDBConsumeQueueIterator.closeRocksIterator();
                if (RocksDBConsumeQueue.this.messageStore.getMessageStoreConfig().isEnableRocksDBLog()) {
                    LOG.info("new ScanIterator not valid Group-threadId{} Topic:{}, queueId:{},startIndex:{}, count:{}",
                        scannerIterKey, topic, queueId, startIndex, count);
                }
                ScanRocksDBConsumeQueueIterator newScanIterator = new ScanRocksDBConsumeQueueIterator(startIndex, num);
                scanIterators.put(scannerIterKey, newScanIterator);
                return newScanIterator;
            } else {
                if (RocksDBConsumeQueue.this.messageStore.getMessageStoreConfig().isEnableRocksDBLog()) {
                    LOG.info("new ScanIterator valid then reuse Group-threadId{} Topic:{}, queueId:{},startIndex:{}, count:{}",
                        scannerIterKey, topic, queueId, startIndex, count);
                }
                scanRocksDBConsumeQueueIterator.reuse(startIndex, num);
                return scanRocksDBConsumeQueueIterator;
            }
        }
        return null;
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;在上面的代码中，首先通过 GetMaxOffsetInQueue 方法获取该 Topic 队列 ConsumeQueue 的 MaxOffset，MaxOffset 结合 Count 参数共同指定 Iterator 扫描的 Offset 区间。&lt;/p&gt; 
&lt;p&gt;然后，我们可以看到 IterateFrom 方法中根据不同的条件判断分支返回不同类型的 Iterator 类对象，RocksDBConsumeQueueIterator 和 ScanRocksDBConsumeQueueIterator。下面是 &amp;nbsp;IteratorUseMultiGet 方法中创建 RocksDBConsumeQueueIterator 对象的调用链中最核心的代码， RangeQuery 方法根据 StartIndex 和 Num 构建了要查询的 Key 列表，然后调用 RocksDB 的 MultiGet 方法查询到 Key 列表对应的 Value 列表，RocksDBConsumeQueueIterator 使用该 Value 列表上提供迭代器的功能。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;nbsp; &amp;nbsp;public List&amp;lt;ByteBuffer&amp;gt; rangeQuery(final String topic, final int queueId, final long startIndex,
        final int num) throws RocksDBException {
        final byte[] topicBytes = topic.getBytes(StandardCharsets.UTF_8);
        final List&amp;lt;ColumnFamilyHandle&amp;gt; defaultCFHList = new ArrayList&amp;lt;&amp;gt;(num);
        final ByteBuffer[] resultList = new ByteBuffer[num];
        final List&amp;lt;Integer&amp;gt; kvIndexList = new ArrayList&amp;lt;&amp;gt;(num);
        final List&amp;lt;byte[]&amp;gt; kvKeyList = new ArrayList&amp;lt;&amp;gt;(num);
        for (int i = 0; i &amp;lt; num; i++) {
            ByteBuffer keyBB;
            // must have used topicMapping
            if (this.topicMappingTable != null) {
                Long topicId = topicMappingTable.get(topic);
                if (topicId == null) {
                    throw new RocksDBException("topic: " + topic + " topicMapping not existed error when rangeQuery");
                }
                keyBB = buildCQFixKeyByteBuffer(topicId, queueId, startIndex + i);
            } else {
                keyBB = buildCQKeyByteBuffer(topicBytes, queueId, startIndex + i);
            }
            kvIndexList.add(i);
            kvKeyList.add(keyBB.array());
            defaultCFHList.add(this.defaultCFH);
        }
        int keyNum = kvIndexList.size();
        if (keyNum &amp;gt; 0) {
            List&amp;lt;byte[]&amp;gt; kvValueList = this.rocksDBStorage.multiGet(defaultCFHList, kvKeyList);
            final int valueNum = kvValueList.size();
            if (keyNum != valueNum) {
                throw new RocksDBException("rocksdb bug, multiGet");
            }
            for (int i = 0; i &amp;lt; valueNum; i++) {
                byte[] value = kvValueList.get(i);
                if (value == null) {
                    continue;
                }
                ByteBuffer byteBuffer = ByteBuffer.wrap(value);
                resultList[kvIndexList.get(i)] = byteBuffer;
            }
        }
        final int resultSize = resultList.length;
        List&amp;lt;ByteBuffer&amp;gt; bbValueList = new ArrayList&amp;lt;&amp;gt;(resultSize);
        for (int i = 0; i &amp;lt; resultSize; i++) {
            ByteBuffer byteBuffer = resultList[i];
            if (byteBuffer == null) {
                break;
            }
            bbValueList.add(byteBuffer);
        }
        return bbValueList;
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ScanRocksDBConsumeQueueIterator 则是使用了 RocksDB 的 Iterator 特性（&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Ffacebook%2Frocksdb%2Fwiki%2FIterator%EF%BC%89%EF%BC%8C%E7%9B%B8%E6%AF%94" target="_blank"&gt;https://github.com/facebook/rocksdb/wiki/Iterator），相比&lt;/a&gt; MultiGet，其拥有更好的性能。&lt;/p&gt; 
&lt;p&gt;下面是 ScanQuery 的实现，代码比较简洁，指定 Iterator 的 BeginKey 和 UpperKey，再调用 RocksDB 的 API 返回 Iterator 对象。&lt;/p&gt; 
&lt;p&gt;BeginKey 是通过 Topic 队列信息和 StartIndex 参数构造的 Key。UpperKey 的构造比较精妙，还记得在 DefaultColumnFamily 介绍里 Key 的格式吧，Key 的倒数第二个部分是 CTRL_1，作为 CqUnit 的 Key 时是个常量，Unicode 值为 1。构造 UpperKey 时，CTRL_1 被替换为 CTRL_2， Uinicode 值为 2，这样能保证 Iterator 扫描区间的上限不超过 Topic 队列 Offset 的理论最大值。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;public RocksIterator scanQuery(final String topic, final int queueId, final long startIndex,
        ReadOptions scanReadOptions) throws RocksDBException {
        final ByteBuffer beginKeyBuf = getSeekKey(topic, queueId, startIndex);
        if (scanReadOptions.iterateUpperBound() == null) {
            ByteBuffer upperKeyForInitScanner = getUpperKeyForInitScanner(topic, queueId);
            byte[] buf = new byte[upperKeyForInitScanner.remaining()];
            upperKeyForInitScanner.slice().get(buf);
            scanReadOptions.setIterateUpperBound(new Slice(buf));
        }
        RocksIterator iterator = this.rocksDBStorage.scan(scanReadOptions);
        iterator.seek(beginKeyBuf.slice());
        return iterator;
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;按时间戳查找消息&lt;/h3&gt; 
&lt;p&gt;与基于文件的实现类似，使用 RocksDB 来按时间戳查找消息，首先也需要确定 Topic 队列 ConsumeQueue 的 MinOffset 和 MaxOffset，然后使用二分查找法查找到最接近指定时间戳的 CqUnit。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp;@Override
    public long getOffsetInQueueByTime(String topic, int queueId, long timestamp,
        BoundaryType boundaryType) throws RocksDBException {
        final long minPhysicOffset = this.messageStore.getMinPhyOffset();
        long low = this.rocksDBConsumeQueueOffsetTable.getMinCqOffset(topic, queueId);
        Long high = this.rocksDBConsumeQueueOffsetTable.getMaxCqOffset(topic, queueId);
        if (high == null || high == -1) {
            return 0;
        }
        return this.rocksDBConsumeQueueTable.binarySearchInCQByTime(topic, queueId, high, low, timestamp,
            minPhysicOffset, boundaryType);
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;与基于文件的实现不同的是，由于 RocksDB 的 CqUnit 里保存了消息存储的时间，比较时间戳时不必再读取 CommitLog 获取消息的存储时间，这样提升了查找的时间效率。&lt;/p&gt; 
&lt;h2&gt;总结及展望&lt;/h2&gt; 
&lt;p&gt;本文和读者分享了 ConsumeQueue 的设计与实现，着重介绍其在消息消费场景的应用。鉴于篇幅限制，仍有许多细节未涉及，比如 ConsumeQueue 的容错恢复、过期清理机制等。近些年，RocketMQ 往 Serveless 化方向发展，在 5.0 的架构里，已经将计算和存储分离，Proxy 作为计算集群，Broker 作为存储集群。从实际应用上来讲，Broker 作为存储角色，从计算的角色释放出来之后，多出的性能和资源应该用于承载更多的 Topic，而基于文件的 ConsumeQueue 实现限制了 Broker 的上限，因此我们需要 RocksDB 的实现方案来解决这个问题。&lt;/p&gt; 
&lt;p&gt;目前，腾讯云的 TDMQ RabbitMQ Serveless、MQTT 产品均基于 RocketMQ 5.0 的架构部署运行，Broker 集群已采用 RocksDB 的方案支持百万级的 Topic 队列，满足 RabbitMQ 和 MQTT 协议需要大量 Topic 支持的场景。在腾讯云 RocketMQ 5.0 的产品上，我们开始逐渐在新版本中灰度开启该方案，为客户提供更好性能更稳定的消息队列服务。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/4587289/blog/18499646</link>
      <guid isPermaLink="false">https://my.oschina.net/u/4587289/blog/18499646</guid>
      <pubDate>Sun, 03 Aug 2025 02:23:00 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>阿里通义发布开源文生图模型 Qwen-Image</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;阿里通义千问团队开源了其首个图像生成基础模型 Qwen-Image。该模型是一个拥有 200 亿参数的 MMDiT（多模态扩散 Transformer）模型，基于 Apache 2.0 许可证开源。&lt;/p&gt; 
&lt;p&gt;Qwen-Image 在复杂文本渲染和精确图像编辑方面取得了显著进展，尤其在中文文本渲染上表现卓越。&lt;/p&gt; 
&lt;p&gt;Qwen-Image 的主要特性包括：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;卓越的文本渲染能力:&amp;nbsp;Qwen-Image 在复杂文本渲染方面表现出色，支持多行布局、段落级文本生成以及细粒度细节呈现。无论是英语还是中文，均能实现高保真输出。&lt;/li&gt; 
 &lt;li&gt;一致性的图像编辑能力:&amp;nbsp;通过增强的多任务训练范式，Qwen-Image 在编辑过程中能出色地保持编辑的一致性。&lt;/li&gt; 
 &lt;li&gt;强大的跨基准性能表现:&amp;nbsp;在多个公开基准测试中的评估表明，Qwen-Image 在各类生成与编辑任务中均获得 SOTA，是一个强大的图像生成基础模型。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;如需体验 Qwen-Image，访问 QwenChat（chat.qwen.ai) 并选择「图像生成」功能。同时该模型已在魔搭社区与 Hugging Face 开源。&lt;/p&gt; 
&lt;p&gt;ModelScope：https://modelscope.cn/models/Qwen/Qwen-Image&lt;br&gt; Hugging Face：https://huggingface.co/Qwen/Qwen-Image&lt;br&gt; GitHub：https://github.com/QwenLM/Qwen-Image&lt;br&gt; Technical report：https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/Qwen_Image.pdf&lt;br&gt; Demo:&amp;nbsp;https://modelscope.cn/aigc/imageGeneration?tab=advanced&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;示例展示&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;宫崎骏的动漫风格。平视角拍摄，阳光下的古街热闹非凡。一个穿着青衫、手里拿着写着「阿里云」卡片的逍遥派弟子站在中间。旁边两个小孩惊讶的看着他。左边有一家店铺挂着「云存储」的牌子，里面摆放着发光的服务器机箱，门口两个侍衞守护者。右边有两家店铺，其中一家挂着「云计算」的牌子，一个穿着旗袍的美丽女子正看着里面闪闪发光的电脑屏幕；另一家店铺挂着「云模型」的牌子，门口放着一个大酒缸，上面写着「千问」，一位老板娘正在往里面倒发光的代码溶液。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0805/101844_6QPa_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364222</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364222</guid>
      <pubDate>Sun, 03 Aug 2025 02:18:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>KiCad - 开源免费电子设计自动化（EDA）套件</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                        
                                                                                    &lt;p style="color:#000000; margin-left:auto; margin-right:auto; text-align:start"&gt;KiCad EDA 是一款开源的电子设计自动化（EDA）软件，基于 GPLv3 开源协议，最初由法国人 Jean-Pierre Charras 于 1992 年推出，现由 KiCad 开源社区维护。&lt;/p&gt;

&lt;p style="color:#000000; margin-left:auto; margin-right:auto; text-align:start"&gt;KiCad 提供了一个完整的设计流程，从原理图到 PCB 布局，以及 3D 模型和 BOM 生成。KiCad 支持多种文件格式，可以与其他 EDA 软件兼容，并且可以在多种操作系统上运行，包括 Windows，Linux 和 Mac OS X，软件包含工程项目管理、原理图设计、线路板绘制、符号库设计、封装库设计、线路板 3D 显示、Gerber 查看、线路板实用计算等工具。&lt;/p&gt;

&lt;p style="color:#000000; margin-left:auto; margin-right:auto; text-align:start"&gt;&lt;img alt="" height="1080" src="https://oscimg.oschina.net/oscnet/up-416b95962f155bd5c9c0b3172706d97017f.png" width="1920" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

&lt;p style="color:#000000; margin-left:auto; margin-right:auto; text-align:start"&gt;&lt;img alt="" height="1080" src="https://oscimg.oschina.net/oscnet/up-a671e278e4aae67095a053103e543bc62a3.png" width="1920" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

&lt;p style="color:#000000; margin-left:auto; margin-right:auto; text-align:start"&gt;&lt;img alt="" height="1080" src="https://oscimg.oschina.net/oscnet/up-6501cd15c8e91e73f511b9edeaeed9ac221.png" width="1920" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

&lt;p&gt;KiCad EDA 官网：&lt;a href="https://www.kicad.org/"&gt;https://www.kicad.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;KiCad EDA 开源中国&amp;nbsp;&lt;a href="https://gitee.com/kicad-eda"&gt;https://gitee.com/kicad-eda&lt;/a&gt;&lt;/p&gt;

                                                                    &lt;/div&gt;
                                                                </description>
      <link>https://www.oschina.net/p/kicad</link>
      <guid isPermaLink="false">https://www.oschina.net/p/kicad</guid>
      <pubDate>Sun, 03 Aug 2025 02:11:00 GMT</pubDate>
    </item>
    <item>
      <title>​Perplexity AI 被指控秘密抓取被禁止的网站内容</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p style="margin-left:0; margin-right:0"&gt;根据互联网基础设施提供商 Cloudflare 的&lt;span&gt;最新&lt;/span&gt;研究&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.cloudflare.com%2Fperplexity-is-using-stealth-undeclared-crawlers-to-evade-website-no-crawl-directives%2F" target="_blank"&gt;报告&lt;/a&gt;，人工智能初创公司 Perplexity 被指控在抓取网站内容时忽视了明确的阻止指令。Cloudflare 表示，他们观察到 Perplexity 在尝试抓取网页时隐藏了自己的身份，以此规避网站的偏好设置。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;&lt;img height="312" src="https://oscimg.oschina.net/oscnet/up-cc5a99ff2f39168f9451614222dc2d73118.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Perplexity 等人工智能产品通常依赖于从互联网收集大量数据，而这些初创公司长期以来在未获得许可的情况下抓取文本、图像和视频，以便支持其产品的正常运作。近年来，许多网站通过使用标准的 Robots.txt 文件来应对这一问题，该文件指示搜索引擎和 AI 公司哪些页面可以被索引，哪些页面不可以。然而，当前这些努力的成效并不显著。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;根据 Cloudflare 的分析，Perplexity 似乎通过更改其机器人的 「用户代理」 来绕过这些限制。「用户代理」 是指用于识别网站访问者的设备和版本类型的信号。Cloudflare 还提到，Perplexity 更改了其自治系统网络（ASN），这是一个识别互联网上大型网络的数字标识。Cloudflare 在数万个域名和数百万个请求中观察到了这一行为，凭借机器学习和网络信号的结合成功识别了这一爬虫。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Perplexity 的发言人 Jesse Dwyer 对 Cloudflare 的指控表示反驳，并称其博客文章为 「推销」。他补充称，文中截图显示并没有访问内容。他进一步声称，Cloudflare 所提到的爬虫并非其所拥有的。Cloudflare 表示，他们最初注意到这些问题是由于客户投诉 Perplexity 仍在抓取其网站内容，尽管这些网站已通过 Robots 文件阻止了该爬虫的访问。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;Cloudflare 的分析表明，Perplexity 不仅使用了其声明的用户代理，还在其被阻止时利用一个模拟 Google Chrome 的通用浏览器。最终，Cloudflare 决定将 Perplexity 的爬虫从其验证列表中移除，并采取新的技术来阻止其活动。&lt;/p&gt; 
&lt;p style="margin-left:0; margin-right:0"&gt;值得注意的是，Cloudflare 最近对人工智能爬虫表示反对，并推出了一个市场，允许网站所有者向访问其网站的 AI 爬虫收费。Cloudflare 的首席执行官马修・普林斯曾警告称，人工智能正在破坏互联网的商业模式，尤其是出版商的盈利模式。这并非 Perplexity&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;第一&lt;/span&gt;次面临未经授权抓取的指控，早在去年，《连线》杂志等媒体就曾指控 Perplexity 抄袭其内容。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364217</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364217</guid>
      <pubDate>Sun, 03 Aug 2025 01:59:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>「香山」实现业界首个开源芯片的产品级交付与首次规模化应用</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 开源高性能 RISC-V 处理器核「香山」产业落地取得里程碑式突破。7 月 16-19 日，在上海举办的 2025 RISC-V 中国峰会期间，北京开源芯片研究院（以下简称开芯院）在大会报告中宣布第三代「香山」（昆明湖）IP 核已实现了首批量产客户的产品级交付。7 月 26-28 日，世界人工智能大会期间，集成了第二代「香山」（南湖）IP 核的某国产量产 GPGPU 芯片正式亮相，基于该芯片的智能加速卡出货量已上万——「香山」（南湖）IP 核实现规模化应用。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 香山 IP 核在业界首次实现了产品级交付与规模化应用，标志着开源高性能处理器 IP 核正式进入产业落地阶段，为 RISC-V 产业技术研发、商业落地开辟了一条不同于传统 ARM 模式、基于开源模式的新路径。香山 IP 核的首次产品级交付与规模化应用，就如 1990 年代中期开源操作系统 Linux 首次在企业中部署应用，具有重要的里程碑意义，必将产生深远影响。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:center"&gt;&lt;strong&gt;开源高性能 RISC-V 处理器核&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:center"&gt;&lt;strong&gt;第二代「香山」（南湖）已实现首次规模化应用&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 开源高性能 RISC-V 处理器核「香山」，源于中国科学院在 2019 年的前瞻布局。中国科学院计算技术研究所（以下简称计算所）于 2021 年 6 月成功研制了第一代开源高性能 RISC-V 处理器核「香山（雁栖湖）」，性能对标 ARM A73，SPECINT2006 7 分/GHz，是同期全球性能最高的开源处理器核。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 为了加速香山的技术演进和应用落地，加快 RISC-V 生态建设, 2021 年北京市与中国科学院达成战略合作，发挥北京市应用牵引和芯片定义的优势，组织 18 家行业龙头企业和国内顶尖科研单位共同发起成立开芯院。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 2023 年 5 月 26 日，开芯院在中关村论坛上正式对外发布由多家单位联合开发的第二代「香山」（南湖）。这是一款性能对标 ARM Cortex-A76 的高性能开源 RISC-V 处理器核，主频 2GHz@14nm，SPECCPU2006 分值达到 10 分/GHz，专门针对工业控制、汽车、通信等泛工业领域。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 「香山」（南湖）成功带动一批企业加速布局 RISC-V 产品线，开始在一些芯片产品中集成「香山」（南湖）IP 核，取得积极成效。近日，在上海举办的世界人工智能大会上，某国产 GPU 芯片厂商展示的自研智算加速卡中成功集成了「香山」（南湖）IP 核。据了解，该国产 GPU 公司已经实现全国产千卡千亿模型算力集群的交付，正朝着万卡智算集群加速迭代。这标志着「香山」（南湖）IP 核首次实现规模化应用，也推动了 RISC-V 在人工智能智算集群中的产业化落地。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 「香山」(南湖)IP 核作为高性能片内主控 CPU，也被用于芯动科技高性能全功能 GPGPU 芯片「风华 3 号"中,实现 CPU+AI 强强联手。其中，香山 CPU 核负责从主 CPU 卸载的一些关键功能，包括处理跨芯片通讯与数据搬运、启动控制及片内 IP 配置、实现低功耗与动态功耗控制、确保系统稳定运行、提供异常处理能力等。香山 CPU 核与高性能风华 GPU 的结合，在重度负载渲染、高性能 AI 计算、多芯片集群互联等使用场景中能发挥各自的优势，提供了高性能低功耗、灵活定制和成本效益的处理器的解决方案。据悉，该产品即将面市。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:center"&gt;&lt;strong&gt;基于开源模式联合研发&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:center"&gt;&lt;strong&gt;第三代「香山」（昆明湖）&lt;/strong&gt;&lt;strong&gt;已&lt;/strong&gt;&lt;strong&gt;实现产品级交付&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 2022 年 8 月，开芯院牵头联合计算所、腾讯、阿里、中兴通讯、中科创达、奕斯伟、算能等形成了联合研发团队，在全球首次采用基于开源的处理器核联合研发模式，共同研制第三代「香山」（昆明湖）开源高性能 RISC-V 处理器核，性能对标 ARM N2。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 2024 年 4 月，「香山」（昆明湖）正式发布，SPECCPU2006 分值达到 15 分/GHz，符合 RVA23 标准，性能进入全球 RISC-V 处理器第一梯队。同时，「香山」开源芯片项目在全球最大的开源项目托管平台 GitHub 上获得超过 6500 个星标（Star），形成超过 780 个分支（Fork），远超其他开源硬件项目，成为国际开源社区性能最强、最活跃的 RISC-V 处理器核。「香山」及其敏捷开发基础设施入选「计算机体系结构领域年度全球十二大亮点成果」，连续两年入选「2024 中关村论坛 10 项重大科技成果」和 2025 中关村论坛「北京重大开源成果」。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 在发布后的一年多时间中，开芯院联合企业共同完成了针对「香山」（昆明湖）的产品级验证工作，包括按规模量产芯片企业要求构建了一套严格的测试验证流程，形成了「单元级测试 UT集成级测试 IT系统测试 ST原型系统测试 Prototype」四个层次的验证规范，开发了超过 2 万个测试用例，建立了一套包含数十个商业工具、开源工具、形式化工具等验证工具箱等等。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 实践表明，基于开源的联合开发模式，平摊了验证成本，提升了验证效率——用户/企业贡献了近 1600 个测试用例，发现的 1470 项 BUG 中企业累计提交了 492 项。通过开芯院与多家企业的共同努力， 「香山」（昆明湖）最终实现了验证覆盖率近 100%，同时大幅降低了企业获得性能对标 ARM N2 的产品级 RISC-V IP 核的成本。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:start"&gt;&amp;nbsp; 目前，「香山」（昆明湖）已实现首批量产客户的产品级交付。进迭时空正基于「香山」（昆明湖）自研 X200 核，并研发其第三代旗舰 RISC-V AI CPU 芯片，预计 2026 年底进入量产。同时，进迭时空研发的首款 RISC-V 服务器芯片将于近期流片，其中内置 6 个「香山」（昆明湖）核。在双方团队的努力下，进迭时空服务器芯片已在 FPGA 平台上稳定地运行 Linux 操作系统及虚拟机。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/364198</link>
      <guid isPermaLink="false">https://www.oschina.net/news/364198</guid>
      <pubDate>Sun, 03 Aug 2025 01:06:00 GMT</pubDate>
      <author>作者: 开源科技</author>
    </item>
  </channel>
</rss>
