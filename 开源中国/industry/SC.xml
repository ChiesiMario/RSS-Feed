<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - industry - 简体中文</title>
    <link>https://www.oschina.net/news/industry</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news/industry" rel="self" type="application/rss+xml"/>
    <description>已对该 RSS 进行格式化操作：中英字符之间插入空格、使用直角引号、标点符号修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-cn</language>
    <lastBuildDate>Wed, 25 Jun 2025 07:43:43 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>Anthropic 未经许可使用书籍训练 AI 模型属于「合理使用」</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;美国旧金山联邦法官威廉・阿尔苏普（William Alsup）&lt;u&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.reuters.com%2Flegal%2Flitigation%2Fanthropic-wins-key-ruling-ai-authors-copyright-lawsuit-2025-06-24%2F" target="_blank"&gt;裁定&lt;/a&gt;&lt;/u&gt;，&lt;strong&gt;Anthropic 在未经作者许可的情况下使用已出版书籍训练其 AI 模型属于「合理使用 (fair use)」&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0625/153420_i6T1_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;这标志着法院首次认可 AI 公司的主张，即当 AI 公司使用受版权保护的材料训练大型语言模型（LLM）时，合理使用原则可使其免于承担责任。&lt;/p&gt; 
&lt;p&gt;法官指出，AI 模型对作品的训练类似于读者阅读并从中汲取灵感以创作新内容，而非复制或取代原作。然而，判决也指出，Anthropic 在 2021 年至 2022 年期间从 Books3、Library Genesis 和 Pirate Library Mirror 等来源下载的超过 700 万本盗版电子书不属于合理使用，这部分内容将面临陪审团审判。Anthropic 曾花费数百万美元购买并扫描大量印刷书籍，将其转换为数字格式用于内部研究。&lt;/p&gt; 
&lt;p&gt;这一裁决被认为是 AI 行业在版权合理使用方面的一个重要里程碑。同时对作者、艺术家和出版商是一个打击，他们已对 OpenAI、Meta、Midjourney、Google 等公司提起数十起诉讼。尽管这一裁决并不能保证其他法官会效仿阿尔苏普法官的做法，但它为支持科技公司而非创作者的先例奠定了基础。&lt;/p&gt; 
&lt;p&gt;这些诉讼通常取决于法官如何解释合理使用原则，这是版权法中一个出了名难以界定的例外条款，该条款自 1976 年以来就未更新过 —— 那时互联网尚未出现，更不用说生成式 AI 训练数据集的概念了。&lt;/p&gt; 
&lt;p&gt;合理使用裁决会考虑作品的使用目的（模仿和教育用途可能是可行的）、是否为商业利益而复制（你可以写《星球大战》同人小说，但不能出售），以及衍生作品与原作相比的转换性程度。&lt;/p&gt; 
&lt;p&gt;像 Meta 这样的公司在为使用受版权保护的作品进行训练辩护时也提出了类似的合理使用论点，不过在本周的裁决之前，法院会如何裁决还不太明确。&lt;/p&gt; 
&lt;p&gt;在这起具体的 Bartz 诉 Anthropic 案中，原告作者团体还对 Anthropic 获取和存储他们作品的方式提出了质疑。根据诉讼称，Anthropic 试图创建一个 「中央图书馆」，收录 「世界上所有的书籍」 并 「永久」 保存。但这些受版权保护的数百万本书籍是从盗版网站免费下载的，这显然是非法的。&lt;/p&gt; 
&lt;p&gt;尽管法官承认 Anthropic 对这些材料的训练属于合理使用，但法院将对 「中央图书馆」 的性质进行审判。&lt;/p&gt; 
&lt;p&gt;「我们将对用于创建 Anthropic 中央图书馆的盗版副本及其造成的损害进行审判」， 阿尔苏普法官在裁决中写道，「Anthropic 后来购买了一本之前从互联网上窃取的书，并不能免除其盗窃责任，但可能会影响法定损害赔偿的程度。」&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;相关阅读：&lt;a href="https://www.oschina.net/news/353744" target="news"&gt;Reddit&amp;nbsp;起诉 Anthropic 未经许可使用其数据训练 AI 模型&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357201</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357201</guid>
      <pubDate>Wed, 25 Jun 2025 07:34:40 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>WinRAR「压缩包」再度开售，价格和 5 份 WinRAR 正版授权相当</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;WinRAR 官方手提包周边于 2025 年 2 月首次由 WinRAR 与制造商 Tern 联动推出，当时迅速售罄。近日，Tern &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Ftern_et%2Fstatus%2F1935705139429470405" target="_blank"&gt;宣布这款「压缩包」再次开售&lt;/a&gt;，并将于 9 月开始发货。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1052" src="https://static.oschina.net/uploads/space/2025/0625/150540_IMgE_2720166.png" width="1300" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="858" src="https://static.oschina.net/uploads/space/2025/0625/150922_XENV_2720166.png" width="1146" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;以下是具体信息：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;价格&lt;/strong&gt; ：定价 150 美元，按现汇率约合 1077 元人民币，相当于购买五份正版 WinRAR 软件的费用。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;设计&lt;/strong&gt; ：尺寸为 21.4×14×7 厘米，外形参考 WinRAR 的图标设计，呈现出经典的粉、蓝、绿三色书本被皮带捆扎的设计，此次版本的肩带改为可拆卸式，使用夹子固定，方便用户根据需要调整。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;购买方式及发货时间&lt;/strong&gt; ：在 Tern 官网售卖，支持全球 DHL 邮寄，将于 2025 年 9 月发货。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;购买地址：https://in.tern.et/products/winrar-archive-messenger-bag-prod&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357195/winrar-archive-messenger-bag</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357195/winrar-archive-messenger-bag</guid>
      <pubDate>Wed, 25 Jun 2025 07:11:40 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>投资 140 亿却成竞争对手，微软遭遇 ChatGPT 企业市场</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;彭博社最新报道称，微软正面临一个尴尬局面：尽管该公司努力向企业推销 Copilot AI 助手，但越来越多的员工却更青睐其合作伙伴 OpenAI 的 ChatGPT，这一现象正在企业市场引发激烈竞争。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;知名药企安进公司的经历完美诠释了这一市场变化。去年春天，安进宣布为旗下 2 万名员工购买微软 Copilot，成为微软在生成式 AI 领域的重要客户案例。然而，仅仅 13 个月后，安进员工却纷纷转向使用 OpenAI 的 ChatGPT。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;安进高级副总裁肖恩·布鲁伊希表示："OpenAI 成功的秘诀在于，他们把产品做得极具趣味性。"他指出，ChatGPT 在研究和科学文献总结等任务中表现尤为出色，而 Copilot 更多是在配合微软自家软件使用时才显现优势。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;这种现象让微软与 OpenAI 之间的关系变得更加微妙。作为 OpenAI 的最大投资方，微软已累计投资近 140 亿美元，但现在却发现自己在企业市场与被投资方直接竞争。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="333" src="https://oscimg.oschina.net/oscnet/up-4e4e30f181b826c01ebfb42865d828ee27e.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;微软销售团队反映，在推广 Copilot 时经常措手不及，而公司又迫切希望快速扩大客户基础。与此同时，OpenAI 也在积极扩张企业业务，近期更是收购了 AI 代码助手 Windsurf，直接对标微软的 GitHub Copilot。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;尽管两款产品都基于 OpenAI 的大语言模型，但用户体验存在显著差异。许多企业发现，员工普遍更偏爱 ChatGPT，主要原因包括：&lt;/span&gt;&lt;/p&gt; 
&lt;ul style="margin-left:0; margin-right:0"&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;更新速度&lt;/strong&gt;：OpenAI 的模型更新在微软软件中往往延迟数周才能落地&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;用户熟悉度&lt;/strong&gt;：很多职场人士早已在个人场景中体验过 ChatGPT&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;功能体验&lt;/strong&gt;：ChatGPT 在某些专业任务中表现更优&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;微软解释称，延迟更新是因为需要进行企业级安全测试和用户体验验证。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;面对这一竞争态势，不同企业采取了不同策略。纽约人寿保险公司决定在 12000 名员工中同时推广 ChatGPT 和 Copilot，根据使用反馈决定最终选择。金融科技公司 Finastra 选择微软 Copilot，看重其与微软办公软件的深度整合优势。贝恩咨询公司则向 16000 名员工部署 ChatGPT，绝大多数员工日常使用，而仅有约 2000 名员工使用 Copilot 且主要搭配 Excel。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;OpenAI 表示已拥有 300 万付费企业用户，短短几个月内增长 50%。微软则回应称 Copilot 已覆盖 70% 的财富 500 强企业，付费用户数量比去年同期增加两倍。在定价方面，微软 Copilot 每用户每月 30 美元，相比 ChatGPT 企业版的 60 美元更具价格优势，但 OpenAI 也推出了按使用量收费的灵活方案。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;Gartner 分析师杰森·王认为，目前许多公司仍在小范围测试阶段，市场竞争格局尚未完全确定，但这无疑是"OpenAI 与微软之间的正面对决"。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357193</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357193</guid>
      <pubDate>Wed, 25 Jun 2025 07:10:40 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>支付宝为 AI 开发者提供国内首个「AI 打赏」服务</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;支付宝宣布为 AI 开发者提供国内首个「AI 打赏」服务，并首发上线蚂蚁百宝箱平台、阿里云百炼，为开发者提供便捷收款能力，进一步推动 AI 技术的商业化应用。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;据介绍，「AI 打赏」服务旨在满足 AI 智能体内收取赞赏、小费等需求，为开发者提供一种轻量化的收款解决方案。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;个人开发者只需登录蚂蚁百宝箱平台或阿里云百炼，选择开通「AI 打赏」功能并给智能体挂载该服务，即可快速启用打赏功能。开通后，用户打赏的金额将直接转入开发者账户。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="546" src="https://oscimg.oschina.net/oscnet/up-d77d6a355574699398813e0bde3af584ec9.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;值得一提的是，今年 4 月，支付宝曾推出国内首个支付 MCP，助力 AI 开发者具备支付收款能力，实现服务订阅、付费解锁等商业化功能。而此次推出的「AI 打赏」服务，则更侧重于让用户主动表达赞赏和感谢，两者结合将形成基础付费与灵活激励并存的多元服务模式。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357178</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357178</guid>
      <pubDate>Sun, 11 May 2025 06:09:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>AI 编辑器 Void 发布 Beta，可作为 Cursor 开源替代方案</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;Void 是一款开源 AI 编辑器，可作为 Cursor 的替代品。Void&amp;nbsp;支持跟 Cursor 一样的功能，比如 Tab 补全代码，Ctrl + K 编辑选中内容，支持用 AI 搜索代码库，支持编辑和查看底层提示。&lt;/p&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-a9e2f51d6bea16cac1680764e79150155e1.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fvoideditor%2Fvoid%2Freleases%2Ftag%2Fbeta" target="_blank"&gt;Void 最近发布了 Beta 版本&lt;/a&gt;，其作为 VS Code 的分支，旨在解决私有 AI 辅助编程工具的安全隐私和费用问题。闭源编辑器可能需要通过后端发送私有代码数据，这会带来隐私问题，另一个问题是持续的订阅费用。&lt;/p&gt; 
&lt;p&gt;Void 提供了多种选项，确保开发者能控制自己的数据。它能利用多种大模型，可以使用任何本地的 LLM 驱动，也可以使用 Claude、GPT 或 Gemini 的 API，不会留存你的数据，避开了第三方中间人。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357154/void-editor-beta</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357154/void-editor-beta</guid>
      <pubDate>Sun, 11 May 2025 03:56:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>ElevenLabs 发布移动端 AI 语音工具 APP</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;ElevenLabs 是一家专注于开发人工智能语音模型和工具的 AI 公司，近日&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Felevenlabs.io%2Fblog%2Fintroducing-the-elevenlabs-app" target="_blank"&gt;宣布&lt;/a&gt;推出官方 ElevenLabs 移动应用，为用户提供最强大的 AI 语音工具，支持 iOS 和 Android 用户随时随地将文本转成语音片段。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-e444a0db29602ba48998df1adbdf75133fd.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;据介绍，&lt;span&gt;ElevenLabs 的免费套餐为用户提供大约 10 分钟的音频生成时间。网页版与移动版应用之间共享信用额度，用户可以根据自身需求选择不同的模型，在成本与音质之间进行平衡。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;应用还接入了 ElevenLabs 最新的 v3 alpha 文本转语音模型，该模型允许用户通过标签控制语音的情感表达。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-0bd8cefff977a3158c5aa87f0aa53985792.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;相关阅读：&lt;a href="https://www.oschina.net/news/353936/eleven-v3-alpha" target="_blank"&gt;ElevenLabs 发布文本转语音模型 Eleven v3（Alpha 版）&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357151/elevenlabs-app</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357151/elevenlabs-app</guid>
      <pubDate>Sun, 11 May 2025 03:41:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>加州法院裁定使用版权内容训练 AI 合规</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;美国加州北区地方法院&lt;/span&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fstorage.courtlistener.com%2Frecap%2Fgov.uscourts.cand.434709%2Fgov.uscourts.cand.434709.231.0_2.pdf" target="_blank"&gt;裁定&lt;/a&gt;&lt;span style="color:#000000"&gt;，Anthropic 公司在未经作者许可的情况下，使用已出版的书籍训练其 AI 模型是合法的。这标志着法院首次认可 AI 公司的说法，即合理使用原则可以免除 AI 公司在使用受版权保护的材料训练大语言模型（LLM）时的过错。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="373" src="https://oscimg.oschina.net/oscnet/up-882fd79bf13f9f7781721a1a048d0fa9333.png" width="700" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;根据科技媒体 AppleInsider 的报道，许多创作者和艺术家长期以来都在为人工智能公司未经许可抓取其作品而苦恼。这些公司利用抓取的数据来训练大型语言模型（LLM），并将其商业化，然而内容的原创者却未能得到应有的补偿。对此，Andrea Bartz、Charles Graeber 和 Kirk Wallace Johnson 于 2024 年向法院提起诉讼，指控 Anthropic 公司侵犯其版权，使用了盗版材料。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;法官 William Alsup 在裁决中支持了双方的部分请求，但最终认为用于训练特定大语言模型的副本属于合理使用。这一裁定意味着 AI 公司在训练其模型时可以合法使用受版权保护的内容，而这也让众多艺术家、音乐家和作家感到失望。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;对于这些创作者来说，这项决定可能会使他们面临更大的商业风险，AI 模型的生成能力有可能进一步侵蚀他们的作品价值。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;Alsup 法官&lt;/span&gt;&lt;span style="background-color:#ffffff; color:#242424"&gt;在判决书中明确表示:"我们将就 Anthropic 公司用于创建中央图书馆的盗版书籍及其造成的损失进行审理。Anthropic 公司后来购买了之前从网上盗取的书籍，这并不能免除其盗窃责任，但这可能会影响法定赔偿的数额。"&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357149</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357149</guid>
      <pubDate>Sun, 11 May 2025 03:26:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Redis 是单线程模型？</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;span id="OSC_h1_1"&gt;&lt;/span&gt; 
&lt;h1&gt;一、背景&lt;/h1&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;使用过 Redis 的同学肯定都了解过一个说法，说 Redis 是单线程模型，那么实际情况是怎样的呢？&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;其实，我们常说 Redis 是单线程模型，&lt;strong&gt;是指 Redis 采用单线程的事件驱动模型，只有并且只会在一个主线程中执行 Redis 命令操作&lt;/strong&gt;，这意味着它在处理请求时不使用复杂的上下文切换或锁机制。尽管只是单线程的架构，但 Redis 通过非阻塞的 I/O 操作和高效的事件循环来处理大量的并发连接，性能仍然非常高。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;然而在 Redis4.0 开始也引入了一些后台线程执行异步淘汰、异步删除过期 key、异步执行大 key 删除等任务，然后，在 Redis6.0 中引入了多线程 IO 特性，将 Redis 单节点访问请求从 10W 提升到 20W。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;而在去年 Valkey 社区发布的 Valkey8.0 版本，在 I/O 线程系统上进行了重大升级，特别是异步 I/O 线程的引入，使主线程和 I/O 线程能够并行工作，可实现最大化服务吞吐量并减少瓶颈，使得 Valkey 单节点访问请求可以提升到 100W。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;&lt;strong&gt;那么在 Redis6.0 和 Valkey8.0 中多线程 IO 是怎么回事呢？是否改变了 Redis 原有单线程模型？&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;2024 年，Redis 商业支持公司 Redis Labs 宣布 Redis 核心代码的许可证从 BSD 变更为 RSALv2，明确禁止云厂商提供 Redis 托管服务，这一决定直接导致社区分裂。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;为维护开源自由，Linux 基金会联合多家科技公司（包括 AWS、Google、Cloud、Oracle 等）宣布支持 Valkey，作为 Redis 的替代分支。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Valkey8.0 系 Valkey 社区发布的首个主要大版本。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;最新消息，在 Redis 项目创始人 antirez 今年加入 Redis 商业公司 5 个月后，Redis 宣传从 Redis8 开始，Redis 项目重新开源。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;本篇文章主要介绍 Redis6.0 多线程 IO 特性。&lt;/span&gt;&lt;/p&gt; 
&lt;span id="OSC_h1_2"&gt;&lt;/span&gt; 
&lt;h1&gt;二、Redis6.0 多线程 IO 概述&lt;/h1&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;Redis6.0 引入多线程 IO，但多线程部分只是用来处理网络数据的读写和协议解析，&lt;strong&gt;执行命令仍然是单线程。默认是不开启的&lt;/strong&gt;，需要进程启动前开启配置，并且在运行期间无法通过&lt;strong&gt;&amp;nbsp;config set&amp;nbsp;&lt;/strong&gt;命令动态修改。&lt;/span&gt;&lt;/p&gt; 
&lt;span id="OSC_h2_3"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span style="color:#000000"&gt;参数与配置&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;多线程 IO 涉及下面两个配置参数：&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;# io-threads 4 &amp;nbsp;IO 线程数量
# io-threads-do-reads no &amp;nbsp;读数据及数据解析是否也用 IO 线程&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&amp;nbsp;io-threads&amp;nbsp;&lt;/strong&gt;表示 IO 线程数量，&lt;strong&gt;&amp;nbsp;io-threads&amp;nbsp;&lt;/strong&gt;设置为 1 时（代码中默认值），表示只使用主线程，不开启多线程 IO。因此，若要配置开启多线程 IO，需要设置&lt;strong&gt;&amp;nbsp;io-threads&amp;nbsp;&lt;/strong&gt;大于 1，但不可以超过最大值 128。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;但在默认情况下，Redis 只将多线程 IO 用于向客户端写数据，因为作者认为通常使用多线程执行读数据的操作帮助不是很大。如果需要使用多线程用于读数据和解析数据，则需要将参数&lt;strong&gt;&amp;nbsp;io-threads-do-reads&amp;nbsp;&lt;/strong&gt;设置为&lt;strong&gt;&amp;nbsp;yes&amp;nbsp;&lt;/strong&gt;。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;此两项配置&lt;strong&gt;参数在 Redis 运行期间无法通过&amp;nbsp;config set&amp;nbsp;命令修改，并且开启 SSL 时，不支持多线程 IO 特性。&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;若机器 CPU 将至少超过 4 核时，则建议开启，并且至少保留一个备用 CPU 核，使用超过 8 个线程可能并不会有多少帮助。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="OSC_h2_4"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span style="color:#000000"&gt;执行流程概述&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;Redis6.0 引入多线程 IO 后，读写数据执行流程如下所示：&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img src="https://static001.geekbang.org/infoq/f1/f1751e15cbe5116df1883b0155195ec5.png" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;流程简述&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;主线程负责接收建立连接请求，获取 socket 放入全局等待读处理队列。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;主线程处理完读事件之后，通过 RR（Round Robin）将这些连接分配给这些 IO 线程，也会分配给主线程自己。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;主线程先读取分配给自己的客户端数据，然后阻塞等待其他 IO 线程读取 socket 完毕。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;IO 线程将请求数据读取并解析完成（这里只是读数据和解析、并不执行）。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;主线程通过单线程的方式执行请求命令。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;主线程通过 RR（Round Robin）将回写客户端事件分配给这些 IO 线程，也会分配给主线程自己。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;主线程同样执行部分写数据到客户端，然后阻塞等待 IO 线程将数据回写 socket 完毕。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;设计特点&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;IO 线程要么同时在读 socket，要么同时在写，不会同时读和写。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;IO 线程只负责读写 socket 解析命令，不负责命令执行。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;主线程也会参与数据的读写。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;span id="OSC_h1_5"&gt;&lt;/span&gt; 
&lt;h1&gt;三、源码分析&lt;/h1&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;多线程 IO 相关源代码都在源文件 networking.c 中最下面。&lt;/span&gt;&lt;/p&gt; 
&lt;span id="OSC_h2_6"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span style="color:#000000"&gt;初始化&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;主线程在 main 函数中调用 InitServerLast 函数，InitServerLast 函数中调用&lt;strong&gt;initThreadedIO 函数&lt;/strong&gt;，在 initThreadedIO 函数中根据配置文件中的线程数量，创建对应数量的 IO 工作线程数量。&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;/* Initialize the data structures needed for threaded I/O. */
void&amp;nbsp;initThreadedIO(void)&amp;nbsp;{
&amp;nbsp; &amp;nbsp; io_threads_active =&amp;nbsp;0;&amp;nbsp;/* We start with threads not active. */
&amp;nbsp; &amp;nbsp;&amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;/* Don't spawn any thread if the user selected a single thread:
&amp;nbsp; &amp;nbsp; &amp;nbsp;* we'll handle I/O directly from the main thread. */
&amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(server.io_threads_num ==&amp;nbsp;1)&amp;nbsp;return;
&amp;nbsp; &amp;nbsp;&amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(server.io_threads_num &amp;gt; IO_THREADS_MAX_NUM) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;serverLog(LL_WARNING,"Fatal: too many I/O threads configured. "
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;"The maximum number is %d.", IO_THREADS_MAX_NUM);
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;exit(1);
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;/* Spawn and initialize the I/O threads. */
&amp;nbsp; &amp;nbsp;&amp;nbsp;for&amp;nbsp;(int&amp;nbsp;i =&amp;nbsp;0; i &amp;lt; server.io_threads_num; i++) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;/* Things we do for all the threads including the main thread. */
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; io_threads_list[i] =&amp;nbsp;listCreate();
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(i ==&amp;nbsp;0)&amp;nbsp;continue;&amp;nbsp;/* Thread 0 is the main thread. */
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;/* Things we do only for the additional threads. */
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;pthread_t&amp;nbsp;tid;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;pthread_mutex_init(&amp;amp;io_threads_mutex[i],NULL);
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; io_threads_pending[i] =&amp;nbsp;0;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;pthread_mutex_lock(&amp;amp;io_threads_mutex[i]);&amp;nbsp;/* Thread will be stopped. */
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(pthread_create(&amp;amp;tid,NULL,IOThreadMain,(void*)(long)i) !=&amp;nbsp;0) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;serverLog(LL_WARNING,"Fatal: Can't initialize IO thread.");
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;exit(1);
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; io_threads[i] = tid;
&amp;nbsp; &amp;nbsp; }
}&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;如果&lt;strong&gt;&amp;nbsp;io_threads_num&amp;nbsp;&lt;/strong&gt;的数量为 1，则只运行主线程，&lt;strong&gt;&amp;nbsp;io_threads_num&amp;nbsp;&lt;/strong&gt;的 IO 线程数量不允许超过 128。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;序号为 0 的线程是主线程，因此实际的工作线程数目是 io-threads - 1。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;初始化流程&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;为包括主线程在内的每个线程分配 list 列表，用于后续保存待处理的客户端。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;为主线程以外的其他 IO 线程初始化互斥对象 mutex，但是立即调用 pthread_mutex_lock 占有互斥量，将 io_threads_pending[i]设置为 0，接着创建对应的 IO 工作线程。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;占用互斥量是为了创建 IO 工作线程后，可暂时等待后续启动 IO 线程的工作，因为 IOThreadMain 函数在 io_threads_pending[id] == 0 时也调用了获取 mutex，所以此时无法继续向下运行，等待启动。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;在 startThreadedIO 函数中会释放 mutex 来启动 IO 线程工作。何时调用 startThreadedIO 打开多线程 IO，具体见下文的「多线程 IO 动态暂停与开启」。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;IO 线程主函数&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;IO 线程主函数代码如下所示：&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;void&amp;nbsp;*IOThreadMain(void&amp;nbsp;*myid)&amp;nbsp;{
&amp;nbsp; &amp;nbsp;&amp;nbsp;/* The ID is the thread number (from 0 to server.iothreads_num-1), and is
&amp;nbsp; &amp;nbsp; &amp;nbsp;* used by the thread to just manipulate a single sub-array of clients. */
&amp;nbsp; &amp;nbsp;&amp;nbsp;long&amp;nbsp;id = (unsigned&amp;nbsp;long)myid;
&amp;nbsp; &amp;nbsp;&amp;nbsp;char&amp;nbsp;thdname[16];
&amp;nbsp; &amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;snprintf(thdname,&amp;nbsp;sizeof(thdname),&amp;nbsp;"io_thd_%ld", id);
&amp;nbsp; &amp;nbsp;&amp;nbsp;redis_set_thread_title(thdname);
&amp;nbsp; &amp;nbsp;&amp;nbsp;redisSetCpuAffinity(server.server_cpulist);
&amp;nbsp; &amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;while(1) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;/* Wait for start */
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;for&amp;nbsp;(int&amp;nbsp;j =&amp;nbsp;0; j &amp;lt;&amp;nbsp;1000000; j++) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(io_threads_pending[id] !=&amp;nbsp;0)&amp;nbsp;break;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;/* Give the main thread a chance to stop this thread. */
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(io_threads_pending[id] ==&amp;nbsp;0) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;pthread_mutex_lock(&amp;amp;io_threads_mutex[id]);
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;pthread_mutex_unlock(&amp;amp;io_threads_mutex[id]);
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;continue;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;serverAssert(io_threads_pending[id] !=&amp;nbsp;0);
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(tio_debug)&amp;nbsp;printf("[%ld] %d to handle\n", id, (int)listLength(io_threads_list[id]));
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;/* Process: note that the main thread will never touch our list
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;* before we drop the pending count to 0. */
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; listIter li;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; listNode *ln;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;listRewind(io_threads_list[id],&amp;amp;li);
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;while((ln =&amp;nbsp;listNext(&amp;amp;li))) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; client *c =&amp;nbsp;listNodeValue(ln);
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(io_threads_op == IO_THREADS_OP_WRITE) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;writeToClient(c,0);
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; }&amp;nbsp;else&amp;nbsp;if&amp;nbsp;(io_threads_op == IO_THREADS_OP_READ) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;readQueryFromClient(c-&amp;gt;conn);
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; }&amp;nbsp;else&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;serverPanic("io_threads_op value is unknown");
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;listEmpty(io_threads_list[id]);
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; io_threads_pending[id] =&amp;nbsp;0;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(tio_debug)&amp;nbsp;printf("[%ld] Done\n", id);
&amp;nbsp; &amp;nbsp; }
}&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;从 IO 线程主函数逻辑可以看到：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;如果 IO 线程等待处理任务数量为 0，则 IO 线程一直在空循环，因此后面主线程给 IO 线程分发任务后，需要设置 IO 线程待处理任务数&lt;strong&gt;&amp;nbsp;io_threads_pending[id]&amp;nbsp;&lt;/strong&gt;，才会触发 IO 线程工作。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;如果 IO 线程等待处理任务数量为 0，并且未获取到 mutex 锁，则会等待获取锁，暂停运行，由于主线程在创建 IO 线程之前先获取了锁，因此 IO 线程刚启动时是暂停运行状态，需要等待主线程释放锁，启动 IO 线程。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;IO 线程待处理任务数为 0 时，获取到锁并再次释放锁，是为了让主线程可以暂停 IO 线程。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;只有 io_threads_pending[id]不为 0 时，则继续向下执行操作，根据 io_threads_op 决定是读客户端还是写客户端，从这里也可以看出 IO 线程要么&lt;strong&gt;同时读&lt;/strong&gt;，要么&lt;strong&gt;同时写&lt;/strong&gt;。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="OSC_h2_7"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span style="color:#000000"&gt;读数据流程&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;主线程将待读数据客户端加入队列&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;当客户端连接有读事件时，会触发调用 readQueryFromClient 函数，在该函数中会调用 postponeClientRead。&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;void&amp;nbsp;readQueryFromClient(connection *conn) {
&amp;nbsp; &amp;nbsp; client *c = connGetPrivateData(conn);
&amp;nbsp; &amp;nbsp;&amp;nbsp;int&amp;nbsp;nread, readlen;
&amp;nbsp; &amp;nbsp; size_t qblen;
&amp;nbsp; &amp;nbsp;&amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;/* Check if we want to read from the client later when exiting from
&amp;nbsp; &amp;nbsp; &amp;nbsp;* the event loop. This is the case if threaded I/O is enabled. */
&amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(postponeClientRead(c))&amp;nbsp;return;
&amp;nbsp; &amp;nbsp; ......以下省略
}


/* Return 1 if we want to handle the client read later using threaded I/O.
&amp;nbsp;* This is called by the readable handler of the event loop.
&amp;nbsp;* As a side effect of calling this function the client is put in the
&amp;nbsp;* pending read clients and flagged as such. */
int&amp;nbsp;postponeClientRead(client *c) {
&amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(io_threads_active &amp;amp;&amp;amp;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; server.io_threads_do_reads &amp;amp;&amp;amp;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; !ProcessingEventsWhileBlocked &amp;amp;&amp;amp;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; !(c-&amp;gt;flags &amp;amp; (CLIENT_MASTER|CLIENT_SLAVE|CLIENT_PENDING_READ)))
&amp;nbsp; &amp;nbsp; {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; c-&amp;gt;flags |=&amp;nbsp;CLIENT_PENDING_READ;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; listAddNodeHead(server.clients_pending_read,c);
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;return&amp;nbsp;1;
&amp;nbsp; &amp;nbsp; }&amp;nbsp;else&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;return&amp;nbsp;0;
&amp;nbsp; &amp;nbsp; }
}&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;如果开启多线程，并且开启多线程读（io_threads_do_reads 为 yes），则将客户端标记为 CLIENT_PENDING_READ，并且加入 clients_pending_read 列表。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;然后 readQueryFromClient 函数中就立即返回，主线程没有执行从客户端连接中读取的数据相关逻辑，读取了客户端数据行为等待后续各个 IO 线程执行。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;主线程分发并阻塞等待&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;主线程在 beforeSleep 函数中会调用 handleClientsWithPendingReadsUsingThreads 函数。&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;/* When threaded I/O is also enabled for the reading + parsing side, the
&amp;nbsp;* readable handler will just put normal clients into a queue of clients to
&amp;nbsp;* process (instead of serving them synchronously). This function runs
&amp;nbsp;* the queue using the I/O threads, and process them in order to accumulate
&amp;nbsp;* the reads in the buffers, and also parse the first command available
&amp;nbsp;* rendering it in the client structures. */
int&amp;nbsp;handleClientsWithPendingReadsUsingThreads(void)&amp;nbsp;{
&amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(!io_threads_active || !server.io_threads_do_reads)&amp;nbsp;return&amp;nbsp;0;
&amp;nbsp; &amp;nbsp;&amp;nbsp;int&amp;nbsp;processed =&amp;nbsp;listLength(server.clients_pending_read);
&amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(processed ==&amp;nbsp;0)&amp;nbsp;return&amp;nbsp;0;
&amp;nbsp; &amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(tio_debug)&amp;nbsp;printf("%d TOTAL READ pending clients\n", processed);
&amp;nbsp; &amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;/* Distribute the clients across N different lists. */
&amp;nbsp; &amp;nbsp; listIter li;
&amp;nbsp; &amp;nbsp; listNode *ln;
&amp;nbsp; &amp;nbsp;&amp;nbsp;listRewind(server.clients_pending_read,&amp;amp;li);
&amp;nbsp; &amp;nbsp;&amp;nbsp;int&amp;nbsp;item_id =&amp;nbsp;0;
&amp;nbsp; &amp;nbsp;&amp;nbsp;while((ln =&amp;nbsp;listNext(&amp;amp;li))) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; client *c =&amp;nbsp;listNodeValue(ln);
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;int&amp;nbsp;target_id = item_id % server.io_threads_num;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;listAddNodeTail(io_threads_list[target_id],c);
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; item_id++;
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp;&amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;/* Give the start condition to the waiting threads, by setting the
&amp;nbsp; &amp;nbsp; &amp;nbsp;* start condition atomic var. */
&amp;nbsp; &amp;nbsp; io_threads_op = IO_THREADS_OP_READ;
&amp;nbsp; &amp;nbsp;&amp;nbsp;for&amp;nbsp;(int&amp;nbsp;j =&amp;nbsp;1; j &amp;lt; server.io_threads_num; j++) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;int&amp;nbsp;count =&amp;nbsp;listLength(io_threads_list[j]);
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; io_threads_pending[j] = count;
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;/* Also use the main thread to process a slice of clients. */
&amp;nbsp; &amp;nbsp;&amp;nbsp;listRewind(io_threads_list[0],&amp;amp;li);
&amp;nbsp; &amp;nbsp;&amp;nbsp;while((ln =&amp;nbsp;listNext(&amp;amp;li))) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; client *c =&amp;nbsp;listNodeValue(ln);
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;readQueryFromClient(c-&amp;gt;conn);
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp;&amp;nbsp;listEmpty(io_threads_list[0]);
&amp;nbsp; &amp;nbsp;&amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;/* Wait for all the other threads to end their work. */
&amp;nbsp; &amp;nbsp;&amp;nbsp;while(1) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;unsigned&amp;nbsp;long&amp;nbsp;pending =&amp;nbsp;0;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;for&amp;nbsp;(int&amp;nbsp;j =&amp;nbsp;1; j &amp;lt; server.io_threads_num; j++)
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; pending += io_threads_pending[j];
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(pending ==&amp;nbsp;0)&amp;nbsp;break;
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(tio_debug)&amp;nbsp;printf("I/O READ All threads finshed\n");
&amp;nbsp; &amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;/* Run the list of clients again to process the new buffers. */
&amp;nbsp; &amp;nbsp;&amp;nbsp;while(listLength(server.clients_pending_read)) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; ln =&amp;nbsp;listFirst(server.clients_pending_read);
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; client *c =&amp;nbsp;listNodeValue(ln);
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; c-&amp;gt;flags &amp;amp;= ~CLIENT_PENDING_READ;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;listDelNode(server.clients_pending_read,ln);
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(c-&amp;gt;flags &amp;amp; CLIENT_PENDING_COMMAND) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; c-&amp;gt;flags &amp;amp;= ~CLIENT_PENDING_COMMAND;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(processCommandAndResetClient(c) == C_ERR) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;/* If the client is no longer valid, we avoid
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;* processing the client later. So we just go
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;* to the next. */
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;continue;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;processInputBuffer(c);
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp;&amp;nbsp;return&amp;nbsp;processed;
}&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;先检查是否开启多线程，以及是否开启多线程读数据（io_threads_do_reads），未开启直接返回。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;检查队列 clients_pending_read 长度，为 0 直接返回，说明没有待读事件。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;遍历 clients_pending_read 队列，通过 RR 算法，将队列中的客户端循环分配给各个 IO 线程，包括主线程本身。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;设置 io_threads_op = IO_THREADS_OP_READ，并且将 io_threads_pending 数组中各个位置值设置为对应各个 IO 线程分配到的客户端数量，如上面介绍，目的是为了使 IO 线程工作。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;主线程开始读取客户端数据，因为主线程也分配了任务。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;主线程阻塞等待，直到所有的 IO 线程都完成读数据工作。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;主线程执行命令。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;IO 线程读数据&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;在 IO 线程主函数中，如果&lt;strong&gt;&amp;nbsp;io_threads_op == IO_THREADS_OP_READ&amp;nbsp;，&lt;/strong&gt;则调用 readQueryFromClient 从网络中读取数据。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;&lt;strong&gt;IO 线程读取数据后，不会执行命令。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;在 readQueryFromClient 函数中，最后会执行 processInputBuffer 函数，在 processInputBuffe 函数中，如 IO 线程检查到客户端设置了 CLIENT_PENDING_READ 标志，则不执行命令，直接返回。&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; ......省略
/* If we are in the context of an I/O thread, we can't really
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;* execute the command here. All we can do is to flag the client
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;* as one that needs to process the command. */
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(c-&amp;gt;flags &amp;amp;&amp;nbsp;CLIENT_PENDING_READ) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; c-&amp;gt;flags |=&amp;nbsp;CLIENT_PENDING_COMMAND;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;break;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; ...... 省略&lt;/code&gt;&lt;/pre&gt; 
&lt;span id="OSC_h2_8"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span style="color:#000000"&gt;写数据流程&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;命令处理完成后，依次调用：&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;addReply--&amp;gt;prepareClientToWrite--&amp;gt;clientInstallWriteHandler，将待写客户端加入队列 clients_pending_write。&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;void&amp;nbsp;clientInstallWriteHandler(client *c) {
&amp;nbsp; &amp;nbsp;&amp;nbsp;/* Schedule the client to write the output buffers to the socket only
&amp;nbsp; &amp;nbsp; &amp;nbsp;* if not already done and, for slaves, if the slave can actually receive
&amp;nbsp; &amp;nbsp; &amp;nbsp;* writes at this stage. */
&amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(!(c-&amp;gt;flags &amp;amp;&amp;nbsp;CLIENT_PENDING_WRITE) &amp;amp;&amp;amp;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; (c-&amp;gt;replstate ==&amp;nbsp;REPL_STATE_NONE&amp;nbsp;||
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;(c-&amp;gt;replstate ==&amp;nbsp;SLAVE_STATE_ONLINE&amp;nbsp;&amp;amp;&amp;amp; !c-&amp;gt;repl_put_online_on_ack)))
&amp;nbsp; &amp;nbsp; {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;/* Here instead of installing the write handler, we just flag the
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;* client and put it into a list of clients that have something
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;* to write to the socket. This way before re-entering the event
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;* loop, we can try to directly write to the client sockets avoiding
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;* a system call. We'll only really install the write handler if
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;* we'll not be able to write the whole reply at once. */
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; c-&amp;gt;flags |=&amp;nbsp;CLIENT_PENDING_WRITE;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;listAddNodeHead(server.clients_pending_write,c);
&amp;nbsp; &amp;nbsp; }
}&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;在 beforeSleep 函数中调用 handleClientsWithPendingWritesUsingThreads。&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;int&amp;nbsp;handleClientsWithPendingWritesUsingThreads(void)&amp;nbsp;{
&amp;nbsp; &amp;nbsp;&amp;nbsp;int&amp;nbsp;processed =&amp;nbsp;listLength(server.clients_pending_write);
&amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(processed ==&amp;nbsp;0)&amp;nbsp;return&amp;nbsp;0;&amp;nbsp;/* Return ASAP if there are no clients. */
&amp;nbsp; &amp;nbsp;&amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;/* If I/O threads are disabled or we have few clients to serve, don't
&amp;nbsp; &amp;nbsp; &amp;nbsp;* use I/O threads, but thejboring synchronous code. */
&amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(server.io_threads_num ==&amp;nbsp;1&amp;nbsp;||&amp;nbsp;stopThreadedIOIfNeeded()) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;return&amp;nbsp;handleClientsWithPendingWrites();
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp;&amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;/* Start threads if needed. */
&amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(!io_threads_active)&amp;nbsp;startThreadedIO();
&amp;nbsp; &amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(tio_debug)&amp;nbsp;printf("%d TOTAL WRITE pending clients\n", processed);
&amp;nbsp; &amp;nbsp;&amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;/* Distribute the clients across N different lists. */
&amp;nbsp; &amp;nbsp; listIter li;
&amp;nbsp; &amp;nbsp; listNode *ln;
&amp;nbsp; &amp;nbsp;&amp;nbsp;listRewind(server.clients_pending_write,&amp;amp;li);
&amp;nbsp; &amp;nbsp;&amp;nbsp;int&amp;nbsp;item_id =&amp;nbsp;0;
&amp;nbsp; &amp;nbsp;&amp;nbsp;while((ln =&amp;nbsp;listNext(&amp;amp;li))) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; client *c =&amp;nbsp;listNodeValue(ln);
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; c-&amp;gt;flags &amp;amp;= ~CLIENT_PENDING_WRITE;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;int&amp;nbsp;target_id = item_id % server.io_threads_num;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;listAddNodeTail(io_threads_list[target_id],c);
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; item_id++;
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;/* Give the start condition to the waiting threads, by setting the
&amp;nbsp; &amp;nbsp; &amp;nbsp;* start condition atomic var. */
&amp;nbsp; &amp;nbsp; io_threads_op = IO_THREADS_OP_WRITE;
&amp;nbsp; &amp;nbsp;&amp;nbsp;for&amp;nbsp;(int&amp;nbsp;j =&amp;nbsp;1; j &amp;lt; server.io_threads_num; j++) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;int&amp;nbsp;count =&amp;nbsp;listLength(io_threads_list[j]);
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; io_threads_pending[j] = count;
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp;&amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;/* Also use the main thread to process a slice of clients. */
&amp;nbsp; &amp;nbsp;&amp;nbsp;listRewind(io_threads_list[0],&amp;amp;li);
&amp;nbsp; &amp;nbsp;&amp;nbsp;while((ln =&amp;nbsp;listNext(&amp;amp;li))) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; client *c =&amp;nbsp;listNodeValue(ln);
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;writeToClient(c,0);
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp;&amp;nbsp;listEmpty(io_threads_list[0]);
&amp;nbsp; &amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;/* Wait for all the other threads to end their work. */
&amp;nbsp; &amp;nbsp;&amp;nbsp;while(1) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;unsigned&amp;nbsp;long&amp;nbsp;pending =&amp;nbsp;0;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;for&amp;nbsp;(int&amp;nbsp;j =&amp;nbsp;1; j &amp;lt; server.io_threads_num; j++)
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; pending += io_threads_pending[j];
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(pending ==&amp;nbsp;0)&amp;nbsp;break;
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(tio_debug)&amp;nbsp;printf("I/O WRITE All threads finshed\n");
&amp;nbsp; &amp;nbsp;&amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;/* Run the list of clients again to install the write handler where
&amp;nbsp; &amp;nbsp; &amp;nbsp;* needed. */
&amp;nbsp; &amp;nbsp;&amp;nbsp;listRewind(server.clients_pending_write,&amp;amp;li);
&amp;nbsp; &amp;nbsp;&amp;nbsp;while((ln =&amp;nbsp;listNext(&amp;amp;li))) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; client *c =&amp;nbsp;listNodeValue(ln);
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;/* Install the write handler if there are pending writes in some
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;* of the clients. */
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(clientHasPendingReplies(c) &amp;amp;&amp;amp;
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;connSetWriteHandler(c-&amp;gt;conn, sendReplyToClient) == AE_ERR)
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;freeClientAsync(c);
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; &amp;nbsp;&amp;nbsp;listEmpty(server.clients_pending_write);
&amp;nbsp; &amp;nbsp;&amp;nbsp;return&amp;nbsp;processed;
}&lt;/code&gt;&lt;/pre&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;判断 clients_pending_write 队列的长度，如果为 0 则直接返回。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;判断是否开启了多线程，若只有很少的客户端需要写，则不使用多线程 IO，直接在主线程完成写操作。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;如果使用多线程 IO 来完成写数据，则需要判断是否先开启多线程 IO（因为会动态开启与暂停）。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;遍历 clients_pending_write 队列，通过 RR 算法，循环将所有客户端分配给各个 IO 线程，包括主线程自身。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;设置 io_threads_op = IO_THREADS_OP_WRITE，并且将 io_threads_pending 数组中各个位置值设置为对应的各个 IO 线程分配到的客户端数量，目的是为了使 IO 线程工作。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;主线程开始写客户端数据，因为主线程也分配了任务，写完清空任务队列。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;阻塞等待，直到所有 IO 线程完成写数据工作。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;再次遍历所有客户端，如果有需要，为客户端在事件循环上安装写句柄函数，等待事件回调。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;span id="OSC_h2_9"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span style="color:#000000"&gt;多线程 IO 动态暂停与开启&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;从上面的写数据的流程中可以看到，在 Redis 运行过程中多线程 IO 是会动态暂停与开启的。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;在上面的写数据流程中，先调用 stopThreadedIOIfNeeded 函数判断是否需要暂停多线程 IO，&lt;strong&gt;当等待写的客户端数量低于线程数的 2 倍时，会暂停多线程 IO，&lt;/strong&gt;否则就会打开多线程。&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;int&amp;nbsp;stopThreadedIOIfNeeded(void)&amp;nbsp;{
&amp;nbsp; &amp;nbsp;&amp;nbsp;int&amp;nbsp;pending = listLength(server.clients_pending_write);
&amp;nbsp; &amp;nbsp;&amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;/* Return ASAP if IO threads are disabled (single threaded mode). */
&amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(server.io_threads_num ==&amp;nbsp;1)&amp;nbsp;return&amp;nbsp;1;
&amp;nbsp; &amp;nbsp;
&amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(pending &amp;lt; (server.io_threads_num*2)) {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;if&amp;nbsp;(io_threads_active) stopThreadedIO();
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;return&amp;nbsp;1;
&amp;nbsp; &amp;nbsp; }&amp;nbsp;else&amp;nbsp;{
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;return&amp;nbsp;0;
&amp;nbsp; &amp;nbsp; }
}&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;在写数据流程 handleClientsWithPendingWritesUsingThreads 函数中，stopThreadedIOIfNeeded 返回 0 的话，就会执行下面的 startThreadedIO 函数，开启多线程 IO。&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;void&amp;nbsp;startThreadedIO(void) {
&amp;nbsp; &amp;nbsp;&amp;nbsp;serverAssert(server.io_threads_active&amp;nbsp;==&amp;nbsp;0);
&amp;nbsp; &amp;nbsp;&amp;nbsp;for&amp;nbsp;(int j =&amp;nbsp;1; j &amp;lt; server.io_threads_num; j++)
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;pthread_mutex_unlock(&amp;amp;io_threads_mutex[j]);
&amp;nbsp; &amp;nbsp; server.io_threads_active&amp;nbsp;=&amp;nbsp;1;
}


void&amp;nbsp;stopThreadedIO(void) {
&amp;nbsp; &amp;nbsp;&amp;nbsp;/* We may have still clients with pending reads when this function
&amp;nbsp; &amp;nbsp; &amp;nbsp;* is called: handle them before stopping the threads. */
&amp;nbsp; &amp;nbsp;&amp;nbsp;handleClientsWithPendingReadsUsingThreads();
&amp;nbsp; &amp;nbsp;&amp;nbsp;serverAssert(server.io_threads_active&amp;nbsp;==&amp;nbsp;1);
&amp;nbsp; &amp;nbsp;&amp;nbsp;for&amp;nbsp;(int j =&amp;nbsp;1; j &amp;lt; server.io_threads_num; j++)
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;pthread_mutex_lock(&amp;amp;io_threads_mutex[j]);
&amp;nbsp; &amp;nbsp; server.io_threads_active&amp;nbsp;=&amp;nbsp;0;
}&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;从上面的代码中可以看出：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;开启多线程 IO 是通过释放 mutex 锁来让 IO 线程开始执行读数据或者写数据动作。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;暂停多线程 IO 则是通过加锁来让 IO 线程暂时不执行读数据或者写数据动作，此处加锁后，IO 线程主函数由于无法获取到锁，因此会暂时阻塞。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="OSC_h1_10"&gt;&lt;/span&gt; 
&lt;h1&gt;四、性能对比&lt;/h1&gt; 
&lt;span id="OSC_h2_11"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span style="color:#000000"&gt;测试环境&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;两台物理机配置：CentOS Linux release 7.3.1611(Core) ，12 核 CPU1.5GHz，256G 内存（free 128G）。&lt;/span&gt;&lt;/p&gt; 
&lt;span id="OSC_h2_12"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span style="color:#000000"&gt;Redis 版本&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;使用 Redis6.0.6，多线程 IO 模式使用线程数量为 4，即&lt;strong&gt;&amp;nbsp;io-threads 4&amp;nbsp;&lt;/strong&gt;，参数&lt;strong&gt;&amp;nbsp;io-threads-do-reads&amp;nbsp;&lt;/strong&gt;分别设置为&lt;strong&gt;&amp;nbsp;no&amp;nbsp;&lt;/strong&gt;和&lt;strong&gt;&amp;nbsp;yes&amp;nbsp;&lt;/strong&gt;，进行对比测试。&lt;/span&gt;&lt;/p&gt; 
&lt;span id="OSC_h2_13"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span style="color:#000000"&gt;压测命令&lt;/span&gt;&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-auto"&gt;redis-benchmark -h 172.xx.xx.xx -t set,get -n 1000000 -r 100000000 --threads ${threadsize} -d ${datasize} -c ${clientsize}


单线程 threadsize 为 1，多线程 threadsize 为 4
datasize 为 value 大小，分别设置为 128/512/1024
clientsize 为客户端数量，分别设置为 256/2000
如：./redis-benchmark -h 172.xx.xx.xx -t set,get -n 1000000 -r 100000000 --threads 4 -d 1024 -c 256&lt;/code&gt;&lt;/pre&gt; 
&lt;span id="OSC_h2_14"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span style="color:#000000"&gt;统计结果&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;当&lt;strong&gt;&amp;nbsp;io-threads-do-reads&amp;nbsp;&lt;/strong&gt;为&lt;strong&gt;&amp;nbsp;no&amp;nbsp;&lt;/strong&gt;时，统计图表如下所示（c 2000 表示客户端数量为 2000）。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="300" src="https://oscimg.oschina.net/oscnet/up-604470825e1668f75a0927b74d09cdd58e1.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;当&lt;strong&gt;&amp;nbsp;io-threads-do-reads&amp;nbsp;&lt;/strong&gt;为&lt;strong&gt;&amp;nbsp;yes&amp;nbsp;&lt;/strong&gt;时，统计图表如下所示（c 256 表示客户端数量为 256）。&lt;/span&gt;&lt;/p&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;img height="303" src="https://oscimg.oschina.net/oscnet/up-c53c725b46c9562ff0702a925aec3b10067.png" width="500" referrerpolicy="no-referrer"&gt; 
&lt;/div&gt; 
&lt;span id="OSC_h2_15"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;span style="color:#000000"&gt;结论&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;使用 redis-benchmark 做 Redis6 单线程和多线程简单 SET/GET 命令性能测试：&lt;/span&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;从上面可以看到 GET/SET 命令在设置 4 个 IO 线程时，QPS 相比于大部分情况下的单线程，性能几乎是翻倍了。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;连接数越多，多线程优势越明显。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;value 值越小，多线程优势越明显。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;使用多线程读命令比写命令优势更加明显，当 value 越大，写命令越发没有明显的优势。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;参数&lt;strong&gt;&amp;nbsp;io-threads-do-reads&amp;nbsp;&lt;/strong&gt;为 yes，性能有微弱的优势，不是很明显。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;总体来说，以上结果基本符合预期，结果仅作参考。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;span id="OSC_h1_16"&gt;&lt;/span&gt; 
&lt;h1&gt;五、6.0 多线程 IO 不足&lt;/h1&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;尽管引入多线程 IO 大幅提升了 Redis 性能，但是 Redis6.0 的多线程 IO 仍然存在一些不足：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;CPU 核心利用率不足：当前主线程仍负责大部分的 IO 相关任务，并且当主线程处理客户端的命令时，IO 线程会空闲相当长的时间，同时值得注意的是，主线程在执行 IO 相关任务期间，性能受到最慢 IO 线程速度的限制。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;IO 线程执行的任务有限：目前，由于主线程同步等待 IO 线程，线程仅执行读取解析和写入操作。如果线程可以异步工作，我们可以将更多工作卸载到 IO 线程上，从而减少主线程的负载。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;不支持带有 TLS 的 IO 线程。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;最新的 Valkey8.0 版本中，通过引入异步 IO 线程，将更多的工作转移到 IO 线程执行，同时通过&lt;strong&gt;批量预读取内存数据&lt;/strong&gt;减少内存访问延迟，大幅提高 Valkey 单节点访问 QPS，单个实例每秒可处理 100 万个请求。我们后续再详细介绍 Valkey8.0 异步 IO 特性。&lt;/span&gt;&lt;/p&gt; 
&lt;span id="OSC_h1_17"&gt;&lt;/span&gt; 
&lt;h1&gt;六、总结&lt;/h1&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;Redis6.0 引入多线程 IO，但多线程部分只是用来处理网络数据的读写和协议解析，&lt;strong&gt;执行命令仍然是单线程&lt;/strong&gt;。通过开启多线程 IO，并设置合适的 CPU 数量，可以提升访问请求一倍以上。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;Redis6.0 多线程 IO 仍然存在一些不足，没有充分利用 CPU 核心，在最新的 Valkey8.0 版本中，引入异步 IO 将进一步大幅提升 Valkey 性能。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;往期回顾&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#586c90"&gt;1.&lt;/span&gt;得物社区活动：组件化的演进与实践&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#586c90"&gt;2.&lt;/span&gt;从 CPU 冒烟到丝滑体验：算法 SRE 性能优化实战全揭秘｜得物技术&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#586c90"&gt;3.&lt;/span&gt;CSS 闯关指南：从手写地狱到「类」积木之旅｜得物技术&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#586c90"&gt;4.&lt;/span&gt;以细节诠释专业，用成长定义价值——对话@孟同学 ｜得物技术&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#586c90"&gt;5.&lt;/span&gt;大语言模型的训练后量化算法综述 | 得物技术&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;文 / 竹径&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;关注得物技术，每周更新技术干货&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;要是觉得文章对你有帮助的话，欢迎评论转发点赞～&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#3e3e3e"&gt;未经得物技术许可严禁转载，否则依法追究法律责任。&lt;/span&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/5783135/blog/18628004</link>
      <guid isPermaLink="false">https://my.oschina.net/u/5783135/blog/18628004</guid>
      <pubDate>Sun, 11 May 2025 03:15:00 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>Ubuntu 默认主题的「回收站」应用图标将更新</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Ubuntu 贡献者&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fubuntu%2Fyaru%2Fissues%2F1170" target="_blank"&gt;目前正在构思&lt;/a&gt;一个新的垃圾桶图标，该图标最早可能在 10 月份 Ubuntu 25.10 发布时出现在 Dock 栏中。&lt;/p&gt; 
&lt;p&gt;关于 Ubuntu 垃圾桶图标外观的讨论在 2019 年持续进行，直到同年 3 月结束。今年 5 月，该讨论再次重启，并定期更新图标建议。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-5b02880cb80a289c29cbfec182c0689f9b6.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;现有垃圾桶图标的主要缺陷在于，没有回收图标，它看起来更像一个信箱，而不是一个垃圾桶。当垃圾桶里有文件时，里面的文件看起来相当整齐，就像信件一样，这更让人觉得它是一个邮箱，而不是垃圾桶。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-cd86113e7df0a39fd3981f4d372b785a065.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;垃圾桶图标位于左侧 Dock 底部，看起来像一个信箱&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;目前建议的设计方案是顶部完全打开，以便更清楚地显示它是一个垃圾桶；当里面有文件时，会显示皱巴巴的纸张。设计师 ochi12 发布了多个图标版本，并将它们设置为不同的尺寸，以测试在不同用例下的效果。最新版本的图标在缩小尺寸后似乎依然能够正常显示。&lt;/p&gt; 
&lt;p&gt;目前，设计师仍在听取其他贡献者的反馈，因此，即使最终达成一致，目前做出的修改也不太可能最终在 Ubuntu 中实现。这一点也很重要，因为这些修改可能永远不会被达成一致，我们也就可能永远看不到它们真正实现。&lt;/p&gt; 
&lt;p&gt;值得注意的是，一些评论者表示他们喜欢现有的垃圾桶图标，希望它不要改变，这体现了垃圾桶图标本身的主观性。希望我们能够就设计达成共识，以便 Ubuntu 用户在升级到即将发布的 Ubuntu 版本时能够获得更新鲜的体验。&lt;/p&gt; 
&lt;p&gt;距离 10 月份 Ubuntu 25.10 发布还有很长一段时间，所以我们到那时就可以看到新的图标，但如果没有，也许我们会在 Ubuntu 26.04 LTS 中看到它。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357137</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357137</guid>
      <pubDate>Sun, 11 May 2025 02:40:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>开源 AI 助手平台 Cherry Studio 企业版开启公测</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;Cherry Studio &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FFHtJqZlULDhRw7fT1ALagA" target="_blank"&gt;宣布&lt;/a&gt;其企业版已开始公测，这是专为企业打造的私有化 AI 生产力平台。&lt;/p&gt; 
&lt;p style="color:#1f2329; margin-left:0; margin-right:0; text-align:start"&gt;&lt;span style="color:#1f2329"&gt;&lt;span&gt;下表展示了两个版本之间的定位与功能差异：&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;table cellspacing="0" style="-webkit-tap-highlight-color:transparent; -webkit-text-stroke-width:0px; background-color:#ffffff; border-collapse:collapse; border-color:#dee0e3; box-sizing:border-box !important; color:rgba(0, 0, 0, 0.9); display:table; font-family:&amp;quot;PingFang SC&amp;quot;,system-ui,-apple-system,&amp;quot;system-ui&amp;quot;,&amp;quot;Helvetica Neue&amp;quot;,&amp;quot;Hiragino Sans GB&amp;quot;,&amp;quot;Microsoft YaHei UI&amp;quot;,&amp;quot;Microsoft YaHei&amp;quot;,Arial,sans-serif; font-size:15px; font-style:normal; font-variant-caps:normal; font-variant-ligatures:normal; font-weight:400; letter-spacing:0.544px; margin:0px 0px 10px; max-width:626px !important; min-width:297px; orphans:2; outline:0px; overflow-wrap:break-word !important; padding:0px; text-align:justify; text-decoration-color:initial; text-decoration-style:initial; text-decoration-thickness:initial; text-transform:none; white-space:normal; widows:2; width:626px; word-spacing:0px"&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td style="border-color:#cccccc; border-style:solid; border-width:1px; white-space:normal"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;对比维度&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/td&gt; 
   &lt;td style="border-color:#cccccc; border-style:solid; border-width:1px; white-space:normal"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;社区版 （Community）&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/td&gt; 
   &lt;td style="border-color:#cccccc; border-style:solid; border-width:1px; white-space:normal"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;企业版 （Enterprise）&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="border-color:#cccccc; border-style:solid; border-width:1px; white-space:normal"&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;目标用户&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/td&gt; 
   &lt;td style="border-color:#cccccc; border-style:solid; border-width:1px; white-space:normal"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;个人开发者、AI 爱好者&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/td&gt; 
   &lt;td style="border-color:#cccccc; border-style:solid; border-width:1px; white-space:normal"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;中小型企业、大型企业内部团队、对数据安全有高要求的组织&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="border-color:#cccccc; border-style:solid; border-width:1px; white-space:normal"&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;开源策略&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/td&gt; 
   &lt;td style="border-color:#cccccc; border-style:solid; border-width:1px; white-space:normal"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;✅ Github 开源&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/td&gt; 
   &lt;td style="border-color:#cccccc; border-style:solid; border-width:1px; white-space:normal"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;⭕️ 针对伙伴客户端源码开放&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="border-color:#cccccc; border-style:solid; border-width:1px; white-space:normal"&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;商业模式&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/td&gt; 
   &lt;td style="border-color:#cccccc; border-style:solid; border-width:1px; white-space:normal"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;个人免费 / 商用授权&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/td&gt; 
   &lt;td style="border-color:#cccccc; border-style:solid; border-width:1px; white-space:normal"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;买断+可选服务费&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="border-color:#cccccc; border-style:solid; border-width:1px; white-space:normal"&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;核心差异&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/td&gt; 
   &lt;td style="border-color:#cccccc; border-style:solid; border-width:1px; white-space:normal"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;专注于个人生产力&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/td&gt; 
   &lt;td style="border-color:#cccccc; border-style:solid; border-width:1px; white-space:normal"&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;企业集中管理能力&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="border-color:#cccccc; border-style:solid; border-width:1px; white-space:normal"&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;部署方式&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/td&gt; 
   &lt;td style="border-color:#cccccc; border-style:solid; border-width:1px; white-space:normal"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;客户端应用&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/td&gt; 
   &lt;td style="border-color:#cccccc; border-style:solid; border-width:1px; white-space:normal"&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;客户端 + 服务端私有化部署&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="border-color:#cccccc; border-style:solid; border-width:1px; white-space:normal"&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;核心价值&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/td&gt; 
   &lt;td style="border-color:#cccccc; border-style:solid; border-width:1px; white-space:normal"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;强大的个人 AI 辅助工具&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/td&gt; 
   &lt;td style="border-color:#cccccc; border-style:solid; border-width:1px; white-space:normal"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;安全、可控、高效的&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.cherry-ai.com%2Fenterprise" target="_blank"&gt;https://www.cherry-ai.com/enterprise&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"&gt;点击查看企业版体验手册：&lt;/p&gt; 
 &lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"&gt;https://doc.weixin.qq.com/doc/w3_ASIAPQaBALgCNdQv1pcxUTJGhXLsX?scode=APkA7AeJABIVWchL1vASIAPQaBALg&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"&gt;Cherry Studio 是一款支持多个大语言模型（LLM）服务商的开源桌面客户端，兼容 Windows、Mac 和 Linux 系统。&lt;/p&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-23dab8c50bfcc8126ab84229b00dbc2115c.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357134</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357134</guid>
      <pubDate>Sun, 11 May 2025 02:22:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>xAI 正在为 Grok 开发高级文件编辑器</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;工程师 Nima Owji 在 X 平台&lt;u&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Fnima_owji%2Fstatus%2F1937146584493375900" target="_blank"&gt;发文称&lt;/a&gt;&lt;/u&gt;，马斯克旗下 xAI 公司正为 Grok 开发一款支持表格的高级文件编辑器。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;img height="1826" src="https://static.oschina.net/uploads/space/2025/0624/194050_x6IF_2720166.png" width="1940" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;科技媒体 TechCrunch&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftechcrunch.com%2F2025%2F06%2F23%2Fleak-reveals-grok-might-soon-edit-your-spreadsheets%2F" target="_blank"&gt; &lt;u&gt;认为&lt;/u&gt;&lt;/a&gt;&amp;nbsp;xAI 此举表明他们正采取措施，通过在生产力工具中整合 AI 辅助功能，与 OpenAI、谷歌和微软等巨头展开竞争。OpenAI 和微软已拥有类似工具，最为相似的是 Gemini Workspace for Sheets、Docs 和 Gmail，可以编辑文档和表格，并支持用户在查看或编辑文档时与 Gemini 对话。&lt;/p&gt; 
&lt;p&gt;目前尚不清楚 xAI 的编辑器除了表格之外还可能支持哪些类型的文件，也不清楚 xAI 是否计划构建一个完整的生产力套件，以与谷歌 Workspace 和微软 Microsoft 365 竞争。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357076/leak-reveals-grok-might-soon-edit-your-spreadsheets</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357076/leak-reveals-grok-might-soon-edit-your-spreadsheets</guid>
      <pubDate>Sat, 10 May 2025 11:44:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>为什么所有浏览器的的 User-Agent 字符串开头都是</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;你有没有注意过，几乎所有浏览器的 User-Agent 字符串开头都是「Mozilla/」？&lt;/p&gt; 
&lt;p&gt;无论是 Chrome、Safari、还是 IE、Edge，都有「Mozilla」的痕迹。其实，这一切都源自一场「浏览器伪装」的历史闹剧。&lt;/p&gt; 
&lt;p&gt;这事得从 90 年代说起，那时互联网刚起步，第一个流行浏览器叫 Mosaic。后来，有人造出一款更强的浏览器，号称「Mosaic Killer」，代号 Mozilla。&lt;/p&gt; 
&lt;p&gt;它上线后，不光能看图还能加载网页框架（frames），可谓是当年超前的黑科技。&lt;/p&gt; 
&lt;p&gt;很多网站为了兼容，只愿给「Mozilla」发完整版页面，其他浏览器只能看阉割版。&lt;/p&gt; 
&lt;p&gt;这就引发了一个问题：网站开始「嗅探」浏览器身份，也就是所谓的 User-Agent 识别。&lt;/p&gt; 
&lt;p&gt;后来，微软做了 Internet Explorer（IE），本来想正大光明竞争，但一看：网站只对 Mozilla 好，干脆就让自己也伪装成 Mozilla。&lt;/p&gt; 
&lt;p&gt;于是 IE 的 User-Agent 字符串成了这样：&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Mozilla/1.22 (compatible; MSIE 2.0; Windows 95)&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;就这样，IE 成功骗过了网站，用户数也提升了。&lt;/p&gt; 
&lt;p&gt;而这场「伪装游戏」一旦开始，就收不住了。&lt;/p&gt; 
&lt;p&gt;1、Firefox 自己引以为傲的 Gecko 渲染引擎，也以 Mozilla 自称：&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Mozilla/5.0 (...) Gecko/... Firefox/...&lt;/code&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;2、Linux 社区做的浏览器，用的是 KHTML 引擎，他们开始模仿 Gecko 写法：&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Mozilla/5.0 (...) (KHTML, like Gecko)&lt;/code&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;3、苹果搞了 Safari，用的是 WebKit，而 WebKit 是 KHTML 的一个分支：&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Mozilla/5.0 (...) AppleWebKit/... (KHTML, like Gecko) Safari/...&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;4、到了谷歌 Chrome 时代，它用的也是 WebKit，为了吃到 Safari 的待遇，其 User-Agent 变成了这种奇葩组合：&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Mozilla/5.0 (...) AppleWebKit/... (KHTML, like Gecko) Chrome/... Safari/...&lt;/code&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;到这时，每个浏览器都在 User-Agent 里堆满了「族谱」：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Chrome 假装是 Safari&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Safari 假装是 KHTML&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;KHTML 假装是 Gecko&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Gecko 假装是 Mozilla&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;而真正的 Mozilla，其实早就不在了。&lt;/p&gt; 
&lt;p&gt;最后结果就是：User-Agent 成了一串「你是谁并不重要，重要的是你要说自己是 Mozilla」的魔性自报家门。&lt;/p&gt; 
&lt;p&gt;也难怪现在的前端开发者一边调试一边吐槽：「我到底在给谁写页面啊？」&lt;/p&gt; 
&lt;p&gt;感兴趣的小伙伴可以点击原文：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwebaim.org%2Fblog%2Fuser-agent-string-history%2F" target="_blank"&gt;https://webaim.org/blog/user-agent-string-history/&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357071/user-agent-string-history</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357071/user-agent-string-history</guid>
      <pubDate>Sat, 10 May 2025 11:07:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>【直播预告】三步上手鸿蒙开发：工具・能力・进阶</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;p style="margin-left:0.0001pt; margin-right:0px; text-align:left"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;依托「一次开发、多端部署」的核心理念，HarmonyOS 的分布式能力正在革新万物互联时代的应用开发范式——从智能家居到移动办公，开发者可高效实现跨终端无缝协同。&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;然而，许多开发者仍面临以下问题：&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;对鸿蒙核心开放能力（如元服务、分布式技术、AI 能力）缺乏系统认知；&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;对鸿蒙专属开发工具（ArkUI、DevEco Studio）的操作不熟悉；&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;缺少从入门到进阶的完整学习路径，难以快速上手实战开发。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="margin-left:0.0001pt; margin-right:0px; text-align:left"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;为此，7 月 8 日晚，开源中国 OSCHINA 《数智漫谈》直播栏目聚焦「工具 · 能力 · 进阶」三大模块，邀请三位鸿蒙生态专家，通过场景化演示与案例拆解，帮助开发者高效掌握鸿蒙应用开发的核心技能，抓住万物互联时代的创新机遇。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="margin-left:0.0001pt; margin-right:0px; text-align:left"&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;直播主题：&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;三步上手鸿蒙开发：工具 · 能力 · 进阶&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="margin-left:0.0001pt; margin-right:0px; text-align:left"&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;平台：&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;视频号「OSC 开源社区」&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="margin-left:0.0001pt; margin-right:0px; text-align:left"&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;时间：&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;7 月 8 日（周二） 19:00-20:40&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="margin-left:0.0001pt; margin-right:0px; text-align:left"&gt;&lt;img height="495" src="https://oscimg.oschina.net/oscnet/up-0cec6a2cf81222917368850d2b811addc01.jpg" width="300" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h4_1"&gt;&lt;/span&gt; 
&lt;h4&gt;&lt;strong&gt;🔥 直播核心看点抢先揭秘：&lt;/strong&gt;&lt;/h4&gt; 
&lt;p style="margin-left:0.0001pt; margin-right:0px; text-align:left"&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;&lt;span&gt;&lt;span&gt;分享主题 1：解锁鸿蒙核心能力，打造跨端智能应用&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="margin-left:0.0001pt; margin-right:0px; text-align:left"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;&lt;span&gt;&lt;span&gt;鸿蒙操作系统以「分布式架构」为核心，打破设备边界，实现跨终端无缝协同与算力共享，通过元服务、多端统一开发、AI 等能力，重塑万物互联场景体验。本次演讲将解读其技术革新内核，并探讨在各领域的应用实践，揭示鸿蒙如何为生态融合与数字化转型提供新范式。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;&lt;span&gt;&lt;span&gt;分享主题 2：&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;DevEco Studio：从零到一搭建鸿蒙应用&lt;/strong&gt;&lt;br&gt; &amp;nbsp;本演讲以开发者视角系统性解析鸿蒙应用开发全流程：从 DevEco Studio 环境配置与真机调试技巧，到 ArkUI 声明式开发范式的核心实践（状态管理、组件化开发），并结合跨设备联调、卡片服务等典型场景，直击多端适配与调试中的高频问题，提供华为 HDE 总结的实战解决方案，助力开发者快速构建高质量鸿蒙应用。&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style="color:#000000"&gt;&lt;span&gt;&lt;span&gt;分享主题 3：&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;从学习到实战：鸿蒙开发者成长指南&lt;/strong&gt;&lt;br&gt; 聚焦鸿蒙开发者从入门到精进的成长路径，解析如何通过高效学习框架与实战经验，掌握分布式开发、多端协同等核心技术，跨越「单一设备」到「场景化创新」的鸿沟。内容涵盖开发工具链使用、典型场景案例拆解及生态机遇洞察，助力开发者在万物互联时代抢占技术先机，实现从技能提升到价值落地的闭环。&lt;/p&gt; 
&lt;span id="OSC_h4_2"&gt;&lt;/span&gt; 
&lt;h4&gt;&lt;strong&gt;👨‍💻 重磅嘉宾阵容：&lt;/strong&gt;&lt;/h4&gt; 
&lt;p style="margin-left:0.0001pt; margin-right:0px; text-align:left"&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;姚圣伟，&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;华为云 HCDE、鸿蒙应用认证开发者、微软 Insider Dev Tour China、.Net Conf China 讲师、中科院开源之夏优秀导师、升腾 CANN 训练营优秀开发者、腾讯腾源会开源摘星 100 人，天津敏捷社区核心组织者，中国 DevOps 社区理事会成员。现从事信创、电子政务、人工智能、云开发平台等领域的设计、研发工作。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;&lt;strong&gt;&lt;span&gt;张一弛，&lt;/span&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;华为开发者专家（HDE）、湖南长沙虚拟盒子鸿蒙架构师、鸿蒙兔习惯 APP 作者、欢友社交应用，出境元服务架构师。多年移动端开发经验，专注于 IM 领域，目前主要从事鸿蒙元服务相关工作。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="text-align:left"&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;&lt;strong&gt;&lt;span&gt;祝欣蓉，&lt;/span&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;上海杉达学院副教授，华为开发者专家（HDE），HarmonyOS 应用开发高级工程师、华为路由交换高级网络工程师、华为大数据高级工程师、华为 HDG 上海核心组成员，曾担任丹阳市委网信办网络安全顾问，主要研究方向：鸿蒙移动应用开发，OpenHarmony 软硬协同开发，企业级项目开发，曾主持横向课题 1 项，教育部产学合作协同育人项目 2 项，市级重点课程建设 1 门，校级重点课题 4 项，出版教材 3 本。曾任教课程：鸿蒙移动应用开发，HarmonyOS 软硬协同创新实践，Java Web 开发技术，数据挖掘技术与应用，组网技术，园区网络安全技术等课程。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;🚀 立即行动，开启你的鸿蒙开发进阶之旅！&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;📅 直播时间：2025 年 7 月 8 日 (周二) 19:00 - 20:40&lt;/strong&gt;&lt;br&gt; &lt;strong&gt;📍 直播平台：视频号搜索【OSC 开源社区】&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;👉 现在预约直播，开播不错过！&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="margin-left:0.0001pt; margin-right:0px; text-align:left"&gt;&lt;img height="660" src="https://oscimg.oschina.net/oscnet/up-a2420114f6a13a893712842053fb7fce6ef.jpg" width="400" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;p style="margin-left:0.0001pt; margin-right:0px; text-align:left"&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;《数智漫谈》直播栏目介绍&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="margin-left:0.0001pt; margin-right:0px; text-align:left"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;《数智漫谈》是开源中国推出的一档直播栏目，每月 1 期，已推出 22 期。以「深度对话、多元视角、前沿洞察」为核心理念，聚焦 IT 技术、开源治理、行业趋势与创新实践，通过轻松互动形式搭建开源领域的思想交流平台。区别于传统技术直播的单向输出，突出「围坐畅聊」的互动感和思想交锋的张力，打造开源领域的「圆桌派」。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="margin-left:0.0001pt; margin-right:0px; text-align:left"&gt;有兴趣的朋友，可以联系我~&lt;/p&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:center"&gt;&lt;img height="537" src="https://oscimg.oschina.net/oscnet/up-4dd54c1b0b817689ceefa15aa66d79cfae8.png" width="400" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="margin-left:0.0001pt; margin-right:0px; text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="margin-left:0.0001pt; margin-right:0px; text-align:left"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p style="margin-left:0.0001pt; margin-right:0px; text-align:justify"&gt;&amp;nbsp;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/3859945/blog/18635616</link>
      <guid isPermaLink="false">https://my.oschina.net/u/3859945/blog/18635616</guid>
      <pubDate>Sat, 10 May 2025 10:32:00 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>云原生周刊：Argo CD v3.1 正式发布</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;h2&gt;开源项目推荐&lt;/h2&gt; 
&lt;h3&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fkubewall%2Fkubewall" target="_blank"&gt;Kubewall&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Kubewall 是一个轻量级的开源 Kubernetes 仪表盘，支持多集群管理，主打单二进制部署和浏览器访问，提供实时资源监控、YAML 编辑、拓扑视图、日志查看等功能。它使用 Go 与 React 构建，支持通过 Docker、Helm、Homebrew 等多种方式安装，适合追求简洁、高效、多环境统一管理体验的开发者与运维人员。项目活跃迭代，是 Lens、Headlamp 等重量级工具的轻量替代方案。&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fkapicorp%2Fkapitan" target="_blank"&gt;Kapitan&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Kapitan 是 Kapicorp 开发的一个开源、以 Python 为基础，的高级配置管理工具，通过，层级化 inventory（YAML）驱动、多种模板引擎（如 Jinja、Jsonnet、Helm、Kadet）和原生秘密管理，帮助用户生成 Kubernetes、Terraform、脚本、文档等多环境、一致且可追踪的配置，适合平台工程／GitOps 流程，且项目活跃、使用 Apache 2.0 许可。&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fgefyrahq%2Fgefyra" target="_blank"&gt;Gefyra&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Gefyra 是一个开源工具，旨在将本地开发环境无缝连接到 Kubernetes 集群中，实现代码热更新和快速迭代。它通过创建加密网络桥接（基于 WireGuard）、代理流量并复用集群资源，让开发者无需每次更改都执行构建、推送和部署流程。支持 Docker、macOS、Windows 和 Linux，可通过命令行或 GUI 操作，广泛适用于微服务、本地调试、端到端测试等场景，极大提升了云原生开发效率。&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fkube-burner%2Fkube-burner" target="_blank"&gt;Kube-burner&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Kube‑burner 是一个用 Go 语言开发的开源 Kubernetes 性能与扩展测试编排框架，它可以按用户定义大规模创建、删除、更新 Kubernetes 资源，同时集成 Prometheus 度量、索引、告警功能，用于评估集群的性能瓶颈和扩展极限。&lt;/p&gt; 
&lt;h2&gt;文章推荐&lt;/h2&gt; 
&lt;h3&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcloudnativenow.com%2Fcontributed-content%2Fadvanced-devops-for-ai-continuous-delivery-of-models-using-jenkins-and-docker%2F" target="_blank"&gt;基于 Jenkins 与 Docker 的 AI 模型持续交付实战：构建高效 MLOps 流程&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;本文介绍了如何通过结合 Jenkins 和 Docker 实现 AI 模型的高级 DevOps（开发运维）流程，重点在于模型的持续交付（CD）。作者详细讲解了从模型训练、容器化、测试、部署到上线的自动化流程，展示了如何构建一个高效、可重复的 MLOps（机器学习运维）管道。通过这种方式，团队可以更快速、稳定地将 AI 模型部署到生产环境中，加速从开发到业务落地的过程。&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.cncf.io%2Fblog%2F2025%2F06%2F09%2Fgitops-in-2025-from-old-school-updates-to-the-modern-way%2F%3Fsessionid%3D-1595116295" target="_blank"&gt;GitOps 2025：从传统部署到自动化运维新时代&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;在 2025 年，GitOps 已从一种新兴理念发展为管理现代应用程序的基础标准，特别是在 Kubernetes 环境中。它通过将 Git 作为系统配置的唯一真实来源，结合自动化代理持续应用这些配置，实现了自动化、一致性和可追溯性，从而简化了云原生软件运维的复杂性。GitOps 的核心原则包括声明式配置、Git 作为唯一配置来源、通过 Pull/Merge 请求进行更改以及由代理持续进行环境同步。主要工具如 Argo CD 和 Flux CD 已成为主流选择，分别适用于需要强大 UI 和模块化灵活性的场景。&lt;/p&gt; 
&lt;p&gt;尽管 GitOps 的采用带来了诸多优势，如更快的发布、更安全的操作和更容易的回滚，但也面临着学习曲线陡峭、工具碎片化和文化转变等挑战。总体而言，GitOps 正在成为 DevOps 实践的关键组成部分，推动软件交付的自动化和可靠性。&lt;/p&gt; 
&lt;h2&gt;云原生动态&lt;/h2&gt; 
&lt;h3&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fkubeedge%2Fkubeedge%2Fblob%2Fmaster%2FCHANGELOG%2FCHANGELOG-1.21.md" target="_blank"&gt;KubeEdge 1.21 版本发布&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;KubeEdge 1.21 版本的更新日志展示了多个关键改进，包括云端和边缘组件的功能增强、系统稳定性优化以及 bug 修复。此版本引入了更灵活的 CRI 支持、增强的 EdgeMesh 服务治理能力和更完善的安全机制，同时提升了 DevOps 体验和兼容性。整体来看，1.21 版本进一步强化了边缘计算场景下的可扩展性与可靠性。&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.argoproj.io%2Fannouncing-argo-cd-v3-1-f4389bc783c8" target="_blank"&gt;Argo CD v3.1 正式发布&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;继 v3.0「轻量但强大」的发布奠定基础后，Argo CD v3.1 带来了 v3 系列的首批重大更新。新版本支持 OCI 镜像作为应用源，引入 CLI 插件机制、Hydrator 架构更新，以及 UI 的多项可用性提升。除了新功能，v3.1 还修复了大量安全漏洞与已知问题，显著提升了系统稳定性、扩展性和多集群支持能力。通过更快的同步性能、更细粒度的权限控制和增强的 SSO 机制，Argo CD v3.1 为 DevOps 团队打造了更高效、安全、可持续的 Kubernetes 应用交付体验。&lt;/p&gt; 
&lt;h3&gt;关于 KubeSphere&lt;/h3&gt; 
&lt;p&gt;KubeSphere （&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fkubesphere.io%EF%BC%89%E6%98%AF%E5%9C%A8" target="_blank"&gt;https://kubesphere.io）是在&lt;/a&gt; Kubernetes 之上构建的开源容器平台，提供全栈的 IT 自动化运维的能力，简化企业的 DevOps 工作流。&lt;/p&gt; 
&lt;p&gt;KubeSphere 已被 Aqara 智能家居、本来生活、东方通信、微宏科技、东软、华云、新浪、三一重工、华夏银行、四川航空、国药集团、微众银行、紫金保险、去哪儿网、中通、中国人民银行、中国银行、中国人保寿险、中国太平保险、中国移动、中国联通、中国电信、天翼云、中移金科、Radore、ZaloPay 等海内外数万家企业采用。KubeSphere 提供了开发者友好的向导式操作界面和丰富的企业级功能，包括 Kubernetes 多云与多集群管理、DevOps (CI/CD)、应用生命周期管理、边缘计算、微服务治理 (Service Mesh)、多租户管理、可观测性、存储与网络管理、GPU support 等功能，帮助企业快速构建一个强大和功能丰富的容器云平台。 &amp;gt; 本文由博客一文多发平台 &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopenwrite.cn%3Ffrom%3Darticle_bottom" target="_blank"&gt;OpenWrite&lt;/a&gt; 发布！&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/4197945/blog/18635610</link>
      <guid isPermaLink="false">https://my.oschina.net/u/4197945/blog/18635610</guid>
      <pubDate>Sat, 10 May 2025 10:19:00 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>「开源惠全球·集智创未来」——2025 全球数字经济大会全球开源创新发展论坛即将召开</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;img height="720" src="https://static.oschina.net/uploads/space/2025/0624/175355_3VoR_2720166.png" width="1279" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;开源是数字时代以开放、共建、共享、共治为主要特征的新型生产方式，已经成为全球信息技术产业发展的重要协作方式和生态构建形式。为抢抓开源繁荣发展机遇，促进全球数字协作，助力北京市数字友好城市建设，全球开源创新发展论坛（以下简称「论坛」）将于 7 月 5 日上午在北京国家会议中心举行。论坛由全球数字经济大会组委会主办，国家工业信息安全发展研究中心、开源中国、CNCF 基金会、国际内源基金会联合承办。&lt;/p&gt; 
&lt;p&gt;本次论坛以「开源惠全球·集智创未来」为主题，旨在搭建国际开源交流合作平台，广泛邀请国内外开源领域知名专家学者，顶尖开源组织、先锋开源企业、开源社区代表等齐聚一堂，聚焦全球开源技术的最新发展趋势，探讨优质开源社区培育路径，共商开源区域协作，推动形成开源发展合力，充分释放数字经济的放大、叠加、倍增效应，助力全球数字经济高质量发展。本次论坛有以下亮点：&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#d35400"&gt;&lt;strong&gt;&lt;strong&gt;亮点一：全球协作，国际开源创新聚合力&lt;/strong&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;论坛邀请开源中国、CNCF 基金会、国际内源基金会等国内外知名开源组织联合承办，广泛汇聚 Apache 基金会、Linux 基金会、FOSSASIA（亚洲开源）、华为、平凯星辰、蚂蚁等开源机构，重磅嘉宾云集，共同推动国内外开源组织、社区加强互动合作、共享技术成果，共筑全球开源发展未来。&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#d35400"&gt;&lt;strong&gt;&lt;strong&gt;亮点二：开源全景，多元视角汇聚发展之声&lt;/strong&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;论坛以「全球视野、本土创新」双轮驱动，邀请国内外开源代表围绕 AI 驱动下的开源生态建设、全球开源社区文化与发展前沿动态、全球开源明星项目中的「中国声音」等领域进行主题演讲，分享开源托管平台、开源基金会、国际开源协作、AI 智能体开源、OSPO 等方面的前沿视角与经验，确保内容专业性高、权威性强。&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#d35400"&gt;&lt;strong&gt;&lt;strong&gt;亮点三：紧扣前沿，重磅发布 AI 开源北京宣言&lt;/strong&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;论坛紧跟行业前沿趋势，抢抓新一代人工智能发展机遇，深入探索大模型时代开源发展路径。期间，中心将联合 Linux 基金会、CNCF 基金会、Apache 基金会、国际内源基金会等国际组织，以及华为、开源中国等国内企业，联合发起「人工智能开源协作倡议—北京宣言」，展现 AI 力量，携手推动开源 AI 普惠发展。&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#d35400"&gt;&lt;strong&gt;&lt;strong&gt;亮点四：前瞻布局，首次发布开源项目白名单&lt;/strong&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;论坛将首次发布「中国优秀开源项目白名单」，该名单由十余家「产学研用金」单位联合构建开源项目成长潜力分析预测模型，研究提出开源项目发展评价指标体系，形成优秀开源项目白名单，指导和促进开源项目健康发展，并为其他项目提供可借鉴范例，进一步推动开源生态繁荣发展。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;论坛议程大致如下：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="772" src="https://static.oschina.net/uploads/space/2025/0624/175503_n4LF_2720166.png" width="720" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;2025 全球数字经济大会—全球开源创新发展论坛即将启幕，论坛内容丰富、形式多样，欢迎扫描以下二维码报名参会。&lt;/p&gt; 
&lt;p&gt;&lt;img height="300" src="https://static.oschina.net/uploads/space/2025/0624/175514_O8qv_2720166.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;（报名方式：扫描上方二维码，完成个人信息注册后，下滑参会日程，选择&lt;strong&gt;&lt;strong&gt;7 月 5 日上午「全球开源创新发展论坛」&lt;/strong&gt;&lt;/strong&gt;，点击申请报名，您将收到【收到报名】短信通知，请等待审核。）&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357056</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357056</guid>
      <pubDate>Sat, 10 May 2025 09:56:00 GMT</pubDate>
      <author>来源: 投稿</author>
    </item>
    <item>
      <title>百度日志中台前端重构实践</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;p&gt;日志中台是百度内部针对打点数据的全生命周期管理平台，作为公司日志数据的唯一入口，承担以下核心职能：1.功能覆盖：提供从数据采集、传输、存储到查询分析的一站式服务，支持产品运营分析、研发性能监控、运维管理等多元场景。2.业务赋能：通过标准化流程实现用户行为日志的埋点申请、审批及退场管理，助力 APP 端、服务端等业务线挖掘数据价值。3.生态协同：与大数据平台、推荐中台、性能平台深度联动，避免重复建设，提升资源利用率，强化业务中台能力。&lt;/p&gt; 
&lt;span id="OSC_h1_1"&gt;&lt;/span&gt; 
&lt;h1&gt;01 项目背景&lt;/h1&gt; 
&lt;p&gt;2020 年初启动的日志中台前端项目，随着业务发展逐渐暴露出严重问题。整个前端项目技术负债多，有 500 多个文件，共 11 万多行源码。项目已经变得老旧而臃肿。面临线上 bug 频发、排查问题效率低下等各种问题，陈旧的技术栈与低效的流程也制约了团队的生产力。因此需进行全面全面重构，通过基于业务导向的架构优化、开发测试流程规范化，从而提升前端开发效率，使项目具备长期稳健发展的技术基础。本文将重点介绍我在重构项目过程中的一些实践经验。&lt;/p&gt; 
&lt;span id="OSC_h2_2"&gt;&lt;/span&gt; 
&lt;h2&gt;02 前端项目面临的问题&lt;/h2&gt; 
&lt;p&gt;先介绍下日志中台前端项目的基本情况&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;核心框架：Vue 2.6 + Vuex 3.1.1 + VueRouter 3.0.6&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;UI 组件库：ElementUI 2.15.13&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;构建工具：@vue/cli-service 3.11.0（基于 Webpack 4）&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;部署平台：测试环境（FIS3）、生产环境（Tower）&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;下面我将从 4 个维度来分析下前端项目所面临的各种问题。&lt;/p&gt; 
&lt;span id="OSC_h2_3"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;2.1 代码质量&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;由于项目没有接入代码格式化 prettier 和，代码规范检查 eslint，导致项目的代码质量堪忧，各种各样的代码风格并存。在开发需求过程中，各自的编码风格不一致，维护时需额外适应时间，甚至由此引发线上问题。&lt;/p&gt; 
&lt;span id="OSC_h2_4"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;2.2 基础建设&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;1. 代码臃肿，维护困难&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;全项目 500+源文件中，30+文件超 1000 行，5+文件超 2000 行，最大文件达 5000 行。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;巨型文件导致：&lt;/p&gt; &lt;p&gt;IDE 卡顿（Mac 开发时频繁卡住）。&lt;/p&gt; &lt;p&gt;热更新失效（&amp;gt;2s 延迟，大文件需手动刷新浏览器）。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;2. 技术栈陈旧&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;仍使用已停止维护的&lt;code&gt;vue-cli&lt;/code&gt;（Webpack 4 时代工具链），与现代构建工具（Vite、Webpack 5）存在代差。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="OSC_h2_5"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;2.3 构建和部署&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;测试环境&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;测试环境的部署采用的是&lt;/strong&gt;&lt;/strong&gt; &lt;strong&gt;&lt;strong&gt;fis3&lt;/strong&gt;&lt;/strong&gt;，这是百度 FE 团队早期自研的集构建、部署于一身前端构建工具，日志中台项目使用其部署测试环境的功能。具体流程就是在开发者本地执行打包操作，然后将打包产物通过 fix3 推送到后端的服务器上去，替换掉之前的打包产物，从而实现部署新版本。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;这种方式存在诸多问题：&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;本地构建依赖不一致，易引发环境差异问题。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;无 CDN 缓存，静态资源直推后端服务器。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;无版本管理，存在代码覆盖风险。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;FIS3 已停止维护，社区无支持。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;本质问题&lt;/strong&gt;&lt;/strong&gt;：前后端未完全分离，违背当前主流协作模式。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;生产环境&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;生产环境的部署则采用的是 Tower 平台，这是百度内部的线上部署平台，通过平台的形式将 master 分支的代码在服务器上编译构建，将打包后的产物推送到线上环境对应的服务器上，从而实现完整的上线流程。这种上线方式同样存在诸多不足：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;上线耗时长达 30 分钟，无增量构建能力。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;多服务器部署时存在「漂移现象」（请求路由不一致）。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;操作流程复杂，平台限制多（如回滚困难）。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;仍缺失 CDN 加速，影响页面加载性能。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="OSC_h2_6"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;2.4 优质组件&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;在 Vue 技术栈中，模块和组件的模糊概念，导致很多开发者无法区分其区别。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;1. 组件与模块概念混淆&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;src/components&lt;/code&gt;目录下堆积 40+文件夹，但 90% 为一次性业务模块（如 5 个重复封装的 Table 组件），缺乏真正的复用价值。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;2. 基础建设缺失&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;无通用业务组件库，开发依赖 Element UI 原始组件。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;高频逻辑（如表单校验、数据请求）需重复实现，通过「复制粘贴」开发，导致代码冗余和一致性风险。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="OSC_h1_7"&gt;&lt;/span&gt; 
&lt;h1&gt;03 全面重构拆分&lt;/h1&gt; 
&lt;p&gt;下面是针对以上项目中的各个痛点的重构具体手段。&lt;/p&gt; 
&lt;span id="OSC_h2_8"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;3.1 接入工程化&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;前端项目若缺乏统一的代码规范和质量控制，随着业务增长，代码可维护性会急剧下降，最终导致开发效率低下、线上问题频发。因此，引入业界成熟的工程化方案是提升代码质量的关键。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-44faf28db07e05e98073becb5657960b557.jpg" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h4_9"&gt;&lt;/span&gt; 
&lt;h4&gt;&lt;strong&gt;&lt;strong&gt;工程化改造步骤&lt;/strong&gt;&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;1. 清理冗余配置&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;移除项目中无用的、过时的配置（如废弃的&lt;code&gt;.babelrc&lt;/code&gt;、冗余的&lt;code&gt;webpack&lt;/code&gt;配置等），减少干扰项。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;2. 统一基础配置文件&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;在项目根目录下添加必要的配置文件，确保团队开发环境一致：&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;.vscode/settings.json&lt;/code&gt;（统一 VSCode 编辑器配置）&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;.editorconfig&lt;/code&gt;（统一缩进、换行等基础格式）&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;.npmrc&lt;/code&gt;（设置为百度 npm 镜像）&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;.browserslistrc&lt;/code&gt;（明确目标浏览器兼容范围）&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;3. 接入代码规范工具&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;Prettier&lt;/strong&gt;&lt;/strong&gt;：自动格式化代码，统一风格（如缩进、引号、分号等）。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;ESLint&lt;/strong&gt;&lt;/strong&gt;：检查 JavaScript/Vue 代码质量，避免常见错误。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;Stylelint&lt;/strong&gt;&lt;/strong&gt;（可选）：规范 CSS/Less 代码风格。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;4. 优化开发体验&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;推荐安装必要的 VSCode 插件（如 ESLint、Prettier、Volar 等），提升开发效率。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;5. 提交时增量强制校验（Git Hooks）&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;接入&lt;code&gt;husky&lt;/code&gt;+&lt;code&gt;lint-staged&lt;/code&gt;，在&lt;code&gt;git commit&lt;/code&gt;时自动执行代码检查，阻止不合规代码提交。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;strong&gt;配置参考&lt;/strong&gt;&lt;/strong&gt;：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;VSCode 统一配置&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.yuque.com%2Fshuoshubao%2Fbg00go%2Fwc87bknptk3lomed%23NStKP" target="_blank"&gt;https://www.yuque.com/shuoshubao/bg00go/wc87bknptk3lomed#NStKP&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;工程化配置方案 https://www.yuque.com/shuoshubao/bg00go/wc87bknptk3lomed#SJTr2&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;span id="OSC_h4_10"&gt;&lt;/span&gt; 
&lt;h4&gt;&lt;strong&gt;&lt;strong&gt;历史代码修复策略&lt;/strong&gt;&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;原则：「自动修复优先，手动修复补充」，避免无限制添加&lt;code&gt;eslint-disable&lt;/code&gt;或&lt;code&gt;ignore&lt;/code&gt;规则，导致规范形同虚设。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;具体执行步骤&lt;/strong&gt;&lt;/strong&gt;：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;1. 自动格式化（Prettier）&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;2. ESLint 自动修复&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;3. 分析剩余问题&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;使用&lt;code&gt;eslint-formatter-html&lt;/code&gt;生成报告，评估剩余问题。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;调整 ESLint 规则（如放宽部分历史代码限制），拆解为多个小任务手动修复。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;4. 回归测试&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;联合熟悉业务的同学进行全量测试，确保修复过程不影响系统功能。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="OSC_h4_11"&gt;&lt;/span&gt; 
&lt;h4&gt;&lt;strong&gt;&lt;strong&gt;效果验证&lt;/strong&gt;&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;代码风格统一&lt;/strong&gt;&lt;/strong&gt;：所有新提交的代码均符合规范，减少风格争议。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;错误率下降&lt;/strong&gt;&lt;/strong&gt;：低级语法错误、边界条件导致的 JS 报错大幅减少。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;strong&gt;开发体验提升&lt;/strong&gt;&lt;/strong&gt;：IDE 卡顿减少（格式化后代码更简洁），热更新效率提高。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="OSC_h2_12"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;3.2 升级基建&lt;/strong&gt;&lt;/h2&gt; 
&lt;span id="OSC_h3_13"&gt;&lt;/span&gt; 
&lt;h3&gt;3.2.1 源码优化与依赖治理&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;问题现状&lt;/strong&gt;&lt;/strong&gt;：&lt;/p&gt; 
&lt;p&gt;项目存在大量技术债务，包括：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;冗余资源（未压缩图片约 2M）&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;无效依赖（22 个未使用的 npm 包）&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;混合模块规范（require/import 混用）&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;废弃技术栈（如已停止维护的 iView）&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;优化措施&lt;/strong&gt;&lt;/strong&gt;：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;1. 资源优化&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;使用基于 Tinypng 封装的工具批量压缩图片，体积减少 65%&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;清理已下架页面的遗留代码（约 15 个路由）&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;2. 依赖治理&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;移除 22 个无用依赖&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;统一使用 ES Module 规范（手动替换 require 为 import）&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;3. 技术栈升级&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;替换老旧组件库：vue-json-diff、vue-code-diff、vue-codemirror 替换为 monaco-editor&lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="OSC_h3_14"&gt;&lt;/span&gt; 
&lt;h3&gt;3.2.2 &lt;strong&gt;&lt;strong&gt;构建相关&lt;/strong&gt;&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;相对于以往的 Webpack 或者 Vue CLI，存在开发服务器启动慢（平均 45 秒）、热更新延迟高（2.5 秒）、构建流程复杂（需 Babel 转译 ES5）。&lt;/p&gt; 
&lt;p&gt;Vite 配置详见：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.yuque.com%2Fshuoshubao%2Fbg00go%2Fwc87bknptk3lomed%23wyx0p" target="_blank"&gt;https://www.yuque.com/shuoshubao/bg00go/wc87bknptk3lomed#wyx0p&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;接入 Vite 后，低配置电脑同学开发时的平均热更新时间由 2.5 秒缩短到 100 毫秒。在单个需求完成耗时方面，由之前的 4.2 人天缩减到 3.4 人天，综合人效提高&lt;strong&gt;&lt;strong&gt;19%&lt;/strong&gt;&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;另一方面，由于 Vue CLI 是基于 babel 将 esnext 代码转成 es5，而 Vite 基于 esbuild 不需要进行降级编译。在将 Vite 的配置 build.target 设置为 ['chrome100'] 后，甚至连非常新的 esnext 语法糖都不需要转换，浏览器直接可以使用前端的源码，极大的利用了 esnext 带来的开发便利，而不需要关注 Babel 的版本以及各种依赖包和复杂的配置。&lt;/p&gt; 
&lt;span id="OSC_h3_15"&gt;&lt;/span&gt; 
&lt;h3&gt;3.2.3 &lt;strong&gt;&lt;strong&gt;部署相关&lt;/strong&gt;&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;百度内部主流的部署平台是 &lt;strong&gt;&lt;strong&gt;Fcnap&lt;/strong&gt;&lt;/strong&gt;。这是一个类似 Vercel 的前端一站式部署平台，基于 git 分支，只要检测到分支变动，就会触发自动构建和部署。&lt;/p&gt; 
&lt;p&gt;只需配置好各个测试环境以及生产环境的基本信息，后续在需要开发中，只需要将分支和测试环境关联起来，就可以达到随时提交代码随时部署的效果；上线过程更是丝滑，只需要将代码合到 master 分支，就会自动上线。&lt;/p&gt; 
&lt;p&gt;将 fis3 以及 Tower 迁移到 Fcnap 后有如下优势：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;测试和生成环境使用一套部署逻辑&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;上线部署耗时由 30 分钟缩减至 2 分钟&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;提供 cdn 功能，每次上线后增量更新的静态资源只有 500kb&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;上线期间访问系统不会出现白屏现象&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;上线过程对用户无任何影响&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="OSC_h3_16"&gt;&lt;/span&gt; 
&lt;h3&gt;3.2.4 接口调试&lt;/h3&gt; 
&lt;span id="OSC_h4_17"&gt;&lt;/span&gt; 
&lt;h4&gt;传统开发模式的痛点&lt;/h4&gt; 
&lt;p&gt;在传统前后端协作中，存在典型的"接口依赖症"：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;1. 开发阻塞&lt;/strong&gt;&lt;/strong&gt;：前端必须等待后端接口 Ready 才能开始调试&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;2. 效率低下&lt;/strong&gt;&lt;/strong&gt;：联调阶段频繁出现接口变更，导致重复返工&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;3. 数据不可控&lt;/strong&gt;&lt;/strong&gt;：依赖真实测试环境数据，难以覆盖边界场景&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;数据表明：在接口未就绪阶段，前端开发效率会下降 60% 以上&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;span id="OSC_h4_18"&gt;&lt;/span&gt; 
&lt;h4&gt;真正的"前后端分离"实践&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;核心原则&lt;/strong&gt;&lt;/strong&gt;：开发阶段解耦，联调阶段对接&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;1. 规范先行&lt;/strong&gt;&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;后端通过 YAPI 等平台提供完整的接口文档&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;包含：请求方法、参数结构、响应体示例、状态码定义&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;2. Mock 数据要求&lt;/strong&gt;&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;真实业务数据（非简单根据接口文档生成各种随机数据）&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;可自定义异常场景（404， 502 等真实场景还原）&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;支持动态响应（根据参数返回不同数据）&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;针对这个开发环节，我们也基于 Vite 实现了一个非常好用的插件：vite-plugin-mock，用于提升开发效率。整体的设计如下：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-3eb5ab12f6a4aae3eff18b059102be05cc9.jpg" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;相比于传统的 mock 方案，vite-plugin-mock 在开发体验、数据维护上有更好的开发体验。&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;特性&lt;/th&gt; 
   &lt;th&gt;传统 Mock 方案&lt;/th&gt; 
   &lt;th&gt;vite-plugin-mock&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;数据真实性&lt;/td&gt; 
   &lt;td&gt;随机生成，不可用&lt;/td&gt; 
   &lt;td&gt;可在真实接口数据上任意修改&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;开发体验&lt;/td&gt; 
   &lt;td&gt;需要启动 Mock 服务&lt;/td&gt; 
   &lt;td&gt;配置简单，可随时修改数据&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;联调切换&lt;/td&gt; 
   &lt;td&gt;手动修改请求地址&lt;/td&gt; 
   &lt;td&gt;自动代理无缝切换&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;数据维护&lt;/td&gt; 
   &lt;td&gt;独立维护 Mock 数据&lt;/td&gt; 
   &lt;td&gt;数据存放在本地，每个人都可维护单独的数据&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;span id="OSC_h2_19"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;3.3 &lt;strong&gt;构建体积优化&lt;/strong&gt;&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;这一部分主要从以下三个技术方案着手优化，再配合其他人工优化手段，打包体积由开始的 14M 优化到 1.8M，接入 cdn 功能后，则仅有 500kb。&lt;/p&gt; 
&lt;span id="OSC_h3_20"&gt;&lt;/span&gt; 
&lt;h3&gt;3.3.1 element-ui&lt;/h3&gt; 
&lt;p&gt;fork element-ui 源码, 采用 rollup 进行打包，优化部分源码，修复部分 bug，重新发包为 @baidu-log/element-ui&lt;/p&gt; 
&lt;p&gt;这一步骤，js 体积从 1.2M 优化到 500kb。并结合下面 externals 功能，进一步使用 cdn 功能缓存这部分文件体积。&lt;/p&gt; 
&lt;span id="OSC_h3_21"&gt;&lt;/span&gt; 
&lt;h3&gt;3.3.2 引入 externals 功能&lt;/h3&gt; 
&lt;p&gt;将基础包通过 cdn 的形式在 index.html 模板中引入其 umd 格式的文件，从而避免打包这部分内容。这部分会用到 cdn 的缓存功能，会节约掉大约 2M 的体积。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;vite-plugin-externals&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;这个是开源的 vite 插件，配置也比较简单，详见配置：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.yuque.com%2Fshuoshubao%2Fbg00go%2Fwc87bknptk3lomed%23LiR2X" target="_blank"&gt;https://www.yuque.com/shuoshubao/bg00go/wc87bknptk3lomed#LiR2X&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;vite-plugin-assets&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;这个是为了配合上面 vite-plugin-externals 插件，将对应的 externals 的 npm 包对应的 umd 文件插入到模板中，代码详见：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.yuque.com%2Fshuoshubao%2Fbg00go%2Fwc87bknptk3lomed%23xts88" target="_blank"&gt;https://www.yuque.com/shuoshubao/bg00go/wc87bknptk3lomed#xts88&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;为什么不直接写在 index.html 里呢？因为像 vue 和 react 这样的框架，在开发时都提供了对应的开发调试工具：dev-tools。而使用 dev-tools 则需要提供对应的 &lt;strong&gt;&lt;strong&gt;dist/vue.js&lt;/strong&gt;&lt;/strong&gt;，而 react 对应的则是 &lt;strong&gt;&lt;strong&gt;react.development.js&lt;/strong&gt;&lt;/strong&gt;。&lt;/p&gt; 
&lt;span id="OSC_h3_22"&gt;&lt;/span&gt; 
&lt;h3&gt;3.3.3 大包的特殊处理&lt;/h3&gt; 
&lt;p&gt;1. monaco-editor&lt;/p&gt; 
&lt;p&gt;项目中用到了 monaco-editor 这个编辑器组件，直接打包将会非常大，有 10M 以上的体积。根据官方提供的方案即可进行如下封装，其中 cdn 地址由百度的 npm 镜像服务提供支持。&lt;/p&gt; 
&lt;p&gt;代码详见：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.yuque.com%2Fshuoshubao%2Fbg00go%2Fwc87bknptk3lomed%23gozcq" target="_blank"&gt;https://www.yuque.com/shuoshubao/bg00go/wc87bknptk3lomed#gozcq&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;2. xlsx, fabric 等&lt;/p&gt; 
&lt;p&gt;在项目中用到了 xlsx, fabric, markdown-it, echarts, &lt;a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fdraw.io" target="_blank"&gt;draw.io&lt;/a&gt; 这几个体积很大的包，但又不属于很基础的包，只有少部分页面的某个功能点才会用到。针对这些包采用从 cdn 异步加载其 umd 包的形式来引入，而不是通过 import npm 包的形式。&lt;/p&gt; 
&lt;p&gt;代码详见：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.yuque.com%2Fshuoshubao%2Fbg00go%2Fwc87bknptk3lomed%23rEBee" target="_blank"&gt;https://www.yuque.com/shuoshubao/bg00go/wc87bknptk3lomed#rEBee&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;以上两种优化方案，与常见的动态引入方案（dynamic import）是有很大区别的，dynamic import 是通过编译工具将对应的 npm 包打包成一个独立的 chunk，然后在使用的时候再通过 loadScript 方式引入。这种问题在于文件的缓存，一是 chunk 可能会变，二是像 Vercel 这种平台，每次发布都是一个全新的 s3 bucket，上线后缓存功能也就失效了。而上述这种方案，则利用 npm 镜像服务，每次都访问固定的 cdn 地址，也就达到了 cdn 的缓存目的了。&lt;/p&gt; 
&lt;span id="OSC_h2_23"&gt;&lt;/span&gt; 
&lt;h2&gt;&lt;strong&gt;3.4 建设组件库&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;鉴于项目没有优质组件的背景，从零到一搭建了组件库，组件库主要包含以下内容：&lt;/p&gt; 
&lt;p&gt;1. 基于 Vuepress 建设高质量组件库文档&lt;/p&gt; 
&lt;p&gt;2. 迁移 element-ui 文档，并修复其中大量劣质示例代码&lt;/p&gt; 
&lt;p&gt;3. 采用 Vitest 编写工具方法的测试用例&lt;/p&gt; 
&lt;p&gt;4. 提供 9 个高频优质通用组件，10 个业务组件&lt;/p&gt; 
&lt;p&gt;组件库文档：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flogsfe.vercel.app%2F" target="_blank"&gt;https://logsfe.vercel.app/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;文档分为以下几大模块&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;优质组件：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flogsfe.vercel.app%2Fcomponents%2FTable%2F" target="_blank"&gt;https://logsfe.vercel.app/components/Table/&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;组件库里的方法：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flogsfe.vercel.app%2Futils.html" target="_blank"&gt;https://logsfe.vercel.app/utils.html&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;@nbfe/tools 工具库方法： &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flogsfe.vercel.app%2Ftools%2Fdate.html" target="_blank"&gt;https://logsfe.vercel.app/tools/date.html&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ElementUI 文档：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flogsfe.vercel.app%2Felement%2Ficon.html" target="_blank"&gt;https://logsfe.vercel.app/element/icon.html&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;前端定制的开发规范：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flogsfe.vercel.app%2Fcontribute%2F" target="_blank"&gt;https://logsfe.vercel.app/contribute/&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;实际效果：组件库中的组件在项目中目前已被使用 240 次，用户使用体验良好。&lt;/p&gt; 
&lt;span id="OSC_h3_24"&gt;&lt;/span&gt; 
&lt;h3&gt;3.4.1 通用组件&lt;/h3&gt; 
&lt;p&gt;基于大量的 B 端系统开发经验，提炼出配置化表格和配置化表单组件，满足项目中 90% 的开发场景，通过重构部分页面后比较分析，在写对应模块时，能减少 40% 的代码。&lt;/p&gt; 
&lt;p&gt;通用组件均与业务解耦，设计优雅的 api，并提供大量示例。组件库里只提供少量的优质组件，严格把控每一行提交的代码，并为组件中的工具函数提供符合 JSDoc-style 规范的注释，且通过 Vitest 来编写单元测试。&lt;/p&gt; 
&lt;span id="OSC_h3_25"&gt;&lt;/span&gt; 
&lt;h3&gt;3.4.2 element-ui 文档集成&lt;/h3&gt; 
&lt;p&gt;在实际工作中，发现 element-ui 文档存在很多问题且早已不维护。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;主题与日志中台不符，不利于查看&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;组件默认 size 过大，一页都看不了多少示例&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;右侧没有 toc 功能，不方便快速定位&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;示例很多写法不优雅，以及很多冗余代码被人机的复制到了项目中&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;在线调试示例采用的是 codepen 平台，这个平台很慢而且经常挂了&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;基于以上各种问题，将 element-ui 官方的示例 fork 到组件库中，使用和日志中台一样的主题，并修复上述各种问题。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-448a51bef83543cbfa7e0b5a4588348e6b3.jpg" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;并使用纯前端来实现了一个完全可用的 codepen 组件使用示例功能。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-10997cf6d1b599250b533a1a538cfa80d94.jpg" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h3_26"&gt;&lt;/span&gt; 
&lt;h3&gt;3.4.3 通用工具库&lt;/h3&gt; 
&lt;p&gt;基于 B 端系统抽象的实用工具方法集合。在组件库中提供优质的说明文档和使用示例。这个已经发布到 npm 上，并在多个公司和团队使用。&lt;/p&gt; 
&lt;p&gt;包括日期处理、数据处理、接口数据格式化、针对 element-ui 的一些实用封装。目前已在项目中被 93 个文件使用 150 次。&lt;/p&gt; 
&lt;p&gt;项目地址：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.npmjs.com%2Fpackage%2F%40nbfe%2Ftools" target="_blank"&gt;https://www.npmjs.com/package/@nbfe/tools&lt;/a&gt;&lt;/p&gt; 
&lt;span id="OSC_h1_27"&gt;&lt;/span&gt; 
&lt;h1&gt;04 总结与展望&lt;/h1&gt; 
&lt;p&gt;在频繁的需求迭代过程中，项目迟早会变成臃肿老旧的样子。当开发体验、开发速度、代码质量、项目可维护性、联调测试体验、线上质量等全方位令人举步维艰的时候，就该发起大规模的全面重构了。对每一项重构技术需要深刻掌握，才能掌握重构的深度和保证重构后的项目质量。另外，还定制了很多开发规范和最佳实践指导，但项目中仍存在大量不符合规范的地方，将在未来继续进行全量修复，直到将一个老旧的项目重构到更接近现代化前端项目的程度。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/4939618/blog/18635524</link>
      <guid isPermaLink="false">https://my.oschina.net/u/4939618/blog/18635524</guid>
      <pubDate>Sat, 10 May 2025 07:41:00 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>curl 之父发文介绍 OpenSSL 分支家族</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;curl 之父近日发表文章&lt;u&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdaniel.haxx.se%2Fblog%2F2025%2F06%2F23%2Fa-family-of-forks%2F" target="_blank"&gt;介绍&lt;/a&gt;&lt;/u&gt; OpenSSL 分支家族，展示了它们的差异、相似之处，以及支持它们所需的一些见解。&lt;/p&gt; 
&lt;p&gt;译文如下：&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;curl 支持使用 11 种不同的 TLS 库进行编译。其中六个库是 OpenSSL 或其分支。让我向你展示它们的差异、相似之处，以及支持它们所需的一些见解。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;SSLeay&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;这一切都始于 SSLeay。这是我发现的第一个 SSL 库，我们使用这个库在 1998 年春天为 curl 添加了第一个 HTTPS 支持。显然，SSLeay 项目早在 1995 年就已经启动了。&lt;/p&gt; 
&lt;p&gt;那是一个我们还只支持 SSL 的年代；TLS 会在之后才出现。&lt;/p&gt; 
&lt;p&gt;OpenSSL 一直拥有一个古怪、不一致且极其庞大的 API 集（其中一大部分是从 SSLeay 继承而来的），这进一步被稀疏的文档所复杂化，这些文档留给用户去依靠自己的想象力和技能去查阅源代码，以获取最后的细节解答（即使在 2025 年今天也是如此）。在 curl 中，我们经常收到关于如何使用这个库的偶尔问题报告，即使已经过了几十年。 presumably，这同样适用于所有 OpenSSL 用户。&lt;/p&gt; 
&lt;p&gt;OpenSSL 项目经常受到批评，认为他们在几年前升级到版本 3 之后，在性能方面有所疏忽。他们也一直进展缓慢或不愿采用新的 TLS 技术，例如 QUIC 和 ECH。&lt;/p&gt; 
&lt;p&gt;尽管如此，OpenSSL 已经成为一种主导的 TLS 库，尤其是在开源领域。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;LibreSSL&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;回到 Heartbleed 事件时期，LibreSSL 分叉出来并成为独立的项目。他们删除了他们认为不属于库中的功能，创建了自己的 TLS 库 API。几年后，苹果在 macOS 上使用 LibreSSL 提供 curl。他们有一些本地修补，使它&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdaniel.haxx.se%2Fblog%2F2024%2F03%2F08%2Fthe-apple-curl-security-incident-12604%2F" target="_blank"&gt;行为与其他不同&lt;/a&gt;。&lt;/p&gt; 
&lt;p&gt;LibreSSL 在 QUIC 的支持上落后，不支持 SSLKEYLOGFILE、ECH，而且如今在实现新功能方面似乎比 OpenSSL 更慢。&lt;/p&gt; 
&lt;p&gt;curl 自从创建以来就与 LibreSSL 完美配合。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;BoringSSL&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;在 Heartbleed 事件时期由 Google 分叉出来。&lt;em&gt;Google 为 Google 做的&lt;/em&gt;，他们没有公开发布过，清理了很多原型和变量类型，并在 QUIC API 推动中处于领先地位。总体而言，大多数新的 TLS 发明都已在 BoringSSL 中实现和支持，比其他分叉更早。&lt;/p&gt; 
&lt;p&gt;Google 在 Android 的其他地方也使用这个。&lt;/p&gt; 
&lt;p&gt;curl 从创建以来就与 BoringSSL 完美配合。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;AmiSSL&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;一个为使 OpenSSL 能够在 AmigaOS 上正确编译和运行而制作的 OpenSSL 分支或变种。我对它了解不多，但在这里包含它是为了完整性。它似乎基本上是为 Amiga 系统移植的 OpenSSL。&lt;/p&gt; 
&lt;p&gt;当为 AmigaOS 编译时，curl 也能与 AmiSSL 兼容。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;QuicTLS&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;由于 OpenSSL 延迟响应并拒绝提供 QUIC API，其他分支在 2020 年初期（我尚未看到有人解释原因）采取了行动。微软和 Akamai 分支了 OpenSSL，产生了 &lt;em&gt;QuicTLS&lt;/em&gt;，此后它试图成为一个 &lt;em&gt;轻量级&lt;/em&gt; 的分支，主要只是在与 BoringSSL 和 LibreSSL 支持相同风格的基础上添加 QUIC API。&lt;em&gt;轻量级&lt;/em&gt; 的含义是它们密切跟踪上游开发，并且除了 QUIC API 之外，没有打算在其他方面偏离。&lt;/p&gt; 
&lt;p&gt;在 OpenSSL 3.5 中，他们终于提供了一个与 fork（包括 QuicTLS）提供的 QUIC API 不同的 QUIC API。我认为这促使 QuicTLS 重新考虑其未来的发展方向，但我们仍在等待确切的进展。&lt;/p&gt; 
&lt;p&gt;curl 自从创建以来就与 QuicTLS 完美配合。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;AWS-LC&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;这是由亚马逊维护的一个 BoringSSL 分支。与 BoringSSL 不同的是，他们确实进行了实际的（频繁的）发布，因此看起来像一个项目，即使是非亚马逊用户也可以实际使用和依赖——尽管他们存在的目的是 _维护一个与 AWS 使用的软件和应用程序兼容的安全 libcrypto _。令人惊讶的是，他们维护的不仅仅是「仅仅」 libcrypto。&lt;/p&gt; 
&lt;p&gt;这个分支最近显示出大量的活动，甚至在核心部分也是如此。&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.haproxy.com%2Fblog%2Fstate-of-ssl-stacks" target="_blank"&gt;2025 年 5 月由 HAProxy 团队进行的基准测试&lt;/a&gt; 表明，AWS-LC 显著优于 OpenSSL。&lt;/p&gt; 
&lt;p&gt;AWS-LC 提供的 API 与 BoringSSL 的 API 并不完全相同。&lt;/p&gt; 
&lt;p&gt;curl 与 AWS-LC 从 2023 年初开始就配合得非常好。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;家族树&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/img/202506/24145235_ALUZ.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;OpenSSL 分支家族树&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;OpenSSL 分支家族现状&lt;/h2&gt; 
&lt;p&gt;这六个不同的分支各自有其特定的特性、API 和功能，这些在不同版本中也会发生变化。目前我们仍然支持这六个分支，因为人们似乎仍在使用它们，而且维护起来是可行的。&lt;/p&gt; 
&lt;p&gt;我们使用相同的 &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fcurl%2Fcurl%2Fblob%2Fmaster%2Flib%2Fvtls%2Fopenssl.c" target="_blank"&gt;单个源代码文件&lt;/a&gt; 支持所有这些分支，并通过不断增长的 #ifdef 逻辑来实现。我们通过在 CI 中使用这些分支进行构建验证，尽管只使用了一小部分最近的版本。&lt;/p&gt; 
&lt;p&gt;随着时间的推移，这些分支似乎正在逐渐彼此分离。我认为这还不构成一个问题，但我们当然在监控这种情况，可能在某个时候需要进行一些内部重构以适应这种变化。&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;未来&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;我无法预见会发生什么。如果历史是一堂课，我们似乎更倾向于走向更多的分支，而不是更少的分支。但当然，每一位阅读这篇博客文章的读者现在都会思考，所有这些分支所耗费的重复努力以及由此带来的隐含低效性到底有多少。这不仅适用于这些库本身，也适用于像 curl 这样的用户。&lt;/p&gt; 
&lt;p&gt;我认为我们只能等待观察。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357005/a-family-of-openssl-forks</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357005/a-family-of-openssl-forks</guid>
      <pubDate>Sat, 10 May 2025 06:52:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>MiniMax 上线 AI 音色设计功能</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;MiniMax 稀宇科技&lt;u&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FSUqhAd54Q15Huq-AQ9EeCA" target="_blank"&gt;宣布&lt;/a&gt;&lt;/u&gt;旗下 MiniMax Audio 上线了「Voice Design 音色设计」功能。&lt;/p&gt; 
&lt;p&gt;音色的维度一般分成音频质量、发声方式、情感基调以及人物画像。该功能根据用户对音色需求的描述，模型自动拆解成音色相关的描述信息，并根据上述的描述来得到一个新的音色编码。同视频模型类似，该功能支持对音色的抽卡，如果不满意，多试几次，很容易得到理想中的专属独一音色，并可存储下来做后续的音频内容创作。&lt;/p&gt; 
&lt;p&gt;据介绍，通过 Voice Design 音色设计，用户可以通过自然语言来描述自己心中所想的音色，实现对多个维度的精准控制，甚至生成世界上不存在的音色。同时，Voice Design 与 Speech 02 语音模型在链路上相配合，用户在文字转语音中可真正实现了「所需即所得」，以「任意语言 × 任意口音 × 任意音色」，实现可全自定义的无限组合。&lt;/p&gt; 
&lt;p&gt;此外，Voice Design 解决了语音合成领域的两个挑战：难以精准匹配用户各个细分场景下的多样需求；复刻音色需要用户花费大量时间准备输入素材，并且存在潜在的版权风险。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0624/142945_xJzZ_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;目前，Voice Design 已上线 MiniMax Audio 国内、海外两个版本。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;国内版：minimaxi.com/audio&lt;/li&gt; 
 &lt;li&gt;海外版：minimax.io/audio&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/357002/minimax-voice-design</link>
      <guid isPermaLink="false">https://www.oschina.net/news/357002/minimax-voice-design</guid>
      <pubDate>Sat, 10 May 2025 06:31:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>空间理解模型 SpatialLM 正式发布首份技术报告</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;近日，空间理解模型 SpatialLM 正式发布首份技术报告。这一模型此前曾与 DeepSeek-V3、通义千问 Qwen2.5-Omni 一起登上全球最大的开源社区 HuggingFace 全球趋势榜前三。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0624/140955_PZlV_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;作为一款将大语言模型扩展到 3D 空间理解任务中的模型，SpatialLM 能从 3D 点云输入生成结构化的空间场景描述，这一过程突破了大语言模型对物理世界几何与空间关系的理解局限，让机器具备空间认知与推理能力，为具身智能等相关领域提供空间理解基础训练框架。&lt;/p&gt; 
&lt;p&gt;在开源后经过广泛的实际验证，本次技术报告聚焦 SpatialLM 1.1 升级版本，其不仅包含了详细的消融实验与训练配方，还在点云编码方式、分辨率、用户指定识别类目等维度上实现优化。&lt;/p&gt; 
&lt;p&gt;多项基准测试数据显示：该模型在任务数据集微调后，在空间布局识别、3D 物体检测任务中，均达到了相比与最新专业模型持平或更优的效果。&lt;/p&gt; 
&lt;p&gt;&lt;img height="309" src="https://static.oschina.net/uploads/space/2025/0624/141014_3iIl_2720166.png" width="1280" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;本次报告重点围绕&lt;strong&gt;算法框架&lt;/strong&gt;和&lt;strong&gt;训练数据&lt;/strong&gt;两方面展开。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;在算法架构方面&lt;/strong&gt;，SpatialLM 将大语言模型（LLMs）扩展到 3D 空间理解任务中，特别在结构化室内建模领域实现了重要突破。&lt;/p&gt; 
&lt;p&gt;这一技术路线打破了传统任务专属架构（task-specific architecture）的限制，创新性地采用可编辑的文本形式表达场景结构。这一创新设计具有双重技术优势：&lt;/p&gt; 
&lt;p&gt;一方面&lt;strong&gt;发挥了群核科技强大数据集能力&lt;/strong&gt;，通过持续训练不断优化空间识别精度；另一方面&lt;strong&gt;通过接入大语言模型，系统可直接接收并理解自然语言指令&lt;/strong&gt;，使空间理解模型从简单任务执行工具转变为能够真正理解用户意图的智能系统，从而推进了 LLMs 在空间理解和推理方向的能力边界。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0624/141138_pYOw_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;strong&gt;&lt;strong&gt;SpatialLM 模型的网络结构&lt;/strong&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;在训练数据方面，SpatialLM 构建了一个全新的包含 3D 结构化信息的合成点云数据集，打破了真实数据稀缺且难以标注的局限。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1149" src="https://static.oschina.net/uploads/space/2025/0624/141210_bppP_2720166.png" width="974" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;该数据集包含超 1.2 万场景、5.4 万个房间的结构化室内点云数据，其规模远超 ScanNet（仅包含 1,513 个场景）等现有数据集。所有数据均源自真实项目的专业设计模型，经严格筛选与解析后形成符合真实世界统计分布的虚拟环境，相较程序化生成的 ProcTHOR 等数据集具有更高真实性。&lt;/p&gt; 
&lt;p&gt;项目地址：https://manycore-research.github.io/SpatialLM/&lt;br&gt; 报告详情：https://arxiv.org/abs/2506.07491&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/356998</link>
      <guid isPermaLink="false">https://www.oschina.net/news/356998</guid>
      <pubDate>Sat, 10 May 2025 06:12:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>vivo Pulsar 万亿级消息处理实践 (2) - 从 0 到 1 建设 Pulsar 指标监控链路</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;blockquote&gt; 
 &lt;p&gt;作者：vivo 互联网大数据团队- You Shuo&lt;/p&gt; 
 &lt;p&gt;本文是《vivo Pulsar 万亿级消息处理实践》系列文章第 2 篇，Pulsar 支持上报分区粒度指标，Kafka 则没有分区粒度的指标，所以 Pulsar 的指标量级要远大于 Kafka。在 Pulsar 平台建设初期，提供一个稳定、低时延的监控链路尤为重要。&lt;/p&gt; 
 &lt;p&gt;系列文章：&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;《&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzI4NjY4MTU5Nw%3D%3D%26mid%3D2247501335%26idx%3D1%26sn%3D3701be0b8b7b789e29c1ca53ba142e9d%26scene%3D21%23wechat_redirect" target="_blank"&gt;vivo Pulsar 万亿级消息处理实践-数据发送原理解析和性能调优&lt;/a&gt;》&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;本文是基于 Pulsar 2.9.2/kop-2.9.2 展开的。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;一、背景&lt;/h1&gt; 
&lt;p&gt;作为一种新型消息中间件，Pulsar 在架构设计及功能特性等方面要优于 Kafka，所以我们引入 Pulsar 作为我们新一代的消息中间件。在对 Pulsar 进行调研的时候（比如：性能测试、故障测试等），针对 Pulsar 提供一套可观测系统是必不可少的。Pulsar 的指标是面向云原生的，并且官方提供了 Prometheus 作为 Pulsar 指标的采集、存储和查询的方案，但是使用 Prometheus 采集指标面临以下几个&lt;strong&gt;问题&lt;/strong&gt;：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Prometheus 自带的时序数据库不是分布式的，它受单机资源的限制；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Prometheus 在存储时序数据时消耗大量的内存，并且 Prometheus 在实现高效查询和聚合计算的时候会消耗大量的 CPU。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;除了以上列出的可观测系统问题，Pulsar 还有一些指标本身的问题，这些问题&lt;strong&gt;包括&lt;/strong&gt;：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Pulsar 的订阅积压指标单位是 entry 而不是条数，这会严重影响从 Kafka 迁移过来的用户的使用体验及日常运维工作；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Pulsar 没有 bundle 指标，因为 Pulsar 自动均衡的最小单位是 bundle，所以 bundle 指标是调试 Pulsar 自动均衡参数时重要的观测依据；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;kop 指标上报异常等问题。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;针对以上列出的几个问题，我们在下面分别展开敍述。&lt;/p&gt; 
&lt;h1&gt;二、Pulsar 监控告警系统架构&lt;/h1&gt; 
&lt;p&gt;在上一章节我们列出了使用 Prometheus 作为观测系统的局限，由于 Pulsar 的指标是面向云原生的，采用 Prometheus 采集 Pulsar 指标是最好的选择，但对于指标的存储和查询我们使用第三方存储来减轻 Prometheus 的压力，整个监控告警系统架构如下图所示：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//c032b72031868384106c1cc665fafc42.gif" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在整个可观测系统中，各组件的职能如下：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Pulsar、bookkeeper 等组件提供暴露指标的接口&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Prometheus 访问 Pulsar 指标接口采集指标&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;adaptor 提供了服务发现、Prometheus 格式指标的反序列化和序列化以及指标转发远端存储的能力，这里的远端存储可以是 Pulsar 或 Kafka&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Druid 消费指标 topic 并提供数据分析的能力&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;vivo 内部的检测告警平台提供了动态配置检测告警的能力&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;基于以上监控系统的设计逻辑，我们在具体实现的过程中遇到了几个比较&lt;strong&gt;关键的问题：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;**一、**adaptor 需要接收 Pulsar 所有线上服务的指标并兼容 Prometheus 格式数据，所以在调研 Prometheus 采集 Pulsar 指标时，我们基于 Prometheus 的官方文档开发了 adaptor，在 adaptor 里实现了服务加入集群的发现机制以及动态配置 prometheus 采集新新加入服务的指标：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Prometheus 动态加载配置：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fprometheus.io%2Fdocs%2Fprometheus%2Flatest%2Fconfiguration%2Fconfiguration%2F" target="_blank"&gt;Prometheus 配置-官方文档&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Prometheus 自定义服务发现机制：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fprometheus.io%2Fblog%2F2015%2F06%2F01%2Fadvanced-service-discovery%2F" target="_blank"&gt;Prometheus 自定义服务发现-官方文档&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;在可以动态配置 Prometheus 采集所有线上正在运行的服务指标之后，由于 Prometheus 的指标是基于 protobuf 协议进行传输的，并且 Prometheus 是基于 go 编写的，所以为了适配 Java 版本的 adaptor，我们基于 Prometheus 和 go 提供的指标格式定义文件（remote.proto、types.proto 和 gogo.proto）生成了 Java 版本的指标接收代码，并将 protobuf 格式的指标反序列化后写入消息中间件。&lt;/p&gt; 
&lt;p&gt;**二、**Grafana 社区提供的 Druid 插件不能很好的展示 Counter 类型的指标，但是 bookkeeper 上报的指标中又有很多是 Counter 类型的指标，vivo 的 Druid 团队对该插件做了一些改造，新增了计算速率的聚合函数。&lt;/p&gt; 
&lt;p&gt;druid 插件的安装可以参考官方文档（&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgrafana.com%2Fgrafana%2Fplugins%2Fabhisant-druid-datasource%2F" target="_blank"&gt;详情&lt;/a&gt;）&lt;/p&gt; 
&lt;p&gt;**三、**由于 Prometheus 比较依赖内存和 CPU，而我们的机器资源组又是有限的，在使用远端存储的基础上，我们针对该问题优化了一些 Prometheus 参数，这些参数包括：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;--storage.tsdb.retention=30m&lt;/strong&gt;：该参数配置了数据的保留时间为 30 分钟，在这个时间之后，旧的数据将会被删除。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;--storage.tsdb.min-block-duration=5m&lt;/strong&gt;：该参数配置了生成块（block）的最小时间间隔为 5 分钟。块是一组时序数据的集合，它们通常被一起压缩和存储在磁盘上，该参数间接控制 Prometheus 对内存的占用。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;--storage.tsdb.max-block-duration=5m&lt;/strong&gt;：该参数配置了生成块（block）的最大时间间隔为 5 分钟。如果一个块的时间跨度超过这个参数所设的时间跨度，则这个块将被分成多个子块。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;--enable-feature=memory-snapshot-on-shutdown&lt;/strong&gt;：该参数配置了在 Prometheus 关闭时，自动将当前内存中的数据快照写入到磁盘中，Prometheus 在下次启动时读取该快照从而可以更快的完成启动。&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;三、Pulsar 指标优化&lt;/h1&gt; 
&lt;p&gt;Pulsar 的指标可以成功观测之后，我们在日常的调优和运维过程中发现了一些 Pulsar 指标本身存在的问题，这些问题包括准确性、用户体验、以及性能调优等方面，我们针对这些问题做了一些优化和改造，使得 Pulsar 更加通用、易维护。&lt;/p&gt; 
&lt;h2&gt;3.1 Pulsar 消费积压指标&lt;/h2&gt; 
&lt;p&gt;原生的 Pulsar 订阅积压指标单位是 entry，从 Kafka 迁移到 Pulsar 的用户希望 Pulsar 能够和 Kafka 一样，提供以消息条数为单位的积压指标，这样可以方便用户判断具体的延迟大小并尽量不改变用户使用消息中间件的习惯。&lt;/p&gt; 
&lt;p&gt;在确保配置 brokerEntryMetadataInterceptors=&lt;/p&gt; 
&lt;p&gt;org.apache.pulsar.common.intercept.AppendIndexMetadataInterceptor 情况下，Pulsar broker 端在往 bookkeeper 端写入 entry 前，通过拦截器往 entry 的头部添加索引元数据，该索引在同一分区内单调递增，entry 头部元数据示例如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;biz-log-partition-1 -l 24622961 -e 6
Batch Message ID: 24622961:6:0
Publish time: 1676917007607
Event time: 0
Broker entry metadata index: 157398560244
Properties:
"X-Pulsar-batch-size    2431"
"X-Pulsar-num-batch-message    50"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;以分区为指标统计的最小单位，基于 last add confirmed entry 和 last consumed entry 计算两个 entry 中的索引差值，即是订阅在每个分区的数据积压。下面是 cursor 基于订阅位置计算订阅积压的示意图，其中 last add confirmed entry 在拦截器中有记录最新索引，对于 last consumed entry，cursor 需要从 bookkeeper 中读取，这个操作可能会涉及到 bookkeeper 读盘，所以在收集延迟指标的时候可能会增加采集的耗时。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//f954824fee0a365add038a1a9aed4e3b.gif" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;效果&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;上图是新订阅积压指标和原生积压指标的对比，新增的订阅积压指标单位是条，原生订阅积压指标单位是 entry。在客户端指定单条发送 100w 条消息时，订阅积压都有明显的升高，当客户端指定批次发送 100w 条消息的时候，新的订阅积压指标会有明显的升高，而原生订阅积压指标相对升高幅度不大，所以新的订阅积压指标更具体的体现了订阅积压的情况。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//a62f11043d23bcbc8c667854834e2437.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h2&gt;3.2 Pulsar bundle 指标&lt;/h2&gt; 
&lt;p&gt;Pulsar 相比于 Kafka 增加了自动负载均衡的能力，在 Pulsar 里 topic 分区是绑定在 bundle 上的，而负载均衡的最小单位是 bundle，所以我们在调优负载均衡策略和参数的时候比较依赖 bunlde 的流量分布指标，并且该指标也可以作为我们切分 bundle 的参考依据。我们在开发 bundle 指标的时候做了下面两件事情：&lt;/p&gt; 
&lt;p&gt;统计当前 Pulsar 集群非游离状态 bundle 的负载情况对于处于游离状态的 bundle（即没有被分配到任何 broker 上的 bundle），我们指定 Pulsar leader 在上报自身 bundle 指标的同时，上报这些处于游离状态的 bundle 指标，并打上是否游离的标签。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;效果&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//d465e1009a88707edf424e50711bfd36.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;上图就是 bundle 的负载指标，除了出入流量分布的情况，我们还提供了生产者/消费者到 bundle 的连接数量，以便运维同学从更多角度来调优负载均衡策略和参数。&lt;/p&gt; 
&lt;h2&gt;3.3 kop 消费延迟指标无法上报&lt;/h2&gt; 
&lt;p&gt;在我们实际运维过程中，重启 kop 的 Coordinator 节点后会偶发消费延迟指标下降或者掉 0 的问题，从 druid 查看上报的数据，我们发现在重启 broker 之后消费组就没有继续上报 kop 消费延迟指标。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;（1）原因分析&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;由于 kop 的消费延迟指标是由 Kafka lag exporter 采集的，所以我们重点分析了 Kafka lag exporter 采集消费延迟指标的逻辑，下图是 Kafka-lag-exporter 采集消费延迟指标的示意图：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//e1c60c3dfc3fbdfdcbacbe9501bd9c30.gif" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;其中，kafka-lag-exporter 计算消费延迟指标的逻辑会依赖 kop 的 describeConsumerGroups 接口，但是当 GroupCoordinator 节点重启后，该接口返回的 member 信息中 assignment 数据缺失，kafka-lag-exporter 会将 assignment 为空的 member 给过滤掉，所以最终不会上报对应 member 下的分区指标，代码调试如下图所示：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//44161110538352a1751268f3d5e09c35.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//756d27b220d957877e1713dd1ac7e29a.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;为什么 kop/Kafka describeConsumerGroups 接口返回 member 的 assignment 是空的？因为 consumer 在启动消费时会通过 groupManager.storeGroup 写入__consumer_&lt;/p&gt; 
&lt;p&gt;offset，在 coordinator 关闭时会转移到另一个 broker，但另一个 broker 并没有把 assignment 字段反序列化出来（序列化为 groupMetadataValue，反序列化为 readGroupMessageValue），如下图：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//378c807ffd55bb262d54d26e52e6d73a.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;（2）解决方案&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在 GroupMetadataConstants#readGroup-&lt;/p&gt; 
&lt;p&gt;MessageValue() 方法对 coordinator 反序列化消费组元数据信息时，将 assignment 字段读出来并设置（序列化为 groupMetadataValue，反序列化为 readGroupMessageValue），如下图：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//97fe15c2e044eb609406a2ab3d5e51e8.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h1&gt;四、总结&lt;/h1&gt; 
&lt;p&gt;在 Pulsar 监控系统构建的过程中，我们解决了与用户体验、运维效率、Pulsar 可用性等方面相关的问题，加快了 Pulsar 在 vivo 的落地进度。虽然我们在构建 Pulsar 可观测系统过程中解决了一部分问题，但是监控链路仍然存在单点瓶颈等问题，所以 Pulsar 在 vivo 的发展未来还会有很多挑战。&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/vivotech/blog/18619289</link>
      <guid isPermaLink="false">https://my.oschina.net/vivotech/blog/18619289</guid>
      <pubDate>Sat, 10 May 2025 03:41:00 GMT</pubDate>
      <author>原创</author>
    </item>
  </channel>
</rss>
