<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>开源中国-综合资讯</title>
        <link>https://www.oschina.net/news/industry</link>
        <atom:link href="http://8.134.148.166:30044/oschina/news/industry" rel="self" type="application/rss+xml"></atom:link>
        <description>开源中国-综合资讯 - Powered by RSSHub</description>
        <generator>RSSHub</generator>
        <webMaster>contact@rsshub.app (RSSHub)</webMaster>
        <language>en</language>
        <lastBuildDate>Wed, 16 Apr 2025 21:37:07 GMT</lastBuildDate>
        <ttl>5</ttl>
        <item>
            <title>xAI 发布新 AI 工具 Grok Studio：可生成文档、代码和浏览器游戏</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;xAI 宣布为旗下 AI 聊天助手 Grok 增加全新功能 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Fgrok%2Fstatus%2F1912318583532872166&quot; target=&quot;_blank&quot;&gt;Grok Studio&lt;/a&gt;，可以用于编辑和创建文档，以及基础应用程序。&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-19a1c88ab76742bcfab01237646b8b0dc40.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Grok Studio 将在一个单独的窗口中打开，支持生成文档、代码、报告和浏览器游戏。&lt;/p&gt; 
&lt;p&gt;生成代码时，Grok Studio 会在「预览」选项卡中快速向用户展示其运行效果。HTML 代码片段可以运行 Python、C++、JavaScript、Typescript 和 Bash 脚本，也可以在此预览选项卡中查看。所有新项目都会在 Grok 回复的右侧打开。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;1424&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0416/194259_TbOH_2720166.png&quot; width=&quot;1940&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;xAI 表示，免费和付费的 Grok 用户都可以在 Grok.com 上使用该功能。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/344972/xai-grok-studio</link>
            <guid isPermaLink="false">https://www.oschina.net/news/344972/xai-grok-studio</guid>
            <pubDate>Sun, 13 Apr 2025 11:43:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>腾讯「元宝」可添加为微信好友：一键解析公众号文章、甚至把它置顶</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;就在刚刚，腾讯 AI 助手「元宝」支持添加为微信好友进行聊天。 &amp;nbsp;你可以和他对话，也可以发链接、文件给他——甚至可以把它置顶 。&lt;/p&gt; 
&lt;p&gt;如下图，在微信直接搜索「元宝」，点击「聊天」进入。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0416/190446_4eYx_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0416/191425_HCo3_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;1592&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0416/190642_X4B1_2720166.png&quot; width=&quot;806&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;978&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0416/190707_Op4X_2720166.png&quot; width=&quot;814&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;据介绍，这是腾讯元宝 APP 入驻微信的 AI 助手，搭载了混元和 DeepSeek 双模引擎，可一键解析公众号文章和任何图片和文档，短评后会发送详解文章，支持对解读内容做各种智能互动，支持陪伴互动。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0416/191518_XlEd_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;1662&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0416/191457_Jiq2_2720166.png&quot; width=&quot;764&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/344970</link>
            <guid isPermaLink="false">https://www.oschina.net/news/344970</guid>
            <pubDate>Sun, 13 Apr 2025 11:07:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>中国团队自研 AI 图像生成大模型 HiDream-I1 正式开源</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;HiDream 智象未来团队宣布正式开源图像生成大模型 HiDream-I1 与交互编辑模型 HiDream-E1。&lt;/p&gt; 
&lt;p&gt;HiDream-I1 在权威榜单 Artificial Analysis 中 24 小时内&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F-ouXGp3kyyT7AfFmQ_Y5Cw&quot; target=&quot;_blank&quot;&gt;登顶&lt;/a&gt;&lt;/u&gt;，成为首个跻身全球第一梯队的中国自研生成式 AI 模型，并在图像质量、语义理解、艺术表现三大维度刷新行业纪录，实现图像的多风格生成，涵盖动漫、肖像、科幻等场景。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;984&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0416/183653_vhQa_2720166.png&quot; width=&quot;1462&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;目前，设计工具 Recraft 已集成 HiDream 模型，用户 3 步即可实现 「一键出图 + 智能编辑」。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0416/175700_50WE_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;HiDream-I1&amp;nbsp; 已开源三个版本的模型，分别是：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0416/175710_8HLD_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;其中 HiDream-I1-Full 是由 &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhidreamai.com%2Fhome&quot; target=&quot;_blank&quot;&gt;HiDream.a&lt;/a&gt;i 团队发布的开源图像生成基础模型，具备 170 亿参数，旨在实现高质量的图像生成。该模型采用 Diffusion Transformer（DiT）架构，支持多种风格的图像生成，包括写实、卡通、艺术等，适用于多种创作场景。&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;核心特性&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;卓越的图像质量&lt;/strong&gt;：在多个基准测试中表现出色，HPS v2.1 平均得分为 33.82，优于 SDXL、DALL·E 3 等主流模型&amp;nbsp;。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;强大的提示词理解能力&lt;/strong&gt;：在 GenEval 和 DPG-Bench 等评测中，HiDream-I1 的表现优于其他开源模型，展示了其在理解和执行复杂提示词方面的能力。腾讯网+1 阿里云开发者社区-云计算社区-阿里云+1&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;开源且商业友好&lt;/strong&gt;：采用 MIT 许可证，允许用户在个人、科研和商业项目中自由使用生成的内容。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;strong&gt;性能评估&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;在多个评测中，HiDream-I1 展示了其强大的性能：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;DPG-Bench&lt;/strong&gt;：在整体、实体、属性等多个维度上得分领先，展示了其在图像生成质量方面的优势。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;GenEval&lt;/strong&gt;：在单目标、双目标、计数、颜色等任务中表现优异，反映了其对提示词的准确理解和执行能力。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;HPS v2.1&lt;/strong&gt;：在动画、概念艺术、绘画、照片等风格的图像生成中，HiDream-I1 的得分均高于其他主流模型，展示了其多风格生成的能力。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0416/175722_YQIr_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0416/175731_9IFw_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0416/175741_2jbr_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;HiDream-I1-Full 模型整体采用 MIT 协议开源，可自由商用，但部分依赖组件（如 LLaMA3 编码器）需遵守各自协议，商用前应留意其具体限制。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/344955</link>
            <guid isPermaLink="false">https://www.oschina.net/news/344955</guid>
            <pubDate>Sun, 13 Apr 2025 09:58:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>字节 AI Lab 将全部并入 Seed</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;AI 科技评论独家获悉，字节 AI Lab 即将全部收归 Seed 团队下。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;字节 AI Lab 是 Seed 成立之前字节主要的 AI 研发部门，目前由李航管理，自 2024 年开始向 Seed 时任负责人朱文佳汇报。今年 2 月下旬，原 Google DeepMind 副总裁吴永辉入职字节，成为 Seed 基础研究负责人。此后李航的汇报对象变为吴永辉。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;字节 AI Lab 成立于 2016 年，最初由微软亚洲研究院前常务副院长马维英负责，直接向张一鸣汇报。 AI lab 目前有多个子团队，包括机器人、AI4S 等方向，几乎覆盖人工智能领域所有前沿技术研究。2018 年其团队规模达到 150 人，为字节跳动 AI 研究的核心部门。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;AI Lab 主要研究重点是开发为字节跳动内容平台服务的创新技术，字节推荐算法、短视频特效等功能均脱胎于此。其研究成果应用于今日头条、抖音等产品，是支持抖音成长为国民级应用的基石，并奠定了当时字节在国内 AI 领域的领先地位。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;随着抖音、TikTok 占据绝对优势的市场地位，流量商业化成为字节面临的 Top 级问题，AI Lab 在字节内部重要性下降。2020 年，AI Lab 定位从集团级前瞻性项目转为技术中台，为字节商业化团队业务提供支持，马维英的汇报对象也从张一鸣变为抖音负责人张楠。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;2020 年年中，马维英离开字节，AI Lab 负责人一职由李航接任至今。之后团队重组，2023 年开始，AI Lab 下属负责大语言模型的 NLP 组及开发视频生成模型的 PixleDance 被先后转入 Seed 之下。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;同时为了应对新一轮大模型竞争，字节决定回归「始终创业」的价值观，建立独立的新组织，于是加快筹建了独立于原有组织架构的 Flow 和 Seed，前者做 AI 产品，后者做大模型研发。截至 2023 年底，两者已成为与抖音、TikTok、火山引擎等字节各大业务平级的组织。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Seed 自成立就在不断吸纳来自字节内外的人才。除收拢搜索、AML、AI Lab 等内部部门中大模型方向人才外，对外也在积极争抢人才。以面向应届博士的 Top Seed 招募计划为例，字节会给优秀候选人 3-1 职级，薪资不低于百万元。截至 2024 年底，字节 AI 研究者中超 40％比例是近两年加入的新人，对人才的渴求和重视程度可见一斑。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;根据 AI 科技评论调查，加入字节以来，吴永辉已在字节署名三篇论文，均在强化学习方向。吴永辉于上月在 Seed 内部新建虚拟小组、缩短了汇报流程，创建一个更扁平的汇报体系，此次 AI Lab 将全部并入 Seed，也是吴永辉调整内部组织架构的一个重要举措。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/344946</link>
            <guid isPermaLink="false">https://www.oschina.net/news/344946</guid>
            <pubDate>Sun, 13 Apr 2025 09:17:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>Notion Mail 正式发布：AI 驱动邮箱新体验</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;Notion 正式推出电子邮件服务 Notion Mail，首发登陆 macOS 平台，iOS 和 Android 版即将上线。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;1140&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0416/165124_3l5R_2720166.png&quot; width=&quot;2124&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Notion Mail 并非要取代 Gmail，而是作为重新设计的邮箱前端，提供独特的邮件管理体验。其核心为高度模块化系统，用户可自定义收件箱配置，并整合了丰富的 AI 功能，如智能文件夹、自动分类、快速回复、写作改进及智能会议安排等。产品与 Notion Calendar 无缝衔接，核心 AI 功能提供免费使用限额，无限制需订阅付费。目前仅支持英文，未来将扩展至 13 种语言。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0416/165012_T6gr_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Notion Mail 主页：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.notion.com%2Fproduct%2Fmail&quot; target=&quot;_blank&quot;&gt;https://www.notion.com/product/mail&lt;/a&gt;&lt;br&gt; 下载地址：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.notion.com%2Fproduct%2Fmail%2Fdownload&quot; target=&quot;_blank&quot;&gt;https://www.notion.com/product/mail/download&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/344936/notion-mail</link>
            <guid isPermaLink="false">https://www.oschina.net/news/344936/notion-mail</guid>
            <pubDate>Sun, 13 Apr 2025 08:55:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>从理论到落地：MCP 实战解锁 AI 应用架构新范式</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                    
                                                                                                                                                    &lt;p&gt;作者：计缘&lt;/p&gt; 
&lt;p&gt;编者按：应用越智能，背后的设计会越复杂。软件的本质是解决复杂性问题，MCP 虽打开了智能的创意上限，但也给后端的设计带来了无限的复杂度。本文旨在从 MCP 的技术原理、降低 MCP Server 构建复杂度、提升 Server 运行稳定性等方面出发，分享我们的一些实践心得。文章内容较长，以下是导读大纲。（点击阅读原文，获取 78 页完整版 PPT）&lt;/p&gt; 
&lt;p&gt;1、介绍 MCP 的概念及其运作机制。&lt;/p&gt; 
&lt;p&gt;2、解释 MCP 和 Function Calling 之间的区别。&lt;/p&gt; 
&lt;p&gt;3、讲述 MCP 的本质和挑战，包括描述 MCP 信息的系统提示词的挑战，MCP Client 与 MCP Server 之间协同关系的挑战，快速构建 MCP Server，自建 Dify 的痛点等。&lt;/p&gt; 
&lt;p&gt;4、分析如何解决 MCP 的各个挑战，包括 MCP Register、MCP Server 和 Promt 的统一管理、MCP 效果验证体系和安全性保障、MCP 网关、MCP Server 的动态服务发现、Streamable HTTP、弹性效率、可观测等。&lt;/p&gt; 
&lt;p&gt;5、最后探讨 MCP 对 AI 应用架构新范式的影响，并介绍 MCP Server First 的理念。&lt;/p&gt; 
&lt;h2&gt;AI Agent 现状及架构&lt;/h2&gt; 
&lt;p&gt;人工智能（AI）在商业领域的应用正日益成为推动创新和效率提升的核心力量。其核心在于多个 AI Agent 的协作，这些 AI Agent 通过分工与合作，共同承载 AI 应用所支持的业务需求。这种协作模式不仅优化了企业运营，还展现了 AI 在解决高影响力挑战中的潜力。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-76b1a0b5eb99a3069f519e05de7f8273c8f.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;当前的 AI Agent，无论是和各种 Tools（各类业务服务接口）交互，还是和各类 Memory（各类存储服务接口）交互，亦或是和各类 LLMs（各类大语言模型）交互，都是通过 HTTP 协议的，除了 LLM 因为基本都遵循 OpenAI 范式以外，和其他的 Tools 和 Memory 交互都需要逐一了解它们的返回格式进行解析和适配。当一个 AI 应用包含多个 AI Agent 时，或者一个 AI 应用需要和多个业务服务接口和存储服务接口交互时，整体的开发工作量是很大的，主要体现在 3 个方面：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;找适合该 AI 应用的业务接口和存储服务接口：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;找三方服务接口。&lt;/li&gt; 
   &lt;li&gt;在公司内部找合适的服务的接口。&lt;/li&gt; 
   &lt;li&gt;找不到就自己先开发接口。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;解析接口的返回格式：无论是三方服务接口还是公司内部的服务接口，返回格式可能都千奇百怪，需要逐一进行了解和解析。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;编排多个 AI Agent：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;有 Dify 这类流程可视化的工具辅助编排，减轻了很多编排工作，但复杂度依然不低，且运行效率和性能方面还是有瓶颈的。&lt;/li&gt; 
   &lt;li&gt;通过编码方式做编排（比如使用 Spring AI Alibaba 或 LangChain 等），虽然性能上更优，但是复杂度更高，编排效率和灵活性都有不足。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;所以目前很多 AI 应用就只有少数几个 AI Agent，甚至很多 AI 应用背后就只有一个 AI Agent。这也是目前 AI 应用背后的 AI Agent 依然还处在第一个阶段（Siloed, Single-Purpose Agents）的原因。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-e25347863ed2a9c56ba0532f785a792e3ed.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;为了能使 AI Agent 进入到第二阶段（Platform-Level Agents），我们使用云原生 API 网关做了统一的接入层，通过一个网关三种不同角色的方式，解决了一部分复杂度：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;作为南北向流量网关，统一管理 AI Agent 的入口流量，核心做转发、负载、鉴权认证、安全、流控等。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;作为 AI 网关，代理各类 LLMs，向 AI Agent 屏蔽了繁杂的接入，并且解决了很多生产级的问题，比如多模型切换、模型 Fallback、多 API Key 管理、安全、联网搜索等。&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;AI 网关代理 LLMs 的详细文章参见：&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzUzNzYxNjAzMg%3D%3D%26mid%3D2247573215%26idx%3D1%26sn%3Df77c5dd8423a9480afb6a03fce0d997c%26scene%3D21%23wechat_redirect&quot; target=&quot;_blank&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzUzNzYxNjAzMg%3D%3D%26mid%3D2247573215%26idx%3D1%26sn%3Df77c5dd8423a9480afb6a03fce0d997c%26scene%3D21%23wechat_redirect&quot; target=&quot;_blank&quot;&gt;https://mp.weixin.qq.com/s/tZ0wsTlZK67r9IxNZ57TDQ&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;作为东西向网关，统一管理来自不同源（ACK、ECS、函数计算 FC、SAE、三方服务）的各类服务，供 AI Agent 使用。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;但如我所说，这只解决了一部分复杂度问题，更核心的&lt;strong&gt;找接口&lt;/strong&gt; 和&lt;strong&gt;解析接口&lt;/strong&gt;这两个问题依然没有解决。直到 MCP（Model Context Protocol）的出现，让我们看到了真正通往第二阶段（Platform-Level Agents）的路，甚至可以尝试触摸第三阶段（Universal Agents, Multi-Agents）。&lt;/p&gt; 
&lt;h2&gt;MCP 是什么&lt;/h2&gt; 
&lt;p&gt;MCP 是模型上下文协议（Model Context Protocol）的简称，是一个开源协议，由 Anthropic（Claude 开发公司）开发，旨在让大型语言模型（LLM）能够以标准化的方式连接到外部数据源和工具。它就像 AI 应用的通用接口，帮助开发者构建更灵活、更具上下文感知能力的 AI 应用，而无需为每个 AI 模型和外部系统组合进行定制集成。MCP 被设计为一个通用接口，类似于 USB-C 端口，允许 LLM 应用以一致的方式连接到各种数据源和工具，如文件、数据库、API 等。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-46b7eef569ef0b1f4357d3f79f965843530.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;MCP 目前一共有 3 个核心概念：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;MCP Server：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;基于各语言的 MCP SDK 开发的程序或服务。&lt;/li&gt; 
   &lt;li&gt;基于某种&lt;strong&gt;神秘的机制&lt;/strong&gt;将现存的程序或服务进行了转换，使其成为了 MCP Server。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;MCP Tool：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;MCP Tool 所属于 MCP Server，一个 MCP Server 可以有多个 MCP Tool。可以理解为一个类里有多个方法，或者类似一个服务里有多个接口。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;MCP Client：当一段代码，一个 Agent，一个客户端，基于 MCP 的规范去使用、去调用 MCP Server 里的 MCP Tool 时，它就是 MCP Client。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;MCP 的运作机制&lt;/h3&gt; 
&lt;p&gt;要真正理解 MCP 是什么，我们需要了解它的运作机制，然后你就能知道 MCP 的调用方式和传统的 HTTP 调用方式有什么不同，可能也能隐约体会到为什么我说 MCP 可以让 AI Agent 进入第二阶段。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-3316ba085c95379e9635dfbbefb726a0388.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;如上图所示，一次基于 MCP 的调用，一共有 6 个核心的步骤。我们先拟定一个前提：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;我要开发一个获取时间的 AI Agent，用户在使用这个 AI Agent 时，只需要问类似&quot;现在几点了？&quot;这种问题即可。&lt;/li&gt; 
 &lt;li&gt;我已经有了一个关于处理时间的 MCP Server，这个 MCP Server 里有 2 个 MCP Tool，一个负责获取当前时区，一个负责获取当前时间。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;调用步骤解析：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;第一步：用户向 AI Agent 问&quot;现在几点了？&quot;，此时 AI Agent 就是 MCP Client，它会把用户的问题和处理时间的 MCP Server 以及 MCP Tool 的信息一起发送给 LLM。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;第二步：LLM 拿到信息后开始推理，基于用户的问题和 MCP Server 的信息，选出解决用户问题最合适的那个 MCP Server 和 MCP Tool，然后返回给 AI Agent（MCP Client）。&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;这里 LLM 返回给 AI Agent 的信息是：&quot;你用 time 这个 MCP Server 里的 get_current_time 这个 MCP Tool 吧，它可以解决用户的问题&quot;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;第三步：AI Agent（MCP Client）现在知道该使用哪个 MCP Server 里的哪个 MCP Tool 了，直接调用那个 MCP Tool，获取结果。&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;调用 time 这个 MCP Server 里的 get_current_time 这个 MCP Tool。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;第四步：Time MCP Server 返回结果（当前的时间）给 AI Agent（MCP Client）。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;第五步：AI Agent（MCP Client）也很懒啊，把用户的问题和从 Time MCP Server 处拿到的结果再一次给了 LLM，目的是让 LLM 结合问题和答案再规整一下内容。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;第六步：LLM 把规规整整的内容返回给 AI Agent（MCP Client），最后 AI Agent（MCP Client）再原封不动的返回给了用户。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;在 MCP 的整个调用过程中有一个非常核心的点就是 MCP Server 以及 MCP Tool 的信息。从第一步，第二步可以看出，这个信息非常关键，是它让 LLM 知道了该如何解决用户的问题，这个信息就是 MCP 中最重要的 System Prompt，本质上就是 PE 工程。&lt;/p&gt; 
&lt;h3&gt;MCP System Prompt&lt;/h3&gt; 
&lt;p&gt;MCP 不像传统的协议定义，它没有一个确定的数据结构。它的核心是通过自然语言描述清楚有哪些 MCP Server，承担什么作用，有哪些 MCP Tool，承担什么作用，然后让大语言模型通过推理去选择最合适的 MCP Server 以及 MCP Tool。所以它的核心本质上还是提示词工程。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-120aa561c4f9d67cebd623ba4cb5ceab2eb.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-86391324e806a590732d3b9ffd5e4358359.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;上面两张图是 Cline（一个 MCP Client）中的 System Prompt，可以清晰的看到它对 MCP Server 和 MCP Tool 都有明确的描述。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-8c589278bba00c8154935bf90b5922f5e85.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;上图是流程中的第一步，将用户的问题和 System Prompt 一起发送给 LLM 的内容。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-7f1b4a7b3fc2d4626d822b43aee01b16cef.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;上图是流程中的第二步，LLM 返回了解决用户问题明确的 MCP Server 和 MCP Tool 信息。&lt;/p&gt; 
&lt;h3&gt;MCP 和 Function Calling 之间的区别&lt;/h3&gt; 
&lt;p&gt;看到这，我想大家应该对 MCP 是什么有一定感觉了。MCP 是不是解决了&lt;strong&gt;找接口&lt;/strong&gt; 和&lt;strong&gt;解析接口&lt;/strong&gt;的问题？因为这两个工作都交给了 LLM。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;LLM 负责帮 AI Agent 找到最合适的接口。&lt;/li&gt; 
 &lt;li&gt;AI Agent 调用接口，压根不用做返回结果的解析，原封不动再交给 LLM。&lt;/li&gt; 
 &lt;li&gt;LLM 结合用户问题和接口返回的结果，做内容规整处理。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;那么可能有小伙伴会问，MCP 和 LLM 的 Function Calling 又有什么区别呢？核心区别是是否绑定模型或模型厂商：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;MCP 是通用协议层的标准，类似于&quot;AI 领域的 USB-C 接口&quot;，定义了 LLM 与外部工具 / 数据源的通信格式，但&lt;strong&gt;不绑定任何特定模型或厂商&lt;/strong&gt;，将复杂的函数调用抽象为客户端-服务器架构。&lt;/li&gt; 
 &lt;li&gt;Function Calling &lt;strong&gt;是大模型厂商提供的专有能力&lt;/strong&gt;，由大模型厂商定义，不同大模型厂商之间在接口定义和开发文档上存在差异；允许模型直接生成调用函数，触发外部 API，依赖模型自身的上下文理解和结构化输出能力。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-b8eff8e848cb828269fb1e47357cb5a7ddb.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;如上图所示，LLM Function Calling 需要 LLM 为每个外部函数编写一个 JSON Schema 格式的功能说明，精心设计一个提示词模版，才能提高 Function Calling 响应的准确率，如果一个需求涉及到几十个外部系统，那设计成本是巨大，产品化成本极高。而 MCP 统一了客户端和服务器的运行规范，并且要求 MCP 客户端和服务器之间，也统一按照某个既定的提示词模板进行通信，这样就能通过 MCP 加强全球开发者的协作，复用全球的开发成果。&lt;/p&gt; 
&lt;h3&gt;MCP 的本质和挑战&lt;/h3&gt; 
&lt;p&gt;根据上文的一系列解释，我们可以总结一下 MCP 的本质：&lt;strong&gt;模型上下文协议（Model Context Protocol）并不是一个确定的数据格式或数据结构，它是&lt;/strong&gt; 描述 MCP Server/MCP Tool 信息的系统提示词&lt;strong&gt;和&lt;/strong&gt; MCP Server 与 LLM 之间的协同关系&lt;strong&gt;的结合&lt;/strong&gt; &lt;strong&gt;，&lt;/strong&gt; &lt;strong&gt;解决的是&lt;/strong&gt; 找接口&lt;strong&gt;和&lt;/strong&gt; 解析接口&lt;strong&gt;的问题&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-dd0578fcdb7e680a0c9312663e7727d8586.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;明确了 MCP 本质之后，将其带入到企业级生产应用中，你就会发现，这两个核心点上会有很多挑战，或者说不足。&lt;/p&gt; 
&lt;h4&gt;描述 MCP 信息的系统提示词的挑战&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;系统提示词的安全性如何保证？&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;这个最核心的系统提示词如果被污染了，LLM 就不能准确知道你有哪些 MCP Server，有哪些 MCP Tool，甚至可能告诉 LLM 错误的，有安全漏洞的 MCP Server 和 MCP Tool，那么对你的 AI 应用来说将是巨大的风险，会导致整个 MCP 流程的瘫痪。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;系统提示词如何管理？&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;MCP Server 或者 MCP Tool 有了新版本，系统提示词应该也许要有对应的版本管理策略。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;系统提示词写的不好，如何方便的快速调试？能不能实时生效？&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;系统提示词是没有标准定义的，理论上每个企业可以定义自己的系统提示词模板，类似 PE 工程。提示词不可能一次性就能写好，需要反复调试，需要有机制做快速的调整，并且可以做到使其实时生效。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;如果 MCP Server 很多，那么系统提示词会非常长，岂不是很消耗 Token？如何缩小或精确 MCP Server 和 MCP Tool 的范围？&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;如果你有几十个或更多 MCP Server，那么就有可能有上百个或更多 MCP Tool，所有的信息描述下来放在系统提示词后，这个提示词模板会非常大，显而易见的对 Token 消耗非常大，变相的就是成本高。应该需要一套机制，基于用户的问题，预圈选 MCP Server 和 MCP Tool 的范围，减少 Token，提高效率，很类似联网搜索里的意图识别。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;MCP Client 与 MCP Server 之间协同关系的挑战&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;负责做协同的是 MCP Client，但目前 MCP Client 很少，比如 Cline， Claude，Cursor 等，而且都是 C/S 工具，支持的都是 SSE 协议，企业级的 AI 应用该如何结合？能不能结合？&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;基本上目前市面中的 MCP Client 都无法和企业级的 AI 应用做结合，SSE 这种有状态的协议有很多弊端，比如不支持可恢复性，服务器需要维持长期连接，仅支持服务器 → 客户端消息，无法灵活进行双向通信等。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;现存的传统业务能快速转成 MCP Server 吗？能 0 代码改动的转换吗？&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;开发一个 MCP Server 是强依赖各语言的 MCP SDK 的，目前只支持 Python、Java、TS、Kotlin、C#。那如果是 Go 或者 PHP 技术栈的企业怎么办？并且那么多现存的业务全部用 MCP SDK 重构一遍，工作量巨大，也很不现实。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;MCP Server 会很多，如何统一管理？&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;有自己开发的 MCP Server，有三方的 MCP Server，还有大量通过某种神秘机制将传统业务转换而来的 MCP Server。这些都应该有一个类似 MCP Hub 或 MCP 市场的东西统一管理起来，方便 MCP Client 去使用。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;企业级 AI 应用中，身份认证、数据权限、安全这些如何做？&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;在企业级的应用中，无论哪种协议，哪种架构，哪种业务。身份认证、数据权限、安全防护这些问题都是永远绕不开的。那么在 MCP 这种协同方式下如何实现。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;AI 应用架构新范式&lt;/h2&gt; 
&lt;p&gt;我们结合 MCP 范式，以解决上述挑战点为目的，将 AI Agent 的架构进行了重构。在云原生 API 网关 &lt;strong&gt;，&lt;/strong&gt; 微服务引擎 Nacos &lt;strong&gt;两个产品中做了 MCP 增强能力，解决了上述大部分的挑战点。在&lt;/strong&gt; 函数计算 FC &lt;strong&gt;，&lt;/strong&gt; Serverless 应用引擎 SAE &lt;strong&gt;两个产品中做了 MCP 增强能力，前者解决快速开发 MCP Server 的问题，后者解决开源 Dify 性能的问题。共同构建了基于 MCP 的&lt;/strong&gt; AI 应用开发新范式。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-03e42880c942d913877ccf06ee2afa243e7.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;AI 应用架构新范式剖析&lt;/h3&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-ff32b45cf513d2b21df5bf3986ed678137a.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;首先我对图中的 8 步核心调用链路做以解析：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;第一步：用户向 AI 应用发起请求，请求流量进入流量网关（云原生 API 网关）。&lt;/li&gt; 
 &lt;li&gt;第二步：云原生 API 网关侧维护管理了不同类型的 AI Agent 的 API 或路由规则，将用户请求转发至对应的 AI Agent。&lt;/li&gt; 
 &lt;li&gt;第三步：AI Agent 无论以哪种方式实现，只要其中的节点需要获取数据，便向 MCP 网关（云原生 API 网关）请求获取可用的 MCP Server 及 MCP Tool 的信息。&lt;/li&gt; 
 &lt;li&gt;第四步：因为 MCP 网关处可能维护了很多 MCP 信息，可以借助 LLM 缩小 MCP 范围，减少 Token 消耗，所以向 AI 网关（云原生 API 网关）发请求和 LLM 交互。（这一步可选）&lt;/li&gt; 
 &lt;li&gt;第五步：MCP 网关将确定好范围的 MCP Server 及 MCP Tool 的信息 List 返回给 AI Agent。&lt;/li&gt; 
 &lt;li&gt;第六步：AI Agent 将用户的请求信息及从 MCP 网关拿到的所有 MCP 信息通过 AI 网关发送给 LLM。&lt;/li&gt; 
 &lt;li&gt;第七步：经过 LLM 推理后，返回解决问题的一个或多个 MCP Server 和 MCP Tool 信息。&lt;/li&gt; 
 &lt;li&gt;第八步：AI Agent 拿到确定的 MCP Server 和 MCP Tool 信息后通过 MCP 网关对该 MCP Tool 做请求。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;实际生产中 ③ - ⑧ 步会多次循环交互。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-10882473c8c301e063c2f4542b92db86114.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;我们依然基于 MCP 的两个本质来刨析这个新的架构。&lt;/p&gt; 
&lt;h4&gt;如何解决 MCP 提示词的各个挑战&lt;/h4&gt; 
&lt;p&gt;我们团队是中间件开源最多的团队，比如 Nacos，Higress，Sentinel，RocketMQ，Seata 等，并且还维护着 Spring Cloud Alibaba，Spring AI Alibaba，Dubbo 这些开源开发框架，在微服务架构领域有着丰富的经验。所以在 MCP Server 和 MCP 提示词统一管理这个点上，天然的就想到了微服务领域里基于 Nacos 做服务注册发现和配置统一管理的模式，我们将其转嫁到了 MCP 范式，大家可以想一下以下这些对应关系：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;SpringCloud 服务/Dubbo 服务/Go 服务 -&amp;gt; 各类 MCP Server&lt;/li&gt; 
 &lt;li&gt;SpringCloud 服务/Dubbo 服务/Go 服务暴露的接口 -&amp;gt; 各类 MCP Server 提供的 MCP Tool&lt;/li&gt; 
 &lt;li&gt;SpringCloud 服务/Dubbo 服务/Go 服务暴露的接口描述 -&amp;gt; 各类 MCP Server 提供的 MCP Tool 的描述&lt;/li&gt; 
 &lt;li&gt;SpringCloud 服务/Dubbo 服务/Go 服务的配置文件 -&amp;gt; 各类 MCP Server 的系统提示词&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-6116237b6f29c9c47ddc3920068838f27b1.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;所以在 MSE Nacos 这个产品中，我们做了一系列增强 MCP 的能力，使 MSE Nacos 成为统一管理 MCP Server 的 MCP Register（MCP Server 注册/配置中心）。是 AI 应用开发新范式的核心组件。&lt;/p&gt; 
&lt;p&gt;另外，MCP 官方的 Roadmap 中，也在规划 MCP Register 的能力，我们会基于 Nacos 作为 MCP Register 的方案和 MCP 在开源侧进行共建。&lt;/p&gt; 
&lt;h5&gt;MCP Register（MCP Server 注册/配置中心）&lt;/h5&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-c723b1909c83c053b67c2e0435ec0551977.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h5&gt;MCP Server 统一管理&lt;/h5&gt; 
&lt;p&gt;MCP Server 注册到 MSE Nacos 有两种方式：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;在 MSE Nacos 控制枱手动创建。也就是将 MCP Server 的 Endpoint 配置到 MSE Nacos 中。&lt;/li&gt; 
 &lt;li&gt;通过 Nacos SDK，自动将 MCP Server 注册进 Nacos。和当前 Java SpringCloud，Java Dubbo 服务逻辑一样。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;在 MSE Nacos 中对 MCP Server 进行统一管理，可以实现对 &lt;strong&gt;MCP Server 的健康检查，负载均衡，描述信息 Json 向 XML 转换，MCP Server 上下线管控等功能&lt;/strong&gt;。&lt;/p&gt; 
&lt;h5&gt;MCP Prompt 统一管理&lt;/h5&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-693257655e2f075fee6fe4cf19c69710116.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;在 MSE Nacos 中维护 MCP Server 的 Prompt 有两种方式：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;手动创建 MCP Server 的配置信息，配置文件的 Data ID 的命名格式为&lt;code&gt;[MCP Server name]-mcp-tools.json&lt;/code&gt;。&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;在配置文件中管理 MCP Tool 的提示词信息，比如整体作用描述，入参描述等。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;结合 MSE 治理的能力，如果是 Java 或者 Go，可以自动感知服务的 Schema，自动生成 MCP Server 和 MCP Tool 的提示词信息。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;在 MSE Nacos 中对 MCP Server 提示词进行统一管理，可以实现 &lt;strong&gt;MCP 提示词版本管理（回滚），MCP 提示词灰度管理，MCP 提示词安全管理，MCP 提示词动态调优实时生效等功能&lt;/strong&gt;。&lt;/p&gt; 
&lt;h5&gt;MCP 效果验证体系（进行中）&lt;/h5&gt; 
&lt;p&gt;上文中提到当 MCP Server 很多时，MCP Server 的各描述信息会很多，也就是 Prompt 会很长，Token 消耗很大，所以需要有机制基于用户的输入缩小 MCP Server 范围，减少 Token 消耗，增加 LLM 推理效率。除此以外，大家知道，只要是和 LLM 交互的场景，提示词的好坏是需要多次调试的，MCP 的整个流程强依赖提示词工程，如果提示词调整不好，LLM 无法返回准确的 MCP Server 和 MCP Tool，那么整个流程就是不可用的状态了。所以在 Nacos 中我们正在做一个 MCP 效果验证的体系。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-168928facd996f1d8f0df17cc8fedd1005e.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;核心的原理是我们会提供一个基于 Spring AI Alibaba 开发的 AI Agent，通过用户配置的业务输入、LLM、圈定的 MCP Server 和 MCP Tool 的集合不断的做验证，将结果以视图的方式展现出来（比如成功率等）。用户可以在 Nacos 中动态的对成功率低的 MCP Server 的提示词做调整优化。&lt;/p&gt; 
&lt;h5&gt;MCP 安全性保障（持续完善中）&lt;/h5&gt; 
&lt;p&gt;无论哪种架构，哪种模式，安全性在企业生产中必然都是第一位的，MCP 领域也不例外，并且需要考虑的环节更多。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-2fa44d5a0dbc05da8d02a3a41e69d32e0ce.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;MCP Server 敏感信息安全管理：注册进 MSE Nacos 的各类 MCP Server 都会有类似 API Key、AK/SK、密钥、登录密码等敏感信息。MSE Nacos 和阿里云 KMS 深度集成，可以对这些敏感信息做加密处理。&lt;/li&gt; 
 &lt;li&gt;MCP Prompt 安全管理：同样依托于 MSE Nacos 和 KMS 的深度集成，可以将 MCP Server，MCP Tool 完整的 Prompt（描述信息）做加密处理，避免 Prompt 污染。&lt;/li&gt; 
 &lt;li&gt;MCP Prompt 安全校验：结合上述的验证体系以及与内容安全做集成，实现 MSE Nacos 对 MCP Server 的 Prompt 的合法性校验。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;如何解决 MCP Client 与 MCP Server 之间协同关系的挑战&lt;/h4&gt; 
&lt;p&gt;在 MCP 范式中，其实是三个角色在互相协同：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;MCP Client -&amp;gt; LLM&lt;/li&gt; 
 &lt;li&gt;MCP Client -&amp;gt; MCP Server&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;这两类协同关系本质上还是服务提供方和服务消费方之间的关系，涉及到&lt;strong&gt;代理协作&lt;/strong&gt; 和&lt;strong&gt;流量管控&lt;/strong&gt;两个核心点。在传统开发范式下，通常是由网关来负责的。所以我们在云原生 API 网关中增强了 LLM 代理和 MCP Server 代理的能力，使其同时具备流量网关，AI 网关（LLM 代理）和 MCP 网关的能力。是 AI 应用开发新范式的核心组件。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-be80748631259c1b0803fc6e483b5a0c941.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;所以在企业的整体系统架构中，只需要一个云原生 API 网关，即可作为流量网关、API 网关、微服务网关、AI 网关、MCP 网关，在代理和流量管控层面实现传统业务和 AI 业务的大统一，并且再结合 AI 应用开发的新范式，平滑的将 AI 业务和传统业务相结合。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-8619b9be1118c77b682923cbfbec24b5780.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h5&gt;云原生 API 网关 Dog Food&lt;/h5&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-b0b6f6f53c814320166453d81b7f32b99bf.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;秉承着自己吃自己狗粮的原则，云原生 API 网关在阿里集团内部已经有很多业务在深度使用，在企业级产品能力，稳定性，性能方面已经有多个大体量业务的背书。&lt;/p&gt; 
&lt;h5&gt;AI 网关&lt;/h5&gt; 
&lt;p&gt;MCP Client 与 LLM 之间的交互和传统业务与 LLM 之间的交互本质是一样的，只要应用上生产，都会有一系列的问题需要去解决：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;成本平衡问题：比如部署 DeepSeek R1 671B 满血版模型，至少需要 2 台 8 卡 H20 机器，列表价年度超过 100W，但 2 台的 TPS 有限，无法满足生产部署中多个用户的并发请求。即使 Meta 新发布的 Llama4，也至少需要一张 H100 去运行。所以需要有方案找到 TPS 和成本之间的平衡点。&lt;/li&gt; 
 &lt;li&gt;模型幻觉问题：即使是 DeepSeek R1 671B 满血版模型，如果没有联网搜索，依然有很严重的幻觉问题。&lt;/li&gt; 
 &lt;li&gt;多模型切换问题：单一模型服务有较大的风险和局限性，比如稳定性风险，比如无法根据业务（消费者）选择最优模型。目前也没有开源组件和框架解决这类问题。&lt;/li&gt; 
 &lt;li&gt;安全合规问题：企业客户需要对问答过程做审计，确保合规，减少使用风险。&lt;/li&gt; 
 &lt;li&gt;模型服务高可用问题：自建平台性能达到瓶颈时需要有一个大模型兜底方案，提升客户大模型使用体验。&lt;/li&gt; 
 &lt;li&gt;闭源模型 QPS/Token 限制问题：商业大模型都有基于 API Key 维度的 QPS/Token 配额限制，需要一个好的方式能够做到快速扩展配额限制。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;以上问题都是实实在在的客户在使用过程中遇到的问题，有些是模型自身问题，有些是部署架构问题，如果要客户一个一个去解决，复杂度和时间成本都是比较高的。所以就需要 AI 网关的介入来快速的，统一的收敛掉这些核心问题。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-979c884967f9dd509ee14425480408fa609.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;云原生 API 网关的 AI 网关增强能力主要有四部分：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;多模型适配：可以代理市面上所有主流的模型托管服务，以及兼容 OpenAI 协议的 AI 服务。在这个模块中包括协议转换、多 API Key 管理、Fallback、多模型切换等多个核心功能。&lt;/li&gt; 
 &lt;li&gt;AI 安全防护：安全防护分为三个层面，一个是输入输出的内容安全防护，另一个是保护下游 LLM 服务的稳定，以及管控 AI 接口消费者。在这个模块中包括内容审核、基于 Token 的限流降级、消费者认证等多个核心功能。&lt;/li&gt; 
 &lt;li&gt;AI 插件：AI 网关的灵活扩展机制我们使用插件的形式来实现，目前有很多预置的插件，用户也可以开发自定义插件来丰富 AI 场景流量的管控。比如基于 AI 插件机制我们实现了结果缓存、提示词装饰器、向量检索等能力。&lt;/li&gt; 
 &lt;li&gt;AI 可观测：AI 场景的可观测和传统场景的可观测是有很大区别的，监控和关注的指标都是不同的，云原生 AI 网关结合阿里云日志服务和可观测产品实现了贴合 AI 应用业务语义的可观测模块和 AI 观测大盘，支持比如 Tokens 消费观测，流式/非流式的 RT，首包 RT，缓存命中等可观指标。同时所有的输入输出 Tokens 也都记录在日志服务 SLS 中，可供用户做更详细的分析。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;AI 网关代理 LLM 更详细的方案可以参见我之前的文章： &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FtZ0wsTlZK67r9IxNZ57TDQ&quot; target=&quot;_blank&quot;&gt;AI 网关代理 LLMs 最佳实践&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-ad17af83a1e5f0e85f744f7f6c2467e9eb5.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h5&gt;MCP 网关&lt;/h5&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-1b333f6b4365a768b67f0134cb276dfa56f.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;MCP Client 和 MCP Server 之间的交互和传统的服务提供者和服务消费者之间的交互就有所区别了，所以我们在云原生 API 网关中增加了 MCP 相关的能力，但从产品版本划分层面，MCP 相关的能力依然包含在 AI 网关的能力范畴内。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-3bd22ce32b62b13d4980394307caec2bb17.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h5&gt;MCP Server 动态发现&lt;/h5&gt; 
&lt;p&gt;上文中介绍了 MSE Nacos 作为 MCP Server 注册/配置中心，那么 MCP Client 如何来发现呢？如果是 MCP Client 直接和 MSE Nacos 交互，那么又会在 MCP Client 中引入 Nacos SDK，增加了编码的复杂度。&lt;/p&gt; 
&lt;p&gt;鉴于云原生 API 网关和 MSE Nacos 在传统服务领域早已做了深度集成，打通了云原生 API 网关自动发现注册在 MSE Nacos 中的服务，所以在 MCP 范式下，我们同样实现了云原生 API 网关自动发现注册在 MSE Nacos 中的 MCP Server 的能力。&lt;/p&gt; 
&lt;p&gt;通过这种方式，MCP Client 只需要使用云原生 API 网关的接入点，即可自动的、动态的获取到所有注册在 MSE Nacos 中的 MCP Server。云原生 API 网关（MCP 网关）就变成了一个 MCP Hub，无论如何更新、变更 MCP Server，都只需要在 MSE Nacos 操作即可，MCP Client 无需做任何修改。&lt;/p&gt; 
&lt;h5&gt;将传统服务 0 代码改造转换为 MCP Server&lt;/h5&gt; 
&lt;p&gt;在 AI 的时代下，我认为最有价值的是使用 AI 增强、提升客户的现存业务，使其变成一个 AI 应用或 AI 加持的业务，而不是完全新开发一套 AI 应用。&lt;/p&gt; 
&lt;p&gt;所以开发一个 AI 应用或者做现存业务的 AI 增强，AI Agent 是需要和大量现存业务做交互的，MCP 虽然统一的协议，但将现存业务重构为 MCP Server 的成本是非常高的，并且目前支持的开发语言有限，像 Go，PHP 都没有对应的 MCP SDK，所以会让很多企业想拥抱 MCP，但又无从下手。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-bc61c6005adb6f765e14b59cdbeea0fe3d8.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;网关最擅长做的事情就是协议转换，Nacos 在传统微服务场景下已经注册了很多现存的传统服务，那么两者一拍即合，通过网关将注册在 Nacos 中的传统服务 0 代码改造的转换为 MCP Server。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;注册在 MSE Nacos 中的现存业务服务（SpringCloud 服务、Dubbo 服务、Go 服务）不需要做任何改变。&lt;/li&gt; 
 &lt;li&gt;在 MSE Nacos 中新增&lt;code&gt;[Server Name]-mcp-tools.json &lt;/code&gt;命名规范的配置文件，在配置文件中使用 MCP 规范对现存业务的接口进行描述。&lt;/li&gt; 
 &lt;li&gt;通过云原生 API 网关（MCP 网关），MCP Client 侧自动发现由传统服务转换来的 MCP Server。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h5&gt;将 SSE 转换为 Streamable HTTP&lt;/h5&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-6c6e2f8f134ba0aca592c05df3946d05fc9.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;MCP 范式默认的传输协议是 SSE（Server Sent Event），本质上是一种长连接，有状态的传输协议。这种协议在企业级应用中有很多弊端：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;不支持可恢复性（Resumability）：连接断开后，客户端必须重新开始整个会话。&lt;/li&gt; 
 &lt;li&gt;服务器需要维持长期连接（High Availability Requirement）：服务器必须保持高可用性，以支持持续的 SSE 连接。&lt;/li&gt; 
 &lt;li&gt;SSE 仅支持服务器 → 客户端消息，无法灵活进行双向通信。&lt;/li&gt; 
 &lt;li&gt;目前只有少数几个 C/S 架构的客户端和 MCP 提供的用于测试验证的 Web 客户端支持 MCP 范式和 SSE 协议。无法用在企业级的生产应用中。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;好在 MCP 官方也意识到了该问题，所以在 3 月下旬，发布了新的 Streamable HTTP 协议。Streamable HTTP 改变了 MCP 的数据传输方式，让协议变得更灵活、更易用、更兼容：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;更灵活：支持流式传输，但不强制。&lt;/li&gt; 
 &lt;li&gt;更易用：支持无状态服务器。&lt;/li&gt; 
 &lt;li&gt;更兼容：适用于标准 HTTP 基础设施。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;简单来说，原来的 MCP 传输方式就像是你和客服通话时必须一直保持在线（SSE 需要长连接），而新的方式更像是你随时可以发消息，然后等回复（普通 HTTP 请求，但可以流式传输）。&lt;/p&gt; 
&lt;p&gt;这里大家可以思考一下：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Streamable HTTP 打破了目前几个 C 端 MCP Client 的壁垒。也就意味着任何请求方（甚至就是一段简单的 HTTP Request 代码），都可以像请求标准 HTTP API 的方式一样和 MCP Server 交互。&lt;/li&gt; 
 &lt;li&gt;换句话说，当可以使用标准 HTTP API 的方式和 MCP Server 交互后，是不是就不存在所谓的 MCP Client 了？&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;虽然 Streamable HTTP 还在草案阶段，但云原生 API 网关作为 MCP 网关已经支持了将 SSE 传输协议自动转换为 Streamable HTTP 传输协议。或者说，通过云原生 API 网关（MCP 网关）代理的 MCP Server 同时支持 SSE 和 Streamable HTTP 两种传输协议供 Client 使用。&lt;/p&gt; 
&lt;h5&gt;MCP 模式下的身份认证和权限管控&lt;/h5&gt; 
&lt;p&gt;身份认证和权限管控在任何架构，任何业务场景下都是刚需，在 MCP 范式下也不例外，这里有两个层面的权限管控：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Client 有权使用哪些 MCP Server。有权使用某 MCP Server 里的哪些 MCP Tool。&lt;/li&gt; 
 &lt;li&gt;Client 通过 MCP Tool 有权获取到哪些数据。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-6d6595b637a2f9c12b007739e38fb843885.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h5&gt;MCP Server 和 MCP Tool 的使用权限&lt;/h5&gt; 
&lt;p&gt;大家设想一下，当传统业务可以 0 代码转换为 MCP Server 后，注册在 Nacos 中的 MCP Server 和 MCP Tool 肯定会有很多，从业务领域来说，可能有和财务相关的 MCP Server，有和销售相关的 MCP Server，有和售后服务相关的 MCP Server。在返回 MCP Server 和 MCP Tool 信息时不可能将所有信息都返回，肯定只能返回 Client 身份有权使用的 MCP Server 信息。&lt;/p&gt; 
&lt;p&gt;云原生 API 网关作为 MCP 网关，通过成熟的插件机制提供了 HTTP Basic Auth，OAuth2.0，JWT，API Key，外部认证等多种认证方式，以及基于消费者认证功能，可以让用户灵活的管理和控制 Client 的身份认证和 MCP Server/MCP Tool 使用权限。&lt;/p&gt; 
&lt;h5&gt;MCP Server 和 MCP Tool 的数据权限&lt;/h5&gt; 
&lt;p&gt;当 MCP Server 是数据类服务时会比较常见，比如 Mysql MCP Server，Redis MCP Server 等。权限会下探到库级别，表级别。在这种场景下，云原生 API 网关作为 MCP 网关，可以通过插件机制，改写或增加 Request Header 的值，结合 MSE 治理将 Header 的值透传下去，然后在服务内部进一步做数据权限管控。&lt;/p&gt; 
&lt;p&gt;我举例一个通过这种方式实现的数据库读写分离的场景：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-141201d3bda7f80ef0e2186996e5f78e5fa.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h4&gt;如何快速构建 MCP Server&lt;/h4&gt; 
&lt;p&gt;众所周知，AI 应用里涉及到 LLM 推理的场景，大都用在调用相对稀疏的场景，MCP 范式强依赖 LLM 推理，所以无论是基于 HTTP API 模式的 AI 应用开发架构还是基于 MCP 的 AI 应用开发架构，目前也都是应用在相对稀疏调用的场景。所以这里可以延伸出两个问题：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;在稀疏调用的场景下，运行 MCP Server 的计算资源如何优化资源利用率，说的再直白一些就是如何能做到成本最优。&lt;/li&gt; 
 &lt;li&gt;在新的业务中，如何快速构建 MCP Server。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;在所有的计算产品中，函数计算（FC）这种 Serverless FaaS 类型的计算产品，在资源粒度、弹性策略、弹性效率方面都是最适合稀疏调用场景的。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-226f328add64bac9730162c080868382b86.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;函数计算（FC）目前支持了 Python 和 NodeJS 两种语言的 MCP 运行环境（其他语言的 MCP 运行环境也马上会支持）。用户选择 MCP 运行环境创建函数后，只需要编写 MCP Tool 的业务逻辑即可，不需要考虑如何使用 MCP SDK。并且云原生 API 网关和函数计算（FC）有深度集成，可以天然适配 AI 应用开发的新范式。&lt;/p&gt; 
&lt;h5&gt;MCP Server 的弹性效率&lt;/h5&gt; 
&lt;p&gt;基于函数计算（FC）构建的 MCP Server 在弹性效率方面可以从两个维度来看：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;资源规格细粒度管控。&lt;/li&gt; 
 &lt;li&gt;完全按请求弹性。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;函数计算（FC）的实例规格从 0.05C 128MB 到 16C 32GB 不等，有几十种规格的组合方式，可以灵活的根据不同 MCP Server 承载的业务选择合适的资源规格。另外，在 AI 应用中，尤其是流程式构建的模式中，大多数 AI Agent 的职责都是单一的，计算逻辑不复杂的任务，所以都可以用较小资源规格的函数承载。资源规格小，在资源调度，弹性效率方面自然就会有优势。&lt;/p&gt; 
&lt;p&gt;再看函数计算（FC）的弹性机制，它是完全按照请求弹性的，有多少 QPS，就拉起对应数量的实例，并且实例可以复用，当 QPS 降下来后，空闲的实例会自动释放，整个过程完全不需要用户介入参与。在默认按请求弹性的的基础上，用户还可以自行设置按照时间定时弹，或按照指标阈值弹的策略，进一步满足复杂多变的业务场景，做到资源成本最优。&lt;/p&gt; 
&lt;h5&gt;MCP Server 的可观测&lt;/h5&gt; 
&lt;p&gt;函数计算（FC）有完善的可观测体系，也就意味着，基于函数计算（FC）构建的 MCP Server 同样具备指标、链路、日志三个维度的可观测能力。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-b5f9da48cc9bb3dc9d0a1a704532ed62b69.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;通过这套可观测体系，用户可以清晰的了解每个 MCP Server 的各类运行状态。&lt;/p&gt; 
&lt;h4&gt;如何解决开源自建 Dify 的痛点问题&lt;/h4&gt; 
&lt;p&gt;目前，Dify 基本已是可视化流程编排 AI Agent 使用最广泛的工具，但是目前还没有任何一家云厂商有 Dify 托管产品，所以很多基于开源自建 Dify 平台的客户会遇到很多共性的问题，尤其是从个人开发者、开发 Demo 转向企业级生产应用构建时，这些问题往往都是致命的。&lt;/p&gt; 
&lt;p&gt;企业基于开源自建 Dify 遇到的问题：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;流量防护弱：基于开源自建没有任何防护措施，很容易被穿透。&lt;/li&gt; 
 &lt;li&gt;管控与数据链路耦合：AI 应用设计与 Agent 的执行耦合在一起，在高并发场景下无法保证稳定性。&lt;/li&gt; 
 &lt;li&gt;负载均衡问题：在大流量情况下，Dify 的核心服务可能会因为流量负载不均导致稳定性下降。&lt;/li&gt; 
 &lt;li&gt;可观测缺失：开源 Dify 本身不带可观测能力，需要额外搭建可观测体系。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;为了解决这些问题，阿里云上的 Serverless PaaS 类型的计算产品 Serverless 应用引擎（SAE）做了企业生产级别的 Dify 托管部署方案，旨在解决上述问题，让企业在使用 Dify 的时候不用再关心稳定性、健壮性、性能这些问题。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-c555e83a9fc792a92b986916955d7f9752c.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;h5&gt;快速部署 Dify&lt;/h5&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-1ba31b5a2dd05293386af6ce14de2ee831e.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;SAE 提供了 Dify 应用模板，可以一键拉起 Dify 应用，并且提供可视化构建的能力，可以对 Dify 里的每一个环节进行单独调整。&lt;/p&gt; 
&lt;h5&gt;保障 Dify 稳定高可用&lt;/h5&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-32b0e7199c97341bf0d0222faacf34ca011.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;SAE 部署 Dify 支持配置化，三 AZ 部署，实例粒度的自动化迁移，结合云原生 API 网关和 SAE 内置的服务治理能力，保障负载均衡稳定性，同时还支持 Dify 6 个核心服务的健康检查，以及无损上下线。&lt;/p&gt; 
&lt;p&gt;同样依托于底层 Serverless 架构，部署在 SAE 中的应用同样具备优秀的横向扩展效率，并且支持多种方式的弹性规则配置，使整套 Dify 服务可以根据不同的业务场景进行弹缩，在保证高可用的同时，又兼具成本优势。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-1cd91079e9cbc4492cadd0a21b9bd65f100.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;除此以外，SAE 还支持小流量预热，CPU Burst 等能力，进一步保证 Dify 应用在极端情况下的稳定性。&lt;/p&gt; 
&lt;h5&gt;Dify 任务调度方案&lt;/h5&gt; 
&lt;p&gt;定时执行工作流做 AI 数据处理是通用的业务场景，Dify 官网已经把通过定时任务做 Dify 工作流的定时执行和状态监控作了最佳实践，可以参考&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.dify.ai%2Fzh-hans%2Flearn-more%2Fuse-cases%2Fdify-schedule%E3%80%82%E4%BD%86%E6%98%AF%E8%AF%A5%E5%AE%9E%E8%B7%B5%E4%B8%AD%E7%9A%84&quot; target=&quot;_blank&quot;&gt;https://docs.dify.ai/zh-hans/learn-more/use-cases/dify-schedule。但是该实践中的&lt;/a&gt; Dify Schedule 比较简陋，通过 Github Actions 做定时调度，只能调度公网的 dify 工作流，且不是一个企业级解决方案。&lt;/p&gt; 
&lt;p&gt;开源 Dify 在调度方面的痛点主要有 3 点：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;执行记录过多会导致慢查询。&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;执行历史记录存储在数据库中，数量太多会影响 Dify 性能，导致慢查询。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;执行记录查询不支持条件过滤。&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;比如通过时间区间查询，通过任务状态查询，这些都是通用的需求，但开源 Dify 都不支持。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;没有报警监控。&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;任务调度系统需要监控工作流的执行状态，工作流运行失败，需要报警给对应的负责人，开源无报警监控能力。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;我们的方案是通过 MSE 任务调度（SchedulerX）来解决上述问题。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-89118308221a8bc52e81bd3914f14e5ce97.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;用户在 MSE 任务调度中配置 Dify 的 Endpoint，MSE 任务调度通过 Dify API 拉取工作流应用。&lt;/li&gt; 
 &lt;li&gt;用户通过 MSE 任务调度配置定时调度和报警监控。&lt;/li&gt; 
 &lt;li&gt;Dify 工作流定时调度的时候，MSE 任务调度通过 Dify 提供的 API 调度用户的 Dify 应用，并且实时拉取执行结果和详情，存储在 MSE 的 AI 任务调度中。&lt;/li&gt; 
 &lt;li&gt;通过 AI 任务调度做报警监控、可观测增强。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;MSE 任务调度集成 Dify 方案对比开源方案有以下 7 点优势：&lt;/p&gt; 
&lt;p&gt;| 功能 | MSE 任务调度 + Dify | 开源 Dify | | -------- | ---------------- | ------------------- | | 定时调度 | 有 | 无 | | 监控告警 | 有 | 无 | | 执行记录保留时长 | 保留最近 2 个月 | 无限制，但数据量太大会导致查询性能太差 | | 执行记录查询 | 支持时间区间、状态等多种查询条件 | 过滤条件有限 | | 权限管理 | 操作级别精细化权限管理 | 用户级别 | | 限流 | 应用限流、Token 限流 | 无 | | 失败自动重试 | 有 | 无 |&lt;/p&gt; 
&lt;h4&gt;AI 应用可观测体系&lt;/h4&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-8875e63ae3bd892339ee44b4da9f62c1832.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;结合阿里云可观测产品 ARMS，链路追踪 OpenTelemetry，我们构建了 AI 应用全环节的可观测体系。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-f5517df414e4d4c1f132161ef3acac072f9.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;AI 应用整体的可观测体系构建主要有两部分核心：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;数据采集。&lt;/li&gt; 
 &lt;li&gt;数据串联与分析。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h5&gt;观测数据采集&lt;/h5&gt; 
&lt;p&gt;数据采集的核心是要覆盖足够的广，这里又分两个层面：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;编程语言，开发框架要支持的足够广，足够全。&lt;/li&gt; 
 &lt;li&gt;AI 应用架构新范式里涉及到的云产品也需要以相同的标准上报数据。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;在这两个层面，我们通过阿里云应用监控产品 ARMS 和链路追踪 OpenTelemetry 实现了全覆盖：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;遵循最新 OpenTelemetry 社区 GenAI 语义约定。&lt;/li&gt; 
 &lt;li&gt;支持常见的 AI 框架和 AI 模型，包括 Spring AI Alibaba / LLamaIndex / Langchain / 通义千问 2 / OpenAI / PromptFlow 等。&lt;/li&gt; 
 &lt;li&gt;支持 AI 应用开发的主流编程语言，Python，Java，Go。并且相比社区规范提供更加精细化的埋点和属性。&lt;/li&gt; 
 &lt;li&gt;支持在不同的调用链中传播会话信息。&lt;/li&gt; 
 &lt;li&gt;云原生 API 网关支持 OpenTelemetry 协议，网关自身和插件都会基于 OpenTelemetry 上报观测数据。&lt;/li&gt; 
 &lt;li&gt;函数计算 FC 和 Serverless 应用引擎 SAE 均与应用监控 ARMS 以及链路追踪 OpenTelemetry 版产品均做了深度集成。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h5&gt;数据串联与分析&lt;/h5&gt; 
&lt;p&gt;应用监控 ARMS 中，专门构建了 LLM 应用监控模块，针对 AI 应用场景提供了完善的可观测体系。&lt;/p&gt; 
&lt;p&gt;纵向的指标有：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;在线 AI 应用数。&lt;/li&gt; 
 &lt;li&gt;Trace 数。&lt;/li&gt; 
 &lt;li&gt;Span 数。&lt;/li&gt; 
 &lt;li&gt;大模型数。&lt;/li&gt; 
 &lt;li&gt;Token 使用情况。&lt;/li&gt; 
 &lt;li&gt;会话数。&lt;/li&gt; 
 &lt;li&gt;用户数。&lt;/li&gt; 
 &lt;li&gt;模型调用次数。&lt;/li&gt; 
 &lt;li&gt;Token 消耗情况。&lt;/li&gt; 
 &lt;li&gt;模型调用耗时。&lt;/li&gt; 
 &lt;li&gt;Token 消耗排行。&lt;/li&gt; 
 &lt;li&gt;等等...&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;横向链路方面提供了专业的调用链分析功能：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Span 列表。&lt;/li&gt; 
 &lt;li&gt;Trace 列表。&lt;/li&gt; 
 &lt;li&gt;散点图。&lt;/li&gt; 
 &lt;li&gt;全链路聚合。&lt;/li&gt; 
 &lt;li&gt;全链路拓扑。&lt;/li&gt; 
 &lt;li&gt;错/慢 Trace 分析。&lt;/li&gt; 
 &lt;li&gt;调用链上的每个环节都会输入、输出、Token 消耗的展示。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;更多在途的功能规划&lt;/h4&gt; 
&lt;h5&gt;Dify DSL 转 Spring AI Alibaba 编码&lt;/h5&gt; 
&lt;p&gt;虽然 Dify 在做 AI Agent 开发时已足够便利，但是受限于 Dify 的开发语言（Python）和流程引擎的实现逻辑。在运行复杂 AI 应用时，性能方面是有缺陷的。所以我们在探索将 Dify 流程的 DSL 自动转换为基于 Spring AI Alibaba 开发框架的代码。&lt;/p&gt; 
&lt;p&gt;相当于只使用 Dify 低代码可视化构建 AI 应用的皮，运行的内核基于 Spring AI Alibaba 开发框架的代码，这样既具备了便捷的 AI Agent 编排能力，又具备了更好的运行性能。&lt;/p&gt; 
&lt;h5&gt;基于 LLM 编排 MCP Server&lt;/h5&gt; 
&lt;p&gt;目前的 MCP 模式，LLM 针对用户的输入，只返回一个确定的 MCP Server 和 MCP Tool，这是其实是由系统提示词控制的。理论上 LLM 可以针对用户的输入返回多个 MCP Server 和多个 MCP Tool，并且基于 MCP Server 和 MCP Tool 的描述告诉 Client 它们之间的调用顺序，相当于由 LLM 做好了 MCP Server 的编排。这个模式我们还在探索中，很类似现在的 Multi-Agent 的模式。&lt;/p&gt; 
&lt;h5&gt;提高 MCP 模式的性能&lt;/h5&gt; 
&lt;p&gt;因为 MCP 模式中，会频繁和 LLM 交互，显而易见，相比传统 API 调用，MCP 这种模式的性能是不好的，所以在一些时延敏感的业务场景中，目前大概率还不适合 MCP 模式。&lt;/p&gt; 
&lt;p&gt;目前我们也在探讨和探索如何提高 MCP 模式下的请求性能问题，比如：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;固化 MCP Server/MCP Tool 组合，减少和 LLM 的交互。尤其当实现 LLM 编排 MCP Server 后，和 LLM 的交互可能就只存在于开发态或调试态，云形态时使用的都是固化好的 MCP Server 和 MCP Tool 的调用关系。&lt;/li&gt; 
 &lt;li&gt;函数计算探索边缘场景，将 MCP Server 运行在离用户更近的地方。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;AI 应用架构新范式对企业的影响&lt;/h2&gt; 
&lt;p&gt;至此，企业级 AI 应用架构新范式的介绍就结束了，整个架构里有很多环节，每个环节里又有许多细节，在文章中无法一一展开说明。有兴趣的同学可以联系我共同探讨。&lt;/p&gt; 
&lt;p&gt;我们可以设想一下在这个 AI 应用架构新范式下，企业的运营、产品、研发、运维团队之间的组织结构和协作关系可能会发生哪些变化？应用或系统的开发模式会发生哪些变化？&lt;/p&gt; 
&lt;p&gt;这里我来分享一下我的畅想。&lt;/p&gt; 
&lt;h3&gt;MCP Server First&lt;/h3&gt; 
&lt;p&gt;API First，前后端分离这两个概念已经存在很久了，海外企业遵循和实践的会比较好。因为我深耕在 Serverless 计算领域也有 5 年时间，对 AWS 的 Lambda 架构方案，Azure Functions 架构方案，Azure App Service 架构方案，GCP CloudFunction 架构方案，GCP CloudRun 架构方案有比较多的研究。接触了很多 Serverless FaaS 和 Serverless PaaS 架构的客户案例，包括负责落地了不少从双 A 迁移到阿里云的客户。基本上都是标准的基于 APIG+FaaS 模式的 API First 形态。但是在国内，这个模式实践的并不好，除了高德下决定使用函数计算重构了系统，实现了真正的 API First，前后端分离模式以外，鲜有客户有这种模式的实践，也许是有太重的历史包袱。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-f46ebae566ede2fd2c9350a771a8dffac0e.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;上图为高德前后的架构对比&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;在 AI 应用的时代，本质上依然是对各种 API 的调用，但是将 HTTP API 改成 REST API，改造成本是巨大的。但当 MCP 出现后，当我们的方案可以帮助客户 0 代码的转型 AI 应用架构新范式的时候，MCP Server First 是有可能。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-cb265611348b21fd749d36609d4cfaa1f74.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;运维团队：负责云产品的维护（比如云原生 API 网关，MSE Nacos，Serverless 应用引擎，PAI 这些产品的开通、升配），可观测体系的维护（也是基于云产品），和云厂商保持持续沟通。&lt;/li&gt; 
 &lt;li&gt;研发团队：理解公司业务的原子化能力，负责构建 MCP Server 池。&lt;/li&gt; 
 &lt;li&gt;运营/市场/产品：通过低代码可视化方式构建业务流程（业务编排），大白话描述业务需求，快速完成业务流程的搭建，或者说 AI 应用的构建。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;所以未来很有可能每个企业都有自己的 MCP Server 市场，在 MCP Server 市场里分门别类，每类 MCP Server 有专门的研发团队负责，不用太需要考虑统一返回格式，不用考虑开发语言统一。运营、市场、产品等业务方有业务需求或者有新的产品功能需求时，可以通过统一界面用大白话快速构建 AI 应用，MCP+LLM 来实现业务编排，实现 PRD 即产品（PRD as a Product）的新的开发模式。&lt;/p&gt; 
&lt;p&gt;点击&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.aliyun.com%2Febook%2F8442&quot; target=&quot;_blank&quot;&gt;此处&lt;/a&gt;获取 78 页完整版 PPT&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/u/3874284/blog/18175077</link>
            <guid isPermaLink="false">https://my.oschina.net/u/3874284/blog/18175077</guid>
            <pubDate>Sun, 13 Apr 2025 08:51:00 GMT</pubDate>
            <author>原创</author>
        </item>
        <item>
            <title>微软收紧插件，限制 Cursor 使用 C/C++ 语言服务扩展</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:rgba(0, 0, 0, 0.9)&quot;&gt;&lt;span&gt;近日，微软限制 AI 编程工具 Cursor 使用&amp;nbsp;&lt;/span&gt;&lt;/span&gt;C/C++&amp;nbsp;语言服务扩展的消息&lt;span style=&quot;background-color:#ffffff; color:rgba(0, 0, 0, 0.9)&quot;&gt;&lt;span&gt;在开发者社区引发震动。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0416/163758_tmEx_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-9d6dfb55c7972bbba5e3d23cf00502da411.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:rgba(0, 0, 0, 0.9)&quot;&gt;&lt;span&gt;众多用户发现，微软的 C/C++语言服务扩展从 1.18.21 版本开始不再兼容 Cursor，而此前的 1.17.62 版本仍可正常使用。这一变化并非技术故障，而是微软对其扩展市场规则的严格执行——根据最新许可证条款，&lt;strong&gt;其官方扩展仅允许在 Visual Studio Code（&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;strong&gt;&lt;span style=&quot;background-color:#ffffff; color:var(--weui-LINK)&quot;&gt;VSCode&lt;/span&gt;&lt;/strong&gt;&lt;span style=&quot;background-color:#ffffff; color:rgba(0, 0, 0, 0.9)&quot;&gt;&lt;strong&gt;）、Visual Studio 等微软自家产品中使用，明确禁止在 Cursor 等第三方工具中运行&lt;/strong&gt;。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;有开发者表示，最新版本的扩展程序阻止了它的工作，但其通过降级并禁用自动更新的方式解决了。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;「在扩展程序页面，‘卸载’旁边的下拉菜单中有一个「安装特定版本」。安装版本 1.23.6」。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0416/163421_Vaid_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;微软此举被视为对新兴竞品的主动防御。近年来，Cursor 凭借 AI 驱动的代码生成、Agent Mode 等创新功能快速崛起，成为 VSCode 在 AI 编程领域的直接竞争对手。&lt;/p&gt; 
&lt;p&gt;微软虽开源了 VSCode 基础代码，但通过扩展市场的闭源策略牢牢把控生态——其条款明确将「GitHub Codespaces、Azure DevOps」等自家服务纳入允许范围，却将 Cursor 等第三方工具排除在外。&lt;br&gt; &amp;nbsp;&lt;br&gt; 开发者社区对微软的「生态锁定」策略褒贬不一。支持者认为，微软投入数百万美元开发 VSCode 并免费开放，有权限制第三方利用其技术盈利；反对者则指出，此举暴露了开源与商业利益的矛盾——当企业基于开源代码构建付费产品时，原开发者是否有权通过技术限制进行「反制」？&lt;/p&gt; 
&lt;p&gt;相关链接&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fgetcursor%2Fcursor%2Fissues%2F2976&quot; target=&quot;_blank&quot;&gt;https://github.com/getcursor/cursor/issues/2976&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.reddit.com%2Fr%2Fprogramming%2Fcomments%2F1jrl2zw%2Fmicrosoft_has_released_their_own_agent_mode_so%2F&quot; target=&quot;_blank&quot;&gt;https://www.reddit.com/r/programming/comments/1jrl2zw/microsoft_has_released_their_own_agent_mode_so/&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;阅读更多：&lt;a href=&quot;https://www.oschina.net/news/199863/csharp-extension-for-vscode-will-be-closed-sources&quot; target=&quot;_blank&quot;&gt;微软欲闭源 VS Code 的 C# 扩展惹众怒&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/344934/ms-block-cursor</link>
            <guid isPermaLink="false">https://www.oschina.net/news/344934/ms-block-cursor</guid>
            <pubDate>Sun, 13 Apr 2025 08:41:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>Figma 提交 IPO 文件</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;开发在线设计协作软件的初创公司 Figma &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.figma.com%2Fblog%2Fs1-confidential-submission%2F&quot; target=&quot;_blank&quot;&gt;宣布&lt;/a&gt;，已向美国证券交易委员会 (SEC) 提交了 IPO 的保密文件。&lt;/span&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Figma, Inc. 已向美国证券交易委员会秘密提交了一份 S-1 表格注册声明草案，涉及其 A 类普通股的拟议首次公开发行。此声明并非出售任何证券的要约或购买任何证券的要约邀请。潜在公开发行的股份数量和价格区间尚未确定。任何未来的要约、邀请、购买或出售证券的要约或要约，都将在美国证券交易委员会完成审查程序后，根据《证券法》的注册要求进行。&lt;/span&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img height=&quot;278&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-0b41dafcb8523ec8ffb05a64366ec5fbcda.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;更多信息还需等待文件公开，乐观&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftechcrunch.com%2F2025%2F04%2F15%2Ffigma-ignores-the-fear-files-paperwork-for-an-ipo%2F&quot; target=&quot;_blank&quot;&gt;预计&lt;/a&gt;公开时间大约为一个月后。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Figma 最新估值在 2024 年 5 月达到 125 亿美元，当时该公司完成了一项要约收购，允许现有股东套现部分股份。Adobe 曾试图以 200 亿美元收购 Figma，但由于欧洲和美国监管机构的阻力，该交易于 2023 年失败。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;目前，Figma 已得到了风险投资公司红杉资本、Index Ventures、Greylock 和 Kleiner Perkins 的支持，这些投资者是该公司的董事会成员，此外还有 Andreessen Horowitz 和 IVP 等众多其他机构的支持。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color:#000000&quot;&gt;相关阅读：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/news/271682/figma-adobe-abandon-proposed-merger&quot; target=&quot;_blank&quot;&gt;Adobe 放弃收购 Figma，需支付 10 亿美元补偿费&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/344932/figma-s1-confidential-submission</link>
            <guid isPermaLink="false">https://www.oschina.net/news/344932/figma-s1-confidential-submission</guid>
            <pubDate>Sun, 13 Apr 2025 08:38:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>英伟达对华特供版 AI 芯片（H20 GPU）遭遇出口管制</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;4 月 16 日，英伟达向美国证监会（SEC）提交一份正式文件。&lt;/p&gt; 
&lt;p&gt;文件显示，自 4 月 9 日起，美国政府已通知英伟达，公司「对华特供版」AI 芯片产品 H20 GPU（人工智能加速计算处理器）出口至中国及 D:5 国家（包括中国大陆及香港、澳门等特别行政区、俄罗斯、朝鲜、敍利亚、伊朗等面临美国最严格出口管制的地区），或总部、最终母公司位于这些国家及地区的公司，需要获得美国商务部颁发的出口许可证。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0416/161554_VBb2_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;4 月 14 日，美国政府通知英伟达，H20 出口许可证要求将无限期生效。该许可证旨在应对相关产品可能被用于中国超级计算机或被转移至此类用途的风险。&lt;/p&gt; 
&lt;p&gt;英伟达 15 日交易日股价报收 112.2 美元/股，小幅上涨 1.35%。此消息公布后，股价盘后大跌 6.3%。英伟达主要生产支持 AI 大模型训练及推理所需要的 GPU 加速计算处理器，H20 是公司为应对美国对华高性能 AI 芯片出口禁令推出的特供版产品。&lt;/p&gt; 
&lt;p&gt;2022 年 9 月，在旗下多款先进 AI 芯片 GPU 产品 A100 与 H100、A800 与 H800 相继被禁后，英伟开发了降低性能配置的 H20 芯片，旨在符合美国出口管制的性能红线，同时满足中国市场的需求。&lt;/p&gt; 
&lt;p&gt;H20 采用了与 H100 相同的上一代 Hopper 架构，算力性能据称只有后者的六分之一左右，同时配置了 HBM3E 高带宽内存。根据美国政府最新针对其的出口管制要求，此次不只 H20 制成品，H20 所涉及的高带宽内存、NVlink 通信互连或其他相关技术方案都需要审查。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/344925</link>
            <guid isPermaLink="false">https://www.oschina.net/news/344925</guid>
            <pubDate>Sun, 13 Apr 2025 08:16:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>Anubis —— 阻止 AI 爬虫</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                                                                                                                            &lt;p&gt;Anubis 使用 sha256 工作量证明挑战来衡量你的连接「灵魂」，以保护上游资源免受爬虫机器人的侵害。安装和使用此工具可能会导致你的网站无法被某些搜索引擎索引。&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;background-color:#ffffff; color:#1f2328&quot;&gt;大多数情况下，不需要此功能，使用 Cloudflare 保护特定来源即可。但是，如果你无法或不愿使用 Cloudflare，Anubis 可以为你提供帮助。&lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;便于使用&lt;/strong&gt;&amp;nbsp;Anubis 安装简单、重量轻，并且有助于解决最棘手的问题。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;轻便的&lt;/strong&gt;&amp;nbsp;Anubis 高效且尽可能轻便，可以阻止互联网上最危险的机器人，并轻松保护你在线托管的内容。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多线程&lt;/strong&gt;&amp;nbsp;Anubis 使用多线程工作量证明检查来确保用户浏览器是最新的并支持现代标准。&lt;/li&gt;
&lt;/ul&gt;

                                                                    &lt;/div&gt;
                                                                </description>
            <link>https://www.oschina.net/p/anubis</link>
            <guid isPermaLink="false">https://www.oschina.net/p/anubis</guid>
            <pubDate>Sun, 13 Apr 2025 08:13:00 GMT</pubDate>
        </item>
        <item>
            <title>谷歌官宣：未来全球谷歌搜索流量均将重定向至 Google.com 主域名</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.google%2Fproducts%2Fsearch%2Fcountry-code-top-level-domains%2F&quot; target=&quot;_blank&quot;&gt;谷歌宣布了一项重大变革&lt;/a&gt;&lt;/u&gt;，如果用户尝试通过国家代码顶级域名 (ccTLD)（例如 Google.co.uk 或 Google.com.br ）访问 Google，他们将被重定向到 Google.com。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0416/154434_Aj1l_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;过去，这些不同的域名曾帮助谷歌实现搜索结果的本地化，但由于谷歌能够确定用户的位置，这些 ccTLD 已不再是必需的。&lt;/p&gt; 
&lt;p&gt;需要指出的是，谷歌尚未完全淘汰这些 URL，只是会把这些 URL 重定向到 Google.com，这些域名本身不太可能完全被淘汰，因此使用这些地址的书签应该仍然可以正常使用。&lt;/p&gt; 
&lt;p&gt;大型科技巨头似乎总是会因为这样或那样的原因与世界各地的政府发生冲突。这家搜索巨头表示，即使将重定向至 Google.com，也不会改变其根据运营所在国法律所承担的义务。&lt;/p&gt; 
&lt;p&gt;随着这项变更推广到更多用户，您将在未来几个月内开始注意到重定向到 Google.com。该公司警告称，在此过程中，您可能会被提示重新输入部分搜索偏好设置。但并未提及需要提供哪些偏好设置。&lt;/p&gt; 
&lt;p&gt;这一变化相当微妙，&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.google%2Fproducts%2Fsearch%2Fmaking-search-results-more-local-and-relevant%2F&quot; target=&quot;_blank&quot;&gt;自 2017 年以来&lt;/a&gt;，用户已经可以访问 Google.com 并从搜索中获得本地结果，但绝大多数用户不太可能注意到这一变化。随着移动端的转变，如今许多搜索也来自 Android 上的 Google 应用，在这种情况下，您甚至看不到 URL 栏。&lt;/p&gt; 
&lt;p&gt;虽然这一变化相当显著，但大多数读者无需考虑。只需访问您通常用于访问 Google 的 URL 即可跳转到 Google.com，且仍然可以获得本地搜索结果。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/344914/google-search-to-redirect-its-country-level-tlds-to-google-com</link>
            <guid isPermaLink="false">https://www.oschina.net/news/344914/google-search-to-redirect-its-country-level-tlds-to-google-com</guid>
            <pubDate>Sun, 13 Apr 2025 07:45:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>这是人类能想出来的 Python 包？</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                                                                                                                            &lt;p&gt;TARIFF 是一个独特的 Python 包，它允许你对 Python 包征收 import 关税。通过这个工具，可以让外部导入变得更加昂贵，推动代码库中的经济效益。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The GREATEST, most TREMENDOUS Python package that makes importing great again！&lt;/p&gt;

&lt;p&gt;最强、最令人震撼的 Python 包！让导入再次伟大！&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3&gt;&lt;img height=&quot;702&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0416/145722_gY0L_3820517.png&quot; width=&quot;1256&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/h3&gt;

&lt;h3&gt;安装&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;pip install tariff
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;使用方法&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;首先，导入 &lt;code&gt;tariff&lt;/code&gt; 模块。&lt;/li&gt;
&lt;li&gt;设置你的关税率（包名：百分比）。&lt;/li&gt;
&lt;li&gt;导入你想要应用关税的包。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;示例代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import tariff

# 设置关税率（包名：百分比）
tariff.set({
    &quot;numpy&quot;: 50,  # numpy 的关税为 50%
    &quot;pandas&quot;: 200,  # pandas 的关税为 200%
    &quot;requests&quot;: 150  # requests 的关税为 150%
})

# 现在当你导入这些包时，它们会被征收关税！
import numpy  # 这将使 numpy 变慢 50% 
import pandas  # 这将使 pandas 变慢 200% 
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;工作原理&lt;/h3&gt;

&lt;p&gt;当你导入一个有关税的包时：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;TARIFF 测量原始导入所需的时间。&lt;/li&gt;
&lt;li&gt;根据你的关税百分比，TARIFF 使导入时间更长。&lt;/li&gt;
&lt;li&gt;TARIFF 通过一条惊人的消息宣布关税。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;示例输出&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-plaintext&quot;&gt;JUST IMPOSED a 50% TARIFF on numpy! Original import took 45000 us, now takes 67500 us. American packages are WINNING AGAIN! #MIPA
刚刚对 numpy 征收了 50% 的关税！原始导入耗时 45000 微秒，现在耗时 67500 微秒。美国的包再次获胜！#MIPA
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;这是一个恶搞包。请自行承担风险。让导入再次伟大！&lt;/h3&gt;

                                                                    &lt;/div&gt;
                                                                </description>
            <link>https://www.oschina.net/p/hxu296_tariff</link>
            <guid isPermaLink="false">https://www.oschina.net/p/hxu296_tariff</guid>
            <pubDate>Sun, 13 Apr 2025 06:59:00 GMT</pubDate>
        </item>
        <item>
            <title>全球最快的开源 Lakehouse 引擎？直播间见！</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                                                    
                                                                                                                                                    &lt;div&gt; 
 &lt;div&gt;
   StarRocks 是一个开源的 Lakehouse 引擎， 采用 Apache License v2.0 许可证，商业使用友好，GitHub Star 数已经超过了 9.8 K。 
 &lt;/div&gt; 
 &lt;div&gt;
   &amp;nbsp; 
 &lt;/div&gt; 
 &lt;div&gt;
   在存算分离架构、物化视图查询加速和湖仓分析等方面，StarRocks 具备高效、稳定的性能，支持主流开放数据湖表格式，包括 Apache Hive、Apache lceberg、Apache Hudi、 Delta Lake 和 Apache Paimon，提供高效的查询与写入能力。 
 &lt;/div&gt; 
 &lt;div&gt;
   &amp;nbsp; 
 &lt;/div&gt; 
 &lt;div&gt;
   腾讯音乐数据基建团队曾 
  &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FuJwS6B8Q1pw5pH42dPV0ug&quot; target=&quot;_blank&quot;&gt;分享&lt;/a&gt;过，在将原有的上千节点 ClickHouse 和 Druid 集群业务迁移至 StarRocks 存算分离后，效率不变的情况下，成本下降了 50%。 
 &lt;/div&gt; 
 &lt;div&gt;
   &amp;nbsp; 
 &lt;/div&gt; 
 &lt;div&gt;
   4 月 18 日晚，镜舟科技高级技术专家 
  &lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;单菁茹&lt;/strong&gt;&lt;/span&gt;将做客开源中国直播栏目《技术领航》，聊一聊 StarRocks 的成长之路，解析 StarRocks 技术架构以及核心功能。 
 &lt;/div&gt; 
 &lt;div&gt;
   &amp;nbsp; 
 &lt;/div&gt; 
 &lt;div&gt;
   单菁茹是开源社区 StarRocks 的重要贡献者，主要负责指导 StarRocks 的落地实践与生态工具的开发。本次直播，她还将 
  &lt;span style=&quot;color:#2980b9&quot;&gt;基于管理工具 StarRocks Manager，演示集群状态监控、SQL 执行与优化、任务管理、资源管理和版本升级与维护等。&lt;/span&gt; 
 &lt;/div&gt; 
 &lt;div&gt;
   &amp;nbsp; 
 &lt;/div&gt; 
 &lt;div&gt; 
  &lt;strong&gt;微信扫码，预约直播&lt;/strong&gt; 
 &lt;/div&gt; 
 &lt;div&gt;
   &amp;nbsp; 
 &lt;/div&gt; 
 &lt;div&gt; 
  &lt;img height=&quot;3033&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-b8f2b5a44ee1e622f6f9548894b8923da4e.png&quot; width=&quot;900&quot; referrerpolicy=&quot;no-referrer&quot;&gt; 
 &lt;/div&gt; 
 &lt;div&gt;
   &amp;nbsp; 
 &lt;/div&gt; 
 &lt;div&gt; 
  &lt;div&gt; 
   &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;strong&gt;本次直播得到了诸多社区或组织的大力支持，在此特别表示感谢：&lt;/strong&gt;&lt;/p&gt; 
   &lt;div&gt; 
    &lt;div&gt; 
     &lt;div&gt; 
      &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;Gitee&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
      &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;Gitee（码云）是开源中国于 2013 年推出的基于 Git 的代码托管平台、企业级研发效能平台，提供中国本土化的代码托管服务。&lt;br&gt; 目前，Gitee 已经有超过 1350 万名开发者，累计托管超过 3600 万个代码仓库，是中国境内规模最大的代码托管平台。同时，旗下企业级 DevOps 研发效能管理平台 Gitee 企业版已服务超过 36 万家企业。&lt;/p&gt; 
      &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;网址：&lt;a href=&quot;https://gitee.com/&quot;&gt;https://gitee.com/&lt;/a&gt;&lt;/p&gt; 
      &lt;div&gt; 
       &lt;div&gt; 
        &lt;span style=&quot;color:#2980b9&quot;&gt;&lt;strong&gt;镜舟科技&lt;/strong&gt;&lt;/span&gt; 
       &lt;/div&gt; 
       &lt;div&gt;
         &amp;nbsp; 
       &lt;/div&gt; 
       &lt;div&gt;
         北京镜舟科技有限公司是一家专注开源商业化的中国公司，由 StarRocks 项目核心成员于 2022 年创立。基于开源项目 StarRocks ，镜舟打造了符合国家标准并适配国内外生态体系的企业级分析型（OLAP）数据库，在行业适配度、场景成熟度、产品稳定性等方面有着卓越表现。作为 StarRocks 社区中国最大贡献者，镜舟大力参与社区推广工作，未来也将持续与各大头部厂商一起创造世界顶级的开源项目，为社区发展贡献力量。 
       &lt;/div&gt; 
       &lt;div&gt;
         &amp;nbsp; 
       &lt;/div&gt; 
       &lt;div&gt;
         官网： 
        &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.mirrorship.cn%2Fzh-CN%2Findex&quot; target=&quot;_blank&quot;&gt;https://www.mirrorship.cn/zh-CN/index&lt;/a&gt; 
       &lt;/div&gt; 
      &lt;/div&gt; 
     &lt;/div&gt; 
    &lt;/div&gt; 
   &lt;/div&gt; 
   &lt;div&gt; 
    &lt;p&gt;&amp;nbsp;&lt;/p&gt; 
    &lt;hr&gt; 
    &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;《技术领航》是开源中国 OSCHINA 推出的一档直播栏目，旨在为&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;开源软件、商业产品、前沿技术、知名品牌活动等各类项目&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;提供一个展示平台，每周五晚上开播&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;栏目邀请项目的创始人、核心团队成员或资深用户作为嘉宾，通过路演式直播分享项目的亮点和经验，有助于提高项目的知名度，吸引更多的用户和开发者关注。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
    &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:left&quot;&gt;&lt;span&gt;&lt;span style=&quot;background-color:#ffffff&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;如果你手上也有好的项目，想要跟同行交流分享，欢迎联系我，栏目随时开放～&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
    &lt;p style=&quot;color:#333333; margin-left:0; margin-right:0; text-align:center&quot;&gt;&lt;img height=&quot;537&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-4dd54c1b0b817689ceefa15aa66d79cfae8.png&quot; width=&quot;400&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
   &lt;/div&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
 &lt;div&gt;
   &amp;nbsp; 
 &lt;/div&gt; 
&lt;/div&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
            <link>https://my.oschina.net/u/3859945/blog/18175940</link>
            <guid isPermaLink="false">https://my.oschina.net/u/3859945/blog/18175940</guid>
            <pubDate>Sun, 13 Apr 2025 06:50:00 GMT</pubDate>
            <author>原创</author>
        </item>
        <item>
            <title>抖音公开算法原理：通过神经网络计算预估用户行为、几乎不依赖打标签</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;4 月 16 日，据抖音集团微信公众号，在昨日的抖音安全与信任中心开放日活动上，抖音相关业务负责人基于网站版块，就社会关切的算法和治理问题展开介绍。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0416/141208_AOQJ_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;据介绍，抖音推荐算法核心目标是通过分析用户的「行为」（点击、浏览、喜欢、转发、收藏等），构建个性化的推荐模型。系统会基于用户历史动作、对象特征和上下文环境，采用算法模型，预测用户可能会对什么样的内容产生什么样的行为。&lt;/p&gt; 
&lt;p&gt;抖音算法已几乎不依赖对内容和用户打标签，而是通过神经网络计算，预估用户行为，计算用户观看这条内容获得的价值总和，把排名靠前的内容推给用户。&lt;/p&gt; 
&lt;p&gt;在抖音的实际应用中，推荐系统采取「人工 + 机器」协同的方式进行风险治理，始终有人工运营和治理体系为算法导航；多目标体系算法能主动打破「信息茧房」，为用户带来更丰富多元、实用可靠的推荐结果。&lt;/p&gt; 
&lt;p&gt;抖音应用的深度学习算法包括 Wide&amp;amp;Deep 模型、双塔召回模型等。前者可解决协同过滤算法容易造成信息单一、泛化不足的问题，后者在召回环节提供更好的推荐效果。&lt;/p&gt; 
&lt;p&gt;基于人工智能机器学习和深度学习构建的推荐算法，其本质是数学模型的运算过程，只是在建立用户行为与内容特征之间的数学统计关联，而非理解内容本身。抖音推荐算法的核心逻辑可以简化为「推荐优先级公式」：综合预测用户行为概率×行为价值权重 = 视频推荐优先级。&lt;/p&gt; 
&lt;p&gt;模型需要内容和用户两端的数据做输入，其中主要是学习用户行为数据。结合用户行为和视频本身的价值权重，推算出视频推荐的价值分数，并将综合得出的价值最高的视频推送给用户。&lt;/p&gt; 
&lt;p&gt;官方表示，抖音的价值模型希望实现内容、用户、作者以及平台的多方价值共赢，并通过不断调整参数，对各类价值进行加权。随着算法的进步，抖音已经实现了「分钟级」实时反馈更新。&lt;/p&gt; 
&lt;p&gt;抖音表示，推荐算法通过各种「目标」来预估用户行为。推荐算法诞生之初，只关注单一或者少量的目标。随着抖音的用户愈发多样化，内容风格也日益多元，平台上有了越来越多的优质中长视频，完播率等少数目标难以满足需求，多目标建模成为技术上的自然选择。&lt;/p&gt; 
&lt;p&gt;抖音方面称，已经发展出非常复杂的多目标体系，比如将收藏率纳入多目标，帮助知识类内容推送给有需求的用户；增强「收藏+复访」「关注+追更」「打开+搜索」等组合目标，预估用户长期行为，帮助用户探索长期需求；设置探索类指标，帮助用户探索可能他们自己都还没发现的潜在需求，助力破除「信息茧房」；设置原创性目标，鼓励优质、新颖且具有独特价值的内容推荐。&lt;/p&gt; 
&lt;p&gt;此外，抖音集团还表示，平台治理存在于内容发布与传播的每一个环节，整体遵循两个原则。一是所有在平台发布的内容都会经过评估，流量越高的内容经过评估的次数越多，标准也越严格；二是「人工+机器」审核相互分工又密切配合。&lt;/p&gt; 
&lt;p&gt;一条视频可能触发多个治理研判节点，视频被举报、评论区出现集中质疑、流量激增等情况，均可能触发「人工+机器」审核。而在任一环节，一旦内容被处置，基本都会立即停止进一步的推荐和分发。&lt;/p&gt; 
&lt;p&gt;针对社会普遍关心的、呈现聚集特征的、反复出现的、对用户造成较多困扰的焦点问题，抖音成立了数个专项治理团队，分别设置相应的治理标准、识别策略、处置手段和风险巡查能力，专注应对涉及特殊群体、网络暴力、AIGC 技术滥用等问题治理。&lt;/p&gt; 
&lt;p&gt;据悉，3 月 30 日，「抖音安全与信任中心」网站上线，网站面向社会首次公开抖音算法原理、社区规范、治理体系和用户服务机制。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;相关阅读：&lt;a href=&quot;https://www.oschina.net/news/341960&quot; target=&quot;news&quot;&gt;抖音上线「安全与信任中心」网站，首次公开推送算法&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/344876</link>
            <guid isPermaLink="false">https://www.oschina.net/news/344876</guid>
            <pubDate>Sun, 13 Apr 2025 06:12:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>OpenAI 成立非营利委员会</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;OpenAI &lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopenai.com%2Findex%2Fnonprofit-commission-advisors%2F&quot; target=&quot;_blank&quot;&gt;宣布&lt;/a&gt;成立非营利委员会，旨在为 OpenAI 的慈善事业提供咨询。&lt;/span&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;正如我们所说，OpenAI 的非营利机构不会消失——而这个委员会将是扩大其影响力的关键。该委员会的目标是帮助确保我们的非营利机构成为社区和使命驱动型组织的力量倍增器，以应对从健康和教育到公共服务和科学发现等紧迫的全球挑战。&lt;/span&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img height=&quot;345&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-198107f2f83d46e7e57833277ce4c1edf3c.png&quot; width=&quot;700&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;该公司任命 Dolores Huerta、Monica Lozano、Robert K. Ross 和 Jack Oliver 为非营利委员会顾问，Daniel Zingale 担任委员会召集人。声明称，顾问们将从社区中汲取经验和意见，了解 OpenAI 的慈善事业如何能解决长期的系统性问题，同时考虑人工智能的前景和风险。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;具体包括：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;为董事会提供建议，指导透明的社区参与过程，其中包括受影响的社区和普通民众以及非营利组织和慈善领袖。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;汇集来自健康、科学、教育和公共服务领域人士和组织的顶尖见解，其中包括 OpenAI 总部所在地加利福尼亚州的人士和组织。这些意见对于 OpenAI 构建其投资潜力巨大、有望与公司共同成长的资源的能力至关重要。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style=&quot;color:#000000&quot;&gt;在 90 天内向董事会提交调查结果，作为持续的社区反馈和协作过程中的一个重要里程碑。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;更多详情可查看官方&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopenai.com%2Findex%2Fnonprofit-commission-advisors%2F&quot; target=&quot;_blank&quot;&gt;公告&lt;/a&gt;。&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/344863/openai-nonprofit-commission</link>
            <guid isPermaLink="false">https://www.oschina.net/news/344863/openai-nonprofit-commission</guid>
            <pubDate>Sun, 13 Apr 2025 05:50:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>中国信通院联合发布软件智能开发工具及应用图谱</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;为梳理软件智能开发工具产业及应用市场情况，洞察智能开发产业现状及未来发展趋势，中国信息通信研究院（以下简称「中国信通院」）与中国人工智能产业发展联盟（AIIA）牵头，联合中信、华为、百度、硅心科技、软通动力、东软、煤科总院，聚焦智能开发工具技术演进与产业生态，编制软件智能开发工具及应用图谱，并于 2025 年 4 月 9 日中国人工智能产业发展联盟第十四次全会上共同&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FhYBpLQOwXGvwQHBGCpEqKQ&quot; target=&quot;_blank&quot;&gt;发布&lt;/a&gt;。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;该图谱一方面为工具提供方提供横向能力对比参考，推动产品差异化创新与能力升级；另一方面为应用方构建科学选型参考框架，助力其快速匹配业务需求，加速智能开发技术在生产场景中的规模化落地应用。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;本次软件智能开发工具及应用图谱通过问卷调研、公开资料查询、企业访谈等形式系统梳理软件智能开发产业各领域各环节成熟的厂商、平台、机构，聚焦基础支撑、应用工具、行业应用三大维度。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;基础支撑维度，国内代码大模型数量超过国外，集成开发环境（IDE）以国外 VS Code、JetBrains 等为主；应用工具维度，智能开发工具以插件形式的编码助手为主但 AI 原生趋势渐显，大模型为低代码平台、D2C 工具赋能，增强了复杂问题解决能力；行业应用维度，各行业积极融入智能化浪潮，其中金融、软件服务业、通信、互联网、科技等行业落地进展较快。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;708&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-760e359819cfee321fe3e2a0dbd1d8511b0.webp&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/344860</link>
            <guid isPermaLink="false">https://www.oschina.net/news/344860</guid>
            <pubDate>Sun, 13 Apr 2025 05:41:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>清华大学将推 AI 辅修学位</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FH8qkBuP-gz10ccSCe0q5aA&quot; target=&quot;_blank&quot;&gt;据首都教育微信公众号消息&lt;/a&gt;&lt;/u&gt;，继今年 3 月清华大学宣布适度扩招、成立新书院后，学校教学委员会近期审议通过了 AI 辅修学位培养方案，决定今年秋季学期起推出新的 AI 辅修学位。&lt;/p&gt; 
&lt;p&gt;清华大学表示 AI 辅修学位将面向校内有志于探索学科与 AI 交叉学生开放报名，&lt;strong&gt;设置基座模块课程帮助学生掌握 AI 思维、AI 技术与 AI 素养，形成正确的 AI 伦理观&lt;/strong&gt;；同时，建设「X+AI」进阶项目模块引导学生在问题导向实践中，开展自身所在学科与 AI 深度交叉融合的创新探索。&lt;/p&gt; 
&lt;p&gt;除辅修学位外，学生还可选择更具通识特色、修读更为灵活的课程证书项目，掌握 AI 通识知识与素养。部分课程和项目，还将借助 AI 赋能教学方式展开教学，用最 AI 的方式满足更多学生的学习需求。&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;681&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0416/114806_PeMc_2720166.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;▲ 清华大学 AI 成长助手「清小搭」&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;阅读更多&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/news/336597&quot; target=&quot;news&quot;&gt;清华大学将扩招本科生，重点培养「AI+」人才&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/news/337947&quot; target=&quot;news&quot;&gt;北京中小学将从秋季开始开展 AI 通识教育&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/news/325162&quot; target=&quot;news&quot;&gt;武汉大学人工智能学院成立，校长担任院长&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/news/296740&quot; target=&quot;news&quot;&gt;复旦大学将推至少 100 门 AI 领域课程&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/news/337095&quot; target=&quot;news&quot;&gt;上海交通大学发布 AI 使用新规&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.oschina.net/news/303861&quot; target=&quot;news&quot;&gt;教育部支持高校布局集成电路、AI 等专业&lt;/a&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/344843</link>
            <guid isPermaLink="false">https://www.oschina.net/news/344843</guid>
            <pubDate>Sun, 13 Apr 2025 03:48:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>MiniMax-01 系列模型上线超算互联网</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;国家超算互联网平台&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FyfUQnh1nX_i6D20jg7kAxg&quot; target=&quot;_blank&quot;&gt;宣布&lt;/a&gt;上线 MiniMax-01 系列模型。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;185&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-bb28832e1976ef9b6a618009cb290f1d2b5.png&quot; width=&quot;700&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;超算互联网的 ChatBot 可视化对话界面，已经接入 MiniMax-01 系列模型，用户可直接体验 32 个专家和 456B 参数的文本模型，同时体验集成了文本、视觉理解、联网一体的智能对话服务。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;此外，语言大模型 MiniMax-Text-01 、视觉多模态大模型 MiniMax-VL-01 现已入驻超算互联网 AI 开源社区，依托平台海量普惠的异构加速算力，大模型能力得以全面释放。MiniMax 相关负责人表示，未来会继续在超算互联网平台上线更多旗舰模型，并期待与平台一起在智能体领域深度合作。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;398&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-754e8260acabe000491a5ab96af410991bc.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li style=&quot;text-align:justify&quot;&gt;&lt;span style=&quot;color:rgba(0, 0, 0, 0.9)&quot;&gt;左：和全球先进模型相比，随着处理文本变长，MiniMax-Text-01 在推理效果（Long-context RULER performance) 上保持最好。&lt;/span&gt;&lt;/li&gt; 
 &lt;li style=&quot;text-align:justify&quot;&gt;&lt;span style=&quot;color:rgba(0, 0, 0, 0.9)&quot;&gt;右：和全球先进模型相比，随着上下文窗口变长，MiniMax-Text-01 的推理延迟上升缓慢。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/344842</link>
            <guid isPermaLink="false">https://www.oschina.net/news/344842</guid>
            <pubDate>Sun, 13 Apr 2025 03:45:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>昆仑万维宣布 Mureka 开放国内登陆入口和 API</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;昆仑万维&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FUzDSLcm3tvku1QPinGDrYA&quot; target=&quot;_blank&quot;&gt;宣布&lt;/a&gt;，旗下 AI 音乐商用创作平台 Mureka 正式开放国内登陆入口和 API，支持国内用户便捷登录。同时，中文版 MV《Mureka 的色彩》国内首发。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;284&quot; src=&quot;https://oscimg.oschina.net/oscnet/up-a4f742cff968d8bb9be94a6ade1274615c7.png&quot; width=&quot;500&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;根据介绍，昆仑万维在 2024 年 4 月发布了第一代音乐生成模型 Mureka V1（SkyMusic）。8 月 14 日，推出 AI 音乐商用创作平台 Mureka。2025 年 3 月 26 日，正式发布 Mureka O1 模型与 Mureka V6 模型。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;Mureka V6 是当前 Mureka 的基座模型，支持纯音乐生成及 10 种语言的 AI 音乐创作。Mureka O1 模型是基于 V6 的推理优化版本，在推理过程中加入 CoT（思维链），可提升音乐品质、音乐创作效率和灵活性。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style=&quot;color:#000000&quot;&gt;昆仑万维宣布称，目前共有超过 100 个国家地区的用户正在使用该平台进行音乐创。流量平台 Similarweb 数据显示，自 2024 年 8 月 Mureka 上线以来，全球访问量持续走高。根据 AI 产品榜 2025 年 3 月全球流量数据，Mureka 全球 3 月访问量增速高达 86.54%，位列全球 AI 音乐市场第一、3 月全球 AI 产品增速榜 Top15，月访问量达 333 万。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color:#000000&quot;&gt;相关阅读：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.oschina.net/news/341110&quot; target=&quot;news&quot;&gt;昆仑万维发布全球首款音乐推理大模型 Mureka O1&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/344839</link>
            <guid isPermaLink="false">https://www.oschina.net/news/344839</guid>
            <pubDate>Sun, 13 Apr 2025 03:37:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
        <item>
            <title>OpenAI 正在开发社交网络产品，主打 AI 生成内容+社交传播</title>
            <description>&lt;div class=&quot;content&quot;&gt;
                                                                                            &lt;p&gt;多个消息源证实，OpenAI 正在在悄悄打造自己的社交网络产品——类似 X（前 Twitter）。&lt;/p&gt; 
&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.theverge.com%2Fopenai%2F648130%2Fopenai-social-network-x-competitor&quot; target=&quot;_blank&quot;&gt;据科技媒体 The Verge 报道&lt;/a&gt;&lt;/u&gt;，OpenAI 内部已经有产品原型，核心是信息流（Social Feed），重点是能结合 ChatGPT 的图像生成能力玩起来。而且，推进速度可能比我们想得快。Sam Altman 已经开始找公司外部的人为这个原型提反馈意见了。&lt;/p&gt; 
&lt;p&gt;一些爆料细节：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;项目还处于早期阶段，但据称内部已完成原型开发&lt;/li&gt; 
 &lt;li&gt;项目重点是 ChatGPT 的图像生成功能和社交信息流&lt;/li&gt; 
 &lt;li&gt;CEO Sam Altman 已私下向圈外人征求反馈&lt;/li&gt; 
 &lt;li&gt;尚不清楚这个项目是作为独立应用发布还是整合进 ChatGPT&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/space/2025/0416/104235_pq00_2720166.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;此前有传闻称，Meta 计划推出独立的 AI 应用，并计划将其与社交信息流结合，当时 Altman 在 X 平台发文暗示：「那好，我们也可能开发一款社交应用。」&lt;/p&gt; 
&lt;p&gt;&lt;img height=&quot;722&quot; src=&quot;https://static.oschina.net/uploads/space/2025/0416/104408_JJRv_2720166.png&quot; width=&quot;1180&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt; 
&lt;p&gt;就今天的爆料信息来看，OpenAI 可能想打造个 AI 版小红书，而截至发稿前，OpenAI 尚未回应报道。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
            <link>https://www.oschina.net/news/344823/openai-social-network-x-competitor</link>
            <guid isPermaLink="false">https://www.oschina.net/news/344823/openai-social-network-x-competitor</guid>
            <pubDate>Sun, 13 Apr 2025 02:45:00 GMT</pubDate>
            <author>来源: OSCHINA</author>
        </item>
    </channel>
</rss>