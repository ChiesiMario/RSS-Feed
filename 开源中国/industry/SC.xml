<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - industry - 简体中文</title>
    <link>https://www.oschina.net/news/industry</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news/industry" rel="self" type="application/rss+xml"/>
    <description>已对该 RSS 进行格式化操作：中英字符之间插入空格、使用直角引号、标点符号修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-cn</language>
    <lastBuildDate>Mon, 01 Sep 2025 07:41:58 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>Chrome 市占率突破 70%</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;根据 Statcounter 最新数据，截至 2025 年 8 月，Google Chrome 在桌面浏览器市场的份额达到了 &lt;strong&gt;70.25%。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-01c74261cc2dbfb7c696b2470d49ab94115.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-c2f7a0ea50348f5fb173220fa35d6164e8d.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;下面是各主要浏览器市场占比变化&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Microsoft Edge&lt;/strong&gt;：稳居第二，但市场份额仅为 &lt;strong&gt;11.8%&lt;/strong&gt;，比上月略增 0.01 个百分点&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Safari&lt;/strong&gt;：虽为第三，但仅占 &lt;strong&gt;6.34%&lt;/strong&gt;，月增幅高达 1.04 个百分点&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Firefox&lt;/strong&gt;：市场占比为 &lt;strong&gt;4.94%&lt;/strong&gt;，环比下降 0.36 个百分点&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Opera&lt;/strong&gt;：排名第五，仅占 &lt;strong&gt;2.06%&lt;/strong&gt;，环比减少 0.13 个百分点&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;整体来看，Chrome 的市场份额持续上升，而其他竞争浏览器则表现相对疲软，其市场份额普遍有所下滑，尤其是 Firefox 和 Opera。Safari 略有回升，但仍远远落后于 Chrome 和 Edge。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369661/chrome-increases-its-overwhelming-market-share-now-over-70</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369661/chrome-increases-its-overwhelming-market-share-now-over-70</guid>
      <pubDate>Mon, 01 Sep 2025 07:32:56 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>蚂蚁开源医学智能体 MedResearcher-R1</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;蚂蚁集团联合研究团队近日开源发布了针对医疗领域的知识驱动轨迹合成框架 MedResearcher-R1。旨在解决领域特定 AI 推理的挑战，通过智能化的数据生成和合成，为医疗研究提供支持。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;MedResearcher-R1 包含三个集成的核心模块，分别是知识图谱构建、轨迹生成管道和评估管道。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;img alt="" height="455" src="https://oscimg.oschina.net/oscnet/up-aec107f3f7a8b85da21f5a8356da520584b.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;知识图谱构建模块是该框架的核心创新。该模块能够将领域知识转化为高质量的问答对，借助自动推理路径生成，构建出完整的知识图谱。此外，系统还提供了交互式网络可视化，用户可以通过 D3.js 力导向图来直观展示知识图谱结构。先进的采样算法和统一的问答生成方法，使得复杂的子图提取与多种形式的问题合成得以实现。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;接下来是轨迹生成管道。该模块实现了多轮推理与工具集成的自动化处理，可以将问答对转换为多轮推理轨迹，并进行质量过滤。通过高效的质量过滤机制，系统能够检测到错误并进行自动修正，确保生成内容的准确性。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;评估管道为模型的推理性能提供了全面的评估与验证框架。它不仅支持单问题模式的详细过程可视化，还可以进行批量数据集评估，提高评估效率。通过这些模块，MedResearcher-R1 提供了一整套从知识提取到模型训练数据生成和评估的解决方案，推动医疗领域专用推理模型的开发。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;该框架还开源了由知识图谱构建模块生成的高质量问答数据集，包含复杂推理问答对和详细的推理路径，为研究者提供了宝贵的资源。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369660</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369660</guid>
      <pubDate>Mon, 01 Sep 2025 07:28:56 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>腾讯 ARC 实验室发布 AudioStory 音频生成技术</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;腾讯 ARC 实验室发布了 AudioStory 音频生成技术，实现复杂敍事场景的好莱坞级音效一键生成，可处理视频配音、音频续写和长篇敍事音频。 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;AudioStory 将大语言模型与文本‑音频系统结合，能够把复杂的敍事请求拆分为有顺序的子任务，保证场景转换和情感基调的一致性。 &amp;nbsp;它采用「解耦桥接机制」来分别处理事件内部语义对齐与跨事件一致性，并通过端到端训练提升理解和生成的协同。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-be8db71e777c33e2c8aacdb79b9d3982b1a.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;用户通过自然语言描述（如「悬疑追逐战：脚步溅水，雷声轰鸣」），系统即可自动分解事件序列，结合大语言模型与文生音频技术，生成具有时序逻辑与情绪层次的高质量音频。&lt;/p&gt; 
&lt;p&gt;AudioStory 核心技术突破在于采用语义令牌与残差令牌双通道机制，精准协调宏观敍事与微观音效细节，并通过三阶段渐进训练解决长音频连贯性问题。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-9239d4f9913f6c1dcebed99c4ff48ace5f9.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;据了解，AudioStory 技术已应用于视频自动配音、音频智能续写等场景，在万级测试集 AudioStory-10K 中展现领先的指令遵循力与一致性，为有声书、游戏音效等领域提供全新创作工具。&lt;/p&gt; 
&lt;p&gt;开源地址：&lt;em&gt;https://github.com/TencentARC/AudioStory&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369654</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369654</guid>
      <pubDate>Mon, 01 Sep 2025 07:09:56 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>亚马逊 AWS 旗下 AI 编程工具 Kiro 继续免费至 9 月 15 日</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;AWS 推出的 AI 编程 IDE 工具 Kiro &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fkiro.dev%2Fblog%2Ffree-until-september-15%2F" target="_blank"&gt;宣布&lt;/a&gt;，鉴于近期定价调整，公司将原定于 9 月 1 日结束的免费使用政策延长至 &lt;strong&gt;9 月 15 日&lt;/strong&gt;。在此期间，所有支付了订阅费用的用户都将获得全额退款，让用户可以自由使用至 9 月 15 日。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0901/145841_xWsX_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;与此同时，Kiro 用户的计划使用额度已于 &lt;strong&gt;9 月 1 日&lt;/strong&gt;恢复为正常订阅限额，用户还可免费启用 &lt;strong&gt;overages 超额请求&lt;/strong&gt;功能，额外获赠 1,000 次 vibe 请求与 200 次 spec 请求，且相关费用将被退还。&lt;/p&gt; 
&lt;p&gt;官方强调，所有退款已陆续处理，使用额度也已提前重置，以保障用户可畅快使用工具。&lt;/p&gt; 
&lt;hr&gt; 
&lt;h3&gt;重要 FAQ 概览&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;问题&lt;/th&gt; 
   &lt;th&gt;澄清说明&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;用户何时被收费？何时退款？&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;系统一旦在 9 月 1 日扣款，将于 &lt;strong&gt;9 月 15 日前&lt;/strong&gt;全额退还。&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;超额使用费用是否退款？&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;已于 8 月产生的超额费用将被退还；9 月期间启用的 overages，如产生费用，同样会在月底退回。&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;现在订阅是否算免费？&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;若在 &lt;strong&gt;9 月 14 日之前&lt;/strong&gt;购买订阅，会立即享受免费使用至 9 月 14 日，并获得 &lt;strong&gt;9 月费用退款&lt;/strong&gt;。&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;参加 hackathon 的用户影响如何？&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;参与 9 月 15 日截止的 Kiro hackathon 的开发者，可在截至日前免费使用 Kiro，享有相同退款与使用政策。&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;官方也表示，他们正在听取用户反馈，并计划在 9 月 15 日之后推出更新后的定价方案，届时将提前通知用户。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369652</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369652</guid>
      <pubDate>Mon, 01 Sep 2025 06:59:56 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>韩国 2026 财年预算总额创新高，AI 成发力重点</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;据环球网援引韩联社报道，韩国政府 8 月 29 日召开国务会议审议并通过 2026 财年预算案。预算总支出规模达 728 万亿韩元（1000 韩元约合人民币 5.1 元），较今年增长 8.1%，远超今年 2.5% 的增幅，总额创历年新高。这是李在明政府编制的首份预算，标志着韩国财政政策正式从前政府时期的「紧缩」转向「扩张」。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;报道称，面对该国经济结构性动力不足的危机，韩国政府将预算重点投向拉动增长的人工智能（AI）和研发领域：研发预算由 29.6 万亿韩元提高至 35.3 万亿韩元，增幅 19.3%，为历年最大，以加速 AI、生物、文创内容、军工、能源、制造六大关键领域的创新。其中，AI 预算由 3.3 万亿韩元增至 10.1 万亿韩元，增幅逾两倍。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;韩媒称，为应对美国政府对国防军费增额的压力，韩国防预算由 61.25 万亿韩元增至 66.3 万亿韩元，重点用于改善部队官兵福利，以及新一代隐形战机、AI、无人机、机器人等尖端武器研发。据《韩国日报》报道，这是韩国国防预算单年首次增逾 5 万亿韩元，增长率（8.2%）高于总支出，达到 2008 年以来的最高水平。若获国会通过，国防开支占国内生产总值（GDP）比重将增至 2.42%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;此外，产业政策支出扩大 14.7% 至 32.3 万亿韩元，以支持受关税影响的出口商。文化产业支出也将增长 8.8% 至 9.6 万亿韩元。衞生、福利和就业支出达 269.1 万亿韩元。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;为筹措财源，政府将砍掉约 1300 个项目，合计腾挪 27 万亿韩元。即便如此，大部分新增支出仍需依赖大规模举债，国家债务将突破 1400 万亿韩元，占 GDP 的 51.6%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;预算案公布后，韩国国内迅速引发热议。质疑与批评主要集中在筹资能力与财政可持续性上。《朝鲜日报》社论警告，国债未来 4 年内或增至 1789 万亿韩元，大规模发债可能推高利率、抬升企业与家庭融资成本，并对国家信用评级构成压力。《韩民族日报》援引高丽大学教授金泰逸的观点称，可以理解政府希望让财政发挥积极作用，但从财政可持续性看「仍有不少不足」，且「看不到对财政支出绩效管理与税基扩充的努力，这一点令人担忧」。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;韩国《京乡新闻》社论则强调，健全财政固然重要，财政状况良好方能增强经济韧性、抵御意外冲击，但健全财政本身不能成为目的。在过去 3 年财政紧缩与富人减税政策背景下，韩国经济停滞、民生受损，新政府的扩张预算应以创新增长引导经济进入良性循环。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;韩国政府表示，将通过扩张性财政刺激经济，带动税收回升，实现「以财养财」的良性循环。韩国经济副总理兼企划财政部长官具润哲称，新政府上任即面临「经济萎缩、民生冻结」的重大课题，财政预算必须发挥「引水」作用，引导经济恢复成长。据报道，该预算案将在 9 月初提交国会审议，预计 12 月最终确定。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369642</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369642</guid>
      <pubDate>Mon, 01 Sep 2025 06:43:56 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>字节跳动开源 USO，支持统一风格与主体定制的图像生成模型</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;字节跳动团队近日发布并开源&lt;strong&gt;USO（Unified Style-Subject Optimized）&lt;/strong&gt;模型，这是一个「统一风格-主体」定制生成框架，首次把「风格驱动」与「主体驱动」两类原本对立的图像生成任务合并到单一模型里，并在这两个维度上都达到了开源领域的最佳水平（SOTA）。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1844" src="https://static.oschina.net/uploads/space/2025/0901/143002_VA7Q_2720166.png" width="1440" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;USO&lt;span style="background-color:#ffffff; color:#000000"&gt;通过解耦内容与风格特征并引入奖励学习机制，首次实现了风格驱动与主体驱动生成任务的统一框架。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="background-color:#ffffff; color:#000000"&gt;现有方法通常将风格相似性与主体一致性视为对立目标，而 USO 通过构建包含 20 万组三元组数据（风格参考图、去风格化主体图、风格化结果图）的训练集，提出跨任务协同解耦范式：利用主体生成模型生成高质量风格化数据，再通过风格奖励引导的解耦训练优化主体模型。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-c3f05e9b2f38db1ea199c3afc32dbbedbd8.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="background-color:#ffffff; color:#000000"&gt;技术上采用 SigLIP 多尺度特征投影实现风格对齐训练，并通过内容-风格解耦编码器分离条件特征，最终结合风格奖励学习（SRL）进一步提升解耦效果。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;USO 代码与权重已在 Hugging Face 与 GitHub 公开，并配套在线 Demo 与一键安装脚本。&lt;/p&gt; 
&lt;p&gt;https://huggingface.co/bytedance-research/USO&lt;br&gt; https://huggingface.co/spaces/bytedance-research/USO&lt;br&gt; https://github.com/bytedance/USO&lt;br&gt; https://huggingface.co/papers/2508.18966&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369639</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369639</guid>
      <pubDate>Sun, 31 Aug 2025 06:36:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>中国企业调用大模型日均超 10 万亿 Tokens</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#262626; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;9 月 1 日，国际市场调研机构沙利文（Frost&amp;amp;Sullivan）发布了最新的《中国 GenAI 市场洞察：企业级大模型调用全景研究，2025》，报告显示，2025 年上半年，中国企业级市场大模型的日均总消耗量为 10.2 万亿 Tokens，其中，阿里通义占比 17.7% 位列第一，成为目前中国企业选择最多的大模型。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#262626; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;img height="349" src="https://oscimg.oschina.net/oscnet/up-ec7a4444a4499da57f2b48d55c388a3bca3.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#262626; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;&lt;em&gt;《&lt;/em&gt;&lt;em&gt;中国 GenAI 市场洞察：企业级大模型调用全景研究，2025》发布&lt;/em&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#262626; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;沙利文调研国内 700 家企业，领域横跨金融、制造、互联网、消费电子、汽车等多个重点行业， 覆盖不同营收层级和 AI 投入规模的企业，以全面反映中国企业大模型真实使用现状。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#262626; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;报告显示，中国大模型企业级市场呈爆发式增长：较 2024 年下半年，2025 年上半年日均调用量暴增 363%，已逾 10 万亿 tokens；其中，阿里通义占比 17.7%，字节豆包占比 14.1%，DeepSeek 占比 10.3%，前三名合计占比超 40%。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#262626; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;公有云上使用大模型成为主流。沙利文报告显示，七成企业选择公有云部署或调用大模型，71% 企业还表示未来将增加公有云形态的生成式 AI 服务。报告进一步指出，中国企业正从「追求单⼀最强模型」，转向「为特定业务场景寻求最优解」，对不同的模态、尺寸和落地场景匹配的需求将进一步爆发。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#262626; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;开源模型成为大模型企业级市场新一轮增长的关键驱动力。沙利文报告认为，随着千问 Qwen、DeepSeek 等国产模型在 2025 年持续开源，开源模型与国际顶级闭源模型的性能差距几近抹平。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#262626; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#000000"&gt;此外，开源模型尺寸、类别丰富，企业还能完全掌握自主权，根据自身业务特点定制模型及应用，报告预测，未来超过 80% 的企业将采用开源大模型，预示着开源模型将在行业应用中占据主导性增长。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369635</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369635</guid>
      <pubDate>Sun, 31 Aug 2025 06:16:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>微信：将对 AI 生成合成内容添加显式和隐式标识</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#242424"&gt;微信珊瑚安全发布「&lt;/span&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fye36XFlR06Ix29CVPqQmsg" target="_blank"&gt;关于进一步规范人工智能生成合成内容标识的公告&lt;/a&gt;&lt;span style="color:#242424"&gt;」指出，&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;根据《人工智能生成合成内容标识办法》要求，&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;平台&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;应对&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;A&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;I&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;生成&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;合成内容添加&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;显式标识&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;和&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;隐式标识&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;为&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;保障用户信息获取的透明度与可信度，平台进一步优化内容识别能力。用户通过平台获取的 AI 生成合成内容，可能带有显式标识或隐式标识。平台也会对可能是 AI 生成合成的内容进行&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;相应提示，以便用户清晰辨识。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="301" src="https://oscimg.oschina.net/oscnet/up-a73a30dc64edfc343477afce40b675857ce.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;为&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;避免&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;发布&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;的&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;内容&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;在&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;传播&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;过程&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;中&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;引起&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;混淆&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;或&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;误&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;认&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;，&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;用户&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;发布&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;的内容&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;为&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;A&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;I&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;生成&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;合成&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;的&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;，&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;发布&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;时&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;需主动&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;进行&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;声明&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;。&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;标识&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;案例参考&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;：&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="235" src="https://oscimg.oschina.net/oscnet/up-87bc397da078033415bcdbcfdecec3aa6d3.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="156" src="https://oscimg.oschina.net/oscnet/up-a75c22e96cd27be87ad2e22a505a4911d69.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="604" src="https://oscimg.oschina.net/oscnet/up-b493c74959e71a3b436b03d56a2d5085ce2.png" width="300" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;依&lt;/span&gt;&lt;/span&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;据《人工智能生成合成内容标识办法》规定，用户在发布或传播 AI&amp;nbsp;生成合成内容时，不得以任何方式删除、篡改、伪造或隐匿平台添加的 AI 标识。同时不得利用 AI 技术制作传播虚假信息、侵权信息以及从事任何违法违规活动。对于违反法律法规及平台规范的行为，平台将视违规情况进行处罚。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;公告称，&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;微信将不断优化产品功能，保障平台内容的真实性，鼓励创作者发布高质量内容。同时也呼吁广大创作者严格按照《人工智能生成合成内容标识办法》要求及平台规范，发布 AI 生成合成内容时主动进行声明，共创清朗网络空间。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;span style="color:rgba(0, 0, 0, 0.9)"&gt;相关阅读：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.oschina.net/news/369234" target="_blank"&gt;9 月 1 日起，AI 生成合成内容必须添加标识&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369634</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369634</guid>
      <pubDate>Sun, 31 Aug 2025 06:09:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Linus 将 Bcachefs 文件系统标记为由外部维护</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgit.kernel.org%2Fpub%2Fscm%2Flinux%2Fkernel%2Fgit%2Ftorvalds%2Flinux.git%2Fcommit%2F%3Fid%3Debf2bfec412ad293a0b118fb1a20a551088ebc9b" target="_blank"&gt;根据 Linux 内核的代码提交记录&lt;/a&gt;，Linus Torvalds 近日&lt;span style="background-color:#ffffff; color:#474747"&gt;更新了内核维护者文档 MAINTAINERS&lt;/span&gt;，已将 Bcachefs 文件系统的维护状态标记为「外部维护」，&lt;span style="background-color:#ffffff; color:#474747"&gt;发出了他不会接受 Bcachefs 新 PR 的信号。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;这意味着短期内 Bcachefs 的更新不会合并到 Linux 内核主线，该文件系统暂时不会被移除，现有用户仍可继续使用，但不排除未来可能被彻底移出内核。&lt;/p&gt; 
&lt;p&gt;&lt;img height="1456" src="https://static.oschina.net/uploads/space/2025/0901/135607_2YlH_2720166.png" width="1450" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;此次变动源于 Linus Torvalds 与 Bcachefs 主要维护者 Kent Overstreet 之间长期存在的开发流程分歧，尤其是在内核候选发布阶段提交新功能补丁，违反了「仅修复 Bug」的规则。&lt;span style="background-color:#ffffff; color:#474747"&gt;Linus Torvalds 当时表态考虑移除 Bcachefs 文件系统。本月早些时候发布的 Linux 6.17-rc1 就没有合并来自 Overstreet 的任何拉取请求。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="background-color:#ffffff; color:#474747"&gt;Bcachefs 代码目前仍然存在于主线 Linux 内核中，可能是为了防止现有用户在使用 Bcachefs 时遇到问题。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369631</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369631</guid>
      <pubDate>Sun, 31 Aug 2025 06:00:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>OC Auto-POC 开源，一键搞定 OS 深度测试</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="color:#000000"&gt;OpenCloudOS&amp;nbsp;团队研发并&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F27jTw0kHKzJ5LkrmccMPZQ" target="_blank"&gt;开源&lt;/a&gt;了&amp;nbsp;Auto-POC (Proof of Concept) 项目。这是一个专为 OS 功能和性能验证打造的自动化测试套件。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;在&amp;nbsp;OpenCloudOS&amp;nbsp;中，Auto-POC&amp;nbsp;通过「开箱即用」的方式，为用户提供包括基础功能测试、性能微基准测试、安全专项测试在内的测试能力，并提供报告一键生成和 Word 等多文本格式导出的功能。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;目前 Auto-POC 脚本可用于执行一系列的系统测试任务，包括用户管理、系统配置、YUM 操作、磁盘 I/O 测试、内存稳定性测试、CPU 稳定性测试以及 UnixBench 测试。具体涵盖以下几大模块：&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;基础功能验证&lt;/strong&gt;：网络配置（DHCP/静态）、软件源可用性、磁盘管理、CPU 基础能力、内存稳定性等 15 类测试；&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;系统完整性测试&lt;/strong&gt;：验证系统关键组件和文件的完整性；&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;性能压测引擎：&lt;/strong&gt;&lt;/span&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;span style="color:#000000"&gt;磁盘：集成 FIO，测试磁盘 IOPS、带宽、延迟等关键指标；&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span style="color:#000000"&gt;内存：集成 IOZone，测试内存文件操作性能，覆盖读写、随机读写等；&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span style="color:#000000"&gt;综合：UnixBench 系统评分&lt;/span&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;安全合规专项：&lt;/strong&gt;&lt;/span&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;span style="color:#000000"&gt;国密算法（SM2/SM3/SM4）支持性验证&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span style="color:#000000"&gt;文件完整性（AIDE）检测&lt;/span&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;span style="color:#000000"&gt;SELinux 强制访问控制规则测试&lt;/span&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;Auto-POC 技术设计亮点&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;一键启动，全自动执行：&lt;/strong&gt;&amp;nbsp;Auto-POC 设计了自动化流程。从环境初始化开始，到最终测试报告生成，一是减少了手动配置投入，二是全程极少需要人工干预。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;标准化输出，结果清晰可比：&lt;/strong&gt;所有测试结果以结构化、标准化的格式（如 JSON）输出，同时也支持 Word 等标准文档格式输出。方便查看、对比分析和集成到其他系统。性能测试数据具有横向和纵向可比性。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;高度可扩展：&lt;/strong&gt;1）框架设计灵活，用户可根据自身需求添加新的测试用例或集成更多测试工具，持续扩展测试能力；2）不仅支持&amp;nbsp;OpenCloudOS&amp;nbsp;系统，也可以支持部署和测试其他类 CentOS 系统。&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span style="color:#000000"&gt;&lt;strong&gt;拥抱自动化运维/DevOps：&lt;/strong&gt;&amp;nbsp;命令行接口友好，可集成到 CI/CD 流水线、自动化运维平台中，实现 OS 部署后的无人值守自动化验证。&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369629</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369629</guid>
      <pubDate>Sun, 31 Aug 2025 05:58:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>阶跃发布并开源端到端语音大模型 Step-Audio 2 mini</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;阶跃星辰正式发布最强开源端到端语音大模型&amp;nbsp;Step-Audio 2 mini，该模型在多个国际基准测试集上取得 SOTA 成绩。&lt;/p&gt; 
&lt;p&gt;它将语音理解、音频推理与生成统一建模，在音频理解、语音识别、跨语种翻译、情感与副语言解析、语音对话等任务中表现突出，并率先支持语音原生的 Tool Calling 能力，可实现联网搜索等操作。&lt;/p&gt; 
&lt;p&gt;一句话总结，Step-Audio 2 mini 「&lt;strong&gt;听得&lt;strong&gt;&lt;strong&gt;清楚&lt;/strong&gt;&lt;/strong&gt;、想得明白、说得自然&lt;/strong&gt;」。&lt;/p&gt; 
&lt;p&gt;据介绍，Step-Audio 2 mini 在多个关键基准测试中取得 SOTA 成绩，在音频理解、语音识别、翻译和对话场景中表现突出，综合性能超越 Qwen-Omni 、Kimi-Audio 在内的所有开源端到端语音模型，并在大部分任务上超越 GPT-4o Audio。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0901/134137_uHCe_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0901/134144_WabG_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;在通用多模态音频理解测试集 MMAU 上，Step-Audio 2 mini 以 73.2 的得分位列开源端到端语音模型榜首；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;在衡量口语对话能力的 URO Bench 上， Step-Audio 2 mini 在基础与专业赛道均拿下开源端到端语音模型最高分，展现出优秀的对话理解与表达能力；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;在中英互译任务上， Step-Audio 2 mini 优势明显，在 CoVoST 2 和 CVSS 评测集上分别取得 39.3 和 29.1 的分数，大幅领先 GPT-4o Audio 和其他开源语音模型；&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;在语音识别任务上，Step-Audio 2 mini 取得多语言和多方言第一。其中开源中文测试集平均 CER（字错误率） 3.19，开源英语测试集平均 WER（词错误率） 3.50，领先其他开源模型 15% 以上。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Step-Audio 2 mini 通过创新架构设计，有效解决了此前语音模型存在的问题，做到「走脑又走心」。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;真端到端多模态架构&lt;/strong&gt;：Step-Audio 2 mini 突破传统 ASR+LLM+TTS 三级结构，实现原始音频输入到语音响应输出的直接转换，架构更简洁、时延更低，并能有效理解副语言信息与非人声信号。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img height="525" src="https://static.oschina.net/uploads/space/2025/0901/134156_klxf_2720166.png" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;图：Step-Audio 2 mini 模型架构图&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;CoT 推理结合强化学习：&lt;/strong&gt;Step-Audio 2 mini 在端到端语音模型中首次引入链式思维推理（Chain-of-Thought， CoT）与强化学习联合优化，能对情绪、语调、音乐等副语言和非语音信号进行精细理解、推理并自然回应。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;音频知识增强&lt;/strong&gt;：模型支持包括 web 检索等外部工具，有助于模型解决幻觉问题，并赋予模型在多场景扩展上的能力。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;模型现已上线 GitHub、Hugging Face 等平台。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;GitHub：&lt;em&gt;https://github.com/stepfun-ai/Step-Audio2&lt;/em&gt;&lt;br&gt; Hugging Face：&lt;em&gt;https://huggingface.co/stepfun-ai/Step-Audio-2-mini&lt;/em&gt;&lt;br&gt; ModelScope：&lt;em&gt;https://www.modelscope.cn/models/stepfun-ai/Step-Audio-2-mini&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369623</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369623</guid>
      <pubDate>Sun, 31 Aug 2025 05:43:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>阿里巴巴正在开发一款新 AI 芯片，已进入测试阶段</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.wsj.com%2Ftech%2Fai%2Falibaba-ai-chip-nvidia-f5dc96e3" target="_blank"&gt;据报道&lt;/a&gt;，阿里巴巴正在开发一款新的人工智能芯片，意在填补英伟达在中国市场的空白。目前，这款芯片已进入测试阶段，主要面向更广泛的 AI 推理任务，并与英伟达的架构兼容。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0901/114403_3NgP_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;此外，新的芯片不再由台积电代工，转为由国内一家企业代工。&lt;/p&gt; 
&lt;p&gt;目前，阿里巴巴已经推出了多款自研 AI 芯片以及 AI 大模型，并通过阿里云将 AI 算力与解决方案服务化，提供给广泛的企业用户。值得注意的是，阿里云 AI 相关产品收入已连续八个季度保持三位数的同比增长，显示出该业务的持续爆发力。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369611</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369611</guid>
      <pubDate>Sun, 31 Aug 2025 03:44:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>快手开源向量化引擎 Auron 正式加入 Apache 孵化器</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;由快手开源并捐赠的向量化引擎 Auron 项目（原 Blaze 项目）近期&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F9yVMKH7Zc0W_OsYdEBeBUw" target="_blank"&gt;正式&lt;/a&gt;进入全球最大开源基金会（ASF）的孵化器，移交到 Apache 软件基金会名下。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p style="color:#000000; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#4a4a4a"&gt;加入 Apache 软件基金会孵化，源于我们对开源生态的深刻认同，以及对项目长期可持续发展的考量。目前 Auron 已在多家公司应用落地，验证了其实用价值。然而，开源项目的生命力不仅依赖技术先进性，更需要通过开放的社区治理、多元的贡献者参与和透明的决策机制实现持续进化。我们始终相信，开源的价值在于「共享」与「共创」。&lt;/span&gt;&lt;/p&gt; 
 &lt;p style="color:#000000; margin-left:0; margin-right:0; text-align:justify"&gt;&lt;span style="color:#4a4a4a"&gt;我们将严格遵守 Apache 的治理规范，推动项目代码、文档与社区的全面透明化，吸引更多开发者参与技术迭代，与 Apache 生态中的其他大数据项目（如 Spark/Flink/Celeborn 等）形成技术互补，共同推动大数据领域的技术持续创新。&lt;/span&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Auron 是基于向量化技术开发的一套 Native 执行引擎，可以充分利用 Native 代码和 SIMD 指令向量化优势，以实现减少资源开销、加速执行的目的。&lt;/p&gt; 
&lt;p&gt;&lt;img height="308" src="https://oscimg.oschina.net/oscnet/up-98c7a3292dbcf3b0b36743234cbae342e56.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;其核心能力包括：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;原生执行：采用 Rust 实现，消除 JVM 开销以取得更好的性能表现；&lt;/li&gt; 
 &lt;li&gt;向量化计算：基于 Apache Arrow 列式格式构建，充分利用 SIMD 指令优化批处理；&lt;/li&gt; 
 &lt;li&gt;可插拔架构：与 Apache Spark 无缝集成，同时设计上支持未来扩展至其他计算引擎；&lt;/li&gt; 
 &lt;li&gt;生产环境强化优化：开发了多级内存管理、优化 Shuffle 格式及自适应执行策略等优化机制，并在生产环境大规模应用落地。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;基于 Auron，在 TPC-DS 上相比 Spark 可以取得 2+倍的性能提升：&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" height="382" src="https://oscimg.oschina.net/oscnet/up-f70fed02e6787d0c28479a8cf4f008c930d.webp" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h4&gt;&lt;strong&gt;Auron 发展历史与现状&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;2022 年 1 月，快手大数据 Spark 引擎团队正式启动了 Blaze 并开源。2023 年 9 月，Blaze 在 TPC-H/TPC-DS 等 Beachmark 上取得了显著的性能提升。同时，团队也围绕生产环境进行了大量优化，并在快手内部实现了大规模应用。目前，Blaze 在快手日均运行数十万个任务、处理 EB 级别数据，展现出卓越的性能和稳定性，为快手每年节约数千万元服务器成本。&lt;/p&gt; 
&lt;p&gt;在社区运营方面，快手于 2024 年 1 月展开了开源社区的管理工作。自 Blaze 开源以来，已累计发布了 10 余个版本。目前，Blaze 已被滴滴、携程、汽车之家、58 同城、OPPO 等多个行业的公司广泛使用。&lt;/p&gt; 
&lt;p&gt;2025 年 8 月，Blaze 项目正式进入 ASF 孵化器，并更名为 Auron（发音： [ˈɔːrɑːn] ），其灵感来源 Aura（能量场）。Auron 寓意为大数据引擎所带来的强大性能，未来，Auron 社区计划提供更多先进功能，如 Flink 引擎、数据湖系统、GPU/DPU 硬件的集成等。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369610</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369610</guid>
      <pubDate>Sun, 31 Aug 2025 03:39:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Anthropic 承认 Claude Opus 4.1 和 Opus 4 模型近期「降智」</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;8 月 30 日，Anthropic 官方发布&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fstatus.anthropic.com%2Fincidents%2Fh26lykctfnsz" target="_blank"&gt;事件报告&lt;/a&gt;，确认了 Claude Opus 4.1 和 Opus 4 近期出现质量下降的情况。（官方表示已修复）&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0901/113337_hTvV_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;报告显示，从 8 月 25 日 17:30 至 8 月 28 日 2:00 期间（世界统一时间），Claude Opus 4.1 对某些请求的回答质量有所下降，用户端可能会看到较低的智能表现、格式错误的响应或 Claude Code 中工具调用出现问题。&lt;/p&gt; 
&lt;p&gt;Anthropic 方面表示，上述情况是由于其推理堆栈的推送所引起，目前已将 Claude Opus 4.1 进行回滚。Anthropic 表示，虽然更新是为了提高模型效率和吞吐量，但其意图始终是保持相同的模型响应质量。&lt;/p&gt; 
&lt;p&gt;另外，Anthropic 还发现 Claude Opus 4.0 也受到了上述问题影响，目前同样进行回滚处理。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369609</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369609</guid>
      <pubDate>Sun, 31 Aug 2025 03:34:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>马斯克承认 xAI 代码库遭窃，前员工转投 OpenAI</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;马斯克最近爆料称，他所创立的 xAI 公司的整个代码库遭到窃取。xAI 已经对一名前员工提起诉讼，指控他窃取了公司的商业机密，且此人已跳槽至竞争对手 OpenAI。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;img height="328" src="https://oscimg.oschina.net/oscnet/up-cf86b5f71d69bdbb4fa495886290e53f865.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;据悉，涉事的前员工名叫 Xuechen Li，他曾是 xAI 的核心成员之一。根据 xAI 向加州北区联邦地方法院递交的起诉书，Li 面临四项指控，涉及违反保密协议、侵犯商业秘密、违反加州计算机数据法规以及欺诈。xAI 要求法院对 Li 实施禁令，禁止其在 OpenAI 等竞争对手工作，并要求其归还所有被盗取的数据。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;事件的起因追溯到 Li 于 7 月 28 日从 xAI 辞职，辞职前的三天，他便已将大量公司的数据上传至个人系统。令人瞩目的是，在辞职前夕，Li 还将手中的 xAI 股份套现，获得了近 700 万美元的收益。虽然 Li 在离职时签署了相关文件，承诺归还公司财产和删除所有副本，但他仍然采取了一系列手段来掩盖其窃密行为。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;根据 xAI 的调查，8 月 11 日，公司的安全软件检测到数据外泄的迹象，随即向 Li 发函要求他归还被盗信息。可 Li 不但没有配合，反而更改了存储盗取数据的账户密码，试图阻止公司的访问和恢复。在承认窃密的过程中，Li 的律师也在场，但他仍然隐瞒了多项关键信息。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;值得一提的是，Li 在离职前已收到 OpenAI 的邀请，并于 8 月 19 日加入该公司。xAI 在起诉书中表示，Li 涉嫌盗取的商业机密包括了 「超越 ChatGPT 及其他竞品的&lt;span&gt;尖端&lt;/span&gt;人工智能技术」，这些技术可能为 OpenAI 及其他竞争对手节省数十亿美元的研发费用。&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;Xuechen Li 的背景同样引人关注。他是斯坦福大学的计算机博士，曾在 Google 和微软实习，是 xAI 早期团队的一员。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369606</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369606</guid>
      <pubDate>Sun, 31 Aug 2025 03:25:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>强化学习的 「GPT-3 时刻」 即将到来</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                                                                                        &lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;编者按：&lt;/strong&gt; 强化学习能否像 GPT-3 改变自然语言处理那样，通过大规模扩展实现质的飞跃？为什么强化学习至今仍困在"先预训练，再微调"的传统模式中？为什么即使是最先进的 RL 模型，一旦脱离训练环境就变得如此脆弱？&lt;/p&gt; 
 &lt;p&gt;无论是自动驾驶、机器人控制，还是复杂系统优化，我们都需要能够快速适应新任务、具备真正泛化能力的智能体。然而当前的 RL 模型就像是"高分低能"的应试选手 ------ 在熟悉的测试环境中表现优异，但面对真实世界的复杂性时却束手无策。&lt;/p&gt; 
 &lt;p&gt;本文提出了 replication training 范式，为强化学习的规模化扩展指明了全新方向。作者不再拘泥于传统的游戏环境或仿真场景，而是大胆提议让 AI 复制现有的软件产品。它利用了互联网上丰富的软件资源，提供了客观明确的评估标准，同时训练了 AI 在长周期项目中保持稳定输出的能力。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;作者 | Matthew Barnett, Tamay Besiroglu, Ege Erdil&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;编译 | 岳扬&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;GPT-3 证明了，仅仅通过扩大语言模型的规模，就能带来强大的、task-agnostic（译者注：模型不依赖特定任务的设计或微调，就能处理多种不同类型的任务。）、few-shot（译者注：模型仅需极少量示例，就能快速理解并执行新任务。）的性能，其表现通常优于经过精心微调的模型。在 GPT-3 出现之前，要达到最先进的性能，首先需要在大型通用文本语料库上对模型进行预训练，然后再针对特定任务进行微调。&lt;/p&gt; 
&lt;p&gt;如今的强化学习同样困在类似 GPT-3 之前的范式里。我们首先是对大模型进行预训练，然后在高度专业化的环境中，对特定任务进行精细的微调。但这种方法的根本局限在于：由此获得的能力难以泛化，导致性能"脆弱"（brittle performance） ------ &lt;strong&gt;模型一旦脱离训练期间接触的精确语境，性能便会迅速退化。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet/up-e782b46339412b51a1a128eda50eb48d456.png" alt="" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;我们认为强化学习（RL）即将迎来其"GPT-3 时刻"。相比在有限数量的训练场景或任务设置上微调模型，我们预计该领域将转向在数千个多样化环境上进行大规模训练。有效实施这一做法将催生出具有 few-shot、task-agnostic 能力的 RL 模型，能够快速适应全新的任务。但实现这一点需要训练环境在规模和多样性上远超当前任何的可用资源。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;01 究竟需要多少 RL 资源？&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;当前的 RL 数据集相对较小。例如，DeepSeek-R1 在大约 60 万个数学问题上进行了训练，这相当于人类连续努力六年的工作量（假设每个任务耗时五分钟完成）。相比之下，重建 GPT-3 那包含 3000 亿 token 的训练语料库，若按人类平均书写速度计算，需要大约数万年的写作时间。&lt;/p&gt; 
&lt;p&gt;需要说明的是，&lt;strong&gt;要达到与当前前沿模型预训练预算相当的 RL 计算支出，按人类完成相同任务所需时长来衡量，可能需要大约上万年。&lt;/strong&gt; DeepSeek-R1 在 RL 阶段使用了约 6e23 FLOP 的计算量[1]，按人类效率折算，对应约 6 年的时长。假设未来的训练任务使用与 DeepSeek-R1 相似的训练轮次（epochs）和组大小（group sizes），将此扩展至约 6e26 FLOP 意味着需要人类约 6000 年的工作时长。&lt;/p&gt; 
&lt;p&gt;尚不确定未来的强化学习训练会需要更大的还是更小的组规模（group sizes）、抑或是更多的训练轮次（epochs），尤其是随着任务分布多样性的增加。我们在这方面缺乏足够的数据，因此精确估算等效的人类工作时间仍很困难，尽管 1 万年左右似乎是一个较为合理的数量级。&lt;/p&gt; 
&lt;p&gt;这一过程要求模型完成的工作量，其规模可与 Windows Server 2008、GTA V 或 Red Hat Linux 7.1 等大型项目相当 ------ 每个项目估计都需要约 1 万年的累计人类工作量。&lt;/p&gt; 
&lt;p&gt;将强化学习（RL）扩展到这一规模在经济上是高效的。&lt;strong&gt;由于算力成本在总训练成本中占据主导地位，将强化学习的规模提升到与预训练预算相当的水平，能在不明显增加总成本的情况下带来大幅的性能提升。&lt;/strong&gt; 然而，要实现这一目标，就必须大规模扩展强化学习环境（RL environments）的体量，同时确保任务能够实现自动化评估。这很可能需要开发新的构建强化学习环境的方法。&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;02 Replication training&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;想象一下，每次当你想要通过下一个词预测方法（next-token prediction）预训练语言模型时，都必须亲手创建整个训练语料库。显然，这极其不切实际。因此，我们转而利用海量的现有内容 ------ 如书籍、学术论文、博客帖子和 Reddit 讨论内容来构建训练语料库。&lt;/p&gt; 
&lt;p&gt;同样，&lt;strong&gt;我们推测，RL（强化学习）领域的"GPT-3 时刻"将主要依托于一种称为 replication training 的新范式来实现。&lt;/strong&gt; 该范式要求 AI 复制现有的软件产品或其内部特定功能。实现复杂的哈希与加密算法的简单命令行工具是较为理想的初期目标，这种方案可以轻松扩展到更复杂的软件，例如网站、专业软件和游戏。&lt;/p&gt; 
&lt;p&gt;每项复制任务（replication tasks）均包含详细的说明规范和用于参考的实现方案。其核心思想是，AI 模型经过训练后能够生成与用于参考的实现方案完全一致的方案。这种清晰直接的方法极大地简化了评估过程，因为评分标准客观且明确：生成的实现方案的行为要么与用于参考的实现方案完全一致，要么就是不一致。&lt;/p&gt; 
&lt;p&gt;尽管这些复制任务（replication tasks）可能与日常的软件工程活动有所不同，但它们专门针对当前 AI 系统难以掌握的关键能力。例如，复制一个复杂的算法（如依据详细规范进行开发的、包含万行量级代码的加密/解密 CLI 工具），要求模型必须做到：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;准确阅读并深度理解详细指令。&lt;/li&gt; 
 &lt;li&gt;一丝不苟且精确无误地执行指令。&lt;/li&gt; 
 &lt;li&gt;能够发现早期错误并可靠地恢复。&lt;/li&gt; 
 &lt;li&gt;在长时间周期（相当于人类数月时间的开发工作量）内保持稳定输出 ------ 在此过程中，质量优劣完全由功能正确性直接判定。&lt;/li&gt; 
 &lt;li&gt;在遇到困难时展现出韧性，而非草率止步于看起来"差不多能用"的方案。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;我们预测，replication training 将成为 AI 领域的下一个范式，因为它顺延了我们在 AI 发展过程中已观察到的趋势 ------ 利用海量的现有人类生成数据来创建新任务。就像自然语言一样，软件在互联网上同样资源丰富。因此，replication training 提供了一种可扩展的途径，能高效生成复杂任务，推动我们实现可端到端完成完整软件项目的 AI。&lt;/p&gt; 
&lt;p&gt;然而，这种方法也面临着几项挑战。编写有效且全面的测试仍然是一项非同小可的任务，需要大量的工程投入。此外，复制任务（replication tasks）本身具有一定的人造性，因为精确复制现有软件并非日常软件工程的典型工作（尽管在软件移植、遗留系统重构、净室重新实现【译者注：clean-room reimplementations，指在严格隔离原始代码知识的前提下，仅通过分析功能规范或外部行为，重新实现与原有软件功能相同的程序。该过程需确保开发团队从未接触过原始源代码，以避免法律上的版权/专利侵权风险。】）等场景中确有其例。&lt;/p&gt; 
&lt;p&gt;尽管存在这些挑战，但我们认为 replication training 为将强化学习环境（RL environments）扩展到实现有意义泛化所需的庞大规模提供了一条清晰明确的路径。它很可能将成为解锁强化学习"GPT-3 时刻"的关键，为达成稳健的、task-agnostic 的性能提供所需的数万年量级的经验积累。&lt;/p&gt; 
&lt;p&gt;replication training 会是解锁 full automation of labor（译者注：通过 AI / 机器人系统实现人类所有劳动形式的自动化替代，达到无需人类直接参与即可完成经济生产活动的终极状态。）的终极范式吗？对此我们持怀疑态度。虽然它能催生可在精确设计规范下自主完成高复杂度软件项目的系统，但我们推测，这些能力仍将逊色于人类所具备的开放式能力。即便 AI 成为高级编程专家，它们在狭窄的软件领域之外的高层管理（译者注：high-level management，指组织架构中涉及战略决策、资源分配和跨部门协调的顶层管理职能。）与自主规划（agentic planning）方面也未必能胜任。&lt;/p&gt; 
&lt;p&gt;然而，正如我们需要先发明预训练，才能迈向 replication training，replication training 仍可作为通往下一范式的桥梁。我们对这一新范式的未来潜力充满期待。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;END&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;本期互动内容 🍻&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;❓您预测 RL 领域的"GPT-3 时刻"会在什么时间节点出现？3 年内、5-10 年，还是更久？请分享您的判断依据。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;本文经原作者授权，由&lt;/strong&gt; &lt;strong&gt;Baihai IDP&lt;/strong&gt; &lt;strong&gt;编译。如需转载译文，请联系获取授权。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;原文链接：&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.mechanize.work%2Fblog%2Fthe-upcoming-gpt-3-moment-for-rl%2F" target="_blank"&gt;https://www.mechanize.work/blog/the-upcoming-gpt-3-moment-for-rl/&lt;/a&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/IDP/blog/18689960</link>
      <guid isPermaLink="false">https://my.oschina.net/IDP/blog/18689960</guid>
      <pubDate>Sun, 31 Aug 2025 03:01:00 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>百度「91 助手」将于本月底全面停止所有服务</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2F91zs.soupingguo.com%2Fnotice%2Fservice-shutdown.html" target="_blank"&gt;根据 91 助手官方消息&lt;/a&gt;，该应用将于 2025 年 9 月 27 日 23:59 起全面停止所有服务，包括但不限于手机连接管理、文件传输、应用安装卸载、系统清理等功能。&lt;/p&gt; 
&lt;p&gt;官方建议用户在此期间及时备份相关数据，服务终止后，用户在 91 助手的数据将永久丢失，无法再以任何方式找回。此外 91 助手将向仍处于会员期的用户进行退费，但需要用户主动申请，未主动申请退费的用户将无法获得退款。 &amp;nbsp;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0901/103517_Wl4q_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;作为百度 2013 年以 19 亿美元重金收购的标志性资产，其停服不仅意味着一款产品的生命周期终结，更折射出整个移动应用分发行业的深刻变革，以及移动互联网早期红利的终结。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-25f9a25a818ee772e357f2e25fad5bb1098.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;截至发稿，百度方面未作回应。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369597</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369597</guid>
      <pubDate>Sun, 31 Aug 2025 02:38:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>苹果开源两大模型家族：FastVLM 与 MobileCLIP2</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;苹果在 Hugging Face 上开源了 FastVLM 和 MobileCLIP2 两大模型家族：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;FastVLM 是一种视觉语言模型（VLM），专为高分辨率图像设计，旨在减少 token 输出并压缩编码时间，可以高效地理解图像和文本之间的关系。&lt;/li&gt; 
 &lt;li&gt;MobileCLIP2 是 CLIP 模型的轻量化、移动端优化版本，专注于在手机或边缘设备上进行高效推理。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;FastVLM 系列包含 0.5B、1.5B、7B 三个参数规模，全部基于新提出的 FastViTHD 混合视觉编码器，专为高分辨率图像输出更少 token、压缩编码时间。FastVLM-7B 在 DocVQA 达到 93.2，在 ScienceQA 达到 96.7，表现领先。&lt;/p&gt; 
&lt;p&gt;MobileCLIP2 系列同样上线，提供相应 Hugging Face Collections 页面。&lt;/p&gt; 
&lt;p&gt;CVPR 2025 论文《FastVLM: Efficient Vision Encoding for Vision Language Models》已公开，引用信息随模型仓库一并提供。&lt;/p&gt; 
&lt;p&gt;https://huggingface.co/collections/apple/fastvlm-68ac97b9cd5cacefdd04872e https://huggingface.co/spaces/apple/fastvlm-webgpu&lt;br&gt; https://huggingface.co/collections/apple/mobileclip2-68ac947dcb035c54bcd20c47&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369583</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369583</guid>
      <pubDate>Sun, 31 Aug 2025 02:22:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>美团正式发布并开源 LongCat-Flash-Chat</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;美团正式&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FhNIJPJZ0L7ORR0ziOzovaw" target="_blank"&gt;发布&lt;/a&gt; LongCat-Flash-Chat，并同步开源。LongCat-Flash 采用创新性混合专家模型（Mixture-of-Experts, MoE）架构，总参数 560 B，激活参数 18.6B~31.3B（平均 27B），实现了计算效率与性能的双重优化。&lt;/p&gt; 
&lt;p&gt;基准测试评估表明，作为一款非思考型基础模型，LongCat-Flash-Chat 在仅激活少量参数的前提下，性能比肩当下领先的主流模型，尤其在智能体任务中具备突出优势。并且，因为面向推理效率的设计和创新，LongCat-Flash-Chat 具有明显更快的推理速度，更适合于耗时较长的复杂智能体应用。&lt;/p&gt; 
&lt;h4&gt;&lt;strong&gt;技术亮点&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;LongCat-Flash 模型在架构层面引入「零计算专家（Zero-Computation Experts）」机制，总参数量 560 B，每个 token 依据上下文需求仅激活 18.6B~31.3 B 参数，实现算力按需分配和高效利用。为控制总算力消耗，训练过程采用 PID 控制器实时微调专家偏置，将单 token 平均激活量稳定在约 27 B。&lt;/p&gt; 
&lt;p&gt;此外，LongCat-Flash 在层间铺设跨层通道，使 MoE 的通信和计算能很大程度上并行，极大提高了训练和推理效率。配合定制化的底层优化，LongCat-Flash 在 30 天内完成高效训练，并在 H800 上实现单用户 100+ tokens/s 的推理速度。LongCat-Flash 还对常用大模型组件和训练方式进行了改进，使用了超参迁移和模型层叠加的方式进行训练，并结合了多项策略保证训练稳定性，使得训练全程高效且顺利。&lt;/p&gt; 
&lt;p&gt;针对智能体（Agentic）能力，LongCat-Flash 自建了 Agentic 评测集指导数据策略，并在训练全流程进行了全面的优化，包括使用多智能体方法生成多样化高质量的轨迹数据等，实现了优异的智能体能力。&lt;/p&gt; 
&lt;p&gt;通过算法和工程层面的联合设计，LongCat-Flash 在理论上的成本和速度都大幅领先行业同等规模、甚至规模更小的模型；通过系统优化，LongCat-Flash 在 H800 上达成了 100 tokens/s 的生成速度，在保持极致生成速度的同时，输出成本低至 5 元/百万 token。&lt;/p&gt; 
&lt;h4&gt;&lt;strong&gt;性能评估&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;&lt;img height="278" src="https://oscimg.oschina.net/oscnet/up-5e19256b0b1e9bbac8bd2b80e54ff349516.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p style="color:#2b2b2b; margin-left:8px; margin-right:8px"&gt;&lt;span style="color:#2b2b2b"&gt;在&lt;/span&gt;&lt;strong&gt;&lt;span style="color:#2b2b2b"&gt;通用领域知识&lt;/span&gt;&lt;/strong&gt;&lt;span style="color:#2b2b2b"&gt;方面，LongCat-Flash 表现出强劲且全面的性能：在 ArenaHard-V2 基准测试中取得 86.50 的优异成绩，位列所有评估模型中的第二名，充分体现了其在高难度「一对一」对比中的稳健实力。在基础基准测试中仍保持高竞争力，MMLU（多任务语言理解基准）得分为 89.71，CEval（中文通用能力评估基准）得分为 90.44。这些成绩可与目前国内领先的模型比肩，且其参数规模少于 DeepSeek-V3.1、Kimi-K2 等产品，体现出较高的效率。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="color:#2b2b2b; margin-left:8px; margin-right:8px"&gt;&lt;span style="color:#2b2b2b"&gt;在&lt;/span&gt;&lt;strong&gt;&lt;span style="color:#2b2b2b"&gt;智能体（Agentic）工具使用&lt;/span&gt;&lt;/strong&gt;&lt;span style="color:#2b2b2b"&gt;方面，LongCat-Flash 展现出明显优势：即便与参数规模更大的模型相比，其在 τ2-Bench（智能体工具使用基准）中的表现仍超越其他模型；在高复杂度场景下，该模型在 VitaBench（复杂场景智能体基准）中以 24.30 的得分位列第一，彰显出在复杂场景中的强大处理能力。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="color:#2b2b2b; margin-left:8px; margin-right:8px"&gt;&lt;span style="color:#2b2b2b"&gt;在&lt;/span&gt;&lt;strong&gt;&lt;span style="color:#2b2b2b"&gt;编程&lt;/span&gt;&lt;/strong&gt;&lt;span style="color:#2b2b2b"&gt;方面，LongCat-Flash 展现出扎实的实力：其在 TerminalBench（终端命令行任务基准）中，以 39.51 的得分位列第二，体现出在实际智能体命令行任务中的出色熟练度；在 SWE-Bench-Verified（软件工程师能力验证基准）中得分为 60.4，具备较强竞争力。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p style="color:#2b2b2b; margin-left:8px; margin-right:8px"&gt;&lt;span style="color:#2b2b2b"&gt;在&lt;/span&gt;&lt;strong&gt;&lt;span style="color:#2b2b2b"&gt;指令遵循&lt;/span&gt;&lt;/strong&gt;&lt;span style="color:#2b2b2b"&gt;方面，LongCat-Flash 优势显著：在 IFEval（指令遵循评估基准）中以 89.65 的得分位列第一，展现出在遵循复杂且细致指令时的卓越可靠性；此外，在 COLLIE（中文指令遵循基准）和 Meeseeks-zh（中文多场景指令基准）中也斩获最佳成绩，分别为 57.10 和 43.03，凸显其在中英文两类不同语言、不同高难度指令集上的出色驾驭能力。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;strong&gt;模型部署&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;&lt;span style="color:#2b2b2b"&gt;同步提供了分别基于 SGLang 和 vLLM 的两种高效部署方案。以下为使用 SGLang 进行&lt;/span&gt;&lt;span style="color:#2b2b2b"&gt;单机部署的示例：&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&lt;span&gt;python3 -m sglang&lt;span&gt;.launch_server&lt;/span&gt;&amp;nbsp;\&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp;&lt;span&gt;--model&lt;/span&gt;&amp;nbsp;meituan-longcat/LongCat-Flash-Chat-FP8 \&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp;&lt;span&gt;--trust-remote-code&lt;/span&gt;&amp;nbsp;\&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp;&lt;span&gt;--attention-backend&lt;/span&gt;&amp;nbsp;flashinfer \&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp;&lt;span&gt;--enable-ep-moe&lt;/span&gt;&amp;nbsp;\&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp;&lt;span&gt;--tp&lt;/span&gt;&amp;nbsp;&lt;span style="color:#0e9ce5"&gt;8&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369581</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369581</guid>
      <pubDate>Sun, 31 Aug 2025 02:15:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>书生·万象 InternVL3.5 开源发布</title>
      <description>&lt;div class="content"&gt;
                                                                    
                                                        &lt;p&gt;&lt;span style="background-color:#ffffff; color:rgba(0, 0, 0, 0.9)"&gt;上海人工智能实验室（上海 AI 实验室）于近日开源发布了书生·万象 InternVL3.5，通过创新的级联式强化学习、动态视觉分辨率路由与解耦部署架构，实现推理能力、部署效率与通用能力的全面升级。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="background-color:#ffffff; color:rgba(0, 0, 0, 0.9)"&gt;公告称，InternVL3.5 在从 10 亿到 2410 亿参数的全量级版本中均刷新开源模型性能标杆，在通用多模态感知、多模态推理、文本能力等各种任务均达到领先水平，同时在图形用户界面（GUI）智能体、具身空间感知、矢量图像理解与生成等多种特色任务上取得了显著的性能提升。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="248" src="https://oscimg.oschina.net/oscnet/up-936adbc9ef1e0912569573f841adb8bce1e.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span&gt;提供 10 亿至 2410 亿参数共九种尺寸模型，覆盖不同资源需求场景，包含稠密模型和专家混合模型（MoE），首个支持 GPT-OSS 语言模型基座的开源多模态大模型；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span&gt;旗舰模型 InternVL3.5-241B-A28B 在多学科推理基准 MMMU 中获得开源模型最高分 77.7 分，多模态通用感知基准 MMStar 和 OCRBench 分别取得 77.9 分和 90.7 分，超越 GPT-5（75.7 分/80.7 分），文本推理基准 AIME25 和 MMLU-Pro 分别达到 75.6 和 81.3 分，全面领先现有开源多模态大模型；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span&gt;依托级联式强化学习框架（Cascade RL），全系列模型推理性能相比上一代平均提升 16.0 分。其中 InternVL3.5-241B-A28B 综合推理性能达到 66.9 分，超越上一代模型的 54.6 分以及 Claude-3.7-Sonnet 的 53.9 分，在数学推理、逻辑推理等复杂任务中表现突出；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span&gt;借助创新的视觉分辨率路由（ViR）与解耦部署框架（DvD），38B 模型在 896 分辨率下的响应速度大幅提升，单次推理延迟由 369 ms 缩短至 91 ms（提升约 4 倍）；与此同时，轻量化的 InternVL3.5-Flash 在将视觉序列长度减少 50% 的情况下，仍能保持接近 100% 的性能水平；&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;span&gt;加强 GUI 智能体、具身智能体、SVG 图形理解与生成等智能体核心能力，在 ScreenSpot GUI 定位（92.9 分）、VSI-Bench 空间推理（69.5 分）、SGP-Bench 矢量图理解（70.6 分）等任务中超越主流开源模型。&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span&gt;&lt;img alt="" height="586" src="https://oscimg.oschina.net/oscnet/up-718e3bebb7da2d8228ee07369c2e4e5f0d0.webp" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;更多详情可&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F28l0HdFv7baHk2UfReUHtg" target="_blank"&gt;查看官方公告&lt;/a&gt;。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/369578</link>
      <guid isPermaLink="false">https://www.oschina.net/news/369578</guid>
      <pubDate>Sun, 31 Aug 2025 02:02:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
  </channel>
</rss>
