<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>oschina - industry - 简体中文</title>
    <link>https://www.oschina.net/news/industry</link>
    <atom:link href="http://127.0.0.1:30044/oschina/news/industry" rel="self" type="application/rss+xml"/>
    <description>已对该 RSS 进行格式化操作：中英字符之间插入空格、使用直角引号、标点符号修正</description>
    <generator>RSSHub</generator>
    <webMaster>contact@rsshub.app (RSSHub)</webMaster>
    <language>zh-cn</language>
    <lastBuildDate>Thu, 14 Aug 2025 17:23:11 GMT</lastBuildDate>
    <ttl>5</ttl>
    <item>
      <title>微软高管：下一代 Windows 深度整合 AI，实现多模态交互</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;微软 Windows 负责人 Pavan Davuluri 近日在采访中&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.windowscentral.com%2Fmicrosoft%2Fwindows-11%2Fmicrosoft-teases-windows-12-next-version-os-agentic-ai-ambient-computing-copilot" target="_blank"&gt;阐述&lt;/a&gt;了操作系统的发展愿景，强调 AI 将深度融入计算体验，推动交互方式革新。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0814/193722_X7Xj_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;他表示，未来的 Windows 将更具「环境感」，通过语音、语义理解等多模态技术，实现更自然的操作。例如，用户可直接通过语音与电脑对话，系统能理解上下文并智能响应。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-86cbf23e01e3a3048f8eb1c6388e1214b14.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;微软此前发布的《Windows 2030 Vision》视频也提到，语音输入将成为重点，逐步超越传统键鼠操作。&lt;/p&gt; 
&lt;p&gt;此外，Davuluri 透露，AI 智能体的引入将彻底改变 Windows 界面，未来五年可能推出从底层整合 AI 的新系统，如 Windows 12。 尽管语音交互可能需适应期，但微软认为，结合云计算与本地计算的无缝体验，AI 驱动的操作系统将重塑人机交互方式。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/366239</link>
      <guid isPermaLink="false">https://www.oschina.net/news/366239</guid>
      <pubDate>Tue, 12 Aug 2025 11:38:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Siri 迄今为止最大的升级计划：「Linwood」和「Glenwood」亮相</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;苹果正在开发两项重要的人工智能计划，&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fappleinsider.com%2Farticles%2F25%2F08%2F13%2Fsiris-biggest-upgrade-yet-takes-shape-with-linwood-glenwood" target="_blank"&gt;代号为 Linwood 和 Glenwood&lt;/a&gt;，这是一项全面计划的一部分，旨在让苹果在生成人工智能时代变得更加强大和具有竞争力。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0814/192600_Yrmu_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;其中 Linwood 是苹果自主研发的核心项目，由 Apple Foundation Models 团队构建大语言模型，全面重构 Siri 的「大脑」。该系统能理解复杂语义、维持多轮对话，并有望首次安全调用用户个人数据提供个性化建议。&lt;/p&gt; 
&lt;p&gt;苹果软件主管克雷格・费德里吉透露，该项目为「端到端的彻底重建」，承认旧架构已落后，升级后的 Siri 将从被动响应转向主动服务，将集成于 iPhone、iPad 及未来智能家居设备。&lt;/p&gt; 
&lt;p&gt;与之并行的 Glenwood 项目则代表战略转向：苹果打破苹果长期依赖自研技术的传统，正测试使用 Anthropic 的 Claude 作为 Siri 核心引擎，并曾评估 ChatGPT 与谷歌 Gemini。&lt;/p&gt; 
&lt;p&gt;该项目由前 Vision Pro 负责人 Mike Rockwell 统一领导，公司尚未决定采用纯自研、第三方或混合方案，选择 Linwood 可强化隐私与系统整合优势，而引入外部 AI 则可能加速追赶竞争对手。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/366235</link>
      <guid isPermaLink="false">https://www.oschina.net/news/366235</guid>
      <pubDate>Tue, 12 Aug 2025 11:26:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>赛昉推出 RISC-V 开发板 VisionFive 2 Lite，售价约 143 元</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;中国 RISC-V 企业赛昉科技 StarFive 在众筹平台 Kickstarter 推出了一款入门级 RISC-V 开发板 VisionFive 2 Lite。&lt;/p&gt; 
&lt;p&gt;这款单板计算机采用与树莓派相同的 85mm×56mm 外形规格，定价最低的 2GB 内存版本仅售 19.9 美元（约合 143 元人民币）。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-30475d153e4565154d5c7f8d8d3ffd4bb27.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-e209ba50acc1392efe86e0ad5c2228c7a70.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;VisionFive 2 Lite 搭载 2GB / 4GB / 8GB LPDDR4 内存，正面提供了 1 组 40-Pin GPIO、3 个 USB-A 480Mbps、1 个 USB-A 3.0/2.0、1 个仅供电 USB-C、1 个 MIPI-CSI、1 个 MIPI-DSI、1 个 HDMI 2.0、1 个 RJ45 1GbE；反面则包含 1 个 microSD 卡槽和&amp;nbsp;1 个 M.2 2242 PCIe 2.0×1 盘位。&lt;/p&gt; 
&lt;p&gt;赛昉 VisionFive 2 Lite 开发板的&amp;nbsp;2GB 内存版本众筹价 19.9 美元；2GB 内存 + Wi-Fi / BT 无线芯片版本 23 美元；4GB 内存 + 无线芯片版本 30 美元；8GB 内存 + 无线芯片版本 37 美元；8GB 内存 + 64GB eMMC 闪存 + 无线芯片版本 45 美元。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/366232</link>
      <guid isPermaLink="false">https://www.oschina.net/news/366232</guid>
      <pubDate>Tue, 12 Aug 2025 11:13:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>Kaisen Linux 发布最后一次功能更新，作者宣布项目结束</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;Kaisen Linux 是一个面向 IT 专业人士的发行版，特点是提供一套专门的系统管理工具，涵盖了诊断、处理系统和组件的故障等等所有需求，以及众多其他的功能。它支持多种桌面环境，包括 KDE Plasma、LXQt、MATE 和 Xfce，并且有一个独特的 「toram」 模式，可以将整个操作系统加载到内存中，从而方便用户在无硬盘环境中使用。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-f74f49a86a23591fe9996e36cedc385edf8.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Kaisen Linux 开发者 Kevin Chevreuil&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fkaisenlinux.org%2Fblog%2Fkaisenlinuxrolling3.0.php"&gt;宣布&lt;/a&gt;，这个专为 IT 专业人士设计的 Linux 发行版即将停止维护。&lt;/p&gt; 
&lt;p&gt;在 Rolling 3.0 版本的发布中，Chevreuil 提到，由于新的个人和职业项目占据了大量时间，他无法继续开发 Kaisen Linux，这个版本将成为项目的绝唱。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0814/184928_bSsO_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;在 Rolling 3.0 中，Kaisen Linux 基于最新发布的 Debian Trixie（Debian 13），Chevreuil 对软件包列表进行了精简，移除了包括 neofetch、dmraid、hping3 和 reiser4progs 等在内的多个工具。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-f31ae240967ec66730b2dec919d0a87acbc.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;此外，该版本将 KDE Plasma 6 设为默认桌面环境，并用 SDDM 替换了 LightDM 显示管理器，其他更新包括：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;apt upgrade 命令现在重定向到 apt full-upgrade，以防止系统损坏。&lt;/li&gt; 
 &lt;li&gt;新增 kaisen-timeshift-fast-restore 命令，允许用户通过一条命令恢复 BTRFS 快照。&lt;/li&gt; 
 &lt;li&gt;ZFS 管理工具和内核模块现在得到完全支持。&lt;/li&gt; 
 &lt;li&gt;中央化的手册页面现在包含超过 1705 页，修复了链接并新增了分类。&lt;/li&gt; 
 &lt;li&gt;安装程序的自动分区现在只提供 / 和 /home 的分离，以保护快照功能。&lt;/li&gt; 
 &lt;li&gt;GPG 密钥已更新，有效期至 2029 年。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Kaisen Linux 将继续提供长达两年的安全更新支持&lt;/strong&gt;。Chevreuil 表示，这将为用户提供足够的时间来迁移到新的系统。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/366231</link>
      <guid isPermaLink="false">https://www.oschina.net/news/366231</guid>
      <pubDate>Tue, 12 Aug 2025 10:55:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>vivo Pulsar 万亿级消息处理实践（4）-Ansible 运维部署</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;blockquote&gt; 
 &lt;p&gt;作者：Liu Sikang、互联网大数据团队-Luo Mingbo&lt;/p&gt; 
 &lt;p&gt;Pulsar 作为下一代云原生架构的分布式消息中间件，存算分离的架构设计能有效解决大数据场景下分布式消息中间件老牌一哥"Kafka"存在的诸多问题，2021 年 vivo 分布式消息中间件团队正式开启对 Pulsar 的调研，2022 年正式引入 Pulsar 作为大数据场景下的分布式消息中间件，本篇文章主要从 Pulsar 运维痛点、Ansible 简介、Ansible 核心模块详解、Ansible 自动化部署 zk 集群、Ansible 自动化部署 Pulsar 集群几个维度向大家介绍 vivo Pulsar 万亿级消息处理实践之运维部署。&lt;/p&gt; 
 &lt;p&gt;注：本文是《vivo Pulsar 万亿级消息处理实践》系列文章第 4 篇。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;1 分钟看图掌握核心观点👇&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//99d2e2b2667e8f57cfab5dd3873106b2.gif" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;h1&gt;1、简介&lt;/h1&gt; 
&lt;h2&gt;1.1 Pulsar 运维面临的问题&lt;/h2&gt; 
&lt;p&gt;新业务增长快，很多新业务接入需要搭建独立的集群或者资源组。&lt;/p&gt; 
&lt;p&gt;升级频次高，对于 bug 修复，配置更改以及依赖组件替换等，都需要对全集群进行升级、配置更改或组件替换。&lt;/p&gt; 
&lt;p&gt;人力投入大，在集群运维时，需要对公共的执行步骤进行批处理封装，否则会耗费大量人力在集群的部署和升级上。&lt;/p&gt; 
&lt;h2&gt;1.2 什么是 Ansible Playbook&lt;/h2&gt; 
&lt;p&gt;Asnible Playbooks 是 Ansible 自动化工具的核心部分。它是基于 YAML 文件格式，用于在多个主机上执行的任务。通过在 Playbook 中设置变量、处理器、角色和任务标签等功能，可以大大提高自动化脚本的复用性和可维护性。可以理解为批处理任务。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//dac2881aeee293b2e482bf5893266acf.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;上图中我们看到 Playbook 的主要模块如下：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Ansible&lt;/strong&gt;：Ansible 的核心程序。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;HostInventory&lt;/strong&gt;：记录由 Ansible 管理的主机信息，包括端口、密码、ip 等。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Playbooks&lt;/strong&gt;："剧本" YAML 格式文件，多个任务定义在一个文件中，定义主机需要调用哪些模块来完成的功能。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;CoreModules&lt;/strong&gt;：核心模块，主要操作是通过调用核心模块来完成管理任务。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;CustomModules&lt;/strong&gt;：自定义模块，完成核心模块无法完成的功能，支持多种语言。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ConnectionPlugins&lt;/strong&gt;：连接插件，Ansible 和 Host 通信使用。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;2、Playbook 语法&lt;/h1&gt; 
&lt;h2&gt;2.1 书写格式&lt;/h2&gt; 
&lt;p&gt;playbook 常用到的 YMAL 格式：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;文件的第一行应该以 "---" (三个连字符) 开始，表明 YMAL 文件的开始。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;在同一行中，# 之后的内容表示注释，类似于 shell，python 和 ruby。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;YMAL 中的列表元素以 "-" 开头然后紧跟着一个空格，后面为元素内容。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;同一个列表中的元素应该保持相同的缩进。否则会被当做错误处理。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;play 中 hosts，variables，roles，tasks 等对象的表示方法都是键值中间以 ":" 分隔表示，":" 后面还要增加一个空格。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;以下是 Playbook 的基本语法书写格式：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;- name: playbook 的名称
  hosts: 目标主机或主机组     # 可以使用普通的 IP 地址或域名，也可以使用主机组名称
  remote_user: 远程用户        # 使用 SSH 登录远程主机时使用的用户名
  become: yes                # 是否使用特权（例如 sudo）运行命令
  tasks:                     # Playbook 中的任务列表
     - name: 任务名称
       module_name: 参数       # Ansible 模块的名称和参数组成的字典，用于执行操作
       tags:                    # 与该任务相关的标记列表，用于执行特定的任务
         - 标签名称
       when: 条件             # 指定该任务在满足特定条件下才会被执行
       notify: 通知列表       # 指定依赖于该任务的另一个任务列表，当这个任务被执行后会自动触发这些任务
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;2.2 Tasks &amp;amp; Modules&lt;/h2&gt; 
&lt;p&gt;在 Ansible Playbook 的语法中，&lt;/p&gt; 
&lt;p&gt;"Tasks"和"Modules"是两个核心概念。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Tasks（任务）&lt;/strong&gt;：Tasks 是 Playbook 中的操作步骤或任务，它们定义了要在目标主机上执行的操作。可以在 Playbook 中定义一个或多个任务。Tasks 按照顺序执行，并且可以有条件地执行或跳过。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Modules（模块）&lt;/strong&gt;：Modules 提供了执行特定任务的功能单元。每个模块负责处理不同的操作，如管理文件、安装软件包、查询系统信息等。Ansible 提供了许多内置模块，可以满足大多数常见的操作。&lt;/p&gt; 
&lt;p&gt;通过组合不同的模块和任务，可以构建复杂的 Playbooks 来执行各种操作和配置任务。&lt;/p&gt; 
&lt;h2&gt;2.3 任务之间的依赖关系&lt;/h2&gt; 
&lt;p&gt;在 Ansible 的 playbook 中，任务之间可以有依赖关系，你可以使用 dependencies 或者 notify 语句来定义。&lt;/p&gt; 
&lt;h3&gt;2.3.1 使用 dependencies 定义任务依赖关系&lt;/h3&gt; 
&lt;p&gt;如果任务 A 依赖任务 B 完成，可以使用 dependencies 定义任务依赖关系，语法如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;- hosts: web
  tasks:
    - name: Install Nginx
      yum:
        name: nginx
        state: present

    - name: Start Nginx
      service:
        name: nginx
        state: started
      become: true
      dependencies:
        - Install Nginx
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;在上面的示例中，Start Nginx 任务在 Install Nginx 任务完成之后才会执行。如果在执行 Start Nginx 任务之前，Install Nginx 任务未完成或者执行失败，则 Start Nginx 任务也会失败。&lt;/p&gt; 
&lt;h3&gt;2.3.2 使用 notify 定义任务依赖关系&lt;/h3&gt; 
&lt;p&gt;如果任务 A 完成后需要通知任务 B 执行，可以使用 notify 定义任务依赖关系，语法如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;- hosts: web
  tasks:
    - name: Install Nginx
      yum:
        name: nginx
        state: present
      notify:
        -Start Nginx
         
    - name: Start Nginx
      service:
        name: nginx
        state: started
      become: true
      listen: Start Nginx
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;在上面的示例中，Install Nginx 任务完成后会通知 Start Nginx 任务执行。然后 Start Nginx 任务会通过 listen 参数监听，等待通知执行。&lt;/p&gt; 
&lt;p&gt;总之，Ansible 支持在 playbook 中定义任务之间的依赖关系。你可以使用 dependencies 或 notify 语句来定义任务之间的顺序和依赖关系。&lt;/p&gt; 
&lt;h2&gt;2.4 条件判断&lt;/h2&gt; 
&lt;p&gt;在 Playbook 中，可以使用 when 关键字来添加条件判断。when 关键字后面跟一个条件表达式，如果表达式返回 True，则任务会被执行；如果返回 False，则任务会被跳过。&lt;/p&gt; 
&lt;p&gt;条件表达式可以使用 Ansible 的 Jinja2 模板来编写，例如：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;tasks:
  - name: Install Apache if not installed
    package:
      name: apache2
      state: present
    when: ansible_pkg_mgr == 'apt'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;在这个例子中，如果 ansible_pkg_mgr 变量等于"apt"，则安装 Apache；否则跳过这个任务。&lt;/p&gt; 
&lt;p&gt;除了使用任务级别的条件判断，还可以使用 Play 级别的条件判断来控制整个 Playbook 的执行。这可以通过在 Play 的开始处添加 when 关键字来实现，例如：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;- name: Deploy Web App
  hosts: all
  vars:
    deploy_web_app: true
  tasks:
    - name: Install Dependencies
      apt:
        name: "{{ item }}"
        state: present
      with_items:
        - python3
        - python3-pip
      when: deploy_web_app
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;在这个例子中，deploy_web_app 变量的值为 True 时，才会执行任务 Install Dependencies。如果 deploy_web_app 变量的值为 False，则跳过整个 Playbook 的执行。&lt;/p&gt; 
&lt;h2&gt;2.5 循环&lt;/h2&gt; 
&lt;p&gt;在 Playbook 中，可以使用循环结构来遍历列表或其他可迭代对象，并对每个迭代项执行相同的任务。这可以使用 Ansible 的 with_*系列模块来实现。&lt;/p&gt; 
&lt;p&gt;以下是一些常见的循环结构的示例：&lt;/p&gt; 
&lt;h3&gt;2.5.1 使用 with_items 模块来遍历列表&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;tasks:
  - name: Install packages
    apt:
      name: "{{ item }}"
      state: present
    with_items:
      - python3
      - python3-pip
      - git
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;在这个例子中，将依次安装 python3、python3-pip 和 git。&lt;/p&gt; 
&lt;h1&gt;3、Playbook 组织&lt;/h1&gt; 
&lt;h2&gt;3.1 Inclusions&lt;/h2&gt; 
&lt;p&gt;在 Playbook 的组织中，include 和 import 两个指令都可以用来将其他的 yaml 文件（也就是 Tasks 文件）包含到当前的 Playbook 中。&lt;/p&gt; 
&lt;p&gt;它们的区别在于，当主 Playbook 执行到 include 指令时，它将处理包含的文件中的所有任务，并且在处理完之后继续主 Playbook 的执行。而当主 Playbook 执行到 import 指令时，它只会处理被导入的文件中的变量定义，而不会处理任务，任务只有在需要的时候才会被引入执行。&lt;/p&gt; 
&lt;p&gt;下面是一个使用 include 指令包含其他文件的例子：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;- hosts: webservers
  tasks:
    - name: Include web tasks
      include: web-tasks.yml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;在这个例子中，主 Playbook 从 web-tasks.yml 文件中导入任务，并在执行完后继续执行余下的任务。&lt;/p&gt; 
&lt;p&gt;下面是一个使用 import 指令包含其他文件的例子：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;- name: Load variables
  import_vars: vars.yml

- name: Deploy web app
  hosts: webservers
  tasks:
    - name: Install dependencies
      apt:
        name: "{{ item }}"
        state: present
      with_items:
        - python3
        - python3-pip

    - name: Deploy app
      include: app-tasks.yml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;在这个例子中，在主 Playbook 中使用 import_vars 指令来导入变量定义，然后在每个任务中都可以使用这些变量。然后我们使用 include 指令从 app-tasks.yml 文件中包含任务，这些任务可以使用在 vars.yml 文件中定义的变量。这种方式可以在需要时懒加载任务，提高性能。&lt;/p&gt; 
&lt;p&gt;需要注意的是，在被引入的文件中，不能再次使用- hosts:指令定义新的主机组，因为 Ansible 只允许在主 Playbook 中定义主机组。被引入的文件只包含任务，任务必须使用被定义的主机组来指定目标主机。&lt;/p&gt; 
&lt;h2&gt;3.2 Roles&lt;/h2&gt; 
&lt;p&gt;Ansible 的 Roles 是一种组织 Playbook 的方式，它将 Playbook 和相关的变量、模板和其他资源打包在一起，并且可以轻松地在 Playbook 中重用和分享。一个 Role 通常适用于一种操作或功能，比如安装和配置一个应用程序、部署 Web 服务、安装软件包等等。&lt;/p&gt; 
&lt;p&gt;一个 Role 目录通常包含以下文件和目录：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;my-role/
├── README.md
├── defaults/
│   └── main.yml
├── files/
├── handlers/
│   └── main.yml
├── meta/
│   └── main.yml
├── tasks/
│   └── main.yml
├── templates/
├── tests/
│   ├── inventory
│   └── test.yml
└── vars/
    └── main.yml
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;README.md&lt;/strong&gt;：Role 的说明文档。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;defaults/main.yml&lt;/strong&gt;：默认变量定义文件。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;files&lt;/strong&gt;：包含角色使用的文件。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;handlers/main.yml&lt;/strong&gt;：Role 的处理程序。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;meta/main.yml&lt;/strong&gt;：Role 的元数据，例如角色名称、作者、依赖等。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;tasks/main.yml&lt;/strong&gt;：包含 Role 组成部分的主要任务。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;templates&lt;/strong&gt;：包含角色使用的 Jinja2 模板。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;tests&lt;/strong&gt;：Role 的测试脚本。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;vars/main.yml&lt;/strong&gt;：包含 Role 的变量。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;要使用 Role，需要在 Playbook 中定义 roles 扩展，例如：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;- hosts: webservers
  roles:
    - my-role
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;这将运行 my-role 目录中包含的所有任务。&lt;/p&gt; 
&lt;p&gt;通过使用 Role，可以更好地组织和重复使用代码，并提高代码的可读性和可维护性。它还可以帮助您在 Ansible 社区中分享自己的工作，或从其他用户那里获得高质量的 Roles。&lt;/p&gt; 
&lt;h2&gt;3.3 引用/定义变量&lt;/h2&gt; 
&lt;p&gt;在 Playbook 中，可以使用 vars 关键字来定义变量。例如：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;vars:
  my_var: "Hello World"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;这将定义一个名为 my_var 的变量，其值为字符串"Hello World"。&lt;/p&gt; 
&lt;p&gt;要在 Playbook 中访问这个变量，可以使用{{ my_var }}语法。例如：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;tasks:
  - name: Print Message
    debug:
      msg: "{{ my_var }}"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;除了在 vars 中定义变量，还可以通过 set_fact 模块来动态设置变量。例如：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;tasks:
  - name: SetDynamic Variable
    set_fact:
      my_var: "{{ inventory_hostname }} is awesome"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;3.4 使用插件和模板&lt;/h2&gt; 
&lt;p&gt;Ansible 提供了插件和模板的功能，使得在 Playbook 中使用动态内容变得更加简单和方便。&lt;/p&gt; 
&lt;p&gt;插件是一种可以扩展和定制 Ansible 功能的机制，可以在 Playbook 中调用和使用。常见的插件包括 Action、Lookup、Filter、Callback 等。使用插件和模板可以使 Playbook 更加具有可读性和可维护性，使得动态内容的生成更加灵活和方便。&lt;/p&gt; 
&lt;h1&gt;4、服务安装与主机管理&lt;/h1&gt; 
&lt;h2&gt;4.1 安装服务器依赖&lt;/h2&gt; 
&lt;p&gt;Playbook 是 Ansible 的核心组件之一，用于定义和执行一系列任务。在使用 Playbook 之前，需要确保服务器上已经安装了 Ansible 和相关的依赖项。以下是安装服务器依赖的步骤：&lt;/p&gt; 
&lt;h3&gt;4.1.1 安装 Python3 及其相关依赖项&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;sudo apt update
sudo apt-get install -y python3 python3-pip python3-dev build-essential libssl-dev libffi-dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;4.1.2 安装 Ansible&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;sudo apt-add-repository ppa:ansible/ansible
sudo apt update
sudo apt-get install -y ansible
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;4.1.3 （可选）安装 git&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;sudo apt update
sudo apt-get install -y git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;4.1.4 检查 Ansible 是否安装&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;ansible --version
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;这样，您的服务器就已经安装了所需的依赖项以及 Ansible。如果您计划在多台服务器上使用 Ansible，则需要在每台服务器上重复这些步骤。&lt;/p&gt; 
&lt;h2&gt;4.2 配置远程服务器&lt;/h2&gt; 
&lt;p&gt;在使用 Playbook 配置远程服务器之前，需要确保 Ansible 已经正确安装在本地机器上。然后，您需要做以下几个步骤：&lt;/p&gt; 
&lt;h3&gt;4.2.1 创建 inventory 文件&lt;/h3&gt; 
&lt;p&gt;创建新的 inventory 文件，用于定义您要配置的远程服务器的 IP 地址或域名。例如，您可以创建一个名为 inventory 的文件，幷包含以下内容：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;[webservers]
192.168.1.100
192.168.1.101

[dbservers]
192.168.1.102
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;在此示例中，我们定义了两个组，webservers 和 dbservers，并列出了它们中每个服务器的 IP 地址。&lt;/p&gt; 
&lt;h3&gt;4.2.2 编写 Playbook&lt;/h3&gt; 
&lt;p&gt;编写一个 Playbook，用于在远程服务器上执行特定的任务。例如，您可以创建一个名为 web.yml 的 Playbook，幷包含以下内容：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;- name: Install andstart Nginx
  hosts: webservers
  become: true
  tasks:
  - name: Install Nginx
    apt:
      name: nginx
      update_cache: yes
      state: latest
  - name: Start Nginx
    service:
      name: nginx
      state: started
      enabled: true
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;在这个 Playbook 示例中，我们定义了一个名为 Install and start Nginx 的任务，它会在 webservers 组中的服务器上启动 Nginx 服务器。&lt;/p&gt; 
&lt;h3&gt;4.2.3 运行 Playbook&lt;/h3&gt; 
&lt;p&gt;运行 Playbook，在远程服务器上执行配置任务。例如，要在远程服务器上运行示例中的 web.yml Playbook，可以使用以下命令：&lt;/p&gt; 
&lt;p&gt;ansible-playbook -i inventory web.yml&lt;/p&gt; 
&lt;p&gt;在执行此命令后，Ansible 将使用 inventory 文件中定义的远程服务器的 IP 地址，并执行 web.yml Playbook 中定义的任务。&lt;/p&gt; 
&lt;p&gt;这是一个基本的 Playbook 配置远程服务器的示例。需要根据具体的场景和任务需求来进行个性化配置和修改。&lt;/p&gt; 
&lt;h2&gt;4.3 部署应用程序&lt;/h2&gt; 
&lt;p&gt;Playbook 部署应用程序一般步骤：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;1、准备应用程序的部署包。这通常是一个.tar.gz 或.zip 文件，包含应用程序代码、依赖项和其他必要文件。
2、在目标主机上安装所需的依赖项和软件包。例如，在部署 Python 应用程序时，需要安装 Python 解释器、pip 和其他依赖项。
3、创建一个目录用于应用程序的部署。这通常是在目标主机上的一个新目录，例如/home/user/myapp。
4、上传应用程序部署包到目标主机并解压缩。您可以使用 copy 模块将部署包部署到目标主机上。
5、配置应用程序的运行环境。例如，在部署 Flask 应用程序时，需要设置环境变量、安装必要的 Python 包等。
6、配置 Web 服务器以侦听应用程序的请求。例如，您可以使用 Nginx 或 Apache 等 Web 服务器来代理应用程序请求。
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;5、常用模块的 playbook 语法&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;file 模块&lt;/strong&gt;：可以管理文件系统中的文件和目录。下面是该模块的常用参数：&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;copy 模块&lt;/strong&gt;：可以将本地文件复制到远程服务器上。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;unarchive 模块&lt;/strong&gt;：Ansible 中用于将压缩文件解压缩的模块。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;apt 模块&lt;/strong&gt;：可以在 Ubuntu 或 Debian 系统上安装、升级、删除软件包。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;service 模块&lt;/strong&gt;：可以在系统上管理服务。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;user 模块&lt;/strong&gt;：可以管理系统用户。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;shell 模块&lt;/strong&gt;：可以在远程服务器上运行基于命令行的任务。该模块只能运行命令，不能使用管道、重定向和通配符。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;script 模块&lt;/strong&gt;：可以将本地脚本或可执行文件上传到远程服务器并在远程服务器上运行。该模块适用于运行复杂的命令和复杂的脚本。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;template 模块&lt;/strong&gt;：可以将在 Ansible 中定义的 Jinja2 模板应用于远程服务器上的文件。在应用模板时，您可以使用变量来一次生成多个文件的不同版本。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;lineinfile 模块&lt;/strong&gt;：可以从文件中添加、修改或删除单行文本。该模块可用于修改文件中的配置文件或语言文件，或添加新行。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;blockinfile 模块&lt;/strong&gt;：可以在远程服务器文件中添加、修改或删除代码块。该模块可以替代 lineinfile 模块，以单个块更新文件。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;debug 模块&lt;/strong&gt;：可以输出调试信息。该模块在编写 Playbooks 时非常有用，因为可以检查任务的变量和结果。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;6、Ansible 部署 Pulsar 集群运维实战&lt;/h1&gt; 
&lt;h2&gt;6.1 部署 zookeeper 集群&lt;/h2&gt; 
&lt;h3&gt;6.1.1 定义 host 文件&lt;/h3&gt; 
&lt;p&gt;host 文件指定了要在哪些主机上执行任务。在 playbook 中，可以将 hosts 指定为一个变量，也可以通过 -i 参数指定一个主机清单文件，该文件包含要操作的主机列表。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;[all:vars]
ansible_ssh_user=xxx
ansible_ssh_pass=xxx

[zk]
127.xxx.xxx.1 myid=1
127.xxx.xxx.2 myid=2
127.xxx.xxx.3 myid=3
127.xxx.xxx.4 myid=4
127.xxx.xxx.5 myid=5
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;6.1.2 定义变量&lt;/h3&gt; 
&lt;p&gt;group_vars 目录用于存放针对不同主机组的变量文件，其中 all 文件是一种特殊的变量文件，它包含了全局的变量定义，将适用于所有主机组。路径结构如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;group_vars/
├── all
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;在 all 文件中，我们可以定义安装路径、JDK 版本为、zookeeper 版本以及 zookeeper 相关的配置信息。比如：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;inst_home: /opt/bigdata/inst
app_home: /opt/bigdata/app
zk_inst_home: zookeeper-3.6.3
zk_app_home: zookeeper
jdk_inst_home: jdk1.8.0_192
jdk_app_home: jdk
jdk_tgz: jdk1.8.0_192.tar.gz
zk_tgz: zookeeper-3.6.3.tar.gz

cluster_name=clusterName
client_port=2181
server_port1=2881
server_port2=2882
jmx_port=9012
admin_port=18080
dataDir="/data/bigdata/zookeeper_{{cluster_name}}/zkDataDir"
dataLogDir="/data/bigdata/zookeeper_{{cluster_name}}/zkDataLogDir"
zoo_log_dir="/opt/bigdata/inst/zookeeper-3.6.3-{{cluster_name}}/logs/"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;6.1.3 编辑 roles 模块&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;① check_port：检查端口&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;判断配置的端口是否被占用，如果被占用，则不能执行后续的步骤。&lt;/p&gt; 
&lt;p&gt;目录结构如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;check_port/
├── tasks/
│   └── main.yml
main.yml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;#循环检查端口是否是停用状态&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;- name: Check port
  wait_for:
    host: "{{ inventory_hostname }}"
    port: "{{ item }}"
    delay: 2
    timeout: 3
    state: stopped
  register: result
  with_items:
    - "{{ client_port }}"
    - "{{ server_port1 }}"
    - "{{ server_port2 }}"
    - "{{ jmx_port }}"
    - "{{ admin_port }}"

- name: print result
  debug:
    msg: "Port {{ item.item }} is {{ item.state }}"
  with_items: "{{ result.results }}"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;② dispatch_zk：分发安装包&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;目录结构如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;dispatch_zk/
├── files/
│   └── zookeeper-3.6.3.tar.gz
├── tasks/
│   └── main.yml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;files：放 zookeeper 安装包文件。&lt;/p&gt; 
&lt;p&gt;main.yml&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;#分发 zk 安装包并解压到/tmp 路径下
- name: dispatch_zk
  unarchive:
    src: "{{zk_tgz}}"
    dest: "/tmp"
    mode: 755
    owner: root
    group: root
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;③ config_zk：配置 zookeeper&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;目录结构如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;config_zk/
├── tasks/
│   └── main.yml
├── templates/
│   └── zoo.cfg
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;main.yml&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;#zoo.cfg 模板文件应用到指定的路径下
- name: zoo.cfg
  template:
    src: zoo.cfg
    dest: "{{ app_home }}/zk-{{ cluster_name }}/conf"
#创建 zoo_log_dir 目录
- name: mkdir forlog
  shell: mkdir -p "{{zoo_log_dir}}"

#创建 zk 数据目录
- name: mkdir for dataDir
  shell: mkdir -p "{{dataDir}}"

#创建 zk 日志目录
- name: mkdir for dataLogDir
  shell: mkdir -p "{{dataLogDir}}"

#myid 文件中输入每台主机的编号
- name: myid file
  shell: echo "{{myid}}" &amp;gt; {{dataDir}}/myid
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;zoo.cfg：zookeeper 配置文件模板。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;tickTime=2000
initLimit=10
syncLimit=5
maxClientCnxns=65535
autopurge.snapRetainCount=30
autopurge.purgeInterval=48
clientPort={{client_port}}
admin.serverPort={{admin_port}}
dataDir={{dataDir}}
dataLogDir={{dataLogDir}}
{% for host in groups.zk%}
server.{{ hostvars[host]['myid'] }}={{host}}:{{server_port1}}:{{server_port2}}
{% endfor %}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;④ deploy_zk：部署 zookeeper 服务&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;目录结构如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;deploy_zk/
├── files/
│   └── env.sh
│   └── jdk1.8.0_192.tar.gz
├── tasks/
│   └── main.yml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;env.sh：jdk 环境变量配置&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;JAVA_HOME=/opt/bigdata/app/jdk
JRE_HOME=$JAVA_HOME/jre
PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$PATH
CLASSPATH=$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib/rt.jar:$CLASSPATH
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;main.yml&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;#创建/opt/bigdata/inst 目录
- name: mkdir for inst_home
  shell: mkdir -p {{ inst_home }}

#创建/opt/bigdata/app 目录
- name: mkdir for app_home
  shell: mkdir -p {{ app_home }}

#注册 zk_dir 变量
- name: stat_dir
  stat: path={{ inst_home }}/{{zk_inst_home}}-{{cluster_name}}
  register: zk_dir

#当 zk_dir 存在时，将/tmp 路径安装包移到指定目录并重命名
- name: rename zookeeper dir
  command: mv /tmp/{{zk_inst_home}} {{ inst_home }}/{{zk_inst_home}}-{{cluster_name}}
  when: zk_dir.stat.exists == False

#创建 zk 集群软连接
- name: soft link
  file:
    path: "{{ app_home }}/zk-{{ cluster_name }}"
    src: "{{ inst_home }}/{{ zk_inst_home }}-{{ cluster_name }}"
    state: link

#分发并解压 jdk 安装包
- name: deploy jdk
  unarchive:
    src: "{{ jdk_tgz }}"
    dest: "{{ inst_home }}"

#创建 jdk 软连接
- name: create soft link for jdk
  file:
    path: "{{ app_home }}/{{ jdk_app_home }}"
    src: "{{ inst_home }}/{{ jdk_inst_home }}"
    state: link

#运行 jdk 环境变量，使其生效
- name: env
  script: env.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;⑤ start_zk：启动 zookeeper 服务&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;目录结构如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;start_zk/
├── tasks/
│   └── main.yml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;main.yml&lt;/p&gt; 
&lt;p&gt;#启动 zk 服务&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;- name: start zookeeper
  shell: cd {{ app_home }}/zk-{{cluster_name}}; sh bin/zkServer.sh start
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;6.1.4 编辑任务执行和启动脚本&lt;/h3&gt; 
&lt;p&gt;zookeeper.yml：任务执行脚本&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;---
- name: check_port
  hosts: zk
  remote_user: root
  roles:
    - check_port
  tags: check_port

- name: dispatch_zk
  hosts: zk
  remote_user: root
  roles:
    - dispatch_zk
  tags: dispatch_zk

- name: deploy_zk
  hosts: zk
  remote_user: root
  roles:
    - deploy_zk
  tags: deploy_zk

- name: config_zk
  hosts: zk
  remote_user: root
  roles:
    - config_zk
  tags: config_zk
   
- name: start_zk
  hosts: zk
  remote_user: root
  roles:
    - start_zk
  tags: start_zk
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;6.1.5 部署并启动 zookeeper 服务&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;# 部署并启动 zookeeper 服务
ansible-playbook -i hosts-clusterName zookeeper.yml

#只检查端口和分发安装包
ansible-playbook -i hosts-clusterName zookeeper.yml --tags "check_port,dispatch_packages"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;6.2 部署 Pulsar 集群&lt;/h2&gt; 
&lt;h3&gt;6.2.1 定义 hosts 文件&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;[all:vars]
ansible_ssh_user=xxx
ansible_ssh_pass=xxx

[pulsar]
127.xxx.xxx.1
127.xxx.xxx.2
127.xxx.xxx.3
127.xxx.xxx.4
127.xxx.xxx.5
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;6.2.2 定义全局变量&lt;/h3&gt; 
&lt;p&gt;group_vars 目录用于存放针对不同主机组的变量文件，其中 all 文件是一种特殊的变量文件，它包含了全局的变量定义，将适用于所有主机组。路径结构如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;group_vars/
├── all
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;all 文件内容中定义变量信息，如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;bigdata_home: /opt/bigdata
inst_home: /opt/bigdata/inst
app_home: /opt/bigdata/app
pulsar_app_home: pulsar
pulsar_inst_home: apache-pulsar-2.9.2-1.3
pulsar_tgz: apache-pulsar-2.9.2-1.3-bin.tar.gz
pulsar_conf: "{{ app_home }}/pulsar/conf"
secret_key_dir: "{{ app_home }}/pulsar/data"

#bookkeeper.conf
ledgerDirectories: /data1/bookkeeper/ledger,/data2/bookkeeper/ledger,/data3/bookkeeper/ledger,/data4/bookkeeper/ledger

#broker.conf or client.conf
zkServers: "127.xxx.xxx.1:2183,127.xxx.xxx.2:2183,127.xxx.xxx.3:2183/clusterName"
clusterName: wenzhu
webServiceUrl: http://clusterNamexxxx:8080
brokerServiceUrl: pulsar://clusterNamexxxx:6650
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;6.2.3 编辑 roles 模块&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;① dispatch_pulsar：分发安装包&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;目录结构如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;dispatch_pulsar/
├── files/
│   └── apache-pulsar-2.9.2-1.3-bin.tar.gz
├── tasks/
│   └── main.yml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;main.yml&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;#创建 inst_home 定义的目录
- name: mkdir_inst_home
  file:
    path: "{{ inst_home }}"
    state: directory

#创建 app_home 定义的目录
- name: mkdir_app_home
  file:
    path: "{{ app_home }}"
    state: directory

#分发并解压 pulsar 安装包到指定目录
- name: dispatch_packages
  unarchive:
    src: "{{ pulsar_tgz }}"
    dest: "{{ inst_home }}"

#创建 pulsar 软连接
- name: soft_link
  file:
    path: "{{ app_home }}/pulsar"
    src: "{{ inst_home }}/{{ pulsar_inst_home }}"
    state: link
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;② check_nar：校验分层存储和 kop 扩展的依赖包&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;目录结构如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;check_nar/
├── tasks/
│   └── main.yml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;main.yml&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;#匹配指定路径 protocols 和 offloaders 下是否有 nar 后缀的文件
- name: check nar
  find:
    paths: "{{ app_home }}/pulsar/{{ item }}/"
    patterns: "*.nar"
  register: result
  with_items:
    - "offloaders"
    - "protocols"

#设置文件匹配的结果（大于 0 表示文件存在）
- name: set nar_files_exist variable
  set_fact:
    nar_files_exist_{{item.item}}: "{{ item.matched &amp;gt; 0 }}"
  with_items: "{{ result.results }}"

#如果文件不存在，进行提示
- name: nar files not exist
  fail:
    msg: "{{ item.item }} nar files not found"
  when: nar_files_exist_{{ item.item }} == false
  ignore_errors: true
  with_items: "{{ result.results }}"

#如果文件存在，列出存在的文件名
- name: print nar files list
  debug:
    msg: "{{ item.files | map(attribute='path') | list }}"
  when: nar_files_exist_{{item.item}}
  with_items: "{{ result.results }}"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;③ config_pulsar：配置 pulsar&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;目录结构如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;config_pulsar/
├── tasks/
│   └── main.yml
├── templates/
│   └── bkenv.sh
│   └── pulsar_env.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;main.yml&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;#匹配 broker.conf 中的 advertisedAddress 值并设置为远程主机 ip 地址
- name: config_advertisedAddress
  lineinfile:
    path: "{{ pulsar_conf }}/broker.conf"
    regexp: "^advertisedAddress="
    line: "advertisedAddress={{ inventory_hostname }}"

#配置 broker.conf 中的 zookeeperServers 值
- name: config_zookeeperServers
  lineinfile:
    path: "{{ pulsar_conf }}/broker.conf"
    regexp: "^zookeeperServers="
    line: "zookeeperServers={{ zkServers }}"

#配置 broker.conf 中的 clusterName 值
- name: config_clusterName
  lineinfile:
    path: "{{ pulsar_conf }}/broker.conf"
    regexp: "^clusterName="
    line: "clusterName={{ clusterName }}"
     
#配置 broker.conf 中的 kafkaAdvertisedListeners 值
- name: config_kafkaAdvertisedListeners
  lineinfile:
    path: "{{ pulsar_conf }}/broker.conf"
    regexp: "^kafkaAdvertisedListeners="
    line: "kafkaAdvertisedListeners=PLAINTEXT://{{ inventory_hostname }}:9093"

#配置 bookkeeper.conf 中的 advertisedAddress 值，设置为主机 ip 地址
- name: config_bk_advertisedAddress
  lineinfile:
    path: "{{ pulsar_conf }}/bookkeeper.conf"
    regexp: "^advertisedAddress="
    line: "advertisedAddress={{ inventory_hostname }}"

#将模板文件 bkenv.sh 应用到 pulsar 的配置文件中
- name: config_bkenv.sh
  template:
    src: bkenv.sh
    dest: "{{ pulsar_conf }}"

#将模板文件 pulsar_env.sh 应用到 pulsar 的配置文件中
- name: config_pulsar_env.sh
  template:
    src: pulsar_env.sh
    dest: "{{ pulsar_conf }}"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;④ create_data_dir：创建存储数据的目录&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;目录结构如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;create_data_dir/
├── tasks/
│   └── main.yml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;main.yml&lt;/p&gt; 
&lt;p&gt;#循环创建 with_items 中的数据目录&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;- name: mkdir_data_dir
  file:
    path: "{{ item }}"
    state: directory
  with_items:
    - /data1/bookkeeper/ledger
    - /data2/bookkeeper/ledger
    - /data3/bookkeeper/ledger
    - /data4/bookkeeper/ledger
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;⑤ config_secret_key：配置安全秘钥&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;目录结构如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;config_secret_key/
├── files/
│   └── admin-secret.key
├── tasks/
│   └── main.yml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;main.yml&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;#创建存放安全秘钥的目录
- name: create_secret_key_dir
  file:
    path: "{{ secret_key_dir }}"
    owner: root
    group: root
    state: directory

#将安装秘钥文件分发到指定的路径下
- name: dispatch_secret.key
  copy:
    src: admin-secret.key
    dest: "{{ secret_key_dir }}"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;⑥ init_meta：初始化集群元数据&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;目录结构如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;init_meta/
├── tasks/
│   └── main.yml
├── templates/
│   └── init_meta.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;main.yml&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;#应用 init_meta.sh 脚本到远程主机
- name: scp init_meta.sh
  template:
    src: init_meta.sh
    dest: "{{ app_home }}/pulsar"

#执行初始化脚本文件
- name: init_meta
  shell: nohup sh {{ app_home }}/pulsar/init_meta.sh &amp;gt; {{ app_home }}/pulsar/init.log2&amp;gt;&amp;amp;1 &amp;amp;

#等待 20s 查询初始化日志中是否出现初始化成功的日志
- name: wait20s
  wait_for:
    path: "{{ app_home }}/pulsar/init.log"
    search_regex: "Cluster metadata for '{{ clusterName }}' setup correctly"
    delay: 20

#杀掉集群元数据初始化进程
- name: kill metadata
  shell: ps -efww|grep PulsarClusterMetadataSetup|grep -v grep|cut -c 9-15|xargs kill -9
init_meta.sh：初始化集群元数据脚本

{{ app_home }}/pulsar/bin/pulsar initialize-cluster-metadata \
--cluster {{ clusterName }} \
--zookeeper {{ zkServers }} \
--configuration-store {{ zkServers }} \
--web-service-url {{ webServiceUrl }} \
--broker-service-url {{ brokerServiceUrl }}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;⑦ start_service：启动 broker 和 bookkeeper 服务&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;start_service/
├── tasks/
│   └── main.yml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;main.yml&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;#启动远程主机 bookkeeper 服务
- name: start bookie
  shell: sh {{ app_home }}/pulsar/bin/pulsar-daemon start bookie

#启动远程主机 broker 服务
- name: start broker
  shell: sh {{ app_home }}/pulsar/bin/pulsar-daemon start broker
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;6.2.4 编辑任务执行脚本&lt;/h3&gt; 
&lt;p&gt;pulsar.yml：任务执行脚本&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;---
#分发 pulsar 安装包
- name: dispatch_pulsar
  hosts: pulsar
  remote_user: root
  become: yes
  become_flags: '-i'
  roles:
    - dispatch_pulsar
  tags: dispatch_pulsar

#检查安装包中 kop 和分层存储 nar 包是否存在
- name: check_nar
  hosts: pulsar
  remote_user: root
  roles:
    - check_nar
  tags: check_nar

#修改 pulsar 配置
- name: config_pulsar
  hosts: pulsar
  remote_user: root
  become: yes
  become_flags: '-i'
  roles:
    - config_pulsar
  tags: config_pulsar
   
  #创建磁盘数据目录
- name: create_data_dir
  hosts: pulsar
  remote_user: root
  become: yes
  become_flags: '-i'
  roles:
    - create_data_dir
  tags: create_data_dir

#配置证书文件
- name: config_secret_key
  hosts: pulsar
  remote_user: root
  become: yes
  become_flags: '-i'
  roles:
    - config_secret_key
  tags: config_secret_key

#初始化 meta 信息
- name: init_meta
  hosts: pulsar[0]
  remote_user: root
  become: yes
  become_flags: '-i'
  roles:
    - init_meta
  tags: init_meta
   
#启动 broker 和 bookkeeper 服务
- name: start_service
  hosts: pulsar
  remote_user: root
  become: yes
  become_flags: '-i'
  roles:
    - start_service
  tags: start_service
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;6.2.5 执行 playbook 任务&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;#执行所有 pulsar.yml 中的任务
ansible-playbook -i hosts pulsar.yml

#只执行 pulsar.yml 中标签为 dispatch_pulsar,check_nar 的任务
ansible-playbook -i hosts pulsar.yml --tags "dispatch_pulsar,check_nar"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;7、Playbooks 运维 Pulsar 集群总结&lt;/h1&gt; 
&lt;h2&gt;7.1 Pulsar 运维实践总结&lt;/h2&gt; 
&lt;p&gt;Pulsar 作为新一代云原生架构的分布式消息中间件，目前再超大流量规模、海量分区、超高 QPS 等场景下缺乏长时间的稳定性验证，在极端场景下还存在较多稳定性风险，当前社区版本迭代活跃；Pulsar 集群在 vivo 内部日均处理消息达万亿+，需要不断的合并社区 issue 及灰度升级高版本。运维事项较多、投入的运维人力较大。vivo 分布式消息中间件团队通过借助 Ansible 的模块化、任务依赖、配置 check、批量脚本执行等能力实现 Pulsar 集群从 zk 集群搭建、Pulsar 安装包编译、自动化配置填充、批量分发部署、服务启动的一键运维部署能力。大大缩减了 Pulsar 集群的运维人力投入，Pulsar 组件存算分离的架构设计优秀，但部署配置项非常繁杂，通过自动化配置填充可有效规避配置信息不一致、版本不一致等高频错误。&lt;/p&gt; 
&lt;h2&gt;7.2 playbooks 服务部署步骤&lt;/h2&gt; 
&lt;p&gt;根据以上实战经验，我们可以总结出部署某个服务时编写 playbooks 脚本的一般步骤如下：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://oscimg.oschina.net/oscnet//9d95c4a3f898dea31aa737cfc4ae82ae.png" alt="图片" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Ansible 更多运维实践可参考：&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fansible%2Fansible-examples" target="_blank"&gt;https://github.com/ansible/ansible-examples&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;猜你喜欢&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzI4NjY4MTU5Nw%3D%3D%26mid%3D2247501335%26idx%3D1%26sn%3D3701be0b8b7b789e29c1ca53ba142e9d%26scene%3D21%23wechat_redirect" target="_blank"&gt;vivo Pulsar 万亿级消息处理实践（1）-数据发送原理解析和性能调优&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzI4NjY4MTU5Nw%3D%3D%26mid%3D2247501426%26idx%3D1%26sn%3D76c04879cfa2c6b38a731b5c49f19d3a%26scene%3D21%23wechat_redirect" target="_blank"&gt;vivo Pulsar 万亿级消息处理实践（2）-从 0 到 1 建设 Pulsar 指标监控链路&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzI4NjY4MTU5Nw%3D%3D%26mid%3D2247505310%26idx%3D1%26sn%3D541f7a56e7db9a0909f649a412c82d6d%26scene%3D21%23wechat_redirect" target="_blank"&gt;vivo Pulsar 万亿级消息处理实践（3）-KoP 指标异常修复&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/vivotech/blog/18688145</link>
      <guid isPermaLink="false">https://my.oschina.net/vivotech/blog/18688145</guid>
      <pubDate>Tue, 12 Aug 2025 10:40:00 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>马斯克必须面对 OpenAI 提出的骚扰指控</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;&lt;span style="color:#000000"&gt;OpenAI 此前在诉状中指称，埃隆·马斯克（Elon Musk）在法庭内外对公司展开了「长达数年的骚扰行动」。一名联邦法官已裁定，马斯克必须面对这一指控。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;当地时间本周二，美国地方法院法官伊冯·冈萨雷斯·罗杰斯（Yvonne Gonzalez Rogers）驳回了马斯克的一项请求。OpenAI 指控马斯克利用法律诉讼、社交媒体发文及媒体言论等手段意图打压公司，从而为他自己创办的生成式人工智能公司 xAI 争取竞争优势，而马斯克请求法官驳回的正是这些指控。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="326" src="https://oscimg.oschina.net/oscnet/up-e9920e1f6a481cb62d069a91a566ad84a38.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;此项裁决为双方始于去年的法律战带来了最新转折。当时，马斯克指控 OpenAI 背弃非营利组织的创立初衷，2019 年也就是马斯克离开 OpenAI 董事会的次年，开始从微软接受数十亿美元的投资。马斯克本人则在 2023 年创立了 xAI 公司。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;罗杰斯法官并未就 OpenAI 反诉状中「马斯克试图持续打压公司」的指控本身作出实质性评判，但她裁定，OpenAI 于今年 4 月份提交的反诉在法律上理由充分，可以继续推进。此外，罗杰斯法官还驳回了马斯克针对 OpenAI 和微软的部分指控。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;法官指出，双方都互相指责对方虚伪。她在裁决书中写道：「双方的博弈伎俩昭然若揭，各自都出尔反尔。」&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;此案计划于明年 3 月在加州奥克兰联邦法院开庭审理。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/366220</link>
      <guid isPermaLink="false">https://www.oschina.net/news/366220</guid>
      <pubDate>Tue, 12 Aug 2025 10:19:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>港大联手月之暗面等机构开源 OpenCUA，可自主操作电脑的 Agent</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;香港大学 XLANG Lab 联合月之暗面、斯坦福大学等机构，正式开源了一个名为 OpenCUA 的完整框架，旨在帮助开发者低门槛地构建和扩展 CUA（Computer-Use Agent，计算机使用智能体）。&lt;/p&gt; 
&lt;p&gt;该框架包括：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;无缝捕获人类计算机使用演示的注释基础设施&lt;/li&gt; 
 &lt;li&gt;第一个跨越 3 个操作系统以及超 200 个应用程序和网站的大规模计算机使用任务数据集 AgentNet&lt;/li&gt; 
 &lt;li&gt;一个可扩展的、能将演示转换为具有反思性长思维链推理 「状态 - 动作」 对的工作流程。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;香港大学计算机科学系助理教授 Tao Yu（余涛）为项目负责人，月之暗面、斯坦福大学、滑铁卢大学、卡内基梅隆大学的研究人员参与，月之暗面创始人、CEO 杨植麟在作者名单之列。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0814/181052_drco_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;据介绍，OpenCUA 通过开源完整的数据、工具和模型，让 「人人都能打造自己的专属电脑智能体」，并已在关键基准上超越 GPT-4o，成为当前最强的开源 CUA 方案。&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;相关链接&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;论文地址：&lt;em&gt;https://arxiv.org/pdf/2508.09123&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;OpenCUA 主页（工具、模型、数据集）：&lt;em&gt;https://opencua.xlang.ai&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Hugging Face 地址：&lt;em&gt;https://huggingface.co/collections/xlangai/opencua-open-foundations-for-computer-use-agents-6882014ebecdbbe46074a68d&lt;/em&gt;&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/366219</link>
      <guid isPermaLink="false">https://www.oschina.net/news/366219</guid>
      <pubDate>Tue, 12 Aug 2025 10:14:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>英伟达发布包含 300 万条高质量样本的视觉语言模型训练数据集</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;英伟达&lt;u&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fblog%2Fnvidia%2Fnvidia-vlm-dataset-v1" target="_blank"&gt;发布&lt;/a&gt;&lt;/u&gt;了一个包含 300 万高质量样本的视觉语言模型训练数据集，以支持 OCR、VQA 和图像字幕生成等多种应用。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet/up-ea78950d32fa51e2aeb1dd78dc70cd8336b.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0814/175743_fO0l_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;数据集构成&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;67.0% 视觉问答（VQA）样本&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;28.4% OCR 样本&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;4.6% 图像描述（Captioning）样本&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;主要用途&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;文档理解&lt;/strong&gt;：支持复杂版面、表格、图文混排的 OCR 与内容提取。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;企业级 AI 开发&lt;/strong&gt;：数据已清除版权限制，&lt;strong&gt;可直接商用&lt;/strong&gt;。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;模型训练支持&lt;/strong&gt;：配套 &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.nvidia.com%2Fnemo-framework%2Fuser-guide%2F24.09%2Fnemotoolkit%2Fmultimodal%2Fmllm%2Fdatasets.html" target="_blank"&gt;NVIDIA NeMo Curator&lt;/a&gt; 工具，便于进一步清洗和定制。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;数据来源与构建方式&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;基于开源数据集重新标注，确保可商用；&lt;/li&gt; 
 &lt;li&gt;使用 NVIDIA 自研模型进行增强，如加入链式思考（Chain-of-Thought）解释、模板化问答生成、答案扩展等；&lt;/li&gt; 
 &lt;li&gt;提供中英双语的 OCR 数据，涵盖字符级、词级、页面级标注。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;模型配套&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;该数据集是 &lt;strong&gt;Llama 3.1 Nemotron Nano VL 8B&lt;/strong&gt; 模型的训练基础，该模型在 OCRBench V2、DocVQA、ChartQA 等基准测试中表现领先，已作为 NVIDIA NIM API 和 Hugging Face 模型库的一部分开放使用。&lt;/p&gt; 
&lt;p&gt;如需获取数据集，可直接访问 Hugging Face 页面：&lt;em&gt;https://huggingface.co/datasets/nvidia/Llama-Nemotron-VLM-Dataset-v1&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/366217</link>
      <guid isPermaLink="false">https://www.oschina.net/news/366217</guid>
      <pubDate>Tue, 12 Aug 2025 09:56:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>甲骨文云计算部门启动裁员</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;外媒报道称，多位知情人士向媒体透露，全球科技巨头甲骨文（Oracle）正在其云计算业务部门推进裁员计划，受影响员工已于本周陆续收到通知。此次裁员被视为甲骨文在持续加码人工智能（AI）基础设施投资背景下，优化运营成本、提升组织效率的重要举措。&lt;/p&gt; 
&lt;p&gt;&lt;img height="304" src="https://oscimg.oschina.net/oscnet/up-434e52918013b94f4053ef878cd50cb724e.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;据三位不愿具名的知情人士称，甲骨文本轮裁员覆盖云计算部门多个团队，但具体裁撤比例及地区分布尚未明确。其中两位人士强调，部分员工的离职与年度绩效评估结果直接挂钩，甲骨文或借此机会淘汰低效岗位，同时保留核心战略领域人才。&lt;/p&gt; 
&lt;p&gt;「这不是全面收缩，而是针对性优化。」一位接近甲骨文内部的消息人士表示，「公司仍在为 AI 相关项目招聘高端工程师，但希望团队更加精干。」&lt;/p&gt; 
&lt;p&gt;甲骨文近年来在云计算与 AI 领域动作频频。2024 年，公司宣布未来三年将投入超 200 亿美元扩建数据中心，以支持其 AI 训练与推理服务，并与英伟达等企业深化合作，构建高性能计算网络。然而，激进扩张也带来成本压力——最新财报显示，甲骨文 2025 财年第二季度资本支出同比增长 45%，而运营利润率较去年同期下滑 2 个百分点。&lt;/p&gt; 
&lt;p&gt;尽管部分团队面临调整，但甲骨文云计算部门仍在开放多个职位。根据 LinkedIn 招聘信息，该公司近期发布了针对 AI 基础设施架构师、云安全专家及数据库优化工程师等岗位的招募需求，工作地点涵盖美国、印度及欧洲多地。&lt;/p&gt; 
&lt;p&gt;甲骨文并非唯一一家在扩张期优化成本的企业。2025 年以来，亚马逊 AWS、微软 Azure 等云服务提供商均被曝出调整团队结构，重点削减非战略业务线人员，同时加大对生成式 AI、数据安全等领域的投入。&lt;span style="background-color:#ffffff; color:#222222"&gt;（环球网科技，青山）&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/366210</link>
      <guid isPermaLink="false">https://www.oschina.net/news/366210</guid>
      <pubDate>Tue, 12 Aug 2025 09:34:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>国内多数模型训练使用中文数据已超 60%</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;&lt;span style="color:#000000"&gt;国务院新闻办公室举行「高质量完成‘十四五’规划」系列主题新闻发布会。国家数据局局长刘烈宏在新闻发布会上&lt;a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Ft.cn%2FA6s9V2KV" target="_blank"&gt;表示&lt;/a&gt;，当前我国网民数量达到 11.23 亿人，互联网普及率达到 79.7%。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img height="285" src="https://oscimg.oschina.net/oscnet/up-a7bc5b1720f5064d91d4f1b64ef3359ba32.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;刘烈宏表示，我国数据产业处于快速发展阶段，数据产业链加速形成。据国家数据发展研究院研究，2024 年，全国数据企业数量超过 40 万家，数据产业规模达 5.86 万亿元，较「十三五」末增长 117%，预计未来几年仍将保持较高的增长水平。数据「采存算管用」全链条焕新迭代，催生出「数据即服务」「知识即服务」「模型即服务」等新模式、新业态。2024 年，上市数据企业平均研发投入较「十三五」末增长 79%，年均增速 15.7%，产业链创新活力持续增强。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;作为人工智能发展的三大核心要素之一，数据在推动「人工智能+」过程中发挥着关键作用，特别是高质量数据集的建设至关重要。例如在医疗健康领域，通过标注的医学影像高质量数据集，模型的疾病诊断准确率可以提升 15% 以上。2024 年初，我国日均 Token（即通常所说的词元，处理文本的最小数据单元）的消耗量为 1 千亿，截至今年 6 月底，日均 Token 消耗量已经突破 30 万亿，1 年半时间增长了 300 多倍，反映出我国人工智能应用规模的快速增长。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;截至今年 6 月底，我国已经建设高质量数据集超过 3.5 万个，总体量超过了 400PB（1PB 可存储约 5 亿张 2MB 大小的高清照片，400PB 的总量相当于中国国家图书馆数字资源总量的 140 倍左右）。人工智能模型的训练也推动了数据交易需求的攀升，截至今年 6 月底，各地高质量数据集累计交易额近 40 亿元，数据交易机构挂牌的高质量数据集总规模达到了 246PB。目前已布局成都、沈阳、合肥等 7 个数据标注基地，助力高质量数据集的建设。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="color:#000000"&gt;中文数据在国内大模型的训练性能提升方面发挥着重要作用。经过一段时间的努力，国内多数模型训练使用的中文数据占比已经超过了 60%，有的模型已达到 80%。中文高质量数据的开发和供给能力持续增强，推动我国人工智能模型性能的快速提升。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/366203</link>
      <guid isPermaLink="false">https://www.oschina.net/news/366203</guid>
      <pubDate>Tue, 12 Aug 2025 09:17:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>ECA - 与编辑器无关的 AI 结对编程功能</title>
      <description>&lt;div class="content"&gt;
                                                                                                                                                                        
                                                                                    &lt;p&gt;&lt;span style="background-color:#ffffff; color:#1f2328"&gt;Editor Code Assistant&lt;span&gt;&amp;nbsp;&lt;/span&gt;(ECA) - 人工智能结对编程功能，不受编辑器限制。&lt;/span&gt;旨在轻松连接 LLM 和编辑器，并通过定义明确的协议为 AI 结对编程提供最佳的用户体验。服务器采用 Clojure 编写，并深受&amp;nbsp;&lt;a href="https://microsoft.github.io/language-server-protocol/"&gt;LSP 协议&lt;/a&gt;的启发，LSP 协议是此类集成的一个成功案例。&lt;/p&gt;

&lt;p style="text-align:start"&gt;&lt;span&gt;&lt;span&gt;&lt;span style="color:#1f2328"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span style="background-color:#ffffff"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;该协议使其他编辑器更容易集成，并且在中间有一个服务器有助于快速添加更多功能，以下是一些示例：&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;工具调用管理&lt;/li&gt;
&lt;li&gt;多 LLM 互动&lt;/li&gt;
&lt;li&gt;功能使用情况遥测&lt;/li&gt;
&lt;li&gt;为任何编辑器配置的单一方法&lt;/li&gt;
&lt;li&gt;相同的用户体验，易于人员和团队加入。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img height="326" src="https://static.oschina.net/uploads/space/2025/0814/141201_GYAY_4252687.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;与编辑器无关&lt;/strong&gt;：适合任何编辑器集成的协议。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;单一配置&lt;/strong&gt;：通过全局或本地配置配置 eca，使其在任何编辑器中都能正常工作。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;聊天&lt;/strong&gt;界面：提出问题、审查代码、共同编写代码。&lt;/li&gt;
&lt;li&gt;&amp;nbsp;&lt;strong&gt;Agentic&lt;/strong&gt;：让 LLM 使用其原生工具和你可以配置的 MCP 作为代理工作。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;背景&lt;/strong&gt;：支持：向 LLM 提供有关你的代码的更多详细信息，包括 MCP 资源和提示。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多模型&lt;/strong&gt;：OpenAI、Anthropic、Ollama 本地模型和自定义用户配置模型。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;&lt;a href="https://github.com/editor-code-assistant/eca-emacs"&gt;使用 eca-emacs 的&lt;/a&gt;演示&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt="" height="302" src="https://static.oschina.net/uploads/space/2025/0814/140948_Py2U_4252687.gif" width="500" referrerpolicy="no-referrer"&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href="https://github.com/editor-code-assistant/eca-vscode"&gt;使用 eca-vscode 的&lt;/a&gt;演示&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;img alt="" height="302" src="https://static.oschina.net/uploads/space/2025/0814/141100_VXGm_4252687.gif" width="500" referrerpolicy="no-referrer"&gt;&lt;/em&gt;&lt;/p&gt;

                                                                    &lt;/div&gt;
                                                                </description>
      <link>https://www.oschina.net/p/eca</link>
      <guid isPermaLink="false">https://www.oschina.net/p/eca</guid>
      <pubDate>Tue, 12 Aug 2025 08:28:00 GMT</pubDate>
    </item>
    <item>
      <title>阶跃星辰发布 StepFun-Prover-Preview 系列模型，用于形式化定理证明</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;阶跃星辰（StepFun）发布了 StepFun-Prover-Preview 系列模型，包括 7B 和 32B 两个版本，专门用于形式化定理证明（formal theorem proving）。&lt;/p&gt; 
&lt;p&gt;据介绍，StepFun-Prover-Preview&amp;nbsp;专为数学推理设计，通过工具集成推理（tool-integrated reasoning）实现高效的形式化定理证明，支持 Lean 4 证明语言。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0814/162405_SPkB_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;论文地址：&lt;em&gt;https://arxiv.org/abs/2507.20199&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;性能方面，StepFun-Prover-Preview-7B 在 miniF2F-test (pass@1) 基准测试中，其表现与 DeepSeek-Prover-V2-671B 和 Kimina-Prover-72B 持平。更强大的 StepFun-Prover-Preview-32B 模型在 miniF2F-test (pass@1) 上超越了所有已知同类模型 4% 以上。该系列模型还具备类人证明细化能力，非常适合推动数学推理领域研究的科研人员。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0814/162334_3IsT_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;模型地址：&lt;em&gt;https://huggingface.co/stepfun-ai/StepFun-Prover-Preview-32B&lt;/em&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/366192</link>
      <guid isPermaLink="false">https://www.oschina.net/news/366192</guid>
      <pubDate>Tue, 12 Aug 2025 08:25:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>微软本月从 Windows 11 移除 PowerShell 2.0</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;微软&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fsupport.microsoft.com%2Fzh-cn%2Ftopic%2F%25E4%25BB%258E-windows-%25E4%25B8%25AD%25E5%2588%25A0%25E9%2599%25A4-powershell-2-0-fe6d1edc-2ed2-4c33-b297-afe82a64200a"&gt;发布&lt;/a&gt;公告称，将从 Windows 11 版本 24H2 中彻底移除 Windows PowerShell 2.0 这一遗留组件。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0814/155423_MsTY_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;PowerShell 2.0 最早随 Windows 7 推出，早在 2017 年，已被微软官方弃用。此次移除旨在清理旧代码、降低系统复杂性、提升安全性，并不会影响 PowerShell 5.1 或 PowerShell 7.x 的正常使用。&lt;/p&gt; 
&lt;p&gt;Windows 11 23H2 及更早版本仍保留 PowerShell 2.0 作为可选组件，用户可手动启用或卸载。&lt;/p&gt; 
&lt;p&gt;根据支持文档，大多数用户不会注意到这一变化，因为 PowerShell 2.0 的移除不会影响 PowerShell 5.1 和 PowerShell 7.x 的使用。而且由于 PowerShell 5.1（默认模式）具有向后兼容性，大多数脚本应该可以按预期运行。&lt;/p&gt; 
&lt;p&gt;但对于某些特定需要 PowerShell 2.0 的应用程序，可能会出现安装失败的情况，因此微软建议用户使用较新的版本以避免兼容性问题。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/366187</link>
      <guid isPermaLink="false">https://www.oschina.net/news/366187</guid>
      <pubDate>Tue, 12 Aug 2025 07:56:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>​字节开源 Agent 专用模型 M3-Agent-Control</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;字节跳动在人工智能领域再度发力，推出了最新的 AI 模型 ——M3-Agent-Control。该模型旨在推动智能技术的开放和普及，为各行业的发展提供强有力的技术支持。M3-Agent-Control 的问世，不仅展示了字节跳动在 AI 领域的创新实力，也为开发者和企业提供了更多的可能性。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;&lt;img height="317" src="https://oscimg.oschina.net/oscnet/up-79e584a04d882e72ff6db02d29cdfd15668.png" width="500" referrerpolicy="no-referrer"&gt;&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;M3-Agent-Control 基于 Qwen332B 训练，是一个拥有 328 亿参数的高性能 AI 模型，采用了 BF16 的张量类型。这意味着该模型在处理复杂计算时，能够更快地响应并提供更高效的结果。字节跳动一直以来致力于通过开放源代码和科学共享的方式，加速人工智能技术的发展，M3-Agent-Control 的推出正是这一战略的延续。&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#242424; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span style="color:#000000"&gt;目前，M3-Agent-Control 尚未由任何推理提供者进行部署，但字节跳动鼓励更多的开发者和企业参与到这个项目中来，探索 AI 的无限可能性。通过这个模型，开发者可以进行各种应用场景的开发，推动智能化进程，为社会各个领域带来技术革新。&lt;/span&gt;&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/366182</link>
      <guid isPermaLink="false">https://www.oschina.net/news/366182</guid>
      <pubDate>Tue, 12 Aug 2025 07:41:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>蚂蚁集团开源新一代 JVM 即时编译器 Jeandle</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;蚂蚁集团&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FYaMmozWGMKV3x7ZM5IP2KQ" target="_blank"&gt;宣布&lt;/a&gt;正式开源基于 LLVM 的 JVM JIT 编译器 Jeandle。公告写道：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;以「筋斗云」为喻，希望 Jeandle 可以为 JVM 加足马力，拓宽它的性能与生态边界，让 Java 如腾云驾雾般瞬息万里。&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0814/152859_uBKb_2720166.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;开源地址：&lt;/p&gt; 
&lt;p&gt;https://github.com/jeandle/jeandle-jdk&lt;br&gt; https://github.com/jeandle/jeandle-llvm&lt;/p&gt; 
&lt;p&gt;据介绍，Jeandle 是基于 OpenJDK Hotspot JVM 的全新 Just-In-Time（简称 JIT，即时）编译器，利用 LLVM 进行编译优化与代码生成，将 LLVM 的性能优势和生态优势引入 JVM 中。&lt;/p&gt; 
&lt;p&gt;&lt;img height="533" src="https://static.oschina.net/uploads/space/2025/0814/152948_uqyv_2720166.png" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;为了整合 JVM 和 LLVM 两个复杂的系统，Jeandle 需要攻克多个技术难题：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;支持 JVM 的垃圾回收机制&lt;/li&gt; 
 &lt;li&gt;为 JVM 中的各种功能分别定制 LLVM 特性&lt;/li&gt; 
 &lt;li&gt;基于 LLVM 实现针对 Java 语言的多类优化算法&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;......&lt;/p&gt; 
&lt;p&gt;Jeandle 开源伊始，目前已经实现了若干关键功能，同时也有大量的研发工作仍在进行中。未来规划：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;2025 年全量 Bytecode 支持：社区计划在今年年底的版本中完成各类基础功能的支持，包括 exception、GC、sychronization 等等，覆盖全量的 bytecode。&lt;/li&gt; 
 &lt;li&gt;2026 年持续聚焦于性能优化的「黑科技」：&lt;/li&gt; 
 &lt;li&gt;推出&amp;nbsp;Java 定制优化套件：研发针对 Java 语言的各类优化算法，使 Jeandle 具备全面的优化能力，包括但不限于锁优化、类型分析、逃逸分析、inline 等。同时实现基于运行时 profile 信息的优化能力和 deoptimization 能力&lt;/li&gt; 
 &lt;li&gt;加入 intrinsic：通过针对各类特殊场景定制的高效代码提升 Java 语言性能&lt;/li&gt; 
 &lt;li&gt;支持 on-stack replacement&lt;/li&gt; 
 &lt;li&gt;支持 G1 GC 算法&lt;/li&gt; 
&lt;/ul&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/366179</link>
      <guid isPermaLink="false">https://www.oschina.net/news/366179</guid>
      <pubDate>Tue, 12 Aug 2025 07:31:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>小马智行 Q2 财报亮眼，文远知行 Robotaxi 营收暴涨，行业增长动能强劲</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p style="color:#000000; text-align:left"&gt;头部 Robotaxi 玩家相继公布 2025 年第二季度财报。数据显示，Robotaxi 行业正呈现出显著的向好态势。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;8 月 12 日，小马智行发布了二季报，管理层强调了 Robotaxi 业务的增长——收入为 1090 万元，同比增长 157.8%，营收占比 7.09%，毛利率 16.1%。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;7 月 31 日，文远知行披露了二季度的财报，在报告中、Robotaxi 的营收增长异常亮眼——营收 4590 万元，同比大幅增长 836.7%，营收占比 36.1%，毛利率 28.1%。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:center"&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//a61df81f0738135b5fa89608d73057b5.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;从这两家头部企业的财报数据来看，Robotaxi 业务的高速增长并非个例，而是行业发展的缩影。这种增长态势背后，离不开技术的持续迭代与运营规模的不断扩大。随着自动驾驶技术的日益成熟，Robotaxi 的安全性和稳定性得到了大幅提升，这为其大规模商业化运营奠定了坚实基础。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;以文远知行为例，目前已在全球 10 个国家的 30 多个城市布局自动驾驶研发测试与商业化运营网络。尤其在过去半年，企业深耕中东市场取得突破性进展，成功打通了当地自动驾驶服务的商业化路径。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:center"&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//1c70bb776ad3e2874b4824409ba35d02.jpeg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;2025 年 7 月 28 日，文远知行宣布旗下 Robotaxi 获得沙特阿拉伯首张自动驾驶牌照。由此，文远知行成为全球唯一一家旗下产品拥有六国自动驾驶牌照的科技公司，涵盖沙特、中国、阿联酋、新加坡、法国和美国。凭借该牌照，文远知行获准在沙特开展自动驾驶运营，并可在沙特全国范围内部署 Robotaxi 服务。首期，文远知行携手 Uber 及当地合作伙伴 AiDriver 在利雅得开展试点运营。&lt;/p&gt; 
&lt;p style="color:#000000; text-align:center"&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//25fde64980a0b1627caa7b1513c924a1.jpeg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p style="color:#000000; text-align:left"&gt;整体来看，Robotaxi 行业在技术突破、市场扩张与商业化落地等方面均取得实质性进展，未来增长潜力值得期待。随着政策对自动驾驶商业化的支持力度加大、技术成本的进一步优化以及用户接受度的不断提高，Robotaxi 行业正从「技术验证期」全面迈入「规模增长期」，未来在出行市场的渗透率将持续提升，长期增长动能强劲。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/366177</link>
      <guid isPermaLink="false">https://www.oschina.net/news/366177</guid>
      <pubDate>Tue, 12 Aug 2025 07:27:00 GMT</pubDate>
      <author>作者: 开源科技</author>
    </item>
    <item>
      <title>AI 初创公司 Midjourney 更新功能，允许标准订阅用户生成高清视频</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;&lt;span&gt;AI 初创公司 Midjourney&amp;nbsp;&lt;/span&gt;今天&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Fmidjourney%2Fstatus%2F1955773050751963144" target="_blank"&gt;宣布&lt;/a&gt;，他们根据社区反馈发布了一系列的新功能，其中标准订阅用户现在可以生成高清视频。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0814/152433_quiN_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;此外还有改善的视频审核功能和批量制作视频等功能。具体如下：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;服务更新相关：基于社区反馈对服务进行了一系列小改进，包括 HD 视频生成对 Standard Plan 用户开放、视频作业可生成更小批量（1 或 2 个视频/次，通过设置面板或命令行参数--bs 1 、--bs 2 触发 ）、Moodboards 分离到侧边菜单可访问的独立页面、视频作业缩略图改为显示最后一帧、视频内容审核准确性提升 。&lt;/li&gt; 
 &lt;li&gt;涉及公司/团队：未明确提及具体公司，但围绕服务更新，推测是某提供视频等服务的团队 。&lt;/li&gt; 
 &lt;li&gt;提及的人物标签：@everyone@here ，属于通知类标签 。&lt;/li&gt; 
 &lt;li&gt;相关话题标签：#ideas-and-features ，用于提交想法和功能建议 。&lt;/li&gt; 
&lt;/ol&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/366176</link>
      <guid isPermaLink="false">https://www.oschina.net/news/366176</guid>
      <pubDate>Tue, 12 Aug 2025 07:25:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>字节跳动 VeOmni 框架开源：统一多模态训练效率飞跃！</title>
      <description>&lt;div class="content"&gt;
                                                                                                                    
                                                                                                                                                    &lt;p&gt;资料来源： &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.volcengine.com%2F" target="_blank"&gt;火山引擎-开发者社区&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;多模态时代的训练痛点，终于有了「特效药」&lt;/p&gt; 
&lt;p&gt;当大模型从单一语言向文本 + 图像 + 视频的多模态进化时，算法工程师们的训练流程却陷入了 「碎片化困境」：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;当业务要同时迭代 DiT、LLM 与 VLM 时，很难在一套代码里顺畅切换；&lt;/li&gt; 
 &lt;li&gt;而当模型形态一旦变化，底层并行组合和显存调度往往需要大量手工改写，耗时耗力；&lt;/li&gt; 
 &lt;li&gt;DIT 模型蒸馏需要大量的资源消耗，但是缺少高效的训练 infra 支持来提升效率……&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;这些困扰行业的痛点，字节跳动的工程师们早就遇到了 —— 于是，VeOmni 应运而生。作为字节内部验证过的 「统一多模态训练框架」，它经过内部千卡级别真实训练任务检验，训练了 UI-Tars1.5 等重要模型，为了能将字节跳动核心 AI Infra 能力服务更多用户，字节跳动决定开源 VeOmni，火山引擎基于进一步上支持了视频模型训练等功能，让 VeOmni 支持了更多模型训练场景，可以更好地服务更多用户。&lt;/p&gt; 
&lt;span id="OSC_h4_1"&gt;&lt;/span&gt; 
&lt;h4&gt;&lt;strong&gt;VeOmni 是什么？一套框架，搞定所有多模态训练&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;VeOmni 是字节 Seed 团队与火山机器学习平台、IaaS 异构计算团队联合研发的统一多模态模型训练框架，核心定位是三个统一：「统一多模态、统一并行策略、统一算力底座」。&lt;/p&gt; 
&lt;p&gt;它通过统一的 API 将 LoRA 轻量微调、FSDP、Ulysses 和 Expert Parallel 等多种混合并行策略以及自动并行搜索能力内置于框架内部。无论是百亿级语言模型、跨模态视觉语言模型，还是 480P/720P、长序列的文本到视频（T2V）或图像到视频（I2V）生成模型，开发者都能够基于统一的训练流程快速启动训练。&lt;/p&gt; 
&lt;p&gt;框架支持在千卡级 GPU 集群上自动完成权重张量的切分、通信拓扑的优化、动态显存回收和异步 checkpoint。在开源的 Wan 2.1 等模型上实测显示，相较于同类开源方案，VeOmni 能够将训练吞吐提高超过 40%，同时显著降低显存使用与跨节点通信带宽压力。&lt;/p&gt; 
&lt;p&gt;借助 VeOmni，字节跳动成功实现了「支持最快落地的新模型形态、最大化超大规模算力利用率、最小化业务改动成本」三大目标，有效弥补了开源社区训练框架在扩展性和抽象层面上的不足，为包括 LLM 和 VLM 在内的多模态生成场景提供了一条统一且高效的训练路径。&lt;/p&gt; 
&lt;p&gt;火山引擎的用户可在机器学习平台中运用 VeOmni 的强大功能。&lt;/p&gt; 
&lt;span id="OSC_h4_2"&gt;&lt;/span&gt; 
&lt;h4&gt;&lt;strong&gt;五大核心优势，破解训练效率瓶颈&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;VeOmni 的 「高效」 不是口号，而是用技术细节堆出来的。我们拆解了多模态训练的核心痛点，给出了针对性解决方案：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;显存计算双优化：用最少的额外计算，换最多的显存&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;传统 「大颗粒」 重计算要么全关、要么全开，往往用 10%–20% 的额外计算只换一点显存。VeOmni 不一样 —— 它先给每个前向张量算一笔 「ROI 账」：省 1MB 显存需要付出多少微秒计算。然后按 ROI 排序，只选性价比最高的算子重计算（比如 gate1_mul 省 40MB 只要 180μs，down_proj 要 4000μs，差距 22 倍！）。&lt;/p&gt; 
&lt;p&gt;VeOmni 框架在训练启动前自动把 ROI 排序，同等显存收益只选择性价比最高的算子进入重计算池：例如 gate1_mul 和 down_proj 都可回收约 40 MB，但前者只需 180µs、后者要 4000µs，差距达 22 倍。这样就能在保证显存不会超出的前提下，把额外计算开销压到最低。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//f15963df249589141d372ad78130413e.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;结果是：显存够用的前提下，额外计算开销压到最低。实测显示，相比按层重计算，VeOmni 的 Recompute 占比从 60% 降到 30%（Recompute 越低，效率越高），对 DiT 720P 视频训练、千亿 LLM 长序列训练效果显著。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;混合并行 「组合拳」：一键匹配最优算力切分方案，显存峰值降 55%&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;VeOmni 内置多维并行体系，支持 FSDP、Ulysses 和 Expert Parallel (EP) 等多种并行原语，通过启动脚本可以一键进行笛卡尔组合，自动搜索最优的算力切分方案：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;FSDP 负责将参数、梯度和优化器状态切片到各个 GPU，突破显存瓶颈，横向扩批简单可靠；&lt;/li&gt; 
 &lt;li&gt;Ulysses Parallel 针对长序列任务，将注意力沿 head 维度拆解，有效缓解单卡显存压力；&lt;/li&gt; 
 &lt;li&gt;Expert Parallel 专门适用于 MoE 模型，可高效支持超大规模专家网络的训练。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;这套并行体系已被应用于字节跳动内部多种模型的训练中，在处理 480P 和 720P 分辨率的 T2V/I2V 任务时，通过 FSDP 和 Ulysses 的组合，单轮迭代显存峰值可降低至原有基线的 45%。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;算子级性能深挖：小核算子融合，访存次数降百倍&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;针对 DiT 中大量「小核算子」导致的访存抖动，VeOmni 把注意力-FFN-残差链路重写为单核 Kernel，长序列下显存碎片显著减少，访存次数下降数百倍。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//afce03f7d8cbcb2bff0b447b13dc1dc4.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//bb18ae198fde761b3d2d081c8696cb6b.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;跨模型通吃：LLM/VLM/ 视频生成，一套框架全搞定&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;VeOmni 的优化不是 「针对性补丁」，而是对，生成式视频模型、千亿级语言模型，与 视觉语言模型，全部生效：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;DiT 训练显存减半；&lt;/li&gt; 
 &lt;li&gt;LLM 长上下文窗口训练从「手动调显存」变为「自动无感切分」；&lt;/li&gt; 
 &lt;li&gt;VLM 双塔/单塔架构在 Ring 模式下可线性扩展到更多 GPU，负载均衡无需改代码。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;凭借「算子粒度重计算 + 混合并行 + 算子融合/升级」三大引擎，VeOmni 把原本困扰开源框架的扩展瓶颈彻底拆解，为字节跳动以及合作伙伴在多模态内容生成和大模型服务化道路上提供了即插即用、极致高效的算力底座。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;蒸馏加速：减少推理步数，降低推理成本&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在生成式模型的推理中，步数蒸馏是提升效率的关键一环。然而，蒸馏的训练周期极长，需要的计算资源也十分庞大。VeOmni 集成了轨迹蒸馏、分布匹配蒸馏（DMD）、自回归蒸馏等学界前沿方法，并将框架原生的训练加速能力（如显存优化、混合并行等）应用在蒸馏算法上，极大地减少了蒸馏的迭代周期和资源消耗。用户可通过启动脚本指定蒸馏目标：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;支持 DMD 等效果优秀的蒸馏方法，能将模型稳定蒸馏至 4 步、8 步等目标步数甚至更少；&lt;/li&gt; 
 &lt;li&gt;支持蒸馏掉 CFG（无分类器引导）以消除冗余计算；&lt;/li&gt; 
 &lt;li&gt;支持用户自由编排蒸馏工作流，例如组合 「轨迹蒸馏预处理 + DMD 精调」 的多阶段蒸馏逻辑。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;经由 VeOmni 蒸馏出的模型能够显著减少推理所需步数，同时保持生成结果的高质量，这对于降低计算成本、加速模型部署具有重要意义。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;实测性能：比开源方案快 40%，多场景数据说话&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;VeOmni 的效率不是 「自说自话」，而是用真实模型测试验证的，以 Wan2.1-14B 模型为例（Lora 训练）：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;计算型大卡：I2V 720P 训练速度比开源方案快 48% 以上，T2V 720P 快 44.4% 以上；&lt;/li&gt; 
 &lt;li&gt;访存型大卡：I2V 720P 快 59.5% 以上，T2V 720P 快 57.4% 以上；&lt;/li&gt; 
 &lt;li&gt;小参数量模型（Wan2.1-1.3B）：T2V 480P 训练速度提升 51% 以上。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;卡型 1（计算型大卡）&lt;/p&gt; 
&lt;p&gt;I2V 训练速度：&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//f3127bf08dc9774d20b1262c87352e99.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;T2V 训练速度：&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//f39465da6844f0065bb3fd82d1a9d701.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;卡型 2（访存型大卡）&lt;/p&gt; 
&lt;p&gt;I2V 训练速度：&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//21548247b760eccd256454a9b19f30b7.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;T2V 训练速度：&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//1c05eb55c9148e8fc9ee41fbc7df7de4.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;不管是大模型还是小模型，不管是计算型还是访存型硬件，VeOmni 都能让算力发挥到极致！&lt;/p&gt; 
&lt;span id="OSC_h4_3"&gt;&lt;/span&gt; 
&lt;h4&gt;&lt;strong&gt;上手超简单：火山平台一键训练，性能分析可视化&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;VeOmni 不是 「专家专属工具」，而是开箱即用。目前，火山引擎在机器学习平台和 AI 云原生训练套件 TrainingKit 上都提供了 VeOmni 训练的最佳实践。下面我们以机器学习平台实践为例，介绍基于 VeOmni 训练框架对开源模型 Wan 进行 lora 训练，后续我们将推出基于 TrainingKit 的 VeOmni 部署实践。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;创建训练任务&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在快速入门选择需要训练的模型，并且配置实例规格及模型输出路径。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//31590466661f9e2eef65fef5916f8414.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//f88fe1716fd8d5abaf5b023312375de0.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;查看训练任务详情&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;创建任务后可在「任务详情-日志」中查看训练详情。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//8a54f636c8d814cc29a730da2628b5c1.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;GPU 性能分析&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;1.导航到，自定义任务 &amp;gt; 任务详情，页面，在目标任务的管理页面单击，创建性能分析。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//82c9f5d93421f97b88b854ded7e4f46d.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;2.完成采集后，可在性能分析结果列表页面管理所有分析任务。点击「查看详情」将跳转至 perfetto 中展示性能分析火焰图。&lt;/p&gt; 
&lt;p&gt;每个 Worker 节点会根据其拥有的 GPU 数量或进程数生成多个进程文件。平台会将这些文件聚合成一个单一的结果文件，并根据 perfetto 的限制进行自动分片。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//b6a5e8d04c0237d22283b1f6eb94d3e2.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//a0617c1987ef8a9f509d17535d5c12b9.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;span id="OSC_h4_4"&gt;&lt;/span&gt; 
&lt;h4&gt;从训练到推理，全链路打通&lt;/h4&gt; 
&lt;p&gt;目前火山引擎机器学习平台提供的数据集是用来训练飞天效果的 Lora 数据集，客户也可以自己选择合适的数据集进行预处理后来进行训练，具体使用方法随后更新。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;获取输出结果&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在一键训练的时候客户指定了训练的模型结果保存的地方，如下图所示。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://oscimg.oschina.net/oscnet//6cac77d83565659304a433c9f88d9e67.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span&gt;输出模型结果保存文件路径类似下图&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span&gt;&lt;code class="language-text"&gt;checkpoints/  
├── &lt;span style="color:#d73a49"&gt;global&lt;/span&gt;\_step\_xxx/           &lt;span style="color:#6a737d"&gt;# 每次保存的权重快照  &lt;/span&gt;
│   ├── extra\_state/            &lt;span style="color:#6a737d"&gt;# 训练状态（按 rank 切分）  &lt;/span&gt;
│   │   └── extra\_state\_rank\_*.pt  
│   ├── hf\_ckpt/                &lt;span style="color:#6a737d"&gt;# HuggingFace 兼容格式  &lt;/span&gt;
│   │   ├── config.json  
│   │   └── diffusion\_pytorch\_model.safetensors  
│   ├── model/                  &lt;span style="color:#6a737d"&gt;# 模型参数分片  &lt;/span&gt;
│   │   └── \_\_*\_*.distcp  
│   └── optimizer/              &lt;span style="color:#6a737d"&gt;# 优化器状态分片  &lt;/span&gt;
│       └── \_\_*\_*.distcp  
└── latest\_checkpointed\_iteration.txt  &lt;span style="color:#6a737d"&gt;# 记录最新步数  &lt;/span&gt;&lt;/code&gt;&lt;/span&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span&gt;客户可以到 hf_ckpt 下看到保存的 Lora 训练权重，得到的路径为 checkpoints/global_step_xxx/hf_ckpt/diffusion_pytorch_model.safetensors&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span&gt;使用脚本进行权重格式转换&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span&gt;将下列代码保存为 convert.p&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre style="margin-left:0; margin-right:0; text-align:left"&gt;&lt;span&gt;&lt;code class="language-text"&gt;&lt;span style="color:#6a737d"&gt;#!/usr/bin/env python  # convert.py  ——  把 「blocks.…default.weight」 → 「diffusion\_model.blocks.…weight」  from pathlib import Path  &lt;/span&gt;
from safetensors.torch import &lt;span style="color:#d73a49"&gt;load&lt;/span&gt;\_file, &lt;span style="color:#d73a49"&gt;save&lt;/span&gt;\_file  
&lt;span style="color:#d73a49"&gt;import&lt;/span&gt; &lt;span style="color:#d73a49"&gt;sys&lt;/span&gt;  
  
&lt;span style="color:#d73a49"&gt;if&lt;/span&gt; &lt;span style="color:#d73a49"&gt;len&lt;/span&gt;(sys.argv) != &lt;span&gt;2&lt;/span&gt;:  
    sys.exit(f&lt;span style="color:#032f62"&gt;"用法: python {Path(\_\_file\_\_).name} &amp;lt;input.safetensors&amp;gt;"&lt;/span&gt;)  
  
inp = &lt;span style="color:#d73a49"&gt;Path&lt;/span&gt;(sys.argv[&lt;span&gt;1&lt;/span&gt;]).expanduser().resolve()  
&lt;span style="color:#d73a49"&gt;out&lt;/span&gt; = inp.with\_name(inp.stem + &lt;span style="color:#032f62"&gt;"\_styleB.safetensors"&lt;/span&gt;)  
  
tensors = &lt;span style="color:#d73a49"&gt;load&lt;/span&gt;\_file(&lt;span style="color:#d73a49"&gt;str&lt;/span&gt;(inp))  
converted = {}  
  
&lt;span style="color:#d73a49"&gt;for&lt;/span&gt; k, v &lt;span style="color:#d73a49"&gt;in&lt;/span&gt; tensors.items():  
    &lt;span style="color:#6a737d"&gt;# 若无前缀则加 diffusion\_model.  &lt;/span&gt;
    ifnot k.startswith(&lt;span style="color:#032f62"&gt;"diffusion\_model."&lt;/span&gt;):  
        k = &lt;span style="color:#032f62"&gt;"diffusion\_model."&lt;/span&gt; + k  
    &lt;span style="color:#6a737d"&gt;# 去掉 .default.  &lt;/span&gt;
    k = k.replace(&lt;span style="color:#032f62"&gt;".default."&lt;/span&gt;, &lt;span style="color:#032f62"&gt;"."&lt;/span&gt;)  
    converted[k] = v  
  
&lt;span style="color:#d73a49"&gt;save&lt;/span&gt;\_file(converted, &lt;span style="color:#d73a49"&gt;str&lt;/span&gt;(&lt;span style="color:#d73a49"&gt;out&lt;/span&gt;))  
print(f&lt;span style="color:#032f62"&gt;"✓ 已保存: {out}"&lt;/span&gt;)&lt;/code&gt;&lt;/span&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span&gt;执行下面的命令，得到转换后的权重&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div style="text-align:left"&gt; 
 &lt;pre style="margin-left:0; margin-right:0"&gt;&lt;span&gt;&lt;code class="language-text"&gt;&lt;span style="color:#6f42c1"&gt;python&lt;/span&gt; &lt;span style="color:#032f62"&gt;convert.py   &lt;/span&gt;
&lt;span style="color:#6a737d"&gt;yourpath/checkpoints/global\_step\_xxx/hf\_ckpt/diffusion\_pytorch\_model.safetensors&lt;/span&gt;     &lt;/code&gt;&lt;/span&gt;&lt;/pre&gt; 
&lt;/div&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span&gt;使用 Vefuser 推理&lt;/span&gt;&lt;/p&gt; 
&lt;p style="color:#333333; margin-left:0; margin-right:0; text-align:left"&gt;&lt;span&gt;训练完的模型，用火山 veFuser 推理更高效 ——veFuser 是火山引擎的扩散模型服务框架，针对 VeOmni 训练的 LoRA / 全量微调模型做了优化，能实现 「超低延迟」 视频生成。从训练到部署，全链路流畅！&lt;/span&gt;&lt;/p&gt;
                                                                                    &lt;/div&gt;
                                                                            </description>
      <link>https://my.oschina.net/u/6800876/blog/18688206</link>
      <guid isPermaLink="false">https://my.oschina.net/u/6800876/blog/18688206</guid>
      <pubDate>Tue, 12 Aug 2025 07:19:00 GMT</pubDate>
      <author>原创</author>
    </item>
    <item>
      <title>全球首个 AI Agent 市场「Mule Run」开启 Beta 测试</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;MuleRun &lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Fmulerun_ai%2Fstatus%2F1955661198072115373" target="_blank"&gt;宣布&lt;/a&gt;其 AI Agent 市场「Mule Run」开启 Beta 测试。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0814/151422_yt3n_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Mule Run 定位为全球首个 AI Agent 市场，旨在成为 AI 领域的 「eBay」，用户只需一个入口即可访问大量 AI Agent。这些 Agent 能够执行多种任务，包括游戏、编程，甚至帮助用户赚钱。&lt;/p&gt; 
&lt;p&gt;&lt;img alt="" src="https://static.oschina.net/uploads/space/2025/0814/151614_zZWD_2720166.jpg" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;Mule Run 的 Beta 测试目前采用邀请制。感兴趣的用户可以通过 Discord 加入其社区以获取更多信息。MuleRun 还通过社交媒体活动提供 5 个激活码，有效期为 72 小时，参与者需转发、关注并开启通知。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/366173</link>
      <guid isPermaLink="false">https://www.oschina.net/news/366173</guid>
      <pubDate>Tue, 12 Aug 2025 07:16:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
    <item>
      <title>AI 正以创纪录的速度创造新的亿万富翁</title>
      <description>&lt;div class="content"&gt;
                                                                                            &lt;p&gt;&lt;a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.cnbc.com%2F2025%2F08%2F10%2Fai-artificial-intelligence-billionaires-wealth.html" target="_blank"&gt;据 CNBC 报道&lt;/a&gt;，全球已有近 500 家 AI 「独角兽」（估值超过 10 亿美元）的初创企业，它们的总估值达 2.7 万亿美元，且其中 100 家是在近两年内成立。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://static.oschina.net/uploads/space/2025/0814/150500_YLO8_2720166.png" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;这波浪潮造就了数十位新晋亿万富翁，集中分布在旧金山湾区。代表人物包括 Scale AI 联合创始人亚历山大·王（36 亿美元）、郭露西（10 亿美元+）、Anthropic CEO 达里奥·阿莫迪（12 亿美元+）、CoreWeave CEO 迈克尔·因特罗特（100 亿美元）、DeepSeek CEO 梁文锋、Figure AI 创始人布雷特·阿德科克、Perplexity CEO 阿拉温德·斯里尼瓦斯，以及 OpenAI 前高管伊利亚·苏茨凯夫和米拉·穆拉蒂等。&lt;/p&gt; 
&lt;p&gt;&lt;img height="241" src="https://static.oschina.net/uploads/space/2025/0814/145511_mDy0_2720166.png" width="1080" referrerpolicy="no-referrer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;来源：量子位 https://mp.weixin.qq.com/s/tIYoRz4zm6SlylG-eq-maw&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;这些公司普遍保持私营，依赖风投、主权基金等融资，并频繁发生并购与股权转让。创始人更注重财富管理与二级市场操作，如股权抵押借款、投资同类科技公司等。&lt;/p&gt; 
&lt;p&gt;据了解，AI 财富高度集中于湾区，其中旧金山成为全球亿万富翁聚集地，生活成本、房地产等也因 AI 扎根而交由巨大影响。&lt;/p&gt;
                                                                                &lt;/div&gt;
                                                                            </description>
      <link>https://www.oschina.net/news/366170</link>
      <guid isPermaLink="false">https://www.oschina.net/news/366170</guid>
      <pubDate>Tue, 12 Aug 2025 07:07:00 GMT</pubDate>
      <author>来源: OSCHINA</author>
    </item>
  </channel>
</rss>
